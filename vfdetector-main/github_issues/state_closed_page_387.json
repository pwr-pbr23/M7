[{"number": 42368, "title": "Why add the parameter \u2018fallback_to_while_loop\u2019 in the 2.3 API tf.vectorized_map( fn, elems, fallback_to_while_loop=True)", "body": "Hello,\r\n       Does the parameter 'fallback_to_while_loop' is to fix some bugs so as to the function could perform normally?", "comments": ["@zjzh,\r\nIf `fallback_to_while_loop` is set to `True`, on failing to vectorize an operation, the unsupported op is wrapped in a tf.while_loop to execute the map iterations. \r\n\r\n\r\nFor more information, please take a look at the [documentation](https://www.tensorflow.org/api_docs/python/tf/vectorized_map#args_1) for the arguments of `tf.vectorized_map`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42367, "title": "\"No module named tensorflow\" though it is installed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code:\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): \"pip install tensorflow-gpu\"\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.10\r\n- CUDA/cuDNN version: Cuda==10.1, cuDNN==7.6.5\r\n- GPU model and memory: NVIDIA GeForce GT 710\r\n\r\n\r\n**Describe the current behavior**\r\nTensorflow is installed and works if run it from the Terminal. \"Hello World\" test for Tensorflow also works. \r\nWhen Running code from Jupyter an error occurs (described lower) \r\n\r\n**Describe the expected behavior**\r\nCode from Jupyter must run and give a default output photo.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe code is taken from your examples. The name: \"object_detection_tutorial.ipynb\"\r\n\r\n**Other info / logs** \r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-cdfb01024084> in <module>\r\n      4 import sys\r\n      5 import tarfile\r\n----> 6 import tensorflow as tf\r\n      7 import zipfile\r\n      8 \r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n", "comments": ["I am having the same issue.\r\nI installed tensorflow following the website by creating a virtual env (conda create -n tf-gpu tensorflow-gpu;conda activate tf-gpu).\r\nMy current Python is 3.8.3 (Anaconda) but the tf-gpu python version is automatically 3.7.7 for compatibility.\r\nImporting  tensorflow works in terminal, but it does not in Jupyter after I imported the kernel using ipykernel.\r\nI removed the env and reinstalled it and removed the tf-gpu kernel in Jupyter and reimported it a few times. But Jupyter still cannot import tensorflow.", "This is probably an issue on your installation of packages. You can try adding the correct environment variables inside the notebook to find the tensorflow package:\r\n\r\n```python\r\nimport sys\r\n\r\nsys.path.append('/home/Kosta404/path_to_your_py_packages/')\r\n```\r\n\r\nAnyway, I would recommend having separated environments for system libraries and for development. You can have a look to virtualenv - venv packages.", "Maybe it is related to #42297 .", "@Kosta404 \r\nLooks like the Jupyter notebook is using a different Python interpreter from the one where TensorFlow is installed.\r\nPlease make sure you are installing TensorFlow and running Jupyter from the same Python installation and check if it works. \r\n\r\nAlso please check if your [AVX is supported](https://www.tensorflow.org/install/pip#hardware-requirements) (cpu supports AVX instruction sets.), if not  only options are either compiling from source or #19584\r\n\r\n\r\n\r\nCheck below resolve issues for reference:\r\n#32147 #40664 #9473 [link](https://stackoverflow.com/questions/46568913/tensorflow-import-error-no-module-named-tensorflow) [link1](https://stackoverflow.com/questions/42244198/importerror-no-module-named-tensorflow) [link2](https://geekcops.com/tutorials/modulenotfounderror-no-module-named-tensorflow-solved/) [link3](https://github.com/tensorflow/tensorflow/issues/10474#issuecomment-306668929)\r\nThanks!\r\n\r\n", "@knslee07 \r\nhttps://www.youtube.com/watch?v=tPq6NIboLSc\r\nthis one helped\r\nbut i did all this stuff on a newly-reinstalled system", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42367\">No</a>\n"]}, {"number": 42366, "title": "Error converting multilingual universal sentence encoder to TFLite. Input 1 of node StatefulPartitionedCall was passed float from statefulpartitionedcall_args_1:0 incompatible with expected resource.", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.3.0\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# I've downloaded model and unarchived it to save_path\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(save_path)\r\ntflite_model = converter.convert()\r\n```\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    496         results = c_api.TF_GraphImportGraphDefWithResults(\r\n--> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\n    498         results = c_api_util.ScopedTFImportGraphDefResults(results)\r\n\r\nInvalidArgumentError: Input 1 of node StatefulPartitionedCall/sequential/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/sequential/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/input/_1007:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-55fd8585264a> in <module>\r\n      1 #convert model to tensorflow lite\r\n      2 converter = tf.lite.TFLiteConverter.from_saved_model(save_path)\r\n----> 3 tflite_model = converter.convert()\r\n      4 # open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1074         Invalid quantization parameters.\r\n   1075     \"\"\"\r\n-> 1076     return super(TFLiteConverterV2, self).convert()\r\n   1077 \r\n   1078 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    876     frozen_func, graph_def = (\r\n    877         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n--> 878             self._funcs[0], lower_control_flow=False))\r\n    879 \r\n    880     input_tensors = [\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n   1107 \r\n   1108   frozen_func = _construct_concrete_function(func, output_graph_def,\r\n-> 1109                                              converted_input_indices)\r\n   1110   return frozen_func, output_graph_def\r\n   1111 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)\r\n    999   new_func = wrap_function.function_from_graph_def(output_graph_def,\r\n   1000                                                    new_input_names,\r\n-> 1001                                                    new_output_names)\r\n   1002 \r\n   1003   # Manually propagate shape for input tensors where the shape is not correctly\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)\r\n    648     importer.import_graph_def(graph_def, name=\"\")\r\n    649 \r\n--> 650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n    651   import_graph = wrapped_import.graph\r\n    652   return wrapped_import.prune(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n    626           signature=signature,\r\n    627           add_control_dependencies=False,\r\n--> 628           collections={}),\r\n    629       variable_holder=holder,\r\n    630       signature=signature)\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985 \r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987 \r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)\r\n     85 \r\n     86   def __call__(self, *args, **kwargs):\r\n---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)\r\n     88 \r\n     89   def call_with_variable_creator_scope(self, fn):\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)\r\n     91     def wrapped(*args, **kwargs):\r\n     92       with variable_scope.variable_creator_scope(self.variable_creator_scope):\r\n---> 93         return fn(*args, **kwargs)\r\n     94 \r\n     95     return wrapped\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()\r\n    646 \r\n    647   def _imports_graph_def():\r\n--> 648     importer.import_graph_def(graph_def, name=\"\")\r\n    649 \r\n    650   wrapped_import = wrap_function(_imports_graph_def, [])\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)\r\n    403       return_elements=return_elements,\r\n    404       name=name,\r\n--> 405       producer_op_list=producer_op_list)\r\n    406 \r\n    407 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)\r\n    499       except errors.InvalidArgumentError as e:\r\n    500         # Convert to ValueError for backwards compatibility.\r\n--> 501         raise ValueError(str(e))\r\n    502 \r\n    503     # Create _DefinedFunctions for any imported functions.\r\n\r\nValueError: Input 1 of node StatefulPartitionedCall/sequential/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/sequential/keras_layer/StatefulPartitionedCall/StatefulPartitionedCall/input/_1007:0 incompatible with expected resource.\r\n```\r\n\r\n**https://tfhub.dev/google/universal-sentence-encoder-multilingual/3**\r\n\r\nI've tried the large model also and got the same error. Can someone help me?", "comments": ["This model can be converted using TF_SELECT option, since it has some unsupported operations by TF Lite.\r\nSee https://www.tensorflow.org/lite/guide/ops_select\r\nAlso, added @abattery who converted this model before and can guide you if needed.", "@abattery Hi! Could you help me? \r\nThis approach doesn't work either:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nI'm getting the same error: \"ValueError: Input 1 of node StatefulPartitionedCall was passed float from statefulpartitionedcall_args_1:0 incompatible with expected resource.\"", "Sorry @Extremesarova \r\n\r\nActually, this model requires e2e hash table support. We are working on delivering the e2e hash table feature. I will update this thread when the feature is landed.", "Got it. \r\nCould you, please, suggest some pretrained multilingual embeddings from tf.hub, that can be converted to Tensorflow Lite?", "I had experiments on converting models in TF hub, especially for the models which need a hash table support. Except few exceptions, most of models will be convertible to TFLite after e2e hash table proposal including https://tfhub.dev/google/universal-sentence-encoder-multilingual/3.", "@abattery Hi! Could you help me with some workaround before delivering this feature? May you provide e2e hash table or something so that I can convert multilingual USE?", "> I had experiments on converting models in TF hub, especially for the models which need a hash table support. Except few exceptions, most of models will be convertible to TFLite after e2e hash table proposal including https://tfhub.dev/google/universal-sentence-encoder-multilingual/3.\r\n\r\nI'm also curious if there are any updates? Or a time-line", "Hi, guys! Any updates on the problem?", "@abattery Is there any update on the e2e hash table? I'm also running into this issue. Thanks!", "Any update would be great!\r\n", "There is a better support on the recent TF version through the tf.lite.TFLiteConverter.from_saved_model.", "was able to do the conversion using the latest tf version, however was running into problems using it on mobile. \r\n\r\nthis conversion worked for me\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\n```\r\nBut on mobile i get the error \r\n\r\n`Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference`\r\n\r\nI think https://www.tensorflow.org/lite/guide/ops_select#android_aar may be related, but wasnt able to get it to work as of now.\r\n\r\nsee https://github.com/am15h/tflite_flutter_plugin/issues/101 for more details ", "@Extremesarova,\r\n\r\nWe are checking to see if this is still an issue. Can you try updating TF to latest stable version of i.e `2.6.0` and let us know if the issue persists? Thanks! ", "@sanatmpa1 I will definitely try it one more time and will be back with updated information. \r\nLast time I've checked I was able to covert to tf-lite format and test inference of tf-lite model on Python backend, but unfortunately inference on Android device wasn't successful ", "@Extremesarova,\r\n\r\nThanks for the update and this [guide](https://www.tensorflow.org/lite/guide/inference) can be a good reference for you to test inference of tf-lite model.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanatmpa1, hi!\r\nI've tried everything and the inference on Android's backend doesn't work - error concerns the lack of SentencepieceOp.\r\nBut this is another issue, because this issue was about error in convertion to tf-lite - and it seems to work now. ", "@Extremesarova,\r\n\r\nThanks for the confirmation. In this case where it seems to work now, Can you close this issue and open a new one for Android backend issue, if it still persists in recent stable version? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42366\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42366\">No</a>\n"]}, {"number": 42365, "title": "[tflite] make `//tensorflow/lite/kernels:all` build", "body": "make\r\n```\r\nbazel build --config android_arm64 //tensorflow/lite/kernels:all\r\n```\r\nwork.\r\n\r\nI want to run all test cases on an Android devices, but  there are some problems why trying to build `//tensorflow/lite/kernels:all`. All of them are libm related problems.", "comments": ["Thx for reporting the issue! Could you paste the linking error output here? It's likely there's a build toolchain issue here that we didn't catch in our continuous integration tests.", "@yyoon and @terryheo, just wondering if this libm linking issue with bazel rings any bell to you? Many thanks!", "I'll check my ANDROID SDK and NDK settings.\r\n\r\nFYR. For `acceleration_test_util_internal_test`, I got\r\n```\r\nERROR: /hack/freedom/tensorflow/tf-clean/tensorflow/lite/kernels/BUILD:155:1: Linking of rule '//tensorflow/lite/kernels:acceleration_test_util_internal_test' failed (Exit 1)\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstrings.so: undefined reference to `nan'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sbase_Slibexponential_Ubiased.so: undefined reference to `log2'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Stime_Slibtime.so: undefined reference to `modf'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstrings.so: undefined reference to `frexp'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstrings.so: undefined reference to `nanf'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Snumeric_Slibint128.so: undefined reference to `truncl'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Snumeric_Slibint128.so: undefined reference to `ldexpl'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libexternal_Scom_Ugoogle_Uabsl_Sabsl_Sstrings_Slibstrings.so: undefined reference to `ldexpf'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\n```\r\n\r\nfor `eigen_support_test`, I got\r\n```\r\nERROR: /hack/freedom/tensorflow/tf-clean/tensorflow/lite/kernels/BUILD:269:1: Linking of rule '//tensorflow/lite/kernels:eigen_support_test' failed (Exit 1)\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libtensorflow_Slite_Skernels_Sinternal_Slibportable_Utensor_Uutils.so: undefined reference to `tanhf'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libtensorflow_Slite_Skernels_Sinternal_Slibquantization_Uutil.so: undefined reference to `logf'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libtensorflow_Slite_Skernels_Sinternal_Slibquantization_Uutil.so: undefined reference to `frexp'\r\nbazel-out/arm64-v8a-opt/bin/_solib_arm64-v8a/libtensorflow_Slite_Skernels_Sinternal_Slibportable_Utensor_Uutils.so: undefined reference to `expf'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "my ndk and sdk settings `.tf_configure.bazelrc`\r\n```\r\nuild --action_env ANDROID_NDK_HOME=\"/home/freedom/work/android-ndk-r20\"\r\nbuild --action_env ANDROID_NDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_BUILD_TOOLS_VERSION=\"29.0.2\"\r\nbuild --action_env ANDROID_SDK_API_LEVEL=\"28\"\r\nbuild --action_env ANDROID_SDK_HOME=\"/home/freedom/work/android-sdk-linux\"\r\n```", "@freedomtan \r\n\r\nI tend to think that it's probably better to add the linker flags to the final binary/test targets rather than the intermediate library targets. How exactly were you going to run the tests on Android devices, assuming the `kernels:all` build would succeed with this change?", "We are developing a delegate. Currently what we do is\r\n\r\n1.  build all kernels\r\n```\r\n$ bazel build --config android_arm64 //tensorflow/lite/kernels:all\r\n```\r\n\r\n2.  push all libraries to device\r\n```\r\n$ adb shell mkdir /data/local/tmp/test_libs\r\n$ adb push bazel-bin/_solib_arm64-v8a/lib*.so /data/local/tmp/test_libs\r\n$ adb push bazel-bin/_solib___Caarch64-linux-android-clang8.0.7-libcpp/lib*.so /data/local/tmp/test_libs\r\n```\r\n\r\n3. push what we want to test to device, e.g., \r\n```\r\n$ adb push bazel-bin/tensorflow/lite/kernels/*_test /data/local/tmp/kernel_tests\r\n```\r\n4.  then we can run test_add, \r\n```\r\n$ adb shell LD_LIBRARY_PATH=/data/local/tmp/test_libs /data/local/tmp/kernel_tests/add_test\r\n```\r\n\r\n5. we actually do something like the following on devices\r\n```\r\n$ cd /data/local/tmp\r\n$ for a in kernel_tests/*_test; do LD_LIBRARY_PATH=test_libs $a --use_our_delegate=1 >> kernel_test_our_delegate_results.txt 2>1&; done\r\n$ for a in kernel_tests/*_test; do LD_LIBRARY_PATH=test_libs $a --use_nnapi=1 >> kernel_test_nnapi_results.txt 2>1&; done\r\n$ for a in kernel_tests/*_test; do LD_LIBRARY_PATH=test_libs $a  >> kernel_test_tflite_results.txt 2>1&; done\r\n```\r\n6. pull those logs to a linux or mac and comparing them\r\n", "What about building them with `--linkopt=-lm` from the command line?\r\n\r\n    bazel build --config=android_arm64 --linkopt=-lm //tensorflow/lite/kernels:all\r\n\r\nWould this resolve your issue?\r\nAgain, I'm a bit hesitant because of the potential unintended side effects this change might cause to any downstream binaries.", "@yyoon using \"--linkopt=-lm\" looks strange to me since user need to know library dependencies. I think this kind of changes are fine and we already have the similar code. \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/BUILD#L808\r\nAlso all changes are about fixing test targets (acceleration_test_util_internal, eigen_support_test, kernel_util_test)", "Ah, I missed that we already have similar conditions in other places. Sounds good to me.", "@freedomtan  Can you please address Ubuntu Sanity errors? Thanks!", "FYI, this is the error log.\r\n\r\n```\r\n=== Sanity check step 4 of 15: do_buildifier (buildifier check) ===\r\n\r\nRunning do_buildifier on 527 files\r\n\r\ntensorflow/lite/kernels/BUILD # reformat callsort\r\n\r\nbuildifier took 0 s\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\ntensorflow/lite/kernels/BUILD:\r\n149a150,153\r\n>     linkopts = select({\r\n>         \"//tensorflow:windows\": [],\r\n>         \"//conditions:default\": [\"-lm\"],\r\n>     }),\r\n155,158d158\r\n<     linkopts = select({\r\n<         \"//tensorflow:windows\": [],\r\n<         \"//conditions:default\": [\"-lm\"],\r\n<     }),\r\n279a280,283\r\n>     linkopts = select({\r\n>         \"//tensorflow:windows\": [],\r\n>         \"//conditions:default\": [\"-lm\"],\r\n>     }),\r\n287,290d290\r\n<     linkopts = select({\r\n<         \"//tensorflow:windows\": [],\r\n<         \"//conditions:default\": [\"-lm\"],\r\n<     }),\r\n465a466,469\r\n>     linkopts = select({\r\n>         \"//tensorflow:windows\": [],\r\n>         \"//conditions:default\": [\"-lm\"],\r\n>     }),\r\n472,475d475\r\n<     linkopts = select({\r\n<         \"//tensorflow:windows\": [],\r\n<         \"//conditions:default\": [\"-lm\"],\r\n<     }),\r\nexit status 1\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "@terryheo  Thanks. Done. Sorry for forgetting to run `buildifier` before git push."]}, {"number": 42364, "title": "`tf.data.experimental.snapshot()` hangs when using GCS paths", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.5.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: Nvidia Tesla T4\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.data.experimental.snapshot()` hangs when using a Google Storage path.\r\n\r\n**Describe the expected behavior**\r\n`tf.data.experimental.snapshot()` works.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfor _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/my-path')): break\r\n```\r\n\r\nhangs. Using a local path\r\n\r\n```\r\nfor _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break\r\n```\r\n\r\nworks as expected.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["ping @piwell @carlthome ", "Confirming that I was able to reproduce this in Colab and with a GCP deep learning vm instance. Although it was hanging, the metadata files did get written to the bucket. Did you see the same?", "Yes, @nikitamaia, we saw the same behavior with files popping up on the bucket.", "> Using a local path\r\n> \r\n> ```\r\n> for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break\r\n> ```\r\n\r\nPing @dakl, the MCVE is not using a local path. Edit the OP?\r\n", "Thanks for reporting this issue! I've investigated and the underlying cause is gcs_file_system.cc not handling calls to `NewAppendableFile() `correctly when the file does not yet exist.\r\n\r\nThe deadlock happens due to the following sequence of events: \r\n\r\nThe error is propagated all the way back to `AsyncWriter::WriterThread`, which then returns with the error code.  \r\n\r\nHowever, by this time, the entire writing process has finished and the main iterator thread calls `SignalEOF` from `GetNextInternal`. `SignalEOF` acquires the WriterThread's `mu_` lock and then tries to clear() `writers_` vector, containing all the `AsyncWriters`. \r\n\r\nTo clear the vector, C++ needs to call AsyncWriter's default destructor, which then blocks on the `thread_` within `AsyncWriter` finishing.\r\n\r\nNow, the `AsyncWriter` thread is still trying to call the `done()` function that was passed in, which tries to acquire the same `mu_` lock that SignalEOF was already holding.\r\n\r\nThis results in a deadlock where the main thread calling `writers_.clear()` cannot proceed because the `AsyncWriter` thread has not terminated, but the `AsyncWriter` thread is blocked trying to acquire the `mu_` lock held by the main thread.\r\n\r\nWill fix the underlying problem and the deadlock in an upcoming commit.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364\">No</a>\n"]}, {"number": 42363, "title": "SimpleRNN possibly not using GPU", "body": "**System information**\r\nColab with K80 GPU\r\n\r\nTensorFlow git version: v2.3.0-0-gb36436b087 2.3.0\r\n\r\n**Describe the current behavior**\r\nThe SimpleRNN cell is taking close to 15 longer to run compared to the GRU, and so it appears that the simpleRNN is not using the GPU\r\n\r\n**Describe the expected behavior**\r\nIt should be faster, or at least as fast as the GRU layer. \r\n**Standalone code to reproduce the issue**\r\nHere's the [Colab](https://colab.research.google.com/drive/1SiLNLSweAMVf5O8i5UUDShv5Jf5AWtEA?usp=sharing) to reproduce the issue.", "comments": ["@joaogui1 \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/37a5c98528b19792fa606d7077630d3a/untitled373.ipynb).", "@Saduf2019 \r\nThe line `test_dataset = test_dataset.padded_batch(BATCH_SIZE)` is missing after doing the same for the train_dataset on the sixth cell", "@joaogui1 \r\nI ran the code shared, please refer to the [gist](https://colab.research.google.com/gist/Saduf2019/c072980edf074309a9cda6acd63232f0/untitled373.ipynb) if it confirms your issue.", "@Saduf2019 it confirms my issue (SimpleRNN takes 15 times longer than GRU)", "Hi @joaogui1, thanks for reporting the issue.\r\n\r\nI think the performance of GRU is because of fused cudnn kernel used, which is way faster than standard kernel. Unfortunately, we don't have a fused cudnn kernel for simpleRNN (cudnn kernel is provide by Nvidia), which is way simpleRNN is slower than GRU. It doesn't necessary mean the simpleRNN doesn't run on GPU.\r\n\r\nI think there isn't any action item I need to address here. Closing this bug for now, and feel free to reopen if you feel there are other issue need to be addressed."]}, {"number": 42362, "title": "Run SWIG in the default environment", "body": "This avoids failures when swig is build in custom envs e.g. with a compiler installed into another location (e.g. /opt)\r\nSee also https://github.com/bazelbuild/bazel/issues/4053\r\nPatch by @boegel from https://github.com/bazelbuild/bazel/issues/4053#issuecomment-343134886\r\n\r\nFixes #41806", "comments": ["What version or branch of TensorFlow is this for? I got rid of all the SWIG compile logic around 2.2-2.3. Would it make more sense as a patch to a specific branch? ", "Well the code here is unchanged so I just assumed that the issue still persists. (We always build with this patch applied)\r\nOr is the `_py_wrap_cc ` rule unused? If so it should be removed. \r\n\r\nBesides that this has been an issue since forever so it could be applied to any branch if another release is planned for that.", "Okay, sounds good. I'll approve this so anyone can cherrypick easily into any branch. I'll send a change to remove that rule as I don't think anyone still uses it at HEAD.", "FTR: Do you know which released version does not use this anymore? Or the commit/PR which removed that? Then we could remove our patch from that version.", "https://github.com/tensorflow/tensorflow/commit/7fd9e941e10f76abcea3a398fd02eb93ca343012#diff-38c70d53f71218cab252a7f458600d6d\r\nhttps://github.com/tensorflow/tensorflow/commit/5fb9558424c2bf8f8c3de762a34bec59c11fa26e#diff-38c70d53f71218cab252a7f458600d6d\r\n\r\n2.3 will have these changes.", "Alright thanks! As a follow-up it might be a good idea to remove all references to swig.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/5fb9558424c2bf8f8c3de762a34bec59c11fa26e#diff-bb845bf0c867fef59b6527c0946799f9L213 looks like it is no longer used but e.g. https://github.com/tensorflow/tensorflow/blob/505a1599c3abfb11fcaafd53d28830886ffd30f8/third_party/systemlibs/syslibs_configure.bzl#L44 still has it.", "That's a good idea, I'll look into this now. Thanks!", "@Flamefire can you please resolve conflicts ?", "Actually sorry, my commit deleting the SWIG stuff went in early. If you'd like you can make the PR to the 2.2 branch and others can cherrypick that commit.", "Rebased and changed the target branch", "Test failures look like what is fixed with https://github.com/tensorflow/tensorflow/pull/40728 (Numpy 1.19 compat)", "> Test failures look like what is fixed with #40728 (Numpy 1.19 compat)\r\n\r\nFor the Windows builds - yes.\r\nThe 2.2 branch has the following line in its setup.py `'numpy >= 1.16.0, < 2.0',`, although I doubt that the CI system honours setup.py. I am unsure from where the CI system gets its environment from, given the fact that the Linux builds succeeded, while the Windows ones did not.\r\nThe master branch had the upper limit on numpy changed to 1.19, exclusive.\r\n\r\nThe MacOS build failures are unrelated to the numpy 1.19 compatibility issue, no?", "PRs against release branches cannot use the same CI infrastructure as PRs against master. We need to wait until there is a patch release and only then can evaluate PR for mergeability.\r\n\r\nSince 2.3 is out, you might want to also make a PR against `r2.3` branch"]}, {"number": 42361, "title": "Googleapis no longer usable as a system lib with ignored com_github_googleapis_googleapis", "body": "**Describe the problem**\r\n\r\nTF 2.3.0 introduced a regression with https://github.com/tensorflow/tensorflow/commit/b9e1252bcb8cf8247e7393a0df65adf5b0a18c16:\r\n\r\nAlthough a `com_github_googleapis_googleapis` value is still valid for `TF_SYSTEM_LIBS` it is no longer used and the project is always downloaded which might conflict with system installed versions of it.\r\n\r\nNote how the name changed from com_github_googleapis_googleapis to com_google_googleapis and the system_build_file option is gone: https://github.com/tensorflow/tensorflow/commit/b9e1252bcb8cf8247e7393a0df65adf5b0a18c16#diff-455a4c7f8e22d7c514e8c2caa27506c5", "comments": ["I think best way forward here would be to make a PR to fix forward.", "There are 2 solutions: Either re-add the possibility of using the system installed library or remove the entry in `VALID_LIBS`. I don't know why it was removed, if that was an oversight or a conscious decision because something doesn't work otherwise. So I can't tell which solution is the right one to do.\r\n\r\nFrom the commit I can't tell who the committer was, so is there any way to find that out and ask that person?", "It is very likely it was an oversight, as the change looks to be made by someone outside of TF team", "Closing this issue since the above PR is merged and fixes it. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42361\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42361\">No</a>\n"]}, {"number": 42359, "title": "How to connect or use my dataset to put in a CNN LSTM model", "body": "I am a beginner and need guidance in writing the code to connect my dataset to the model i found somewhere in Github.\r\n\r\nMy dataset looks like:\r\n\r\nThis is How it look in my directory which shows the number of samples i have\r\n![1](https://user-images.githubusercontent.com/35618437/90235614-3f07c080-de3f-11ea-9744-809c3c82d5ef.jpg)\r\n\r\nInside each sample there are 2 category AM/PM in which my model have to finally classify\r\n![2](https://user-images.githubusercontent.com/35618437/90235644-48912880-de3f-11ea-95c6-d4d480405153.jpg)\r\n\r\nThis is the time series images, I have 8 images with certail time interval\r\n![3](https://user-images.githubusercontent.com/35618437/90235664-5181fa00-de3f-11ea-8251-3423671c9038.jpg)\r\n\r\n`epochs = 52`\r\n `time = 8`\r\n `n_classes = 2`\r\n `width,height,color_channels = 210,140,3`\r\n `number_of_hiddenunits = 32`\r\n` batch_size = 16`\r\n\r\n` def get_conv_vgg(self,input_batch):`\r\n\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu') )(input_batch)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='SAME',strides=(2,2)))(conv_model)\r\n\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='SAME',strides=(2,2)))(conv_model)\r\n\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='SAME',strides=(2,2)))(conv_model)\r\n\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(512, (3,3), padding='same', activation='relu') )(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.BatchNormalization())(conv_model)\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='SAME',strides=(2,2)))(conv_model)\r\n\r\n    #embedded\r\n    conv_model = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(conv_model)\r\n\r\n    return conv_model`\r\n\r\n`def create_network(self,model_name):`\r\n    \r\n    input_batch = tf.keras.layers.Input(shape = (time,height,width,color_channels))\r\n    if model_name == 'vgg':\r\n        image_features = self.get_conv_vgg(input_batch)\r\n        lstm_network = tf.keras.layers.LSTM(number_of_hiddenunits, return_sequences=True,dropout=0.5,recurrent_dropout=0.5)(image_features)\r\n        lstm_network = tf.keras.layers.LSTM(number_of_hiddenunits, return_sequences=False,dropout=0.5,recurrent_dropout=0.5)(lstm_network)\r\n        lstm_network = tf.keras.layers.Dense(1024,activation='relu')(lstm_network)\r\n        lstm_network = tf.keras.layers.BatchNormalization()(lstm_network)\r\n        lstm_network = tf.keras.layers.Dropout(0.5)(lstm_network)\r\n        lstm_network = tf.keras.layers.Dense(512,activation='relu')(lstm_network)\r\n        lstm_network = tf.keras.layers.Dropout(0.5)(lstm_network)\r\n        lstm_network = tf.keras.layers.Dense(64,activation='relu')(lstm_network)\r\n        lstm_network = tf.keras.layers.Dropout(0.5)(lstm_network)    \r\n        lstm_network = tf.keras.layers.Dense(n_classes,activation='softmax')(lstm_network)\r\n\r\n    elif model_name == 'inception':\r\n        image_features = self.get_conv_inception(input_batch)\r\n        lstm_network = tf.keras.layers.LSTM(number_of_hiddenunits, return_sequences=True,dropout=0.5,recurrent_dropout=0.5)(image_features)\r\n        lstm_network = tf.keras.layers.LSTM(number_of_hiddenunits, return_sequences=False,dropout=0.5,recurrent_dropout=0.5)(lstm_network)\r\n        lstm_network = tf.keras.layers.Dense(512,activation='relu')(lstm_network)\r\n        lstm_network = tf.keras.layers.Dense(64,activation='relu')(lstm_network)\r\n        lstm_network = tf.keras.layers.Dropout(0.5)(lstm_network)    \r\n        lstm_network = tf.keras.layers.Dense(n_classes,activation='softmax')(lstm_network)\r\n\r\n    full_network = tf.keras.Model([input_batch],lstm_network)\r\n    full_network.summary()\r\n    return full_network`\r\n\r\n`def _trainer(network,train_generator,val_generator):`\r\n\r\n    network.compile(optimizer = 'adam', loss= 'binary_crossentropy',metrics = ['accuracy'])\r\n    network.save_weights(checkpoint_path.format(epoch=0))\r\n    history =network.fit_generator(train_generator,epochs=epochs,steps_per_epoch=len(os.listdir(train_folder)) // batch_size,validation_data=val_generator,validation_steps=1,callbacks=[cp_callback,tensorboard_callback])`\r\n\r\n**After seeing above situation my question is how to create train_generator and val_generator if looking my above dataset format\r\n\r\nIt is CNN LSTM architecture in which the 8 images sequence is passed through vgg and LSTM\r\n\r\nMy doubt is how can i pass this 8 images of each category in my model to train it**", "comments": ["@akshat-suwalka \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nYou may refer to these links:\r\n[link](https://www.kaggle.com/tags/lstm) [link1](https://analyticsindiamag.com/how-to-code-your-first-lstm-network-in-keras/)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42358, "title": "tf.data.Dataset.list_files(file_path, shuffle=True) fails (on Windows w/ CUDA)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN 10 LTSC BUILD 17763\r\n- TensorFlow installed from (source or binary): BINARY - https://pypi.org/project/tensorflow/2.3.0/\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1, CUDNN 7.6.5\r\n- GPU model and memory: Quadro M2000M, 4096MiB\r\n\r\nTF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.3.0-rc2-23-gb36436b087 2.3.0 \r\n\r\n\r\n**Describe the current behavior**\r\nSpecifically on this Windows system, with CUDA enabled (not happening w/ `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"`, and not happening on my Linux system at all), calling tf.data.Dataset.list_files(tfrecord_path, shuffle=True) produces the following error message:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/lbortolotti/PerformanceMethods/python_generic_toolbox/bad_list_files.py\", line 22, in <module>\r\n    tf.data.Dataset.list_files(tfrecord_path, shuffle=True)\r\n  File \"C:\\Python\\36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1125, in list_files\r\n    dataset = dataset.shuffle(buffer_size, seed=seed)\r\n  File \"C:\\Python\\36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1240, in shuffle\r\n    return ShuffleDataset(self, buffer_size, seed, reshuffle_each_iteration)\r\n  File \"C:\\Python\\36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3676, in __init__\r\n    **self._flat_structure)\r\n  File \"C:\\Python\\36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 6215, in shuffle_dataset_v3\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"C:\\Python\\36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: buffer_size must be greater than zero. [Op:ShuffleDatasetV3]\r\n\r\n**Describe the expected behavior**\r\nNo errors, shuffled file list returned.\r\n\r\n**Standalone code to reproduce the issue**\r\nGist: https://gist.github.com/optiluca/d84d169315035b8a1dadc97d52b88f24", "comments": ["@optiluca \r\nI ran the code sharedand face a different error, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/cc7035e9a28774c7c32e4d2387ac345f/untitled374.ipynb).", "Hi. The error is presumably related to a quirk of colab where __file__ is not defined. The aim of that line is to simply establish what the working directory is so we can write some (empty) test files needed to reproduce the issue.\r\n\r\nIn any case, as I mentioned in my original post, the issue only occurs on windows with cuda enabled, so we won't be able to see it on colab. ", "Does this happen on Linux system enabled with cuda support?\r\nThe cuda compute capability for Quadro M2000M | 5.0 which is not in the [list of supported architectures](https://www.tensorflow.org/install/gpu#hardware_requirements).", "Hi @ymodak .  I've tried with:\r\n\r\nW10 + M2000M (as per original issue) -> broken\r\nUbuntu+ V100 -> working\r\nW10 + T2000 -> working\r\n\r\nSo it appears to be specific to my card, presumably because 5.0 is no longer supported.\r\n\r\nAs a slight aside, what's the criterion by which CUDA architectures are supported?  Judging by the slightly-arbirary-looking list of supported versions it doesn't seem to be as simple as having a \"recent\" card?\r\n\r\nThanks,\r\n\r\nLuca", "Thanks for sharing your analysis. T2000 has cuda compute capability of 7.5 and V100 has 7.0 both are supported by TF.\r\nFor more information you may try posting the question on stackoverflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42358\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42358\">No</a>\n"]}, {"number": 42357, "title": "tfl.cast cannot convert tensor to uint8 while tfv1 works without problems", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/16zUebMOvR4vvF3GxlxTWXUy8iMzsAQbO?usp=sharing\r\n```python\r\n# Copy and paste here the exact command\r\nimport tensorflow.keras as keras \r\nimport tensorflow as tf\r\nipt=keras.Input(shape=(256,256,3),dtype=\"float32\")\r\ntmp=tf.cast(ipt,\"uint8\")\r\nmodel=keras.Model(inputs=ipt,outputs=tmp)\r\nconverter=tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model=converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\nConverterError: See console for info.\r\n2020-08-14 08:31:08.628838: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-08-14 08:31:08.628892: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-08-14 08:31:08.636532: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-08-14 08:31:08.641744: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200145000 Hz\r\n2020-08-14 08:31:08.642003: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2719480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-14 08:31:08.642043: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-14 08:31:08.644263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-08-14 08:31:08.647089: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-08-14 08:31:08.647135: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e497356b9cd3): /proc/driver/nvidia/version does not exist\r\nloc(callsite(\"model_5/tf_op_layer_Cast_5/Cast_5\"(\"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\":865:0) at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\":959:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\":435:0 at callsite(\"<ipython-input-7-93af565c8972>\":6:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2882:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2822:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2718:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\":537:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\":208:0 at \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\":399:0)))))))))): error: 'tfl.cast' op result #0 must be tensor of 32-bit float or 1-bit integer or 32-bit integer or 64-bit integer or complex type with 32-bit float elements values, but got 'tensor<1x256x256x3x!tf.uint8>'\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:865:9: error: 'tfl.cast' op result #0 must be tensor of 32-bit float or 1-bit integer or 32-bit integer or 64-bit integer or complex type with 32-bit float elements values, but got 'tensor<1x256x256x3x!tf.uint8>'\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:959:5: note: called from\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n    ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:435:5: note: called from\r\n    concrete_func = func.get_concrete_function()\r\n    ^\r\n<ipython-input-7-93af565c8972>: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:17: note: called from\r\n                exec(code_obj, self.user_global_ns, self.user_ns)\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:17: note: called from\r\n                if self.run_code(code, result):\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:20: note: called from\r\n                   interactivity=interactivity, compiler=compiler, result=result)\r\n                   ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:537:9: note: called from\r\n        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:208:13: note: called from\r\n            res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n            ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:41: note: called from\r\n                                        user_expressions, allow_stdin)\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n# See embedded model definition\r\n```\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/58348a479a8f7cbbbb754db66345182d/42357-2-3.ipynb#scrollTo=bOKPIXgDEPsb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/813ac1c8dd80c245c800c0dcfdbee5d9/42357-tf-nightly.ipynb). \r\n\r\nUsing the default converter fails, but `tf.compat.v1.lite.TFLiteConverter` works. Please find the attached gist. Thanks!", "Everything looks working as expected from the gists @amahendrakar  shared.\r\n1) @amahendrakar The failure showing in the gists,  is because you're trying to set inference_output_type to uint8 without any quantization, we disallow setting the flag to int8/uint8 unless you have quantized options set.\r\n\r\n2) The model generated looks correct, float input and cast then output is already uint8.\r\n\r\nClosing, please reopen or file new one if you have any issues.\r\n\r\nThanks\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42357\">No</a>\n"]}, {"number": 42356, "title": "Indexing into EagerTensor returns zeros (on Windows with CUDA)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN 10 LTSC BUILD 17763\r\n- TensorFlow installed from (source or binary): BINARY - https://pypi.org/project/tensorflow/2.3.0/\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1, CUDNN 7.6.5\r\n- GPU model and memory: Quadro M2000M, 4096MiB\r\n\r\nTF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.3.0-rc2-23-gb36436b087 2.3.0 \r\n\r\n**Describe the current behavior**\r\nSpecifically on this Windows system, with CUDA enabled (not happening w/ os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\", and not happening on my Linux system at all), reading a tfrecord with tf.data.* returns an EagerTensor which attempting to index results in incorrect zero output.\r\n\r\n**Describe the expected behavior**\r\nIndexing into the EagerTensor should return the correct output!\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nGist: https://gist.github.com/optiluca/ff34d153c9ff339adedf5f905cdc8240\r\n\r\nOn a working system the output is:\r\ntf.Tensor([100. 105. 110. 115. 120.], shape=(5,), dtype=float32)\r\n100.0\r\n100.0\r\n\r\nOn my windows system:\r\ntf.Tensor([100. 105. 110. 115. 120.], shape=(5,), dtype=float32)\r\n0.0\r\n100.0\r\n\r\n\r\n", "comments": ["Managed to submit this twice, closing this one", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42356\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42356\">No</a>\n"]}, {"number": 42355, "title": "Indexing into EagerTensor returns all zeros (on Windows w/ CUDA)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WIN 10 LTSC BUILD 17763\r\n- TensorFlow installed from (source or binary): BINARY - https://pypi.org/project/tensorflow/2.3.0/\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.1, CUDNN 7.6.5\r\n- GPU model and memory: Quadro M2000M, 4096MiB\r\n\r\nTF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv2.3.0-rc2-23-gb36436b087 2.3.0 \r\n\r\n**Describe the current behavior**\r\nSpecifically on this Windows system, with CUDA enabled (not happening w/ `os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"`, and not happening on my Linux system at all), reading a tfrecord with tf.data.* returns an EagerTensor which attempting to index results in incorrect zero output.\r\n\r\n**Describe the expected behavior**\r\nIndexing into the EagerTensor should return the correct output!\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nGist: https://gist.github.com/optiluca/ff34d153c9ff339adedf5f905cdc8240\r\n\r\nOn a working system the output is:\r\ntf.Tensor([100. 105. 110. 115. 120.], shape=(5,), dtype=float32)\r\n100.0\r\n100.0\r\n\r\nOn my windows system:\r\ntf.Tensor([100. 105. 110. 115. 120.], shape=(5,), dtype=float32)\r\n0.0\r\n100.0\r\n\r\n\r\n", "comments": ["I have tried in colab with TF version 2.3 and i am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/66a5b557f05848ae794eec1c902bac0d/untitled256.ipynb).\r\nI have not tested on windows.Thanks!", "Hi @ravikyram.  As per issue #42506 I've tried with:\r\n\r\nW10 + M2000M (as per original issue) -> broken\r\nUbuntu+ V100 -> working\r\nW10 + T2000 -> working\r\n\r\nSo it appears to be specific to my card, presumably because CUDA architecture 5.0 is no longer supported.  The particular failure mode in this case still leaves something to be desired, I'd expect some sort of error to be thrown at least!\r\n\r\nThanks,\r\n\r\nLuca", "> So it appears to be specific to my card, presumably because CUDA architecture 5.0 is no longer supported.\r\n\r\nCan you please check TF nightly?  If this works on TF nightly (which includes SASS for 5.0) then it should work on TF 2.4 as well.\r\n\r\nI agree that the (lack of) error reporting is surprising, can you please attach the full logs from your run?", "@optiluca \r\nCould you please refer to [this gist](https://colab.research.google.com/gist/Saduf2019/02568b0315cfa0a409f2c69412dc08a8/untitled597.ipynb) and confirm if this issue can be closed.", "Sorry for the silence, yes I've just checked and on TF 2.4 the issue was resolved.  Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42355\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42355\">No</a>\n", "@optiluca \r\nThank you for your update, glad the issue is solved."]}, {"number": 42354, "title": "pip install tensorflow - dlerror: cudart64_101.dll not found", "body": "I check my error by 'python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"' and receive the notice: 'dlerror: cudart64_101.dll not found', help me, please\r\n\r\n2020-08-14 14:25:24.281317: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-08-14 14:25:24.282305: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-08-14 14:25:28.855733: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-08-14 14:25:29.708195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.62GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-08-14 14:25:29.709324: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-08-14 14:25:29.710114: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-08-14 14:25:29.804134: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-08-14 14:25:29.847868: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-08-14 14:25:30.189676: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-08-14 14:25:30.190778: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-08-14 14:25:30.191709: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-08-14 14:25:30.191873: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-08-14 14:25:30.195810: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-08-14 14:25:30.227477: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20de40d74e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-14 14:25:30.227660: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-08-14 14:25:30.229248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-08-14 14:25:30.229390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]\r\ntf.Tensor(-742.7283, shape=(), dtype=float32)\r\n\r\n**System information**\r\n- OS: Windows10 64 bit\r\n- TensorFlow version:  2.3.0\r\n- Python version: 3.8\r\n- Installed using: pip\r\n- CUDA/cuDNN version:  V11.0.167\r\n- GPU model and memory: Name\tNVIDIA GeForce GTX 1050 Ti\r\n", "comments": ["@mmkhoa,\r\nPlease try installing CUDA 10.1 and cuDNN 7.4 and check if you are facing the same issue.\r\n\r\nFor more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu). Thanks!", "It worked! Many thanks, @amahendrakar !"]}, {"number": 42353, "title": "tf.keras.losses has no attribute 'SparseCategoricalCrossentropy'", "body": "`model.compile(optimizer='adam',\r\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n            metrics=['accuracy'])`\r\nUse above code, something went wrong:\r\n![image](https://user-images.githubusercontent.com/42128459/90222246-48dcf400-de3e-11ea-93bc-2a5c8ecd8ecc.png)\r\nLooking forward for your help.\r\nThank you!", "comments": ["My tf version is 1.3.", "@zhjw0927 \r\nIs there any particular reason for using this old version of tensorflow, support is available from tf 1.5 and 2.x, please upgrade to later versions and let us know if you still face the error in 2.x\r\nPlease refer to below links:\r\n[link](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)\r\n[link1](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) #26012 \r\n\r\nIn case the error still persist in 2.x version as well, please provide with stand alone indented code or a colab gist with the error for us to replicate.", "Try to install the latest version of TensorFlow, maybe that might fix your error", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@zhjw0927 Please replace `'metrics=accuracy'` with `metrics='sparse_categorical_accuracy'` as mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-674232499). \r\n\r\nThis is a current workaround and we will update it soon. We will track the progress with [this issue](https://github.com/tensorflow/tensorflow/issues/42045). \r\n\r\nPlease verify once and close this issue and follow the progress in the above issue. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This is resolved in recent `tf-nightly`. Please feel free to reopen if you notice the issue. \r\nThis is available in stable `TF2.4` in the near future. Thanks!", "Please replace 'loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)' with loss=loss='sparse_categorical_crossentropy' as mentioned in https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-674232499.\r\n"]}, {"number": 42352, "title": "How to add TFLite_Detection_PostProcess ops for my own detection model?", "body": "I have built the efficientdet-lite model using tf.keras, and it can predict the boxes, scores, classes, but it need postprocess,such as NMS. Now I have the frozen pb file, what should i do to add TFLite_Detection_PostProcess, what's the input for TFLite_Detection_PostProcess ops", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!"]}, {"number": 42351, "title": "How to add TFLite_Detection_PostProcess ops", "body": "", "comments": []}, {"number": 42350, "title": "Fix tests when TensorFloat-32 is enabled", "body": "If TensorFloat-32 is enabled, many tests fail due to precision issues. In all such cases, the failure is caused by a call to assertAllClose or a similar function. This change disables TF32 or uses float64 matmuls for such tests to fix the issues. For Python tests where TF32 is disabled, I verified which ops need TF32 disabled to pass.\r\n\r\nIt is expected as tests are added or modified, more tests will fail with TF32 after this change. Because the nightly tests do not run with Ampere, the only way to catch such cases is to manually run the tests on Ampere.", "comments": ["@reedwm  Can you please check @sanjoy's comments and keep us posted ? Thanks!"]}, {"number": 42347, "title": "Deploy micro_speech to ESP32 using ESP IDF", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Esp32\r\n- esp-idf version: 4.0.1\r\n\r\n**Describe the problem**\r\nUnable to build the micro_speech in Tensorflow using esp-idf.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nGenerate examples\r\n```\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\n```\r\nBuilding the example\r\n```\r\nidf.py build \r\n```\r\n**Error**\r\n\r\n```\r\nChecking Python dependencies...\r\nPython requirements from /home/george/esp/esp-idf/requirements.txt are satisfied.\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /home/george/Documents/MLANDAI/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[3/265] cd /home/george/Documents/MLANDAI/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp...data/bin/cmake -E echo \"*******************************************************************************\"\r\nPartition table binary generated. Contents:\r\n*******************************************************************************\r\n# Espressif ESP32 Partition Table\r\n# Name, Type, SubType, Offset, Size, Flags\r\nnvs,data,nvs,0x9000,24K,\r\nphy_init,data,phy,0xf000,4K,\r\nfactory,app,factory,0x10000,1M,\r\n*******************************************************************************\r\n[4/265] Performing build step for 'bootloader'\r\nninja: no work to do.\r\n[13/263] Building C object esp-idf/freemodbus/CMakeFiles/__idf_freemodbus.dir/serial_slave/modbus_controller/mbc_serial_slave.c.obj\r\nFAILED: esp-idf/freemodbus/CMakeFiles/__idf_freemodbus.dir/serial_slave/modbus_controller/mbc_serial_slave.c.obj \r\n/home/george/.espressif/tools/xtensa-esp32-elf/esp-2020r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc  -Iconfig -I/home/george/esp/esp-idf/components/freemodbus/common/include -I/home/george/esp/esp-idf/components/freemodbus/common -I/home/george/esp/esp-idf/components/freemodbus/port -I/home/george/esp/esp-idf/components/freemodbus/modbus -I/home/george/esp/esp-idf/components/freemodbus/modbus/ascii -I/home/george/esp/esp-idf/components/freemodbus/modbus/functions -I/home/george/esp/esp-idf/components/freemodbus/modbus/rtu -I/home/george/esp/esp-idf/components/freemodbus/modbus/tcp -I/home/george/esp/esp-idf/components/freemodbus/modbus/include -I/home/george/esp/esp-idf/components/freemodbus/serial_slave/port -I/home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller -I/home/george/esp/esp-idf/components/freemodbus/serial_master/port -I/home/george/esp/esp-idf/components/freemodbus/serial_master/modbus_controller -I/home/george/esp/esp-idf/components/newlib/platform_include -I/home/george/esp/esp-idf/components/freertos/include -I/home/george/esp/esp-idf/components/heap/include -I/home/george/esp/esp-idf/components/log/include -I/home/george/esp/esp-idf/components/soc/esp32/include -I/home/george/esp/esp-idf/components/soc/include -I/home/george/esp/esp-idf/components/esp_rom/include -I/home/george/esp/esp-idf/components/esp_common/include -I/home/george/esp/esp-idf/components/xtensa/include -I/home/george/esp/esp-idf/components/xtensa/esp32/include -I/home/george/esp/esp-idf/components/esp32/include -I/home/george/esp/esp-idf/components/driver/include -I/home/george/esp/esp-idf/components/esp_ringbuf/include -I/home/george/esp/esp-idf/components/esp_event/include -I/home/george/esp/esp-idf/components/tcpip_adapter/include -I/home/george/esp/esp-idf/components/lwip/include/apps -I/home/george/esp/esp-idf/components/lwip/include/apps/sntp -I/home/george/esp/esp-idf/components/lwip/lwip/src/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include -I/home/george/esp/esp-idf/components/lwip/port/esp32/include/arch -I/home/george/esp/esp-idf/components/vfs/include -I/home/george/esp/esp-idf/components/esp_wifi/include -I/home/george/esp/esp-idf/components/esp_wifi/esp32/include -I/home/george/esp/esp-idf/components/esp_eth/include -I/home/george/esp/esp-idf/components/efuse/include -I/home/george/esp/esp-idf/components/efuse/esp32/include -I/home/george/esp/esp-idf/components/app_trace/include -mlongcalls -Wno-frame-address   -ffunction-sections -fdata-sections -fstrict-volatile-bitfields -nostdlib -Wall -Werror=all -Wno-error=unused-function -Wno-error=unused-but-set-variable -Wno-error=unused-variable -Wno-error=deprecated-declarations -Wextra -Wno-unused-parameter -Wno-sign-compare -ggdb -Og -std=gnu99 -Wno-old-style-declaration -D_GNU_SOURCE -DIDF_VER=\\\"v4.0.1\\\" -DGCC_NOT_5_2_0 -DESP_PLATFORM -MD -MT esp-idf/freemodbus/CMakeFiles/__idf_freemodbus.dir/serial_slave/modbus_controller/mbc_serial_slave.c.obj -MF esp-idf/freemodbus/CMakeFiles/__idf_freemodbus.dir/serial_slave/modbus_controller/mbc_serial_slave.c.obj.d -o esp-idf/freemodbus/CMakeFiles/__idf_freemodbus.dir/serial_slave/modbus_controller/mbc_serial_slave.c.obj   -c /home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller/mbc_serial_slave.c\r\n/home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller/mbc_serial_slave.c: In function 'mbc_serial_slave_start':\r\n/home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller/mbc_serial_slave.c:99:28: error: 'MB_SLAVE_ID_SHORT' undeclared (first use in this function); did you mean 'MB_SLAVE_ASSERT'?\r\n     status = eMBSetSlaveID(MB_SLAVE_ID_SHORT, TRUE, (UCHAR*)mb_slave_id, sizeof(mb_slave_id));\r\n                            ^~~~~~~~~~~~~~~~~\r\n                            MB_SLAVE_ASSERT\r\n/home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller/mbc_serial_slave.c:99:28: note: each undeclared identifier is reported only once for each function it appears in\r\n/home/george/esp/esp-idf/components/freemodbus/serial_slave/modbus_controller/mbc_serial_slave.c:99:61: error: 'mb_slave_id' undeclared (first use in this function); did you mean 'mbc_slave_init'?\r\n     status = eMBSetSlaveID(MB_SLAVE_ID_SHORT, TRUE, (UCHAR*)mb_slave_id, sizeof(mb_slave_id));\r\n                                                             ^~~~~~~~~~~\r\n                                                             mbc_slave_init\r\n[22/263] Building C object esp-idf/libsodium/CMakeFiles/__idf_libsodium.dir/libsodium/src/libsodium/crypto_core/curve25519/ref10/curve25519_ref10.c.obj\r\nninja: build stopped: subcommand failed.\r\nninja failed with exit code 1\r\n\r\n\r\n```\r\n", "comments": ["@gigwegbe Can you please share the `sdkconfig` file created in the project directory ( `micro_speech/esp-idf/`).\r\n", "@gigwegbe Could you please let us know if the issue still persists ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42347\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42347\">No</a>\n"]}, {"number": 42345, "title": "using tensorflow images in kubernetes got permission denied when accessing a hostpath(PV)", "body": "First, i'm sorry for my poor English ( sad\r\n\r\nthe following is my pod's yaml\r\n\r\n```\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: aNameOfPods\r\n  labels:\r\n    app: tensorflow-runtime\r\nspec:\r\n  restartPolicy: Never\r\n  containers:\r\n    - name: tensorflow-runtime\r\n      image: 'tensorflow/tensorflow:1.8.0'\r\n      imagePullPolicy: IfNotPresent\r\n      command:\r\n        - /bin/sh\r\n      args:\r\n        - '-c'\r\n        - >-\r\n          cd /catdog;\r\n          ;echo \"your application log start below...\" > $LogPath;\r\n          sh ./do.sh >> $LogPath 2>&1;\r\n          ;echo \"Console diconnected!...\" >> $LogPath;done\r\n      volumeMounts:\r\n        - mountPath: /catdog/\r\n          name: data\r\n        - mountPath: /logs/\r\n          name: log\r\n      resources:\r\n        limits:\r\n          nvidia.com/gpu: 0\r\n  volumes:\r\n    - name: data\r\n      hostPath:\r\n        path:  /<hidden>/\r\n        type: Directory\r\n    - name: log\r\n      hostPath:\r\n        path: /<hidden>/\r\n        type: Directory\r\n```\r\n\r\nWhen \"cd /catdog;\" run, i got an `/bin/sh: 1: ./do.sh: Permission denied`.\r\n\r\nBut, if I mounting data under `/tmp`, and then `cd /tmp` is works. \r\n\r\nIs there any solution, or it is the point that i should always put my data in `/tmp`?\r\n\r\n", "comments": ["@jokie7585 \r\n\r\nIt is not a bug or feature request related to Tensorflow. Please, get in touch with Kubernetes team regarding this.Thanks!"]}, {"number": 42344, "title": "Grad check v1", "body": "@saxenasaurabh  @alextp \r\n\r\nPreliminary working implementation of numerical gradient checking.", "comments": ["This was branched off https://github.com/tensorflow/tensorflow/pull/41432/files , so once this older PR is checked into master I can rebase and get rid of the older commits", "Also just updated the function signature for `CalcNumericalGrad` : instead of just returning the numerical gradient data in a float array, we return an AbstractTensorHandle* with this data instead.", "Looks like some of the tests failed. Could you take a look?", "There seem to be some BUILD failures internally with `-c opt`\r\n\r\n```\r\ntensorflow/c/eager/mnist_gradients_testutil.cc:31:10: error: module //tensorflow/c/eager:mnist_gradients_testutil does not depend on a module exporting 'tensorflow/c/tf_status_helper.h'\r\ntensorflow/c/eager/mnist_gradients_testutil.cc:32:10: error: module //tensorflow/c/eager:mnist_gradients_testutil does not depend on a module exporting 'tensorflow/c/tf_tensor.h'\r\n```"]}, {"number": 42343, "title": "CQT implementation - Feature request", "body": "Hi TF,\r\n\r\nI noticed that TF currently does not offer an implementation for CQT (constant-q transform). Are you planning to add this functionality?\r\n\r\nThank you!\r\n", "comments": ["Can you please fill the feature request template?\r\nThis helps others working on this topic down the line.\r\nThanks!\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@andrekassis,\r\nCan you please respond to the above comment? Also, please let us know the use cases for this Feature Request. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42342, "title": "Help!! Unable to predict CNN model: How to resolve incompatible layers?", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.4rc1\r\n\r\n**Describe the current behavior**\r\nCan't predict model due to unexpected layer (expects 100,352 instead gets 131,072)\r\n\r\n**Standalone code to reproduce the issue**\r\nWill provide a jupyternotebook link soon\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n**ISSUE**\r\nHelp!! I can't predict my model because it's giving me the error:\r\n`ValueError: Input 0 of layer dense_3 is incompatible with the layer: expected axis -1 of input shape to have value 100352 but received input with shape [None, 131072]`\r\n\r\nI just finished training a CNN using a ResNet50 architecture and a few top layers.\r\nThis is the code I used for creating the model...\r\n\r\n```\r\nbase_model = ResNet50(weights = None, include_top=False, input_shape=(200, 200, 3))\r\n\r\nx = base_model.output\r\nx = Flatten()(x)\r\nx = Dropout(0.2)(x)\r\nx = Dense(32, activation='relu')(x)\r\nx = Dense(16, activation='relu')(x)\r\npredictions = Dense(num_class, activation='softmax')(x)\r\n\r\n# The model to be trained\r\nmodel = Model(inputs=base_model.input, outputs=predictions)\r\n\r\nmodel.compile(loss='binary_crossentropy', \r\n              optimizer='rmsprop', \r\n              metrics=['accuracy'])\r\n\r\ncallbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', verbose=1)]\r\nmodel.summary()\r\n```\r\n\r\nAs you can see, I only used a few layers on top of the ResNet50 architecture. I also used an image size input of 200, 200, 3\r\nWent it calls back, this is the summary for the FLATTEN to the 2ND LAST DENSE LAYER.\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nflatten_1 (Flatten)             (None, 100352)       0           conv5_block3_out[0][0]           \r\n__________________________________________________________________________________________________\r\ndropout_1 (Dropout)             (None, 100352)       0           flatten_1[0][0]                  \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 32)           3211296     dropout_1[0][0]                  \r\n__________________________________________________________________________________________________\r\n```\r\n\r\nThe dense layer expects an input of shape 100,352 but instead it receives an input of 131,072!! Hence, the value error when running the predict code via...\r\n\r\n```\r\nimg_path = 'train/10_right.jpeg'\r\nimg = image.load_img(img_path, target_size =(256,256))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\n\r\npreds = model.predict(x)\r\n```", "comments": ["@JacobOfCorns \r\nPlease share complete stand alone code for us to replicate the issue or if possible share a colab gist with the error reported.", "Note that your input image has size 256 x 256. Assuming it has two channels, that gives you a vector of size 131072. In your network definition, you wrote input shape=(200, 200, 3). Please check that.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42341, "title": "Cache captured_inputs", "body": "", "comments": ["Benchmarks over 3 batches of 10 trials each\r\n\r\nMinimum\r\ndefun_matmul_2_by_2_CPU: 156 -> 155 us\r\ndefun_matmul_2_by_2_CPU_async: 84 -> 82.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 206.5 -> 201 us\r\n\r\nMean\r\ndefun_matmul_2_by_2_CPU: 177 -> 173 us\r\ndefun_matmul_2_by_2_CPU_async: 86.5 -> 85 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 223 -> 233 us\r\n\r\nMedian\r\ndefun_matmul_2_by_2_CPU: 176 -> 173 us\r\ndefun_matmul_2_by_2_CPU_async: 85.5 -> 83.5 us\r\ndefun_matmul_2_by_2_with_signature_CPU: 224 -> 224 us", "Please take a look at these Ubuntu CPU CI fails and see if you can reproduce https://source.cloud.google.com/results/invocations/f185b429-a41f-471c-9f59-9deb5a016144/targets/%2F%2Ftensorflow%2Fpython%2Feager:function_test/tests;group=__main__.MultiDeviceTest;test=testDeferredCaptureTypeError;row=1", "Closing this PR because updates to the return values of the function calls `x()` cannot be appropriately cached. `testDeferredCapture` is an example"]}, {"number": 42340, "title": "TF 2.3 fails to build with CUDA 11.0 using only 7.5 compute", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro x64 2004 Edition\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: No, a direct accessible installation\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): MSVC 2019 C++ compiler\r\n- CUDA/cuDNN version: 11.0/ cuDNN ver 8.0\r\n- GPU model and memory: RTX 2080 GPU memory : 8 GB , system memory 32 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\nRepository command failed\r\n'C:/Program' is not recognized as an internal or external command, operable program or batch file.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nStep 0 : git cloned the master, cd tensorflow, git checkout r2.3\r\nStep 1 : python ./configure.py\r\n   -> specified : (1) Python and default library path to the installed python 3.8.5 64 bit with all requisite keras packages.\r\n  -> yes to CUDA : Found CUDA 11.0 and Found cuDNN 8.0, specified a compute capability of 7.5 (only one)\r\n  -> optimization flags : /arch:AVX2 and /std:c++17\r\n  -> eigen inline : N (longer compile time acceptable).\r\n -> no to ROCm, and no to android.\r\nStep 2 :  bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nProceeded a few steps for one minute and then failed.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n PS C:\\Users\\gautam\\tensorflow> bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\gautam\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Program Files/Python3/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\gautam\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\gautam\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files/Python3/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python3/lib/site-packages --python_path=C:/Program Files/Python3/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\gautam\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\gautam\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\gautam\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\gautam\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file c:\\users\\gautam\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --copt=/std:c++17 --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\gautam\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\gautam\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\gautam\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\gautam\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/gautam/_bazel_gautam/4wn3vkrp/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl:1399:33: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n'C:/Program' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n'C:/Program' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1051, in _create_local_cuda_repository\r\n                _find_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 598, in _find_libs\r\n                _check_cuda_libs(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/gpus/cuda_configure.bzl\", line 500, in _check_cuda_libs\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/gautam/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n'C:/Program' is not recognized as an internal or external command,\r\noperable program or batch file.\r\nINFO: Elapsed time: 55.288s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n19:14:49 PS C:\\Users\\gautam\\tensorflow>", "comments": ["@quasar66 \r\n\r\nCan you try with tested build configurations from [here](https://www.tensorflow.org/install/source_windows#gpu) and try building TensorFlow with CUDA 10.1, and check if it works.\r\nPlease, refer #[42084 (comment) ](https://github.com/tensorflow/tensorflow/issues/42084#issuecomment-670208947) for cuda 11.0 support. Thanks!", "Hi...\r\nUpdate - the build process fails for CUDA 10.2, and 10.1 with identical errors at same stage. The input parameters remain the same : namely, use GPU, use AVX2, use c++17.\r\nAn observation : The error seems to be a string parsing error as the software seems to be searching for 'C:\\Program' which is probably incorrect use of ' '(space character) for parsing a execution string (short for C:\\Program Files\\...)\r\n\r\nThe source is release 2.3.", "Try changing your python path since the python path created similar problem for me ", " --python_path=C:/Program Files/Python3/python.exe -> this to C:/Python3/python.exe somethinglike this\r\n", "Hi...\r\nThe build process kickstarted properly with the change in Python installation directory .. However, fails to handle the cudnn with the error as shown below. cudnn was installed properly.\r\n\r\nERROR: C:/users/gautam/tensorflow/tensorflow/stream_executor/cuda/BUILD:319:11: undeclared inclusion(s) in rule '//tensorflow/stream_executor/cuda:cudnn_stub':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cudnn_stub.cc':\r\n  'tensorflow/stream_executor/cuda/cudnn_version.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2652.569s, Critical Path: 377.54s\r\nINFO: 5670 processes: 5670 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nThe opt : AVX2, dropped the other opt (c++17 as most of the compilation overwrote the flag to c++14 - the default C++).\r\nThe compilation is with **CUDA 10.1**. Also failed to build with CUDA 10.0 (specified as supported build). The error with 10.0 is as reproduced below.\r\n\r\nERROR: C:/users/gautam/tensorflow/tensorflow/core/util/BUILD:351:24: output 'tensorflow/core/util/version_info.cc' was not created\r\nERROR: C:/users/gautam/tensorflow/tensorflow/core/util/BUILD:351:24: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 779.140s, Critical Path: 13.99s\r\nINFO: 174 processes: 174 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nBut thanks @wolwire for the idea regarding the python installation path !! (Wondering why I didn't think of it myself...)\r\n@ravikyram - I need to recompile from source as I need to generate the C++ library for running loadsavedmodel() in a C++ program. If this library is available ready, will be happy to use pre-compiled library.\r\n\r\nThanks", "Hi..\r\n\r\nFurther attempt to compile completely without CUDA support, and taking ONLY the default options also failed. This time the error is not very clear, but the only error line seems to be (not sure why we need b_float compatibility)\r\n\r\nERROR: C:/users/gautam/tensorflow/tensorflow/python/BUILD:501:11: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 2): cl.exe failed: error executing command\r\n<many more lines of output>\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6344.527s, Critical Path: 390.88s\r\nINFO: 8875 processes: 8875 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nEnvironment : bazel 3.4.1 - all Windows C++ specifications installed, MSYS2 installed, MSVC 14.27.29110, Windows SDK : 10.0.19041.1\r\nrunning on windows 10 Pro 64 bit updated to 2004 Edition.\r\n\r\nThanks..\r\n", "@meteorcloudy could you take a look?", "Hi @quasar66 , can you rebuild  with `--verbose_failures` and provide the error message here? Thanks!", "Hi @meteorcloudy \r\n\r\nI am currently attempting to build with Bazel 3.1.0, AVX2, c++17, the master branch (not r2.3), CUDA 11.0, cuDNN 8 , build for tensorflow.dll (which is what I actually need) - and its been running for last 16 hours (my PC is little old) but has not yet shown any errors. 14107 of 14375 .. 100% CPU for last 16 hours :-) and for some time it indeed had used up all 32 GB of RAM.\r\n\r\nIf this succeeds, I will most certainly let you know. If it fails, I shall run with the flag you mentioned and will share the result.\r\n\r\nThanks...\r\n\r\n", "Success !!! after running for 180,786.54 seconds (~50 hours), \"Build was successful... 14,375 actions.\r\n\r\nThanks for all your attention and support.\r\nWill test the generated DLL file for linking etc.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42340\">No</a>\n"]}, {"number": 42339, "title": "[INTEL MKL] MKL DNN v0.x cleanup - Reset MKL build config and remove DNN v0.x related macros", "body": "This is the first PR for MKL DNN v0.x clean - for Tensorflow 2.4, only MKL DNN v1.x will be supported\r\n   (1) Reset MKL DNN related build config (mostly in third_part/mkl or mkl_dnn folder)\r\n   (2) Remove MKL DNN v0.x related macros in core/util/mkl_types.c\r\n   (3) Minor code style fix in one MKL kernel op. \r\n\r\nThere will be a sequence of PR's which will clean up all related MKL kernels ops.", "comments": ["Thank you!  \r\nI will create a series of PR's to clean up all MKL kernel ops. For each op, remove 5-10% DNN 0.x code and replace MACRO's with actual DNN 1.x API's.  ", "Sounds good to me. Thank you for the heads up! :)", "@gzmkl  Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned. Hi, we just found that we (Intel) do not support \"ngraph\" bridge (third_party/ngraph) any more. We will soon support a new PR to clean away ngraph related configuration settings in Tensorflow. Once that PR is merged. This one will pass sanity check. \r\n\r\nTHANKS!  -GZ\r\n\r\n", "@gbaned  I pushed a commit to fix ngraph build issue for this PR.  We will continue to submit a new PR to remove ngraph bridge. But this PR will not pend on it.   Thanks!"]}, {"number": 42338, "title": "Fix deprecated usage of collections ABC", "body": "Importing the ABCs from `collections` instead of from `collections.abc` is deprecated since Python 3.3, so TensorFlow provides a compatibility layer to prevent warning from being raised.\r\n\r\nThis PR removes the deprecated usage to make TensorFlow a bit less annoying to use with modern versions of Python.\r\n\r\nSame changes as in #41293", "comments": ["@tanzhenyu @gbaned Do you mind taking a look at this?", "@lgeiger  Can you please resolve conflicts? Thanks!", "@gbaned I can't rebase this PR since @qlzh727 commits 69d9fb6c3c4bbbe7f35221ed20dd18aca0958c08 and 9278b9421f64a2103d18a67d825a9b6be243e211 made these changes obsolete.\r\n\r\nThis isn't the first time that internal changes have made my PRs obsolete. If PRs woud get reviewed in a timely manner I think it would save a lot of duplicated work and would make the process of contributing to TensorFlow a lot less frustrating.", "@lgeiger sorry about that , we are trying to fix our PR review process and want to avoid these kind of issues again. Please bear with us while we fix this. ", "@goldiegadde Thanks for looking into it. I hope splitting Keras into a separate will already be a big improvement."]}, {"number": 42337, "title": "[40171] Limit size of kernel cache", "body": "The `EagerContext` class caches instances of kernels that have been used by the current session.  Because kernel state is opaque to `EagerContext`, the caching mechanism is very conservative about reusing kernels.  Also, there is no limit on the number of kernels cached.  As a result of these two factors, the size of the kernel cache grows in an unbounded fashion if the application calls `tf.saved_model.load()` repeatedly.  This growth is the root cause of one of the memory leaks observed in issue #40171 .\r\n\r\nThis PR puts an upper limit on the size of the kernel cache in `EagerContext`.  If the number of kernels exceeds this limit, the cache discards the least recently used kernel. I have hard-coded the capacity to a conservative number of 10000 kernels, which should be enough to prevent thrashing in normal usage while still providing protection against memory leaks.\r\n\r\nI also added a `RemoveKernelFromCache()` method to go with the `AddKernelToCache()` method.\r\n\r\nHere is a graph showing memory usage for a Python script that repeatedly loads and unloads a toy Keras model:\r\n![mem_before_and_after](https://user-images.githubusercontent.com/12436991/90189530-38f7de00-dd72-11ea-99b5-27b6e6e45624.png)\r\nThe blue points show the memory footprint of that script before applying these changes; and the orange points show the memory footprint after applying these changes.\r\n\r\nAfter the changes in this PR, memory usage of my test script increases until the kernel cache reaches its maximum size. After that, the memory usage increases more slowly, because there are additional memory leaks in `saved_model.load()` that are not addressed in this PR.\r\n\r\n", "comments": ["Thanks for this, but I do not believe this is the correct fix here. The kernel cache constantly growing is a sign of a different bug, i.e. kernels that should not be cached are being cached. By limiting the kernel cache we risk hiding actual bugs with poor performance.\r\n\r\nLooking at your thorough investigation, it seems the problem is `VarHandleOp` is being cached due to the unique shared_name. If so, we should simply remove it from being a kernel cache candidate like we do for the tf.data ops. The change can be made here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/eager/execute.cc#L92", "Thanks for the advice. I'll try blacklisting `VarHandleOp` and see if that fixes the problem. It will take a day or two to recompile.", "Opened a new PR #42377 that excludes `VarHandleOp` from kernel caching. I'll close the current PR in favor of the new one."]}, {"number": 42336, "title": "Error when trying decode_raw for FixedLenSequenceFeature in TFRecord dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: Python 3.6.10 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\n```python\r\ndef _bytes_feature(value):\r\n    if isinstance(value, type(tf.constant(0))):\r\n        value = value.numpy()\r\n\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n\r\nsequence_dict = {\r\n    'frames': tf.train.FeatureList(feature=frames),\r\n    \"label\": tf.train.FeatureList(feature=[tf.train.Feature(int64_list=tf.train.Int64List(value=[token])) for token in tokens]),\r\n}\r\ncontext_dict = {\r\n    \"frames_count\": tf.train.Feature(int64_list=tf.train.Int64List(value=[frames_count])),\r\n    \"num_tokens\": num_tokens,\r\n}\r\n\r\nsequence_context = tf.train.Features(feature=context_dict)\r\nsequence_list = tf.train.FeatureLists(feature_list=sequence_dict)\r\nexample = tf.train.SequenceExample(context=sequence_context, feature_lists=sequence_list)\r\n```\r\n- `frames` is a sequence of `112x112` single-channel gray images, represented by a list of `results of _bytes_feature function`.\r\n- `label` is a sequence of tokens.\r\n\r\nMy task is `seq2seq`, so the whole sequence of tokens corresponds to the whole sequence of frames `(len(frames) != len(label))`. The task is lip-reading, if that makes more sense.\r\nI'm loading the dataset this way:\r\n```python\r\nsequence_features = {\r\n    'frames': tf.io.FixedLenSequenceFeature([], dtype=tf.string),\r\n    \"label\": tf.io.FixedLenSequenceFeature([], dtype=tf.int64),\r\n}\r\ncontext_features = {\r\n    \"frames_count\": tf.io.FixedLenFeature([], dtype=tf.int64),\r\n    \"num_tokens\":  tf.io.FixedLenFeature([], dtype=tf.int64),\r\n}\r\ndataset = tf.data.TFRecordDataset(\"train-0.tfrecord\")\r\ndataset = dataset.padded_batch(3)\r\ndataset = dataset.map(_parse_function)\r\n```\r\n```python\r\ndef _parse_function(example_proto):\r\n    context, sequence, _ = tf.io.parse_sequence_example(example_proto, context_features=context_features, sequence_features=sequence_features)\r\n    image = tf.io.decode_raw(sequence[\"frames\"], tf.int8)\r\n    label = sequence[\"label\"]\r\n\r\n    return image, label\r\n```\r\nWhen I'm trying to iterate over the dataset, I receive\r\n```\r\nfor item in dataset:\r\n    print(item)\r\n    break\r\n```\r\n```\r\nInvalidArgumentError: DecodeRaw requires input strings to all be the same size, but element 1 has size 2444 != 2456  [[{{node DecodeRaw}}]]\r\n```\r\n**Describe the expected behavior**\r\nI want to iterate over padded batches of sequences of tf.int8 tensors representing frames for one video along with batches of corresponding labels. Also I want to avoid using VarLenFeature, because sparse tensors [don't play well](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss#notes_2) with CTC loss on GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1trJ8zxQO7rtISTmYoDS24jzzONdIpvsS?usp=sharing\r\n```python\r\nsequence_features = {\r\n    'frames': tf.io.FixedLenSequenceFeature([], dtype=tf.string),\r\n    \"label\": tf.io.FixedLenSequenceFeature([], dtype=tf.int64),\r\n}\r\ncontext_features = {\r\n    \"frames_count\": tf.io.FixedLenFeature([], dtype=tf.int64),\r\n    \"num_tokens\":  tf.io.FixedLenFeature([], dtype=tf.int64),\r\n}\r\n\r\ndef _parse_function(example_proto):\r\n    context, sequence, _ = tf.io.parse_sequence_example(example_proto, context_features=context_features, sequence_features=sequence_features)\r\n    image = tf.io.decode_raw(sequence[\"frames\"], tf.int8)\r\n    label = sequence[\"label\"]\r\n\r\n    return image, label\r\n\r\ndataset = tf.data.TFRecordDataset(\"train-0.tfrecord\")\r\ndataset = dataset.map(_parse_function)\r\ndataset = dataset.padded_batch(3)\r\n\r\nfor item in dataset:\r\n    print(item)\r\n    break\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n<details>\r\n  <summary>Click to expand the error log</summary>\r\n\r\n```\r\nnvalidArgumentErrorTraceback (most recent call last)\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    757             output_types=self._flat_output_types,\r\n--> 758             output_shapes=self._flat_output_shapes)\r\n    759 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2609     except _core._NotOkStatusException as e:\r\n-> 2610       _ops.raise_from_not_ok_status(e, name)\r\n   2611     except _core._FallbackException:\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n\r\n/opt/conda/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: DecodeRaw requires input strings to all be the same size, but element 1 has size 2444 != 2456\r\n\t [[{{node DecodeRaw}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentErrorTraceback (most recent call last)\r\n<ipython-input-196-f8a40c965ca8> in <module>\r\n     29 dataset = dataset.map(_parse_image_function)\r\n     30 \r\n---> 31 for item in dataset:\r\n     32     print(item)\r\n     33     break\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    734 \r\n    735   def __next__(self):  # For Python 3 compatibility\r\n--> 736     return self.next()\r\n    737 \r\n    738   def _next_internal(self):\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)\r\n    770   def next(self):\r\n    771     try:\r\n--> 772       return self._next_internal()\r\n    773     except errors.OutOfRangeError:\r\n    774       raise StopIteration\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    763       except AttributeError:\r\n--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    765 \r\n    766   @property\r\n\r\n/opt/conda/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2103     finally:\r\n   2104       ctx.executor = executor_old\r\n-> 2105       executor_new.wait()\r\n   2106 \r\n   2107 \r\n\r\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: DecodeRaw requires input strings to all be the same size, but element 1 has size 2444 != 2456\r\n\t [[{{node DecodeRaw}}]]\r\n```\r\n</details>", "comments": ["@eawer,\r\nOn running the Colab notebook you have shared, I am facing an error stating `NotFoundError: train-0.tfrecord; No such file or directory`. \r\n\r\nIn order to reproduce the issue reported here, could you please share the dataset you are using? Thanks!", "Also, please take a look at [this StackOverflow comment](https://stackoverflow.com/a/48408161) from a similar issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42335, "title": "When converting tf to tflite it changes the dtypes so operations like add won't work beacuse of different dtypes RuntimeError: tensorflow/lite/kernels/add.cc:93 input1->type != input2->type (INT32 != FLOAT32)Node number 296 (ADD) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary):\r\ntensorflow 1.15 installed with pip. altough I have tried with older versions\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\ntflite_convert --graph_def_file=./saved_model2.pb --output_file=./mask-rcnn-model2.tflite --output_format=TFLITE --input_arrays=input_image,input_image_meta,input_anchors --input_shapes=1,1024,1024,3:1,14:1,261888,4 --output_arrays=mrcnn_class/Softmax,mrcnn_bbox/Reshape --enable_select_tf_ops --allow_custom_ops --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n2020-08-12 16:57:53.877756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-08-12 16:57:53.899894: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\r\n2020-08-12 16:57:53.899966: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: AI\r\n2020-08-12 16:57:53.899989: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: AI\r\n2020-08-12 16:57:53.900089: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.57.0\r\n2020-08-12 16:57:53.900150: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.100.0\r\n2020-08-12 16:57:53.900158: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 440.100.0 does not match DSO version 450.57.0 -- cannot find working devices in this configuration\r\n2020-08-12 16:57:53.900446: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-08-12 16:57:53.905236: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2799925000 Hz\r\n2020-08-12 16:57:53.905657: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58a7850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-08-12 16:57:53.905678: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n/home/tensorbook/.local/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py:854: UserWarning: Property target_ops is deprecated, please use target_spec.supported_ops instead.\r\n  \"target_spec.supported_ops instead.\" % name)\r\n2020-08-12 16:57:55.568703: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-08-12 16:57:55.568867: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-08-12 16:58:22.490992: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-08-12 16:58:22.491020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1967 nodes (-1026), 2180 edges (-1093), time = 26054.7871ms.\r\n2020-08-12 16:58:22.491025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1967 nodes (0), 2180 edges (0), time = 410.548ms.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\nhttps://drive.google.com/file/d/17JF-Gco7xSbszPXAL-qjFbdDdlW73QJM/view?usp=sharing        \"saved frozed model\"\r\nhttps://drive.google.com/file/d/1t2sNK8wBatOUum3XHprO0EejWaGoMKGB/view?usp=sharing     \"tflite converted model\"\r\n```\r\n\r\n**Failure details**\r\nI'm working with instance segmentation keras/tensorflow model \"https://github.com/matterport/Mask_RCNN\", I'm not using the standard model but retrained or transfer learned to a different dataset... \r\n\r\nthe model was converted successfully, but when I try to infer with this code:\r\n\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"../../logs/mask-rcnn-model2.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nimage = cv2.imread('./655.png')\r\nimage = image[..., ::-1]\r\nimage = image.astype(np.float32)\r\nresized = cv2.resize(image, (1024,1024))\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], [resized])\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nbut if fails at interpreter.invoke() as I said in the title:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"infer_tflite.py\", line 529, in <module>\r\n    interpreter.invoke()\r\n  File \"/home/tensorbook/Documents/Mask_RCNN_Kikes/tf_night_env/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 525, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: tensorflow/lite/kernels/add.cc:93 input1->type != input2->type (INT32 != FLOAT32)Node number 296 (ADD) failed to prepare.\r\n```\r\n\r\nI've tried using tf.cast() or even keras K.cast() from tf.float32 to tf.int32 cause later I will bump with a tf.gather_ND which only accepts int32 as index... I'm able to advance and solve the error if I cast to tf.float32 but when I try to cast it back to int32 it does it in tf but in tflite it doesn't cast it. the model architecture is something like this...\r\n\r\n```\r\nimage_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)\r\nroi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\r\nroi_level_2 = K.cast(tf.round(roi_level), tf.int32)\r\nsum_to_roi_level = K.cast(4, tf.int32)\r\nroi_level = tf.minimum(5, tf.maximum(\r\n    2, sum_to_roi_level + roi_level_2))\r\nroi_level = tf.squeeze(roi_level, 2)\r\n\r\n# Loop through levels and apply ROI pooling to each. P2 to P5.\r\npooled = []\r\nbox_to_level = []\r\nfor i, level in enumerate(range(2, 6)):\r\n    ix = tf.where(tf.equal(roi_level, level))\r\n    level_boxes = tf.gather_nd(boxes, ix)\r\n```\r\n\r\nAny toughts on this ? I have tried many cast to many elements in the model but the casts to int32 doesn't seem to make effect on tflite. I would greatly appreciate your help. thanks a lot in advance.\r\n", "comments": ["@danielsernabuitrago \r\nThe code seems incomplete to replicate, please share complete stand alone code such that we can replicate the error faced or if possible share a colab gist with the error faced.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]