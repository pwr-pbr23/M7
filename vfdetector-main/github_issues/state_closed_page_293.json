[{"number": 45503, "title": "Prevent unitialized memory access in `GraphConstructor::MakeEdge`", "body": "The `MakeEdge` implementation assumes that there exists an output at `output_index` of `src` node and an input at `input_index` of `dst` node. However, if this is not the case this results in accessing data out of bounds. Because we are accessing an array that is a private member of a class and only in read only mode, this usually results only in unitialized memory access. However, it is reasonable to think that malicious users could manipulate these indexes to actually read data outside the class, thus resulting in information leakage and further exploits.\r\n\r\nPiperOrigin-RevId: 346343288\r\nChange-Id: I2127da27c2023d27f26efd39afa6c853385cab6f", "comments": []}, {"number": 45502, "title": "Prevent CHECK-fail in LSTM/GRU with zero-length input.", "body": "PiperOrigin-RevId: 346239181\r\nChange-Id: I5f233dbc076aab7bb4e31ba24f5abd4eaf99ea4f", "comments": []}, {"number": 45501, "title": "Completely rewrite `GetMatchingPaths`.", "body": "The current parallel implementation is too complex (lambda inside lambda, two levels of parallelism) and has a read outside of bounds issue.\r\n\r\nThe new implementation cleans up artifacts from the previous implementations that were left in the code as it evolves. We add multiple helper functions, and document invariants and preconditions as well as every major step. This way, we fix the security issue and a potential new one which was not caught before\r\n\r\nPiperOrigin-RevId: 346146220\r\nChange-Id: Iec0f44673f43349797bf9944dffe9b2f779137d8", "comments": []}, {"number": 45500, "title": "Mark `MemmappedTensorAllocator` as returning opaque handle.", "body": "This allocator is used for `ImmutableConstantOp` and it returns a handle to the contents of a memory mapped file which is supposed to represent a tensor.\r\n\r\nFor tensors of complex types (resources, variables and strings), allocators which are not marked as returning opaque handles will call placement new to initialize each element. This means writing to the buffer. However, in our case, the buffer is immutable and already contains the tensor data. Hence, writing to it is both destructive and causes a crash.\r\n\r\nPiperOrigin-RevId: 345786451\r\nChange-Id: I46369c50fa60b3431709ffe068a728d3061f49c4", "comments": []}, {"number": 45499, "title": "Validate that `DataFormat*` attributes form a permutation.", "body": "The `src_format` and `dst_format` attributes for the `DataFormatDimMap` and `DataFormatVecPermute` raw ops are supposed to determine a permutation. However, this was not validated and could result in unitialized memory accesses as well as writes outside of bounds and potential crashes.\r\n\r\nWhile here, we also test that the format attributes have the needed length, add tests for all validation failure cases, remove unnecessary calls to `strings::StrCat`, and fix a few grammar errors.\r\n\r\nThis will be cherry-picked on the supported release branches.\r\n\r\nPiperOrigin-RevId: 346135579\r\nChange-Id: I1c76392382c89ad8f072d5bc93d70669851eb404", "comments": []}, {"number": 45498, "title": "Default initialize fixed point Eigen types.", "body": "In certain cases, tensors are filled with default values of the type. But, for these fixed point types, these values were uninitialized. Thus, we would have uninitialized memory access bugs, some of which were caught by MSAN.\r\n\r\nPiperOrigin-RevId: 344101137\r\nChange-Id: I14555fda74dca3b5f1582da9008901937e3f14e2", "comments": []}, {"number": 45497, "title": "> It  work for me.Thank you", "body": "> It didn't work for me. Still getting that error.\r\n\r\ntry\r\n`conda update wrapt`\r\n\r\n_Originally posted by @congee524 in https://github.com/tensorflow/tensorflow/issues/30191#issuecomment-514249543_", "comments": ["@CodePythonFollow,\r\nCould you please describe in detail the issue you are facing and provide the following details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nThanks!", "Closing as spam"]}, {"number": 45496, "title": "Changed TOSA common legalizations to use llvm::Optional<>", "body": "This is one of the TODOs from https://github.com/tensorflow/tensorflow/pull/44851\r\n\r\n- Changed TOSA common legalizations to use llvm::Optional<> with Value\r\n  ValueRange, as appopriate\r\n- Removed legalizations return type macros\r\n- Added an affirmative check for the \"NONE\" type in\r\n  convertFusedActivation() to avoid an ambiguous meaning for the\r\n  nullptr return value when the return type was an Operator *\r\n", "comments": ["Thanks for the cleanup!", "> \r\n> \r\n> You change from Operation* to Optional seem to assume a single result everywhere, There aren't any multi-results op?\r\n\r\nThere are only a few multi-result ops that have common legalization functions (search for 'ValueRange').  One example in legalize_common.h:63\r\n\r\nllvm::Optional<ValueRange> convertUnpackOp(PatternRewriter& rewriter,"]}, {"number": 45495, "title": "ellaborated docs on determinism in dataset ops", "body": "Following on from [this discussion](https://github.com/tensorflow/tensorflow/issues/44491), `Dataset.map` docs imply setting `determinism=True` in `Dataset.map` will make the returned dataset deterministic (assuming the original is). This clarifies the behaviour is undefined if `map_func` is not pure (doc addition similar to that used in `interleave`).", "comments": ["@aaudiber clarified. Is it worth adding an example? e.g.\r\n\r\n```python\r\n\r\ntf.random.set_seed(0)\r\ndataset = Dataset.range(\r\n    1, 6, output_type=tf.float32)  #  ==>[ 1., 2., 3., 4., 5. ]\r\ndataset = dataset.map(lambda x: x + tf.random.uniform(()),\r\n    num_parallel_calls=tf.data.AUTOTUNE,\r\n    deterministic=True)\r\nprint(list(dataset.as_numpy_iterator()))\r\n# [1.5166783, 2.1485605, 3.5400312, 4.0197573, 5.468353]\r\nprint(list(dataset.as_numpy_iterator()))\r\n# [1.019757, 2.5400312, 3.4683528, 4.5166783, 5.1485605]\r\n```"]}, {"number": 45494, "title": "TF2 GPUs on Kubernetes", "body": "I made a helm chart for TF2 and it doesn't seem to be able to recognize the GPU, although cupy seems to work fine. \r\nIs there any avail guidance on this or working helm chart example to inspect?", "comments": ["@ymodak would you be able to assist please?", "What is your TF version and cuda version?\r\nSee https://www.tensorflow.org/install/source#gpu for TF and cuda version compatibility.", "The AWS AMI is CUDA 11, the docker image the helm chart pulls from is CUDA 10.1.", "Does TF2 distribute across VMs/ your whole K8s cluster?", "@sanjoy ?", "@joehoeller Can you share the logs from the TF run?  I'm not familiar with helm, but TF 2.4 depends on CUDA 11.0, if you have some other version of CUDA installed then GPU support will be disabled.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45494\">No</a>\n"]}, {"number": 45493, "title": "Delete unnecessary BUILD files as part of flatbuffer patching for TFLM.", "body": "Fixes https://github.com/tensorflow/tensorflow/issues/45492\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45492, "title": "downloaded flatbuffers result in bazel build errors (when attempting to recursively build all the bazel targets)", "body": "@tensorflow/micro\r\n\r\nIf we download flatbuffers via the TFLM makefile and then use the following command:\r\n```\r\nbazel build tensorflow/lite/micro/...\r\n```\r\n\r\nwe get an error due to the BUILD files in tensorflow/lite/micro/downloads/flatbuffers.\r\n\r\nThese build files are not needed and can be deleted.", "comments": []}, {"number": 45491, "title": "Downgrade Docker container Pip to <20.3", "body": "As in pypa/pip#9187, the new dependency resolver\r\nin Pip 20.3 takes tens of hours to complete (and may never do so)\r\nwith TF's dependencies.\r\nThere's not much helpful information out there yet, so I'm\r\ndowngrading Pip in hopes that future versions do not have this problem.", "comments": []}, {"number": 45490, "title": "Add option in the TFLM makefile to disable downloads.", "body": "Tested that the following command\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile list_hello_world_make_files DISABLE_DOWNLOADS=true\r\n```\r\ndoes not attempt to download any libraries.\r\n\r\nWhereas this command does\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile list_hello_world_make_files\r\n```\r\n\r\nAddresses http://b/174445546\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45489, "title": "TFLM: Makefile changes to resolve issue 43898 for CEVA-BX1", "body": "Finally addressing what @advaitjain raised in https://github.com/tensorflow/tensorflow/issues/43898\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@yair-ehrenwald  Can you please resolve conflicts? Thanks!", "I think https://github.com/tensorflow/tensorflow/pull/46841 addresses all of the current changes for the CEVA Makefile. I'm closing this PR."]}, {"number": 45488, "title": "Selective builds to reduce tensorflow-lite binary size missing needed java package org.tensorflow.lite", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nTry to build a custom .aar file from https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh\r\n\r\n\r\n**Describe the problem**\r\nThe result aar.file missing all java-wrapper classes (Interpreter, etc) under package: org.tensorflow.lite. These classes are needed to perform inference from a model in Android.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFollowing the instruction building selective build for a given tflite model using docker: https://www.tensorflow.org/lite/guide/reduce_binary_size#selectively_build_tensorflow_lite_with_docker\r\n\r\n```\r\ncurl -o build_aar_with_docker.sh \\\r\n  https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/tools/build_aar_with_docker.sh &&\r\nchmod +x build_aar_with_docker.sh\r\n```\r\n\r\nand \r\n```\r\nsh tensorflow/lite/tools/build_aar.sh \\\r\n  --input_models=/a/b/my_model.tflite \\\r\n  --target_archs=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --checkpoint=master \\\r\n```\r\n\r\n**Any other info/ logs**\r\nRename resulted aar files to .zip and attached:\r\n\r\n1. aar file built from Mobilenet_1.0_224(float) model: [tensorflow-lite-with-mobile-net-model.zip](https://github.com/tensorflow/tensorflow/files/5661364/tensorflow-lite-with-mobile-net-model.zip)\r\n2. aar file built from my custom model: [tflite-with-my-custom-model.zip](https://github.com/tensorflow/tensorflow/files/5661324/tflite-with-my-custom-model.zip)\r\n\r\nBoth missing java classes\r\n\r\nNote: did try to copy needed java classes into the resulted_aar but this approach seems not working. Might-be the generated .so files missing C++ files which support org.tensorflow.lite java classes as well.", "comments": ["Hi @duudat What it means by \"in order for it to run locally: org.tensorflow.lite.Interpreter, etc.\"?\r\nAre you using it in an Android project?", "@wangtz I found it has been this way since day 1 of this feature.\r\nHowever, It seems to work fine with Android since then.\r\nDo you have any insights on this?", "@thaink Yes, you are correct, I use the generated aar file in my Android project to cutting the size of 'org.tensorflow:tensorflow-lite:x.x.x'.\r\nSpecifically, my assumption by follow the instructions: https://www.tensorflow.org/lite/guide/reduce_binary_size#selectively_build_tensorflow_lite_with_docker is that:\r\nThe resulted `tensorflow-lite.aar` will replace 'org.tensorflow:tensorflow-lite:x.x.x' dependency in my Android project. \r\n\r\nIn Android, to run on-device inference out of a tflite model, it requires`Interpreter`, and other java classes under package: org.apache.tensorflow-lite which is missing in the new generated`tensorflow-lite.aar`, so I got compile errors of `cannot find symbol class Interpreter.`\r\n", "Minor thing I notices is that: the generated aar file seems to also exclude other files inside normal 'org.tensorflow:tensorflow-lite:x.x.x' aar file from jCenter: proguard.txt, headers.\r\n\r\n<img width=\"799\" alt=\"Screen Shot 2020-12-09 at 12 13 59 AM\" src=\"https://user-images.githubusercontent.com/65645909/101602842-8f3cfc80-39b3-11eb-8b31-5c10d0287c06.png\">\r\n\r\nAt least proguard.txt is, for sure, needed in most aar files if the Android app, which uses the aar, enabled size optimization, resource shrinking.\r\n", "@thaink you have suggestion to get around with the errors: `cannot find symbol class Interpreter.` without adding 'org.tensorflow:tensorflow-lite:x.x.x'  dependency to my build.gradle file.", "@duudat Your use is right. Let me check the cause of this.", "@thaink any update on this issue?", "I could locally include needed java classes by modyfing build_aar.sh as following.\r\n\r\n(.sh is changed into .txt since not allowed to upload a .sh file)\r\n[modified_build_aar.txt](https://github.com/tensorflow/tensorflow/files/5698153/modified_build_aar.txt)\r\n\r\nThe aar file generated by the modified scripts didn't encounter the missing java classes problems.", "thanks for the info @duudat. I was able to reproduce this issue.", "The issue is fixed at master branch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45488\">No</a>\n"]}, {"number": 45487, "title": "Try to switch to h5py 3.1.0", "body": "This reverts commit d36db2955c460d2abe185ced9cc5bdfabc67b631.\r\n\r\nReopen https://github.com/tensorflow/tensorflow/pull/45380 that was merged when still under tests.\r\n\r\n/cc @mihaimaruseac \r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/44467\r\n", "comments": ["@mihaimaruseac Please we could work here after you have investigated on the Win/Macos machine. I  have allowed you edits directly in the PR", "I put some blackbox (as I have no access to Win/Macos machine) workarounds for windows and Macos. \r\nNow we have only `OSError: Unable to open file (unable to open file)` on Windows  only for the `core` driver test case:\r\n\r\n```\r\n      # Test non-default options in h5\r\n      with h5py.File('_', driver='core',\r\n                     backing_store=False) as h5file:\r\n```\r\nAnd a MLIR error for Macos", "Very strange on Windows GPU all tests are passing.", "/cc @silvasean any hint about your MLiR test on MacOS?", "> /cc @silvasean any hint about your MLiR test on MacOS?\r\n\r\nSorry, no specific hints. I ran into these kinds of random breakages and very painfully debugged them at a distance by re-running presubmits as I don't have a mac. My sympathies go out to you!\r\n", "@silvasean Thanks but it is unrelated cause it is failing with the same MLIR test also in other PRs"]}, {"number": 45486, "title": "tflite: Add casts to std::min/std::max when comparing mis-matched types", "body": "On some targets, such as the STM32L4, type deduction fails for calls to `std::min` & `std::max` when comparing values with mis-matched types: \r\n\r\n`quantization_util.cpp:293:79: error: no matching function for call to 'min(double, float)'`\r\n\r\n`quantization_util.cpp:293:79: note:   deduced conflicting types for parameter 'const _Tp' ('double' and 'float')`\r\n\r\nThis PR fixes this compilation error by casting `<double>` or `<float>` respectively to these type mis-matched `std::min`/`std::max` calls.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F45486) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@abattery Ready for re-review"]}, {"number": 45485, "title": "[Intel MKL] Enabling QuantizeV2 with native format", "body": "", "comments": []}, {"number": 45484, "title": "[Intel MKL] Enabling quantized pooling ops with native format", "body": "", "comments": ["Thank you for the comments. I addressed them.", "I have addressed the comment. Thank you!", "Will re-tag `ready-to-pull` after #45107 and #45108 are merged."]}, {"number": 45483, "title": "Same code - difference result from PC and jetson nano", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device: Jetson nano\r\n- TensorFlow version : 2.3.1+nv20.11\r\n- Python version: 3.6\r\n- tflite-runtime: 2.5.0\r\n\r\n**Describe the current behavior**\r\n I run same code below. but difference result from my pc (ubuntu 18.04) and my jetson nano (ubuntu 16.04 without tpu) \r\n```\r\ndef __init__(self, model_dir, tpu=True, mask=False):\r\n\r\n        self.tpu = tpu\r\n        self.mask = mask\r\n        if self.tpu:\r\n            self.interpreter = tflite.Interpreter(model_path=model_dir,\r\n                                                  experimental_delegates=[tflite.load_delegate(EDGETPU_SHARED_LIB)])\r\n        else:\r\n            self.interpreter = tf.compat.v1.lite.Interpreter(model_path=model_dir)\r\n        self.interpreter.allocate_tensors()\r\n        self.rec_input_index = self.interpreter.get_input_details()[0]['index']\r\n        if self.mask:\r\n            self.rec_output_index = self.interpreter.get_output_details()[1]['index']\r\n            self.mask_output_index = self.interpreter.get_output_details()[0]['index']\r\n            print('here')\r\n        else:\r\n            self.rec_output_index = self.interpreter.get_output_details()[0]['index']\r\n\r\n    def rec(self, image, landmark=None):\r\n        aligned = image\r\n\r\n        if not self.tpu:\r\n            aligned_norm = np.expand_dims(aligned, axis=0).astype(np.float32)\r\n        else:\r\n            aligned_norm = np.expand_dims(aligned, axis=0)\r\n\r\n        self.interpreter.set_tensor(self.rec_input_index, aligned_norm)\r\n        self.interpreter.invoke()\r\n        feature = get_quant_int8_output(self.interpreter, self.rec_output_index)\r\n```\r\n\r\n**Describe the expected behavior**\r\n i wish i got the same result from these two devices", "comments": ["my code can be rewrite as below\r\n```\r\nimport argparse\r\nimport json\r\nimport multiprocessing\r\nimport platform\r\nimport time\r\nimport numpy as np\r\n\r\nimport cv2\r\nimport tensorflow as tf\r\nimport tflite_runtime.interpreter as tflite\r\n\r\n\r\nREC_MODEL_PATH = \"/home/jetson/Documents/Mobilefacenet-TF2-coral_tpu/pretrained_model/training_model/inference_model_993_quant.tflite\"\r\n\r\nimage_path = '/home/jetson/Documents/Mobilefacenet-TF2-coral_tpu/dataset/134.jpg'\r\n\r\n\r\ninterpreter = tf.compat.v1.lite.Interpreter(REC_MODEL_PATH)\r\ninterpreter.allocate_tensors()\r\nrec_input_index = interpreter.get_input_details()[0]['index']\r\nrec_output_index = interpreter.get_output_details()[0]['index']\r\n\r\n\r\nimage = cv2.imread(image_path)\r\naligned_norm = np.expand_dims(image, axis=0).astype(np.float32)\r\nprint('aligned_norm: ', aligned_norm)\r\ninterpreter.set_tensor(rec_input_index, aligned_norm)\r\ninterpreter.invoke()\r\nfeature = interpreter.get_tensor(rec_output_index)\r\nprint('feature: ', feature)\r\n```\r\n\r\nenv:\r\n```\r\ntensorboard                   2.4.0\r\ntensorboard-plugin-wit        1.7.0\r\ntensorflow                    2.3.1+nv20.11\r\ntensorflow-estimator          2.3.0\r\ntensorrt                      7.1.3.0\r\ntermcolor                     1.1.0\r\ntestresources                 2.0.1\r\ntflite-runtime                2.5.0\r\ntoml    \r\n```", "Could you elaborate? There could be a small difference expected in output by some reasons. (such as execution order difference in kernels)\r\n\r\nIf there is significant difference, you'd better compare the result with your original graph in TF model to show which one has the wrong numbers.", "hello @terryheo \r\ni am sure that aligned_norm is the same value between my pc and my jetson nano. my input, output have the same value also\r\nonly tensorflow lib version has a little bit different\r\n```\r\non my pc:  tensorflow-gpu                    2.3.1\r\non my jetson nano: tensorflow                    2.3.1+nv20.11\r\n``` \r\nResult from my pc: \r\n```\r\naligned_norm:  [[[[154. 164. 164.]\r\n   [159. 169. 169.]\r\n   [168. 176. 176.]\r\n   ...\r\n   [ 39.  35.  34.]\r\n   [ 38.  36.  35.]\r\n   [ 40.  38.  37.]]\r\n  [[155. 165. 165.]\r\n   [151. 161. 161.]\r\n   [149. 157. 157.]\r\n   ...\r\n   [ 38.  34.  33.]\r\n   [ 37.  35.  34.]\r\n   [ 37.  35.  34.]]\r\n  [[153. 160. 163.]\r\n   [139. 146. 149.]\r\n   [124. 129. 130.]\r\n   ...\r\n   [ 38.  35.  31.]\r\n   [ 37.  34.  30.]\r\n   [ 37.  34.  30.]]\r\n  ...\r\n  [[158. 163. 164.]\r\n   [159. 164. 165.]\r\n   [159. 163. 164.]\r\n   ...\r\n   [ 47.  51.  62.]\r\n   [ 47.  51.  62.]\r\n   [ 46.  50.  61.]]\r\n  [[158. 163. 166.]\r\n   [159. 164. 165.]\r\n   [160. 164. 165.]\r\n   ...\r\n   [ 49.  51.  62.]\r\n   [ 50.  52.  63.]\r\n   [ 49.  51.  62.]]\r\n  [[158. 163. 166.]\r\n   [158. 163. 166.]\r\n   [159. 163. 164.]\r\n   ...\r\n   [ 49.  51.  62.]\r\n   [ 50.  52.  63.]\r\n   [ 50.  52.  63.]]]]\r\nfeature:  [[-4.1049747   1.368325   -0.68416244  0.45610833  0.11402708 -0.9122167\r\n  -0.2280541   0.         -0.34208125  0.5701354  -1.1402707   0.6841625\r\n   1.1402708  -0.34208125 -0.9122167  -1.8244333   0.5701354   0.45610833\r\n   1.254298    1.368325   -1.596379   -0.11402708  2.0524876  -0.34208125\r\n  -1.482352    1.8244333  -2.73665    -0.7981896   3.0787313  -0.2280541\r\n   2.3945687  -0.9122167   0.34208125 -0.68416244 -0.2280541  -1.9384604\r\n   0.5701354   2.2805417  -0.7981896  -1.8244333  -1.2542977  -0.11402708\r\n   1.368325   -0.34208125  1.1402708   0.11402708 -1.368325    1.1402708\r\n  -0.68416244 -1.7104063   1.0262437  -0.2280541  -0.2280541   0.5701354\r\n   0.91221666 -1.7104063   0.22805417  0.5701354  -0.9122167   2.1665146\r\n   0.34208125 -0.9122167  -1.2542977  -1.1402707   0.22805417 -0.68416244\r\n  -1.0262437   2.1665146   0.11402708  0.         -2.73665     0.11402708\r\n  -0.11402708  1.368325    0.45610833  0.7981896   0.34208125 -0.7981896\r\n  -0.9122167  -0.5701354   1.368325    1.7104063  -0.9122167   0.45610833\r\n   0.7981896  -1.1402707   0.7981896  -1.596379    0.45610833  1.5963792\r\n   1.254298   -1.482352    1.368325   -0.5701354  -1.9384604  -0.2280541\r\n   1.9384604   3.4208126  -0.7981896  -1.1402707  -0.2280541  -0.34208125\r\n   1.254298    0.11402708 -0.34208125  2.1665146   0.22805417  0.\r\n  -2.622623   -1.2542977  -2.0524874  -0.11402708 -0.2280541   0.91221666\r\n  -1.368325   -0.5701354   1.5963792   0.6841625   2.1665146  -1.596379\r\n  -1.1402707   1.5963792  -0.7981896  -0.4561084  -1.482352    2.73665\r\n   1.0262437  -0.4561084 ]]\r\n```\r\n\r\nresult from my jetson nano\r\n```\r\naligned_norm:  [[[[154. 164. 164.]\r\n   [159. 169. 169.]\r\n   [168. 176. 176.]\r\n   ...\r\n   [ 39.  35.  34.]\r\n   [ 38.  36.  35.]\r\n   [ 40.  38.  37.]]\r\n\r\n  [[155. 165. 165.]\r\n   [151. 161. 161.]\r\n   [149. 157. 157.]\r\n   ...\r\n   [ 38.  34.  33.]\r\n   [ 37.  35.  34.]\r\n   [ 37.  35.  34.]]\r\n\r\n  [[153. 160. 163.]\r\n   [139. 146. 149.]\r\n   [124. 129. 130.]\r\n   ...\r\n   [ 38.  35.  31.]\r\n   [ 37.  34.  30.]\r\n   [ 37.  34.  30.]]\r\n\r\n  ...\r\n\r\n  [[158. 163. 164.]\r\n   [159. 164. 165.]\r\n   [159. 163. 164.]\r\n   ...\r\n   [ 47.  51.  62.]\r\n   [ 47.  51.  62.]\r\n   [ 46.  50.  61.]]\r\n\r\n  [[158. 163. 166.]\r\n   [159. 164. 165.]\r\n   [160. 164. 165.]\r\n   ...\r\n   [ 49.  51.  62.]\r\n   [ 50.  52.  63.]\r\n   [ 49.  51.  62.]]\r\n\r\n  [[158. 163. 166.]\r\n   [158. 163. 166.]\r\n   [159. 163. 164.]\r\n   ...\r\n   [ 49.  51.  62.]\r\n   [ 50.  52.  63.]\r\n   [ 50.  52.  63.]]]]\r\nfeature:  [[-3.8769207   1.482352   -0.45610833  0.34208125 -0.34208125 -0.5701354\r\n  -0.22805417 -0.11402708 -0.45610833  0.45610833 -0.91221666  0.7981896\r\n   1.482352    0.         -0.91221666 -1.8244333   0.6841625   0.34208125\r\n   1.254298    0.91221666 -1.5963792   0.          1.9384604  -0.22805417\r\n  -1.8244333   1.5963792  -2.2805417  -0.45610833  3.3067853  -0.45610833\r\n   2.1665146  -0.6841625   0.6841625  -1.0262437  -0.5701354  -2.1665146\r\n   0.45610833  2.0524874  -0.91221666 -1.5963792  -1.0262437   0.11402708\r\n   1.368325   -0.45610833  1.1402708  -0.11402708 -1.254298    0.91221666\r\n  -0.5701354  -2.0524874   0.91221666  0.22805417 -0.22805417  0.5701354\r\n   0.7981896  -1.9384604   0.22805417  0.6841625  -0.6841625   1.5963792\r\n   0.45610833 -0.7981896  -0.91221666 -1.1402708   0.11402708 -0.5701354\r\n  -1.1402708   2.3945687   0.11402708  0.22805417 -2.964704    0.22805417\r\n   0.11402708  1.482352    0.5701354   0.6841625   0.45610833 -0.5701354\r\n  -0.5701354  -0.5701354   1.5963792   1.254298   -1.482352    0.45610833\r\n   0.6841625  -1.1402708   1.482352   -1.5963792   1.0262437   1.5963792\r\n   1.482352   -1.1402708   1.5963792  -0.45610833 -2.3945687  -0.5701354\r\n   1.8244333   3.0787313  -0.7981896  -1.1402708  -0.34208125 -0.22805417\r\n   0.7981896  -0.22805417 -0.22805417  2.508596    0.34208125  0.\r\n  -2.622623   -1.368325   -1.5963792   0.22805417 -0.45610833  0.91221666\r\n  -1.9384604  -0.6841625   1.5963792   0.7981896   1.8244333  -1.9384604\r\n  -1.0262437   1.1402708  -0.7981896  -0.5701354  -1.254298    2.508596\r\n   0.7981896  -0.5701354 ]]\r\n```\r\n\r\nMy model and input image are in this [folder](https://drive.google.com/drive/u/2/folders/1DPTgMzSmJNJcidFxjVD_PRacj1QdfhxC) ", "It seems that you're using quantized TFLite model. I have some questions.\r\n\r\n1. Did you test float32 model?\r\n2. Did you test the quantized model without using EdgeTPU?\r\n3. Could you share the conversion command you used? I wanted to know which quantization you applied.\r\n\r\nFYI, quantization is a technique to get the performance improvement with some loss on accuracy. So accuracy discrepancy could be expected.   ", "hello @terryheo \r\ni did test with both float32 and quantized model by using my CPU(itel i7 9900k) and it worked\r\nMy quantized code is below\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(file_name)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = data_generator\r\n    converter.experimental_new_converter = False\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_quant_model = converter.convert()\r\n```", "It seems that it's an expected accuracy loss.\r\n\r\n@daverim, could you check if there is anything suspicious?\r\n", "@HoangTienDuc It looks like you are using an older Version of Tensorflow (2.3). Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable version of TF (2.5) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45483\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45483\">No</a>\n"]}, {"number": 45482, "title": "Running a part of a calculation in double precision", "body": "Is it possible to use float32 generally and for only  a part of the code use float64 and then switch back to float32?\r\n", "comments": ["@mehradans92,\r\nCould you please explain the use case you are trying to implement and also provide the TensorFlow version and a minimal code snippet, so that we can look into this. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45482\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45482\">No</a>\n"]}, {"number": 45480, "title": "[Intel MKL] Enabling Dequantize op with native format", "body": "", "comments": []}, {"number": 45479, "title": "[Intel MKL] Enabling QunatizedConcatV2 with native format", "body": "", "comments": []}, {"number": 45478, "title": "Fail to run tflite on DSP - BATCH_TO_SPACE_ND failed to prepare", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nAndroid \r\n\r\n- TensorFlow installed from (source or binary):\r\nsource (dde0fe783c8208358971979460611caca75dc363)\r\n\r\n- Tensorflow version (commit SHA if source):\r\ntf-nightly 2.4\r\n\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\ndsp on mobile\r\n\r\n**Describe the problem**\r\nINFO: Created TensorFlow Lite delegate for Hexagon.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Interpreter::UseNNAPI() is deprecated. Use tflite::NnApiDelegate() directly instead.\r\nINFO: Hexagon delegate: 52 nodes delegated out of 88 nodes with 10 partitions.\r\n\r\nERROR: tensorflow/lite/kernels/batch_to_space_nd.cc:81 output_batch_size % block_shape[dim] != 0 (1 != 0)\r\nERROR: Node number 6 (BATCH_TO_SPACE_ND) failed to prepare.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\ntflite model attached\r\n[myModel4DSP.zip](https://github.com/tensorflow/tensorflow/files/5660174/myModel4DSP.zip)\r\n\r\n", "comments": ["Seems like a Micro issue.", "From the error message, it looks like the Lite (not Micro) kernels are being use.\r\n\r\n@shlomi-amitai can you please confirm if you are looking to run on the Android CPU + delegates (in which case this would be a Tensorflow Lite Mobile issue) or on an embedded DSP (in which case this would be a TensorflowLite Micro issue).\r\n", "Hi, I'm trying to run a quantized tflite model on an android mobile DSP.\r\nbelow listed conversion's params (I use IOQ):\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT] # dynamic range quantization\r\n    if quantType == 'ffq' or quantType == 'ioq':\r\n        converter.representative_dataset = representative_data_gen  # + float fallback quantization\r\n        if quantType == 'ioq':\r\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # +integer only quantization\r\n            converter.inference_input_type = tf.int8 # +integer only quantization\r\n            converter.inference_output_type  = tf.int8 ", "Solved.\r\nFor those who might meet this issue.\r\nInput batch size should be changed to the exact number (e.g. 1)  instead of None (as when training).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45478\">No</a>\n"]}, {"number": 45477, "title": "When pre-built libtensorflow C API will be updated to 2.4?", "body": "I rely on pre-build C API libs that I download from https://www.tensorflow.org/install/lang_c, so I wonder when it is going to be updated with recent tensorflow 2.4 version with CUDA 11 support.", "comments": ["When TF 2.4 pip package is released, later this week or the next.", "Thanks, hope prebuild C API will be updated with 2.4 release.", "@VladislavAD,\r\nNow that **`pip package`** is released for [Tensorflow Version 2.4](https://pypi.org/project/tensorflow/2.4.0/), can you please confirm if your issue is resolved? Thanks! ", "Precompiled C API is still 2.3.1 version to download https://www.tensorflow.org/install/lang_c so issue is not resolved yet.", "+1 We just got the unfortunate surprise of not being able to run our application on our new RTX 3090 hardware. ", "Any estimated time when precompiled C API libs will be updated?", "If you copy the URL and change it to 2.4.0 it's there. I deployed my app successfully this past week on ubuntu 20.04 using an RTX 3090.", "Wow thanks, I downloaded them with your advice. Well this isn't obvious that libs exists and are avaliable.", "Yeah. I wish there was a listing of versions available to be honest. I took a guess and it worked out \ud83e\udd23 ", "Apologies. This being the holiday season the website pipeline has been frozen and no update happened. In January we should get the URLs fixed.", "What CUDA version needed for precompiled lib? 11.0, 11.1 or 11.2?", "We are using 11.1 and the most recent cuDNN v8 (maybe 8.0.2?)", "The links are updated now, closing this one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45477\">No</a>\n"]}, {"number": 45476, "title": "tf.nn larger support for RaggedTensor", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI would like to be able to use `tf.nn`, and in turn `tf.keras.layers` such as `Conv2D` on `RaggedTensor`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nI think it shouldn't in any way, just that keras layers and tf.nn operations should accept ragged tensors.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone wanting to use keras layers on ragged tensors. \r\nI am giving a bit more explanation on my specific use case below.\r\n\r\n**Any Other info.**\r\n\r\nSo in my use case, I want to perform distributed training using images that have widely different sizes (and it doesn't make sense to pad them). I would want for each of the elements of the ragged tensor to be distributed on different GPUs.\r\nBut if I use a ragged tensor as is, it is not supported by my models with an error like:\r\n\r\n```\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=tf.RaggedTensor(values=tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:4\", shape=(None, 1), dtype=complex64), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:3\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:2\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:1\", shape=(None,), dtype=int64)), row_splits=Tensor(\"RaggedFromVariant/RaggedTensorFromVariant:0\", shape=(None,), dtype=int64)). Consider casting elements to a supported type.\r\n```\r\n\r\nEven if I use the setting advised [here](https://github.com/tensorflow/tensorflow/issues/27170#issuecomment-587571120), I still get this error.\r\n\r\n", "comments": ["Is this still an issue?  If so, can you provide a code snippet to reproduce it?", "@zaccharieramzi Can you please share a simple standalone code as requested above? Thanks!", "Yes sorry about the delayed response, I am not really suffering from this issue anymore, but here is a simple standalone code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\na = tf.ragged.stack([\r\n  tf.random.normal(shape=[2, 2, 1]),\r\n  tf.random.normal(shape=[3, 3, 1]),\r\n], axis=0)\r\nmodel = keras.models.Sequential(keras.layers.Conv2D(1, 3, padding='same'))\r\nmodel(a)\r\n```\r\n\r\nYou can find a colab [here](https://colab.research.google.com/drive/1SJKUp28Oe94DIoV-zVOLSwKdsycycAy9?usp=sharing).", "@edloper Can you please take a look at this issue. I can reproduce the issue with `TF2.7`. Thanks!", "@zaccharieramzi I don't think `Conv2D` supports ragged tensors. Some other keras layers supports it.\r\n@edloper Can you please comment whether there is any workaround? Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 45475, "title": "Add gradient for QR decomposition of complex matrices.", "body": "As titled. In my previous PR I added gradients for wide real matrices. In this PR I add gradients for QR of complex matrices of all shapes.\r\n\r\nThe gradient for the QR decomposition was requested in issue [#6504](https://github.com/tensorflow/tensorflow/issues/6504) .\r\n\r\nThe methodology is explained in detail in the included article:\r\n[QR and LQ Decomposition Matrix Backpropagation Algorithms for Square, Wide, and Deep Real and Complex Matrices and Their Software Implementation](https://arxiv.org/abs/2009.10071)\r\n\r\nThe complex case details in the documentation are being updated.", "comments": ["I will take a look at the linked paper and then review. Adding @ezhulenev in case he can review faster.", "@saxenasaurabh @ezhulenev I'll add the paragraph on specific complex discussion to the paper within a day.\r\n", "@saxenasaurabh I'd say given how small is the change and that it passes all the tests, I'm fine with submitting it. I'll probably need to take few years to get deep understanding of what's going on here :)", "@saxenasaurabh It is taking me a little longer than I thought to update the paper so it's clear, I'll have it out as soon as possible and tag you here, thanks.", "@saxenasaurabh Here is a copy of the updated [paper](https://www.dropbox.com/s/0oidzfzxq6r8ite/QR.pdf?dl=0). Section 4 derives complex QR backprop. Specifically, Section 4.4 outlines the code change and the necessary correction. The official arXiv is being updated as well.", "Thanks a lot. I will take a look."]}, {"number": 45474, "title": "TFLu: Fix bug in PPD op", "body": "This is a fix for issue: https://github.com/tensorflow/tensorflow/issues/45473\r\n\r\nWith this fix TFL and TFLM gives the same output for SSD Mobilenet v2 (updated with fixed output dimensions).\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45473, "title": "SSD Mobilenet v2 model fail in PPD op", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 0939f0b7a8aed30559b54ae0dd5b9e6b312de1c5\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nSSD Mobilenet v2 crash in detection postprocess operator because of a bug.\r\n \r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45473\">No</a>\n"]}]