[{"number": 38358, "title": "Update the network_tester example", "body": "Update the network tester with some more features:\r\nMultiple input tensors\r\nMultiple output tensors\r\nOutput is printed in json format\r\nCan call invoke() more than once using the NUM_INFERENCES\r\n\r\nUpdated README", "comments": ["Ping for review", "Ping for review!", "@petewarden @wangtz Ping for review", "@jenselofsson Can you please resolve conflicts? Thanks!", "@gbaned Conflicts resolved!"]}, {"number": 38357, "title": "[MixedPrecision] DynamicLossScale should accept scale_loss smaller than one", "body": "https://github.com/tensorflow/tensorflow/blob/r2.2/tensorflow/python/training/experimental/loss_scale.py#L391\r\n\r\n``` python\r\nnew_loss_scale = math_ops.maximum(\r\n      self._current_loss_scale / self._multiplier, 1)\r\n```\r\n\r\n`self._current_loss_scale >=1` is unnecessary and wrong.\r\nIn some use cases, loss is possible overflow `inf` in float16, therefore `self._current_loss_scale<1` is necessary for mixed precision training.\r\n", "comments": ["@chychen, Will it possible to provide the sample code to analyze the reported issue. Thanks!", "@gadagashwini Please see Before/After in following pdf file.\r\n\r\n[TF2#38357.pdf](https://github.com/tensorflow/tensorflow/files/4455245/TF2.38357.pdf)\r\n(I fake it by multiply loss with a large constant.)\r\n", "@chychen, Instead of pdf can you share the executable code or provide the colab gist to reproduce the issue . Thanks", "```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\nprint('Compute dtype: %s' % policy.compute_dtype)\r\nprint('Variable dtype: %s' % policy.variable_dtype)\r\ninputs = keras.Input(shape=(784,), name='digits')\r\nnum_units = 4096\r\ndense1 = layers.Dense(num_units, activation='relu', name='dense_1')\r\nx = dense1(inputs)\r\ndense2 = layers.Dense(1, activation='relu', name='dense_2')\r\noutputs = dense2(x)\r\n\r\nmodel = keras.Model(inputs=inputs, outputs=outputs)\r\n\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\n\r\noptimizer = keras.optimizers.RMSprop()\r\noptimizer = mixed_precision.LossScaleOptimizer(optimizer, loss_scale='dynamic')\r\nloss_object = tf.keras.losses.MeanSquaredError()\r\ntrain_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n                 .shuffle(10000).batch(1024))\r\n\r\n@tf.function\r\ndef train_step(x, y):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(x)\r\n        loss = loss_object(y, predictions) * 10000.\r\n        scaled_loss = optimizer.get_scaled_loss(loss)\r\n    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\r\n    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n    return loss\r\n\r\nfor epoch in range(2):\r\n    for i, (x, y) in enumerate(train_dataset):\r\n        mp_loss_scale = optimizer.loss_scale().numpy()\r\n        loss = train_step(x, y)\r\n        print('epoch {}: step {}: loss={}, loss_scale={}'.format(epoch, i, loss, mp_loss_scale))\r\n```\r\n\r\nCompute dtype: float16\r\nVariable dtype: float32\r\nepoch 0: step 0: loss=inf, loss_scale=32768.0\r\nepoch 0: step 1: loss=inf, loss_scale=16384.0\r\nepoch 0: step 2: loss=inf, loss_scale=8192.0\r\nepoch 0: step 3: loss=inf, loss_scale=4096.0\r\nepoch 0: step 4: loss=inf, loss_scale=2048.0\r\nepoch 0: step 5: loss=inf, loss_scale=1024.0\r\nepoch 0: step 6: loss=inf, loss_scale=512.0\r\nepoch 0: step 7: loss=inf, loss_scale=256.0\r\nepoch 0: step 8: loss=inf, loss_scale=128.0\r\nepoch 0: step 9: loss=inf, loss_scale=64.0\r\nepoch 0: step 10: loss=inf, loss_scale=32.0\r\nepoch 0: step 11: loss=inf, loss_scale=16.0\r\nepoch 0: step 12: loss=inf, loss_scale=8.0\r\nepoch 0: step 13: loss=inf, loss_scale=4.0\r\nepoch 0: step 14: loss=inf, loss_scale=2.0\r\nepoch 0: step 15: loss=inf, loss_scale=1.0\r\nepoch 0: step 16: loss=inf, loss_scale=1.0\r\nepoch 0: step 17: loss=inf, loss_scale=1.0\r\nepoch 0: step 18: loss=inf, loss_scale=1.0\r\n... (inf loss is overflow...)", "In practice, I have never seen losses or intermediate gradients so large that they overflow in float16 when the loss scale is 1. The max float16 value is 65504, which be an enormous gradient. The reason the loss scale cannot go below 1 is that conceptually the purpose of a loss scale is to avoid underflow.\r\n\r\n@nluehr, @benbarsdell, any thoughts on having a loss scale below 1? I don't think this would happen in practice much, and if it did, I imagine the model would not converge to a good accuracy. The current behavior of keeping the loss scale at 1 also would converge poorly, as it just causes every step to be skipped.", "I could replicate the issue with Tf 2.2rc2.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/d0a5487ee6374076d6c7d6d3a6f20098/untitled509.ipynb). Thanks", "In the example above, the INF is generated in the forward pass. Running in FP32, I see an initial loss of 279232.1875, which exceeds the representable range of fp16. Loss scaling gets applied after the loss is computed, so no value of loss scale will help in this case.", "True, in this case it wouldn't help, but what about in general? Do you think it's likely the backwards pass could overflow without the forwards pass overflowing? Even if it's unlikely, should we allow the loss scale to go below 1?", "@nluehr  in many cases, we should cast loss to fp32 before scaling, and cast back to fp16 after scaling. \r\n@reedwm agree with you, I don't understand why not?", "> in many cases, we should cast loss to fp32 before scaling, and cast back to fp16 after scaling.\r\n\r\nJust casting the loss is not sufficient; it it is already NAN in fp16 and will be NAN when cast to fp32. The forward pass needs to cast the unsafe ops that produced the NAN values to fp32. When this is done, the corresponding backward ops will also use fp32, so those should have no numerical issues if the loss scale clamps at one. Other ops did not produce NANs on the forward pass, and are expected to produce smaller values in backward, so a loss scale of one should be fine here as well.\r\n\r\nIn the provided example, the second dense layer is essentially serving as a reduction. When it is performed with `dtype=tf.float32`, the nan losses are avoided and the loss scale initially drops no lower than 4 before rising back up to 128 (if you extend training for many epochs).\r\n\r\n4 is rather close to 1, so it is possible a loss scale below 1 could be required in some corner case. In practice I have not seen such a case. On the other hand, the loss scale cannot be used to work around a NaN in the **loss**, that requires assigning ops to fp32.\r\n\r\nSome lower bound on the loss scale is necessary (letting it be reduced all the way to zero, for example, has obvious problems). Setting the lower bound to the point where loss scaling becomes a numerical no-op (i.e., multiply by one) is appealing, but not strictly necessary. However, we would need an example model requiring a lower loss scale in order to determine where to set a new lower bound.", "Dear @nluehr,\r\n\r\nI don't see any strong reasons you insisted not to adjust it.\r\nI reported here because it trapped my research project. (related to random network distillation on anomaly detection task, and I can't post my project code here surely.)\r\n\r\nif possible the scale smaller than one, why not just relax it? I really can't understand.", "@chychen, I am not insisting against reducing the lower bound below one. However, I do think some lower bound is needed. It is better to error out with overflowing gradients than let the loss scale drop near zero and silently quench convergence (because many/most gradients underflow to zero).\r\n\r\nIt is difficult for me to decide what the lower bound on the loss scaler should be without some typical models that need this change. I would also like to understand the problem in more detail in order to evaluate whether reducing the loss_scale is the best solution. It might be better to cast some additional layers to float32 because an agressively small loss scale will cause underflow in a subset of the model parameters and could result in poor convergence compared to fp32.\r\n\r\nI'm curious, have you adjusted the loss scale interval in your TF installation to unblock your research? With this change did your models train all the way to expected accuracy? What was the smallest required loss scale?\r\n\r\nGiven the uncertainties and risks, I would rather not change the default behavior. Perhaps a custom loss scale interval could be optionally provided when constructing the optimizer. @reedwm, any thoughts? \r\n", "in my experience, yes, adjust the loss scale unblock my research.\r\nin the beginning of training, L2 loss is large so scale smaller than one is required. after few training steps, loss become stabler and ok with larger scale.\r\n\r\nI think custom interval is a good solution. or maybe check whether loss is zero to scale back the factor? just like the solution for nan overflow situation.", "As @nluehr stated, it's difficult to come to a decision here without knowing of any model that requires a loss scale below 1. \r\n\r\nOne potential workaround is to copy the [DynamicLossScale implementation](https://github.com/tensorflow/tensorflow/blob/30559f13a1a68ab6933b0078515a5f54bdb2dc89/tensorflow/python/training/experimental/loss_scale.py#L290), but change the [minimum loss scale](https://github.com/tensorflow/tensorflow/blob/30559f13a1a68ab6933b0078515a5f54bdb2dc89/tensorflow/python/training/experimental/loss_scale.py#L392) from 1 to some lower number. Note the loss scale classes are still experimental so the copied class may break in a future version of TensorFlow.\r\n\r\n> in the beginning of training, L2 loss is large so scale smaller than one is required\r\n\r\nPerhaps the variable initialization should be modified so the L2 loss is not as big. Altneratively, you can start training in float32 for the first few steps, then switch to mixed precision training. This can be accomplished by saving a checkpoint from a float32 model then restoring the checkpoint into an equivalent mixed precision model.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38357\">No</a>\n", "Hi there @reedwm and @nluehr. Im currently having the same problem as @chychen.\r\n\r\n> I would also like to understand the problem in more detail in order to evaluate whether reducing the loss_scale is the best solution. It might be better to cast some additional layers to float32 because an agressively small loss scale will cause underflow in a subset of the model parameters and could result in poor convergence compared to fp32.\r\n> \r\n> I'm curious, have you adjusted the loss scale interval in your TF installation to unblock your research? With this change did your models train all the way to expected accuracy? What was the smallest required loss scale?\r\n\r\nI have monkey-patched the `update` method to allow a loss_scale below 1 (I removed the lower limit completely). This worked, and my models now train to the same validation loss as with float32, which means I can now double the batch_size and get better/faster convergence.\r\nI my case the loss scale drops to around 0.01-0.06 in the very beginning, then doubles every 2000 steps until it reached 64. My loss a Focal Loss used to detect areas of interest in a 1024x512 pixel image with sigmoid output activations.\r\n\r\n> Perhaps a custom loss scale interval could be optionally provided when constructing the optimizer.\r\n\r\nThat would be great. But why not remove the constraint completely? It sound like the argument is \"We want to always prevent underflow, and for that reason we put a limit which will cause overflow.\". I mean, if the worst thing that can happen is underflow, why should that be a reason to enforce an overflow?\r\n\r\n\r\nI would be happy to try other approaches to lower the loss if you have any sensible (best practice) suggestions, but I will not hack my way around it.", "Here is the loss scale in two different training runs\r\n![image](https://user-images.githubusercontent.com/665484/98786320-3bfc7c00-23fe-11eb-9802-f22381168245.png)\r\n", "It is difficult to make a decision in the `LossScaleOptimizer` itself without a specific example of a model that requires a loss scale below 1. @elgehelge, can you provide the model which had the loss scale drop to 0.01-0.06?\r\n\r\nAlso, `LossScale` is now deprecated and the new non-experimental `tf.keras.mixed_precision.LossScaleOptimizer` directly implements loss scaling, so monkey patching `update` will no longer work in TF 2.4 and the TF nightlies. The best workaround in 2.4 and tf-nightly is to override the `get_scaled_loss` and `get_scaled_gradients` to use a lower loss scale than `self.loss_scale`, which effectively treats the loss scale as being lower than it actually is. For example:\r\n\r\n```python\r\nclass LossScaleBelowOneOptimizer(tf.keras.mixed_precision.LossScaleOptimizer):\r\n\r\n  MULTIPLIER = 2 ** 10\r\n\r\n  @property\r\n  def actual_loss_scale(self):\r\n    return self.loss_scale / self.MULTIPLIER\r\n\r\n  def get_scaled_loss(self, loss):\r\n    if callable(loss):\r\n      def new_loss():\r\n        loss_val = loss()\r\n        return loss_val * tf.cast(self.actual_loss_scale, loss_val.dtype)\r\n      return new_loss\r\n    else:\r\n      return loss * tf.cast(self.actual_loss_scale, loss.dtype)\r\n\r\n  def get_unscaled_gradients(self, grads):\r\n    reciprocal = 1. / self.actual_loss_scale\r\n    return [g * reciprocal if g is not None else None for g in grads]\r\n```\r\n\r\nThis allows the effective loss scale to go as low as `1 / self.MULTIPLIER`, which is approximately 0.001.\r\n\r\n> But why not remove the constraint completely? It sound like the argument is \"We want to always prevent underflow, and for that reason we put a limit which will cause overflow.\". I mean, if the worst thing that can happen is underflow, why should that be a reason to enforce an overflow?\r\n\r\nI am worried that allowing the loss scale to go below 1 will cause confusion. A model may get NaNs on the forward pass, in which case the gradients will be NaN regardless of the loss scale. If the loss scale could go below 1, the loss scale would be repeatedly lowered every step until it reached zero.\r\n\r\nI think we should raise an error if the loss scale reaches 1 or if the loss is NaN, but this is tricky to implement without reducing performance. If we find there are a significant number of models which benefit from a loss scale below 1, we can also add a `minimum_loss_scale` argument. @fchollet @nluehr, let me know if you have any thoughts.", "Thanks for the pointer on how to adjust for newer versions of tensorflow. Appreciate it.\r\n\r\n> I think we should raise an error if the loss scale reaches 1 or if the loss is NaN, but this is tricky to implement without reducing performance.\r\n\r\nAgree. I took me quite some digging to find out why my loss did not converge, would have been very helpful with an error message.\r\n\r\n> @elgehelge, can you provide the model which had the loss scale drop to 0.01-0.06?\r\n\r\nIt is for commercial use so I cannot go into a lot of details unfortunately. But it an hourglass network with a lot of 3x3 convolutions."]}, {"number": 38355, "title": "Update SlurmClusterResolver documentation", "body": "Extracted from https://github.com/tensorflow/tensorflow/pull/38112 to only contain the docu changes and slightly enhanced.\r\n\r\nThe existing docu for the Slurm cluster resolver became outdated after it was enhanced in #36159.\r\n\r\nccing @frankchn ", "comments": ["Thank you!"]}, {"number": 38354, "title": "[TFLite 16x8] Tanh/Sigmoid implementation is extended to the general parameter scale", "body": "The current implementation of Tanh/Sigmoid activation functions for int16 (input/output) assume that\r\nthe parameter scale is POT (power of two).\r\n\r\nThis PR provides changes to extend it to the case when the parameter scale is not POT.\r\nThe reference implementation is used in this case.\r\n\r\nTests + versioning are included.\r\nIt has been tested on big networks as well.", "comments": ["Hi @jianlijianli, Could please review this PR ?\r\nThese changes enable Tanh/Sigmoid int16 for the general case, when the scale parameter is not POT.\r\nThanks!", "@talumbau can you help w/ review?", "Hi @jdduke, corrected, please take a look."]}, {"number": 38353, "title": "Support input-based loss-functions in tf.keras.Model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.2.0rc2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn `tf.keras.Model`, there should be an API for supplying loss functions and metrics that are based on the input `x_input` and not just the output `y_pred`. \r\n\r\nCurrently when supplying custom loss functions in `tf.keras.Model` or metrics they need to be on the form `loss(y_true, y_pred)`. The most import example is in `Model.compile()` which accepts a functional argument `loss` of this type but not one having access to `x_input`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, but there are many possible implementations in which compatibility can be preserved. For example `Model.compile()` could accept an argument `loss_x` (supplying both this and `loss` in the same call throws an error). This should then accept a function as follows.\r\n\r\n`def custom_loss_x(y_true, y_pred, x_input): ... `\r\n\r\nor preferably\r\n\r\n`def custom_loss_x(y_true, x_input): ... `\r\n\r\nas y_pred can be retained by `y_pred = model(x_input)`.\r\n\r\nIn an analogous way custom metrics for models can be constructed and supplied.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAny model with a loss function or metrics that uses information available in the input layer but not in the output layer.\r\n\r\nAn example is when the input consists of a segmented image and the output is an image of the same shape. If different segments are to be weighted differently in the loss function this loss function will depend on `x_input`. \r\n\r\n**Any Other info.**\r\n", "comments": ["@viktor765 Can you please try `model.add_loss(y_true, y_pred, weights)` as in the following example. \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.losses import Loss\r\nimport numpy as np\r\n \r\ninp = Input(shape=(1,), dtype=tf.float32)\r\ntargets = Input(shape=(1,), dtype=tf.float32)\r\nw = Input(shape=(1,), dtype=tf.float32)\r\nout = Dense(1)(inp)\r\nm = Model(inputs=[inp, w, targets], outputs=out)\r\n \r\n \r\n \r\ndef weighted_loss(y_true, y_pred, weights):\r\n      loss = tf.math.squared_difference(y_pred, y_true)\r\n      w_loss = tf.multiply(weights, loss)\r\n      return tf.reduce_mean(tf.reduce_sum(w_loss, axis=-1))\r\n  \r\nm.add_loss(weighted_loss(targets, out, inp))\r\n \r\nm.compile(loss=None, optimizer='adam')\r\nx = np.ones((512,1))\r\nm.fit([x, x, x], epochs=10)\r\n```\r\nThere are other ways to define custom loss functions as shown in this [resource](https://stackoverflow.com/questions/45961428/make-a-custom-loss-function-in-keras?rq=1). Thanks! ", "@jvishnuvardhan That achieves precisely what I want and more, thank you. I was unaware of Model.add_loss but I think its versatility makes it the best way of defining loss functions.  \r\n\r\nPerhaps it should be more present in the Tensorflow documentation? I believe that jlh's answer here explains its case quite well: https://stackoverflow.com/questions/50063613/what-is-the-purpose-of-the-add-loss-function-in-keras"]}, {"number": 38351, "title": "Add public API to get CUDA version and path, CUDNN version and path.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.2.0rc2\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFollow-up of the SIG build meeting of the 07/04/2020.\r\n\r\nCurrently TensorFlow is compiled against a specific CUDA and CUDNN version. When people want to build TensorFlow Addons from source, they have to specify manually the CUDA and CUDNN paths and version to use: https://github.com/tensorflow/addons/blob/master/configure.py#L159 \r\n\r\nBut it should not be needed to ask this information to the user. Because the user have to use the exact CUDA and CUDNN versions that TF was compiled against. Also TF is able to detect where are the CUDA and CUDNN directories on the user filesystem. If we could get this information out of TensorFlow, that would make the building of custom ops easier. We could also do some version checking at runtime and improve the user experience when there is a version mismatch.\r\n\r\n**Will this change the current api? How?**\r\n\r\nWe would need 4 new functions/attributes (the names are examples or course):\r\n\r\n* `tf.config.get_cuda_version_used_to_compile_tf()`\r\n* `tf.config.get_cudnn_version_used_to_compile_tf()`\r\n* `tf.get_cuda_path()`\r\n* `tf.get_cudnn_path()`\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople writing custom ops, as it makes the configuration easier for compilation, it also allows checking that people using the TF addons binaries have a compatible TF installed as the cuda and cudnn versions have to match.\r\n\r\n**Any Other info.**\r\n\r\nI was asked during the sig-build meeting to tag:\r\n@perfinion @gunan @angerson @yifeif\r\n\r\n@seanpmorgan as he manages TF Addons\r\n", "comments": ["@alextp for the proposed new APIs.", "SGTM but I'd like the cuda_path and cudnn_path ones to also be in tf.config.\r\n\r\nMaybe we should just have a tf.config.cuda_information() which returns a namedtuple of the information we want (versions, paths, and whatever else we need in the future)?", "I don't have any preference regarding how it's named and under which namespace it's available. The ones given in the issue description are just examples.", "I like the idea of having these under a separate namespace.\r\n\r\nThe cuda and cudnn paths are read from `LD_LIBRARY_PATH`.\r\nSo it is not known to us at build time. And we rely on OS shared object resolution, we just do `dlopen(libcuda...` Do you want us to replicate the OS so resolution in TF?", "> So it is not known to us at build time.\r\n\r\nYes of course, since the path depends on the user system. I'm asking the path found at runtime. Since TF already knows at runtime where is the CUDA and CUDNN path, we'd like to grab that. If it's trivial and already available in an environement variable, we (Addons) ask the user too much already and we can deduce the path automatically. I just wasn't aware of the mechanism.\r\n\r\nThe request for the CUDA and CUDNN versions used at compile time is still valid though.", "We actually do not know the paths. We simply load `libcuda.so` or `libcufft.so`.\r\nThe logic is here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/platform/default/dso_loader.cc#L42\r\nThe operating system checks `LD_LIBRARY_PATH`, and loads whatever it can. So it is all on the operating system.", "Maybe we can extract the information using `dlinfo`:\r\nhttp://man7.org/linux/man-pages/man3/dlinfo.3.html", "Thanks @gunan for the explanation! We'll implement that then. We just need the two \r\n\r\ntf.config.get_cuda_version_used_to_compile_tf()\r\ntf.config.get_cudnn_version_used_to_compile_tf()\r\n\r\nthen, or equivalent.", "Agreed, let's definitely implement cuda/cudnn version queries.\r\nThey have no blockers. Then we can take our time to see if `dlinfo` tells us the path to the shared objects. If not, we can discuss about the paths again.", "TensorFlow also has this mechanism:\r\n\r\n```\r\nfrom tensorflow.python.platform import build_info\r\n```\r\n\r\nAuto-generated during build time via https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/build_info/gen_build_info.py and `tf_py_build_info_genrule` in https://osscs.corp.google.com/tensorflow/tensorflow/+/master:tensorflow/tensorflow.bzl. Right now it contains very little, but perhaps we could extend it with the information from https://cs.opensource.google/tensorflow/tensorflow/+/master:third_party/gpus/find_cuda_config.py that's used in `configure.py`?", "+1 to extending build info over new apis\n\nOn Wed, Apr 8, 2020, 16:05 Austin Anderson <notifications@github.com> wrote:\n\n> TensorFlow also has this mechanism:\n>\n> from tensorflow.python.platform import build_info\n>\n> Auto-generated during build time via\n> https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/build_info/gen_build_info.py\n> and tf_py_build_info_genrule in\n> https://osscs.corp.google.com/tensorflow/tensorflow/+/master:tensorflow/tensorflow.bzl.\n> Right now it contains very little, but perhaps we could extend it with the\n> information from\n> https://cs.opensource.google/tensorflow/tensorflow/+/master:third_party/gpus/find_cuda_config.py\n> that's used in configure.py?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38351#issuecomment-611238429>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJPOXKLNPNANK6YVSLRLT7LPANCNFSM4MDZ3HQA>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38351\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38351\">No</a>\n", "Thanks! ", "#38964 was reverted due to brittleness -- @r4nt gave some advice on improving it, so I'll work on re-enabling the functionality this week.", "It took longer than I thought, but I have an internal change prepared for review that adds the same functionality in a different way.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38351\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38351\">No</a>\n", "@angerson It seems that building tf-nightly now requires `opt_einsum` in host environment:\r\n\r\n```\r\nMon Jun 8 13:08:20 UTC 2020 : === Building wheel\r\n2020-06-08 13:08:21.249561: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 47, in <module>\r\n    from tensorflow.python.platform import build_info\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/__init__.py\", line 47, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/keras/models.py\", line 23, in <module>\r\n    from tensorflow.python.keras import backend as K\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/keras/backend.py\", line 37, in <module>\r\n    from tensorflow.python.distribute import distribute_coordinator as dc\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/distribute/distribute_coordinator.py\", line 33, in <module>\r\n    from tensorflow.python.training import monitored_session\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/training/monitored_session.py\", line 45, in <module>\r\n    from tensorflow.python.training.tracking import graph_view\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/training/tracking/graph_view.py\", line 27, in <module>\r\n    from tensorflow.python.training import optimizer as optimizer_v1\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/training/optimizer.py\", line 36, in <module>\r\n    from tensorflow.python.ops import gradients\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/ops/gradients.py\", line 27, in <module>\r\n    from tensorflow.python.ops.gradients_impl import gradients\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/ops/gradients_impl.py\", line 30, in <module>\r\n    from tensorflow.python.ops import linalg_grad  # pylint: disable=unused-import\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/ops/linalg_grad.py\", line 47, in <module>\r\n    from tensorflow.python.ops.linalg import linalg_impl as _linalg\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/ops/linalg/linalg_impl.py\", line 34, in <module>\r\n    from tensorflow.python.ops import special_math_ops\r\n  File \"/tmp/tmp.7oZofsqRJ1/tensorflow/python/ops/special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\nImportError: No module named 'opt_einsum'\r\n```\r\n\r\nwhich seems to [break tf-nightly on Linux](https://pypi.org/project/tf-nightly/2.3.0.dev20200606/#files) since your CL landed; not sure if it's going to be fixed in TF or the build environment.\r\n\r\ncc @yifeif @av8ramit \r\n\r\nEDIT: adding `opt-einsum` pip3 dependency to the build environment seems to fix the issue above.", "That is an unrelated problem. Could you open a new issue?", "@gunan Just found out that it's been fixed by 67099d08529a00f48ee8a444f471067bedcc2f5b. So no need to open a new issue :)", "Thanks for the fix, @gunan!\r\n\r\n@gabrieldemarmiesse tf.sysconfig.get_build_info() is working in the latest tf-nightly (thanks to follow-up fixes from @chsigg and @gunan). Can you check it out?", "Thanks everyone, I'll take a look :) ", "@angerson this is awesome! Thanks.\r\n```\r\n>>> import tensorflow as tf\r\nW tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/rh-python36/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib64:/opt/rh/devtoolset-8/root/usr/lib:/opt/rh/devtoolset-8/root/usr/lib64/dyninst:/opt/rh/devtoolset-8/root/usr/lib/dyninst:/usr/local/lib64:/usr/local/lib\r\nI tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>>\r\n>>> tf.__version__\r\n'2.4.0-dev20200706'\r\n>>>\r\n>>> tf.sysconfig.get_build_info()\r\n{'cuda_version': '10.1', 'cudnn_version': '7', 'cuda_compute_capabilities': ['sm_35', 'sm_37', 'sm_52', 'sm_60', 'sm_61', 'compute_70'], 'cpu_compiler': '/usr/bin/gcc-5', 'is_rocm_build': False, 'is_cuda_build': True}\r\n```"]}, {"number": 38350, "title": "ImportError: No module named numpy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  2.1.0\r\n- Python version: - Bazel\r\nversion (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/api/tests\r\nNumpy is installed still numpy Import Error .\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nbazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n\t\t_create_local_python_repository(<1 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 213, in _create_local_python_repository\r\n\t\t_get_numpy_include(<2 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 187, in _get_numpy_include\r\n\t\texecute(repository_ctx, <3 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nProblem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nERROR: While resolving toolchains for target //tensorflow/tools/api/tests:api_compatibility_test: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n\t\t_create_local_python_repository(<1 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 213, in _create_local_python_repository\r\n\t\t_get_numpy_include(<2 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 187, in _get_numpy_include\r\n\t\texecute(repository_ctx, <3 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nProblem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nERROR: Analysis of target '//tensorflow/tools/api/tests:api_compatibility_test' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n\t\t_create_local_python_repository(<1 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 213, in _create_local_python_repository\r\n\t\t_get_numpy_include(<2 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/py/python_configure.bzl\", line 187, in _get_numpy_include\r\n\t\texecute(repository_ctx, <3 more arguments>)\r\n\tFile \"/home/ayushman/Documents/github/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n\t\tfail(<1 more arguments>)\r\nProblem getting numpy include path.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: No module named numpy\r\nIs numpy installed?\r\nINFO: Elapsed time: 0.357s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n", "comments": ["@ayushmankumar7\r\nplease refer to below issues and let us know if it helps\r\n#37888 #30465  #[link](https://github.com/tensorflow/tensorflow/issues/29845#issuecomment-521006935)  #[link](https://github.com/tensorflow/tensorflow/issues/22794)", "Thanks for the help.. it was resolved. ", "@ayushmankumar7 \r\nGlad to help, moving this issue to closed as it is resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38350\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38350\">No</a>\n", "Yes.. It worked for me .. My problem is solved.\n\nOn Mon, Apr 13, 2020, 10:59 Saduf2019 <notifications@github.com> wrote:\n\n> @ayushmankumar7 <https://github.com/ayushmankumar7>\n> please confirm if i may move this to closed status as its resolved.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/38350#issuecomment-612757089>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJ7X65TWHODWOGEFPRJT323RMKPJXANCNFSM4MDZJS6A>\n> .\n>\n"]}, {"number": 38349, "title": "`nan` gradient when `tf.where` is used", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Debian GNU/Linux 10 (buster)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 / v1.12.1-29016-g38797a1c8b 2.2.0-dev20200407\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWell-defined function with `tf.where` has `nan` gradients at points where `tf.where` inactive branch is undefined.\r\n\r\n**Describe the expected behavior**\r\nInactive branch should be ignored in gradients calculations.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\n\r\nfor ex in range(-3, 3):\r\n    x = tf.convert_to_tensor(10.**ex)\r\n    with tf.GradientTape() as g:\r\n        g.watch(x)\r\n        y = tf.where(x >= -1., x, tf.math.log1p(-x))\r\n#         y = tf.where(x >= -1., x, tf.math.log(1.-x))\r\n#         y = tf.where(x >= -1., x, 1./(1.-x))\r\n    dy_dx = g.gradient(y, x)\r\n    print(f'y({x})={y}, dy/dx({x})={dy_dx}')\r\n```\r\n\r\nAll 3 functions above are well defined for positive values used for testing. Still they show no gradient at point `1.`. while it has to be equal to `1.`\r\n\r\n```\r\ny(0.0010000000474974513)=0.0010000000474974513, dy/dx(0.0010000000474974513)=1.0\r\ny(0.009999999776482582)=0.009999999776482582, dy/dx(0.009999999776482582)=1.0\r\ny(0.10000000149011612)=0.10000000149011612, dy/dx(0.10000000149011612)=1.0\r\ny(1.0)=1.0, dy/dx(1.0)=nan\r\ny(10.0)=10.0, dy/dx(10.0)=1.0\r\ny(100.0)=100.0, dy/dx(100.0)=1.0\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/806f63f2cf04070a4601289d7003cf0a/untitled24.ipynb). Thanks!", "This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.\r\n\r\nYou can find more detail here, along with a recipe for how to avoid it: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444\r\n\r\nIn short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349\">No</a>\n", "Shouldn't this be documented with big warning in `tf.where` docs in this case?", "Indeed it should.", "@mdanatg Hello, this is my first time contributing to TensofFlow lib. From the thread I gather you would require the `tf.where` be updated. If it is so can I work on this?", "Hello @0x0badc0de , @mdanatg \r\nShould the updated doc contain a something like a warning? or will a small note at the end, about the input not being Nan will do? Also should the workaround for avoiding it also be added to the doc?", "@joemaren @anorak-k \r\n\r\nSorry for the delay. Feel free to send a PR - it's only a matter of adding a paragraph to the docstring.\r\n\r\nThe text should be more in the lines of a warning. Something like: **Important: if any of the inputs contain NaN values, etc.**. And yes, it should include the workaround as well, which is something in the lines of: instead of `tf.where(x, ops_that_can_nan(z), ...)`, write `tf.where(x, ops_that_can_nan(tf.where(x, z, safe_value)), ...)`.", "@mdanatg I have added the change and raised a PR #38467 ", "@mdanatg Thanks for your reply. However, I would like to mention that this behavior also happens when the generated value in the inactive branch **is not finite** (i.e. `inf` or `-inf`). Here is a minimal reproducible example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.Variable(10.)\r\nwith tf.GradientTape() as tape:\r\n  out = tf.where(a < 15., a, tf.math.pow(10.0, tf.math.exp(a)))\r\n  grads = tape.gradient(out, a)\r\n\r\nprint(grads)\r\n# tf.Tensor(nan, shape=(), dtype=float32)\r\n```\r\n\r\nAnd also if we reverse the condition such that the branch with infinite value is selected, the gradient would be infinite (which is a bit surprising that it does not generate `nan` instead, like above):\r\n```python\r\nwith tf.GradientTape() as tape:\r\n  out = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(a)))\r\n  grads = tape.gradient(out, a)\r\n\r\nprint(grads)\r\n# tf.Tensor(inf, shape=(), dtype=float32)\r\n```\r\nSo this behavior happens for both `nan` and **infinite values** in inactive branch. I wish it wasn't like this, because it's a bit unreasonable and makes it impossible to use user-defined ops/functions which generate extremely large values for some input values; hence, that inner `tf.where` workaround may not be practical always (unfortunately, even gradient clipping does not help with this, because clipping a `nan` value produces `nan` in TF).\r\n\r\nCC: @anorak-k for potential consideration in your PR after @mdanatg confirms this.", "@mkaze that's true - nan, inf and any other special FP value will disrupt the gradient calculation.\r\n\r\nWhat happens internally is that the gradients are aggregated in this fashion: `1 * <grad of branch taken> + 0 * <grad of branch not taken>`. In the former case, you have `0 * inf = nan`. In the latter case, you have `1 * inf = inf`. I agree it's very confusing, unfortunately a naive fix would add significant overhead to gradient calculations.\r\n\r\nMoreover, the forward calculation doesn't need to result in a nan or inf. You can also get weird results if the gradient alone is nan or inf. For example, the cube root function is defined and well-behaved everywhere, but its derivative at zero is infinite. So this will give you a nan gradient too:\r\n\r\n```\r\na = tf.Variable(0.0)\r\nwith tf.GradientTape() as tape:\r\n  out = tf.where(a < 1, a, tf.pow(a, 1.0/3.0))\r\n  grads = tape.gradient(out, a)\r\nprint(grads)\r\n```\r\n\r\nI think the tf.where workaround is useful with infinite values as well, so long as the branch not taken is forced to take a gradient that can be safely multiplied by 0. For your example, it would be something like this:\r\n\r\n```\r\ndummy_safe_value = 0\r\nsafe_a = tf.where(a > 15., dummy_safe_value, a)\r\nout = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(safe_a)))\r\n```\r\n\r\nI agree that it sometimes can be impractical to do, but in principle it should always be possible as long as you control the inputs to the sensitive functions - all they have to do is force finite values in all the elements that are dropped.", "I want to fix the issue [#38349](https://github.com/tensorflow/tensorflow/issues/38349)", "> This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.\r\n> \r\n> You can find more detail here, along with a recipe for how to avoid it: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444\r\n> \r\n> In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.\r\n\r\nYou can simply have it raise a value error if its getting Nan inputs. Or does it not work like that?", "Can I work on this issue if someone isn't now?", "@tushar-dalal The challenge is that verifying for such NaN inputs can be taking on performance. When debugging, `tf.debugging.check_numerics` can indeed help with that.\r\n\r\n@unicorn-io Feel free to tackle it, but note that it's extremely challenging to solve. That said, there was a PR (#38467) to add a warning message to the docs of tf.where, it would be useful to revive it.", "I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue.", "> I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue. @mdanatg \r\n\r\n", "@unicorn-io You can start by looking at the [gradient code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h#L149) and understanding how it works. Then you can reproduce when happens in the case of a where with bad gradients.", "Cool I'll get to it", "Hey i would like to work on it. can also help please ", "> \r\n> \r\n> Cool I'll get to it\r\n\r\nThis bug cannot be fixed as of now it seems.", "It's indeed very challenging to fix. However, the documentation of affected ops, like `tf.where` can still be updated to alert the users about it.", "@mdanatg isn't #38497 addressing this and is closed?", "You mean #38467? It's closed due to staleness, and it would be useful to revive. By the looks of it it's safe to assume noone else is working on it.", "Seems like its a long time since the last activity. Is this issue still open to be worked on?", "I think so. There are two parts to it: (1) updating the docs of tf.where, which is fairly straightforward, and (2) actually trying to address the issue, which is a significant undertaking because it involves a rather fundamental issue.", "Is this issue still addressable ?", "Nice to be part of the group.\r\nPlease, have a look to my pull request for the workaround: #41721 \r\nI'm going to work on the main issue too.\r\nI'll be happy to cooperate with anybody else interested.", "@codeadmin-peritiae The PR appears to be empty. Perhaps there's an issue with the git client?", "Just to follow up on the events. It looks like codeadmin-peritiae had an issue with his original PR #41721 where he had trouble with his SSH certificate. He then opened up another PR #41775 which is currently blocked since some of the checks haven't completed. By the looks of it, the documentation update part of this problem is almost completed.", "Is it issue solvable for a beginner ? if yes can I work on it? @Harsh188 @mdanatg ", "I think this issue should be closed, #41775 got merged and fixed the issue with the documentation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349\">No</a>\n", "in case someone is still stuck with NaN inputs, tf.where can be entirely replaced with:\r\n```python\r\ntf.minimum(tensor_having_nans, value_that_replaces_nans)\r\n```\r\nand gradients don't truncate."]}, {"number": 38348, "title": "Cannot convert tf.math.conj Op in Keras model to tflite", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Linux-4.19.0-8-amd64-x86_64-with-debian-10.3\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-dev20200406\r\n- **Python version**: 3.7.6\r\n- **CUDA/cuDNN version**: 10.0\r\n- **GPU model and memory**: 4x GeForce RTX2080Ti, 12Gb\r\n\r\n### Describe the problem\r\nUnsupported Op tf.math.conj when converting keras model to tflite. Is there a workaround for this?\r\n\r\n### Source code (MWE)\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\n\r\nif __name__ == '__main__':\r\n    model_in = Input(shape=(128,1))\r\n\r\n\r\n    fft = Lambda(lambda x: tf.signal.rfft(x), name='RFFT')\r\n    sig_spec = fft(model_in)\r\n\r\n    conjugate = Lambda(lambda x: tf.math.conj(x), name='conjugate')(sig_spec)\r\n    correlation_spec = sig_spec * conjugate\r\n    correlation = Lambda(lambda x: tf.signal.irfft(x), name='IRFFT')(correlation_spec)\r\n\r\n    model = Model(inputs=model_in, outputs=correlation)\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.experimental_new_converter = True\r\n    converter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\n    tflite_model = converter.convert()\r\n```\r\n### Error log\r\n`<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.Conj {device = \"\"}\r\n`\r\n\r\n\r\n", "comments": ["@JonathanDZiegler,\r\nCould you please try adding the below line to your code.\r\n`converter.allow_custom_ops = True`\r\n\r\nWhen I did, I was able to run the code without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3dae3c7398f5f931814d1d47d45e7195/38348-2-2.ipynb#scrollTo=GOxQjNMrmnGv&line=11&uniqifier=1). Thanks!", "@amahendrakar the error disappeared (in the minimal example above and also in the actual code), thank you very much! I was under the impression that setting `allow_custom_ops = True` required custom handlers to be written for the converter. I will test the converted model to see if the error disappeared or just the error message and report back asap. ", "As suspected, the issue is only displaced by this. Now the problem occurs when trying to use the model for inference. Tried writing a custom function for conj, but to no avail: the converter is unable to process `tf.complex`, `tf.math.real `and `tf.math.imag`.", "When you enable SELECT_TF_OPS the converter adds to the model the op as an a custom op to be consumed by TF(Select) delegate.\r\nYou will need to use the AAR that supports SELECT and it should work.\r\nSee instructions\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nFor running the converted model see this section https://www.tensorflow.org/lite/guide/ops_select#running_the_model\r\n\r\nIf you want to write a custom implementation for an existing TF op, then this is different than SELECT_TF\r\nsee https://www.tensorflow.org/lite/guide/ops_custom\r\n", "Thank you for your reply. Sadly, the python implementation is still under active development. Hoping something will be available soon. Otherwise I will try the C++ implementation. If possible, I would leave this thread open and report back, as soon as any progress has been made?", "@JonathanDZiegler Can you please check whether the issue was resolved in recent `tf-nightly`. I didn't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/30712fbf3caa8996fbc098455b4e184f/38348.ipynb). Thanks!", "@jvishnuvardhan thanks for your reply! I tested it on the mwe and it seems to work. Interestingly, the base methods tf.complex, tf.math.real and tf.math.imag still don't work but tf.math.conj does. I'll get to testing the actual model next week and report back.", "@JonathanDZiegler After verifying it, please close this issue. If there are any ops (not part of this model) that are not working, open a  new issue with a standalone code to reproduce the issue. Thanks!", "@jvishnuvardhan I was able to convert the model without problems now, thank you very much! Running the converted model in python currently does not seem to be supported but that would be another issue."]}, {"number": 38347, "title": "Waymo dataset: module 'tensorflow' has no attribute 'enable_eager_execution'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I am replicating the waymo dataset tutorial code for understanding and learning\r\n- Using Notebook of Google COLAB (as New user to Google Colab)\r\n- TensorFlow version (use command below): 2.1.0 (as installed by default in Google Colab)\r\n\r\n**Describe the current behavior**\r\n\r\nThis is the line of code: (sharing since it is available for public view as well by google in it's tutorial)\r\n``` \r\n!pip3 install waymo-open-dataset\r\nimport os\r\nimport tensorflow as tf\r\nimport math\r\nimport numpy as np\r\nimport itertools\r\n\r\ntf.enable_eager_execution()\r\n\r\nfrom waymo_open_dataset.utils import range_image_utils\r\nfrom waymo_open_dataset.utils import transform_utils\r\nfrom waymo_open_dataset.utils import frame_utils\r\nfrom waymo_open_dataset import dataset_pb2 as open_dataset\r\n```\r\n\r\n**Error I get upon executing the above lines of code**\r\n```\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n6 import itertools\r\n7\r\n----> 8 tf.enable_eager_execution()\r\n9\r\n10 from waymo_open_dataset.utils import range_image_utils\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\r\n```\r\n\r\nEven Upon commenting the line \" tf.enable_eager_execution()\" - as tensor 2.x version doesn't need, a new error pops up.\r\n```\r\n!pip3 install waymo-open-dataset\r\nimport os\r\nimport tensorflow as tf\r\nimport math\r\nimport numpy as np\r\nimport itertools\r\n\r\n#tf.enable_eager_execution()\r\n\r\nfrom waymo_open_dataset.utils import range_image_utils\r\nfrom waymo_open_dataset.utils import transform_utils\r\nfrom waymo_open_dataset.utils import frame_utils\r\nfrom waymo_open_dataset import dataset_pb2 as open_dataset\r\n\r\nI get the following error\r\n\r\nAttributeError Traceback (most recent call last)\r\nin ()\r\n8 #tf.enable_eager_execution()\r\n9\r\n---> 10 from waymo_open_dataset.utils import range_image_utils\r\n11 from waymo_open_dataset.utils import transform_utils\r\n12 from waymo_open_dataset.utils import frame_utils\r\n\r\n/usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py in ()\r\n57 value,\r\n58 shape,\r\n---> 59 pool_method=tf.unsorted_segment_max):\r\n60 \"\"\"Similar as tf.scatter_nd but allows custom pool method.\r\n61\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'unsorted_segment_max'\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error should be there.\r\n\r\n**Standalone code to reproduce the issue**  \r\nLink to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/gist/Viki250893/40008cf236a25736d1cdefc7e0e403ea/untitled1.ipynb\r\n\r\n", "comments": ["@Viki250893, \r\nIn Tensorflow 2.1,use `tf.compat.v1.enable_eager_execution()` instead of `tf.enable_eager_execution()`. \r\nAnd this error AttributeError: module 'tensorflow' has no attribute 'unsorted_segment_max' is not related to Tensorflow. Thanks\r\n\r\n", "I would like to highlight 2 points here,\r\n**Point 1:**\r\nWAYMO/Author whoever it is, has updated the tutorial script with\r\n**Updated script has the following**\r\n``` \r\nimport tensorflow.compat.v1 as tf  \r\n```\r\n\r\n**but, when I reported it, (Previously it was )**\r\n```\r\nimport tensorflow as tf\r\n```\r\nhttps://colab.research.google.com/gist/Viki250893/40008cf236a25736d1cdefc7e0e403ea/untitled1.ipynb#scrollTo=3etKdSB4gIeR\r\n\r\nI'm not sure what's happening. \r\n\r\nI would like to understand the difference of it being compatible with updated \"compat.v1\". This is on importing the library, not the eager enabler.\r\n\r\n**Point 2:**\r\n Upon following the instruction of the using - tf.compat.v1.enable_eager_execution()\r\nin code as below without change in line 2( import tensorflow as tf ) of code\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport math\r\nimport numpy as np\r\nimport itertools\r\n\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nfrom waymo_open_dataset.utils import range_image_utils\r\nfrom waymo_open_dataset.utils import transform_utils\r\nfrom waymo_open_dataset.utils import frame_utils\r\nfrom waymo_open_dataset import dataset_pb2 as open_dataset\r\n```\r\nThe following error is produced, which I believe occurs when I use the recommendation. \r\n```\r\ntf.compat.v1.enable_eager_execution()\r\n```\r\nError produced is \r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-4-8608b8bb903e> in <module>()\r\n      8 tf.compat.v1.enable_eager_execution()\r\n      9 \r\n---> 10 from waymo_open_dataset.utils import range_image_utils\r\n     11 from waymo_open_dataset.utils import transform_utils\r\n     12 from waymo_open_dataset.utils import frame_utils\r\n\r\n/usr/local/lib/python3.6/dist-packages/waymo_open_dataset/utils/range_image_utils.py in <module>()\r\n     57                           value,\r\n     58                           shape,\r\n---> 59                           pool_method=tf.unsorted_segment_max):\r\n     60   \"\"\"Similar as tf.scatter_nd but allows custom pool method.\r\n     61 \r\n\r\nAttributeError: module 'tensorflow' has no attribute 'unsorted_segment_max'\r\n```\r\nI hope this is indirectly related to the above issue of tensorflow version.\r\nHowever as mentioned, the error is not produced with the update script in WAYMO Tutorial that uses \"compat.v1\" tensorflow library.\r\n\r\nI'm not sure if you like to close this issue or help realise better understanding\r\n\r\nThank you", "@Viki250893, Tensorflow 2.x has default eager enabled. To enable explicitly use `tf.compat.v1.enable_eager_execution()` which is same as \r\nimport tensorflow.compat.v1 as tf  and tf.enable_eager_execution().\r\n\r\nThis error `AttributeError: module 'tensorflow' has no attribute 'unsorted_segment_max'` is outside the scope of Tensorflow. Thanks\r\n\r\n", "Thank you very much for the support. !!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38347\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38347\">No</a>\n"]}, {"number": 38346, "title": "Not able to load a tf.keras model", "body": "**System information** \r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: MacOs Mojave version 10.14.6\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n TensorFlow version: v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2\r\n- Python version: - Python 3.6.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: - GPU model and memory:N/A\r\n\r\n**Describe the current behavior**\r\nFor the below model(using tf.signal.frame), saving to keras format works, but when trying to load it, we get the following error:\r\n```\r\n2020-04-08 00:38:22.847450: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-08 00:38:22.865528: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13ce3feb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-08 00:38:22.865552: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1654, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Inconsistent values for attr 'Tshape' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Reshape/Reshape' using Op<name=Reshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 8, in <module>\r\n    keras_model = tf.keras.models.load_model(\"test.h5\")\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 184, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 170, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 109, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 373, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 982, in from_config\r\n    config, custom_objects)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 2024, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1972, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 922, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2830, in call\r\n    return self._make_op(inputs)\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2852, in _make_op\r\n    c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n  File \"/Users/spareek/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1657, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Inconsistent values for attr 'Tshape' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Reshape/Reshape' using Op<name=Reshape; signature=tensor:T, shape:Tshape -> output:T; attr=T:type; attr=Tshape:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n```\r\n\r\n**Describe the expected behavior**\r\nModel loading should work fine.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\n  \r\nx = tf.keras.Input(shape=(None, 12), dtype=\"float32\", name=\"input\")\r\nx_frequency = tf.signal.frame(x, 2, 1, axis=1)\r\nmodel = tf.keras.Model(inputs=x, outputs=x_frequency, name=\"test\")\r\nmodel.save(\"test.h5\")\r\n\r\nkeras_model = tf.keras.models.load_model(\"test.h5\")\r\n```\r\n", "comments": ["I think the issue is coming due to this line - https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/ops/signal/shape_ops.py#L127\r\n\r\nI can reproduce it with this too:\r\n```\r\nimport tensorflow as tf\r\n  \r\nx = tf.keras.Input(shape=(), dtype=\"float32\", name=\"input\")\r\nx_reshape = tf.reshape(x, [])\r\nmodel = tf.keras.Model(inputs=x, outputs=x_reshape, name=\"test1\")\r\nmodel.save(\"test1.h5\")\r\n\r\nkeras_model = tf.keras.models.load_model(\"test1.h5\")\r\n```\r\nI guess due to empty shape TF is getting confused whether the shape is of type float(same type as input) or int which is expected?", "i have replicated this issue, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/92782091ec5a43be75b9a78c020b764b/untitled131.ipynb)", "I tried to produce the summary of model \"test.h5\". However the model isnt getting loaded and is giving the error because its an empty model with zero parameters.\r\n\r\n\r\nModel: \"test\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput (InputLayer)              [(None, None, 12)]   0                                            \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Shape_14 (TensorFlo [(3,)]               0           input[0][0]                      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_split_7 (TensorFlow [(1,), (1,), (1,)]   0           tf_op_layer_Shape_14[0][0]       \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape_28 (TensorF [()]                 0           tf_op_layer_split_7[0][1]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Sub_7 (TensorFlowOp [()]                 0           tf_op_layer_Reshape_28[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_FloorDiv_14 (Tensor [()]                 0           tf_op_layer_Sub_7[0][0]          \r\n__________________________________________________________________________________________________\r\ntf_op_layer_FloorDiv_15 (Tensor [()]                 0           tf_op_layer_Reshape_28[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_14 (TensorFlo [()]                 0           tf_op_layer_FloorDiv_14[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_ZerosLike_7 (Tensor [(3,)]               0           tf_op_layer_Shape_14[0][0]       \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mul_14 (TensorFlowO [()]                 0           tf_op_layer_FloorDiv_15[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Maximum_7 (TensorFl [()]                 0           tf_op_layer_AddV2_14[0][0]       \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_21/values_1  [(1,)]               0           tf_op_layer_Mul_14[0][0]         \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Shape_15 (TensorFlo [(1,)]               0           tf_op_layer_ZerosLike_7[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Range_7 (TensorFlow [(None,)]            0           tf_op_layer_Maximum_7[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_21 (TensorFl [(3,)]               0           tf_op_layer_split_7[0][0]        \r\n                                                                 tf_op_layer_concat_21/values_1[0]\r\n                                                                 tf_op_layer_split_7[0][2]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Fill_7 (TensorFlowO [(3,)]               0           tf_op_layer_Shape_15[0][0]       \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_22/values_1  [(2,)]               0           tf_op_layer_FloorDiv_15[0][0]    \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Mul_15 (TensorFlowO [(None,)]            0           tf_op_layer_Range_7[0][0]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape_30/shape (T [(2,)]               0           tf_op_layer_Maximum_7[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_StridedSlice_7 (Ten [(None, None, None)] 0           input[0][0]                      \r\n                                                                 tf_op_layer_ZerosLike_7[0][0]    \r\n                                                                 tf_op_layer_concat_21[0][0]      \r\n                                                                 tf_op_layer_Fill_7[0][0]         \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_22 (TensorFl [(4,)]               0           tf_op_layer_split_7[0][0]        \r\n                                                                 tf_op_layer_concat_22/values_1[0]\r\n                                                                 tf_op_layer_split_7[0][2]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape_30 (TensorF [(None, 1)]          0           tf_op_layer_Mul_15[0][0]         \r\n                                                                 tf_op_layer_Reshape_30/shape[0][0\r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape_29 (TensorF [(None, None, 1, Non 0           tf_op_layer_StridedSlice_7[0][0] \r\n                                                                 tf_op_layer_concat_22[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_AddV2_15 (TensorFlo [(None, 2)]          0           tf_op_layer_Reshape_30[0][0]     \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_23/values_1  [(2,)]               0           tf_op_layer_Maximum_7[0][0]      \r\n__________________________________________________________________________________________________\r\ntf_op_layer_GatherV2_7 (TensorF [(None, None, 2, 1,  0           tf_op_layer_Reshape_29[0][0]     \r\n                                                                 tf_op_layer_AddV2_15[0][0]       \r\n__________________________________________________________________________________________________\r\ntf_op_layer_concat_23 (TensorFl [(4,)]               0           tf_op_layer_split_7[0][0]        \r\n                                                                 tf_op_layer_concat_23/values_1[0]\r\n                                                                 tf_op_layer_split_7[0][2]        \r\n__________________________________________________________________________________________________\r\ntf_op_layer_Reshape_31 (TensorF [(None, None, 2, 12) 0           tf_op_layer_GatherV2_7[0][0]     \r\n                                                                 tf_op_layer_concat_23[0][0]      \r\n==================================================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n\r\n**SOLUTION**\r\n1. Adding some parameters can resolve the issue\r\n2. Try this syntax to save your model-\r\n              import tensorflow as tf\r\n  \r\nx = tf.keras.Input(shape=(None, 12), dtype=\"float32\", name=\"input\")\r\nx_frequency = tf.signal.frame(x, 2, 1, axis=1)\r\nmodel = tf.keras.Model(inputs=x, outputs=x_frequency, name=\"test\")\r\nmodel= tf.keras.models.Sequential([model,tf.keras.layers.Flatten(input_shape=(None,12))])\r\n\r\ntf.saved_model.save(model, \"test1.h5\")\r\nkeras_model = tf.saved_model.load(\"test1.h5\")", "Thanks for looking into this. My original model does have trainable params but I still get the same error. I will try the syntax provided by you and update.\r\n", "Tried out the work around suggested. It  seems to work for this case:\r\nx = tf.keras.Input(shape=(None, 12), dtype=\"float32\", name=\"input\")\r\nx_frequency = tf.signal.frame(x, 2, 1, axis=1)\r\nmodel = tf.keras.Model(inputs=x, outputs=x_frequency, name=\"test\")\r\nmodel= tf.keras.models.Sequential([model,tf.keras.layers.Flatten(input_shape=(None,12))])\r\n\r\nBut can you please help me understand what the last line is exactly doing and how it resolves the issue?\r\n\r\nIt does not seem to work for the second example:\r\nimport tensorflow as tf\r\n  \r\nx = tf.keras.Input(shape=(1), dtype=\"int32\", name=\"input\")\r\nx_reshape = tf.reshape(x, [])\r\nmodel = tf.keras.Model(inputs=x, outputs=x_reshape, name=\"test1\")\r\nmodel= tf.keras.models.Sequential([model,tf.keras.layers.Flatten(input_shape=(1))])\r\nmodel.save(\"test6.h5\")\r\nkeras_model = tf.keras.models.load_model(\"test6.h5\")\r\nMaybe I am doing something wrong?\r\n", "There's an issue when reshape is called with an empty list as the shape. I can't think of a workaround but looking into a way to fix this.", "I have what appears to be the same or related issue.  I've reduced this down to a very simple case.  My problem is I don't understand the workaround stated above.\r\n\r\nMy code:  \r\n```\r\nDNN_model = Sequential()\r\nDNN_model.add(Dense(units = 1, activation = 'elu'))\r\nDNN_model.add(Dense(units = 10, activation = 'elu'))\r\nDNN_model.add(Dense(units = 1))\r\n#\r\nDNN_model.compile(loss = MeanSquaredError())\r\n#\r\nDNN_model.fit(np.array(x), np.array(y),\r\n              epochs = 25,\r\n              verbose = 1)\r\n#\r\nDNN_model.save('keras_parallel_' + ts() + str(np.random.randint(1e8, 9e8)) + '.hdf5')\r\n```\r\n\r\nIn the above the ts() and the random number just generatre unique signatures for the file name.\r\n\r\nAn example file is:  \r\nkeras_parallel_2020-09-11_18_41_27296955088.hdf5  \r\n\r\nIf I then use tensorflow.keras.models.load_model('keras_parallel_2020-09-11_18_41_27296955088.hdf5') I get:\r\n\r\n```\r\nload_model('keras_parallel_2020-09-11_18_41_27296955088.hdf5')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-63-5890c3f6cf58>\", line 1, in <module>\r\n    load_model('keras_parallel_2020-09-11_18_41_27296955088.hdf5')\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 303, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 380, in from_config\r\n    model.build(build_input_shape)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 260, in build\r\n    super(Sequential, self).build(input_shape)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 682, in build\r\n    self.call(x, **kwargs)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 281, in call\r\n    outputs = layer(inputs, **kwargs)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 737, in __call__\r\n    self.name)\r\n\r\n  File \"C:\\Users\\bbate\\Miniconda3\\envs\\keras-gpu-4\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\", line 193, in assert_input_compatibility\r\n    str(x.shape.as_list()))\r\n\r\nValueError: Input 0 of layer dense_33 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\r\n```\r\n\r\n", "> I have what appears to be the same or related issue. I've reduced this down to a very simple case. \r\n\r\nUpdate:  \r\nThis was solved by changing the input array passed to the model:\r\n```\r\nnp.array(x).reshape(-1, 1)\r\n```\r\nAlthough the model trains in both cases, the saved model cannot be loaded in the case where the array is not explicitly 2-d.\r\n", "@sonu1-p Looks like this was resolved in recent `tf-nightly`. I am not able to reproduce the loading issue with `tf-nightly`. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6ee54960bcb6491bc3a886f9d9879ee8/untitled131.ipynb)\r\n\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it was resolved in recent `tf-nightly`. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38346\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38346\">No</a>\n"]}, {"number": 38345, "title": "TFLu: Add ethos-u55 kernel", "body": "The PR adds the Ethos-U55 TFLu integration code, including a README with initial instructions. This enables a user to use Ethos-U55 in combination with TFLu.", "comments": ["@mansnils Can you please resolve conflicts? Thanks!", "@mansnils Can you please resolve conflicts? Thanks!", "@petewarden ping for review", "@gbaned ready to pull", "@rthadur Can you please re-approve changes?", "@mansnils can you please resolve conflicts ?", "@rthadur Conflics are resolved. Could you please re-approve?", "@mansnils here is the inrernal error , please check \r\n\r\n`ld.lld: error: undefined symbol: tflite::ops::micro::custom::Register_ETHOSU()\r\n>>> referenced by all_ops_resolver.cc\r\n>>>  tensorflow/lite/micro/kernels/_objs/portable_optimized_ops_resolver/all_ops_resolver.o:(tflite::ops::micro::AllOpsResolver::AllOpsResolver())\r\n\r\nld.lld: error: undefined symbol: tflite::ops::micro::custom::GetString_ETHOSU()\r\n>>> referenced by all_ops_resolver.cc\r\n>>>      lite/micro/kernels/_objs/portable_optimized_ops_resolver/all_ops_resolver.o:(tflite::ops::micro::AllOpsResolver::AllOpsResolver())\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`", "@rhadur What is the build command to get that error?", "@mansnils i guess this test fails [here](https://github.com/tensorflow/tensorflow/blob/7193ecaf505239876f70c3649fa68e3565e116c1/tensorflow/lite/micro/kernels/BUILD#L238) ,\r\ncc @petewarden ", "@rthadur Thanks, error is resolved now.", "@rthadur New conflicts resolved. Could you please re-approve?", "Changes have merged internally , waiting for auto-merge to happen. Thank you ", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 38344, "title": "tf.data.experimental.make_csv_dataset ERROR", "body": "**TensorFlow2.0-GPU**\r\n\r\n**My Code:**\r\n![image](https://user-images.githubusercontent.com/30002208/78754918-e2852800-79aa-11ea-954f-b8293fdf85a1.png)\r\n\r\n\r\n\r\n\r\n**My Original CSV Data Example(Lager CSV):**\r\n\r\n![image](https://user-images.githubusercontent.com/30002208/78753676-caaca480-79a8-11ea-8cc2-b2c5bd00e162.png)\r\n\r\n**Code Result:**\r\n![image](https://user-images.githubusercontent.com/30002208/78753734-eb74fa00-79a8-11ea-8be4-c75b0b739031.png)\r\n\r\n\r\n", "comments": ["@woaij100 \r\n\r\nPlease, share the colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "> @woaij100\r\n> \r\n> Please, share the colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!\r\n\r\n```python\r\nclass GetRecord:\r\n    def __init__(self):\r\n        self.valCsvPath = \"./test.csv\"\r\n        self.column_names = [\"uid\", \"user_location\", \"user_phone\", \"sex\", \"reg_time\", \"total_score\", \"active_days\",\r\n                             \"last_rep_time\", \"share_num\", \"user_click_emd\", \"article_id\", \"item_muid\", \"item_ty\",\r\n                             \"item_elapse_time\", \"item_title_len\", \"item_content_len\", \"item_img_count_show\", \"is_hot\",\r\n                             \"is_timely\", \"read_count\", \"comment_count\", \"share_count\", \"embedding\", \"label\"\r\n                             ]\r\ndef dateSet(self, csvPath):\r\n    titanic_batches = tf.data.experimental.make_csv_dataset(\r\n        file_pattern=[csvPath],\r\n        column_names=self.column_names,\r\n        header=True,\r\n        batch_size=1,\r\n        num_epochs=1,\r\n        label_name=\"label\",\r\n    )\r\n    for feature_batch, label_batch in titanic_batches.take(1):\r\n        tf.print(feature_batch)\r\n        tf.print(label_batch)\r\n        \r\n def record(self):\r\n    self.dateSet(self.valCsvPath)\r\n    \r\n if __name__ == '__main__':\r\n      getRecord = GetRecord()\r\n      getRecord.record()`\r\n     \r\n    \r\n################################ CSV Data #######################################\r\n**\"uid\",\"user_location\",\"user_phone\",\"sex\",\"reg_time\",\"total_score\",\"active_days\",\"last_rep_time\",\"share_num\",\"user_click_emd\",\"article_id\",\"item_muid\",\"item_ty\",\"item_elapse_time\",\"item_title_len\",\"item_content_len\",\"item_img_count_show\",\"is_hot\",\"is_timely\",\"read_count\",\"comment_count\",\"share_count\",\"embedding\",\"label\"\r\n\"17985487\",4861,21753,1,1542071202,9397457,505,1585671751,1093,\"-0.25167966 -0.10032236 -0.20872113 0.21591923 0.260856 -0.17230628 -0.32972524 0.14190519 -0.1580164 0.2921775 -0.4921809 0.06366294 -0.1658238 -0.12961124 -0.21030532 -0.053090576 -0.23462565 0.45479047 0.5019187 0.04097429 0.18520656 0.1786165 0.13657352 -0.01976816 0.06910556 -0.124581136 0.4920518 -0.07594248 -0.6678689 0.58351386 0.16381356 -0.05195817 0.018817816 0.38820574 -0.14120482 0.19807835 -0.23848142 -0.20636757 -0.15078802 -0.4584044 -0.18382208 0.44269606 0.033945326 -0.39445785 0.15125223 -0.014372674 0.2661851 -0.45704883 -0.8017462 -0.7181106 0.25280666 -0.17663889 -0.7271539 0.032011747 -0.07120442 -0.054399997 -1.0001855 -0.015896799 0.09995063 0.28230545 0.636636 0.6184965 -0.2002947 0.22887753 0.18759274 0.4414419 -0.6715581 -0.6777693 0.06373328 0.94392294 0.24161474 0.23545563 -0.52339065 0.20200019 0.040595103 -0.08810647 -0.14525968 -0.1225167 -0.11132452 -0.19851208 0.5824302 0.014062549 0.27092758 0.20573182 -0.4195253 -0.050238617 0.20615943 -0.21276593 0.25522068 0.42480442 0.25149792 -0.71267223 0.13217977 -0.3427521 0.06390469 0.29725933 -0.11575365 0.3429323 -0.0718286 0.021932794 -0.13860787 0.017204462 0.29776365 0.13869835 0.16186395 0.2949204 -0.056505516 0.531207 -0.312772 0.08434303 -0.22614197 0.71487993 0.25012204 0.045338895 0.14093539 -0.004408737 0.03785949 0.3653543 -0.03796722 0.20353052 0.63507664 -0.013011506 0.11840205 0.7250151 -0.44824985 -0.39736903 -0.15328966 -0.0530441:-0.23750542 0.112305395 -0.32060584 0.30996263 0.10580547 -0.1421101 -0.19340828 0.20578349 -0.85617703 0.383378 -0.61103487 0.12053767 -0.427906 -0.080407724 -0.3604722 -0.05191031 0.43252993 0.510211 0.33908245 0.18067789 -0.08968375 0.85103524 0.42147493 0.1638593 0.36407363 -0.31267536 0.21107537 -0.116141155 -0.5652517 0.5683056 0.330949 -0.08705672 0.32243577 0.430772 -0.45246303 0.35371968 -0.15313523 -0.26074573 -0.55015075 -0.23265065 -0.36868325 0.39255774 0.43898782 -0.90766096 0.19371055 0.05391206 0.25899494 -0.6777457 -0.49123603 -0.49307138 -0.30907327 -0.26856327 -0.6779684 -0.27798107 0.033665944 -0.122680075 -0.622082 0.24377827 0.20314 0.7385307 0.21025287 0.62493783 -0.0359818 -0.1594337 0.060733706 0.26660421 -0.88772386 -0.67147756 -0.061905906 0.9523802 0.33809274 0.16361976 -0.5369995 0.020655626 0.4884081 0.27466205 -0.11246535 0.20913126 0.3447284 -0.22196928 0.9380058 0.14708875 0.49494532 -0.18714085 -0.35043716 0.20671724 0.32068694 -0.22489876 0.09292589 -0.13866016 -0.06340204 -0.5663325 0.18538809 0.097050235 0.12760735 0.37647697 -0.18295178 0.70932835 0.005389721 0.10735154 0.240722 0.46240377 0.29707333 -0.0594498 0.17678735 0.19071592 0.5181565 0.3150179 -0.33978495 0.31814054 -0.034403205 -0.028531998 0.48359865 0.25704104 0.41483557 -0.2139828 -0.082628384 0.19225432 0.3088851 0.13549007 0.3343489 0.24512918 0.3484266 0.7659921 -0.67896384 -0.48893395 -0.7220061 0.40883482:-0.2290219 0.095068105 -0.26163334 0.19822767 0.18309507 -0.25900176 -0.32615876 0.4839906 -0.59034646 0.22952682 -0.5760037 -0.047583826 -0.393659 -0.058417216 -0.119776495 0.14696288 0.37109905 0.37505057 0.28872135 -0.05950706 -0.14181751 0.57986146 0.22725084 0.31958488 0.38201672 -0.34369972 0.19121903 -0.084023066 -0.34615842 0.55220765 0.24684441 -0.00810766 0.29940492 0.45106834 -0.34927332 0.25020632 -0.3128665 -0.23767315 -0.2812649 -0.20216858 -0.59751976 0.2438226 0.2353237 -0.60482097 0.20254958 0.0539017 0.023182683 -0.52868277 -0.4910182 -0.5642391 0.044467494 -0.1447689 -0.6140861 -0.14011066 -0.20263027 -0.15437198 -0.56108904 0.1561204 0.3526758 0.3986535 0.5298502 0.7148146 -0.22078887 -0.13292354 0.16329354 0.3322514 -0.46003473 -0.5210308 -0.0058891587 1.0339748 0.29656237 0.21244961 -0.38581452 -0.29478562 0.4018823 0.31494588 0.03856075 0.3557011 0.062961966 -0.16704528 0.95228344 0.34653422 0.41051757 -0.014892526 -0.25353453 0.39496985 0.055043906 -0.106232926 0.35775736 -0.24568158 0.06184312 -0.44543663 0.48587522 0.09768255 0.119459905 0.24524821 -0.43679088 0.8066725 -0.22768828 0.14411017 0.17486745 0.5219606 0.3236006 -0.24992889 0.21638002 0.33289096 0.5669158 0.2174407 -0.17140183 -0.1336392 0.071937345 0.12659214 0.5160304 0.16201495 0.24844202 -0.45514044 -0.24189591 0.3768518 0.15451807 0.4399176 0.12712792 0.1030487 0.40682665 0.6782951 -0.62483215 -0.39919433 -0.49313498 0.25520873:-0.23240183 -0.2708679 0.545481 -0.24317448 -0.69616276 0.060653344 0.7014974 -0.07564723 0.34034148 0.45972973 -0.3250994 0.30692944 -0.15727772 0.2824782 0.018925812 0.43480802 0.261112 -0.20361732 0.21603012 0.33354723 0.5943538 0.19304036 -0.06886524 0.3432926 0.37783056 0.23547758 0.2796791 0.3999864 -0.07515195 0.14002705 -0.5927814 -0.06272103 -0.07822964 -0.23613273 0.0062159216 -0.105453275 -0.31375214 -0.21875544 -0.28061375 0.61733514 0.17138122 0.4945797 -0.28483567 -0.58347386 -0.2729335 0.06948729 0.025220193 -0.09921324 -0.26295456 0.82354623 0.20073985 -0.36597306 0.5522894 -1.0734346 -0.33601853 -0.73188275 -0.51348174 0.9984156 0.015987942 -0.19841368 -0.005474187 0.4884424 -0.04468811 -0.5624523 0.2132628 -0.43502393 0.6216747 0.44532144 -0.45892608 -0.3646686 -1.067908 -0.205817 -0.06930555 -0.28570747 0.6237013 -0.3330884 -0.5590652 0.18727605 -0.70474344 0.110777475 0.0060719475 -0.24098122 0.64869374 -0.49970222 0.042370513 0.3886692 -0.13566896 -0.3211529 -0.42126927 -0.19073816 -0.15719578 0.28601417 0.028088784 -0.63021433 0.4114301 0.3806257 0.25305414 0.089719966 0.519314 0.6923503 -0.25609657 -0.10765161 -0.463114 -0.36438248 0.44651365 0.13087794 0.5290369 -0.04256178 -0.004793505 0.13715084 -0.236165 -0.6318168 -0.10502044 0.10108881 0.006596751 -0.4424202 0.23728566 0.08774905 0.31769362 0.5768932 -0.03472586 -0.10133576 -0.5068457 0.06928665 -0.13188669 -0.2883746 -0.23523684 0.04399884:-0.30510092 0.07010954 -0.1116139 0.19457784 0.43508 0.42101288 -0.22062281 -0.21378332 -0.73547816 0.33972186 -0.33384258 -0.083032966 -0.5324309 0.17263976 -0.2832362 0.09163409 -0.00150793 0.5965752 0.12737672 -0.2489149 0.054807965 0.7664089 0.41736242 0.11749503 0.24682854 -0.37845716 0.19872469 0.04781768 -0.8278635 0.41845313 0.5472202 0.17540638 -0.2585966 0.43663198 0.20470797 0.3148625 -0.12322025 0.07782403 -0.0071956795 0.32062578 -0.073577 -0.0021720217 0.21367705 0.051933236 -0.08923543 0.13515294 0.14857242 -0.17915854 -0.089753754 -0.8073507 -0.013222519 0.26132917 -0.31627253 0.03491712 -0.08807745 -0.43289968 -0.34600395 0.006036497 0.42262754 0.3993662 0.1541272 0.31577843 -0.43162018 -0.17809092 0.15705222 -0.27844748 -0.3186201 -0.16251084 -0.049989246 0.7400219 0.06492572 0.43115413 -0.762162 0.07277681 -0.21053186 -0.18948464 -0.28342965 0.25866288 -0.11119539 -0.24407941 0.6985138 0.20114635 0.58699495 -0.15650247 -0.3757383 -0.05168631 0.37163633 -0.2950018 0.041883595 -0.21173796 0.2747683 -0.5627793 0.41740438 0.12793195 -0.07024442 0.45193782 -0.25018588 0.5973738 0.27054897 0.056732293 0.012149459 0.33676484 0.43263704 0.10703354 0.2893141 0.60295475 0.4142759 0.31357735 -0.5501185 -0.023631657 -0.08692662 -0.023463596 0.053479027 0.3866174 0.2863894 -0.19931974 -0.48244536 0.23125945 0.079157636 0.45226175 0.5415162 0.13977809 0.27190807 0.22413167 -1.240523 -0.68788606 -0.44459143 0.17272423:-0.052219335 -0.14160728 -0.2348883 0.04280207 0.4859539 -0.2545 -0.14311829 0.048477013 0.013300183 0.19847202 -0.71941733 0.016545294 -0.4827476 -0.063179515 -0.056628987 0.34730506 0.0769612 0.11243106 0.54557437 -0.25090536 0.079569556 0.46673006 0.54004765 -0.19541475 -0.05483827 -0.08410155 0.24761076 -0.18149523 -0.25393075 0.7106602 0.2897435 0.037719887 -0.26336455 0.3252902 -0.032556243 0.36234587 -0.14896168 -0.18734676 -0.0640781 -0.48843962 -0.0433197 0.38365844 0.2170664 -0.5784084 0.23210548 -0.122419484 0.1694058 -0.5787888 -0.70071685 -0.6588095 -0.09924649 -0.1628271 -0.5419371 -0.022367373 -0.32347763 -0.0897705 -0.51267236 0.14277063 -0.04628085 0.67051584 0.26309338 0.5696572 -0.3063616 -0.26120293 0.0430199 -0.0210476 -0.5859663 -0.715663 -0.17900556 0.91180885 -0.21441615 0.34725368 -0.53535545 0.024459023 -0.019341733 0.104338065 -0.19047154 0.16562861 -0.02505434 -0.0731802 0.7100283 0.36498255 0.6147988 -0.027879562 -0.20688441 0.3236633 0.53069896 -0.035280876 0.3139095 -0.14742464 0.48162907 -0.5859071 0.2980421 -0.07247176 -0.22271112 0.5229622 -0.11565267 0.6421591 0.21056238 0.0016300855 -0.08399159 0.08076115 0.61661613 -0.10788114 0.19500199 0.24152811 0.43101624 0.5973427 -0.2414006 0.29556856 -0.19134131 0.17526665 0.7252588 -0.13729471 0.09420383 0.017514516 -0.047377143 0.32444137 0.5568211 0.39812732 0.36875406 0.033509914 -0.081343696 0.6792779 -0.27167058 -0.3903207 -0.27061313 0.033465996:-0.33087194 -0.3470167 -0.77805763 0.460283 0.23216054 0.0051871124 -0.15134181 0.43916318 -0.8454563 0.22148037 -0.50761753 -0.008201795 -0.112560384 0.21641709 -0.47246474 0.11363475 0.18730861 0.11800184 0.30686876 -8.51372e-05 0.119058855 -0.1467507 0.14938104 -0.40692157 0.19967131 -0.29283583 0.027495435 -0.0019207835 -0.28038764 0.160834 -0.017522557 -0.1381618 0.41033864 0.38393888 -0.17903432 -0.16571285 0.12198901 -0.1484423 -0.26020625 -0.2462144 -0.22688697 -0.12664488 0.11284797 -0.48934758 0.15389322 -0.04858724 0.38646707 -0.67104435 -0.011472313 -0.682007 -0.21024807 0.15619591 -0.1072945 -0.01595645 -0.28210962 0.05686635 -0.7780992 -0.045051064 0.43209013 0.4034524 0.35694316 0.53099245 -0.045644943 -0.21116441 0.14018108 0.31995898 -0.6984979 -0.83678055 -0.078228556 0.8165211 -0.016385445 0.29781502 -0.29268157 0.1795374 0.13561215 -0.060065538 0.17455466 0.22787316 0.25922546 0.21965262 0.9141303 0.40024388 0.55549407 0.069760226 -0.5952243 -0.09539138 0.29055765 -0.2396665 0.42516053 0.12273971 0.19586183 -0.8829817 -0.031008013 0.6216239 -0.23368941 0.6694383 0.114871405 0.9568328 0.32455713 -0.3444505 -0.2804663 0.7512698 0.20008232 -0.18716212 0.6161385 0.1523386 0.67123526 0.282821 -0.5058741 0.036775965 -0.09429034 -0.33108595 0.6091801 -0.1005177 -0.03256847 -0.41354248 0.21898936 0.32110205 0.39744934 0.1014866 0.50875443 0.12586018 0.38430452 0.5687229 -0.2279184 -0.4480241 -0.4568887 0.31076643\",\"22817144\",\"2\",\"20007\",\"8\",\"23\",\"932\",\"1\",0,0,5800,0,58,\"-0.042208724 0.068510205 -0.20845506 0.27718508 0.101768635 -0.14861417 -0.09050552 0.32909307 -0.6385669 0.3288197 -0.5611968 -0.08788351 -0.43029052 -0.16666187 -0.44880447 0.035421625 0.3242783 0.34647542 0.49112156 0.05110529 -0.035580352 0.78673714 0.41433015 0.046359267 0.27968958 -0.25632918 0.17009363 0.004125112 -0.58361953 0.52078414 0.17848702 0.12159419 0.17841259 0.33252648 -0.43631408 0.17447422 -0.04254762 -0.23342445 -0.43112376 -0.2759287 -0.38209996 0.45026737 0.29231325 -0.7853221 0.23309068 -0.09900266 0.34238565 -0.6366342 -0.42366764 -0.5552347 -0.020736523 -0.2417653 -0.5482967 -0.18638147 -0.089191794 0.029780773 -0.6376608 0.023690552 0.100459285 0.41216677 0.19889018 0.50037056 -0.052578237 -0.091489635 0.111478545 0.06391738 -0.738376 -0.70134526 -0.14114758 0.89636284 0.30259633 0.25052008 -0.5746624 0.053830903 0.2098058 0.12209325 0.069783606 0.12522987 0.25418022 -0.049363088 0.97343826 -0.011139113 0.4849296 -0.21026272 -0.34586766 0.09095552 0.10939563 -0.14308463 0.38391984 -0.13568756 0.03273488 -0.7211504 0.21498199 0.1420885 0.10119106 0.40072274 -0.0666339 0.59206676 0.11410267 -0.06494592 0.08180295 0.2906596 0.39010245 -0.24039607 0.284722 0.16128889 0.31234753 0.33049807 -0.19741608 0.1965221 -0.11941312 -0.060911514 0.41420877 0.15838252 0.37435168 -0.4966907 0.03245586 0.24176094 0.37970704 0.14668131 0.1690632 0.2679571 0.4115831 0.84663117 -0.6068748 -0.5399736 -0.588763 0.36734474\",\"0\"**```", "Request you to share test.csv file?. Thanks!", "> Request you to share test.csv file?. Thanks!\r\n\r\n\ud83d\ude14, csv file  has about 10G.\r\nThe result of one line is the same as the result of multiple lines, you can try it. thanks!\r\n\r\n![image](https://user-images.githubusercontent.com/30002208/78758650-09465d00-79b1-11ea-8950-f77627d906e4.png)\r\n![image](https://user-images.githubusercontent.com/30002208/78758667-119e9800-79b1-11ea-9c3d-e0fc4a573ac5.png)\r\n"]}, {"number": 38343, "title": "RuntimeError: Can't copy Tensor with type string to device", "body": "**System information** \r\n- \r\n- OS Platform and Distribution: Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.7\r\n- CUDA/cuDNN version: CUDA 10.2 \r\n- GPU model and memory: Tesla V100-SXM2-32GB \r\n\r\n\r\n\r\n**Describe the current behavior**\r\nRuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.\r\n\r\n**Describe the expected behavior**\r\nRuns perfectly on CPU\r\n\r\n**Standalone code to reproduce the issue** \r\n https://tfhub.dev/google/universal-sentence-encoder-large/5 **ON A GPU**.\r\n\r\n\r\n    import tensorflow_hub as hub\r\n\r\n    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\r\n\r\n    with tf.device('GPU:0'):\r\n    \r\n        embeddings = embed([\"The quick brown fox jumps over the lazy dog.\"])\r\n\r\n    print embeddings\r\n\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"create_sentence_embeddings.py\", line 25, in <module>\r\n    temp = embed(ele)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py\", line 438, in _call_attribute\r\n    return instance.__call__(*args, **kwargs)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 636, in _call\r\n    *args, **kwds)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2185, in canonicalize_function_inputs\r\n    self._flat_input_signature)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2240, in _convert_inputs_to_signature\r\n    value, dtype_hint=spec.dtype)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1302, in convert_to_tensor\r\n    value, dtype=preferred_dtype, name=name, as_ref=as_ref)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 317, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 258, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 266, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/kasaxen/anaconda3/envs/qaenv/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nRuntimeError: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.\r\n", "comments": ["@karansaxena,\r\nI was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/6d6771bfe50c85641669ea0f99bf3d3a/38343-2-1.ipynb). However, the issue seems to be fixed with [TF v2.2.0-rc2](https://colab.research.google.com/gist/amahendrakar/5ebf14eda7b2a936d60698664446c064/38343-2-2.ipynb). Please find the attached gist. Thanks!", "@amahendrakar yes, looks like TF v2.2.0-rc2 is working.\r\nIf I extend the list of sentences, I am consistently running into:\r\n     E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n     F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n     Aborted (core dumped)\r\n\r\nAlthough this looks like a CUDA/cudnn error, but do you happen to know?\r\n\r\n`nvcc --version` output:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n\r\ncudnn version: 7.6.5\r\n\r\n`nvidia-smi` gives CUDA version as 10.2", "@amahendrakar The CUDA_ERROR_ILLEGAL_ADDRESS does not come when I use the 'small' model from https://tfhub.dev/google/universal-sentence-encoder/4.\r\n", "> @amahendrakar The CUDA_ERROR_ILLEGAL_ADDRESS does not come when I use the 'small' model from https://tfhub.dev/google/universal-sentence-encoder/4.\r\n\r\n@karansaxena,\r\nRegarding this, could you please create a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and close this one, so that we can track it there. Thanks!", "Any updates regarding this issue? Thanks!", "Getting same error.  Also wondering if the following behavior happens with anyone else.\r\n\r\nUbuntu 18.04.4 LTS/CUDA 10.2, v10.2.89/Tesla V100-SXM2-16GB/Python 3.6.6/TF 2.1.0\r\n\r\nSurprisingly not only can I work around with\r\n        `with tf.device('/device:CPU:0'):`\r\ninstead of\r\n        `with tf.device('/device:GPU:0'):`  \r\nmy nvidia-smi shows usage of the GPU even though I called the CPU. The gpu usage is showing usage at a level that I would have expected had I explicitly called for use of the GPU.\r\n\r\nCould this be a problem with` tf.device()`? Or is ` tf.device()` just no longer needed?\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38343\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38343\">No</a>\n", "@pjherron,\r\nCould you please create a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track it there. Thanks!"]}, {"number": 38342, "title": "Generic opencl support by porting all cuda related parts to opencl", "body": "I wonder if it is possible to make **tensorflow support generic opencl devices (not just AMD Gpus) by porting cuda to opencl manually ?**\r\n**What are the challenges here (besides the huge maintance costs)?** \r\nCan we use tensorflow without Eigen library (i.e. **all the kernels in Eigen are preproduced with opencl kernels**) so that it will overcome the fact that opencl doesn't support C++.\r\nMaybe someone could help me figure them out, thanks :D", "comments": ["> What are the challenges here (besides the huge maintance costs)?\r\n\r\n\"Huge maintenance\" costs\" sounds like a pretty big challenge in itself. :)\r\n\r\nAll of what you're saying can be done, but we are unlikely to take such a large change to the TF repository unless it was very well motivated.", "I don't see an action item for us here so closing the issue for now."]}, {"number": 38341, "title": "RuntimeError: Encountered unresolved custom op: Dilation2D.Node number 2 (Dilation2D) failed to prepare", "body": "What can i do when i must use Dilation2D op on the tensorflowlite? Thinks.\r\n```\r\nRuntimeError: Encountered unresolved custom op: Dilation2D.Node number 2 (Dilation2D) failed to prepare. \r\n```", "comments": ["@sibadakesi \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\nplease share a simple standalone code to replicate the issue faced.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n ", "Thinks a lot,i chose another  operators to soloved this problem!", "@sibadakesi Could you, please, tell what operators do you use to replace Dilation2D?", "> Thinks a lot,i chose another operators to soloved this problem!\r\n\r\nWhich operators are you talking about?\r\nIf you're talking about adding `allow_custom_ops=True`, could you explain the workflow in the interpreter?"]}, {"number": 38340, "title": "How to control the cpu core?", "body": "I use tf pipline to augment the training data then training the model, but it's useless to set the tf.ConfigProto(device_count={\"CPU\":10},\r\n  inter_op_parallelism_threads=1,\r\n  intra_op_parallelism_threads=1,), all cpu core work as follow;\r\n![image](https://user-images.githubusercontent.com/28778038/78748612-0cd0e880-799f-11ea-8e61-02e30493d372.png)\r\n\r\nI expect to limit the cpu core which can not influence other tasks.\r\nwork platform:\r\nubuntu 18.04\r\n tf-gpu-1.9\r\nNVIDIA GTX1080ti\r\n\r\nAnyone have the solution?\r\n\r\n\r\n", "comments": ["Refer: <a href='https://github.com/tensorflow/tensorflow/issues/25446'>#25446</a>", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@mingqizhang \r\nAny update on this issue please.Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38339, "title": "Make keras model load compatible with old version of models", "body": "This PR makes `tf.keras.models.load_model(...)` compatible with the old versions of keras models (e.g. `tf-1.2.1`). More details about this issue can be found [here](https://github.com/tensorflow/tensorflow/issues/38135). Note that `tf-2.0` supports the models generated by `tf-1.2.1` but [recent changes](https://github.com/tensorflow/tensorflow/commit/c71c58beb262152d4b486eb92ff29b917584c201#diff-ca2f2579ed6d04b0be9c2bacfa1a4d38L224) in `tf-2.1` break it.\r\n\r\nFix #38135  ", "comments": ["Not sure if this is desired. Can't you just update your config?", "> Not sure if this is desired. Can't you just update your config?\r\n\r\nThanks for review, @tanzhenyu! \r\n\r\nWe can update our model to work with `tf-2.1`. However, I think it will save time for users if Keras API in `tf-2.x` can keep compatible with the old models generated by `tf-1.x`. Otherwise, there may be some other users who met similar issues with ours. \r\n\r\nNote that `tf-2.0` can load the above model file but [some recent changes](https://github.com/tensorflow/tensorflow/commit/c71c58beb262152d4b486eb92ff29b917584c201#diff-ca2f2579ed6d04b0be9c2bacfa1a4d38L224) in `tf-2.1` break it. I'm not sure if it is on purpose, or maybe forgot some edge cases. ", "> > Not sure if this is desired. Can't you just update your config?\r\n> \r\n> Thanks for review, @tanzhenyu!\r\n> \r\n> We can update our model to work with `tf-2.1`. However, I think it will save time for users if Keras API in `tf-2.x` can keep compatible with the old models generated by `tf-1.x`. Otherwise, there may be some other users who met similar issues with ours.\r\n> \r\n> Note that `tf-2.0` can load the above model file but [some recent changes](https://github.com/tensorflow/tensorflow/commit/c71c58beb262152d4b486eb92ff29b917584c201#diff-ca2f2579ed6d04b0be9c2bacfa1a4d38L224) in `tf-2.1` break it. I'm not sure if it is on purpose, or maybe forgot some edge cases.\r\n\r\nCan you be more specific on 1) what was the original config, 2) what error message does it provide, 3) what are alternative solutions towards this? And also a unit test would be desired as well.", "@tanzhenyu  The following code can be used to reproduce the issue:\r\n\r\n- Step 1: use `tf-1.2.1` to create an old model:\r\n```\r\ndef create_model_tf12(model_file):\r\n  from tensorflow.contrib.keras.python.keras.models import Sequential\r\n  from tensorflow.contrib.keras.python.keras.layers import Dense, Embedding\r\n  model = Sequential()\r\n  model.add(Embedding(1000, 64, input_length=10))\r\n  model.save(model_file)\r\n```\r\n- Step 2: use `tf-nightly` to load the model:\r\n```\r\ndef load_mode_tf_nightly(model_file):\r\n  model = tf.keras.models.load_model(model_file)\r\n```\r\n\r\n> Can you be more specific on 1) what was the original config, \r\n\r\nThe original config from `tf-1.2.1` is below:\r\n```\r\n{\r\n  'class_name': 'Sequential', \r\n  'config': [\r\n    {\r\n      'class_name': 'Embedding', \r\n      'config': {\r\n        'name': 'embedding_1', \r\n        'trainable': True, \r\n        'batch_input_shape': [None, 10], \r\n        'dtype': 'int32', \r\n        'input_dim': 1000, \r\n        'output_dim': 64, \r\n        'embeddings_initializer': {'class_name': 'RandomUniform', 'config': {'minval': 0, 'maxval': None, 'seed': None, 'dtype': 'float32'}}, \r\n        'embeddings_regularizer': None, \r\n        'activity_regularizer': None, \r\n        'embeddings_constraint': None, \r\n        'mask_zero': False, \r\n        'input_length': 10\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\nThe original config from `tf-nightly` using the python code is below:\r\n```\r\n{\r\n  'class_name': 'Sequential', \r\n  'config': {\r\n    'name': 'sequential', \r\n    'layers': [\r\n       {\r\n         'class_name': 'InputLayer', \r\n         'config': {\r\n           'batch_input_shape': [None, 10], \r\n           'dtype': 'float32', \r\n           'sparse': False, \r\n           'ragged': False, \r\n           'name': 'embedding_input'}\r\n       },\r\n       {\r\n         'class_name': 'Embedding', \r\n         'config': {\r\n           'name': 'embedding', \r\n           'trainable': True, \r\n           'batch_input_shape': [None, 10], \r\n           'dtype': 'float32', \r\n           'input_dim': 1000, \r\n           'output_dim': 64, \r\n           'embeddings_initializer': {'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}}, \r\n           'embeddings_regularizer': None, \r\n           'activity_regularizer': None, \r\n           'embeddings_constraint': None, \r\n           'mask_zero': False, \r\n           'input_length': 10}\r\n       }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\n> 2) what error message does it provide, \r\n\r\nThe error message is \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/feihu/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/feihu/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/feihu/.vscode-server/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/no_wheels/debugpy/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"/home/feihu/.vscode-server/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py\", line 429, in main\r\n    run()\r\n  File \"/home/feihu/.vscode-server/extensions/ms-python.python-2020.3.71659/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py\", line 266, in run_file\r\n    runpy.run_path(options.target, run_name=compat.force_str(\"__main__\"))\r\n  File \"/home/feihu/miniconda3/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/home/feihu/miniconda3/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/home/feihu/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"tf_explore/keras_load_model.py\", line 46, in <module>\r\n    load_mode_tf21(SEQ_MODEL_TF12_FILE)\r\n  File \"tf_explore/keras_load_model.py\", line 30, in load_mode_tf21\r\n    model = tf.keras.models.load_model(model_file)\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 184, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 170, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 173, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 340, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/feihu/py-virtualenv/tf-nightly/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 302, in class_and_config_for_serialized_keras_object\r\n    for key, item in cls_config.items():\r\nAttributeError: 'list' object has no attribute 'items'\r\n```\r\n\r\n> 3) what are alternative solutions towards this? \r\n\r\nDo you mean if there are any other solution to fix this issue?\r\n\r\n> And also a unit test would be desired as well.\r\n\r\nWill add a unit test soon.\r\n\r\n\r\n\r\n", "@tanzhenyu The unit test is added here(https://github.com/tensorflow/tensorflow/pull/38339/commits/49b07e664590f73942bcbaf1c378e59ccab9f04b). Could you please take a look when you have time?", "@tanzhenyu kindly remind that the comments have been addressed here (https://github.com/tensorflow/tensorflow/commit/49b07e664590f73942bcbaf1c378e59ccab9f04b). Could you take a look when you get a chance? Hope this PR can be merged before the official release of `TF-2.2`.", "@k-w-w Could you please take a look at this PR when you get a chance?\r\n"]}, {"number": 38338, "title": "TFLite BATCH_MATMUL should be BATCH_MAT_MUL", "body": "**Describe the current behavior**\r\n\r\nTensorFlow Lite added a new operator `BuiltinOptions.BATCH_MATMUL` which translates to `BatchMatmul` but the corresponding options are named `BatchMatMul`.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe operator should be called `BuiltinOptions.BATCH_MAT_MUL`.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/828fe43cf3aaf1a60ed0b263d4af4b3442e79121#r38357271\r\n\r\n@talumbau", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38338\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38338\">No</a>\n"]}, {"number": 38337, "title": "mobilenet_v2_140_224/classification: Cannot copy between a TensorFlowLite tensor with shape [1, 120] and a Java object with shape [1, 10, 4]", "body": "**System information** \r\n- Windows 10 10.0.18362 Build 18362\r\n- TensorFlow 2.2.0 installed from pip \r\n- Python v3.8.2\r\n\r\n**Describe the current behavior**\r\nI used the make_image_classifier (https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) to generate the TFLITE model based on images available on http://vision.stanford.edu/aditya86/ImageNetDogs/:\r\n```\r\nmake_image_classifier --image_dir images \\\r\n--tfhub_module https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2 \\\r\n--image_size 224 \\\r\n--saved_model_dir models \\\r\n--labels_output_file output/labels.txt \\\r\n--tflite_output_file output/retrained.tflite\r\n```\r\n\r\nThen I copied the .tflite file to my Android project assets folder and tried to execute the app, but it crashed with the following error:\r\n\r\n ```\r\nProcess: org.tensorflow.lite.examples.detection, PID: 9525\r\n    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 120] and a Java object with shape [1, 10, 4].\r\n        at org.tensorflow.lite.Tensor.throwIfShapeIsIncompatible(Tensor.java:412)\r\n        at org.tensorflow.lite.Tensor.throwIfDataIsIncompatible(Tensor.java:369)\r\n        at org.tensorflow.lite.Tensor.copyTo(Tensor.java:247)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:166)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:195)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n        at android.os.Handler.handleCallback(Handler.java:873)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:193)\r\n        at android.os.HandlerThread.run(HandlerThread.java:65)\r\n```\r\nI looked into almost similar issues, but none of them had the TensorFlowLite with a shape based on two parameters (i.e. [1, 120]) while having the Java object with a shape based on three (i.e. [1, 10, 4]).\r\n  \r\nmake_image_classifier: https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier\r\n\r\n\r\n**Describe the expected behavior**\r\nThe app should start with no problems and detect/classify objects based on the provided TFLITE model.\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://github.com/LavitzBr/object_detection\r\n", "comments": ["It looks like you're copying an image classification model to the object detection app, and that leads to the incompability issue.", "@LavitzBr 1. Is this still an issue for you? \r\n2.Did you check @xunkai55 response? \r\n3.Can you please share a standalone code to reproduce the issue? Thanks!", "@xunkai55, I'm following the correct commands for the model to be compatible with the code that calls it in the app. Do you have a clue on why the model not being correctly generated? ", "@LavitzBr Based on the contents in the original post, it looks like you got an \"Image Classification\" model (mobilenet_v2_140_224) from TFHub. That model is used for predict the category of images. However, your app is the \"Object Detection\" app. An \"image classification\" model cannot be used for \"object detection\" without subtle modifications.\r\n\r\nIf you want an Image classification app, please check out https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android rather than the Object Detection app.\r\n\r\nIf you want an Object Detection app, you may want a \"single-shot-detection\" model, like \"MobileNetV1-SSD\" we used in the example app. https://tfhub.dev/tensorflow/lite-model/ssd_mobilenet_v1/1/default/1 ", "@wilerjrxd It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest Version 2.5 or 2.4.1 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38337\">No</a>\n"]}, {"number": 38336, "title": "TFLite Failed to build gpu delegate on Linux", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?: docker (linux env)\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source):clang version 7.0.0-3~ubuntu0.18.04.1\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build tensorflow lite gpu delegate on linux (docker inside imac) but I cannot build using bazel when compiling the gl_delegate. This also happens on v2.2.0rc2 but using bazel 2.0.0 and modifying the configure file to prevent errors from running.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. clone tensorflow\r\n2. checkout v2.1.0 branch\r\n3. install all the python, pip dependenies\r\n4. apt get install `mesa-common-dev libgl1-mesa-dev libgles2-mesa-dev ocl-icd-opencl-dev`\r\n5. run ./configure, press enter to each (so use all default commands), python points to /usr/bin/python etc\r\n6. install `bazel 0.29.1`\r\n7. Run `bazel build tensorflow/lite/delegates/gpu:gl_delegate`\r\n\r\nThen it fails with \r\n\r\n```\r\nIn file included from tensorflow/lite/delegates/gpu/gl/egl_context.cc:21:\r\n./tensorflow/lite/delegates/gpu/gl/gl_call.h:60:29: error: no viable conversion from returned value of type 'tflite::gpu::Status' to function return type 'int'\r\n    if (status.ok()) return OkStatus();\r\n                            ^~~~~~~~~~\r\n./tensorflow/lite/delegates/gpu/gl/gl_call.h:73:29: error: no viable conversion from returned value of type 'tflite::gpu::Status' to function return type 'int'\r\n    if (status.ok()) return OkStatus();\r\n                            ^~~~~~~~~~\r\ntensorflow/lite/delegates/gpu/gl/egl_context.cc:33:3: error: member reference base type 'const int' is not a structure or union\r\n  RETURN_IF_ERROR(GetOpenGlErrors());\r\n  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n./tensorflow/lite/delegates/gpu/common/status.h:68:17: note: expanded from macro 'RETURN_IF_ERROR'\r\n    if (!status2.ok()) return status2; \\\r\n         ~~~~~~~^~~\r\n```\r\nAnd a lot more of this. \r\nNot sure why it tries to convert the Status into an int. Maybe i am targeting the wrong build target?", "comments": ["Have there been any updates on this?", "I'm getting the exact same thing :(", "I have sucessfully built gpu delagate on linux using:\r\n```\r\nbazel build -c opt --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 tensorflow/lite/delegates/gpu:gl_delegate\r\n```\r\nhttps://github.com/tensorflow/tensorflow/issues/28830#issuecomment-619179751", "Can confirm that the command works. Thanks.\r\n\r\nI also needed to install a few packages: \r\nlibegl1-mesa-dev\r\nlibgles2-mesa-dev\r\n\r\nIs there a way to use the delegate in linux? I don't think we can use\r\n\r\n`tflite::evaluation::CreateGPUDelegate`", "@matthewn4444 \r\nplease update if this is still an issue, could you please try on the latest tf version and update.", "I will take a look and answer soon", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I was able to build the delegate using the command before\r\n\r\n`bazel build -c opt --copt -DMESA_EGL_NO_X11_HEADERS --copt -DEGL_NO_X11 tensorflow/lite/delegates/gpu:gl_delegate`\r\n\r\nOn ubuntu using tf 2.3\r\n\r\nand was able to build the delegate with \r\n\r\n```\r\n#include <tensorflow/lite/delegates/gpu/delegate.h>\r\n\r\nauto  _gpuDelegate = tflite::Interpreter::TfLiteDelegatePtr(TfLiteGpuDelegateV2Create(nullptr),\r\n                                                          &TfLiteGpuDelegateV2Delete);\r\n    if (!_gpuDelegate) {\r\n        return error\r\n    }\r\n    if (_interpreter->ModifyGraphWithDelegate(_gpuDelegate.get()) != kTfLiteOk) {\r\n            return error\r\n    }\r\nreturn no_error\r\n\r\n```\r\n\r\nI tried to link the executable but it fails to build because it is finding missing links to std, such as\r\n\r\n`wait.cc:(.text+0x61): undefined reference to `std::chrono::_V2::steady_clock::now()'`\r\n\r\nI assume the delegate didn't build correctly?\r\n\r\n", "@matthewn4444,\r\n\r\nCan you try updating to latest stable version of tensorflow `2.6.0` and let us know if the issue still persists? Thanks!", "Actually I solved this a while ago. I was able to run after compiling tensorflow with custom clang 7 toolchain in linux because our system linking tensorflow was compiling with clang 7 while tensorflow by default was compiling with gcc and the standard library was mis-matched. Compiling with the same clang the standard library was correctly detected and ran.", "@matthewn4444,\r\n\r\nThanks for the update, Can you confirm if we are good to close this issue as the problem is solved. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38336\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38336\">No</a>\n"]}, {"number": 38335, "title": "tflite Interpreter causes iOS app to crash", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Following this tutorial for tflite with C++: https://www.tensorflow.org/lite/guide/inference\r\n\r\n- OS Platform and Distribution: MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: iPhone XS Max\r\n- TensorFlow installed from (binary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: - Bazel \r\nversion (if compiling from source): bazel 1.1.0\r\n- GCC/Compiler version (if compiling from\r\nsource): 4.2.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen using tflite with Mediapipe for hand tracking, building the interpreter causes the app to crash after being built and opened. \r\n\r\n**Steps to reproduce behavior**\r\n\r\n1. Need to clone our project github repo: https://github.com/sne21star/mediapipe\r\n2. Try building the iOS app from command line using this command: bazel build --config=ios_arm64 mediapipe/examples/ios/handtrackinggpu:HandTrackingGpuApp\r\n3. Download to iOS and try out app, will crash due to these lines in hand_gesture_recognition_calculator.cc\r\n\r\n//  Build the interpreter\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    tflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\nWithout these lines, app runs fine (same as their out of box hand-tracking example).\r\n\r\n**Describe the expected behavior**\r\n\r\nBuilding the desktop app runs fine and classifies the ASL letters, so expect it to work the same on iOS.\r\n", "comments": ["Hi @ishaghodgaonkar.\r\nIs this still on issue for you? I tried to follow your repro steps, but the `bazel build` command was failing on my side due to syntax errors in the `WORKSPACE` file in your repo. Looks like some merge conflicts were not correctly resolved in that file. Could you please take a look?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38335\">No</a>\n"]}, {"number": 38334, "title": "Add `socketpair.c` to curl buildable files to fix Windows builds.", "body": "Follow-up from bfb0e49d5844d835ab757a1709a1bcfc216d78f8\r\n\r\nPiperOrigin-RevId: 305351839\r\nChange-Id: Ic7a8b4942394d6d030e93b3ad9179e0bffdc434c", "comments": []}, {"number": 38333, "title": "r2.2 systemlibs cherry-picks", "body": "These are cherry-picks for the r2.2 release branch. It fixes several issues for building against system libraries. I've pushed these patches to the Gentoo package already so they have some testing but it'd be good to get them merged for the release as well for other distros too.\r\n\r\nThe commits are in master in PRs:\r\nhttps://github.com/tensorflow/tensorflow/pull/38326\r\nhttps://github.com/tensorflow/tensorflow/pull/38327\r\nhttps://github.com/tensorflow/tensorflow/pull/38328\r\n\r\n@angerson @mihaimaruseac ", "comments": ["@perfinion @mihaimaruseac presubmits are failing, could you take a look ?", "Yes, the pybind part of the cherry-pick needs slightly more work. Let's wait until we can merge the 3 PRs into master", "@goldiegadde I dropped the jsoncpp patch since im having issues getting kokoro to pass. the other patches are in master now :)"]}, {"number": 38332, "title": "Update NCCL version to the latest release v2.6.4-1", "body": "**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCould you please update NCCL used within TensorFlow building to the latest release v2.6.4-1: https://github.com/NVIDIA/nccl/releases/tag/v2.6.4-1\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?**\r\nNeeded to enable new NCCL features (in particular network collectives).\r\n\r\n**Any Other info.**\r\n", "comments": ["@artemry-nv,\r\nIs this still an issue?\r\n\r\nPlease feel free to close the issue if resolved. Thanks!", "What is the current NCCL version used for TF building?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38332\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38332\">No</a>\n"]}, {"number": 38331, "title": "Model.fit doc fixes", "body": "", "comments": []}, {"number": 38330, "title": "[r2.2:CherryPick] increase the macos 3.8 pip build timeout, disable a failing test.", "body": "PiperOrigin-RevId: 305313616\nChange-Id: I2c34f72b14962181032d5e080183ee3a064d58af", "comments": []}, {"number": 38329, "title": "Added _estimator_type property for KerasClassifier and KerasRegressor\u2026", "body": "\u2026 classes\r\n\r\nIn order to use VotingClassifier and VotingRegressor (and probably many others) sklearn classes, the property _estimator_type must be defined as \"classifier\" and \"regressor\" respectively. Sklearn classes normally inherit this property from ClassifierMixin and RegressorMixin", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38329) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it! ", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38329) for more info**.\n\n<!-- ok -->"]}, {"number": 38328, "title": "systemlibs: unbundle pybind11", "body": "If pybind11 is installed on the system its headers are already captured\r\nby @local_config_python//:python_headers, so the system lib only needs\r\nto depend on that.\r\n\r\nWhen installed correctly, includes should be #include \"pybind11/...\",\r\nthe bundled pybind11 is based off the source repo which does not match\r\nthe install paths. Use bazels strip_include_prefix to align the bundled\r\nheaders correctly.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nSimilar to https://github.com/tensorflow/tensorflow/pull/38327 this fixes up canonical include paths. Most of this change was done with: \r\n`sed -i '/#include.*include.pybind/s/include\\///' $(git grep -l \"\\#include.*include.pybind\")`", "comments": []}, {"number": 38327, "title": "systemlibs: jsoncpp: fix include path", "body": "The path to jsoncpp when installed on the system are\r\njsoncpp/json/json.h, the bundled jsoncpp starts with include/.\r\nUse bazel's include_prefix and strip_include_prefix to make the\r\nbundled use the correct paths.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>\r\n\r\nThis was discussed in the TF SIG-Build meeting last month. most of the change was done with\r\n`sed -i 's@include/json/json.h@jsoncpp/json/json.h@' $(git grep -l \"\\#include.*include.json.json\")`", "comments": ["@perfinion Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]