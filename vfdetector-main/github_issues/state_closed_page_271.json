[{"number": 46261, "title": "different TFLM builds use the same output directory.", "body": "@tensorflow/micro\r\n\r\nIn https://github.com/tensorflow/tensorflow/pull/46242#discussion_r553049656, I was suggesting that the linker was not correctly dropping unused symbols.\r\n\r\nIn fact, what was very likely happening was that I did not do a `make clean` between switching to `BUILD_TYPE=release`. And since the TFLM makefile currently uses the same directory for all `BUILD_TYPE`, only the modified files were being rebuilt with the smaller `release` build.\r\n\r\nWe can reproduce this with the following sequence of commands:\r\n\r\nFirst check what the binary size is for the release build.\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark \r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  46080\t  40204\t  24952\t 111236\t  1b284\ttensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n```\r\n\r\nNext have some intermediate non-release objects and then do a release build:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile clean\r\n\r\n# build non-release\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark\r\n\r\ntouch tensorflow/lite/micro/kernels/xtensa/fully_connected.cc\r\n\r\n#build for release\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark \r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  54736\t  48168\t  25032\t 127936\t  1f3c0\ttensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark\r\n```\r\n\r\nWhat we really should be doing is to change the output directory based on the build type.", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46261\">No</a>\n"]}, {"number": 46260, "title": "Refactor case BuiltinOperator_EXPAND_DIMS in flatbuffer_conversions", "body": "PR1 for issue #46258", "comments": ["@rsun-bdti can you please rebase to head", "@rthadur Rebased to head."]}, {"number": 46259, "title": "[ROCm] Misc XLA updates for the ROCm platform - 210107", "body": "Some minor XLA related updates for the ROCm platform\r\n\r\ncopy-pasting the individual commit messages here for convenience\r\n\r\n*  Relaxing the rtol for some fp16 subtests within dot_operation_test.cc\r\nThe rtol is being given a minor bump from 5e-3 to 7e-3 (for fp16 only) to let some of the subtests pass, that were failing on the ROCm platform, because of the tighter rtol\r\n\r\n*  re-enabling a subtest withing image_ops_test.py, because it no longer fails on the ROCm platform\r\n\r\n*  Re-enabling the XLA convolution_test on the ROCm platform, after disabling a couple of failing subtests.\r\n\r\n---------------------------------------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n\r\n", "comments": []}, {"number": 46258, "title": "Micro: port op EXPAND_DIMS from Lite", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): source\r\nTensorflow version (commit SHA if source): master\r\nTarget platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge\r\n\r\n**Describe the problem**\r\nI am about to port The TF Lite kernel op EXPAND_DIMS to TF Lite Micro.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nPR 1: refactor flatbuffer_conversions parsing function\r\nPR 2: refactor reference implementation from lite/kernels/internal/reference/reference_ops.h into its own header without making any changes\r\nPR 3: copy the EXPAND_DIMS kernel from lite to micro without making any changes. At this point the kernel is in micro but it is not part of the build\r\nPR4: make the EXPAND_DIMS micro kernel fully functional. Also finish the testing code.\r\n\r\n(Update 2021-02-26: There is no function for EXPAND_DIMS in lite/kernels/internal/reference/reference_ops.h, and the TFLM op does not depend on a reference implementation, so PR 2 was skipped) \r\n(Update 2021-02-26: Added PR4 in the sequence)\r\n", "comments": ["Hey @rsun-bdti, how is it going with the PR2? Am I wrong or is there no header available in reference_ops.h?", "@stephanboner, good catch. There is no function for EXPAND_DIMS in lite/kernels/internal/reference/reference_ops.h, and the TFLM op does not depend on a reference implementation, so PR 2 was skipped. I have updated as such in the header above.", "@rsun-bdti cool, thanks for updating. But does it build for you then and can you use the operator? I was struggeling using it today...", "@stephanboner: Are you trying to use the Lite kernel EXPAND_DIMS, or the micro kernel? The latter is still under review and has not been merged into the master yet.", "@rsun-bdti I'm trying to use it in micro and I implemented both your PRs in my projects tensorflow source", "@stephanboner  I see. After PR3, the micro kernel is not fully functional, nor is it part of the build. There is a PR4 pending for review right now. Hopefully you can use it when that PR is approved and merged into the master. As it is, this micro kernel can not be used yet. That's why this issue is still open.\r\n\r\nI updated the header above with PR4. ", "Oh thanks @rsun-bdti. Sorry for causing such a discussion, I just copied the lite kernel instead of taking the adjusted version of your PR4 as well as added the op in `micro_ops.h` in the wrong namespace. Friday afternoon hustle... Thank you! \u270c\ufe0f ", "@stephanboner  My pleasure. The discussion is helpful to me as well. I realize that I need to keep the information for my issues updated. (PS: thanks for approving the PR4) "]}, {"number": 46257, "title": "[XLA] dense_layer_test.py throws internal error in fallback path", "body": "The failure happens in Master as well as r2.4 (these are the 2 branches that I've tested).\r\n\r\nEven with lazy_compilation turned on (via `TF_XLA_FLAGS=--tf_xla_enable_lazy_compilation=true`), the first execution always compiles as per the current implementation. If we tweak this behaviour such that the first execution doesn't compile (and uses the fallback path) or use `--tf_xla_always_defer_compilation=true` to force the fallback path, the test `tensorflow/compiler/tests/dense_layer_test.py` fails with the following signature\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node cluster_3}} {{function_node cluster_3}} Trying to assign variable with wrong dtype. Expected INVALID got float\r\n     [[{{node dense/kernel/Assign}}]]\r\n     [[cluster_3_1/partitioned_call]]\r\n```\r\n\r\nThere are 3 test points in the test. Based on the descriptions, they test that the dense layer node is properly compiled in jit scope. I am not sure if this test is supposed to be used for the fallback path. However, the failure is not merely a test failure but an internal error (`Trying to assign variable with wrong dtype. Expected INVALID got float`) which leads me to think there might be a bug. \r\n\r\nThe error comes from handling of resource variables https://github.com/tensorflow/tensorflow/blob/13d37279f137a96ca6fa5142f20c8747103bf79e/tensorflow/core/kernels/resource_variable_ops.cc#L397-L401", "comments": ["Hi @AyanmoI \r\n\r\nI was not able to reproduce this issue as:\r\n\r\n```\r\n# Git SHA1: 5534f5d320818aad52aac35af193b88b7222d46b\r\n\r\nbazel test -c opt --nodistinct_host_configuration --config=cuda //tensorflow/compiler/tests:dense_layer_test_gpu --test_env=TF_XLA_FLAGS=--tf_xla_always_defer_compilation=true\r\n```\r\n\r\nCan you please share the exact run command and the commit at which you're running so that we're not talking past each other?\r\n\r\nThanks!", "Hey @sanjoy ... that is pretty much the repro I used. However, it seems that simply using  `--tf_xla_always_defer_compilation=true` doesn't reproduce this error. I believe there might still be an issue in the interaction of  resource variable and partioned_call but it cannot be exposed in this fashion. \r\nI'll see if there is a better repro and address this then. For now I am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46257\">No</a>\n"]}, {"number": 46256, "title": "Copy lite/kernels/zeros_like.cc to lite/micro/kernels/zeros_like.cc", "body": "PR3 for issue #46049 (kernel ZEROS_LIKE has no reference op implementation, so we are skipping PR2: refactor reference_ops.h)", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Rebase to HEAD 2021-0120 as this branch was 10 days behind."]}, {"number": 46253, "title": "Saved weights as checkpoints are randomised on loading them in the model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, i have a custom U-Net model with conv1d and custom loss function for ignoring the ignore labels\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2021-01-07 16:16:22.597961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nv2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe checkpoints (save_weights) after successfully training the model randomise after loading them into model. The accuracy and precision plummet drastically on the same data which showed 99+ % accuracy while training.\r\n**Describe the expected behavior**\r\nThe checkpoints must show the same results after loading, as while the training.\r\n\r\n**Standalone code to reproduce the issue**\r\ninput_layer = l.Input((seq_len, seq_dep), name='seq')\r\n    model_unet_1d = get_unet(input_layer, labels-1, 16, dropout=0.05, batchnorm=True)\r\n    model_unet_1d.compile(optimizer=Adam(), loss=custom_loss, metrics=[custom_metrics])\r\n    \r\n    print(\"<<<< Model Compiled >>>>\")\r\n    \r\n    callbacks = [\r\n    EarlyStopping(patience=10, verbose=1),\r\n    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\r\n    ModelCheckpoint('model_chkp/model-ctc_4tiles_bce_sam.h5', verbose=1, save_best_only=True, save_weights_only=True)\r\n    ]\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@qaimHassan \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in debugging faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46253\">No</a>\n"]}, {"number": 46252, "title": "TensorBoard callback with update_freq='batch' fails when fitting multiple times", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\nYes. The bug happens in the minimal example in the TensorFlow documentation, with a small tweak.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Mojave 10.14.6\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: Python 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nIf we add a callback for TensorBoard when fitting a Keras Model (say `model`) as in `callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs', update_freq='batch')]`,\r\n\r\ncalling the function `model.fit` twice causes an error (see logs below)\r\n\r\n**Describe the expected behavior**\r\n\r\nIdeally we would be able to call `model.fit` a many times as we want. If we don't use the `update_freq` parameter, it does not give an error and behaves as expected.\r\n\r\n**Standalone code to reproduce the issue**\r\nAny model will do. The one in the tensorflow docs tutorial, for example:\r\n\r\n```bash\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dropout(0.2),\r\n    tf.keras.layers.Dense(10)\r\n])\r\n\r\ncallbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs', update_freq='batch')]\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, callbacks=callbacks)\r\nmodel.fit(x_train, y_train, callbacks=callbacks)\r\n```\r\n\r\n**Other info / logs** \r\n\r\nLog of the error:\r\n\r\n```bash\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.4814 - accuracy: 0.8604\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-24-6d870932cb3b> in <module>\r\n     17 \r\n     18 model.fit(x_train, y_train, callbacks=callbacks)\r\n---> 19 model.fit(x_train, y_train, callbacks=callbacks)\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1098                 _r=1):\r\n   1099               callbacks.on_train_batch_begin(step)\r\n-> 1100               tmp_logs = self.train_function(iterator)\r\n   1101               if data_handler.should_sync:\r\n   1102                 context.async_wait()\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    826     tracing_count = self.experimental_get_tracing_count()\r\n    827     with trace.Trace(self._name) as tm:\r\n--> 828       result = self._call(*args, **kwds)\r\n    829       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    830       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    853       # In this case we have created variables on the first call, so we run the\r\n    854       # defunned version which is guaranteed to never create variables.\r\n--> 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    856     elif self._stateful_fn is not None:\r\n    857       # Release the lock early so that multiple threads can perform the call\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2940       (graph_function,\r\n   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\r\n-> 2942     return graph_function._call_flat(\r\n   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n   2944 \r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1916         and executing_eagerly):\r\n   1917       # No tape is watching; skip to running the function.\r\n-> 1918       return self._build_call_outputs(self._inference_function.call(\r\n   1919           ctx, args, cancellation_manager=cancellation_manager))\r\n   1920     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    553       with _InterpolateFunctionError(self):\r\n    554         if cancellation_manager is None:\r\n--> 555           outputs = execute.execute(\r\n    556               str(self.signature.name),\r\n    557               num_outputs=self._num_outputs,\r\n\r\n~/WellcomeML/build/virtualenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nNotFoundError:  Resource localhost/_AnonymousVar1925/N10tensorflow22SummaryWriterInterfaceE does not exist.\r\n\t [[{{node cond/then/_0/batch_loss}}]] [Op:__inference_train_function_108360]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1f9c5a3d464fa88f88f12d111499b5c4/46252.ipynb#scrollTo=nPStrYH4k6-R). Thanks!", "@aCampello,\r\nIt is resulting in `error` but can you please give some Real World Use Cases where and why we execute **`model.fit`** immediately after another **`model.fit`** command because, executing the entire cell, with a single **`model.fit`** command any number of times is not resulting in error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/653a49aba124e384db87cfa11c501111/46252.ipynb) of working code.\r\n\r\nThanks!", "Hi @rmothukuru. Thanks for helping with this!\r\n\r\nThis is a made up example, but there are a number of real cases where you would like to fit multiple times. For instance, if you are working on a Jupyter notebook, you might want to run `.fit` for a couple of epochs, do something else, then run `.fit` for a couple more, etc, especially when you are iterating your model/try to define your architecture.\r\n\r\nPartial fits are quite useful in the real world, to be fair. This actually showed up in a real project when I was doing just what I described above.", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/46ea70c9ddbfd5e0298ae3d445d8bb6f/untitled163.ipynb?authuser=1)..Thanks !", "This still appears to be an issue at least in `2.7.0`.\r\n\r\nAnother use case to consider is for [distributed evaluation in the Parameter Server Strategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training#inline_evaluation). Basically if you model is too big to fit on a single machine you need to manually do an eval loop and if you want to do this multiple times during training you'll have to call `fit` multiple times. ", "@aCampello I was able to run your code successfully  on colab using TF v2.8.0  .Could you please have a look at the gist [here](https://colab.research.google.com/gist/sushreebarsa/e1cdb14dfe451f245b979bf89ae3e9f0/46252.ipynb) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46252\">No</a>\n"]}, {"number": 46251, "title": "Getting TypeError: when using label smoothing in Categorical Cross entropy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):2.3.1\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version Colab default \r\n- GPU model and memory:V100 SXM2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nI am using label smoothing with Categorical Cross entropy. When I turn off label smoothing the model works fine but when I turn it The model gives the following error\r\n`    TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n`\r\nI am not able to understand what is the problem with my code?. Here is My code\r\n\r\nhttps://drive.google.com/file/d/1lEVPMOtWOWesDmpRDgTJQPTFOQ6UTI2o/view?usp=sharing\r\n\r\nI have tried setting the dtype to float16 in Image Data Generator. But that also does not work. Can somebody Help me? I have not got any answers on stack overflow also\r\n\r\n", "comments": ["@king398 \r\nI ran the code shared and face a different error, please share all dependencies for us to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e70d7393aa8cb66747b5686317d9eb69/untitled.ipynb).", "I think you don`t have the right dataset, try this dataset I am using https://www.kaggle.com/c/cassava-leaf-disease-classification/data and tensorflow version 2.3.1", "@Saduf2019  please respond ", "@king398 \r\nI am unable to download dataset from the mentioned link, can you please directly attach sample dataset to replicate the issue faced.", "@Saduf2019 Ok so i fixed the error by changing \r\n`tf.keras.layers.Dense(5, activation='softmax')`\r\nto\r\n`tf.keras.layers.Dense(5, activation='softmax', dtype='float32')`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46251\">No</a>\n"]}, {"number": 46250, "title": "ci: create issue labeled reply", "body": "ref: https://github.com/tensorflow/tensorflow/issues/46249 https://github.com/tensorflow/tensorflow/issues/46230\r\n\r\nWhen you add `stat:awaiting response` label, the GitHub Actions will help you comment this.", "comments": ["I don't see the link between this PR and the linked issues.\r\n\r\nAlso, what feature does this bring?", "This is not directly related to the issue. This is a GitHub Actios. It can help you deal with the issue.\r\n\r\nWhen you add stat:awaiting response label, the GitHub Actions will help you comment this.\r\n\r\n", "It won't bring anything new, still requires manual effort."]}, {"number": 46249, "title": "TypeError: '<' not supported between instances of 'function' and 'str'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Ubuntu)\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.6.9\r\n\r\n**Code**\r\nI've save a model using callbacks:\r\n\r\n```python\r\nmc = tf.keras.callbacks.ModelCheckpoint( filepath=save_directory + '/trial10_model.h5',\r\n                                        monitor=\"val_loss\",\r\n                                        save_freq=\"epoch\",\r\n                                        save_best_only=True,\r\n                                        save_weights_only=False)\r\ncallback_list = [mc]\r\n\r\nhistory = model.fit_generator(get_train_set_,validation_data = get_val_set_, validation_steps=validation_steps,\r\n                              steps_per_epoch = steps_per_epoch,epochs=15, callbacks=callback_list)\r\n```\r\n\r\nWhile testing the model using evaluate or evaluate_generator:\r\n```python\r\nmodel = load_model(\"/content/gdrive/MyDrive/keras_tuner/good_trail/trial10_model.h5\", custom_objects={ 'dice_coef': dice_coef })\r\nres = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1)\r\n```\r\n**Error Message**\r\nI get this error message:\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-56-3ae064222286> in <module>()\r\n     12 callback_list = [mc]\r\n     13 \r\n---> 14 res = model.evaluate(get_test_set_,steps=test_steps_per_epoch, verbose=1, callbacks=callback_list)\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise\r\n\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1219 run_step  **\r\n        with ops.control_dependencies(_minimum_control_deps(outputs)):\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2793 _minimum_control_deps\r\n        outputs = nest.flatten(outputs, expand_composites=True)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:341 flatten\r\n        return _pywrap_utils.Flatten(structure, expand_composites)\r\n\r\n    TypeError: '<' not supported between instances of 'function' and 'str'\r\n```", "comments": ["@abdallah1097 \r\n\r\nCan you please share colab link or simple standalone code with supporting files(`model.h5`) to reproduce the issue in our environment.It helps us in debugging faster. Thanks!", "try switching to tf 2.3... or try saving configurations and weights separately.... had exactly the same problem and these hacks worked for me.", "Downgrading to tf2.3 did not work for me, and isn't an acceptable solution due to the lack of support for CUDA 11 and CUDNN 8. However, saving the model as weights and then loading as weights worked (as opposed to loading the entire model). This behaviour can be found in Intel Unet. https://github.com/IntelAI/unet/tree/master/3D\r\nSimply run `python train.py --data_path $DECATHLON_ROOT_DIRECTORY --epochs=1` with tf 2.4. I was unable to reproduce this with a minimal example. I assume the use or loading of custom loss/metrics in both of our examples is causing this. Thanks.\r\n\r\nEdit: **System Information**\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n* TensorFlow version (use command below): 2.4.0\r\n* Python version: 3.8.5\r\n* CUDA/cuDNN version: 11.1/8.0.4\r\n* GPU model and memory: RTX3090 24gb", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@abdallah1097 \r\n\r\nPlease go through below link\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/save_and_load\r\n\r\nTry to save the model weights, rather than saving the whole model and then evaluated using the latest weights on a new model instance. See it works?\r\n\r\nThanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46249\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46249\">No</a>\n", "@ravikyram is there a fix for someone who doesn't want to save the weights separately (because for example they want to also save the optimizer's state)?\r\n\r\nEDIT\r\n----\r\nThere is always the possibility to manually save the weights of course (like laid out in [this SO answer](https://stackoverflow.com/a/49504376/4332585)), but it's a bit tedious and I guess error-prone.", "I had the same issue, solved it by compiling the model again after loading:\r\n\r\n```\r\nmodel = load_model(...)\r\nmodel.compile(...)\r\nmodel.evaluate(...)\r\n```", "@LaurinHerbsthofer your solution definitely works when only evaluating the model afterwards and not resuming training.\r\n\r\nif you want to resume training however the problem still stands because you cannot compile the model again: it will get rid of the optimizer\u2019s state.", "See this: https://github.com/tensorflow/tensorflow/issues/45903#issuecomment-804973541"]}, {"number": 46248, "title": "Tensorboard callback not logging learning rate when not using LearningRateSchedule (e.g. when using ReduceLROnPlateau)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip3\r\n- TensorFlow version: tf-nightly 2.5.0-dev20210104\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 11.0.3 / 8.0.4\r\n- GPU model and memory: Quadro RTX 5000 / 16384 MB\r\n\r\n**Describe the current behavior**  \r\nIf there is no LearningRateSchedule (either LR is constant or is changed via callback) then [Tensorboard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) doesn't log learning rate. This is especially relevant when using [ReduceLROnPlateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau) callback.\r\n\r\n**Describe the expected behavior**  \r\nLearning rate should be always logged or at least in the case of using a callback.\r\n\r\n**Standalone code to reproduce the issue**  \r\n```\r\nimport tensorflow as tf\r\nimport datetime\r\nimport os\r\n\r\nclass SimpleModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(SimpleModel, self).__init__()\r\n        self.Dense_in = tf.keras.layers.Dense(units = 50, \r\n                                activation=tf.nn.relu, \r\n                                use_bias=True)\r\n        self.Dense_hidden = tf.keras.layers.Dense(units = 20, \r\n                                activation=tf.nn.relu, \r\n                                use_bias=True)\r\n        self.Dense_out = tf.keras.layers.Dense(units = 1, \r\n                                activation=None, \r\n                                use_bias=True)\r\n\r\n    def call(self, inputs, training = True):\r\n        X = self.Dense_in(inputs)\r\n        X = self.Dense_hidden(X)\r\n        X = self.Dense_out(X)\r\n        return X\r\n\r\ndummy_data = tf.random.normal((10000, 50))\r\ndummy_y = tf.random.uniform((10000,), minval=0, maxval=2, dtype=tf.dtypes.int32)\r\n\r\n\r\n#### LR is not logged here ####\r\nmodel = SimpleModel()\r\n\r\nloss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)\r\nopt = tf.keras.optimizers.Adam(learning_rate = 0.001)\r\nmodel.compile(optimizer = opt, loss = loss, metrics=[\"acc\"])\r\n\r\n_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nlog_dir = os.path.join('dummy_logs', _datetime)\r\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n\r\nmodel.fit(x = dummy_data, y = dummy_y, epochs = 10,\r\n        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', \r\n                        factor=0.5, patience=10, verbose=0, mode='auto', \r\n                        min_delta=0.0001, cooldown=0, min_lr=0),\r\n                    tb_callback])\r\n\r\n#### LR is logged here ####\r\nmodel = SimpleModel()\r\n\r\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate = 0.001,\r\n    decay_steps=100000,\r\n    decay_rate=0.96)\r\n\r\nloss = tf.keras.losses.BinaryCrossentropy(name='binary_crossentropy', from_logits=True)\r\nopt = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\r\nmodel.compile(optimizer = opt, loss = loss, metrics=[\"acc\"])\r\n\r\n_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nlog_dir = os.path.join('dummy_logs', _datetime)\r\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n\r\nmodel.fit(x = dummy_data, y = dummy_y, epochs = 10,\r\n        callbacks=[tb_callback])\r\n```\r\n![Screenshot from 2021-01-07 12-14-15](https://user-images.githubusercontent.com/32313554/103887434-517ee100-50e3-11eb-9597-4978bee27972.png)\r\n\r\n\r\n**Other info / logs**   \r\nI think that the problem is in tf.keras.callbacks.TensorBoard in _collect_learning_rate (see code block below). If self.model.optimizer.lr is sheduler then logs['learning_rate'] is created and LR can be seen in tensorboard. However if self.model.optimizer.lr is not an instance of learning_rate_schedule.LearningRateSchedule then logs['learning_rate'] does not exists and thus LR is not logged.\r\n```\r\ndef _collect_learning_rate(self, logs):\r\n    lr_schedule = getattr(self.model.optimizer, 'lr', None)\r\n    if isinstance(lr_schedule, learning_rate_schedule.LearningRateSchedule):\r\n      logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)\r\n    return logs\r\n```\r\n\r\nA quick fix that I see here could be to add *else* or *elif* and directly log self.model.optimizer.lr. Changing as below fixed the issue for me when using ReduceLROnPlateau callback. This fix assumes that if no LearningRateSchedule is used then self.model.optimizer.lr could be logged as scalar by tensorboard callback. If this is not always possible then I guess adding some reasonable *elif* to be able to catch LR that can be logged (e.g. scalar) could resolve the problem.\r\n```\r\nclass TensorBoardCallback(tf.keras.callbacks.TensorBoard):\r\n    def __init__(self, **kwargs):\r\n        super(TensorBoardCallback, self).__init__(**kwargs)\r\n\r\n    def _collect_learning_rate(self, logs):\r\n        lr_schedule = getattr(self.model.optimizer, 'lr', None)\r\n        if isinstance(lr_schedule, tf.keras.optimizers.schedules.LearningRateSchedule):\r\n            logs['learning_rate'] = lr_schedule(self.model.optimizer.iterations)\r\n        elif lr_schedule is not None:\r\n            logs['learning_rate'] = lr_schedule\r\n        return logs\r\n```", "comments": ["@MatejHl,\r\nTensorboard issues are tracked in tensorflow/tensorboard repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/tensorboard/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Thanks for the fast response. \r\nI opened new issue [here](https://github.com/tensorflow/tensorboard/issues/4522) and I am closing this one.\r\n\r\nHave a nice day !"]}, {"number": 46246, "title": "Speedup Resize{Bilinear,NearestNeighbor}", "body": "This PR speedup `Resize{Bilinear,NearestNeighbor}` with eigen tensor generator class. The original implementation specialize 3-channeled image with SSE, but it's still less performant compared with this PR. Below is my benchmark with `tensorflow/core/kernels/image/resize_op_benchmark_test.cc`.\r\n\r\nOn ***Intel(R) Core(TM) i5-9600K CPU @ 3.70GHz***\r\n\r\n### Nearest neighbor\r\n\r\n| input_shape (N x IH x IW x C) | output_shape (OH x OW) | Old (M items/s) | New (M items/s) |\r\n| ----- | ---- | ---- | ----- |\r\n|  10x499x499x1       | 250x250       |  614        |  2849      |\r\n|  10x499x499x3       | 250x250       |  2267        |  4304      |\r\n|  10x499x499x1       | 998x998       |   58       |  368        |\r\n|  10x499x499x3       | 998x998       |   170       |  538        |\r\n\r\nwhere M items/s is computed as  `time / (iterations * N * IH * IW * C)`, the higher the better.\r\n\r\n### Bilinear\r\n\r\n| input_shape (N x IH x IW x C) | output_shape (OH x OW) | Old (M items/s) | New (M items/s) |\r\n| ----- | ---- | ---- | ----- |\r\n|  10x499x499x1       | 250x250       |  1282        |  2362 |\r\n|  10x499x499x3       | 250x250       |  2106        |  3275 |\r\n|  10x499x499x1       | 998x998       |  103        |  182   |\r\n|  10x499x499x3       | 998x998       |  179        |  362   |\r\n\r\nwhere M items/s is computed as  `time / (iterations * N * IH * IW * C)`, the higher the better.", "comments": []}, {"number": 46245, "title": "tf.keras Model predict slow in flask service", "body": "\r\nSystem info:\r\ntensorflow: 1.12.0\r\nflask: 1.1.2\r\nos: centos7.6\r\nuse cpu\r\n\r\nproblem:\r\ntf.keras Model predict slow in flask service\r\nimport the follow code in python console, and run predict method directly\uff0c only cost about 800ms time;\r\nbut in flask service\uff0c run predict methodin an interface, it need about 12s time.\r\n\r\ncode:\r\n    def __init__(self):\r\n        self.height = 960\r\n        self.width = 960\r\n        self.channels = 3\r\n        self.model = ResNet50(include_top=False, input_shape=(self.height, self.width, self.channels))\r\n        self.graph = tf.get_default_graph()\r\n\r\n    def image_resize(self, image):\r\n        raw_height = image.shape[0]\r\n        raw_width = image.shape[1]\r\n        if raw_height > self.height or raw_width > self.width:\r\n            ratio1 = raw_height * 1.0 / self.height\r\n            ratio2 = raw_width * 1.0 / self.width\r\n            if ratio2 > ratio1:\r\n                ratio1 = ratio2\r\n            raw_height = int(raw_height / ratio1)\r\n            raw_width = int(raw_width / ratio1)\r\n            image = cv2.resize(image, (raw_width, raw_height))\r\n        new_img = np.pad(image,\r\n                         pad_width=((0, self.height - raw_height),\r\n                                    (0, self.width - raw_width),\r\n                                    (0, 0)\r\n                                    ),\r\n                         mode=\"constant\", constant_values=(0, 0))\r\n\r\n        return new_img\r\n\r\n    def predict(self):\r\n        image = cv2.imread('1.jpg')\r\n        image = self.image_resize(image)\r\n        time1 = int(round(time.time() * 1000))\r\n        with self.graph.as_default():\r\n            self.model.predict(np.array([image]))\r\n        time2 = int(round(time.time() * 1000))\r\n        cost = time2 - time1\r\n        return str(cost) + \" ms.\"", "comments": ["I have find the reason, my flask service has an other model based on paddlepaddle, and it predict faster after i remove the other model ", "Hy, @qiuming-93  if possible would you please send your whole code and instructions to replicate it. So, that I can test the bottle neck of your issue.\r\nthanks.", "I have find the reason, you can add \"from paddleocr import PaddleOCR\" at the head of the code to replicate it.\r\nthanks\r\n", "@qiuming-93 \r\n\r\nIs this still an issue?\r\nIf issue still persists please share complete code snippet with proper indentation and supporting files to reproduce the issue in our environment.It helps us in debugging faster. Thanks!", "#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n# @Time    : 2021/1/4 20:05\r\n# @Author  : qiuming\r\n\r\nimport flask\r\nfrom tensorflow.python.keras.applications import ResNet50\r\nimport tensorflow as tf\r\nimport cv2\r\nimport numpy as np\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\nhost='0.0.0.0'\r\n# \u5b9e\u4f8b\u5316 flask\r\napp = flask.Flask(__name__)\r\n\r\nheight = 960\r\nwidth = 960\r\nchannels = 3\r\nmodel = ResNet50(include_top=False, input_shape=(height, width, channels))\r\n\r\ndef image_resize(image):\r\n    height=960\r\n    width=960\r\n    raw_height = image.shape[0]\r\n    raw_width = image.shape[1]\r\n    if raw_height > height or raw_width > width:\r\n        ratio1 = raw_height * 1.0 / height\r\n        ratio2 = raw_width * 1.0 / width\r\n        if ratio2 > ratio1:\r\n            ratio1 = ratio2\r\n        raw_height = int(raw_height / ratio1)\r\n        raw_width = int(raw_width / ratio1)\r\n        image = cv2.resize(image, (raw_width, raw_height))\r\n    new_img = np.pad(image,\r\n                     pad_width=((0, height - raw_height),\r\n                                (0, width - raw_width),\r\n                                (0, 0)\r\n                                ),\r\n                     mode=\"constant\", constant_values=(0, 0))\r\n\r\n    return new_img\r\n\r\ndef predict():\r\n    image = cv2.imread('1.jpg')\r\n    import time\r\n    print(time.time())\r\n    if not isinstance(image, np.ndarray):\r\n        raise BaseException('image must be numpy array')\r\n    shape = image.shape\r\n    if len(shape) == 2:\r\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\r\n    assert len(image.shape) == 3\r\n    raw_height = 960\r\n    raw_width = 960\r\n    if image.shape[0] > 960:\r\n        raw_height = image.shape[0]\r\n    if image.shape[1] > 960:\r\n        raw_width = image.shape[1]\r\n    if raw_width / 960 > raw_height / 960:\r\n        raw_height = int(raw_width / 960 * 960)\r\n    else:\r\n        raw_width = int(raw_height / 960 * 960)\r\n    image_copy = image.copy()\r\n    image_copy = image_resize(image_copy)\r\n    print(time.time())\r\n    pred = model.predict(np.array([image_copy]))\r\n    print(time.time())\r\n\r\n\t\r\n# \u5c06\u9884\u6d4b\u51fd\u6570\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u8def\u7531\r\n@app.route(\"/predict\", methods=[\"GET\"])\r\ndef predict2():\r\n    data = {\"success\": False}\r\n    predict()\r\n    return flask.jsonify(data)\r\n\r\n\r\n# \u542f\u52a8Flask\u5e94\u7528\u7a0b\u5e8f\uff0c\u5141\u8bb8\u8fdc\u7a0b\u8fde\u63a5\r\nif __name__ == '__main__':\r\n    app.run(debug=False, host=host,threaded=False,port=5000)\r\n", "@qiuming-93 \r\n\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4 and check if you are facing the same issue. Thanks!", "ok"]}, {"number": 46244, "title": "Different results for add_loss() within TimeDistributed for eager vs. compiled execution of keras Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: '3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) \\n[GCC 9.3.0]'\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11.0 / 8\r\n- GPU model and memory: TITAN X (Pascal) computeCapability: 6.1\r\n\r\n**Describe the current behavior**\r\n\r\nIn the minimal example code below, I have a keras Layer which calls `add_loss()`, and this layer is wrapped in `TimeDistributed`. When I execute the layer in eager mode, the losses have an extra timestep, as the first timestep is double-counted. However, if I evaluate the compiled model, the correct loss is computed.\r\n\r\nDuring eager execution, the `add_loss` gets called first when computing `output_timestep_zero`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4452\r\n\r\nand subsequently called 5 (== # timesteps) times by: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4538\r\n\r\n**Describe the expected behavior**\r\n\r\nI expected layer.losses to count each timestep once and not double count the first timestep. I would think that the loss would be cleared after the first call used to obtain `output_timestep_zero`. I'm not entirely sure I understand why the first timestep is double-counted, but it would be useful to have the eager mode execution return the same losses as reported when the model is compiled.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nbelow I run a simple example with 5 timesteps, where the input equals the timestep value + 1, so the correct loss would be\r\n`mean([1, 2, 3, 4, 5]) == 3`. In eager execution, the first timestep is counted twice\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\n\r\n# build inputs(b, t, i) == t + 1 with size [batch, timesteps, inputs]\r\nbatch, timesteps, ninputs = 1, 5, 1\r\ntimevec = (\r\n    np.arange(timesteps, dtype=np.float32) + 1\r\n)  # we start at one since the first gets double-counted\r\ninputs = np.broadcast_to(\r\n    timevec[np.newaxis, :, np.newaxis], (batch, timesteps, ninputs)\r\n)\r\n\r\n\r\nclass AddsLossLayer(keras.layers.Layer):\r\n    \"\"\"Identity layer which calls add_loss on mean of input\"\"\"\r\n\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        # inputs is batch x ninputs\r\n        # return input but add_loss(mean of current input over ninputs dimension, should == timestep + 1)\r\n        self.add_loss(tf.math.reduce_mean(inputs, axis=0, name=\"loss\"))\r\n        return inputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n\r\nadds_loss_layer = AddsLossLayer()\r\ntd_layer = keras.layers.TimeDistributed(adds_loss_layer)\r\n\r\noutputs = td_layer(inputs)\r\n\r\nprint(f\"inputs shape = {inputs.shape}\")\r\nprint(f\"outputs shape = {outputs.shape}\")\r\nprint(f\"len(td_layer.losses) == {len(td_layer.losses)}\")\r\nprint(f\"len(adds_loss_layer.losses) == {len(adds_loss_layer.losses)}\")\r\nprint(\"loss values == \", [loss.numpy() for loss in td_layer.losses])\r\n\r\n# inputs shape = (3, 5, 1)\r\n# outputs shape = (3, 5, 1)\r\n# len(td_layer.losses) == 6\r\n# len(adds_loss_layer.losses) == 6\r\n# loss values ==  [array([1.], dtype=float32), array([1.], dtype=float32), array([2.], dtype=float32), array([3.], dtype=float32), array([4.], dtype=float32), array([5.], dtype=float32)]\r\n\r\n# so we double count the first timestep!\r\n\r\nmodel_inputs = tf.keras.Input(shape=inputs.shape[1:], dtype=\"float32\")\r\nadds_loss_layer = AddsLossLayer()\r\ntd_layer = keras.layers.TimeDistributed(adds_loss_layer, name=\"td\")\r\nmodel_outputs = td_layer(model_inputs)\r\n\r\nmodel = keras.Model(model_inputs, model_outputs)\r\nmodel.compile()\r\nmodel.evaluate(inputs, None, verbose=2)\r\n\r\n# 1/1 - 0s - loss: 3.0000\r\n# (this is correct, (1 + 2 + 3 + 4 + 5) / 5 == 3)\r\n\r\nmodel.fit(inputs, None, epochs=1)\r\n\r\n# 1/1 [==============================] - 0s 100ms/step - loss: 3.0000\r\n# (this is correct, (1 + 2 + 3 + 4 + 5) / 5 == 3)\r\n\r\noutputs = model(inputs)\r\n\r\nprint(\"mean model.losses = \", tf.math.reduce_mean(model.losses).numpy())\r\n\r\n# mean model.losses =  2.6666667\r\n# due to double counted first timestep, (1 + 1 + 2 + 3 + 4 + 5) / 6 = 2.666667\r\n```\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b22c6930b5dc31c750993ee0de021c8b/46244.ipynb). Thanks!", "@djoshea I can reproduce this with recent `TF2.6` and `tf-nightly`. May be this is a potential bug. \r\n\r\nAs Keras moved to separate repo (i can't move the issue), can you please close here and open in keras repo https://github.com/keras-team/keras/issues\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46244\">No</a>\n"]}, {"number": 46243, "title": "Copy lite/kernels/gather.cc to lite/micro/kernels/gather.cc", "body": "PR3 for issue #45196", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46242, "title": "Refactoring fully_connected to share code between reference and optimized kernels.", "body": "Summary:\r\n\r\n * Move shared structs / helper functions into fully_connected_common.cc\r\n * Clean up some of the existing code to directly call the reference implementations (made possible by the refactor of the helper functions).\r\n\r\nAlso, this refactor addresses the sign flip in fully_connected: http://b/138810107", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@freddan80 @felix-johnny: we're trying to see what might work to increase code-sharing between reference and optimized kernels. Let us know if you have any suggestions.\r\n\r\n@yair-ehrenwald: this PR is a stab at improved code sharing. Let us know if you have any suggestions.", "Ready for review again.", "@advaitjain I'll havbe a look at it today"]}, {"number": 46241, "title": "Refactor gather.h from TFLite reference_ops.h", "body": "PR2 for issue #45196", "comments": ["@rsun-bdti Can you please resolve conflicts? Thanks!", "Merging is hopelessly tangled up. Close this PR and start over."]}, {"number": 46240, "title": "Add Sony Spresense target", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): any\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): master\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sony Spresense board with Cortex-M4F\r\n\r\n**Describe the problem**\r\nTo add support for sony spresense board.\r\nSpresense has own development environment and it is bit conflict with Tensorflow Lite for Microcontrollers.\r\nTensorflow Lite for Microcontrollers can be used as a linkable library as libtensorflow-microlite.a.\r\n\r\nA simple solution is to build the library as on Coretex-M4 platform with CMSIS-NN, and use it on the board development environment.\r\nBut the problem is how to use a examples.\r\nFortunately, the examples provide main_functions.cc with just 2 function as externd \"C\" like setup() and loop().\r\nSo I try to create a spresense target to build the library archive with example's sources without main.cc.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nI add a Makefile for downloading and build it on spresense sdk environment.\r\nThe draft work is in https://github.com/takayoshi-k/spresense/tree/add_tf_lm.\r\nThe makefile is https://github.com/takayoshi-k/spresense/blob/add_tf_lm/externals/tensorflow/Makefile.\r\n", "comments": ["Hi @takayoshi-k ! Sony Spresense has been added in  tested micocontrollers list now. Attaching relevant[ thread ](https://www.tensorflow.org/lite/microcontrollers#supported_platforms)for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46240\">No</a>\n"]}, {"number": 46239, "title": "Refactor case BuiltinOperator_GATHER in flatbuffer_conversions", "body": "PR1 for issue #45196", "comments": []}, {"number": 46238, "title": "Include and build xa_nnlib with TARGET_ARCH=fusion_f1.", "body": " * Needed to add some defines, disable warning and exclude some sources from xa_nnlib to build everything.\r\n * The kernels are still unchanged (i.e. have a reference fallback).\r\n\r\nManually verified that the following command downloads xa_nnlib:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_keyword_benchmark -j8\r\n```\r\n\r\nAnd the latency is unchanged:\r\n```\r\nInitializeKeywordRunner() took 280862 ticks (280 ms)\r\nKeywordRunNIerations(1) took 170431 ticks (170 ms)\r\nKeywordRunNIerations(10) took 1703817 ticks (1703 ms)\r\n```\r\n\r\nAnd the size with the release build:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n```\r\n\r\nis also unchanged:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  51168\t  40132\t  24872\t 116172\t  1c5cc\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1/bin/keyword_benchmark\r\n```\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 46237, "title": "[tf.data] enhance encapsulation for IteratorResource State", "body": "This PR enhances the encapsulation capability of the `State` of `tensorflow:data:IteratorResource` by representing it as a `class` instead of a `struct`. Additionally, the necessary `getter` methods have been added.\r\n\r\ncc: @aaudiber this was one of the TODO items which required better encapsulation. Please let me know if this approach is suitable and if any changes are required.\r\n", "comments": []}, {"number": 46235, "title": "Fix the CUDNN RNN params dim size", "body": "The CudnnRnnParamsDescriptor::Create() uses the RNN params size _in bytes_ as the dimension for cudnnSetFilterNdDescriptor(). This PR fixes it by dividing the params size in bytes by the byte size of data type.\r\n\r\n\r\n\r\nFYI. @nluehr \r\n\r\n\r\n", "comments": []}, {"number": 46233, "title": "Tensorflow 2.4 not showing available GPU even after successful cuda and cudnn installation", "body": "OS - ubuntu 20.04\r\npython -3..8.5\r\nGPU - RTX 2060\r\nCudnn - v7.6.5\r\n\r\nOutput of `tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)`:\r\n`WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2021-01-07 00:08:12.313080: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-07 00:08:12.315365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-07 00:08:12.403099: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2021-01-07 00:08:12.403164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: aum-GF65-Thin-9SEXR\r\n2021-01-07 00:08:12.403176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aum-GF65-Thin-9SEXR\r\n2021-01-07 00:08:12.403275: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.80.2\r\n2021-01-07 00:08:12.403328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2\r\n2021-01-07 00:08:12.403345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.80.2\r\nFalse\r\n`\r\nOUTPUT of `dpkg -l | grep cuda-toolkit`\r\n\r\n`ii  cuda-toolkit-11-0                               11.0.2-1                              amd64        CUDA Toolkit 11.0 meta-package\r\nii  nvidia-cuda-toolkit                             10.1.243-3                            amd64        NVIDIA CUDA development toolkit\r\n`\r\n\r\nOUTPUT of `nvidia-smi` \r\n`+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 2060    Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   48C    P8     6W /  N/A |    306MiB /  5934MiB |      5%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A       920      G   /usr/lib/xorg/Xorg                 45MiB |\r\n|    0   N/A  N/A      1469      G   /usr/lib/xorg/Xorg                106MiB |\r\n|    0   N/A  N/A      1647      G   /usr/bin/gnome-shell              125MiB |\r\n|    0   N/A  N/A      2647      G   /usr/lib/firefox/firefox            3MiB |\r\n|    0   N/A  N/A      2690      G   /usr/lib/firefox/firefox            3MiB |\r\n|    0   N/A  N/A      2723      G   /usr/lib/firefox/firefox            3MiB |\r\n|    0   N/A  N/A      3681      G   /usr/lib/firefox/firefox            3MiB `\r\n\r\nAfter succesful installation of CUDA 11, nvidia-cuda-toolkit 10.1 and tensorflow 2.4 (also tried tensorflow-gpu 2.2.0 but same error) I am still getting NO GPUs FOUND.\r\n\r\nPLEASE HELP!!\r\n\r\n\r\n", "comments": ["@patilaum \r\nCould you check this issue which is similar to your [issue](https://github.com/tensorflow/tensorflow/issues/19266#issuecomment-399686258). If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu and try following the instructions from [TensorFlow](https://www.tensorflow.org/install/source) website.\r\nalso refer to [link](https://stackoverflow.com/questions/59499764/tensorflow-not-tensorflow-gpu-failed-call-to-cuinit-unknown-error-303]), [link1](https://victorwyee.com/engineering/failed-call-to-cuinit-cuda-error-unknown/)Thanks!", "@Saduf2019  Uninstalled and installed cuda and cudnn many times now. Have tried all the steps given in those links that you shared but ended up broken packages of NVIDIA. Now I have reinstalled the OS on my system. Could you tell me the precise steps to install tensorflow 2.2.0 and above on ubuntu 20.04 with python 3.8.5? ", "Solved!\r\nHere's what I did:\r\n1. installed [cuda toolkit ](https://developer.nvidia.com/cuda-11.0-download-archive) 11.0- important thing here is I did not install 10.1 as I think it creates a conflict with already installed cuda 11.0 shown in nvidia smi\r\n2. reboot after `sudo apt-get install nvidia-modprobe` is must\r\n3. disabled SECURE BOOT - most important\r\n@Saduf2019  Thanks a lot", "@patilaum\r\nGlad to hear the issue is resolved, thank you for the update."]}, {"number": 46232, "title": "[ROCm] Update ROCm XLA backend to use GCN Arch Name ", "body": "Starting with ROCm 4.1(?),  TF XLA will need to correctly populate the \"target-feature\" when creating the LLVM AMDGPUTarget, in order to get optimal performance. The \"target-feature\"s supported by the underlying GPU can queried via the `hipGetDeviceProperties` API and are stored in the `hipDeviceProp_t::gcnArchName` field.\r\n\r\nThis PR has 3 commits\r\n1. Add the hooks to retrieve and query the `hipDeviceProp_t::gcnArchName` field\r\n2. Update the `GpuVersion` datatype for AMDGPU in XLA code to include the `gcnArchName` string\r\n3. parse the `gcnArchName` string and map the tokens within it, to appropriate \"target-feature\" string tokens.\r\n\r\nThe mapping done in the 3rd commit will be straight-forward once support for this feature gets upstreamed to the public LLVM repo. Until that happens and the TF LLVM pointer is moved beyond the upstream commit, we will need to special case the mapping. \r\n\r\n------------------------------------------------------------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n", "comments": ["Seems to break some tests?\r\n\r\n```\r\nFAIL: //tensorflow/python/eager:function_test_gpu (shard 7 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/9a46bec5c8e82ceb5b4b229201d44d70/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_7_of_15/test_attempts/attempt_1.log)\r\n```\r\n\r\nDoes this work on your end?", "```\r\nterminate called without an active exception\r\nFatal Python error: Aborted\r\n\r\nThread 0x00007ff50d7b2700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff50ffb7700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff510fb9700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff511fbb700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nCurrent thread 0x00007ff5287e8700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff528fe9700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff5327fc700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff5397fa700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff53b7fe700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff5496a2700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\nThread 0x00007ff9357fa700 (most recent call first):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py\", line 58 in __del__\r\n\r\n```", "> Seems to break some tests?\r\n> \r\n> ```\r\n> FAIL: //tensorflow/python/eager:function_test_gpu (shard 7 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/9a46bec5c8e82ceb5b4b229201d44d70/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_7_of_15/test_attempts/attempt_1.log)\r\n> ```\r\n> \r\n> Does this work on your end?\r\n\r\nThe changes in this PR do not (or atleast should not) have any impact on functionality...so doubtful that they are the cause of the regression.\r\n\r\nLooking at the log (and assuming you are running internally on ROCm) , it seems that you are running with sharding enabled. Enabling sharding is not something we have had much success with internally...our CI script runs with sharding explicitly disabled!\r\n\r\nLet me run that test locally and confirm it is working", "> > Seems to break some tests?\r\n> > ```\r\n> > FAIL: //tensorflow/python/eager:function_test_gpu (shard 7 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/9a46bec5c8e82ceb5b4b229201d44d70/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_7_of_15/test_attempts/attempt_1.log)\r\n> > ```\r\n> > \r\n> > \r\n> > Does this work on your end?\r\n> \r\n> The changes in this PR do not (or atleast should not) have any impact on functionality...so doubtful that they are the cause of the regression.\r\n> \r\n> Looking at the log (and assuming you are running internally on ROCm) , it seems that you are running with sharding enabled. Enabling sharding is not something we have had much success with internally...our CI script runs with sharding explicitly disabled!\r\n> \r\n> Let me run that test locally and confirm it is working\r\n\r\nOk, I am trying to fix this now. There is some adjustment needed in gpu_kernel_to_blob_pass.cc (for the AMD specific code in kernel generator). The function_test_gpu seemed to pass inside our Google infrastructure when I tried it now, maybe it was indeed an unrelated error, or it only fails in the Kokoro setup. In any case, the failure reported by @cheshire was when running on the cuda platform, so theoretically should be unrelated. The presubmit for the ROCM platform didn't get so far as to run any tests, because it failed during compilation.", "> Ok, I am trying to fix this now. There is some adjustment needed in gpu_kernel_to_blob_pass.cc (for the AMD specific code in kernel generator).\r\n\r\n@akuegel , when I tried to do `bazel test //tensorflow/core/kernels/mlir_generated/...` locally, all 165 tests contained within it failed. \r\n\r\nThe cause turned out be that `--arch=sm_70,compute_75` is hardcoded here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/mlir_generated/build_test.sh#L27)\r\n\r\nIf I change the argument `--arch=gfx900` locally, all tests start passing.\r\n\r\nNot sure whether the error you running into on your end is the same, or something different.\r\n\r\nThe above experiment was done on the `master` branch (i.e. not on the branch containing my changes). , going to try that next to see if there are any further failures caused by it.\r\n\r\nAlso please let me know if the error(s) you are seeing on your end are different, and if so how to go about reproducing them on my end", "with the branch for this PR, I am getting an additional error\r\n```\r\nExecution platform: @local_execution_config_platform//:platform\r\ntensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc: In member function 'xla::StatusOr<std::vector<unsigned char, std::allocator<unsigned char> > > mlir::kernel_gen::transforms::{anonymous}::GpuKernelToBlobPass::GetGpuBinaryBlob(mlir::gpu::GPUModuleOp)':\r\ntensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:124:62: error: could not convert 'arch' from 'uint32_t {aka unsigned int}' to 'xla::gpu::GpuVersion {aka absl::lts_2020_02_25::variant<std::pair<int, int>, std::pair<int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >}'\r\n           llvm_module_copy.get(), arch, config, libdevice_dir);\r\n                                                              ^\r\ntensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:137:52: error: no matching function for call to 'std::vector<stream_executor::HsacoImage>::push_back(<brace-enclosed initializer list>)'\r\n       images.push_back({arch_str, std::move(hsaco)});\r\n                                                    ^\r\nI\r\n```\r\n\r\nworking on fixing it now", "@cheshire @akuegel \r\n\r\nI have rebased the PR ( to the tip) and pushed out two more commits\r\n1. to address the review comments from George\r\n2. to address the build error in `gpu_kernel_to_blob_pass.cc` (potentially the same one that Adrian ran into) \r\n\r\nPlease re-review and re-approve.\r\n\r\n@akuegel,  I am assuming you will handle the issue of `--arch=sm_70,compute_75` being hard-coded in `.../build_test.sh` on your end.\r\n\r\n\r\n", "> @cheshire @akuegel\r\n> \r\n> I have rebased the PR ( to the tip) and pushed out two more commits\r\n> \r\n> 1. to address the review comments from George\r\n> 2. to address the build error in `gpu_kernel_to_blob_pass.cc` (potentially the same one that Adrian ran into)\r\n> \r\n> Please re-review and re-approve.\r\n> \r\n> @akuegel, I am assuming you will handle the issue of `--arch=sm_70,compute_75` being hard-coded in `.../build_test.sh` on your end.\r\n\r\nI was talking about the gpu_kernel_to_blob_pass.cc. The other thing is not an issue on our side, because we are running the tests only with Nvidia GPUs. Apart from gpu_kernel_to_blob_pass, I had fixed quite a few more things in your PR which ClangTidy pointed out, unfortunately the reviewer I asked for approval didn't react. I added another one now, hopefully I will get approval today."]}, {"number": 46231, "title": "tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\nYes, slightly modified [this](https://www.tensorflow.org/tutorials/structured_data/time_series) guide\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Manjaro Nibia 20.2 Kernel 5.4\r\n-   **TensorFlow installed from (source or binary)**: Manjaro package - [https://discover.manjaro.org/packages/python-tensorflow-cuda](https://discover.manjaro.org/packages/python-tensorflow-cuda)\r\n-   **TensorFlow version (use command below)**: 2.4.0-1\r\n-   **Python version**: Python 3.9.1\r\n-   **CUDA/cuDNN version**: bundled in manjaro package\r\n-   **GPU model and memory**: GeForce GTX 960M total memory 2GB\r\n-   **Exact command to reproduce**: \r\nFollowing [this](https://www.tensorflow.org/tutorials/structured_data/time_series#2_split) guide the error occurs at splitting\r\n\r\n### Describe the problem\r\nOccurred during training. Log states to report this problem.\r\n\r\n### Source code / logs\r\nThis is the log output with AUTOGRAPH_VERBOSITY=10\r\n```log\r\n2021-01-06 19:56:39.014574: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-06 19:56:40.398316: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-06 19:56:40.399294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-01-06 19:56:40.432681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.433049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2021-01-06 19:56:40.433100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-06 19:56:40.436263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-06 19:56:40.436390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-06 19:56:40.437546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-06 19:56:40.437861: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-06 19:56:40.441100: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-01-06 19:56:40.441937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-06 19:56:40.442202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-06 19:56:40.442373: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.442839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.443141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-06 19:56:40.443406: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-06 19:56:40.443944: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-06 19:56:40.444057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.444410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2021-01-06 19:56:40.444455: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-06 19:56:40.444489: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-01-06 19:56:40.444533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-01-06 19:56:40.444573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-01-06 19:56:40.444602: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-01-06 19:56:40.444640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-01-06 19:56:40.444687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-01-06 19:56:40.444720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-01-06 19:56:40.444816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.445205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.445503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-06 19:56:40.445546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-01-06 19:56:40.519438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-06 19:56:40.519471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-01-06 19:56:40.519479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-01-06 19:56:40.519741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.520226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.520640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-01-06 19:56:40.521036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:03:00.0, compute capability: 5.0)\r\n2021-01-06 19:56:40.521335: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nWARNING:tensorflow:AutoGraph could not transform <bound method WindowGenerator.split_window of Total window size: 3\r\nInput indices: [0 1]\r\nLabel indices: [1 2]\r\nLabel column name(s): ['price']> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: invalid syntax (tmp7x98gy_p.py, line 10)\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2021-01-06 19:56:40.756777: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-01-06 19:56:40.757331: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2398910000 Hz\r\nEpoch 1/100\r\n2021-01-06 19:56:40.930451: F ./tensorflow/core/kernels/random_op_gpu.h:244] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), key, counter, gen, data, size, dist) status: Internal: no kernel image is available for execution on the device\r\nAborted (core dumped)\r\n```\r\n", "comments": ["@Heron-TP \r\n\r\nI have tried in colab with TF Gpu version 2.4 and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/27da033ae8c7012562e226dc5ffcb3a8/untitled601.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46231\">No</a>\n"]}, {"number": 46230, "title": "Unable to read", "body": "I'm using Ubuntu ", "comments": ["@za13,\r\nOn running the code, I am facing an error stating `KeyError: 'owl'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1847dee6798aa8e191f695c8466dc803/46230.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet along with the dataset you are using. Thanks!", "@za13,\r\nOn running the code, I am facing an error stating `DecodeError: Error parsing message`.\r\n\r\nHowever using `tf.keras.models.load_model`, I was able to load the model without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6b2d617c743b06717ad012997c605541/46230.ipynb#scrollTo=69zmDVXR2GXi). Thanks!", "@za13 I don't see any issue in reading `*.pb` file. I used a `dummy.pb` file and was able to read it without any problem.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/03fd1176ae129d03598937824f3a3a5c/untitled.ipynb) is a gist for reference. Thanks!\r\n\r\nI attaching the *.pb file (as *.txt file so that GitHub allows me to upload)\r\n\r\n\r\n[dummy_pb.txt](https://github.com/tensorflow/tensorflow/files/6119628/dummy_pb.txt)\r\n\r\nPlease close the issue if this was resolved. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46230\">No</a>\n"]}, {"number": 46229, "title": "No longer compile with `-march=native`.", "body": "Should resolve issue reported in #45744, #45866, #44701 and #45991 as well as multiple other issues from other ecosystem places.", "comments": []}, {"number": 46228, "title": "NFC - minor spelling tweaks under lite directory", "body": "This PR addresses minor spelling tweaks under `tensorflow/lite` directory.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46227, "title": "Unable to access GPU from docker with error failed call to cuInit: CUresult(-1)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04, Google cloud instance\r\n- Tensorflow Docker Version: tensorflow/tensorflow:1.9.0-gpu-py3\r\n- GPU model and memory: NIVIDIA-T4, 16GB\r\n\r\nWe are trying to create a custom docker image to serve image classification model. Using the tensorflow/tensorflow:1.9.0-gpu-py3 as base image. The host machine has the NVIDIA drivers installed and able to run the model on GPU.  \r\n\r\n**Docker file**\r\n\r\n```\r\nFROM tensorflow/tensorflow:1.9.0-gpu-py3 as base\r\nENV CUDA_HOME /usr/local/cuda\r\nENV PATH=/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\nENV LD_LIBRARY_PATH /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\r\nRUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 \\\r\n     && echo \"/usr/local/cuda/lib64/stubs\" > /etc/ld.so.conf.d/z-cuda-stubs.conf \\\r\n     && ldconfig\r\nENV NVIDIA_VISIBLE_DEVICES all\r\nADD . /app\r\nWORKDIR /app\r\nRUN apt-get -yqq update\r\nRUN apt-get install -yqq libsm6 libxext6 libxrender-dev\r\nRUN pip install -r requirements.txt\r\nRUN python3 run_model.py\r\n```\r\nWhile building the image using the command `sudo nvidia-docker build -t name .` Getting the following error:\r\n\r\n```\r\n2021-01-06 17:27:26.453415: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2021-01-06 17:27:26.476869: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUresult(-1)\r\n2021-01-06 17:27:26.476956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:152] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\r\n```\r\n\r\nModel is loaded on CPU instead of GPU.\r\n\r\nI have followed the instructions from:\r\n[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\r\n)\r\n\r\nto install the nvidia-container-toolkit and when I run the following command: `sudo docker run --rm --gpus all nvidia/cuda:9.0-base nvidia-smi`\r\nI see the following output:\r\n```\r\n[+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   63C    P0    32W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+]\r\n```\r\n", "comments": ["Was trying to run the model inference during the build which caused the issue. Now able to access GPU ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46227\">No</a>\n"]}]