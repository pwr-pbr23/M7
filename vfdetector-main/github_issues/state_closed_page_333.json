[{"number": 44172, "title": "Lr keyword parameter for learning rate does not accept a learning rate schdule", "body": "Output of system capture\r\n```\r\n== check python ===================================================\r\npython version: 3.8.5\r\npython branch: \r\npython build version: ('default', 'Jul 21 2020 10:42:08')\r\npython compiler version: Clang 11.0.0 (clang-1100.0.33.17)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\r\nTarget: x86_64-apple-darwin18.7.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                  1.18.5\r\nprotobuf               3.13.0\r\ntensorflow             2.3.1\r\ntensorflow-estimator   2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.1\r\ntf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n-bash: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /Users/nicksorros/code/notebooks/venv/lib/python3.8/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 5, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os 10.14.16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\n\r\nlearning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.01, 10, 0.9)\r\noptimizer = tf.keras.optimizers.Adam(lr=learning_rate)\r\n```\r\n\r\nthrows\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-7b7916db6482> in <module>\r\n      1 learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.01, 10, 0.9)\r\n----> 2 optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\r\n\r\n~/code/notebooks/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/adam.py in __init__(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)\r\n    113                name='Adam',\r\n    114                **kwargs):\r\n--> 115     super(Adam, self).__init__(name, **kwargs)\r\n    116     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\r\n    117     self._set_hyper('decay', self._initial_decay)\r\n\r\n~/code/notebooks/venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in __init__(self, name, **kwargs)\r\n    303                         \"passed to optimizer: \" + str(k))\r\n    304       # checks that all keyword arguments are non-negative.\r\n--> 305       if kwargs[k] is not None and kwargs[k] < 0:\r\n    306         raise ValueError(\"Expected {} >= 0, received: {}\".format(k, kwargs[k]))\r\n    307 \r\n\r\nTypeError: '<' not supported between instances of 'ExponentialDecay' and 'int'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI was expecting this to work. It works as a positional argument. `optimizer = tf.keras.optimizers.Adam(learning_rate)`\r\n", "comments": ["Please check https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/optimizer_v2/adam.py#L362-L363", "Check https://github.com/tensorflow/tensorflow/pull/44295", "> Check #44295\r\n\r\n\ud83d\ude4f ", "The PR add Just an extra check but you need to use `learning_rate` not `lr`", "I have switched to learning_rate already, thanks though", "@nsorros,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!"]}, {"number": 44171, "title": "Specify -mfpu for TARGET_ARCH=cortex-m4+fpu", "body": "The steps to reproduce are comlicated and rely on internal systems but\nthis can likely be reproduced on a sparkfun edge as well. See #44170 for\nmore details.\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils @freddan80 let us know what you think.", "Looks good! Some more changes may be needed. We're looking into the entire file with our compiler experts. @mansnils will provide an update once we know more.", "Thanks @freddan80 and @mansnils. We'll merge this PR as is since it unblocks the M4F case. We can close the issue once @mansnils has an update for all the other target architectures as well."]}, {"number": 44169, "title": "[Intel MKL] Update legacy APIs and reenable mkl_dequantize_op_test", "body": "Small fix to update legacy API in `mkl_dequantize_op_test.cc` and reenable it.\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["@penpornk There's a MacOS building error and I pushed a new commit to fix it, please take a look, thanks!"]}, {"number": 44168, "title": "Create Merge mail_ approve plz", "body": "Merge mail code", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44168) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n>  **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44168) for more info**.\r\n\r\n@googlebot I signed it", "@googlebot I signed it", "This is spam. Please don't spam."]}, {"number": 44164, "title": "Ragged arange?", "body": "I am sorry in advance if a solution for this exists. I am wondering if there is a way to create a ragged `arange` along the axis of an existing ragged tensor?\r\n\r\nFor instance, imagine I have an arbitrarily nested ragged tensor `x` I need to perform masking on. Something like:\r\n\r\n```python\r\nx = tf.ragged.constant([\r\n    [[12, 9], [5]],\r\n    [[10], [6, 8], [42]],\r\n])\r\n```\r\n\r\nThe easiest way for me to mask will be by index of an element. Is there a way to get a ragged arange with the same row lengths/splits like:\r\n\r\n```python\r\nx = tf.ragged.constant([\r\n    [[0, 1], [2]],\r\n    [[0], [1, 2], [3]],\r\n])\r\n```\r\n\r\nIt would be really cool to be able to do something like `tf.ragged.arange_along_axis(x, axis=2)` to get something like this.", "comments": ["For a rank-2 ragged tensor (like you have in your example), you can use:\r\n\r\n```python\r\ntf.ragged.range(x.row_lengths())\r\n```\r\n\r\nIf you have a higher-rank ragged tensor and want to specify the axis, you could use something like this:\r\n\r\n```python\r\ntf.ragged.map_flat_values(tf.ragged.range, x.row_lengths(axis=2))\r\n```\r\n\r\nE.g., the following snippet:\r\n\r\n```python\r\nx = tf.ragged.constant([\r\n    [ [[12, 9], [5]], \r\n      [[10], [20]]         ],\r\n    [ [[10], [6, 8], [42]] ],\r\n])\r\n\r\nfor axis in range(1, 4):\r\n  print('axis', axis)\r\n  print(tf.ragged.map_flat_values(tf.ragged.range, x.row_lengths(axis=axis)))\r\n```\r\n\r\nprints:\r\n\r\n```\r\naxis 1\r\n<tf.RaggedTensor [[0, 1], [0]]>\r\naxis 2\r\n<tf.RaggedTensor [[[0, 1], [0, 1]], [[0, 1, 2]]]>\r\naxis 3\r\n<tf.RaggedTensor [[[[0, 1], [0]], [[0], [0]]], [[[0], [0, 1], [0]]]]>\r\n```", "Hey thanks for the response. This is helpful, although it doesn't _quite_ solve what I'm after. The desired output for rank 3 would be:\r\n\r\n```python\r\n<tf.RaggedTensor [[[[0, 1], [2]], [[3], [4]]], [[[0], [1, 2], [3]]]]>\r\n```\r\n\r\nFor rank 2, it would be:\r\n\r\n```python\r\n<tf.RaggedTensor [[[0, 1], [2, 3]], [[0, 1, 2]]]>\r\n```\r\n\r\nSo for rank 3 the arange would populate elements on axis 3, but would need to increment along axis 2, if that makes any sense. Again, I appreciate your response.", "Actually I think I found a way to do this.\r\n\r\n```python\r\nx = tf.ragged.constant([\r\n    [[12, 9], [5]],\r\n    [[10], [6, 8], [42]],\r\n])\r\n\r\nx_2d = x.merge_dims(inner_axis=-1, outer_axis=1)\r\narange_2d = tf.ragged.range(x_2d.row_lengths())\r\narange_nd = tf.RaggedTensor.from_nested_row_lengths(\r\n    arange_2d.flat_values,\r\n    x.nested_row_lengths(),\r\n)\r\n\r\n>>> arange_nd\r\n<tf.RaggedTensor [[[0, 1], [2]], [[0], [1, 2], [3]]]>\r\n```\r\n\r\nAlthough I have no idea how expensive the merge operation is if it's used solely for this masking purpose.", "That should be possible to do as well, though it's a little trickier.  Here's some code to get you started:\r\n\r\n```\r\nx = tf.ragged.constant([\r\n    [[[12, 9], [5]], \r\n     [[10], [20]]],\r\n    [[[10], [6, 8], [42]]],\r\n])\r\n\r\n# Flatten all but the outermost dimension of x.\r\nx_flat = x.merge_dims(1, -1)\r\n\r\n# Get the number of scalar values in each row of x\r\nx_row_sizes = x_flat.row_lengths()\r\n\r\n# Build a list of indices, corresponding 1:1 with the flat_valeus of `x`.\r\nindices = tf.ragged.range(x_flat.row_lengths()).values\r\n\r\n# Replace x's values with the computed indices.\r\nx.with_flat_values(indices)\r\n```", "Looks like we came up with near-identical solutions within ~15 seconds of one another.  Looks good to me. :)", "Thanks again for all your help. Your solution exposed me to some interesting behaviors in the ragged API with which I was not familiar!"]}, {"number": 44163, "title": "installation issue keras-preprocessing", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nERROR: Could not find a version that satisfies the requirement keras-preprocessing<1.2,>=1.1.1 (from tensorflow==2.3.1) (from versions: none)\r\nERROR: No matching distribution found for keras-preprocessing<1.2,>=1.1.1 (from tensorflow==2.3.1)\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mgood2,\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "Also, I was able to install the keras-preprocessing package without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/08af1a7cdfc4c1d33c9a37cd4c5502fd/44163.ipynb). \r\n\r\nPlease update pip to the latest version using the below command and check if you are still facing the same issue.\r\n\r\n`pip install --upgrade pip`\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44163\">No</a>\n"]}, {"number": 44161, "title": "Downloading pretrained Efficient Det in google colab using TensorFlow Object Detection Api gives a series of unknown warnings?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI have created a notebook that downloads pretrained model from tf2 detection model zoo and performs prediction using saved model . The notebook is working fine on other models such as centernet, faster rcnn etc. But whenever I download efficient det using the download function below it issues a series of warnings.\r\n\r\nMY DOWNLOAD FUNCTION\r\n```\r\ndef download_model(model_name):\r\n  \r\n   \r\n  download_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\r\n  \r\n  \r\n  pretrained_model = model_name\r\n  \r\n  \r\n  model_file = pretrained_model + '.tar.gz'\r\n\r\n  \r\n  \r\n      \r\n  !wget {download_url + model_file}\r\n  tar = tarfile.open(model_file)\r\n  tar.extractall()\r\n  tar.close\r\n\r\n    \r\n  model_dir= os.getcwd() + '/' + pretrained_model        # determine the path to saved model \r\n  model_dir=pathlib.Path(model_dir)/'saved_model'\r\n  model= tf.saved_model.load(str(model_dir))\r\n\r\n  return model\r\n\r\n```\r\nThe warnings I encounter:\r\n```\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_119744) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_132687) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference___call___37213) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\nWARNING:tensorflow:Importing a function (__inference_EfficientDet-D1_layer_call_and_return_conditional_losses_124597) with ops with custom gradients. Will likely fail if a gradient is requested.\r\n```\r\n\r\n**Describe the expected behavior**\r\nEventhough my Efficient Det Model runs ok, I don't understand why is this warning being issued and how to get rid of it. I am making a notebook tutorial for others to understand TFOD but these warnings make my notebook look messy. Is there anyway I can get rid of those?\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nThe warnings occur while using this code:\r\n```\r\nmodel_name= 'efficientdet_d1_coco17_tpu-32'\r\nmodel= download_model(model_name)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF nightly version(`2.4.0-dev20201019 `) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e0fb22429d6318829485dd4155e5d8e6/untitled473.ipynb).Thanks!", "I think is a dup of https://github.com/tensorflow/tensorflow/issues/40166 /cc @andresp", "#40166 has an error issue. My notebook is not having any error. I am only asking how to suppress those warnings.\r\n", "@AshTech25 Are you training/fine-tuning the model?", "As mentioned before, we don't support custom gradients - but there are too many warnings there, we'll work on making them less verbose.", "> @AshTech25 Are you training/fine-tuning the model?\r\n\r\nNo I'm just using the model for my inferences\r\n", "> > @AshTech25 Are you training/fine-tuning the model?\n> \n> No I'm just using the model for my inferences\n> \n\nSo without the training you will not got the error but the warning has the same origin", "> As mentioned before, we don't support custom gradients - but there are too many warnings there, we'll work on making them less verbose.\r\n\r\nI observe the same with EfficientNetBX from `tf.keras.applications`. Still, I do not get an error when finetuning the model. Is the gradient computation failing silently?", "@AshTech25 I think this was resolved in `tf-nightly`. I cannot see all those warning except few `WARNING:absl:Importing a function` . Please check the gist for the warnings. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/40f945a440ab3385d041f31db140c5b1/untitled473.ipynb).\r\n\r\nPlease note that these updates didn't propagate to `TF2.4rc4` so the updates will be available in `tf-nightly` and future stable `TF2.5` but not in stable `TF2.4`. Thanks!\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44161\">No</a>\n"]}, {"number": 44160, "title": "Normalization.adapt() not working on tf.data.Dataset()", "body": "- TensorFlow version: 2.3.0\r\n- Python version: 3.7.6\r\n\r\nI am trying to use Normalization within my image classification model [ 224x224x3 shaped images, 2 classes with categorical (one hot) labels]. I have built a tf.data.Dataset through the tf.keras.preprocessing.image_dataset_from_directory() function.\r\nCalling the element_spec() method on the resulting dataset results in:\r\n\r\n`(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\r\n TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))`\r\n\r\nWhich is consistent with what I want. However upon using Normalization.adapt() on this Dataset I get the following error:\r\n\r\n`ValueError: as_list() is not defined on an unknown TensorShape.`\r\n\r\nI couldn't find the solution to this anywhere. Any help is deeply appreciated. If I understand correctly from the documentation the adapt() method should be able to take tf.data.Datasets as inputs, right?", "comments": ["Do you have a very minimal but runnable (copy, paste and run) example to reproduce this?", "Sure! Here you go. This code should reproduce the issue entirely. I just created a sample dataset of np.ones in the same shape of my data:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\r\n\r\nsample_dataset = tf.data.Dataset.from_tensor_slices((np.ones((425, 224, 224, 3)), np.ones((425, 2))))\r\nnormalizer = preprocessing.Normalization()\r\nnormalizer.adapt(sample_dataset)\r\n```", "I think that you need to use something like:\r\n```\r\nfeature_ds = sample_dataset.map(lambda x, y: x)\r\nnormalizer = preprocessing.Normalization()\r\nnormalizer.adapt(feature_ds)\r\n```", "This solved the problem! Thank you so much.\r\nMaybe this could be added to the documentation at https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization ?\r\nI'm not sure why the map step is necessary, but in either case it does solve the issue. Thanks once again!", "> This solved the problem! Thank you so much.\n> Maybe this could be added to the documentation at https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization ?\n> I'm not sure why the map step is necessary, but in either case it does solve the issue. Thanks once again!\n\nCan you open a new documentation issue? I think that probably we could also raise an exception.", "/cc @MarkDaoust What do you think?", "> I'm not sure why the map step is necessary,\r\n\r\nBecause you're passing a pair of tensors. The normalization layer doesn't want to guess which tensor's statistics you're trying to extract.\r\n\r\n> Can you open a new documentation issue? I think that probably we could also raise an exception.\r\n> /cc @MarkDaoust What do you think?\r\n\r\nI think it should just throw a clearer error. I think I've got a fix.", "If you data that is not 1d, i.e. image data, then this works:\r\n```\r\nx_shape = sample_dataset.element_spec[0].shape\r\nfeature_ds = sample_dataset.map(lambda x, y: tf.reshape(x, x_shape))\r\nnormalizer = preprocessing.Normalization()\r\nnormalizer.adapt(feature_ds)\r\n```"]}, {"number": 44159, "title": "Incompatibility between versions of TF and CUDA dynamic libraries", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise 64 bit, version 10.0.17763\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tf-nightly-gpu \r\n- TensorFlow version: tf-nightly-gpu 2.4.0.dev20201019\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: Pip (in conda env)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.1/cuDNN 8.4.0.30\r\n- GPU model and memory: NVIDIA Quadro P5000 (16 GB)\r\n\r\n\r\n**Describe the problem**\r\nI installed MS Visual Studio Community 2019, CUDA (express installation) and, cuDNN, as per the respective instructions.\r\nI created a new conda env with Python 3.8.6 and activated it. I installed tf-nightly-gpu using pip. \r\nI launched Python and imported tensorflow, then listed GPU devices: all DLLs were found but one (cusolver64). The version of the library looked for by TF is 10 while CUDA 11.1 installs version 11.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`>>> import tensorflow as tf\r\n2020-10-19 17:17:23.185914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-10-19 17:17:44.311308: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-19 17:17:44.320150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-10-19 17:17:44.361274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:65:00.0 name: Quadro P5000 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 16.00GiB deviceMemoryBandwidth: 269.00GiB/s\r\n2020-10-19 17:17:44.373653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-19 17:17:44.392056: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-19 17:17:44.397830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-10-19 17:17:44.407985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-19 17:17:44.417194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-10-19 17:17:44.424211: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-10-19 17:17:44.433236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-10-19 17:17:44.440903: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-10-19 17:17:44.445862: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I installed CUDA 10.2 over 11.1 and the problem seems to be resolved:\r\n\r\n```python      \r\nPython 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 18:22:52) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-10-19 22:06:00.388483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-10-19 22:06:07.953857: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-19 22:06:07.963030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-10-19 22:06:07.998966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:65:00.0 name: Quadro P5000 computeCapability: 6.1\r\ncoreClock: 1.7335GHz coreCount: 20 deviceMemorySize: 16.00GiB deviceMemoryBandwidth: 269.00GiB/s\r\n2020-10-19 22:06:08.010311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-19 22:06:08.029107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-19 22:06:08.036516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-10-19 22:06:08.049843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-19 22:06:08.057249: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-10-19 22:06:08.068477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-19 22:06:08.078004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-10-19 22:06:08.084994: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-10-19 22:06:08.089183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]     \r\n```         ", "@gdrolet \r\nPlease move the issue to closed status as it is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44159\">No</a>\n", "Closed as resolved", "@gdrolet Could you please provide the cusolver64_10.dll file. I have the same issue and will have to download the whole cuda toolkit but I have been facing some bandwidth issues lately. It would be a huge help.", "@Akhilesh64 DLL is here until 11/12/2020:\r\nhttps://transfert.mffp.gouv.qc.ca/?ShareToken=790B76CF0EBA6B094F39CF985247901B63D4AD19\r\n\r\n\r\n", "@gdrolet You're a lifesaver thanks. ", "@gdrolet you are a hero man", "While I am thankful for @gdrolet for providing a \"solution\" I do not like it one bit, because I simply do not understand how this solves the issue, also this is not really a \"solution\" but an incredible dirty one at best. \r\n\r\nI am using a similar setup as the issue starter, however I am using a RTX 3090, which [*should* be incompatible with CUDA 10 and lower](https://medium.com/@dun.chwong/the-simple-guide-deep-learning-with-rtx-3090-cuda-cudnn-tensorflow-keras-pytorch-e88a2a8249bc). So I installed CUDA 11.1, the latest cuDnn version (all on Windows), switched to python 3.8 and the latest tf-nightly-gpu version (from today. November 10th). \r\n\r\nI encountered the same issue as the OP, and I thought about the different methods described in similar issues (renaming the \\*_11.dll file that to the \"10\" version, downgrading although that should not work, etc), but then I simply decided to install 10.2 over the newer version. \r\n\r\n**This however WILL change the CUDA_PATH to the 10.2 version instead of the 11.1. The installation that does not have cudnn installed.** In essence, tensorflow should have problems (it is incompatible with the RTX 3000 series GPUs and cudnn is missing). But it still does work. But I tested it with a few simple GAN training loops and everything looks in order. I did some  testing (deleting the DLLs that need to be loaded, sometimes from cuda 11, sometimes from version 10), because I know nothing about how loads the necessary dlls and because I thought the CUDA_PATH matters. Interestingly it does not. My TF installation still loads the DLLs from CUDA 11.1 (CUDA_PATH_V11.1) and then automatically goes into the 10.2 folder (CUDA_PATH_V10.2) to load the solver DLL. \r\n\r\nFor my own sanity's sake, I set the (normal) CUDA_PATH to version 11.1 because I am afraid other tools I use might use the wrong version.\r\n\r\n\r\nAlso @Saduf2019 do you know when this is actually solved for real, meaning it looks for the correct cusolver64_11.dll instead of the 10 version? This solution is not a clean one and it might cause problems down the line. When you have [blog posts such as that one](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf) struggling with TF on RTX 3000, one gets worried that their models and computations are not 100% correct or might produce giberish. \r\n\r\n", "I noticed that the cusolver64_10.dll file transfer expired. Can you repost it @gdrolet?", "@linuxmaster0312 here you go:\r\n\r\n https://drive.google.com/file/d/1-3Yk-QZ1eUta1T40BaxFpO4uyn7BPU4o", "~~for other people who may come across this and have v11 installed, I know it may not be a good solution but making a copy of cusolver64_11.dll and renaming it to cusolver64_10.dll seems to be working fine for me.~~ Edit: don't", "@jd1378  are you able to use tensorflow normally with your gpu?", "no, it seems it was only working with a limited case, tried with another and it throws error. don't use it", "Tried the cusolver64_10.dll  workaround which allowed me to import tensorflow nightly gpu and the system recognizes my GPu, but trying to run through the simple train_and_evaluate example notebook crashes with:\r\n\r\n```\r\nInternalError:  Blas xGEMM launch failed : a.shape=[1,64,784], b.shape=[1,784,64], m=64, n=64, k=784\r\n\t [[node model/dense_1/MatMul (defined at <ipython-input-6-69417c50baa2>:2) ]] [Op:__inference_train_function_706]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nand in the jupyter console:\r\n``` Successfully opened dynamic library cublas64_11.dll\r\n2020-11-26 11:02:59.365169: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2020-11-26 11:02:59.365208: W tensorflow/stream_executor/stream.cc:1455] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n[I 11:03:15.022 NotebookApp] Saving file at /Tensorflow/train_and_evaluate.ipynb\r\n```\r\n\r\nThis is a 3070 with cuda 11.1 and the tensorflow-nightly-gpu from pip (windows 10, python 3.8)\r\n\r\n", "@astrowonk add this after importing all the libraries in your code\r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)` ", "For folks trying to use cuda 11.1, we recommend sticking to CUDA 11.0. That's the version we've built & tested against. ", "> For folks trying to use cuda 11.1, we recommend sticking to CUDA 11.0. That's the version we've built & tested against.\r\n\r\n CUDA 10 is too old for the latest Ampere cards, no? I was trying to test running on a gpu with a 3070. ", "@pkanwar23 that is not fully compatible  with rtx 30", "@gdrolet \r\nPlease update if this is still an issue", "@Saduf2019 The issue still persists.", "I installed CUDA 11.2 and its respective cuDNN. When running:\r\n`import tensorflow as tf\r\ntf.config.list_physical_devices('GPU')`\r\nThe .dll that is missing is cusolver64_10.dll. The CUDA installation doesn't have that .dll, it only has the *_11.dll. What I did was install the CUDA 10.2, leaving my CUDA_PATH pointing to my 11.2 installation and copied the file 'cusolver64_10.dll' from my CUDA 10.2 installation path; Which on Windows could be \"%USERPROFILE%/Program files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/bin/\". \r\n\r\nI'm not sure this solution is optimal, in the sense that would break things in your programs or models.\r\n\r\nI would like to know if this solves your problem and is a good solution that won't break things.", "@carlosscmx This should work. I did it in a similar way. My models (using custom training loops / steps) worked fine as well, nothing of note was encountered. ", "Exactly the same issue, haven't been able to try @carlosscmx  workaround.\r\nCUDA 11.2 \r\ncuDNN 8.  (tried a few)\r\nTensorflow 2.4", "I have  Windows 10 x64, CUDA 11.2, cuDNN 8, Tensorflow 2.4\r\nI also have some problem\r\nI solved it with copied file cusolver64_10.dll into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin", "Thanks @fshofmann copying the cusolver64_10.dll into C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin instantly solved it for me as well (without installing the dev version of tf-nightly gpu)", "> For folks trying to use cuda 11.1, we recommend sticking to CUDA 11.0. That's the version we've built & tested against.\r\n\r\nI have a windows machine and tried to download CUDA 11.0 and CUDA 11.0-updated but the download links are broken (I could not download any of them). That is why I had to install CUDA 11.1 instead and I ran into the same issue as the others here.", "> I have a windows machine and tried to download CUDA 11.0 and CUDA 11.0-updated but the download links are broken (I could not download any of them).\r\n\r\nCan you reach out to NVIDIA about this?  CC @nluehr ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44159\">No</a>\n", "> While I am thankful for @gdrolet for providing a \"solution\" I do not like it one bit, because I simply do not understand how this solves the issue, also this is not really a \"solution\" but an incredible dirty one at best.\r\n> \r\n> I am using a similar setup as the issue starter, however I am using a RTX 3090, which [_should_ be incompatible with CUDA 10 and lower](https://medium.com/@dun.chwong/the-simple-guide-deep-learning-with-rtx-3090-cuda-cudnn-tensorflow-keras-pytorch-e88a2a8249bc). So I installed CUDA 11.1, the latest cuDnn version (all on Windows), switched to python 3.8 and the latest tf-nightly-gpu version (from today. November 10th).\r\n> \r\n> I encountered the same issue as the OP, and I thought about the different methods described in similar issues (renaming the *_11.dll file that to the \"10\" version, downgrading although that should not work, etc), but then I simply decided to install 10.2 over the newer version.\r\n> \r\n> **This however WILL change the CUDA_PATH to the 10.2 version instead of the 11.1. The installation that does not have cudnn installed.** In essence, tensorflow should have problems (it is incompatible with the RTX 3000 series GPUs and cudnn is missing). But it still does work. But I tested it with a few simple GAN training loops and everything looks in order. I did some testing (deleting the DLLs that need to be loaded, sometimes from cuda 11, sometimes from version 10), because I know nothing about how loads the necessary dlls and because I thought the CUDA_PATH matters. Interestingly it does not. My TF installation still loads the DLLs from CUDA 11.1 (CUDA_PATH_V11.1) and then automatically goes into the 10.2 folder (CUDA_PATH_V10.2) to load the solver DLL.\r\n> \r\n> For my own sanity's sake, I set the (normal) CUDA_PATH to version 11.1 because I am afraid other tools I use might use the wrong version.\r\n> \r\n> Also @Saduf2019 do you know when this is actually solved for real, meaning it looks for the correct cusolver64_11.dll instead of the 10 version? This solution is not a clean one and it might cause problems down the line. When you have [blog posts such as that one](https://dobromyslova.medium.com/making-work-tensorflow-with-nvidia-rtx-3090-on-windows-10-7a38e8e582bf) struggling with TF on RTX 3000, one gets worried that their models and computations are not 100% correct or might produce giberish.\r\n\r\n@fshofmann  Have you tried to train you model under this configuration? Does this works fine?\r\n\r\n", "@NoahCSHN I faced the same issue and my models are pretty good. I can testify for that. I came looking for the issue everywhere. Whatever works, if it works its fine. Thanks to everyone.", "@NoahCSHN Sorry for the *very* late reply (github issue notifications get easily buried in the inbox), but for other readers' sake I can very much confirm that is does work indeed fine [(as mentioned 4 months ago)](https://github.com/tensorflow/tensorflow/issues/44159#issuecomment-749482971). Since last year I've done extensive evaluations of some of my models that were build using the sequential and functional keras API (custom training loops, my subclassed custom models were a mess due to other reasons) and the results do match what I expected.\r\n\r\nAt the moment I am a bit out of the loop when it comes to this issue, but I sure hope that we will get a clean solution this year :) ", "any updates? it's been 2 years after all. proposed solutions are better than nothing, but their hackiness level is not on par with the prestige TF+NVIDIA have in the ML world"]}, {"number": 44157, "title": "Can't allocate specific number of CPU threads when executing in graph mode", "body": "I'm working on some benchmarking code to determine FPS depending on number of CPU cores. It seems like it isn't possible to select a certain number of CPU threads when running inference in graph mode. I can, however, select a specific number of threads when running inference with eager execution. I confirm this by running the script in the background and watching CPU allocation with 'htop' or 'top.'\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS \r\n- TensorFlow installed from (source or binary): pip 20.2.3\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.6.9\r\n- CUDA/cuDNN version: release 9.0, V9.0.176\r\n- GPU model and memory: N/A\r\n- CPU Model: Intel(R) Core(TM) i5-8400 CPU @ 2.80GHz, \r\n\r\n\r\n**Describe the current behavior**\r\nWhen running in graph execution, only a single CPU core is being used (according to htop) regardless of how many threads I choose. I would expect a higher FPS when more threads are selected, however, I get the same FPS whenever eager mode is disabled. I believe graph execution is ignoring the thread variables. When running in eager execution, the correct number of threads are being used. I get different FPS for different numbers of threads.\r\n\r\n**Describe the expected behavior**\r\nWhen running in graph execution, I expect the script to use as many threads as I explicitly tell it to use. Further, I would expect to see a change in FPS when selecting a different number of threads.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n<pre><code>import tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\r\n\r\nITERATIONS = 100\r\nBATCH_SIZE = 1\r\nINTER_THREADS = 1\r\nINTRA_THREADS = 1\r\n\r\n\"Comment for eager execution (graph mode). Uncomment for Graph mode.\"\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(INTER_THREADS)\r\ntf.config.threading.set_intra_op_parallelism_threads(INTRA_THREADS)\r\n\r\n\r\n@tf.function\r\ndef inference(net, input_data):\r\n    return net(input_data, training=False)\r\n\r\n\r\nmodel = tf.keras.applications.ResNet50(weights=None, classes=10)\r\ndummy_batch = tf.random.uniform([BATCH_SIZE,224,224,3], dtype=tf.dtypes.float32)\r\n\r\n\r\ni = 0\r\ntime_spent = np.zeros(ITERATIONS)\r\nwhile (i < ITERATIONS + 1):\r\n  if i != 0:\r\n       \"clocking inference speeds\"\r\n       start_time = time.time()\r\n       _ = inference(model, dummy_batch)\r\n       time_spent[i-1] = time.time() - start_time\r\n  else:\r\n       \"warm up cpu / gpu\"\r\n       while(i < 50):\r\n           _ = inference(model, dummy_batch)\r\n           i+=1\r\n       i = 0\r\n  i += 1\r\n\r\n\r\nfps = (len(time_spent) * BATCH_SIZE) / np.sum(time_spent)\r\nprint(\"FPS: {}\".format(fps))\r\n</code></pre>\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**Output with eager execution disabled 1 inter thread, 1 intra thread\r\nFPS: 162.246247505461\r\n\r\n**Output with eager execution disabled 1 inter thread, 2 intra thread\r\nFPS: 162.3569705471583\r\n\r\n**Output with eager execution enabled 1 inter thread, 1 intra thread\r\nFPS: 10.065427737674705\r\n\r\n**Output with eager execution enabled 1 inter thread, 2 intra thread\r\nFPS: 17.2407394877956\r\n", "comments": ["I have tried in colab with TF version 2.3 ,nightly version (`2.4.0-dev20201019`) and was able to reproduce the issue.Please, find the gist in [graph mode](https://colab.research.google.com/gist/ravikyram/8bbcf4a68ad40cdccbc5645ef2a2829c/untitled470.ipynb),[eager mode](https://colab.research.google.com/gist/ravikyram/ee983fe7b6605c41059d5f741a0e743e/untitled471.ipynb) here.\r\nThanks!", "Hi @daniel-merrick, in TF2 you leverage graph mode by enabling tf.function and not by disabling eager execution. `tf.compat.v1.disable_eager_execution()` enables legacy graph mode, which you should not do unless you are doing something very specific with TF1 compat code that requires legacy graph.\r\n\r\nIn the code sample you have provided, if you comment out `tf.compat.v1.disable_eager_execution()` then you are running in graph mode because you are making use of tf.function.", "Ah, thank you for the clarification! I still have a few questions though.\r\n\r\nWhat is the explanation for the FPS difference, then? When I don't have tf.compat.v1.disable_eager_execution() commented in the above code, is this similar to running in TF1 (in regard to performance)?\r\n\r\nand\r\n\r\nIf I wanted to benchmark using legacy graph mode, the problem still exists that the threading isn't being properly respected, right?", "I'm not sure you would want to benchmark with the legacy graph mode, since using `@tf.function` is the recommended way to leverage graphs in TF2. So yes commenting out `tf.compat.v1.disable_eager_execution()` and then using `tf.function` is similar to running in TF1. Something else to keep in mind is that when using the Keras API, model.predict() manages `tf.function` for you automatically under the hood.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44154, "title": "\"WARNING:tensorflow:Model failed to initialize as JSON.Ignoring \"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n# **System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Scientific Linux 7\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\nTensorFlow installed from (source or binary):binary\r\nTensorFlow version:Desktop\r\nPython version:3.6.2\r\nInstalled using virtualenv? pip? conda?: virtualenv and pip\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory:\r\n\r\n# **Describe the problem**\r\nI am working in a remote server, creating a virtual environment on one shell with gpu.\r\nI am updating a package for ObjectDetection, which in its original repo uses outdated tf 1.13.1, to tf 2.2.\r\n\r\nNow i managed to make it train (yuhu!) but it seems to have issues in saving, since i get the following error message:\r\n\r\n**\"WARNING:tensorflow:Model failed to initialize as JSON.Ignoring \"**\r\n\r\nplus also these ones (which seem to be ignorable for now, but still..)\r\n\"WARNING:tensorflow:'period' argument is deprecated.Please use 'save_freq' ect\"\r\n\"WARNING:tensorflow:'epsilon' argument is deprecated. Please use 'min_delta' etc\"\r\n\r\ni believe the problem is somewhere in the def \"setTrainConfig\"  attached at the end) but I cant figure it out how to solve it.\r\n\r\n# **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n-------------------------------------------------------------------------------------------------------------------------\r\n\r\n**TF 2.2**\r\npip install --upgrade pip\r\npython3 -m pip install tensorflow-gpu==2.2.0\r\npython3 -m pip install opencv-python\r\npython3 -m pip install keras\r\nmodule load cuda/10.1\r\npython3 -m pip install ImageAI-master/ # editedfrom package\r\npython3 main.py\r\n-------------------------------------------------------------------------------------------------------------------------\r\n## **main.py**\r\nfrom imageai.Detection.Custom import DetectionModelTrainer\r\nimport os\r\n\r\nINPUT_DIR = \"/zhome/94/5/101974/Desktop/data/hololens/\"\r\n\r\ntrainer = DetectionModelTrainer()\r\ntrainer.setModelTypeAsYOLOv3()\r\ntrainer.setDataDirectory(data_directory=INPUT_DIR)\r\ntrainer.setTrainConfig(object_names_array=[\"hololens\", \"oculus\"], batch_size=32, num_experiments=1, train_from_pretrained_model=\"pretrained-yolov3.h5\")\r\ntrainer.trainModel()\r\n\r\n\"\"\" evaluate performance\"\"\"\r\ntrainer = DetectionModelTrainer()\r\ntrainer.setModelTypeAsYOLOv3()\r\ntrainer.setDataDirectory(data_directory=INPUT_DIR)\r\nmetrics = trainer.evaluateModel(model_path=INPUT_DIR+\"models\", json_path=\"hololens/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)\r\nprint(metrics)\r\n\r\n\r\n\r\n# **Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n---------------------------------------------------------------------------------------------------------------------------------------------\r\n## **yolo.py file**\r\nfrom keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda\r\nfrom keras.layers.merge import add, concatenate\r\nfrom keras.models import Model\r\nfrom keras.engine.topology import Layer\r\nimport tensorflow as tf\r\n\r\n\"\"\" \r\nARI MOD \r\nhttps://stackoverflow.com/questions/59811781/tf-function-valueerror-creating-variables-on-a-non-first-call-to-a-function-de\r\n\"\"\" \r\n\r\nbatch_seen = None \r\n\r\nclass YoloLayer(Layer):\r\n\tdef __init__(self, anchors, max_grid, batch_size, warmup_batches, ignore_thresh, \r\n\t\t\t\t\tgrid_scale, obj_scale, noobj_scale, xywh_scale, class_scale, \r\n\t\t\t\t\t**kwargs):\r\n\t\t# make the model settings persistent\r\n\t\tself.ignore_thresh  = ignore_thresh\r\n\t\tself.warmup_batches = warmup_batches\r\n\t\tself.anchors        = tf.constant(anchors, dtype='float', shape=[1,1,1,3,2])\r\n\t\tself.grid_scale     = grid_scale\r\n\t\tself.obj_scale      = obj_scale\r\n\t\tself.noobj_scale    = noobj_scale\r\n\t\tself.xywh_scale     = xywh_scale\r\n\t\tself.class_scale    = class_scale        \r\n\r\n\t\t# make a persistent mesh grid\r\n\t\tmax_grid_h, max_grid_w = max_grid\r\n\t\t\r\n\t\t\"\"\" \r\n                ARI EDIT\r\n\t\thttps://github.com/tensorflow/tensor2tensor/issues/1736\r\n                \"\"\"\r\n\t\tcell_x = tf.cast(tf.reshape(tf.tile(tf.range(max_grid_w), [max_grid_h]), (1, max_grid_h, max_grid_w, 1, 1)), dtype = tf.float32)\r\n\t\tcell_y = tf.transpose(cell_x, (0,2,1,3,4))\r\n\t\tself.cell_grid = tf.tile(tf.concat([cell_x,cell_y],-1), [batch_size, 1, 1, 3, 1])\r\n\r\n\t\tsuper(YoloLayer, self).__init__(**kwargs)\r\n\r\n\tdef build(self, input_shape):\r\n\t\tsuper(YoloLayer, self).build(input_shape)  # Be sure to call this somewhere!\r\n\r\n\r\n         \"\"\"\r\n\tARI MOD\r\n\thttps://stackoverflow.com/questions/59811781/tf-function-valueerror-creating-variables-on-a-non-first-call-to-a-function-de\r\n\"\"\"\r\n\tdef call(self, x):\r\n\t\tinput_image, y_pred, y_true, true_boxes = x\r\n\r\n\t\t# adjust the shape of the y_predict [batch, grid_h, grid_w, 3, 4+1+nb_class]\r\n\t\ty_pred = tf.reshape(y_pred, tf.concat([tf.shape(y_pred)[:3], tf.constant([3, -1])], axis=0))\r\n\t\t\r\n\t\t# initialize the masks\r\n\t\tobject_mask     = tf.expand_dims(y_true[..., 4], 4)\r\n\r\n\t\t# the variable to keep track of number of batches processed\r\n\t\tglobal batch_seen\r\n\t\tif batch_seen is None:\r\n\t\t\tbatch_seen = tf.Variable(0.)\r\n\t\t\t\r\n\t\t# compute grid factor and net factor\r\n\t\tgrid_h      = tf.shape(y_true)[1]\r\n\t\tgrid_w      = tf.shape(y_true)[2]\r\n\t\tgrid_factor = tf.reshape(tf.cast([grid_w, grid_h], tf.float32), [1,1,1,1,2])\r\n\r\n\t\tnet_h       = tf.shape(input_image)[1]\r\n\t\tnet_w       = tf.shape(input_image)[2]            \r\n\t\tnet_factor  = tf.reshape(tf.cast([net_w, net_h], tf.float32), [1,1,1,1,2])\r\n\t\t\r\n\t\t\"\"\"\r\n\t\tAdjust prediction\r\n\t\t\"\"\"\r\n\t\tpred_box_xy    = (self.cell_grid[:,:grid_h,:grid_w,:,:] + tf.sigmoid(y_pred[..., :2]))  # sigma(t_xy) + c_xy\r\n\t\tpred_box_wh    = y_pred[..., 2:4]                                                       # t_wh\r\n\t\tpred_box_conf  = tf.expand_dims(tf.sigmoid(y_pred[..., 4]), 4)                          # adjust confidence\r\n\t\tpred_box_class = y_pred[..., 5:]                                                        # adjust class probabilities      \r\n\r\n\t\t\"\"\"\r\n\t\tAdjust ground truth\r\n\t\t\"\"\"\r\n\t\ttrue_box_xy    = y_true[..., 0:2] # (sigma(t_xy) + c_xy)\r\n\t\ttrue_box_wh    = y_true[..., 2:4] # t_wh\r\n\t\ttrue_box_conf  = tf.expand_dims(y_true[..., 4], 4)\r\n\t\ttrue_box_class = tf.argmax(y_true[..., 5:], -1)         \r\n\r\n\t\t\"\"\"\r\n\t\tCompare each predicted box to all true boxes\r\n\t\t\"\"\"        \r\n\t\t# initially, drag all objectness of all boxes to 0\r\n\t\tconf_delta  = pred_box_conf - 0 \r\n\r\n\t\t# then, ignore the boxes which have good overlap with some true box\r\n\t\ttrue_xy = true_boxes[..., 0:2] / grid_factor\r\n\t\ttrue_wh = true_boxes[..., 2:4] / net_factor\r\n\t\t\r\n\t\ttrue_wh_half = true_wh / 2.\r\n\t\ttrue_mins    = true_xy - true_wh_half\r\n\t\ttrue_maxes   = true_xy + true_wh_half\r\n\t\t\r\n\t\tpred_xy = tf.expand_dims(pred_box_xy / grid_factor, 4)\r\n\t\tpred_wh = tf.expand_dims(tf.exp(pred_box_wh) * self.anchors / net_factor, 4)\r\n\t\t\r\n\t\tpred_wh_half = pred_wh / 2.\r\n\t\tpred_mins    = pred_xy - pred_wh_half\r\n\t\tpred_maxes   = pred_xy + pred_wh_half    \r\n\r\n\t\tintersect_mins  = tf.maximum(pred_mins,  true_mins)\r\n\t\tintersect_maxes = tf.minimum(pred_maxes, true_maxes)\r\n\r\n\t\tintersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\r\n\t\tintersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n\t\t\r\n\t\ttrue_areas = true_wh[..., 0] * true_wh[..., 1]\r\n\t\tpred_areas = pred_wh[..., 0] * pred_wh[..., 1]\r\n\r\n\t\tunion_areas = pred_areas + true_areas - intersect_areas\r\n\t\tiou_scores  = tf.truediv(intersect_areas, union_areas)\r\n\r\n\t\tbest_ious   = tf.reduce_max(iou_scores, axis=4) \r\n\t\t# ARI EDIT\r\n\t\t#https://github.com/tensorflow/tensor2tensor/issues/1736       \r\n\t\tconf_delta *= tf.expand_dims(tf.cast(best_ious < self.ignore_thresh, dtype=tf.float32), 4)\r\n\r\n\t\t\"\"\"\r\n\t\tCompute some online statistics\r\n\t\t\"\"\"            \r\n\t\ttrue_xy = true_box_xy / grid_factor\r\n\t\ttrue_wh = tf.exp(true_box_wh) * self.anchors / net_factor\r\n\r\n\t\ttrue_wh_half = true_wh / 2.\r\n\t\ttrue_mins    = true_xy - true_wh_half\r\n\t\ttrue_maxes   = true_xy + true_wh_half\r\n\r\n\t\tpred_xy = pred_box_xy / grid_factor\r\n\t\tpred_wh = tf.exp(pred_box_wh) * self.anchors / net_factor \r\n\t\t\r\n\t\tpred_wh_half = pred_wh / 2.\r\n\t\tpred_mins    = pred_xy - pred_wh_half\r\n\t\tpred_maxes   = pred_xy + pred_wh_half      \r\n\r\n\t\tintersect_mins  = tf.maximum(pred_mins,  true_mins)\r\n\t\tintersect_maxes = tf.minimum(pred_maxes, true_maxes)\r\n\t\tintersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\r\n\t\tintersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\r\n\t\t\r\n\t\ttrue_areas = true_wh[..., 0] * true_wh[..., 1]\r\n\t\tpred_areas = pred_wh[..., 0] * pred_wh[..., 1]\r\n\r\n\t\tunion_areas = pred_areas + true_areas - intersect_areas\r\n\t\tiou_scores  = tf.truediv(intersect_areas, union_areas)\r\n\t\tiou_scores  = object_mask * tf.expand_dims(iou_scores, 4)\r\n\t\t\r\n\t\tcount       = tf.reduce_sum(object_mask)\r\n\t\tcount_noobj = tf.reduce_sum(1 - object_mask)\r\n\t\t\"\"\" ARI MOD \"\"\"\r\n\t\tdetect_mask = tf.cast((pred_box_conf*object_mask) >= 0.5,dtype=tf.float32)\r\n\t\tclass_mask  = tf.expand_dims(tf.cast(tf.equal(tf.argmax(pred_box_class, -1), true_box_class),dtype=tf.float32), 4)\r\n\t\trecall50    = tf.reduce_sum(tf.cast(iou_scores >= 0.5 ,dtype=tf.float32) * detect_mask  * class_mask) / (count + 1e-3)\r\n\t\trecall75    = tf.reduce_sum(tf.cast(iou_scores >= 0.75 ,dtype=tf.float32) * detect_mask  * class_mask) / (count + 1e-3)    \r\n\t\tavg_iou     = tf.reduce_sum(iou_scores) / (count + 1e-3)\r\n\t\tavg_obj     = tf.reduce_sum(pred_box_conf  * object_mask)  / (count + 1e-3)\r\n\t\tavg_noobj   = tf.reduce_sum(pred_box_conf  * (1-object_mask))  / (count_noobj + 1e-3)\r\n\t\tavg_cat     = tf.reduce_sum(object_mask * class_mask) / (count + 1e-3) \r\n\r\n\t\t\"\"\"\r\n\t\tWarm-up training\r\n\t\t\"\"\"\r\n                \"\"\"\r\n\t\tARI MOD\r\n\t\thttps://github.com/tensorflow/tensorflow/issues/30468\r\n                 \"\"\"\r\n\t\tbatch_seen = tf.compat.v1.assign_add(batch_seen, 1.)\r\n\t\r\n\t\ttrue_box_xy, true_box_wh, xywh_mask = tf.cond(tf.less(batch_seen, self.warmup_batches+1), \r\n\t\t\t\t\t\t\t  lambda: [true_box_xy + (0.5 + self.cell_grid[:,:grid_h,:grid_w,:,:]) * (1-object_mask), \r\n\t\t\t\t\t\t\t\t\t   true_box_wh + tf.zeros_like(true_box_wh) * (1-object_mask), \r\n\t\t\t\t\t\t\t\t\t   tf.ones_like(object_mask)],\r\n\t\t\t\t\t\t\t  lambda: [true_box_xy, \r\n\t\t\t\t\t\t\t\t\t   true_box_wh,\r\n\t\t\t\t\t\t\t\t\t   object_mask])\r\n\r\n\t\t\"\"\"\r\n\t\tCompare each true box to all anchor boxes\r\n\t\t\"\"\"      \r\n\t\twh_scale = tf.exp(true_box_wh) * self.anchors / net_factor\r\n\t\twh_scale = tf.expand_dims(2 - wh_scale[..., 0] * wh_scale[..., 1], axis=4) # the smaller the box, the bigger the scale\r\n\r\n\t\txy_delta    = xywh_mask   * (pred_box_xy-true_box_xy) * wh_scale * self.xywh_scale\r\n\t\twh_delta    = xywh_mask   * (pred_box_wh-true_box_wh) * wh_scale * self.xywh_scale\r\n\t\tconf_delta  = object_mask * (pred_box_conf-true_box_conf) * self.obj_scale + (1-object_mask) * conf_delta * self.noobj_scale\r\n\t\tclass_delta = object_mask * \\\r\n\t\t\t\t\t  tf.expand_dims(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class), 4) * \\\r\n\t\t\t\t\t  self.class_scale\r\n\r\n\t\tloss_xy    = tf.reduce_sum(tf.square(xy_delta),       list(range(1,5)))\r\n\t\tloss_wh    = tf.reduce_sum(tf.square(wh_delta),       list(range(1,5)))\r\n\t\tloss_conf  = tf.reduce_sum(tf.square(conf_delta),     list(range(1,5)))\r\n\t\tloss_class = tf.reduce_sum(class_delta,               list(range(1,5)))\r\n\r\n\t\tloss = loss_xy + loss_wh + loss_conf + loss_class\r\n\r\n\r\n\t\treturn loss*self.grid_scale\r\n\r\n\tdef compute_output_shape(self, input_shape):\r\n\t\treturn [(None, 1)]\r\n\r\ndef _conv_block(inp, convs, do_skip=True):\r\n\tx = inp\r\n\tcount = 0\r\n\t\r\n\tfor conv in convs:\r\n\t\tif count == (len(convs) - 2) and do_skip:\r\n\t\t\tskip_connection = x\r\n\t\tcount += 1\r\n\t\t\r\n\t\tif conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # unlike tensorflow darknet prefer left and top paddings\r\n\t\tx = Conv2D(conv['filter'], \r\n\t\t\t\t   conv['kernel'], \r\n\t\t\t\t   strides=conv['stride'], \r\n\t\t\t\t   padding='valid' if conv['stride'] > 1 else 'same', # unlike tensorflow darknet prefer left and top paddings\r\n\t\t\t\t   name='conv_' + str(conv['layer_idx']), \r\n\t\t\t\t   use_bias=False if conv['bnorm'] else True)(x)\r\n\t\tif conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\r\n\t\tif conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\r\n\r\n\treturn add([skip_connection, x]) if do_skip else x        \r\n\r\ndef create_yolov3_model(\r\n\tnb_class, \r\n\tanchors, \r\n\tmax_box_per_image, \r\n\tmax_grid, \r\n\tbatch_size, \r\n\twarmup_batches,\r\n\tignore_thresh,\r\n\tgrid_scales,\r\n\tobj_scale,\r\n\tnoobj_scale,\r\n\txywh_scale,\r\n\tclass_scale\r\n):\r\n\tinput_image = Input(shape=(None, None, 3)) # net_h, net_w, 3\r\n\ttrue_boxes  = Input(shape=(1, 1, 1, max_box_per_image, 4))\r\n\ttrue_yolo_1 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\r\n\ttrue_yolo_2 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\r\n\ttrue_yolo_3 = Input(shape=(None, None, len(anchors)//6, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\r\n\r\n\t# Layer  0 => 4\r\n\tx = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\r\n\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\r\n\t\t\t\t\t\t\t\t  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\r\n\t\t\t\t\t\t\t\t  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\r\n\r\n\t# Layer  5 => 8\r\n\tx = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\r\n\t\t\t\t\t\t{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\r\n\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\r\n\r\n\t# Layer  9 => 11\r\n\tx = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\r\n\t\t\t\t\t\t{'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\r\n\r\n\t# Layer 12 => 15\r\n\tx = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\r\n\t\t\t\t\t\t{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\r\n\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\r\n\r\n\t# Layer 16 => 36\r\n\tfor i in range(7):\r\n\t\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\r\n\t\t\t\t\t\t\t{'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\r\n\t\t\r\n\tskip_36 = x\r\n\t\t\r\n\t# Layer 37 => 40\r\n\tx = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\r\n\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\r\n\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\r\n\r\n\t# Layer 41 => 61\r\n\tfor i in range(7):\r\n\t\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\r\n\t\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\r\n\t\t\r\n\tskip_61 = x\r\n\t\t\r\n\t# Layer 62 => 65\r\n\tx = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\r\n\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\r\n\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\r\n\r\n\t# Layer 66 => 74\r\n\tfor i in range(3):\r\n\t\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\r\n\t\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\r\n\t\t\r\n\t# Layer 75 => 79\r\n\tx = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\r\n\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\r\n\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\r\n\t\t\t\t\t\t{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\r\n\t\t\t\t\t\t{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], do_skip=False)\r\n\r\n\t# Layer 80 => 82\r\n\tpred_yolo_1 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\r\n\t\t\t\t\t\t\t {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], do_skip=False)\r\n\tloss_yolo_1 = YoloLayer(anchors[12:], \r\n\t\t\t\t\t\t\t[1*num for num in max_grid], \r\n\t\t\t\t\t\t\tbatch_size, \r\n\t\t\t\t\t\t\twarmup_batches, \r\n\t\t\t\t\t\t\tignore_thresh, \r\n\t\t\t\t\t\t\tgrid_scales[0],\r\n\t\t\t\t\t\t\tobj_scale,\r\n\t\t\t\t\t\t\tnoobj_scale,\r\n\t\t\t\t\t\t\txywh_scale,\r\n\t\t\t\t\t\t\tclass_scale)([input_image, pred_yolo_1, true_yolo_1, true_boxes])\r\n\r\n\t# Layer 83 => 86\r\n\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], do_skip=False)\r\n\tx = UpSampling2D(2)(x)\r\n\tx = concatenate([x, skip_61])\r\n\r\n\t# Layer 87 => 91\r\n\tx = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\r\n\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\r\n\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\r\n\t\t\t\t\t\t{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\r\n\t\t\t\t\t\t{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], do_skip=False)\r\n\r\n\t# Layer 92 => 94\r\n\tpred_yolo_2 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\r\n\t\t\t\t\t\t\t {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], do_skip=False)\r\n\tloss_yolo_2 = YoloLayer(anchors[6:12], \r\n\t\t\t\t\t\t\t[2*num for num in max_grid], \r\n\t\t\t\t\t\t\tbatch_size, \r\n\t\t\t\t\t\t\twarmup_batches, \r\n\t\t\t\t\t\t\tignore_thresh, \r\n\t\t\t\t\t\t\tgrid_scales[1],\r\n\t\t\t\t\t\t\tobj_scale,\r\n\t\t\t\t\t\t\tnoobj_scale,\r\n\t\t\t\t\t\t\txywh_scale,\r\n\t\t\t\t\t\t\tclass_scale)([input_image, pred_yolo_2, true_yolo_2, true_boxes])\r\n\r\n\t# Layer 95 => 98\r\n\tx = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], do_skip=False)\r\n\tx = UpSampling2D(2)(x)\r\n\tx = concatenate([x, skip_36])\r\n\r\n\t# Layer 99 => 106\r\n\tpred_yolo_3 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\r\n\t\t\t\t\t\t\t {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\r\n\t\t\t\t\t\t\t {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\r\n\t\t\t\t\t\t\t {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\r\n\t\t\t\t\t\t\t {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\r\n\t\t\t\t\t\t\t {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\r\n\t\t\t\t\t\t\t {'filter': (3*(5+nb_class)), 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], do_skip=False)\r\n\tloss_yolo_3 = YoloLayer(anchors[:6], \r\n\t\t\t\t\t\t\t[4*num for num in max_grid], \r\n\t\t\t\t\t\t\tbatch_size, \r\n\t\t\t\t\t\t\twarmup_batches, \r\n\t\t\t\t\t\t\tignore_thresh, \r\n\t\t\t\t\t\t\tgrid_scales[2],\r\n\t\t\t\t\t\t\tobj_scale,\r\n\t\t\t\t\t\t\tnoobj_scale,\r\n\t\t\t\t\t\t\txywh_scale,\r\n\t\t\t\t\t\t\tclass_scale)([input_image, pred_yolo_3, true_yolo_3, true_boxes]) \r\n\r\n\ttrain_model = Model([input_image, true_boxes, true_yolo_1, true_yolo_2, true_yolo_3], [loss_yolo_1, loss_yolo_2, loss_yolo_3])\r\n\tinfer_model = Model(input_image, [pred_yolo_1, pred_yolo_2, pred_yolo_3])\r\n\r\n\treturn [train_model, infer_model]\r\n\r\ndef dummy_loss(y_true, y_pred):\r\n\treturn tf.sqrt(tf.reduce_sum(y_pred))\r\n--------------------------------------------------------------------------------------------------------------------------------------------\r\n**__init__.py**\r\nimport os\r\nimport re\r\nimport numpy as np\r\nimport json\r\nfrom imageai.Detection.Custom.voc import parse_voc_annotation\r\nfrom imageai.Detection.Custom.yolo import create_yolov3_model, dummy_loss\r\nfrom imageai.Detection.YOLOv3.models import yolo_main\r\nfrom imageai.Detection.Custom.generator import BatchGenerator\r\nfrom imageai.Detection.Custom.utils.utils import normalize, evaluate, makedirs\r\nfrom keras.callbacks import ReduceLROnPlateau\r\nfrom keras.optimizers import Adam\r\nfrom imageai.Detection.Custom.callbacks import CustomModelCheckpoint, CustomTensorBoard\r\nfrom imageai.Detection.Custom.utils.multi_gpu_model import multi_gpu_model\r\nfrom imageai.Detection.Custom.gen_anchors import generateAnchors\r\nimport tensorflow as tf\r\nfrom keras.models import load_model, Input\r\nfrom keras.callbacks import TensorBoard\r\nimport keras.backend as K\r\nimport cv2\r\n\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\n\r\n\r\nclass DetectionModelTrainer:\r\n\r\n    \"\"\"\r\n    This is the Detection Model training class, which allows you to train object detection models\r\n    on image datasets that are in Pascal VOC annotation format, using the YOLOv3.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        self.__model_type = \"\"\r\n        self.__training_mode = True\r\n\r\n        self.__model_min_input_size = 288\r\n        self.__model_max_input_size = 448\r\n        self.__model_anchors = []\r\n        self.__inference_anchors = []\r\n        self.__json_directory = \"\"\r\n        self.__model_labels = []\r\n        self.__num_objects = 0\r\n        self.__pre_trained_model = \"\"\r\n\r\n        self.__train_images_folder = \"\"\r\n        self.__train_annotations_folder = \"\"\r\n        self.__train_cache_file = \"\"\r\n        self.__train_times = 8\r\n        self.__train_batch_size = 4\r\n        self.__train_learning_rate = 1e-4\r\n        self.__train_epochs = 100\r\n        self.__train_warmup_epochs = 3\r\n        self.__train_ignore_treshold = 0.5\r\n        self.__train_gpus = \"0\"\r\n        self.__train_grid_scales = [1, 1, 1]\r\n        self.__train_obj_scale = 5\r\n        self.__train_noobj_scale = 1\r\n        self.__train_xywh_scale = 1\r\n        self.__train_class_scale = 1\r\n        self.__model_directory = \"\"\r\n        self.__train_weights_name = \"\"\r\n        self.__train_debug = True\r\n        self.__logs_directory = \"\"\r\n\r\n        self.__validation_images_folder = \"\"\r\n        self.__validation_annotations_folder = \"\"\r\n        self.__validation_cache_file = \"\"\r\n        self.__validation_times = 1\r\n\r\n    def setModelTypeAsYOLOv3(self):\r\n        \"\"\"\r\n        'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\r\n        for the training instance object .\r\n        :return:\r\n        \"\"\"\r\n        self.__model_type = \"yolov3\"\r\n\r\n    def setDataDirectory(self, data_directory):\r\n\r\n        \"\"\"\r\n\r\n        'setDataDirectory()' is required to set the path to which the data/dataset to be used for\r\n                 training is kept. The directory can have any name, but it must have 'train' and 'validation'\r\n                 sub-directory. In the 'train' and 'validation' sub-directories, there must be 'images' and 'annotations'\r\n                 sub-directories respectively. The 'images' folder will contain the pictures for the dataset and the\r\n                 'annotations' folder will contain the XML files with details of the annotations for each image in the\r\n                 'images folder'.\r\n\r\n                 N.B: Strictly take note that the filenames (without the extension) of the pictures in the 'images folder'\r\n                  must be the same as the filenames (without the extension) of their corresponding annotation XML files in\r\n                  the 'annotations' folder.\r\n\r\n                 The structure of the 'train' and 'validation' folder must be as follows:\r\n\r\n                >> train    >> images       >> img_1.jpg\r\n                            >> images       >> img_2.jpg\r\n                            >> images       >> img_3.jpg\r\n                            >> annotations  >> img_1.xml\r\n                            >> annotations  >> img_2.xml\r\n                            >> annotations  >> img_3.xml\r\n\r\n\r\n                >> validation   >> images       >> img_151.jpg\r\n                                >> images       >> img_152.jpg\r\n                                >> images       >> img_153.jpg\r\n                                >> annotations  >> img_151.xml\r\n                                >> annotations  >> img_152.xml\r\n                                >> annotations  >> img_153.xml\r\n\r\n        :param data_directory:\r\n        :return:\r\n        \"\"\"\r\n\r\n        self.__train_images_folder = os.path.join(data_directory, \"train\", \"images\")\r\n        self.__train_annotations_folder = os.path.join(data_directory, \"train\", \"annotations\")\r\n        self.__validation_images_folder = os.path.join(data_directory, \"validation\", \"images\")\r\n        self.__validation_annotations_folder = os.path.join(data_directory, \"validation\", \"annotations\")\r\n\r\n        os.makedirs(os.path.join(data_directory, \"cache\"), exist_ok=True)\r\n        self.__train_cache_file = os.path.join(data_directory, \"cache\", \"detection_train_data.pkl\")\r\n        self.__validation_cache_file = os.path.join(data_directory, \"cache\", \"detection_test_data.pkl\")\r\n\r\n        os.makedirs(os.path.join(data_directory, \"models\"), exist_ok=True)\r\n\r\n        os.makedirs(os.path.join(data_directory, \"json\"), exist_ok=True)\r\n\r\n        os.makedirs(os.path.join(data_directory, \"logs\"), exist_ok=True)\r\n\r\n        self.__model_directory = os.path.join(data_directory, \"models\")\r\n        self.__train_weights_name = os.path.join(self.__model_directory, \"detection_model-\")\r\n        self.__json_directory = os.path.join(data_directory, \"json\")\r\n        self.__logs_directory = os.path.join(data_directory, \"logs\")\r\n\r\n    def setGpuUsage(self, train_gpus):\r\n        \"\"\"\r\n        'setGpuUsage' function allows you to set the GPUs to be used while training\r\n        train_gpu can be:\r\n        - an integer, indicating the number of GPUs to use\r\n        - a list of integers, indicating the id of the GPUs to be used\r\n        - a string, indicating the it og the id of the GPUs to be used, separated by commas\r\n        :param train_gpus: gpus where to run\r\n        :return:\r\n        \"\"\"\r\n        # train_gpus, could be a string separated by comma, or a list of int or the number of GPUs to be used\r\n        if type(train_gpus) == str:\r\n            train_gpus = train_gpus.split(',')\r\n        if type(train_gpus) == int:\r\n            train_gpus = range(train_gpus)\r\n        # let it as a string separated by commas\r\n        self.__train_gpus = ','.join([str(gpu) for gpu in train_gpus])\r\n\r\n  #  def setTrainConfig(self,  object_names_array, batch_size=4, num_experiments=100, train_from_pretrained_model=\"\"):\r\n\r\n        \"\"\"\r\n\r\n        'setTrainConfig()' function allows you to set the properties for the training instances. It accepts the following values:\r\n\r\n        - object_names_array , this is an array of the names of the different objects in your dataset\r\n        - batch_size (optional),  this is the batch size for the training instance\r\n        - num_experiments (optional),   also known as epochs, it is the number of times the network will train on all the training dataset\r\n        - train_from_pretrained_model (optional), this is used to perform transfer learning by specifying the path to a pre-trained YOLOv3 model\r\n\r\n        :param object_names_array:\r\n        :param batch_size:\r\n        :param num_experiments:\r\n        :param train_from_pretrained_model:\r\n        :return:\r\n        \"\"\"\r\n\r\n        self.__model_anchors, self.__inference_anchors = generateAnchors(self.__train_annotations_folder,\r\n                                                                         self.__train_images_folder,\r\n                                                                         self.__train_cache_file, self.__model_labels)\r\n\r\n        self.__model_labels = sorted(object_names_array)\r\n        self.__num_objects = len(object_names_array)\r\n\r\n        self.__train_batch_size = batch_size\r\n        self.__train_epochs = num_experiments\r\n        self.__pre_trained_model = train_from_pretrained_model\r\n\r\n        json_data = dict()\r\n        json_data[\"labels\"] = self.__model_labels\r\n        json_data[\"anchors\"] = self.__inference_anchors\r\n\r\n       ** with open(os.path.join(self.__json_directory, \"detection_config.json\"), \"w+\") as json_file:\r\n            json.dump(json_data, json_file, indent=4, separators=(\",\", \" : \"),\r\n                      ensure_ascii=True)\r\n\r\n        print(\"Detection configuration saved in \", os.path.join(self.__json_directory, \"detection_config.json\")) **\r\n\r\n    def trainModel(self):\r\n\r\n        \"\"\"\r\n        'trainModel()' function starts the actual model training. Once the training starts, the training instance\r\n        creates 3 sub-folders in your dataset folder which are:\r\n\r\n        - json,  where the JSON configuration file for using your trained model is stored\r\n        - models, where your trained models are stored once they are generated after each improved experiments\r\n        - cache , where temporary traing configuraton files are stored\r\n\r\n        :return:\r\n        \"\"\"\r\n\r\n        train_ints, valid_ints, labels, max_box_per_image = self._create_training_instances(\r\n            self.__train_annotations_folder,\r\n            self.__train_images_folder,\r\n            self.__train_cache_file,\r\n            self.__validation_annotations_folder,\r\n            self.__validation_images_folder,\r\n            self.__validation_cache_file,\r\n            self.__model_labels\r\n\r\n        )\r\n        if self.__training_mode:\r\n            print('Training on: \\t' + str(labels) + '')\r\n            print(\"Training with Batch Size: \", self.__train_batch_size)\r\n            print(\"Number of Training Samples: \", len(train_ints))\r\n            print(\"Number of Validation Samples: \", len(valid_ints))\r\n            print(\"Number of Experiments: \", self.__train_epochs)\r\n\r\n        ###############################\r\n        #   Create the generators\r\n        ###############################\r\n        train_generator = BatchGenerator(\r\n            instances=train_ints,\r\n            anchors=self.__model_anchors,\r\n            labels=labels,\r\n            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3\r\n            max_box_per_image=max_box_per_image,\r\n            batch_size=self.__train_batch_size,\r\n            min_net_size=self.__model_min_input_size,\r\n            max_net_size=self.__model_max_input_size,\r\n            shuffle=True,\r\n            jitter=0.3,\r\n            norm=normalize\r\n        )\r\n\r\n        valid_generator = BatchGenerator(\r\n            instances=valid_ints,\r\n            anchors=self.__model_anchors,\r\n            labels=labels,\r\n            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3\r\n            max_box_per_image=max_box_per_image,\r\n            batch_size=self.__train_batch_size,\r\n            min_net_size=self.__model_min_input_size,\r\n            max_net_size=self.__model_max_input_size,\r\n            shuffle=True,\r\n            jitter=0.0,\r\n            norm=normalize\r\n        )\r\n\r\n        ###############################\r\n        #   Create the model\r\n        ###############################\r\n        if os.path.exists(self.__pre_trained_model):\r\n            self.__train_warmup_epochs = 0\r\n        warmup_batches = self.__train_warmup_epochs * (self.__train_times * len(train_generator))\r\n\r\n        os.environ['CUDA_VISIBLE_DEVICES'] = self.__train_gpus\r\n        multi_gpu = [int(gpu) for gpu in self.__train_gpus.split(',')]\r\n\r\n        train_model, infer_model = self._create_model(\r\n            nb_class=len(labels),\r\n            anchors=self.__model_anchors,\r\n            max_box_per_image=max_box_per_image,\r\n            max_grid=[self.__model_max_input_size, self.__model_max_input_size],\r\n            batch_size=self.__train_batch_size,\r\n            warmup_batches=warmup_batches,\r\n            ignore_thresh=self.__train_ignore_treshold,\r\n            multi_gpu=multi_gpu,\r\n            lr=self.__train_learning_rate,\r\n            grid_scales=self.__train_grid_scales,\r\n            obj_scale=self.__train_obj_scale,\r\n            noobj_scale=self.__train_noobj_scale,\r\n            xywh_scale=self.__train_xywh_scale,\r\n            class_scale=self.__train_class_scale,\r\n        )\r\n\r\n        ###############################\r\n        #   Kick off the training\r\n        ###############################\r\n        callbacks = self._create_callbacks(self.__train_weights_name, infer_model)\r\n\t\t\r\n        train_model.fit_generator(\r\n            generator=train_generator,\r\n            steps_per_epoch=len(train_generator) * self.__train_times,\r\n            validation_data=valid_generator,\r\n            validation_steps=len(valid_generator) * self.__train_times,\r\n            epochs=self.__train_epochs + self.__train_warmup_epochs,\r\n            verbose=1,\r\n            callbacks=callbacks,\r\n            workers=4,\r\n            max_queue_size=8\r\n        )\r\n\r\n    def evaluateModel(self, model_path, json_path, batch_size=4, iou_threshold=0.5, object_threshold=0.2, nms_threshold=0.45):\r\n        \"\"\"\r\n\r\n        'evaluateModel()' is used to obtain the mAP metrics for your model(s). It accepts the following values:\r\n\r\n        - model_path ( model file or folder), this value can be the part to your model file or the path to the folder containing all your saved model files\r\n        - json_path ,   this is the path the the 'detection_config.json' file saved for the dataset during the training\r\n        - iou_threshold , this value is used to set the desired 'IoU' to obtain the mAP metrics for your model(s)\r\n        - object_threshold , this is used to set your desired minimum 'class score' to obtain the mAP metrics for your model(s)\r\n        - nms_threshold , this is used to set your desired 'Non-maximum suppresion' to obtain the mAP metrics for your model(s)\r\n\r\n        :param model_path:\r\n        :param json_path:\r\n        :param batch_size:\r\n        :param iou_threshold:\r\n        :param object_threshold:\r\n        :param nms_threshold:\r\n        :return: list of dictionaries, containing one dict per evaluated model.\r\n            Each dict contains exactly the same metrics that are printed on standard output\r\n        \"\"\"\r\n\r\n        self.__training_mode = False\r\n\r\n        with open(json_path, 'r') as json_file:\r\n            detection_model_json = json.load(json_file)\r\n\r\n        temp_anchor_array = []\r\n        new_anchor_array = []\r\n\r\n        temp_anchor_array.append(detection_model_json[\"anchors\"][2])\r\n        temp_anchor_array.append(detection_model_json[\"anchors\"][1])\r\n        temp_anchor_array.append(detection_model_json[\"anchors\"][0])\r\n\r\n        for aa in temp_anchor_array:\r\n            for aaa in aa:\r\n                new_anchor_array.append(aaa)\r\n\r\n        self.__model_anchors = new_anchor_array\r\n        self.__model_labels = detection_model_json[\"labels\"]\r\n        self.__num_objects = len(self.__model_labels)\r\n\r\n        self.__train_batch_size = batch_size\r\n        self.__train_epochs = 100\r\n\r\n        print(\"Starting Model evaluation....\")\r\n\r\n        _, valid_ints, labels, max_box_per_image = self._create_training_instances(\r\n            self.__train_annotations_folder,\r\n            self.__train_images_folder,\r\n            self.__train_cache_file,\r\n            self.__validation_annotations_folder,\r\n            self.__validation_images_folder,\r\n            self.__validation_cache_file,\r\n            self.__model_labels\r\n\r\n        )\r\n\r\n        if len(valid_ints) == 0:\r\n            print('Validation samples were not provided.')\r\n            print('Please, check your validation samples are correctly provided:')\r\n            print('\\tAnnotations: {}\\n\\tImages: {}'.format(self.__validation_annotations_folder,\r\n                                                           self.__validation_images_folder))\r\n\r\n        valid_generator = BatchGenerator(\r\n            instances=valid_ints,\r\n            anchors=self.__model_anchors,\r\n            labels=labels,\r\n            downsample=32,  # ratio between network input's size and network output's size, 32 for YOLOv3\r\n            max_box_per_image=max_box_per_image,\r\n            batch_size=self.__train_batch_size,\r\n            min_net_size=self.__model_min_input_size,\r\n            max_net_size=self.__model_max_input_size,\r\n            shuffle=True,\r\n            jitter=0.0,\r\n            norm=normalize\r\n        )\r\n\r\n        results = list()\r\n\r\n        if os.path.isfile(model_path):\r\n            # model_files must be a list containing the complete path to the files,\r\n            # if a file is given, then the list contains just this file\r\n            model_files = [model_path]\r\n        elif os.path.isdir(model_path):\r\n            # model_files must be a list containing the complete path to the files,\r\n            # if a folder is given, then the list contains the complete path to each file on that folder\r\n            model_files = sorted([os.path.join(model_path, file_name) for file_name in os.listdir(model_path)])\r\n            # sort the files to make sure we're always evaluating them on same order\r\n        else:\r\n            print('model_path must be the path to a .h5 file or a directory. Found {}'.format(model_path))\r\n            return results\r\n\r\n        for model_file in model_files:\r\n            if str(model_file).endswith(\".h5\"):\r\n                try:\r\n                    infer_model = load_model(model_file)\r\n\r\n                    ###############################\r\n                    #   Run the evaluation\r\n                    ###############################\r\n                    # compute mAP for all the classes\r\n                    average_precisions = evaluate(infer_model, valid_generator, iou_threshold=iou_threshold,\r\n                                                  obj_thresh=object_threshold, nms_thresh=nms_threshold)\r\n\r\n                    result_dict = {\r\n                        'model_file': model_file,\r\n                        'using_iou': iou_threshold,\r\n                        'using_object_threshold': object_threshold,\r\n                        'using_non_maximum_suppression': nms_threshold,\r\n                        'average_precision': dict(),\r\n                        'evaluation_samples': len(valid_ints)\r\n                    }\r\n                    # print the score\r\n                    print(\"Model File: \", model_file, '\\n')\r\n                    print(\"Evaluation samples: \", len(valid_ints))\r\n                    print(\"Using IoU: \", iou_threshold)\r\n                    print(\"Using Object Threshold: \", object_threshold)\r\n                    print(\"Using Non-Maximum Suppression: \", nms_threshold)\r\n\r\n                    for label, average_precision in average_precisions.items():\r\n                        print(labels[label] + ': {:.4f}'.format(average_precision))\r\n                        result_dict['average_precision'][labels[label]] = average_precision\r\n\r\n                    print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))\r\n                    result_dict['map'] = sum(average_precisions.values()) / len(average_precisions)\r\n                    print(\"===============================\")\r\n\r\n                    results.append(result_dict)\r\n                except Exception as e:\r\n                    print('skipping the evaluation of {} because following exception occurred: {}'.format(model_file, e))\r\n                    continue\r\n            else:\r\n                print('skipping the evaluation of {} since it\\'s not a .h5 file'.format(model_file))\r\n\r\n        return results\r\n\r\n    def _create_training_instances(self,\r\n            train_annot_folder,\r\n            train_image_folder,\r\n            train_cache,\r\n            valid_annot_folder,\r\n            valid_image_folder,\r\n            valid_cache,\r\n            labels,\r\n    ):\r\n\r\n        # parse annotations of the training set\r\n        train_ints, train_labels = parse_voc_annotation(train_annot_folder, train_image_folder, train_cache, labels)\r\n\r\n        # parse annotations of the validation set, if any, otherwise split the training set\r\n\r\n        if os.path.exists(valid_annot_folder):\r\n            valid_ints, valid_labels = parse_voc_annotation(valid_annot_folder, valid_image_folder, valid_cache, labels)\r\n            print('Evaluating over {} samples taken from {}'.format(len(valid_ints),\r\n                                                                    os.path.dirname(valid_annot_folder)))\r\n        else:\r\n\r\n            train_portion = 0.8  # use 80% to train and the remaining 20% to evaluate\r\n            train_valid_split = int(round(train_portion * len(train_ints)))\r\n            np.random.seed(0)\r\n            np.random.shuffle(train_ints)\r\n\r\n            valid_ints = train_ints[train_valid_split:]\r\n            train_ints = train_ints[:train_valid_split]\r\n            print('Evaluating over {} samples taken as {:5.2f}% of the training set '\r\n                  'given at {}'.format(len(valid_ints),\r\n                                       (1 - train_portion)*100,\r\n                                       os.path.dirname(train_annot_folder)))\r\n\r\n        print('Training over {} samples  given at {}'.format(len(train_ints), os.path.dirname(train_annot_folder)))\r\n\r\n        # compare the seen labels with the given labels in config.json\r\n        if len(labels) > 0:\r\n            overlap_labels = set(labels).intersection(set(train_labels.keys()))\r\n\r\n            # return None, None, None if some given label is not in the dataset\r\n            if len(overlap_labels) < len(labels):\r\n                if self.__training_mode:\r\n                    print('Some labels have no annotations! Please revise the list of labels in your configuration.')\r\n                return None, None, None, None\r\n        else:\r\n            if self.__training_mode:\r\n                print('No labels are provided. Train on all seen labels.')\r\n                print(train_labels)\r\n\r\n            labels = train_labels.keys()\r\n\r\n        max_box_per_image = max([len(inst['object']) for inst in (train_ints + valid_ints)])\r\n\r\n        return train_ints, valid_ints, sorted(labels), max_box_per_image\r\n\r\n    def _create_callbacks(self, saved_weights_name, model_to_save):\r\n\r\n        checkpoint = CustomModelCheckpoint(\r\n            model_to_save=model_to_save,\r\n            filepath=saved_weights_name + 'ex-{epoch:03d}--loss-{loss:08.3f}.h5',\r\n            monitor='loss',\r\n            verbose=0,\r\n            save_best_only=True,\r\n            mode='min',\r\n            period=1\r\n        )\r\n        reduce_on_plateau = ReduceLROnPlateau(\r\n            monitor='loss',\r\n            factor=0.1,\r\n            patience=2,\r\n            verbose=0,\r\n            mode='min',\r\n            epsilon=0.01,\r\n            cooldown=0,\r\n            min_lr=0\r\n        )\r\n        tensor_board = TensorBoard(\r\n            log_dir=self.__logs_directory\r\n        )\r\n        return [checkpoint, reduce_on_plateau, tensor_board]\r\n\r\n    def _create_model(\r\n            self,\r\n            nb_class,\r\n            anchors,\r\n            max_box_per_image,\r\n            max_grid, batch_size,\r\n            warmup_batches,\r\n            ignore_thresh,\r\n            multi_gpu,\r\n            lr,\r\n            grid_scales,\r\n            obj_scale,\r\n            noobj_scale,\r\n            xywh_scale,\r\n            class_scale\r\n    ):\r\n        if len(multi_gpu) > 1:\r\n            with tf.device('/cpu:0'):\r\n                template_model, infer_model = create_yolov3_model(\r\n                    nb_class=nb_class,\r\n                    anchors=anchors,\r\n                    max_box_per_image=max_box_per_image,\r\n                    max_grid=max_grid,\r\n                    batch_size=batch_size // len(multi_gpu),\r\n                    warmup_batches=warmup_batches,\r\n                    ignore_thresh=ignore_thresh,\r\n                    grid_scales=grid_scales,\r\n                    obj_scale=obj_scale,\r\n                    noobj_scale=noobj_scale,\r\n                    xywh_scale=xywh_scale,\r\n                    class_scale=class_scale\r\n                )\r\n        else:\r\n            template_model, infer_model = create_yolov3_model(\r\n                nb_class=nb_class,\r\n                anchors=anchors,\r\n                max_box_per_image=max_box_per_image,\r\n                max_grid=max_grid,\r\n                batch_size=batch_size,\r\n                warmup_batches=warmup_batches,\r\n                ignore_thresh=ignore_thresh,\r\n                grid_scales=grid_scales,\r\n                obj_scale=obj_scale,\r\n                noobj_scale=noobj_scale,\r\n                xywh_scale=xywh_scale,\r\n                class_scale=class_scale\r\n            )\r\n\r\n            # load the pretrained weight if exists, otherwise load the backend weight only\r\n\r\n        if len(self.__pre_trained_model) > 3:\r\n            if self.__training_mode:\r\n                print(\"Training with transfer learning from pretrained Model\")\r\n            template_model.load_weights(self.__pre_trained_model, by_name=True)\r\n        else:\r\n            if self.__training_mode:\r\n                print(\"Pre-trained Model not provided. Transfer learning not in use.\")\r\n                print(\"Training will start with 3 warmup experiments\")\r\n\r\n        if len(multi_gpu) > 1:\r\n            train_model = multi_gpu_model(template_model, gpus=multi_gpu)\r\n        else:\r\n            train_model = template_model\r\n\r\n        optimizer = Adam(lr=lr, clipnorm=0.001)\r\n        train_model.compile(loss=dummy_loss, optimizer=optimizer)\r\n\r\n        return train_model, infer_model\r\n\r\n\r\nclass CustomObjectDetection:\r\n\r\n    \"\"\"\r\n    This is the object detection class for using your custom trained models. It supports your custom trained YOLOv3 model and allows to you to perform object detection in images.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        self.__model_type = \"\"\r\n        self.__model_path = \"\"\r\n        self.__model_labels = []\r\n        self.__model_anchors = []\r\n        self.__detection_config_json_path = \"\"\r\n        self.__input_size = 416\r\n        self.__object_threshold = 0.4\r\n        self.__nms_threshold = 0.4\r\n        self.__model = None\r\n        self.__detection_utils = CustomDetectionUtils(labels=[])\r\n\r\n    def setModelTypeAsYOLOv3(self):\r\n        \"\"\"\r\n        'setModelTypeAsYOLOv3' is used to set your custom detection model as YOLOv3\r\n        :return:\r\n        \"\"\"\r\n        self.__model_type = \"yolov3\"\r\n\r\n    def setModelPath(self, detection_model_path):\r\n        \"\"\"\r\n        'setModelPath' is used to specify the filepath to your custom detection model\r\n        :param detection_model_path: path to the .h5 model file.\r\n            Usually is one of those under <data_directory>/models/detection_model-ex-ddd--loss-dddd.ddd.h5\r\n        :return: None\r\n        \"\"\"\r\n        self.__model_path = detection_model_path\r\n\r\n    def setJsonPath(self, configuration_json):\r\n        \"\"\"\r\n        'setJsonPath' is used to set the filepath to the configuration JSON file for your custom detection model\r\n        :param configuration_json: path to the .json file. Usually it is <data_directory>/json/detection_config.json\r\n        :return: None\r\n        \"\"\"\r\n        self.__detection_config_json_path = configuration_json\r\n\r\n    def loadModel(self):\r\n\r\n        \"\"\"\r\n        'loadModel' is used to load the model into the CustomObjectDetection class\r\n        :return: None\r\n        \"\"\"\r\n\r\n        if self.__model_type == \"yolov3\":\r\n            detection_model_json = json.load(open(self.__detection_config_json_path))\r\n\r\n            self.__model_labels = detection_model_json[\"labels\"]\r\n            self.__model_anchors = detection_model_json[\"anchors\"]\r\n\r\n            self.__detection_utils = CustomDetectionUtils(labels=self.__model_labels)\r\n\r\n            self.__model = yolo_main(Input(shape=(None, None, 3)), 3, len(self.__model_labels))\r\n\r\n            self.__model.load_weights(self.__model_path)\r\n\r\n    def detectObjectsFromImage(self, input_image=\"\", output_image_path=\"\", input_type=\"file\", output_type=\"file\",\r\n                               extract_detected_objects=False, minimum_percentage_probability=50, nms_treshold=0.4,\r\n                               display_percentage_probability=True, display_object_name=True, thread_safe=False):\r\n\r\n        \"\"\"\r\n\r\n        'detectObjectsFromImage()' function is used to detect objects observable in the given image:\r\n                    * input_image , which can be a filepath or image numpy array in BGR\r\n                    * output_image_path (only if output_type = file) , file path to the output image that will contain the detection boxes and label, if output_type=\"file\"\r\n                    * input_type (optional) , filepath/numpy array of the image. Acceptable values are \"file\" and \"array\"\r\n                    * output_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are \"file\" and \"array\"\r\n                    * extract_detected_objects (optional) , option to save each object detected individually as an image and return an array of the objects' image path.\r\n                    * minimum_percentage_probability (optional, 30 by default) , option to set the minimum percentage probability for nominating a detected object for output.\r\n                    * nms_threshold (optional, o.45 by default) , option to set the Non-maximum suppression for the detection\r\n                    * display_percentage_probability (optional, True by default), option to show or hide the percentage probability of each object in the saved/returned detected image\r\n                    * display_display_object_name (optional, True by default), option to show or hide the name of each object in the saved/returned detected image\r\n                    * thread_safe (optional, False by default), enforce the loaded detection model works across all threads if set to true, made possible by forcing all Keras inference to run on the default graph\r\n\r\n\r\n            The values returned by this function depends on the parameters parsed. The possible values returnable\r\n            are stated as below\r\n            - If extract_detected_objects = False or at its default value and output_type = 'file' or\r\n                at its default value, you must parse in the 'output_image_path' as a string to the path you want\r\n                the detected image to be saved. Then the function will return:\r\n                1. an array of dictionaries, with each dictionary corresponding to the objects\r\n                    detected in the image. Each dictionary contains the following property:\r\n                    * name (string)\r\n                    * percentage_probability (float)\r\n                    * box_points (list of x1,y1,x2 and y2 coordinates)\r\n\r\n            - If extract_detected_objects = False or at its default value and output_type = 'array' ,\r\n              Then the function will return:\r\n\r\n                1. a numpy array of the detected image\r\n                2. an array of dictionaries, with each dictionary corresponding to the objects\r\n                    detected in the image. Each dictionary contains the following property:\r\n                    * name (string)\r\n                    * percentage_probability (float)\r\n                    * box_points (list of x1,y1,x2 and y2 coordinates)\r\n\r\n            - If extract_detected_objects = True and output_type = 'file' or\r\n                at its default value, you must parse in the 'output_image_path' as a string to the path you want\r\n                the detected image to be saved. Then the function will return:\r\n                1. an array of dictionaries, with each dictionary corresponding to the objects\r\n                    detected in the image. Each dictionary contains the following property:\r\n                    * name (string)\r\n                    * percentage_probability (float)\r\n                    * box_points (list of x1,y1,x2 and y2 coordinates)\r\n                2. an array of string paths to the image of each object extracted from the image\r\n\r\n            - If extract_detected_objects = True and output_type = 'array', the the function will return:\r\n                1. a numpy array of the detected image\r\n                2. an array of dictionaries, with each dictionary corresponding to the objects\r\n                    detected in the image. Each dictionary contains the following property:\r\n                    * name (string)\r\n                    * percentage_probability (float)\r\n                    * box_points (list of x1,y1,x2 and y2 coordinates)\r\n                3. an array of numpy arrays of each object detected in the image\r\n\r\n        :param input_image:\r\n        :param output_image_path:\r\n        :param input_type:\r\n        :param output_type:\r\n        :param extract_detected_objects:\r\n        :param minimum_percentage_probability:\r\n        :param nms_treshold:\r\n        :param display_percentage_probability:\r\n        :param display_object_name:\r\n        :param thread_safe:\r\n        :return image_frame:\r\n        :return output_objects_array:\r\n        :return detected_objects_image_array:\r\n        \"\"\"\r\n\r\n        if self.__model is None:\r\n            raise ValueError(\"You must call the loadModel() function before making object detection.\")\r\n        else:\r\n            if output_type == \"file\":\r\n                # from the image file, lets keep the directory and the filename, but remove its  format\r\n                # if output_image_path is path/to/the/output/image.png\r\n                # then output_image_folder is  path/to/the/output/image\r\n                # let's check if it is in the appropriated format soon to fail early\r\n                output_image_folder, n_subs = re.subn(r'\\.(?:jpe?g|png|tif|webp|PPM|PGM)$', '', output_image_path, flags=re.I)\r\n                if n_subs == 0:\r\n                    # if no substitution was done, the given output_image_path is not in a supported format,\r\n                    # raise an error\r\n                    raise ValueError(\"output_image_path must be the path where to write the image. \"\r\n                                     \"Therefore it must end as one the following: \"\r\n                                     \"'.jpg', '.png', '.tif', '.webp', '.PPM', '.PGM'. {} found\".format(output_image_path))\r\n                elif extract_detected_objects:\r\n                    # Results must be written as files and need to extract detected objects as images,\r\n                    # let's create a folder to store the object's images\r\n                    objects_dir = output_image_folder + \"-objects\"\r\n\r\n                    os.makedirs(objects_dir, exist_ok=True)\r\n\r\n            self.__object_threshold = minimum_percentage_probability / 100\r\n            self.__nms_threshold = nms_treshold\r\n\r\n            output_objects_array = []\r\n            detected_objects_image_array = []\r\n\r\n            if input_type == \"file\":\r\n                image = cv2.imread(input_image)\r\n            elif input_type == \"array\":\r\n                image = input_image\r\n            else:\r\n                raise ValueError(\"input_type must be 'file' or 'array'. {} found\".format(input_type))\r\n\r\n            image_frame = image.copy()\r\n\r\n            height, width, channels = image.shape\r\n\r\n            image = cv2.resize(image, (self.__input_size, self.__input_size))\r\n\r\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n\r\n            image = image.astype(\"float32\") / 255.\r\n\r\n            # expand the image to batch\r\n            image = np.expand_dims(image, 0)\r\n\r\n            if self.__model_type == \"yolov3\":\r\n                if thread_safe == True:\r\n                    with K.get_session().graph.as_default():\r\n                        yolo_results = self.__model.predict(image)\r\n                else:\r\n                    yolo_results = self.__model.predict(image)\r\n\r\n                boxes = list()\r\n\r\n                for idx, result in enumerate(yolo_results):\r\n                    box_set = self.__detection_utils.decode_netout(result[0], self.__model_anchors[idx],\r\n                                                                   self.__object_threshold, self.__input_size,\r\n                                                                   self.__input_size)\r\n                    boxes += box_set\r\n\r\n                self.__detection_utils.correct_yolo_boxes(boxes, height, width, self.__input_size, self.__input_size)\r\n\r\n                self.__detection_utils.do_nms(boxes, self.__nms_threshold)\r\n\r\n                all_boxes, all_labels, all_scores = self.__detection_utils.get_boxes(boxes, self.__model_labels,\r\n                                                                                     self.__object_threshold)\r\n\r\n                for object_box, object_label, object_score in zip(all_boxes, all_labels, all_scores):\r\n                    each_object_details = dict()\r\n                    each_object_details[\"name\"] = object_label\r\n                    each_object_details[\"percentage_probability\"] = object_score\r\n\r\n                    if object_box.xmin < 0:\r\n                        object_box.xmin = 0\r\n                    if object_box.ymin < 0:\r\n                        object_box.ymin = 0\r\n\r\n                    each_object_details[\"box_points\"] = [object_box.xmin, object_box.ymin, object_box.xmax, object_box.ymax]\r\n                    output_objects_array.append(each_object_details)\r\n\r\n                drawn_image = self.__detection_utils.draw_boxes_and_caption(image_frame.copy(), all_boxes, all_labels,\r\n                                                                            all_scores, show_names=display_object_name,\r\n                                                                            show_percentage=display_percentage_probability)\r\n\r\n                if extract_detected_objects:\r\n\r\n                    for cnt, each_object in enumerate(output_objects_array):\r\n\r\n                        splitted_image = image_frame[each_object[\"box_points\"][1]:each_object[\"box_points\"][3],\r\n                                                     each_object[\"box_points\"][0]:each_object[\"box_points\"][2]]\r\n                        if output_type == \"file\":\r\n                            splitted_image_path = os.path.join(objects_dir, \"{}-{:05d}.jpg\".format(each_object[\"name\"],\r\n                                                                                                   cnt))\r\n\r\n                            cv2.imwrite(splitted_image_path, splitted_image)\r\n                            detected_objects_image_array.append(splitted_image_path)\r\n                        elif output_type == \"array\":\r\n                            detected_objects_image_array.append(splitted_image.copy())\r\n\r\n                if output_type == \"file\":\r\n                    # we already validated that the output_image_path is a supported by OpenCV one\r\n                    cv2.imwrite(output_image_path, drawn_image)\r\n\r\n                if extract_detected_objects:\r\n                    if output_type == \"file\":\r\n                        return output_objects_array, detected_objects_image_array\r\n                    elif output_type == \"array\":\r\n                        return drawn_image, output_objects_array, detected_objects_image_array\r\n\r\n                else:\r\n                    if output_type == \"file\":\r\n                        return output_objects_array\r\n                    elif output_type == \"array\":\r\n                        return drawn_image, output_objects_array\r\n\r\n\r\nclass CustomVideoObjectDetection:\r\n\r\n\r\n    \"\"\"\r\n\r\n    This is the object detection class for videos and camera live stream inputs using your custom trained detection models. It provides support for your custom YOLOv3 models.\r\n\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        self.__model_type = \"\"\r\n        self.__model_path = \"\"\r\n        self.__model_labels = []\r\n        self.__model_anchors = []\r\n        self.__detection_config_json_path = \"\"\r\n        self.__model_loaded = False\r\n        self.__input_size = 416\r\n        self.__object_threshold = 0.4\r\n        self.__nms_threshold = 0.4\r\n        self.__detector = []\r\n        self.__detection_utils = CustomDetectionUtils(labels=[])\r\n\r\n    def setModelTypeAsYOLOv3(self):\r\n\r\n        \"\"\"\r\n        'setModelTypeAsYOLOv3' is used to set your custom detection model as YOLOv3\r\n        :return:\r\n        \"\"\"\r\n\r\n        self.__model_type = \"yolov3\"\r\n\r\n\r\n    def setModelPath(self, detection_model_path):\r\n        \"\"\"\r\n        'setModelPath' is used to specify the filepath to your custom detection model\r\n\r\n        :param detection_model_path:\r\n        :return:\r\n        \"\"\"\r\n        self.__model_path = detection_model_path\r\n\r\n\r\n    def setJsonPath(self, configuration_json):\r\n        \"\"\"\r\n        'setJsonPath' is used to set the filepath to the configuration JSON file for your custom detection model\r\n\r\n        :param configuration_json:\r\n        :return:\r\n        \"\"\"\r\n        self.__detection_config_json_path = configuration_json\r\n\r\n    def loadModel(self):\r\n        \"\"\"\r\n        'loadModel' is used to load the model into the CustomVideoObjectDetection class\r\n\r\n        :return:\r\n        \"\"\"\r\n\r\n        if (self.__model_loaded == False):\r\n            if(self.__model_type == \"yolov3\"):\r\n                detector = CustomObjectDetection()\r\n                detector.setModelTypeAsYOLOv3()\r\n                detector.setModelPath(self.__model_path)\r\n                detector.setJsonPath(self.__detection_config_json_path)\r\n                detector.loadModel()\r\n\r\n                self.__detector = detector\r\n                self.__model_loaded = True\r\n\r\n\r\n    def detectObjectsFromVideo(self, input_file_path=\"\", camera_input=None, output_file_path=\"\", frames_per_second=20,\r\n                               frame_detection_interval=1, minimum_percentage_probability=50, log_progress=False,\r\n                               display_percentage_probability=True, display_object_name=True, save_detected_video=True,\r\n                               per_frame_function=None, per_second_function=None, per_minute_function=None,\r\n                               video_complete_function=None, return_detected_frame=False, detection_timeout = None):\r\n\r\n\r\n\r\n\r\n        \"\"\"\r\n\r\n        'detectObjectsFromVideo()' function is used to detect objects observable in the given video path or a camera input:\r\n            * input_file_path , which is the file path to the input video. It is required only if 'camera_input' is not set\r\n            * camera_input , allows you to parse in camera input for live video detections\r\n            * output_file_path , which is the path to the output video. It is required only if 'save_detected_video' is not set to False\r\n            * frames_per_second , which is the number of frames to be used in the output video\r\n            * frame_detection_interval (optional, 1 by default)  , which is the intervals of frames that will be detected.\r\n            * minimum_percentage_probability (optional, 50 by default) , option to set the minimum percentage probability for nominating a detected object for output.\r\n            * log_progress (optional) , which states if the progress of the frame processed is to be logged to console\r\n            * display_percentage_probability (optional), can be used to hide or show probability scores on the detected video frames\r\n            * display_object_name (optional), can be used to show or hide object names on the detected video frames\r\n            * save_save_detected_video (optional, True by default), can be set to or not to save the detected video\r\n            * per_frame_function (optional), this parameter allows you to parse in a function you will want to execute after each frame of the video is detected. If this parameter is set to a function, after every video  frame is detected, the function will be executed with the following values parsed into it:\r\n                -- position number of the frame\r\n                -- an array of dictinaries, with each dictinary corresponding to each object detected. Each dictionary contains 'name', 'percentage_probability' and 'box_points'\r\n                -- a dictionary with with keys being the name of each unique objects and value are the number of instances of the object present\r\n                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fourth value into the function\r\n\r\n            * per_second_function (optional), this parameter allows you to parse in a function you will want to execute after each second of the video is detected. If this parameter is set to a function, after every second of a video is detected, the function will be executed with the following values parsed into it:\r\n                -- position number of the second\r\n                -- an array of dictionaries whose keys are position number of each frame present in the last second , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\r\n                -- an array of dictionaries, with each dictionary corresponding to each frame in the past second, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\r\n                -- a dictionary with its keys being the name of each unique object detected throughout the past second, and the key values are the average number of instances of the object found in all the frames contained in the past second\r\n                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed\r\n                                                                    as the fifth value into the function\r\n\r\n            * per_minute_function (optional), this parameter allows you to parse in a function you will want to execute after each minute of the video is detected. If this parameter is set to a function, after every minute of a video is detected, the function will be executed with the following values parsed into it:\r\n                -- position number of the minute\r\n                -- an array of dictionaries whose keys are position number of each frame present in the last minute , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\r\n\r\n                -- an array of dictionaries, with each dictionary corresponding to each frame in the past minute, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\r\n\r\n                -- a dictionary with its keys being the name of each unique object detected throughout the past minute, and the key values are the average number of instances of the object found in all the frames contained in the past minute\r\n\r\n                -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fifth value into the function\r\n\r\n            * video_complete_function (optional), this parameter allows you to parse in a function you will want to execute after all of the video frames have been detected. If this parameter is set to a function, after all of frames of a video is detected, the function will be executed with the following values parsed into it:\r\n                -- an array of dictionaries whose keys are position number of each frame present in the entire video , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\r\n                -- an array of dictionaries, with each dictionary corresponding to each frame in the entire video, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\r\n                -- a dictionary with its keys being the name of each unique object detected throughout the entire video, and the key values are the average number of instances of the object found in all the frames contained in the entire video\r\n\r\n            * return_detected_frame (optionally, False by default), option to obtain the return the last detected video frame into the per_per_frame_function, per_per_second_function or per_per_minute_function\r\n\r\n            * detection_timeout (optionally, None by default), option to state the number of seconds of a video that should be detected after which the detection function stop processing the video\r\n\r\n        :param input_file_path:\r\n        :param camera_input:\r\n        :param output_file_path:\r\n        :param frames_per_second:\r\n        :param frame_detection_interval:\r\n        :param minimum_percentage_probability:\r\n        :param log_progress:\r\n        :param display_percentage_probability:\r\n        :param display_object_name:\r\n        :param save_detected_video:\r\n        :param per_frame_function:\r\n        :param per_second_function:\r\n        :param per_minute_function:\r\n        :param video_complete_function:\r\n        :param return_detected_frame:\r\n        :param detection_timeout:\r\n        :return output_video_filepath:\r\n        :return counting:\r\n        :return output_objects_array:\r\n        :return output_objects_count:\r\n        :return detected_copy:\r\n        :return this_second_output_object_array:\r\n        :return this_second_counting_array:\r\n        :return this_second_counting:\r\n        :return this_minute_output_object_array:\r\n        :return this_minute_counting_array:\r\n        :return this_minute_counting:\r\n        :return this_video_output_object_array:\r\n        :return this_video_counting_array:\r\n        :return this_video_counting:\r\n        \"\"\"\r\n\r\n        output_frames_dict = {}\r\n        output_frames_count_dict = {}\r\n\r\n        input_video = cv2.VideoCapture(input_file_path)\r\n        if (camera_input != None):\r\n            input_video = camera_input\r\n\r\n        output_video_filepath = output_file_path + '.avi'\r\n\r\n        frame_width = int(input_video.get(3))\r\n        frame_height = int(input_video.get(4))\r\n        output_video = cv2.VideoWriter(output_video_filepath, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\r\n                                       frames_per_second,\r\n                                       (frame_width, frame_height))\r\n\r\n        counting = 0\r\n        predicted_numbers = None\r\n        scores = None\r\n        detections = None\r\n\r\n\r\n        detection_timeout_count = 0\r\n        video_frames_count = 0\r\n\r\n\r\n        if(self.__model_type == \"yolov3\"):\r\n\r\n\r\n\r\n            while (input_video.isOpened()):\r\n                ret, frame = input_video.read()\r\n\r\n                if (ret == True):\r\n\r\n                    detected_frame = frame.copy()\r\n\r\n                    video_frames_count += 1\r\n                    if (detection_timeout != None):\r\n                        if ((video_frames_count % frames_per_second) == 0):\r\n                            detection_timeout_count += 1\r\n\r\n                        if (detection_timeout_count >= detection_timeout):\r\n                            break\r\n\r\n                    output_objects_array = []\r\n\r\n                    counting += 1\r\n\r\n                    if (log_progress == True):\r\n                        print(\"Processing Frame : \", str(counting))\r\n\r\n\r\n\r\n                    check_frame_interval = counting % frame_detection_interval\r\n\r\n                    if (counting == 1 or check_frame_interval == 0):\r\n                        try:\r\n                            detected_frame, output_objects_array = self.__detector.detectObjectsFromImage(\r\n                                input_image=frame, input_type=\"array\", output_type=\"array\",\r\n                                minimum_percentage_probability=minimum_percentage_probability,\r\n                                display_percentage_probability=display_percentage_probability,\r\n                                display_object_name=display_object_name)\r\n                        except:\r\n                            None\r\n\r\n\r\n                    output_frames_dict[counting] = output_objects_array\r\n\r\n                    output_objects_count = {}\r\n                    for eachItem in output_objects_array:\r\n                        eachItemName = eachItem[\"name\"]\r\n                        try:\r\n                            output_objects_count[eachItemName] = output_objects_count[eachItemName] + 1\r\n                        except:\r\n                            output_objects_count[eachItemName] = 1\r\n\r\n                    output_frames_count_dict[counting] = output_objects_count\r\n\r\n\r\n                    if (save_detected_video == True):\r\n                        output_video.write(detected_frame)\r\n\r\n                    if (counting == 1 or check_frame_interval == 0):\r\n                        if (per_frame_function != None):\r\n                            if (return_detected_frame == True):\r\n                                per_frame_function(counting, output_objects_array, output_objects_count,\r\n                                                   detected_frame)\r\n                            elif (return_detected_frame == False):\r\n                                per_frame_function(counting, output_objects_array, output_objects_count)\r\n\r\n                    if (per_second_function != None):\r\n                        if (counting != 1 and (counting % frames_per_second) == 0):\r\n\r\n                            this_second_output_object_array = []\r\n                            this_second_counting_array = []\r\n                            this_second_counting = {}\r\n\r\n                            for aa in range(counting):\r\n                                if (aa >= (counting - frames_per_second)):\r\n                                    this_second_output_object_array.append(output_frames_dict[aa + 1])\r\n                                    this_second_counting_array.append(output_frames_count_dict[aa + 1])\r\n\r\n                            for eachCountingDict in this_second_counting_array:\r\n                                for eachItem in eachCountingDict:\r\n                                    try:\r\n                                        this_second_counting[eachItem] = this_second_counting[eachItem] + \\\r\n                                                                         eachCountingDict[eachItem]\r\n                                    except:\r\n                                        this_second_counting[eachItem] = eachCountingDict[eachItem]\r\n\r\n                            for eachCountingItem in this_second_counting:\r\n                                this_second_counting[eachCountingItem] = int(this_second_counting[eachCountingItem] / frames_per_second)\r\n\r\n                            if (return_detected_frame == True):\r\n                                per_second_function(int(counting / frames_per_second),\r\n                                                    this_second_output_object_array, this_second_counting_array,\r\n                                                    this_second_counting, detected_frame)\r\n\r\n                            elif (return_detected_frame == False):\r\n                                per_second_function(int(counting / frames_per_second),\r\n                                                    this_second_output_object_array, this_second_counting_array,\r\n                                                    this_second_counting)\r\n\r\n                    if (per_minute_function != None):\r\n\r\n                        if (counting != 1 and (counting % (frames_per_second * 60)) == 0):\r\n\r\n                            this_minute_output_object_array = []\r\n                            this_minute_counting_array = []\r\n                            this_minute_counting = {}\r\n\r\n                            for aa in range(counting):\r\n                                if (aa >= (counting - (frames_per_second * 60))):\r\n                                    this_minute_output_object_array.append(output_frames_dict[aa + 1])\r\n                                    this_minute_counting_array.append(output_frames_count_dict[aa + 1])\r\n\r\n                            for eachCountingDict in this_minute_counting_array:\r\n                                for eachItem in eachCountingDict:\r\n                                    try:\r\n                                        this_minute_counting[eachItem] = this_minute_counting[eachItem] + \\\r\n                                                                         eachCountingDict[eachItem]\r\n                                    except:\r\n                                        this_minute_counting[eachItem] = eachCountingDict[eachItem]\r\n\r\n                            for eachCountingItem in this_minute_counting:\r\n                                this_minute_counting[eachCountingItem] = int(this_minute_counting[eachCountingItem] / (frames_per_second * 60))\r\n\r\n                            if (return_detected_frame == True):\r\n                                per_minute_function(int(counting / (frames_per_second * 60)),\r\n                                                    this_minute_output_object_array, this_minute_counting_array,\r\n                                                    this_minute_counting, detected_frame)\r\n\r\n                            elif (return_detected_frame == False):\r\n                                per_minute_function(int(counting / (frames_per_second * 60)),\r\n                                                    this_minute_output_object_array, this_minute_counting_array,\r\n                                                    this_minute_counting)\r\n\r\n\r\n                else:\r\n                    break\r\n\r\n            if (video_complete_function != None):\r\n\r\n                this_video_output_object_array = []\r\n                this_video_counting_array = []\r\n                this_video_counting = {}\r\n\r\n                for aa in range(counting):\r\n                    this_video_output_object_array.append(output_frames_dict[aa + 1])\r\n                    this_video_counting_array.append(output_frames_count_dict[aa + 1])\r\n\r\n                for eachCountingDict in this_video_counting_array:\r\n                    for eachItem in eachCountingDict:\r\n                        try:\r\n                            this_video_counting[eachItem] = this_video_counting[eachItem] + \\\r\n                                                            eachCountingDict[eachItem]\r\n                        except:\r\n                            this_video_counting[eachItem] = eachCountingDict[eachItem]\r\n\r\n                for eachCountingItem in this_video_counting:\r\n                    this_video_counting[eachCountingItem] = this_video_counting[\r\n                                                                eachCountingItem] / counting\r\n\r\n                video_complete_function(this_video_output_object_array, this_video_counting_array,\r\n                                        this_video_counting)\r\n\r\n            input_video.release()\r\n            output_video.release()\r\n\r\n            if (save_detected_video == True):\r\n                return output_video_filepath\r\n\r\n\r\nclass BoundBox:\r\n    def __init__(self, xmin, ymin, xmax, ymax, objness=None, classes=None):\r\n        self.xmin = xmin\r\n        self.ymin = ymin\r\n        self.xmax = xmax\r\n        self.ymax = ymax\r\n        self.objness = objness\r\n        self.classes = classes\r\n        self.label = -1\r\n        self.score = -1\r\n\r\n    def get_label(self):\r\n        if self.label == -1:\r\n            self.label = np.argmax(self.classes)\r\n\r\n        return self.label\r\n\r\n    def get_score(self):\r\n        if self.score == -1:\r\n            self.score = self.classes[self.get_label()]\r\n\r\n        return self.score\r\n\r\n\r\nclass CustomDetectionUtils:\r\n    def __init__(self, labels):\r\n        self.__labels = labels\r\n        self.__colors = []\r\n\r\n        for i in range(len(labels)):\r\n            color_space_values = np.random.randint(50, 255, size=(3,))\r\n            red, green, blue = color_space_values\r\n            red, green, blue = int(red), int(green), int(blue)\r\n            self.__colors.append([red, green, blue])\r\n\r\n    @staticmethod\r\n    def _sigmoid(x):\r\n        return 1. / (1. + np.exp(-x))\r\n\r\n    def decode_netout(self, netout, anchors, obj_thresh, net_h, net_w):\r\n        grid_h, grid_w = netout.shape[:2]\r\n        nb_box = 3\r\n        netout = netout.reshape((grid_h, grid_w, nb_box, -1))\r\n        nb_class = netout.shape[-1] - 5\r\n        boxes = []\r\n        netout[..., :2] = self._sigmoid(netout[..., :2])\r\n        netout[..., 4:] = self._sigmoid(netout[..., 4:])\r\n        netout[..., 5:] = netout[..., 4][..., np.newaxis] * netout[..., 5:]\r\n        netout[..., 5:] *= netout[..., 5:] > obj_thresh\r\n\r\n        for row in range(grid_h):\r\n            for col in range(grid_w):\r\n                for b in range(nb_box):\r\n                    # 4th element is objectness score\r\n                    objectness = netout[row, col, b, 4]\r\n\r\n                    if objectness <= obj_thresh:\r\n                        continue\r\n\r\n                    # first 4 elements are x, y, w, and h\r\n                    x, y, w, h = netout[row, col, b, :4]\r\n                    x = (col + x) / grid_w  # center position, unit: image width\r\n                    y = (row + y) / grid_h  # center position, unit: image height\r\n                    w = anchors[2 * b + 0] * np.exp(w) / net_w  # unit: image width\r\n                    h = anchors[2 * b + 1] * np.exp(h) / net_h  # unit: image height\r\n                    # last elements are class probabilities\r\n                    classes = netout[row, col, b, 5:]\r\n                    box = BoundBox(x - w / 2, y - h / 2, x + w / 2, y + h / 2, objectness, classes)\r\n                    boxes.append(box)\r\n\r\n        return boxes\r\n\r\n    @staticmethod\r\n    def correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\r\n        new_w, new_h = net_w, net_h\r\n        for i in range(len(boxes)):\r\n            x_offset, x_scale = (net_w - new_w) / 2. / net_w, float(new_w) / net_w\r\n            y_offset, y_scale = (net_h - new_h) / 2. / net_h, float(new_h) / net_h\r\n            boxes[i].xmin = int((boxes[i].xmin - x_offset) / x_scale * image_w)\r\n            boxes[i].xmax = int((boxes[i].xmax - x_offset) / x_scale * image_w)\r\n            boxes[i].ymin = int((boxes[i].ymin - y_offset) / y_scale * image_h)\r\n            boxes[i].ymax = int((boxes[i].ymax - y_offset) / y_scale * image_h)\r\n\r\n    def _interval_overlap(self, interval_a, interval_b):\r\n        x1, x2 = interval_a\r\n        x3, x4 = interval_b\r\n        if x3 < x1:\r\n            if x4 < x1:\r\n                return 0\r\n            else:\r\n                return min(x2, x4) - x1\r\n        else:\r\n            if x2 < x3:\r\n                return 0\r\n            else:\r\n                return min(x2, x4) - x3\r\n\r\n    def bbox_iou(self, box1, box2):\r\n        intersect_w = self._interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\r\n        intersect_h = self._interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\r\n        intersect = intersect_w * intersect_h\r\n        w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin\r\n        w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin\r\n        union = w1 * h1 + w2 * h2 - intersect\r\n\r\n        try:\r\n            result = float(intersect) / float(union)\r\n            return result\r\n        except:\r\n            return 0.0\r\n\r\n    def do_nms(self, boxes, nms_thresh):\r\n        if len(boxes) > 0:\r\n            nb_class = len(boxes[0].classes)\r\n        else:\r\n            return\r\n\r\n        for c in range(nb_class):\r\n            sorted_indices = np.argsort([-box.classes[c] for box in boxes])\r\n\r\n            for i in range(len(sorted_indices)):\r\n                index_i = sorted_indices[i]\r\n\r\n                if boxes[index_i].classes[c] == 0: continue\r\n\r\n                for j in range(i + 1, len(sorted_indices)):\r\n                    index_j = sorted_indices[j]\r\n\r\n                    if self.bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\r\n                        boxes[index_j].classes[c] = 0\r\n\r\n    def get_boxes(self, boxes, labels, thresh):\r\n        v_boxes, v_labels, v_scores = list(), list(), list()\r\n        # enumerate all boxes\r\n        for box in boxes:\r\n            # enumerate all possible labels\r\n            for i in range(len(labels)):\r\n                # check if the threshold for this label is high enough\r\n                if box.classes[i] > thresh:\r\n                    v_boxes.append(box)\r\n                    v_labels.append(labels[i])\r\n                    v_scores.append(box.classes[i] * 100)\r\n                # don't break, many labels may trigger for one box\r\n        return v_boxes, v_labels, v_scores\r\n\r\n    def label_color(self, label):\r\n        \"\"\" Return a color from a set of predefined colors. Contains 80 colors in total.\r\n\r\n        Args\r\n            label: The label to get the color for.\r\n\r\n        Returns\r\n            A list of three values representing a RGB color.\r\n\r\n            If no color is defined for a certain label, the color green is returned and a warning is printed.\r\n        \"\"\"\r\n        if label < len(self.__colors):\r\n            return self.__colors[label]\r\n        else:\r\n            return 0, 255, 0\r\n\r\n    def draw_boxes_and_caption(self, image_frame, v_boxes, v_labels, v_scores, show_names=False, show_percentage=False):\r\n\r\n        for i in range(len(v_boxes)):\r\n            box = v_boxes[i]\r\n            y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\r\n            width, height = x2 - x1, y2 - y1\r\n            class_color = self.label_color(self.__labels.index(v_labels[i]))\r\n\r\n            image_frame = cv2.rectangle(image_frame, (x1, y1), (x2, y2), class_color, 2)\r\n\r\n            label = \"\"\r\n            if show_names and show_percentage:\r\n                label = \"%s : %.3f\" % (v_labels[i], v_scores[i])\r\n            elif show_names:\r\n                label = \"%s\" % (v_labels[i])\r\n            elif show_percentage:\r\n                label = \"%.3f\" % (v_scores[i])\r\n\r\n            if show_names or show_percentage:\r\n                b = np.array([x1, y1, x2, y2]).astype(int)\r\n                cv2.putText(image_frame, label, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (200, 0, 0), 3)\r\n                cv2.putText(image_frame, label, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\r\n\r\n        return image_frame\r\n--------------------------------------------------------------------------------------------------------------------------------------------\r\n**installed packages in the env**\r\nPackage                Version\r\n---------------------- ---------\r\nabsl-py                0.10.0\r\nastunparse             1.6.3\r\ncachetools             4.1.1\r\ncertifi                2020.6.20\r\nchardet                3.0.4\r\ncycler                 0.10.0\r\ngast                   0.3.3\r\ngoogle-auth            1.22.1\r\ngoogle-auth-oauthlib   0.4.1\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.32.0\r\nh5py                   2.10.0\r\nidna                   2.10\r\nimageai                2.1.5\r\nimportlib-metadata     2.0.0\r\nKeras                  2.4.3\r\nKeras-Preprocessing    1.1.2\r\nkiwisolver             1.2.0\r\nMarkdown               3.3.1\r\nmatplotlib             3.3.2\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopencv-python          4.4.0.44\r\nopt-einsum             3.3.0\r\nPillow                 8.0.0\r\npip                    20.2.4\r\nprotobuf               3.13.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\npyparsing              2.4.7\r\npython-dateutil        2.8.1\r\nPyYAML                 5.3.1\r\nrequests               2.24.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.6\r\nscipy                  1.4.1\r\nsetuptools             50.3.2\r\nsix                    1.15.0\r\ntensorboard            2.2.2\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow-estimator   2.2.0\r\ntensorflow-gpu         2.2.0\r\ntermcolor              1.1.0\r\nurllib3                1.25.11\r\nWerkzeug               1.0.1\r\nwheel                  0.35.1\r\nwrapt                  1.12.1\r\nzipp                   3.3.1\r\n", "comments": ["@arita89,\r\nThe code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!\r\n", "ok, i understand, i will try. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44152, "title": "Issue running test_hello_world_test on MAC", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:na\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:\r\n\r\n\r\n**Describe the problem**\r\nAfter cloning tensorflow.git , cd in tensorflow, I'm trying to run the hello_world_test on Mac and get the following error following the gmake command :\r\n\r\ntensorflow/lite/micro/tools/make/Makefile:413: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:413: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/hello_world_test.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/osx_x86_64/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -lm -framework Foundation -framework AudioToolbox\r\nld: unknown option: --fatal-warnings\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\ngmake: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:34: tensorflow/lite/micro/tools/make/gen/osx_x86_64/bin/hello_world_test] Error 1\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ngmake -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\n\r\n\r\n**Any other info / logs**\r\ngmake --version\r\nGNU Make 4.3\r\nBuilt for x86_64-apple-darwin19.2.0\r\n\r\nusing gmake as my make version is 3.81", "comments": ["Can you please point me to example weblink you are trying? Thanks!", "hello,\r\n\r\nI'm following the book chapter 5 p84 , the corresponding github link is  : \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#run-the-tests-on-a-development-machine\r\n\r\non the readme.md file , section :\r\nRun the tests on a development machine \r\n\r\nhope it helps\r\nthanks", "the issue was liked to a directory permission on my local machine which led to warnings with the gitclone command. Once fixed the gmake works just fine.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44152\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44152\">No</a>\n", "Hello, I have got the same error, @KejaPower could you please share exactly how you solved this issue with some links.\r\nThank you.", "Hello,\r\nThe issue was linked to my directory structure on my MAC so I started the git clone from a different clean directory and then it worked perfectly.\r\nhope it helps"]}, {"number": 44151, "title": " model = GDO.minimize(totalLoss) TypeError: minimize() missing 1 required positional argument: 'var_list'", "body": "*i am trying to use Gradient Descent Optimizer in this function \"wx+b\".*\r\n\r\n\r\n**the code is:**\r\n\r\n    import os\r\n    \r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n    \r\n    import tensorflow as tf\r\n    \r\n    w = tf.Variable([5.0], tf.float32)\r\n    b = tf.Variable([7.0], tf.float32)\r\n    \r\n    def linearRegressionModel(x):\r\n        return w * x + b\r\n    \r\n    def totalLoss(x, y):\r\n        return tf.reduce_sum(tf.square(linearRegressionModel(x) - y))\r\n    \r\n    GDO = tf.keras.optimizers.SGD(0.01)\r\n    model = GDO.minimize(totalLoss)\r\n    for i in range(10000):\r\n        model(x=[0, 1, 2, 3, 4, 5], y=[9, 7, 5, 3, 1, -1])\r\n    print(w, b)\r\n\r\nthe result is:\r\n\r\n**line 20, in <module>\r\n    model = GDO.minimize(totalLoss)\r\nTypeError: minimize() missing 1 required positional argument: 'var_list'\r\nProcess finished with exit code 1**\r\n\r\n", "comments": ["The signature of `minimize()` is \r\n\r\n```\r\nminimize(\r\n    loss, var_list, grad_loss=None, name=None\r\n)\r\n```\r\n\r\nAs the error tells you, you're missing the second **required** parameter `var_list`, e.g `model.trainable_weights`", "@mariamalbarghouti,\r\nAs mentioned by @sehoffmann, the `minimize()` method requires two arguments. For more information, please take a look at the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD#minimize). \r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44150, "title": "XLA Conv Docstrings Fix ", "body": "Related to #42513 \r\n\r\nFiles changed:\r\n- `xla.py`\r\n\r\nChanged the docstring to point to the correct XLA operator.", "comments": ["Yes. This PR should be closed. We should open a new one addressing new documentation on `ConvGeneralDilated`. "]}, {"number": 44149, "title": "[INTEL MKL] Removing MKL blob for Windows builds", "body": "This updates https://github.com/tensorflow/tensorflow/pull/43586 with changes for windows. This PR\r\n1) Removes MKL blob completely\r\n2) Enables Windows build to use llvm_openmp and fixes some openMP build issues.\r\n3) Replaces /w with /W0 for windows build in .bazelrc.  as they are equivalent for MSVC. When /w is passed on to the assembler ml64.exe,it is interpreted differently (treats warnings as errors).", "comments": ["Hi @penpornk, I made the requested changes. Please review again. Thanks.", "Hi @penpornk It looks like there was a failure in the internal testing. Please let me know if the PR needs changes.", "@agramesh1 I'm fixing it internally. It should be fine. Thank you for bringing this up! :)"]}, {"number": 44148, "title": "Fix hdfsNewBuilder memory leak", "body": "Call [hdfsNewBuilder](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h#L272) to generate hdfsBuilder. If [hdfsFreeBuilder](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h#L346) or [hdfsBuilderConnect](https://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h#L264) is not called, it will cause memory leaks\r\n\r\nhttps://github.com/apache/hadoop/blob/release-3.3.0-RC0/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/libhdfs/include/hdfs/hdfs.h#L346\r\n```c\r\n    /**\r\n     * Free an HDFS builder.\r\n     *\r\n     * It is normally not necessary to call this function since\r\n     * hdfsBuilderConnect frees the builder.\r\n     *\r\n     * @param bld The HDFS builder\r\n     */\r\n    LIBHDFS_EXTERNAL\r\n    void hdfsFreeBuilder(struct hdfsBuilder *bld);\r\n```", "comments": ["//cc @jhseu @mihaimaruseac @vnvo2409\r\n\r\n\r\n\r\n"]}, {"number": 44147, "title": "[Q] Graph Transformer API", "body": "When will TFv2 support python graph transforms API so that we can use `from tensorflow.tools.graph_transforms import TransformGraph`", "comments": ["Thanks for your issue. Unfortunately the `Graph Transform Tool` is deprecated intentionally since graphs are not central with TF 2.X way.", "Hey @ymodak so, is it like Graph Transform Tools will not be added back in upcoming releases and if want to optimize models, should I go with other alternative? If yes, what are the best options?", "The alternative would be to use `tf.function` to convert your functions into TensorFlow graph.\r\nSee https://www.tensorflow.org/guide/function", "I think I could not convey what I meant exactly. I want to replicate [this noteook available in GCP repo](https://github.com/GoogleCloudPlatform/tf-estimator-tutorials/blob/master/00_Miscellaneous/model_optimisation/Tutorial%20-%20TensorFlow%20Model%20Optimisation%20for%20Serving%20-%20MNIST.ipynb). My doubts are\r\n\r\n1. How can I do it with latest tensorflow release? \r\n2. Should I really consider using optimisations demonstrated in the notebook? Are there any downsides to them (as graph transformer tool are deprecated)", "Hey @ymodak Is it recommended to use Graph Transformer Tool using\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```", "For optimizations such as pruning and quantization you can try [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization/guide).\r\nSee https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide\r\nAlso `tensorflow.compat.v1`  is the right way to use Graph Transform Tool in TF2. (Note that this feature is out of the support window)\r\n", "Thank you very much @ymodak. Closing the issue."]}, {"number": 44146, "title": "Report: AutoGraph could not transform, module 'gast' has no attribute 'Index'", "body": "\r\n**System information**\r\n- source: **https://www.tensorflow.org/tutorials/text/transformer**\r\n- OS Platform: **ArchLinux 5.8.14 x86_64**\r\n- TensorFlow installed from (source or binary): **binary** : https://www.archlinux.org/packages/community/x86_64/tensorflow-cuda/\r\n- TensorFlow version (use command below): **2.3.1** -2\r\n- Python version: **3.8.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **11.1.0** -2 https://www.archlinux.org/packages/community/x86_64/cuda/\r\n- GPU model and memory: GeForce GTX 1660 SUPER computeCapability: 7.5,  22 deviceMemorySize: 5.80GiB\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform \r\n<function train_step at 0x7f15c0790ee0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, \r\nset the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\npython transformer.py\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n[transformer.zip](https://github.com/tensorflow/tensorflow/files/5402845/transformer.zip)\r\n\r\n\r\n", "comments": ["Simple repro:\r\n```python\r\ndef temp(x):\r\n  return tf.shape(x)[0]\r\ntf.autograph.to_graph(temp)\r\n```\r\n\r\n(fails on Today's nightly and works fine on tf@2.3)", "I have tried in colab with TF nightly version(`2.4.0-dev20201019`) and was able to reproduce the issue. However i am not seeing any issue with TF version 2.3. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/350c57a4facc1801f0021845c10288b1/untitled472.ipynb#scrollTo=C2tuobhQlZjZ). Thanks!", "Please check https://github.com/serge-sans-paille/gast/issues/53#issuecomment-712421906", "In the meantime you can use `pip install gast==0.3.3` with tf-nightly.", "The gast project is mainly used by tensorflow. The latest changes have changed the API.\r\n[https://github.com/serge-sans-paille/gast/commit/ba88fcfd8e1d5962cbd509c03039d76f9d05279b](https://github.com/serge-sans-paille/gast/commit/ba88fcfd8e1d5962cbd509c03039d76f9d05279b)\r\n\r\n We need to put them in Tensorflow.\r\n\r\ngast.Index in\r\ntensorflow/python/autograph/converters/slices.py\r\ntensorflow/python/autograph/pyct/qual_names.py\r\ntensorflow/python/autograph/pyct/ast_util.py\r\ntensorflow/python/autograph/pyct/transformer.py\r\ntensorflow/python/eager/gradient_input_output_exclusions.py\r\n\r\nShould I do pull request?", "As I see It was a wrongly updated dependency", "Yep, this should be fixed in tf-nightly. We do need to upgrade TF to gast 0.4, but that should probably be a separate issue.", "https://docs.python.org/3/library/ast.html\r\nDeprecated since version 3.9: Old classes **ast.Index** and **ast.ExtSlice** are still available, but they will be removed in future Python releases. In the meantime, instantiating them will return an instance of a different class.", "> https://docs.python.org/3/library/ast.html\r\n> Deprecated since version 3.9: Old classes **ast.Index** and **ast.ExtSlice** are still available, but they will be removed in future Python releases. In the meantime, instantiating them will return an instance of a different class.\r\n\r\nI think this is a different ticket as the version was already reverted https://github.com/tensorflow/tensorflow/commit/710f3c83b4147eb76e748efcc218325c4978726c. /cc @mihaimaruseac ", "\r\n> I think this is a different ticket as the version was already reverted [710f3c8](https://github.com/tensorflow/tensorflow/commit/710f3c83b4147eb76e748efcc218325c4978726c). /cc @mihaimaruseac\r\n\r\nThat's cool. \r\nThe problem is that gast is just a wrapper around the python library.\r\nWhat will we do when this will not be supported in the next versions of python?", "I dont know. Index is now deprecated: https://github.com/python/cpython/blob/3.9/Lib/ast.py\r\nIt's working now, but there should be a plan B", "@AlexandrParkhomenko if I understand your point correctly - this is what gast does - it takes a Python AST and translates it into a format that's independent of the Python version. In a way you can say it backports all changes from older Python versions.", "> @AlexandrParkhomenko if I understand your point correctly - this is what gast does - it takes a Python AST and translates it into a format that's independent of the Python version. In a way you can say it backports all changes from older Python versions.\r\n\r\nI may have misunderstood. Can I read a tutorial about gast? With examples of tree construction.", "> Can I read a tutorial about gast? With examples of tree construction.\r\n\r\nThere is some information and tests with examples on the project site: https://github.com/serge-sans-paille/gast", "We believe this issue is resolved now - please reopen if you still see it in the latest version.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44146\">No</a>\n", "The problem will be in the future. It doesn't matter now. "]}, {"number": 44145, "title": "Add flagsaver to system absl_py", "body": "The tpu_test_wrapper_test references this and hence errors when this is not defined", "comments": ["This looks good to me, but I'm confused as to how the test is currently passing in our open source builds.\r\nWhere is it you're encountering the error?", "I'm encountering when using TF_SYSTEM_LIBS=absl_py and trying `bazel test` (which doesn't work yet, but this PR helps it get further)"]}, {"number": 44144, "title": "Fix and improve system protobuf", "body": "This fixes an issue that TF build throws\r\n\r\n> google/protobuf/compiler/plugin.proto: google/protobuf/descriptor.proto is imported, but @com_google_protobuf//:compiler_plugin_proto doesn't directly depend on a proto_library that 'srcs' it.\r\n\r\nThe reason is the missing dependency. To fix this and make future updates easier I replaced the current code with the upstream `WELL_KNOWN_PROTO_MAP` and generate the list of files and the `proto_library` invocations out of that, which is basically a copy of the upstream code. This makes it more maintainable and shorter.\r\n\r\nFurthermore I also fix #37835 by introducing a new env and Bazel variable `PROTOBUF_INCLUDE_PATH` (named after e.g. `PYTHON_INCLUDE_PATH` but I'm open to suggestions) to allow installing each dependency into a separate prefix folder as done e.g. on HPC clusters.", "comments": ["Oh whoops sorry I didn't notice this PR. It'll conflict with https://github.com/tensorflow/tensorflow/pull/44222\r\n\r\nI did almost the same well_known update to protobuf too. but turns out that was not sufficient, I had to add an additional bit with the .h files too in mine. I'm not completely sure why tho cuz basically identical worked on tf-2.3. but now for tf-2.4 I needed the `hdrs = HEADERS` too.\r\n\r\nThe second `PROTOBUF_INCLUDE_PATH` part is good tho, that commit should go in as-is.\r\n\r\n@mihaimaruseac @angerson Looks like both of these area already in copybara, will y'all be able to handle merge conflicts internally or should one of use rebase on the other PR?\r\n\r\n@Flamefire Do you see errors like this? This is what I get without the hdrs= line from my PR\r\n\r\n```\r\nERROR: /home/jason/code/tf/tensorflow/tensorflow/core/kernels/batching_util/BUILD:11:11: undeclared inclusion(s) in rule '//tensorflow/core/kernels/batching_util:periodic_function_dynamic':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/batching_util/periodic_function.cc':\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/io/coded_stream.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/io/zero_copy_stream.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/io/zero_copy_stream_impl_lite.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/descriptor.pb.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/arena.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/descriptor.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/map.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/repeated_field.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/text_format.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/util/json_util.h'\r\n  'bazel-out/k8-opt/bin/external/com_google_protobuf/google/protobuf/util/type_resolver_util.h'\r\n```\r\n", "Will handle the conflict internally. Thank you for the heads up", "FTR: @perfinion as mentioned in https://github.com/tensorflow/tensorflow/pull/44489#issuecomment-720081499 those headers shouldn't be there unless the build dir is reused from when the headers were still copied a while ago. Hence deleting that directory and rebuilding should fix it.\r\n\r\nI added #44522 to revert the header-adding for you to verify"]}, {"number": 44143, "title": "import Error. used on ubuntu,apache", "body": "## env\r\n- ubuntu:16 or 18\r\n- apache:2.4\r\n- python:3.6.9\r\n- tensorflow:2.x.x\r\n- django:2.x\r\n\r\nI don't have a clear reason.\r\nThis issue error message is not found in `/var/log/apache2/error.log`.\r\n\r\n## Reproduction method.\r\n1. `conda create -n django python==3.6.9` or `pyenv install python3.6.9`\r\n2. `conda install tensorflow` or `pip install tensorflow`\r\n3. make django simple project\r\n```python:views.py\r\nfrom django.shortcuts import render, redirect\r\nimport tensorflow as tf\r\n\r\ndef func(request):\r\n    return redirect(\"https://www.google.com/\")\r\n```\r\n4. deploy by Apache2.4 used with mod_wsgi\r\n``` \r\n~/etc/apache2/sites-available/django.conf~\r\nLoadModule wsgi_module /home/ubuntu/anaconda3/envs/django/lib/python3.6/site-packages/mod_wsgi/server/mod_wsgi-py36.cpython-36m-x86_64-linux-gnu.so\r\nWSGIPythonHome /home/ubuntu/anaconda3/envs/django\r\nWSGIPythonPath /home/ubuntu/django_project:/home/ubuntu/anaconda3/envs/django/lib/python3.6/site-packages\r\nWSGIScriptAlias / /home/ubuntu/django_project/project/wsgi.py\r\n<Directory /home/ubuntu/django_project/project>\r\n  <Files wsgi.py>\r\n    Require all granted\r\n  </Files>\r\n</Directory>\r\nAlias /static/ /home/ubuntu/django_project/static/\r\n<Directory /home/ubuntu/django_project/static>\r\n  Require all granted\r\n</Directory>\r\n```\r\n5. Access \r\n6. Maybe you will be kept waiting forever .", "comments": ["@waretaflow,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues) and fill in the template, so that the issue can be tracked there. Thanks!\r\n", "Thanks  amahendrakar.\r\nBad mistake. The issue is occrus in used `pip`.\r\nI have corrected the question.", "@waretaflow,\r\nPlease try running the below code snippet to check if TensorFlow is successfully installed\r\n```\r\nimport tensorflow as tf\r\nprint(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n``` \r\n\r\nAlso, please provide the complete logs if possible so that we can determine the source of the error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44143\">No</a>\n"]}, {"number": 44142, "title": "erfp1, erfm1 for more accurate results with single/half precision.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.0\r\n- Are you willing to contribute it (Yes/No): Yes.\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTF contains an implementation of `erf` function. Sometimes it is desirable to compute a probability of a small interval in a Gaussian, i.e. `P(x - eps < X < x)`. This can be done by using erf: `0.5 * (tf.math.erf(x) - tf.math.erf(x-eps))`. Unfortunately, in case of dealing with smaller precision of floating point numbers (single or half) this computation will often lead to zero, i.e:\r\n\r\n```\r\nInput: tf.math.erf(-4)\r\nOutput:  <tf.Tensor: shape=(), dtype=float32, numpy=-1.0>\r\nInput: tf.math.erf(-4.1)\r\nOutput:  <tf.Tensor: shape=(), dtype=float32, numpy=-1.0>\r\n```\r\nOne of the reasons why this happens is probably because of inability of floating point numbers to represent numbers around 1 with a sufficient resolution. I believe the behaviour could be improved if there were two additional functions provided:\r\n\r\n`erfp1(x)` - that computes `erf(x) + 1` using an approximation that makes it numerically more accurate for negative x\r\n`erfm1(x)`- that computes `erf(x) - 1` using an approximation that makes it numerically more accurate for positive x\r\n\r\n**Will this change the current api? How?**\r\n\r\nJust two new functions are added.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThe developers who work with probabilistic models and need to create differentiable models with `log(P(x - eps < X < x))`\r\n\r\n**Any Other info.**\r\nOne of the questions that I have is how the approximation is done currently, i.e. what kind of formula is used?\r\n", "comments": ["Assign me i want to resolve this issue", "So in the end I went with an own implementation of the approximation from Abramowitz and Stegun:\r\n\r\n`1-(a_1 * t + a_2 * t^2 + a_3 * t^3) * e^(-x^2)`\r\nwhere\r\n`t=1/(1+px)`\r\n\r\nThe good thing is that when I want to compute `log(P(x - eps < X < x))` 1s cancel out, and `e` can be extracted outside of log. Only a special case of x-eps < 0, x>0 has to be handled in another way. From my experiments the computations are numerically stable and they give satisfactory accuracy.\r\n\r\nThe only disadvantage is that it increases my computational graph that has been already large and it takes a lot of time to compile with XLA. Still, I believe this can be a solution for other people looking to solve the same thing.", "This is essentially what erfc is for. These could be done through identities:\r\n\r\nerf(x) - 1 = - (1 - erf(x)) = -erfc(x)\r\nerf(x) + 1 = -erf(-x) + 1 = 1 - erf(-x) = erfc(-x)\r\n\r\n"]}, {"number": 44141, "title": "ModuleNotFoundError: No module named 'tensorflow.python.tools'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04.1 LTS (Focal Fossa)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip using a virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n```\r\n$ . venv/bin/activate\r\n$ python\r\nPython 3.8.5 (default, Jul 28 2020, 12:59:40) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/code/tfr-transforms/venv/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\nModuleNotFoundError: No module named 'tensorflow.python.tools'\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n$ pip freeze\r\nabsl-py==0.10.0\r\nappdirs==1.4.4\r\nastunparse==1.6.3\r\nattrs==20.2.0\r\nautopep8==1.5.4\r\nboto3==1.14.33\r\nboto3-stubs==1.14.33.0\r\nbotocore==1.17.63\r\ncachetools==4.1.1\r\ncertifi==2020.6.20\r\ncffi==1.14.3\r\ncfgv==3.2.0\r\nchardet==3.0.4\r\ncycler==0.10.0\r\nCython==0.29.21\r\ndataclasses==0.6\r\ndill==0.3.2\r\ndistlib==0.3.1\r\ndm-tree==0.1.5\r\ndocutils==0.15.2\r\nfilelock==3.0.12\r\nfindspark==1.4.2\r\nflake8==3.8.4\r\nflake8-annotations==2.4.1\r\nflake8-colors==0.1.6\r\nfuture==0.18.2\r\ngast==0.3.3\r\ngin-config==0.3.0\r\ngoogle-api-core==1.22.4\r\ngoogle-api-python-client==1.12.3\r\ngoogle-auth==1.22.1\r\ngoogle-auth-httplib2==0.0.4\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-cloud-bigquery==2.1.0\r\ngoogle-cloud-core==1.4.3\r\ngoogle-crc32c==1.0.0\r\ngoogle-pasta==0.2.0\r\ngoogle-resumable-media==1.1.0\r\ngoogleapis-common-protos==1.52.0\r\ngrpcio==1.32.0\r\nh5py==2.10.0\r\nhttplib2==0.18.1\r\nidentify==1.5.6\r\nidna==2.10\r\nimportlib-metadata==1.7.0\r\nimportlib-resources==3.0.0\r\niniconfig==1.1.1\r\nisort==5.5.4\r\njmespath==0.10.0\r\nkaggle==1.5.8\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.2.0\r\nlinecache2==1.0.0\r\nMarkdown==3.3.1\r\nmatplotlib==3.3.2\r\nmccabe==0.6.1\r\nmypy-boto3==1.14.33.0\r\nmypy-boto3-ec2==1.14.33.0\r\nmypy-boto3-emr==1.14.33.0\r\nmypy-boto3-s3==1.14.33.0\r\nnodeenv==1.5.0\r\nnumpy==1.18.5\r\noauthlib==3.1.0\r\nopencv-python-headless==4.4.0.44\r\nopt-einsum==3.3.0\r\npackaging==20.4\r\npandas==1.1.3\r\nPillow==8.0.0\r\npluggy==0.13.1\r\npre-commit==2.7.1\r\npromise==2.3\r\nproto-plus==1.10.2\r\nprotobuf==3.13.0\r\npsutil==5.7.2\r\npy==1.9.0\r\npy-cpuinfo==7.0.0\r\npy4j==0.10.9\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycodestyle==2.6.0\r\npycparser==2.20\r\npyflakes==2.2.0\r\npyparsing==2.4.7\r\npyspark==2.4.5\r\npyspark-stubs==3.0.0.post1\r\npytest==6.1.1\r\npython-dateutil==2.8.1\r\npython-slugify==4.0.1\r\npytz==2020.1\r\nPyYAML==5.3.1\r\nrequests==2.24.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.6\r\ns3transfer==0.3.3\r\nscipy==1.5.3\r\nsentencepiece==0.1.91\r\nsix==1.15.0\r\nslugify==0.0.1\r\nspark-data-testing==0.1.1\r\nspark-testing-base==0.10.0\r\ntensorboard==2.3.0\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.3.1\r\ntensorflow-addons==0.11.2\r\ntensorflow-datasets==4.0.1\r\ntensorflow-estimator==2.3.0\r\ntensorflow-hub==0.9.0\r\ntensorflow-metadata==0.24.0\r\ntensorflow-model-optimization==0.5.0\r\ntensorflow-ranking==0.3.2\r\ntensorflow-serving-api==2.3.0\r\ntermcolor==1.1.0\r\ntext-unidecode==1.3\r\ntf-models-official==2.3.0\r\ntf-slim==1.1.0\r\ntoml==0.10.1\r\ntqdm==4.50.2\r\ntraceback2==1.4.0\r\ntypeguard==2.10.0\r\nunittest2==1.1.0\r\nuritemplate==3.0.1\r\nurllib3==1.25.10\r\nvirtualenv==20.0.35\r\nWerkzeug==1.0.1\r\nwrapt==1.12.1\r\nzipp==3.3.1\r\n```", "comments": ["I found stackoverflow answer to your query. You can find the link [here](https://stackoverflow.com/questions/41415629/importerror-no-module-named-tensorflow-python). See if doing the following helps you. You can also refer to this issue #34722", "I've deleted the virtualenv and recreated it. On the second install it worked.\r\n\r\nI wonder if it was because during the first installation I had some wheel build failures which required installing some build tools.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44141\">No</a>\n"]}, {"number": 44140, "title": "TFLu: Update Stm32f4 target", "body": "Some tests were filtered out that are now working.\r\nThe enclosing ifdef TARGET is no longer needed.\r\n\r\nThis partly fix: https://github.com/tensorflow/tensorflow/issues/43898\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44139, "title": "Image Recognition Experimental main.cc resolver name error", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.4.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): mbed Discovery STM32 F746NG\r\n\r\n**Describe the problem**\r\nThe example project \"Image Recognition Experimental\" does not build correctly due to a name error of the resolver argument in the statement \"tflite::MicroInterpreter interpreter(model, **resolver**, tensor_arena, tensor_arena_size, error_reporter);\" of the main.cc source. The argument name **resolver** should be replaced by **micro_op_resolver**. \r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nMaking/building the project:\r\n -) make -f tensorflow/lite/micro/tools/make/Makefile TAGS=disco_f746ng generate_image_recognition_mbed_project\r\n -) cd tensorflow/lite/micro/tools/make/gen/linux_x86_64/prj/image_recognition/mbed/\r\n -) mbed config root . \r\n -) mbed deploy\r\n -) mbed compile -m DISCO_F746NG -t GCC_ARM --profile release\r\n\r\nPull request: #44136", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44139\">No</a>\n"]}, {"number": 44138, "title": "TypeError: 'InputLayer' object is not iterable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.6\r\n- TensorFlow installed from (source or binary): using pip3 install\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nWhen I try to convert tensorflow model to CoreML model using coremltools I get this error: TypeError: 'InputLayer' object is not iterable\r\n\r\n**Describe the expected behavior**\r\nTensorflow model should convert to CoreML model without any errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nmodel.save(modelFolder)\r\n\r\ncoreMLModel = convert(modelFolder, input_names = ['input'], output_names = ['output'])\r\ncoreMLModel.save(folderManager.modelsFolder)\r\n", "comments": ["@PlasticLunatic \r\nPlease refer to these resolved issue with same error and let us know:\r\n#30204 #35813 [link](https://github.com/tensorflow/tensorflow/issues/24622).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44138\">No</a>\n"]}, {"number": 44137, "title": "CUDA_ERROR_NOT_INITIALIZED when using multiprocessing with ImageDataGenerator + random hue preprocessing function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Red Hat 7.4\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): GCC 7.3.0\r\n- CUDA/cuDNN version: CUDA 10.1/cuDNN 7.6.5 \r\n\r\n**Describe the current behavior**\r\n\r\nCurrently using ImageDataGenerator to load my data and apply random augmentations during training. This works fine with multiprocessing (setting `use_multiprocessing=True` in `model.fit`), until I add a preprocessing function into my data generator to perform random hue augmentations (using `tf.image.random_hue`). Doing this gives me the following error: \r\n`2020-10-19 18:33:13.420481: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n`\r\n\r\nI've also tried using `tf.image.stateless_random_hue` from `tf-nightly`, since that adjusts the hue deterministically, however I run into the same issue regardless. \r\n\r\nIt works as expected when I set `use_multiprocessing=False`, however since I'm training on a large dataset this would create a significant bottleneck, so any possible fixes or workarounds would be greatly appreciated.\r\n\r\n**Standalone code to reproduce the issue**\r\n(example code assuming there are _train_ and _test_ directories contain 2 classes of images to be read in using `flow_from_directory`)\r\n\r\n```python\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import (Conv2D, BatchNormalization,\r\n                                     MaxPool2D, Flatten, Dense)\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.image import random_hue\r\nimport numpy as np\r\n\r\n# random hue augmentations - PROBLEMATIC WITH MULTIPROCESSING\r\ndef color_augmentation(image):\r\n    return random_hue(image, 0.1)\r\n\r\n# get the data generators\r\n# apply random augmentations during training including hue augmentations\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1./255,\r\n    vertical_flip = True,\r\n    horizontal_flip = True,\r\n    preprocessing_function = color_augmentation\r\n)\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    'train/',\r\n    target_size = (96, 96),\r\n    batch_size = 32,\r\n)\r\n\r\ntest_generator = test_datagen.flow_from_directory(\r\n    'test/',\r\n    target_size = (96, 96),\r\n    batch_size = 32,\r\n)\r\n\r\n# create model\r\nmodel = Sequential()\r\nmodel.add(Conv2D(filters=32, strides=1,input_shape=(96,96,3),\r\n                 activation='relu', kernel_size=3, padding='same'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool2D())\r\nmodel.add(Conv2D(filters=64, strides=1, activation='relu',\r\n                 kernel_size=3, padding='same'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool2D())\r\nmodel.add(Flatten())\r\nmodel.add(Dense(units=2, activation='softmax'))\r\n\r\n# compile model\r\nmodel.compile(\r\n    optimizer=Adam(learning_rate=0.01),\r\n    loss='categorical_crossentropy',\r\n    metrics=['accuracy']\r\n)\r\n\r\n# fit the model, using multiprocessing\r\nhistory = model.fit(\r\n    x = train_generator,\r\n    steps_per_epoch = len(train_generator),\r\n    epochs = 5,\r\n    verbose = 1,\r\n    validation_data = test_generator,\r\n    validation_steps = len(test_generator),\r\n    workers = 4,\r\n    use_multiprocessing = True,\r\n    max_queue_size = 8\r\n)\r\n\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-10-19 19:34:54.590677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-19 19:34:57.529118: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-19 19:34:57.547866: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\r\n2020-10-19 19:34:57.552945: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558e11bd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-19 19:34:57.552983: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-19 19:34:57.556307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-10-19 19:34:57.784089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555558ea46c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-19 19:34:57.784175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\r\n2020-10-19 19:34:57.786709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:25:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n2020-10-19 19:34:57.786764: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-19 19:34:57.791365: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-10-19 19:34:57.795504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-10-19 19:34:57.797375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-10-19 19:34:57.801231: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-10-19 19:34:57.803387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-10-19 19:34:57.811940: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-10-19 19:34:57.815365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-19 19:34:57.815402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-10-19 19:34:58.501529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-19 19:34:58.501575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-10-19 19:34:58.501581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-10-19 19:34:58.503865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 14951 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:25:00.0, compute capability: 6.0)\r\n2020-10-19 19:34:58.862187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:25:00.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\r\ncoreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\r\n\r\n...\r\n\r\nWARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\r\nFound GPU at: /device:GPU:0\r\nFound 1200 images belonging to 2 classes.\r\nFound 200 images belonging to 2 classes.\r\nEpoch 1/5\r\n2020-10-19 19:34:59.947779: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:34:59.949834: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:34:59.952581: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:34:59.956651: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.090631: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.135802: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.171513: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.221924: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.358671: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-10-19 19:35:00.403905: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n```", "comments": ["@smehra34 \r\n\r\nPlease, share thr complete standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. I have tried in colab and i am seeing the below error message.(`FileNotFoundError: [Errno 2] No such file or directory: 'train/'`). Thanks!", "Sorry about that, here's the same code but loading in the CIFAR10 dataset instead.\r\n\r\n```python\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import (Conv2D, BatchNormalization,\r\n                                     MaxPool2D, Flatten, Dense)\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.image import random_hue\r\nfrom tensorflow.keras.datasets import cifar10\r\nimport numpy as np\r\n\r\n\r\n# random hue augmentations - PROBLEMATIC WITH MULTIPROCESSING\r\ndef color_augmentation(image):\r\n    return random_hue(image, 0.1)\r\n\r\n# get the datagenerators. Apply random augmentations during training including\r\n# random hue augmentations\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1./255,\r\n    vertical_flip = True,\r\n    horizontal_flip = True,\r\n    preprocessing_function = color_augmentation\r\n)\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\n(x_train, y_train),(x_test, y_test) = cifar10.load_data()\r\n\r\ntrain_generator = train_datagen.flow(\r\n     x_train, y=y_train, batch_size=32\r\n)\r\n\r\ntest_generator = test_datagen.flow(\r\n    x_test, y=y_test, batch_size=32\r\n)\r\n\r\n# create model\r\nmodel = Sequential()\r\nmodel.add(Conv2D(filters=32, strides=1,input_shape=(32,32,3),\r\n                 activation='relu', kernel_size=3, padding='same'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool2D())\r\nmodel.add(Conv2D(filters=64, strides=1, activation='relu',\r\n                 kernel_size=3, padding='same'))\r\nmodel.add(BatchNormalization())\r\nmodel.add(MaxPool2D())\r\nmodel.add(Flatten())\r\nmodel.add(Dense(units=10, activation='softmax'))\r\n\r\n# compile model\r\nmodel.compile(\r\n    optimizer=Adam(learning_rate=0.01),\r\n    loss='categorical_crossentropy',\r\n    metrics=['accuracy']\r\n)\r\n\r\n# fit the model, using multiprocessing\r\nhistory = model.fit(\r\n    x = train_generator,\r\n    steps_per_epoch = len(train_generator),\r\n    epochs = 5,\r\n    verbose = 1,\r\n    validation_data = test_generator,\r\n    validation_steps = len(test_generator),\r\n    workers = 4,\r\n    use_multiprocessing = True,\r\n    max_queue_size = 8\r\n)\r\n\r\n```", "@smehra34 \r\n\r\nI tried in colab with TF version 2.3 but the code is keep on running. Please, see the gist [here](https://colab.research.google.com/gist/ravikyram/b45c8035f32ee121cf3c715af034dc19/untitled467.ipynb).Thanks!", "Sorry I probably wasn't clear enough before but when that warning comes up, it seems to just hang and not actually train the model rather than throwing an error and terminating the program. I tried using your gist and the same thing happens. If you look at the runtime log on colab, the same CUDA_ERROR_NOT_INITIALISED warning shows up, and the model doesn't actually seem to be training at all (verbose flag is set to 1, however it doesn't show that anything is training. If you set use_multiprocessing=False however, or get rid of the random_hue preprocessing function, it all works as expected).", "@smehra34 \r\n\r\nGlad to know it worked. Please,close this thread if your issue was resolved. Thanks!", "No I mean it's still not working. Even though the program doesn't terminate due to this warning, it's not executing as it should be either. It just hangs indefinitely without actually training the model.", "@smehra34 If you try with 2.3.x/tf-nightly you will see a warning:\r\n`WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.`", "@smehra34 As mentioned above, try using tf.data would solve your problem", "Thanks @bhack and @gowthamkpr! Just tested it briefly and wrapping the data generators with `tf.data.Dataset.from_generator` seems to do the trick. I'll close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44137\">No</a>\n", "Hi all, \r\nI used multiprocess with apply_async to run multi process with GPUs. I trained model with use_multiprocessing=False and config.gpu_options.allow_growth = True but it returned error:\r\n```\r\n2021-10-06 09:12:50.537049: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.541392: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.543215: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.558302: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2021-10-06 09:12:50.656339: F tensorflow/stream_executor/cuda/cuda_driver.cc:153] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n```"]}, {"number": 44136, "title": "Update main.cc", "body": "Hello, the argument \"resolver\" of    tflite::MicroInterpreter interpreter(model, resolver, tensor_arena, tensor_arena_size, error_reporter);   should be replaced by \"micro_op_resolver\".\r\n\r\nFixes #44139 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44136) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44136) for more info**.\r\n\r\n@googlebot I signed it!"]}, {"number": 44135, "title": "i am a new coder , so i just editied indentations . hope u like it an\u2026", "body": "\u2026d accept my request", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44135) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 44134, "title": "Invoke returns NaN values when performing on floating points", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host Platform: Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from source,\r\n- Tensorflow version: 86726adc39a1\r\n- Target platform: STM32H745 (ARM M7)\r\n\r\nI have Keras model trained on host platform. I want to use this model on my target device, after conversion to .tflite format. Once I perform conversion with quantization and obtain INT8 weights values, model works fine and gives correct predictions.\r\nBut when I try to convert model without quantization and try to run it with floating points values, inout vector and output vector contain NaN values after calling Invoke(). \r\n\r\nInput vector after calling invoke on INT8 weights:\r\n![int](https://user-images.githubusercontent.com/58625554/96422788-c5060600-11f8-11eb-8d23-6e7a12bd7c27.PNG)\r\n\r\nInput vector after calling invoke on Floating Points weights:\r\n![float](https://user-images.githubusercontent.com/58625554/96422840-d5b67c00-11f8-11eb-9cf6-13be10c915a2.PNG)\r\n\r\nCode that I use to convert with quantization:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport numpy as np\r\nimport pickle\r\n\r\nmodel = keras.models.load_model('data/models/best_model.hdf5')\r\n\r\nX_train = pickle.load(open(\"dataset_from_target.pickle\", \"rb\"))['x_train']\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nmodel_quant_tflite = converter.convert()\r\n\r\n\r\ndef representative_dataset():\r\n  for i in range(1000):\r\n    yield([np.expand_dims(X_train[i].astype(np.float32), axis=0)])\r\n\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\nconverter.representative_dataset = representative_dataset\r\nmodel_tflite = converter.convert()\r\n\r\nopen(\"model_target.tflite\", \"wb\").write(model_tflite)\r\n```\r\n\r\n\r\n\r\nCode that I use to convert with no quantization:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\n\r\nmodel = keras.models.load_model('data/models/best_model.hdf5')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nmodel_no_quant_tflite = converter.convert()\r\n\r\n# Set the optimization flag.\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nmodel_tflite = converter.convert()\r\n\r\nopen(\"model_target_new_data_no_quant.tflite\", \"wb\").write(model_tflite)\r\n```\r\n\r\n\r\nProblem is not with RAM memory, which I have already checked. NaN values in 'input->data.f' results in all NaN values in 'output->data.f' which makes predictions impossible. Before calling invoke, input vector contains correct values. \r\n\r\n\r\n", "comments": ["I noticed that the prolem disappear when I get rid of optimization during conversion:\r\n\r\n```\r\nmodel = keras.models.load_model('data/models/best_model.hdf5')\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nmodel_tflite = converter.convert()\r\n\r\nopen(\"model_target_new_data_no_quant.tflite\", \"wb\").write(model_tflite)\r\n```\r\n\r\nBut then the model weight is twice as big as with the optimization.", "Can you try enforcing full integer quantization for all ops including the input and output?\r\nSee https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only\r\n```python\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\nmodel_tflite = converter.convert()\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44134\">No</a>\n"]}]