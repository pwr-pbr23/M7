[{"number": 31765, "title": "Docker images for 1.12 version without jupyter", "body": "**System information**\r\n-  Linux Ubuntu 16.04 - Docker + docker-nvidia\r\n- TensorFlow installed from Docker tensorflow\r\n- TensorFlow version: 1.12.0 - 1.12.3\r\n- Python version: py 2 and py3\r\n- GPU: 2x1080\r\n\r\n\r\n\r\n**Problem Description**\r\n\r\nWhile trying to install older version of tensorflow for testing I was met with unexpected behaviour. It seems that the usual non-jupyter docker tags point to jupyter version. \r\n\r\n`sudo docker run -it --rm tensorflow/tensorflow:1.12.0-gpu-py3`\r\nreturns : \r\n\r\n```\r\nUnable to find image 'tensorflow/tensorflow:1.12.0-gpu-py3' locally\r\n1.12.0-gpu-py3: Pulling from tensorflow/tensorflow\r\n18d680d61657: Already exists \r\n0addb6fece63: Already exists \r\n78e58219b215: Already exists \r\neb6959a66df2: Already exists \r\ne3eb30fe4844: Already exists \r\n852c9b7a4425: Already exists \r\n0a298bf31111: Already exists \r\nf43ecd71dda8: Already exists \r\n9f554feaeba1: Already exists \r\nabf1fc85d970: Already exists \r\n3e67c4ad17bb: Already exists \r\nc60e8159f45c: Already exists \r\n2b01db739666: Already exists \r\n1553de0cb9ac: Already exists \r\n1ed5c01b0218: Already exists \r\n9913722703a5: Already exists \r\n442335dc9a85: Already exists \r\nDigest: sha256:84f0820e151b129c63ac15c6d9c1c5336a834070dca22a271c7de091d490a17f\r\nStatus: Downloaded newer image for tensorflow/tensorflow:1.12.0-gpu-py3\r\n[I 12:58:14.924 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\n[I 12:58:14.946 NotebookApp] Serving notebooks from local directory: /notebooks\r\n[I 12:58:14.946 NotebookApp] The Jupyter Notebook is running at:\r\n[I 12:58:14.946 NotebookApp] http://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5\r\n[I 12:58:14.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 12:58:14.946 NotebookApp] \r\n    \r\n    Copy/paste this URL into your browser when you connect for the first time,\r\n    to login with a token:\r\n        http://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5\r\n^C[I 12:59:06.586 NotebookApp] interrupted\r\nServing notebooks from local directory: /notebooks\r\n0 active kernels\r\nThe Jupyter Notebook is running at:\r\nhttp://(8c019c13c8ce or 127.0.0.1):8888/?token=f940ba4e239d161fcd710a013b9755eaedef83dff5e090d5\r\n```\r\n\r\nAnd not the expected: \r\n\r\n\r\n```\r\n34667c7e4631: Already exists \r\nd18d76a881a4: Already exists \r\n119c7358fbfc: Already exists \r\n2aaf13f3eff0: Already exists \r\n643564d518c8: Already exists \r\n1fea03e629a4: Already exists \r\n45402f4cf61d: Already exists \r\n45f7c407b07b: Already exists \r\n00e5163fe3e0: Already exists \r\n7d071071ef98: Pull complete \r\n5119bdada1e4: Pull complete \r\n64a9355aa772: Pull complete \r\n21f5ce47fe21: Pull complete \r\n5566cd4bac12: Pull complete \r\n58c608a4c711: Pull complete \r\nDigest: sha256:0f949ccc690d9c50e9b46b16d9030c2c6845af621c2e7e82c4bf59803edc69b5\r\nStatus: Downloaded newer image for tensorflow/tensorflow:1.12.0-gpu-py3\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nWARNING: You are running this container as root, which can cause new files in\r\nmounted volumes to be created as the root user on your host machine.\r\n\r\nTo avoid this, run the container by specifying your user's userid:\r\n\r\n$ docker run -u $(id -u):$(id -g) args...\r\n\r\nroot@9f4ce2da7d6a:/# \r\n```\r\n\r\n\r\nRunning the exact same tags on 1.13 seems to work as expected according to the documentation. \r\n", "comments": ["@Vargeel, Try, first pull the image `docker pull tensorflow/tensorflow:1.12.0-gpu-py3`. and then run the container. Let us know how it progresses. Thanks!", "Hi @gadagashwini ,\r\n\r\nhere is my output.\r\n\r\n```\r\nsudo docker pull tensorflow/tensorflow:1.12.0-gpu-py3\r\n1.12.0-gpu-py3: Pulling from tensorflow/tensorflow\r\nDigest: sha256:84f0820e151b129c63ac15c6d9c1c5336a834070dca22a271c7de091d490a17f\r\nStatus: Image is up to date for tensorflow/tensorflow:1.12.0-gpu-py3\r\n```\r\nThis did not affect the output of the docker run command used earlier.", "@Vargeel, Thanks for trying. \r\nThe above mentioned docker image is available in docker [documentation](https://hub.docker.com/r/tensorflow/tensorflow/tags?page=5).Please try other image. let us know. Thanks!  ", "@gadagashwini \r\nYes, I used the hub.docker page to run tests on several different versions of the images. \r\nFrom what I could tell, the 1.12.\\*-jupyter are all missing or unaccessible. But the version of the image that I can access (1.12.\\*-gpu and 1.12.\\*-gpu-py3) are actually built with jupyter notebooks. ", "This is intended behavior (although perhaps not optimal). Docker images prior to 1.13 included Jupyter by default, and we did not republish those earlier containers in order to avoid breaking existing workflows. As described on the [docker hub page](https://hub.docker.com/r/tensorflow/tensorflow):\r\n\r\n> The tags described below are accurate for all releases starting with TF 1.13. Older releases are still tagged using the older format and images.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31765\">No</a>\n", "To run without Jupyter try:\r\n`sudo docker run -it --rm tensorflow/tensorflow:1.12.0-gpu-py3 /bin/bash`\r\n", "> To run without Jupyter try:\r\n> `sudo docker run -it --rm tensorflow/tensorflow:1.12.0-gpu-py3 /bin/bash`\r\n\r\nThanks, this works for me"]}, {"number": 31764, "title": "Protocol \"https\" not supported or disabled in libcurl", "body": "Hi Guys,\r\n \r\ncan anyone help me with the following issue\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from source.\r\n- TensorFlow version: r1.11\r\n- Python version: 2.7.12\r\n- GCC/Compiler version (if compiling from source): GNU 5.4.0\r\n**Describe the problem**\r\n\r\nfrom top directory of tensorflow iam running the script as follows\r\n./tensorflow/tools/ci_build/linux/cmake/run.sh \r\n iam stuck with the following error\r\n\r\nerror: downloading 'https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz' failed\r\n         status_code: 1\r\n         status_string: \"Unsupported protocol\"\r\n         log:\r\n         --- LOG BEGIN ---\r\n         Protocol \"https\" not supported or disabled in libcurl\r\nlike above error there are number of  files that are being not downloaded because of that iam getting build error.\r\n\r\nthis is my curl version iam using \r\n\r\ncurl -V\r\ncurl 7.66.0-DEV (x86_64-pc-linux-gnu) libcurl/7.66.0-DEV OpenSSL/1.0.2g zlib/1.2.8\r\nRelease-Date: [unreleased]\r\nProtocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp \r\nFeatures: AsynchDNS HTTPS-proxy IPv6 Largefile libz NTLM NTLM_WB SSL TLS-SRP UnixSockets\r\n\r\n\r\nThank you\r\nSnathick\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["> Hi Guys,\r\n> \r\n> can anyone help me with the following issue\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n> * TensorFlow installed from source.\r\n> * TensorFlow version: r1.11\r\n> * Python version: 2.7.12\r\n> * GCC/Compiler version (if compiling from source): GNU 5.4.0\r\n>   **Describe the problem**\r\n> \r\n> from top directory of tensorflow iam running the script as follows\r\n> ./tensorflow/tools/ci_build/linux/cmake/run.sh\r\n> iam stuck with the following error\r\n> \r\n> error: downloading 'https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz' failed\r\n> status_code: 1\r\n> status_string: \"Unsupported protocol\"\r\n> log:\r\n> --- LOG BEGIN ---\r\n> Protocol \"https\" not supported or disabled in libcurl\r\n> like above error there are number of files that are being not downloaded because of that iam getting build error.\r\n> \r\n> this is my curl version iam using\r\n> \r\n> curl -V\r\n> curl 7.66.0-DEV (x86_64-pc-linux-gnu) libcurl/7.66.0-DEV OpenSSL/1.0.2g zlib/1.2.8\r\n> Release-Date: [unreleased]\r\n> Protocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp\r\n> Features: AsynchDNS HTTPS-proxy IPv6 Largefile libz NTLM NTLM_WB SSL TLS-SRP UnixSockets\r\n> \r\n> Thank you\r\n> Snathick\r\n\r\n@snathicktechno ,\r\nCan you let us know from which top directory are you running the script ?Thanks\r\n", "Hi oanush, \r\n\r\nI resolved the above error and i now stuck with the following error \r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04)\r\nTensorFlow installing from source using cmake\r\nTensorFlow version: r1.0\r\nPython version: 2.7.12, 3.5 also\r\nGCC/Compiler version (if compiling from source): GNU 5.4.0\r\n\r\nthe steps i followed \r\ncd tensorflow\r\n./tensorflow/tools/ci_build/linux/cmake/run.sh which is calling CMakeLists.txt at path tensorflow-r1.0/tensorflow/contrib/cmake/CMakeLists.txt\r\n\r\nMy Environment cmake probing\r\n\r\n+ rm -rf build\r\n+ mkdir -p build\r\n+ pushd build\r\n~/Downloads/tensorflow-r1.1/build ~/Downloads/tensorflow-r1.1\r\n+ cmake -pthread -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n-- The C compiler identification is GNU 5.4.0\r\n-- The CXX compiler identification is GNU 5.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - found\r\n-- Looking for pthread_create\r\n-- Looking for pthread_create - not found\r\n-- Looking for pthread_create in pthreads\r\n-- Looking for pthread_create in pthreads - not found\r\n-- Looking for pthread_create in pthread\r\n-- Looking for pthread_create in pthread - found\r\n-- Found Threads: TRUE  \r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Success\r\n-- Found PythonInterp: /usr/bin/python (found version \"2.7.12\") \r\n-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython2.7.so (found version \"2.7.12\") \r\n-- Found SWIG: /usr/bin/swig3.0 (found version \"3.0.8\") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/sadiq/Downloads/tensorflow-r1.1/build\r\n\r\niam stuck with following error\r\n\r\nCMakeFiles/Makefile2:79: recipe for target 'CMakeFiles/tf_python_build_pip_package.dir/rule' failed\r\nmake[1]: *** [CMakeFiles/tf_python_build_pip_package.dir/rule] Error 2\r\nMakefile:118: recipe for target 'tf_python_build_pip_package' failed\r\nmake: *** [tf_python_build_pip_package] Error 2\r\n\r\ncan you help me to resolve this issue\r\n\r\nThank you\r\nSnathick\r\n", "@snathicktechno ,\r\nCan you please install TF using [Bazel](https://www.tensorflow.org/install/source) and not cmake? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31764\">No</a>\n"]}, {"number": 31763, "title": "ImportError: DLL load failed with error code -1073741795", "body": "I follow installation instructions from tensorflow.org. I use CPU version.\r\n\r\nI have seen many similar problems, but all of them were due to unsupported AVX. \r\nMy CPU supports it, but tensorflow still doesn't work.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Wisnows 7 64\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: pip in virtualenv\r\n- CUDA/cuDNN version: CPU version\r\n- GPU model and memory: CPU version\r\n\r\n**Describe the problem**\r\nAfter installation I try to run:\r\n\r\n`python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nand I get error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\S1614414\\venv\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\nWhat could be the issue?\r\n\r\nEdit: I have found reason - I had AVX disabled on my system.\r\nThis issue can be closed.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow-GPU** (TF) prebuilt binaries.\n\n **TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.** \n\n* If you have above configuration and using _**Windows**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNNinstallation directories to the %PATH% environment variable. \n     * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n     * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n     * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n\nPlease let us know if this helps.", "I have found reason - I had AVX disabled on my system.\r\nThis issue can be closed.", "Just to verify did you get chance to follow instructions from TensorFlow [website ](https://www.tensorflow.org/install/pip).Please, let us know. \r\nAlso, please, go through below link and see if it helps you.Thanks!\r\nhttps://github.com/tensorflow/tensorflow/issues/24882"]}, {"number": 31762, "title": "Create GPU Delegate failed,  dequantize error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10+\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (use command below): r1.14.0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI converted my tflite model with F16 quantilization, then I create GPU Delegate to run my model,\r\nthe source code version of tensorflow is r1.14.0. I got the following error when running the inference:\r\nInitialized TensorFlow Lite runtime.\r\nInternal error: Failed to apply delegate: tensorflow/lite/kernels/dequantize.cc:62 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 was not true.Node number 1 (DEQUANTIZE) failed to prepare.\r\n\r\n**Describe the expected behavior**\r\nTensorflow Lite can run Float16 model with GPU Delegate correctly.\r\n\r\n**Code to reproduce the issue**\r\ngpuDelegate = new GpuDelegate();\r\ntfliteOptions.addDelegate(gpuDelegate);\r\ntfliteOptions.setAllowFp16PrecisionForFp32(true);\r\ntflite = new Interpreter(tfliteModel,tfliteOptions);\r\n\r\n**Other info / logs**\r\nThe code and cmd to convert the model:\r\npython freeze_graph.py --input_graph=2d-3d-match-pointnet/model_test.pb --input_checkpoint=2d-3d-match-pointnet/model_test.ckpt --output_graph=2d-3d-match-pointnet/tmp/model.pb --output_node_names=moutput\r\n\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"./model.pb\"\r\ninput_arrays = [\"minput\"]\r\noutput_arrays = [\"moutput\"]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.lite.constants.FLOAT16]\r\ntflite_model = converter.convert()\r\nopen(\"model_quant_f16.tflite\", \"wb\").write(tflite_model)\r\n\r\n I would appreciate it if you could help check this issue. thanks again.", "comments": ["Since we added FP16 support, the condition at tensorflow/lite/kernels/ddequantize.cc now reads:\r\n\r\n> TF_LITE_ENSURE(context, op_context.input->type == kTfLiteUInt8 ||\r\n>                               op_context.input->type == kTfLiteInt8 ||\r\n>                               op_context.input->type == kTfLiteInt16 ||\r\n>                               op_context.input->type == kTfLiteFloat16);\r\n\r\nI'm wondering whether your inference engine is actually supporting FP16 or not.  Can you confirm that you're using master's latest snapshot?", "Thank u, my tensorflow version is r1.14.0 which is the official release and not support FP16 yet.\r\nthe latest version maybe is not stable and I got some compile errors, so I chose r1.14.0,  I will try the latest version and check again. \r\n\r\nBR ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31762\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31762\">No</a>\n", "I'm having the same issue! Did anyone figure this out?"]}, {"number": 31761, "title": "[tf.data] Listing files, sampling a subset and repeating iterates over each file when shuffling is enabled", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nTesting the tf.data setup, I want to select a subset of the data to read from disc. Doing that with `tf.data.Dataset.list_files(\"*\", shuffle=True).take(2)` iterates over the entire folder despite the explicit `take`.\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen shuffling is enabled in `tf.data.Dataset.list_files`, I should be able to repeat the random sample indefinitely.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFirst, create dummy data.\r\n\r\n```shell\r\nmkdir scratchpad\r\ncd scratchpad\r\nfor i in {0..100}; do touch $i; done\r\n```\r\n\r\nThen, in IPython,\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\ntf.global_variables_initializer().run()\r\n\r\nit = (\r\n    tf.data.Dataset.list_files(os.path.join(\"*\"))\r\n    .take(2)\r\n    .repeat(None)\r\n    .make_one_shot_iterator()\r\n    .get_next()\r\n)\r\n\r\n[sess.run(it) for _ in range(12)]\r\n```\r\n\r\nThe above works as expected with shuffling disabled:\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\ntf.global_variables_initializer().run()\r\n\r\nit = (\r\n    tf.data.Dataset.list_files(os.path.join(\"*\"), shuffle=False)\r\n    .take(2)\r\n    .repeat(None)\r\n    .make_one_shot_iterator()\r\n    .get_next()\r\n)\r\n\r\n[sess.run(it) for _ in range(12)]\r\n```", "comments": ["Kindly find the [gist](https://colab.sandbox.google.com/drive/1lJcKSBoAtIzDfvMzRMyplRdr4MIR9n9y#scrollTo=MmMVwI7XrefD) of colab when tried executing the given codes.Thanks!", "Your input pipeline is currently repeatedly picking a pair of random files. If you want to instead repeat the two randomly selected files infinitely, put `cache` between `take` and `repeat`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31761\">No</a>\n", "@jsimsa, thanks for your response! I wonder whether it is ok that the example without shuffling works as expected without caching, instead of going over the consecutive pairs of files. Shouldn't caching be enabled by default if there is a repeat coming after shuffling, like it is for the in-order processing?", "When shuffling is enabled, you need to list all files in order to pick two randomly. \r\n\r\nThe input pipeline without `cache` repeatedly randomly samples two files. The input pipeline with `cache` repeats the same two (randomly sampled) files. I don't think that caching should be enabled by default because the behaviors are different (and both meaningful).", "Agreed. Could the description of this behavior be added to the [official docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) at least?", "I am not sure what exactly would you like to be added (as documentation of each tf.data transformation should make sense on its own), but I am happy to review a PR with your suggestion."]}, {"number": 31760, "title": "Build failure seems related with C++14 support of GCC 4.8 w.r.t. MLIR dependency", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA, building\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nFailed to build latest TF from source with command (which works fine on branch `r1.14`, and works on `master` maybe one month ago):\r\n```sh\r\nbazel build --jobs=8 -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --config=noaws --config=nogcp --config=nohdfs --config=noignite --config=nokafka --config=nonccl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nSeems c++14 support is required when building TF (with MLIR dependency introduced), while I am using GCC 4.8, which doesn't include C++14 support. I am wondering what is the recommended GCC version? I see the official builds are marked with GCC 4.8. Do you guys encounter this issue?\r\n\r\nThe failed log:\r\n```\r\nERROR: /home/wzh/.cache/bazel/_bazel_wzh/7830c7af24b3349eb6de8dd12d5e8378/external/local_config_mlir/BUILD:418:1: C++ compilation of rule '@local_config_mlir//:Support' failed (Exit 1)\r\ngcc-4.8: error: unrecognized command line option '-std=c++14'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n", "comments": ["Update: can build when switched to GCC 7. The question is can GCC 4.8 build it.\r\n\r\nPS. I see there is `TF 1.14` label was added, though this is about C++14 on master branch.\r\n\r\nThanks.", "I think @av8ramit can provide more context on this, but the GCC 4.8 we're using is part of a devtoolset package ~~which uses 4.8 as a base and pulls in newer features~~ (see comments below). Most users should start using GCC 7 if possible.\r\n\r\n@av8ramit, do you know if we're planning to publish more information about this?", "So the gcc we use as a base is 5.4, but we build our presubmits using a toolchain which has gcc 7.3.1. If you use our custom-op-gpu docker container that may resolve this.\r\n\r\n`tensorflow/tensorflow:custom-op-ubuntu16`\r\n\r\nToolchain flag: \r\n```\r\n--crosstool_top=//third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010-nvcc-cuda10.0:toolchain\r\n```\r\n\r\nThe toolchain still may not support C++14, but I'm working on a patch right now to get that working.", "[https://www.tensorflow.org/install/source#tested_build_configurations](url)  in this page , the tf 1.14 is assign gcc 4.8 , if you suggest gcc 7 , pls change the document, thx ;", "Thank you everyone, that helps a lot.\r\n\r\nI suggest to move the toolchain related documents (tool version, ABI flags and so on) to a section maybe name as **Requirements** or **Environment Setup**, and put it around https://www.tensorflow.org/install/source#install_python_and_the_tensorflow_package_dependencies. Or maybe let the python dependency document as part of the **Requirements** or **Environment Setup**.\r\n\r\nI remember that when I was new to TensorFlow, the Python version (3.7 is not supported at that time) and the GCC tool version made me having a bad experience.\r\n\r\nI am now closing this issue as it is no longer a problem for me. But it will be great if the building document can improve.\r\n\r\nThank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31760\">No</a>\n", "@angersson \r\n\r\n> but the GCC 4.8 we're using is part of a devtoolset package which uses 4.8 as a base and pulls in newer features.\r\n\r\nThat's not how devtoolset works. The devtoolset GCC is a newer compiler, period. It's not \"4.8 plus new features\". The clever part is that it links to the `libstdc++.so` from an older GCC and _also_ an extra `libstdc++_nonshared.a` with symbols for newer features. But the actual `g++` compiler is still not \"GCC 4.8\".", "Thanks for the clarification, @jwakely! I updated my comment earlier to make sure I don't confuse anyone."]}, {"number": 31759, "title": "Upsampling to a odd number", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary \r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: V100 32GB\r\n\r\n\r\nCurrent Behaviour:\r\nRaises a Value Error: ambiguous dimension hence affecting workflow\r\n\r\nExpected:\r\nUpsample the tensor to desired size using single side padding\r\n\r\nCode to reproduce the issue:\r\n\r\na = tf.random.uniform(shape=[1,45,60,3], dtype=tf.dtypes.float32)\r\nkernel_size = (45/2,60/2)\r\nb = AveragePooling2D(pool_size=kernel_size, strides=kernel_size, padding='same')(a)\r\nc = UpSampling2D(size=kernel_size, data_format='channels_last', interpolation='bilinear')(b)\r\n\r\n\r\n", "comments": ["@drnightbaron, Looks like code is incomplete. please provide the complete code to replicate the issue. Thanks!", "import tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\na = tf.random.uniform(shape=[1,45,60,3], dtype=tf.dtypes.float32)\r\nkernel_size = (45/2,60/2)\r\nb = AveragePooling2D(pool_size=kernel_size, strides=kernel_size, padding='valid')(a)\r\nc = UpSampling2D(size=kernel_size, data_format='channels_last', interpolation='bilinear')(b)\r\n\r\nThank you for the prompt response. This  is the code I am using. I restarted the kernel and I don't get the earlier error but the tensor sizes after upsampling is coming to (1, 44, 60, 3) rather than (1, 45, 60, 3) ", "I am able to replicate the issue with Tensorflow 1.13. Please see the colab [gist](https://colab.research.google.com/drive/1_b--wZipoCTQlbr-WZ3SSCI9kzVKLIjU) here. Thanks!", "This is exactly the error that I faced. Thanks! ", "@drnightbaron I can reproduce the issue even with `tf-nightly` and the [gist is here](https://colab.sandbox.google.com/gist/jvishnuvardhan/f860c0e4d2167fd4839d17c155cae268/tf_31759.ipynb).\r\n\r\nJust curious what kind of use case you have that requires large `pool_size` and `strides`. In your example, shape of `a` is `[1,45,60,3]`. When you apply `pooling` on `a`, it downsamples  `a` and resulting shape of `b` is [1,2,2,3]. Then you want to upsample `b` to `[1,45,60,3]`. Is that right? Thanks!", "Well I was trying to implement a recent paper in tensorflow . An implementation for pytorch is there. But not for tensorflow. Since `Adaptive Average pooling` is not a inbuilt function, I was trying to manually create a workaround for that, and I ran into this snag \r\n\r\nhttps://github.com/CommissarMa/Context-Aware_Crowd_Counting-pytorch for more reference\r\n", "The issue is already fixed in TF 2.x. For the arg `kernel_size` in both `AveragePooling2D` and `UpSampling2D`, it needs to be integers though."]}, {"number": 31758, "title": "dataset as input to estimator is broken.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Does not depend on OS.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-0-g87989f6959, 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nDataset input to estimator is broken\r\n\r\n**Describe the expected behavior**\r\n\r\nPassing the a dataset object to `estimator.fit` method results in the following  error: \r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-6-1ed4dbb580ca> in <module>()\r\n     14 est.fit(\r\n     15     steps=1000,\r\n---> 16     input_fn=lambda : csv_input_fn(train_path, batch_size))\r\n\r\n2 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)\r\n   1036       random_seed.set_random_seed(self._config.tf_random_seed)\r\n   1037       global_step = training_util.create_global_step(g)\r\n-> 1038       features, labels = input_fn()\r\n   1039       self._check_inputs(features, labels)\r\n   1040       training_util._get_or_create_global_step_read()  # pylint: disable=protected-access\r\n\r\nValueError: too many values to unpack (expected 2)\r\n```\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nFollowing example from the docs can be used to reproduce the problem. Check  the definition of the main function.\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nTRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\r\nTEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n\r\nCSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\r\n                    'PetalLength', 'PetalWidth', 'Species']\r\nSPECIES = ['Setosa', 'Versicolor', 'Virginica']\r\n\r\ndef maybe_download():\r\n    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\r\n    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\r\n\r\n    return train_path, test_path\r\n\r\ndef load_data(y_name='Species'):\r\n    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\r\n    train_path, test_path = maybe_download()\r\n\r\n    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\r\n    train_x, train_y = train, train.pop(y_name)\r\n\r\n    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\r\n    test_x, test_y = test, test.pop(y_name)\r\n\r\n    return (train_x, train_y), (test_x, test_y)\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features=dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\n# The remainder of this file contains a simple example of a csv parser,\r\n#     implemented using the `Dataset` class.\r\n\r\n# `tf.parse_csv` sets the types of the outputs to match the examples given in\r\n#     the `record_defaults` argument.\r\nCSV_TYPES = [[0.0], [0.0], [0.0], [0.0], [0]]\r\n\r\ndef _parse_line(line):\r\n    # Decode the line into its fields\r\n    fields = tf.decode_csv(line, record_defaults=CSV_TYPES)\r\n\r\n    # Pack the result into a dictionary\r\n    features = dict(zip(CSV_COLUMN_NAMES, fields))\r\n\r\n    # Separate the label from the features\r\n    label = features.pop('Species')\r\n\r\n    return features, label\r\n\r\n\r\ndef csv_input_fn(csv_path, batch_size):\r\n    # Create a dataset containing the text lines.\r\n    dataset = tf.data.TextLineDataset(csv_path).skip(1)\r\n\r\n    # Parse each line.\r\n    dataset = dataset.map(_parse_line)\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n  \r\n  \r\ndef main():\r\n  train_path, test_path = maybe_download()\r\n  batch_size = 100\r\n  CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\r\n                      'PetalLength', 'PetalWidth', 'Species']\r\n\r\n  est = tf.contrib.learn.LinearClassifier(\r\n      CSV_COLUMN_NAMES,\r\n      n_classes=3\r\n  )\r\n\r\n  est.fit(\r\n      steps=1000,\r\n      input_fn=lambda : csv_input_fn(train_path, batch_size))\r\n  \r\nmain()\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I was able to replicate the issue for TF-1.14 version, kindly find the [Gist](https://colab.research.google.com/drive/1i0lArKoDJRKJsUGqPj73diEnostxEJ6W) of colab.Thanks!", "@aashudwivedi Sorry for the late response. Is this still an issue for you. Can you please go through the [tutorial](https://www.tensorflow.org/tutorials/estimator/linear) on TF website. Thanks!", "No it's not. I implemented my own SVM using tf instead.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31758\">No</a>\n", "Closing as it is no longer an issue"]}, {"number": 31757, "title": "Bug fix for Tensorflow Lite quantized ops tests", "body": "Use division instead of multiplication when applying\r\nthe quantisation scale in [PerChannelQuantizeBias()](https://github.com/tensorflow/tensorflow/blob/1f61f13f8715dc26dabe46a2686216674026d812/tensorflow/lite/kernels/test_util.h#L239)\r\n\r\nFixes #31756", "comments": ["Can one of the admins verify this patch?", "\"Ubuntu Sanity internal CI build failed\" - is there a way to see what went wrong? Build details are linked to for successful builds, but not for the failing one."]}, {"number": 31756, "title": "tflite: incorrect quantisation scale application in unit test utils", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, the bug was discovered when evaluating new quantisation modes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): discoverd on Linux Ubuntu 16.04, but should be applicable to any platform where the tests are run\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master branch\r\n- Python version: Python3.6\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): GCC 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe quantization scale in [PerChannelQuantizeBias()](https://github.com/tensorflow/tensorflow/blob/1f61f13f8715dc26dabe46a2686216674026d812/tensorflow/lite/kernels/test_util.h#L239) in [tensorflow/lite/kernels/test_util.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/test_util.h) appears to be applied incorrectly. The floating point input data are multiplied by the scale value whereas they should be divided by it. The corresponding tests in [conv_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/conv_test.cc) and [depthwise_conv_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/depthwise_conv_test.cc), e.g. [SimplePerChannelTest](https://github.com/tensorflow/tensorflow/blob/1f61f13f8715dc26dabe46a2686216674026d812/tensorflow/lite/kernels/conv_test.cc#L1343) appear to contain the wrong expected values.\r\n\r\n**Describe the expected behavior**\r\nThe division operation should be used to convert the floating point to quantized fixed point value and the corresponding tests changes respectively.\r\n\r\n**Code to reproduce the issue**\r\nN/A\r\n\r\n**Other info / logs**\r\nN/A\r\n", "comments": ["Thanks for filing the bug. Let's continue the discussion on https://github.com/tensorflow/tensorflow/pull/31757", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31756\">No</a>\n"]}, {"number": 31755, "title": "tf.function decorator not found", "body": "Hi,\r\n\r\nin the current nightly build, tf.function cannot be found.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview 2.0.0.dev20190818\r\n- Python version: 3.6\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef my_func():\r\n   print(tf.__version__)\r\n```\r\n\r\n**Other info / logs**\r\nAttributeError: module 'tensorflow' has no attribute 'function'\r\n", "comments": ["Problem solved. This was an issue with my virtual environment. ", "What was the issue?", "@wolfiex TF 1.x was installed. Make sure that TF 2.0 is installed to use the tf.function decorator.", "Ah yes, just made the same mistake, Thanks!\r\n", "My CPU does not support TF 2.x,  but I need this decorator. Is there any proper solution?"]}, {"number": 31754, "title": "No tf.lite.experimental.nn.bidirectional_dynamic_rnn ops is finded", "body": "**System information**\r\n- Linux Ubuntu 16.04:\r\n- TensorFlow installed from anaconda:\r\n- TensorFlow version 1.14:\r\n\r\nwhen i from tensorflow._api.v1.lite.experimental.nn import bidirectional_dynamic_rnn, i get the error:\r\nAttributeError: module 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'bidirectional_dynamic_rnn'\r\n\r\nbut i find in https://tensorflow.google.cn/lite/convert/rnn that the ops has be supported.\r\n\r\nby the way, tf.lite.experimental.nn.dynamic_rnn can be import as https://tensorflow.google.cn/lite/convert/rnn shows.\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nmodule 'tensorflow._api.v1.lite.experimental.nn' has no attribute 'bidirectional_dynamic_rnn'\r\n```\r\n\r\n", "comments": ["Hi banbishan, can you use from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn instead?", "> Hi banbishan, can you use from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn instead?\r\n\r\nHi renjie-liu, thank you for you reply. No errors occurred when I use from tensorflow.lite.experimental.examples.lstm.rnn import bidirectional_dynamic_rnn instead."]}, {"number": 31753, "title": "How to profile in tensorflow 2.0", "body": "I can't find the profile method of tensorflow 2.0 in google or github. How to use profiler and timeline in tensorflow 2.0. Is there any other way profile tensorflow 2.0;", "comments": ["You may try using tensorboard for profiling.\r\nSee https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras", "> You may try using tensorboard for profiling.\r\n> See https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras\r\n\r\nThanks for your reply!\r\nIf I am not using keras,  just a simple matrix operation,  and then use tf.function to turn into a graph.  How to use tensorboard profiling?", "> > You may try using tensorboard for profiling.\r\n> > See https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras\r\n> \r\n> Thanks for your reply!\r\n> If I am not using keras, just a simple matrix operation, and then use tf.function to turn into a graph. How to use tensorboard profiling?\r\n\r\nI find some information in :[](https://www.tensorflow.org/tensorboard/r2/graphs#graphs_of_tffunctions).\r\nMaybe I know how to do it. Thank you very much.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31753\">No</a>\n", "> > > You may try using tensorboard for profiling.\r\n> > > See https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras\r\n> > \r\n> > \r\n> > Thanks for your reply!\r\n> > If I am not using keras, just a simple matrix operation, and then use tf.function to turn into a graph. How to use tensorboard profiling?\r\n> \r\n> I find some information in :.\r\n> Maybe I know how to do it. Thank you very much.\r\n\r\nCould you please tell how to do it? Thanks", "> > > > You may try using tensorboard for profiling.\r\n> > > > See https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras\r\n> > > \r\n> > > \r\n> > > Thanks for your reply!\r\n> > > If I am not using keras, just a simple matrix operation, and then use tf.function to turn into a graph. How to use tensorboard profiling?\r\n> > \r\n> > \r\n> > I find some information in :.\r\n> > Maybe I know how to do it. Thank you very much.\r\n> \r\n> Could you please tell how to do it? Thanks\r\n\r\n@JestinyJie I find a way to do the profile in TF2.0\r\nPlease refer to this link: https://stackoverflow.com/questions/56690089/how-to-graph-tf-keras-model-in-tensorflow-2-0", "Hi, how to use profile API and profile service? The python module can't import after tensorflow.__init__.\r\n"]}, {"number": 31752, "title": "Getting 404 in XLA JIT page", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/xla/jit\r\n\r\n## Description of issue (what needs changing):\r\nGetting not found when opening\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@cheshire : FYI.\r\n\r\nI've got a fix inflight. Thanks for reporting.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31752\">No</a>\n", "We do not recommend using `XLA_*` devices, as their usage can cause various issues, that's why the page was deleted. Instead, the recommended way is to use auto-clustering, documented at https://www.tensorflow.org/xla"]}, {"number": 31751, "title": "ValueError: ('Input has undefined rank:', TensorShape(None))", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nwindows 10, anaconda python 3.7\r\ntensorflow 2.0b1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n![image](https://user-images.githubusercontent.com/27112868/63238990-9babbc00-c27b-11e9-9f3f-be7fd9165bdd.png)\r\n\r\nProblem : ValueError: ('Input has undefined rank:', TensorShape(None))\r\nwhen I used line 62-63 instead of 64,  there will be no errors.\r\nI am not sure why  line 64 is not working correctly.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport time\r\nimport math\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow as keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import plot_model\r\nfrom scipy . stats import multivariate_normal as normal\r\ntf.keras.backend.set_floatx('float64')\r\n\r\nd = 10\r\nT = 0.1\r\nn_time = 5\r\nn_sample = 10\r\nbatch_size = 10\r\nn_maxstep = 400\r\nh = (T + 0.0) / n_time\r\nt_stamp = np.arange (0, n_time) * h\r\n\r\ndef f_tf (t, X, Y, Z):\r\n    V =  Y - tf.math.pow (Y,3)\r\n    return V\r\n\r\ndef g_tf (t, X):\r\n    V =  0.5 / (1 + 0.2*tf.math.reduce_sum (X**2, 1, keepdims=True))\r\n    return V\r\n\r\ndef sample_path ( n_sample ):\r\n    dW_sample = np.zeros ([ n_sample, d, n_time  ], dtype = np.float64)\r\n    X_sample  = np.zeros ([ n_sample, d, n_time+1], dtype = np.float64)\r\n    for i in range (n_time):\r\n        dW_sample [:, :, i  ] = np.reshape ( normal.rvs ( mean = np.zeros(d,dtype = np.float64),\\\r\n                                      cov =1, size = n_sample ) * np.sqrt(h), ( n_sample, d))\r\n\r\n        X_sample  [:, :, i+1] =  X_sample  [:, :, i] + np.sqrt(2) * dW_sample [:, :, i]\r\n    return dW_sample, X_sample\r\n\r\ndef nn_tf(x):\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    x = keras.layers.Dense(d+10, batch_size = n_sample)(x)\r\n\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    x = keras.layers.Dense(d+10, batch_size = n_sample)(x)\r\n\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    x = keras.layers.Dense(d, batch_size = n_sample)(x)\r\n\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n    return x\r\n\r\ndW = keras.Input(shape = (d, n_time  ), batch_size=n_sample, dtype = tf.float64, name = 'dW')\r\nXX = keras.Input(shape = (d, n_time+1), batch_size=n_sample, dtype = tf.float64, name = 'X' )\r\nX = XX\r\nY = tf.ones([n_sample, 1], dtype = tf.float64)\r\nZ = tf.ones([n_sample, d], dtype = tf.float64)\r\n\r\nfor it in range(n_time-1):\r\n    with tf.name_scope(str(it+1)):\r\n        Y = Y - f_tf(t_stamp[it], X[:,:,it], Y, Z) * h\r\n        Y = Y +  tf.math.reduce_sum( Z * dW[:,:,it],  1, keepdims=True)\r\n        #subX = tf.reshape(X[:,:,it], shape = [n_sample, d])\r\n        #Z = nn_tf(subX) / d\r\n        Z = nn_tf(X[:,:,it]) / d\r\n\r\nY = Y - f_tf(t_stamp[n_time-1], X[:,:,n_time-1], Y, Z) * h\r\nY = Y + tf.math.reduce_sum (Z * dW [:, :, n_time-1], 1, keepdims=True)\r\nmodel = keras.Model(inputs=[XX,dW], outputs=[Y])\r\n\r\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\nfor epoch in range (100):\r\n    dW_train, X_train = sample_path ( n_sample )\r\n\r\n    with tf.GradientTape() as tape:\r\n        predictions = model( [X_train, dW_train] )\r\n        label = g_tf (T, X_train[:, :, n_time])\r\n        #print(\"label = \", label)\r\n        #print(\"predictions = \", predictions)\r\n        #loss_value = tf.reduce_sum( tf.keras.losses.MSE (label, predictions ) )\r\n        loss_value = ( tf.keras.losses.MeanSquaredError() (label, predictions ) )\r\n        #print(\"label = \", label)\r\n        #print(\"predictions = \", predictions)\r\n        print(\"lss = \", loss_value )\r\n    grads = tape.gradient(loss_value,  model.trainable_variables)\r\n    optimizer.apply_gradients( zip(grads, model.trainable_variables) )\r\n    accuracy = train_accuracy(label, predictions)\r\n    print(\"step \", epoch, \"loss = \", loss_value.numpy(), \"accuracy = \", accuracy.numpy())\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nrunfile('C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py', wdir='C:/Users/DELL/Desktop/DP_PDE_PY')\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-42-59f0f0a64686>\", line 1, in <module>\r\n    runfile('C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py', wdir='C:/Users/DELL/Desktop/DP_PDE_PY')\r\n\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 826, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py\", line 64, in <module>\r\n    Z = nn_tf(X[:,:,it]) / d\r\n\r\n  File \"C:/Users/DELL/Desktop/DP_PDE_PY/PDE_DP_1.py\", line 40, in nn_tf\r\n    x = keras.layers.BatchNormalization(batch_size = n_sample)(x)\r\n\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 616, in __call__\r\n    self._maybe_build(inputs)\r\n\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1951, in _maybe_build\r\n    self.build(input_shapes)\r\n\r\n  File \"D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\normalization.py\", line 270, in build\r\n    raise ValueError('Input has undefined rank:', input_shape)\r\n\r\nValueError: ('Input has undefined rank:', TensorShape(None))\r\n", "comments": ["Is it possible for you to try latest TF versions(`!pip install tf-nightly-2.0-preview==2.0.0.dev20190819`) and let us know whether the issue persists? .I am not seeing any issues with latest nightly versions. Thanks!", "![image](https://user-images.githubusercontent.com/27112868/63347035-d51e1d80-c388-11e9-8c93-232ed238d9ff.png)\r\n\r\nThank you, When i try your command in anaconda python3.7,   no packages is found.\r\nI could not find a solution,   do you know the reason?  thanks\r\n  \r\n", "I am able to install nightly versions in my environment. Can you check it again by creating virtual environment and install using `pip install tf-nightly-2.0-preview==2.0.0.dev20190819`.I have tried  in Colab as well and I am not seeing any issues in nightly versions.Please, find the [gist ](https://colab.research.google.com/drive/1pZVwwPhaaNs1l55tfMeF1P1xOeJ7lltt)here.Thanks!", "Great thanks!"]}, {"number": 31750, "title": "random crop edge case", "body": "This PR is about random crop edge case to guarantee output size dimensions.", "comments": ["Can one of the admins verify this patch?", "Thanks @alextp, I've corrected it. ", "@alextp can you please review ?", "@alextp, do you have any suggestions? ", "@autoih Can you please check alextp's comments and keep us posted. Thanks!", "@rmlarsen can someone help review this python kernel implementation?", "@rmlarsen Can you please review this PR ? Thanks!", "Hi! Is this PR still relevant?  It looks like it has been some months, but if you can update it and it passes the checks again, I would be happy to review it. \r\nThanks!", "@autoih  Can you please check @jsmeredith's comments and keep us posted ? Thanks!", "@autoih  Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31749, "title": "tf.keras: Error in BinaryCrossentropy loss when output shape is determined by inputs", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.0.0-beta1-4983-g65e6355ad9 2.0.0-rc0\r\n- Python version:\r\n- Bazel version (if compiling from source): bazel 26.1\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nShape mismatch for binary cross-entropy with logits for compiled Keras  when output shape is determined by model inputs (no error when using eager-evaluation):\r\n```\r\nValueError: logits and labels must have the same shape ((None,) vs (None, 1))\r\n```\r\n**Describe the expected behavior**\r\nNo shape mismatch\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninputs = tf.keras.Input(shape=(1,), dtype=tf.int64)\r\noutputs = tf.random.normal(shape=(inputs[0, 0],))\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\nmodel.compile(optimizer='sgd', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\r\n\r\ndummy = np.zeros((10, 1))\r\nshapes = np.random.randint(0, 11, 10).reshape(10, 1, 1)\r\ndataset = tf.data.Dataset.from_tensor_slices(shapes)\r\ndataset = dataset.map(lambda x: (x, tf.random.uniform(minval=0, maxval=2, shape=(x[0,0],), dtype=tf.int32)))\r\n\r\n# eagar mode, works\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\nfor inputs, true in dataset:\r\n    pred = model(inputs)\r\n    print(true)\r\n    print(pred)\r\n    print(loss(true, pred))\r\n    break\r\n\r\n# compiled model, fails\r\nmodel.fit(dataset, epochs = 1, steps_per_epoch=1)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 928, in merge_with\r\n    self.assert_same_rank(other)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 983, in assert_same_rank\r\n    (self, other))\r\nValueError: Shapes (None, 1) and (None,) must have the same rank\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py\", line 168, in sigmoid_cross_entropy_with_logits\r\n    labels.get_shape().merge_with(logits.get_shape())\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\", line 934, in merge_with\r\n    raise ValueError(\"Shapes %s and %s are not compatible\" % (self, other))\r\nValueError: Shapes (None, 1) and (None,) are not compatible\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 25, in <module>\r\n    model.fit(dataset, epochs = 1, steps_per_epoch=1)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 736, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 307, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 121, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 84, in execution_function\r\n    for out in distributed_function(input_fn)]\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 427, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1834, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2134, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2025, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 884, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 320, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 71, in distributed_function\r\n    per_replica_function, args=(model, x, y, sample_weights))\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 760, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1787, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 2132, in _call_for_each_replica\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 299, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 307, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 252, in _process_single_batch\r\n    training=training))\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 166, in _model_loss\r\n    per_sample_losses = loss_fn.call(targets[i], outs[i])\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\", line 216, in call\r\n    return self.fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\", line 989, in binary_crossentropy\r\n    K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 4449, in binary_crossentropy\r\n    return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\r\n  File \"/usr/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py\", line 171, in sigmoid_cross_entropy_with_logits\r\n    (logits.get_shape(), labels.get_shape()))\r\nValueError: logits and labels must have the same shape ((None,) vs (None, 1))\r\n```", "comments": ["@deasmhumhna\r\nThe code seems to work fine in 2.0.0-beta1\r\n<img width=\"1217\" alt=\"Screenshot 2019-08-19 at 10 56 45 AM\" src=\"https://user-images.githubusercontent.com/24864163/63241259-b462a480-c270-11e9-980a-611763da674c.png\">\r\n", "I'm technically using Tensorflow 2.0.0-rc0, built from source.\r\n\r\nHere's another, somewhat cleaner version using `dataset_from_generator`:\r\n```\r\nimport numpy as np\r\n\r\nprint('Tensorflow', tf.__version__)\r\n\r\ninputs = tf.keras.Input(shape=(1,), dtype=tf.int32)\r\noutputs = tf.random.normal(\r\n    shape=(tf.reduce_max(inputs) - tf.reduce_min(inputs) + 1,))\r\n\r\nmodel = tf.keras.Model(inputs, outputs)\r\nmodel.compile(optimizer='sgd',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\r\r\n\r\ndef gen():\r\n    while True:\r\n        x = np.random.normal(5, 10, size=(5, 1)).astype('i4')\r\n        y = np.random.randint(0, 2, size=int(np.ptp(x) + 1)).astype('i4')\r\n        yield (x, y)\r\n\r\ndataset = tf.data.Dataset.from_generator(\r\n    gen, output_types=(tf.int32, tf.int32),\r\n    output_shapes=(tf.TensorShape([5, 1]), tf.TensorShape([None])))\r\n\r\n# eager mode, works\r\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\nfor inputs, true in dataset:\r\n    pred = model(inputs)\r\n    print(true.shape)\r\n    print(pred.shape)\r\n    print(loss(true, pred))\r\n    break\r\n\r\n# compiled model, fails\r\nmodel.fit(dataset, epochs = 1, steps_per_epoch=1)\r\n```  ", "I'm re-building tensorflow right now. I'll see if that helps.", "@deasmhumhna ,\r\nWhen tried executing the code in TF-2.0.0beta1 and 1.14 worked fine. Any update on the issue after you re-building the TF?Thanks!", "I haven't finished re-building yet, but I keep getting strange errors with `@tf.function` that aren't even consistent across models/files (simple models pass, but complex models break the same training loops, simple and complex nested models give different errors, etc). I suspect something might be wrong with the Autograph files in my build. In the meantime, I might just reinstall the `pip` build of Tensorflow 2.0. ", "@deasmhumhna ,\r\nAny update after tried rebuilding TF?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31749\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31749\">No</a>\n"]}, {"number": 31748, "title": "[Todo] Implement edge case to guarantee output size dimensions", "body": "This PR is about the edge case to guarantee output size dimensions of random crop ops. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31748) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 31747, "title": "Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS10.14.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:iPhone7 & iPhone6\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version: tensorflow-lite-gpu:0.0.0\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:cocoapod\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:16G memory\r\n\r\n**Describe the problem**\r\n\r\nI am building my codes with tensorflow_lite_gpu.framework on iPhone 7. The codes is just like Google recommends:\r\n\r\n```\r\n_model = FlatBufferModel::BuildFromFile(modelPathCString);\r\n        ops::builtin::BuiltinOpResolver resolver;\r\n        InterpreterBuilder(*_model, resolver)(&_interpreter);\r\n        _delegate = NewGpuDelegate(nullptr);  // default config\r\n        _interpreter->ModifyGraphWithDelegate(_delegate);\r\n...//Other codes\r\n```\r\nWhen I use the model mobilenet_v1_1.0_224.tflite which Google provides, I get warning: `WARNING: 25 cannot be handled by this delegate.  Only the first 30 ops will run on the GPU, and the remaining 1 on the CPU` from console after excuting `_interpreter->ModifyGraphWithDelegate(_delegate)`. But the model deeplabv3_257_mv_gpu.tflite is good which Google provides too. As for my own model, the result is like mobilenet_v1_1.0_224.tflite does. Someone please help me.\r\n\r\nPS: Here is my podfile: \r\n```\r\nplatform :ios, '10.0'\r\ntarget \"SpeechExample\" do\r\npod 'TensorFlowLiteGpuExperimental'\r\nend\r\n```\r\n", "comments": ["This is a duplicate of https://github.com/tensorflow/tensorflow/issues/31720"]}, {"number": 31746, "title": "[Intel MKL] Fixing a member variable initialization issue", "body": "Fixing a member variable initialization issue detected by static code scans.", "comments": []}, {"number": 31745, "title": "Fix link to XLA documentation", "body": "", "comments": []}, {"number": 31744, "title": "Raspberry Pie 4", "body": "Raspberry Pie 4\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\npi@raspberrypi:/xiaolu/u/soft/tensorflow $ sudo ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n+ set -e\r\n+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n++ cd ./tensorflow/lite/tools/make\r\n++ pwd\r\n+ SCRIPT_DIR=/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make\r\n+ TENSORFLOW_DIR=/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../..\r\n+ make -j 4 TARGET=rpi -C /xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile -latomic\r\nmake: \u8fdb\u5165\u76ee\u5f55\u201c/xiaolu/u/soft/tensorflow\u201d\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/ -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/ruy/kernel_arm32.cc -o /xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o\r\n/tmp/ccIPb9CY.s: Assembler messages:\r\n/tmp/ccIPb9CY.s:451: \u9519\u8bef\uff1aNeon quad precision register expected -- `vld1.32 q11,[r2]'\r\nmake: *** [tensorflow/lite/tools/make/Makefile:244\uff1a/xiaolu/u/soft/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o] \u9519\u8bef 1\r\nmake: \u79bb\u5f00\u76ee\u5f55\u201c/xiaolu/u/soft/tensorflow\u201d\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have the same problem with cross-compiling with Ubuntu on Windows\r\n\r\n\r\n/tmp/ccuFx1oC.s: Assembler messages:\r\n/tmp/ccuFx1oC.s:455: Error: Neon quad precision register expected -- `vld1.32 q11,[r2]'\r\ntensorflow/lite/tools/make/Makefile:244: recipe for target '/mnt/z/linux/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o' failed\r\nmake: *** [/mnt/z/linux/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/ruy/kernel_arm32.o] Error 1", "Having the same problem under Ubuntu 19.04 when cross-compiling with Docker:\r\n\r\n<details>\r\n<summary>Dockerfile</summary>\r\n\r\n```\r\nFROM tensorflow/tensorflow:nightly-devel\r\n\r\nUSER root\r\n\r\nRUN apt-get update && \\\r\n    apt-get install -y --no-install-recommends \\\r\n    build-essential \\\r\n    crossbuild-essential-armhf \\\r\n    git\r\n\r\nRUN git clone https://github.com/tensorflow/tensorflow\r\n\r\nRUN ./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh\r\n\r\nWORKDIR ./tensorflow\r\nRUN ./tensorflow/lite/tools/make/build_rpi_lib.sh\r\n```\r\n</details>", "Looks like Ruy was using something unsupported on RasberryPi.", "It's not a hardware issue, it's an assembler syntax issue.  One line of asm is sloppy and had been accidentally accepted by the android toolchain, but the RPi toolchain is being more rigorous here. The fix should be below. We're submitting this internally so you can expect this to be working soon, but always interesting to hear a confirmation.\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/experimental/ruy/kernel_arm32.cc b/tensorflow/lite/experimental/ruy/kernel_arm32.cc\r\n--- a/tensorflow/lite/experimental/ruy/kernel_arm32.cc\r\n+++ b/tensorflow/lite/experimental/ruy/kernel_arm32.cc\r\n@@ -947,7 +947,7 @@ void Kernel8bitNeonOutOfOrder(const Kern\r\n         \"ldr r5, [%[params], #\" RUY_STR(RUY_OFFSET_RHS_ZERO_POINT) \"]\\n\"\r\n \r\n         // Load 4 lhs_sums values.\r\n-        \"vld1.32 q11, [r2]\\n\"\r\n+        \"vld1.32 {d22, d23}, [r2]\\n\"\r\n         \"vdup.32 d13, r5\\n\" // rhs_zero_point\r\n \r\n         // Compute lhs_sums * rhs_zero_point.\r\n```\r\n", "@bjacob Thanks for the fix. I can confirm that the patch works for my setup."]}, {"number": 31743, "title": "ModuleNotFoundError: No module named 'tensorflow'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIt cannot import tensor flow in jupyterlab\r\n**Describe the expected behavior**\r\nIt should import tensorflow\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\n**Other info / logs**\r\n![Screen Shot 2019-08-18 at 5 37 12 PM](https://user-images.githubusercontent.com/20175738/63232913-1a9fe680-c1e1-11e9-9acf-e51d45600d05.png)\r\n\r\n", "comments": ["pip install tensorflow", "It is installed it appears when I do: \r\npip list\r\nBut the error still appears", "@ediazm ,\r\nCan you please go through this [link](https://www.tensorflow.org/install/pip) ?Thanks!", "There was two problems, even though in the terminal it displayed that tensorflow was installed, when I did \"pip list\" inside a cell of jupyterlab it did not have tensor flow. So I did:\r\n1) pip install\r\n2) Restarted the kernel\r\n![Screen Shot 2019-08-19 at 7 19 14 PM](https://user-images.githubusercontent.com/20175738/63312660-12b27600-c2b7-11e9-88ab-e0c8f814b46d.png)\r\n![Screen Shot 2019-08-19 at 7 19 26 PM](https://user-images.githubusercontent.com/20175738/63312664-16de9380-c2b7-11e9-972a-b292c5234687.png)\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31743\">No</a>\n"]}, {"number": 31742, "title": "r2.0-Cherrypick: Move to cuDN 7.6.2 and TensorRT 5.1.5", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 31741, "title": "Seg Fault with Custom Op built with nightly (works fine with v1.14 and 2.0 Beta)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from : binary\r\n- TensorFlow version: Nightly\r\n- Python version: 3.7\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 1080Ti\r\n\r\nI have a series of custom ops. They build and operate perfectly fine on a Windows 10 (with v1.14, v2.0 beta and nightly).\r\n\r\nI have just tried to build them on Linux, and they again work fine with v1.14 and v2.0 Beta. However with nightly they instantly segfault. As per the instructions I am using -D_GLIBCXX_USE_CXX11_ABI=0 flag due to using gcc 7.4.0\r\n\r\nI have also tried building the op with g++ 4.8 and still have the same issue.", "comments": ["@oracle3001,\r\nIf you are using our prebuilt pip packages, you should follow the instructions at github.com/tensorflow/custom-op to build your custom ops. Thanks!", "I have followed the instructions as presented on https://www.tensorflow.org/guide/extend/op\r\n\r\nAs I say, my ops build perfectly with v1.14 and v2.0Beta. It is only with nightly build that they segfault.", "I should add that I can add the ops and call them as stand alone operators eagerly.\r\n\r\nHowever, if I put them inside a keras model (or create a @tf.function that call its) and try to call / optimize, that is when they segfault occurs.", "I have fixed this issue. Was due to incorrect compiler flags relating to a dependant library used by the custom ops.", "Glad to know that it resolved. Thanks"]}, {"number": 31740, "title": "R1.13: RAdam for optimizer", "body": "Implement Rectified Adam.\r\nPaper: https://arxiv.org/pdf/1908.03265v1.pdf", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31740) for more info**.\n\n<!-- need_sender_cla -->", "Please send this pull request to tensorflow/addons; new optimizers go there instead of in core TF"]}, {"number": 31739, "title": "Guide is needed for transform of models suitable for tensorflow lite with gpu delegate", "body": "Hello,\r\nWhere one can find how to transform models to be suitable for working with tensorflow lite for gpu?\r\nWhat should i do to transfer images from 3 components to 4 components.\r\nI am trying to work with mtcnn. I have converted it successfully to tflite, works fine on the cpu,\r\n\r\nERROR: Next operations are not supported by GPU delegate:\r\nNEG: Operation is not supported.\r\nFirst 2 operations will run on the GPU, and the remaining 40 on the CPU.\r\nWARN: compileToBinary(256):\r\nC:\\fakepath(86,169-182): warning X3556: integer divides may be much slower, try using uints if possible.\r\nC:\\fakepath(86,246-259): warning X3556: integer modulus may be much slower, try using uints if possible.\r\n\r\nERROR: TfLiteGpuDelegate Invoke: ConvertToPHWC4: Input data size does not match expected size: 12288000 != 6912\r\nERROR: Node number 27 (TfLiteGpuDelegate) failed to invoke.", "comments": ["@kingofthebongo2008 \r\n\r\nIs it possible to share the model?\r\n\r\n> First 2 operations will run on the GPU, and the remaining 40 on the CPU.\r\n\r\nThis means that you have some ops that are not compatible with GPU.  You can use visualization tools to check what the 2nd op might be.\r\n\r\n> WARN: compileToBinary(256):\r\n\r\nI'm not familiar with this error message.  Are you using quantized models?  They are not compatible with GPU at the moment.\r\n\r\n> ERROR: TfLiteGpuDelegate Invoke: ConvertToPHWC4: Input data size does not match expected size: 12288000 != 6912\r\n\r\nAre you using batch of size != 1?", "@kingofthebongo2008 \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31738, "title": "delete and append to Tfrecords: Tensorflow", "body": "\r\nI am currently working on a research in which I have to append my training set after some epochs and delete some samples from test set after evaluation. Currently there is no way with which I can access the records (placed at specific indexes) in tfrecord file. Since tfrecords offer very fast training so I avoid using generators. Any chance of adding such feature?\r\n\r\n", "comments": ["@samra-irshad, Will it be possible to provide the sample code and also include the tensorflow version. Thanks!", "@gadagashwini My post is a feature request related to tfrecords in tensorflow. ", "I can explain what I am doing:\r\n1. Train a model on training data with tf.dataset \r\n2. Evaluate the model on test data.\r\n3. Now in next phase, I need to remove some samples from test set and add some samples in training set. So I have to iteratively create tfrecord files, which I am doing currently. But the problem is it takes a very long time to create tfrecord files (in middle of training) repeatedly. To avoid that, I am using generators that is a bottleneck.\r\n\r\nI believe it would be great if we can have an indexing mechanism for tfrecord files", "I don't expect this functionality to be available any time soon as it is non-trivial to support in the context of compression.\r\n\r\nWhat are the samples you are adding and sample you are removing? Could using the `filter` transformation instead of modifying the raw data be of help?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31737, "title": "Unable to learn model weights using `tf.nn.nce_loss`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): KDE Neon 5.16 (based on Ubuntu 18.04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0 (GIT: v1.12.0-0-ga6d8ffae09)\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.19.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.0.1 20180414 (experimental) [trunk revision 259383]\r\n- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.4.2\r\n- GPU model and memory: NVIDIA GeForce GTX 1050 Ti Max-Q, with 4GB of memory\r\n\r\n**Describe the current behavior**\r\nI was trying to build a word2vec model by following [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec). I've changed the code little bit, mainly in the parts where I have to load training data. The problem is that the model won't learn anything. The loss keeps fluctuating up and down, and the weights of my network stay constant throughout the training process (I've checked this using TensorBoard). I am convinced there's something wrong with either the code posted in the tutorial or with the `tf.nn.nce_loss` function, as I don't see any problem with the code I wrote.\r\n\r\n**Describe the expected behavior**\r\nI'm expecting the model to learn _something_ - which doesn't mean that I'm expecting it to achieve a specific accuracy. In other words, I'm expecting the network's weights to be updated after a training step.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nvocabulary_size = 13046\r\nembedding_size = 256\r\nnum_noise = 1\r\nlearning_rate = 1e-3\r\nbatch_size = 1024\r\nepochs = 10\r\n\r\ndef make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs):\r\n    return f'es={embedding_size}_nn={num_noise}_lr={learning_rate}_bs={batch_size}_e={epochs}'\r\n\r\n# These are the hidden layer weights\r\nembeddings = tf.get_variable(name='embeddings', initializer=tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), trainable=True)\r\n\r\n# 'nce' stands for 'Noise-contrastive estimation' and represents a particular loss function.\r\n# Check https://www.tensorflow.org/tutorials/representation/word2vec for more details.\r\n# 'nce_weights' and 'nce_biases' are simply the output weights and biases.\r\n# NOTE: for some reason, even though output weights will have shape (embedding_size, vocabulary_size),\r\n#       we have to initialize them with the shape (vocabulary_size, embedding_size)\r\nnce_weights = tf.get_variable(name='output_weights',\r\n                              initializer=tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / np.sqrt(embedding_size)), \r\n                              trainable=True)\r\nnce_biases = tf.get_variable(name='output_biases', initializer=tf.constant_initializer(0.1), shape=[vocabulary_size], trainable=True)\r\n\r\n# Placeholders for inputs\r\ntrain_inputs = tf.placeholder(tf.int32, shape=[None])    # [batch_size]\r\ntrain_labels = tf.placeholder(tf.int32, shape=[None, 1]) # [batch_size, 1]\r\n\r\n# This allows us to quickly retrieve the corresponding word embeddings for each word in 'train_inputs'\r\nmatched_embeddings = tf.nn.embedding_lookup(embeddings, train_inputs)\r\n\r\n# Compute the NCE loss, using a sample of the negative labels each time.\r\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\r\n                                     biases=nce_biases,\r\n                                     labels=train_labels,\r\n                                     inputs=matched_embeddings,\r\n                                     num_sampled=num_noise,\r\n                                     num_classes=vocabulary_size))\r\n\r\n# Use the SGD optimizer to minimize the loss function\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\r\n\r\n# Add some summaries for TensorBoard\r\nloss_summary = tf.summary.scalar('nce_loss', loss)\r\ninput_embeddings_summary = tf.summary.histogram('input_embeddings', embeddings)\r\noutput_embeddings_summary = tf.summary.histogram('output_embeddings', nce_weights)\r\n\r\n################################################################################\r\n\r\n# Load data\r\ntarget_words = np.genfromtxt('target_words.txt', dtype=int, delimiter='\\n').reshape((-1, 1))\r\ncontext_words = np.genfromtxt('context_words.txt', dtype=int, delimiter='\\n').reshape((-1, 1))\r\n\r\n# Convert to tensors\r\ntarget_words_tensor = tf.convert_to_tensor(target_words)\r\ncontext_words_tensor = tf.convert_to_tensor(context_words)\r\n\r\n# Create a tf.data.Dataset object representing our dataset\r\ndataset = tf.data.Dataset.from_tensor_slices((target_words_tensor, context_words_tensor))\r\ndataset = dataset.shuffle(buffer_size=target_words.shape[0])\r\ndataset = dataset.batch(batch_size)\r\n\r\n# Create an iterator to iterate over the dataset\r\niterator = dataset.make_initializable_iterator()\r\nnext_batch = iterator.get_next()\r\n\r\n# Train the model\r\nwith tf.Session() as session:\r\n\r\n    # Initialize variables\r\n    session.run( tf.global_variables_initializer() )\r\n\r\n    merged_summary = tf.summary.merge_all()\r\n\r\n    # File writer for TensorBoard\r\n    hparam_string = make_hparam_string(embedding_size, num_noise, learning_rate, batch_size, epochs)\r\n    loss_writer = tf.summary.FileWriter(f'./tensorboard/{hparam_string}')\r\n\r\n    global_step = 0\r\n    for epoch in range(epochs):\r\n\r\n        session.run(iterator.initializer)\r\n        while True:\r\n            try:\r\n                inputs, labels = session.run(next_batch)\r\n\r\n                feed_dict = {train_inputs: inputs[:, 0], train_labels: labels}\r\n                _, cur_loss, all_summaries = session.run([optimizer, loss, merged_summary], feed_dict=feed_dict)\r\n\r\n                # Write sumaries to disk\r\n                loss_writer.add_summary(all_summaries, global_step=global_step)\r\n                global_step += 1\r\n\r\n                print(f'Current loss: {cur_loss}')\r\n\r\n            except tf.errors.OutOfRangeError:\r\n                print(f'Finished epoch {epoch}.')\r\n                break\r\n\r\n```\r\n\r\n**Other info / logs**\r\nI'm attaching the training samples (target_words.txt) and the corresponding labels (context_words.txt) in case you wanted to reproduce this issue.\r\n[target_words.txt](https://github.com/tensorflow/tensorflow/files/3512860/target_words.txt)\r\n[context_words.txt](https://github.com/tensorflow/tensorflow/files/3512859/context_words.txt)\r\n\r\n", "comments": ["Issue replicating for TF version-1.12,please find the [gist](https://colab.sandbox.google.com/drive/1wUjGwmsR0NaL4VaT1Jtz05vzwi_1PfIQ#scrollTo=SBTDGV0TCr5u) of colab. Thanks", "Are you going to replicate the issue by yourself or do you want me to do that for you?", "The tutorial you are referencing was built and tested with TF 2.X version.\r\nYou may try using TF 2.X", "Well, this is kind of a makeshift solution. Would you be able to tell me whether `tf.nn.nce_loss` is bugged or not in TF 1.12?", "I tried your code snippet with latest version of tf-nightly ('1.15.0-dev20190821') and was able to execute it successfully.\r\nI cross checked ```nce_loss function``` with TF  version 1.12 and TF version 1.14. There isn't any change in its definition. Thus doesn't look like a root cause in this scenario.", "What do you mean by \"executing that successfully\"? I was able to execute it successfully as well, but the problem was that the model wasn't learning anything, and the loss function kept oscillating up and down instead of decreasing with each training step. That's the whole point of this issue", "There is still a fair amount of isolation and debugging that needs to happen here before we can call this a bug. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31737\">No</a>\n"]}, {"number": 31736, "title": "GPU placing issue with slurm", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): openSUSE Leap 42.3\r\n- TensorFlow installed from (source or binary): binary/pip\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: Python 3.6.9 :: Anaconda, Inc.\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: 4x GeForce RTX 2080 11Gb\r\n\r\n**Describe the current behavior**\r\n\r\n- If I do not change anything then the training crashes with the traceback below.\r\n- If I set `allow_soft_placement=True` the framework begins to place everything on CPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe training starts on 4 GPUs.\r\n\r\n**Code to reproduce the issue**\r\n\r\nSlurm command `srun -N 1 --gres=gpu:4 -c 32 --mem=96G python train.py`\r\n\r\n[PGAN repo](https://github.com/tkarras/progressive_growing_of_gans)\r\n\r\n**Other info / logs**\r\n\r\nTraceback\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 285, in <module>\r\n    tfutil.call_func_by_name(**config.train)\r\n  File \".../pgan-tf/tfutil.py\", line 236, in call_func_by_name\r\n    return import_obj(func)(*args, **kwargs)\r\n  File \".../pgan-tf/train.py\", line 161, in train_progressive_gan\r\n    G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)\r\n  File \".../pgan-tf/tfutil.py\", line 433, in __init__\r\n    self.reset_vars()\r\n  File \".../pgan-tf/tfutil.py\", line 495, in reset_vars\r\n    run([var.initializer for var in self.vars.values()])\r\n  File \".../pgan-tf/tfutil.py\", line 21, in run\r\n    return tf.get_default_session().run(*args, **kwargs)\r\n  File \".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \".../miniconda3/envs/pgantf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation G/lod: node G/lod (defined at .../pgan-tf/networks.py:176) was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:1, /job:localhost/replica:0/task:0/device:XLA_GPU:2, /job:localhost/replica:0/task:0/device:XLA_GPU:3 ]. Make sure the device specification refers to a valid device.\r\n         [[G/lod]]\r\nsrun: error: nvidia99: task 0: Exited with exit code 120\r\n```", "comments": ["@vlasenkov \r\nIn order to expedite the trouble-shooting process, please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    matrix1 = tf.constant([[3., 3.]])\r\n    matrix2 = tf.constant([[2.], [2.]])\r\n\r\nproduct = tf.matmul(matrix1, matrix2)\r\n\r\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n    result = sess.run(product)\r\n    print(result)\r\n```\r\n\r\nThis behaves in the same way. With `allow_soft_placement=True` it moves everything on CPU and says that it failed to load cudnn.so.7 With `allow_soft_placement=False` it crashes with some device naming inconsistency.", "I suspect that TF fails to recognize gpu. Can you please print the output from terminal;\r\n```python\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n```", "With `allow_soft_placement=True`:\r\n```\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:CPU:0\r\nConst: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\nConst_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0\r\n```\r\nWith `allow_soft_placement=False` the script crashes ", "Your TF session fails to recognize gpu. The reason is TF 1.13 and above pre built binaries support cuda 10.0 Please switch to cuda 10.0 and update environment variables for cuda paths. ", "CUDA 10 and CUDA 10.1 are already both installed on the system. After installation of CUDNN everything works now. This was the problem.\r\nThe TF 1.14 error message was misleading. In contrast TF 1.13 crashed exactly because it didn't find CUDNN.", "Thanks for sharing your fix. Closing this issue since it's resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31736\">No</a>\n"]}]