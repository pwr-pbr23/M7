[{"number": 31157, "title": "[LITE] Create the objects to hold the tensor data in elementwise_test and recognize_commands_test.", "body": "The elementwise_test and recognize_commands_test use the deleted temporary object. That might cause the wrong result. I can't pass these two tests locally.\r\n\r\nThis pr creates some local variables to hold the tensor data.", "comments": ["\r\n@petewarden\r\nCould you please review this pr?"]}, {"number": 31156, "title": "Add a uint8 nearest neighbor resize test.", "body": "This test locks down the fix introduced by https://github.com/tensorflow/tensorflow/commit/fd34d066ec5f514d737d771efaf0f6cf93925cf7#diff-e46056ad1b86847e16150ab7906bab3b.\r\n\r\nIt was originally reviewed with https://github.com/tensorflow/tensorflow/pull/30336.\r\n\r\n@ijkilchenko, please review and accept this PR.  I will close https://github.com/tensorflow/tensorflow/pull/30336.", "comments": []}, {"number": 31155, "title": "quantized for SplitV", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):pip install tensorflow==1.14\r\n- TensorFlow version (or github SHA if from source):1.14.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\ntflite_convert  --output_file=quantized.tflite  --graph_def_file=mixnet.pb   --inference_type=QUANTIZED_UINT8     --input_arrays=truediv  --output_arrays=Softmax  --mean_values=128   --std_dev_values=127  --default_ranges_min=0  --default_ranges_max=6\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\nW tensorflow/lite/toco/graph_transformations/quantize.cc:132] Constant array mixnet-s/mixnet_model/blocks_0/conv2d/kernel lacks MinMax information. To make up for that, we will now compute the MinMax from actual array elements. That will result in quantization parameters that probably do not match whichever arithmetic was used during training, and thus will probably be a cause of poor inference accuracy.\r\n2019-07-30 01:54:53.127377: F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type SplitV for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31155\">No</a>\n"]}, {"number": 31154, "title": "Workers are out-of-sync with MultiWorkerMirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.0.0-x86_64-i386-64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-dev20190729\r\n- Python version:3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI follow the guide in https://www.tensorflow.org/beta/tutorials/distribute/multi_worker_with_keras to try MultiWorkerMirroredStrategy with Keras, as my understanding, training would be synced across workers. \r\nBut after I start training as following \r\n\r\npython example_tf2_local.py 0 chief\r\npython example_tf2_local.py 0 worker\r\n\r\nI found worker/chief are training in different pace, e.g. \r\n\r\nif I start chief at first, chief would not wait for worker, it just starts its own training. \r\nIf I start worker/chief at the same time, still I saw one would be behind another one a few epochs sometime.\r\n\r\nBut as document stated \" MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers\"\r\n\r\nAnd it's same if I simply starts two workers without chief.\r\n\r\n**Describe the expected behavior**\r\nAccording to document \"MultiWorkerMirroredStrategy implements synchronous distributed training across multiple workers\", workers need to be synced during training\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport datetime\r\nimport json\r\nimport os\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport subprocess\r\nimport shlex\r\nimport sys\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 64\r\n\r\nNUM_WORKERS = 2\r\nGLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE\r\n\r\nif __name__ == \"__main__\":\r\n  worker_addrs = ['localhost:9999']\r\n  chief_addr = ['localhost:9998']\r\n  os.environ['TF_CONFIG'] = json.dumps({\r\n      'cluster': {\r\n          'worker': worker_addrs,\r\n          'chief' : chief_addr\r\n      },\r\n      'task': {'type': sys.argv[2], 'index': int(sys.argv[1])}\r\n  })\r\n\r\n  print('TF_CONFIG:' + os.environ['TF_CONFIG'])\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  def build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    return model\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                             with_info=True,\r\n                             as_supervised=True)\r\n\r\n  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)\r\n\r\n  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)\r\n\r\n  with strategy.scope():\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n  multi_worker_model.fit(x=train_datasets, epochs=100)\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Did you try setting your TF_CONFIG as described in the tutorial? From the example you pasted above, it looks different. You don't need to explicitly specify a chief. Simply put all your workers in the worker list, and give each of them a different index. Strategy will automatically pick the first one in the list as the chief.\r\nI think what you're seeing is that both the programs are running independently thinking there is only one worker in its cluster. You need to have both the workers in the \"workers\" section of TF_CONFIG.. \r\n\r\n", "I changed code as you suggested, and run \r\npython distribute_run_tf2.py 0\r\n\r\nto only start one worker, I assume in sync-training setup, this worker will wait for another worker(like I did with TF1.0 with SyncReplicasOptimizer + parameter server), but worker 0 just start training, here is the log\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0731 17:08:27.104443 4599719360 lazy_loader.py:50] \r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n2019-07-31 17:08:28.003093: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nW0731 17:08:28.004978 4599719360 cross_device_ops.py:1182] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nTF_CONFIG:{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}\r\nW0731 17:08:28.090763 4599719360 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\r\nW0731 17:08:28.167881 4599719360 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0731 17:08:28.230738 4599719360 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0731 17:08:28.230838 4599719360 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0731 17:08:28.231610 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\n2019-07-31 17:08:28.233445: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}\r\n2019-07-31 17:08:28.234122: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:12345\r\n2019-07-31 17:08:28.234138: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Server already started (target: grpc://localhost:12345)\r\nW0731 17:08:28.235424 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nW0731 17:08:28.280784 4599719360 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0731 17:08:28.280889 4599719360 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0731 17:08:28.281712 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nW0731 17:08:28.282696 4599719360 cross_device_ops.py:1177] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\r\nW0731 17:08:28.282995 4599719360 distributed_training_utils.py:1082] ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nEpoch 1/100\r\n2019-07-31 17:08:28.996419: W ./tensorflow/core/framework/model.h:213] Encountered a stop event that was not preceded by a start event.\r\n    333/Unknown - 10s 29ms/step - loss: 2.2656 - acc: 0.2531\r\n```\r\n\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport datetime\r\nimport json\r\nimport os\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport subprocess\r\nimport shlex\r\nimport sys\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 60000\r\nBATCH_SIZE = 64\r\n\r\nNUM_WORKERS = 2\r\nGLOBAL_BATCH_SIZE = NUM_WORKERS * BATCH_SIZE\r\n\r\nif __name__ == \"__main__\":\r\n  worker_addrs = ['localhost:12345', 'localhost:23456']\r\n  os.environ['TF_CONFIG'] = json.dumps({\r\n      'cluster': {\r\n          'worker': worker_addrs,\r\n      },\r\n      'task': {'type': 'worker', 'index': int(sys.argv[1])}\r\n  })\r\n\r\n  print('TF_CONFIG:' + os.environ['TF_CONFIG'])\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  def build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n        metrics=['accuracy'])\r\n    return model\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                             with_info=True,\r\n                             as_supervised=True)\r\n\r\n  train_datasets_unbatched = datasets['train'].map(scale).shuffle(BUFFER_SIZE)\r\n\r\n  train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)\r\n\r\n  with strategy.scope():\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n  multi_worker_model.fit(x=train_datasets, epochs=100)\r\n```", "Can you try by setting the TF_CONFIG another way (for e.g. by actually setting an env variable when you start the process)? I wonder if the strategy is not picking up your TF_CONFIG updates, especially because you create the strategy before TF_CONFIG is modified (and reading the TF_CONFIG happens during initialization of the strategy instance: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/distribute/collective_all_reduce_strategy.py#L104) ", "Yes, that works. Thanks so much! it would be nice this dependency could be mentioned somewhere.\r\n\r\nClose the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31154\">No</a>\n"]}, {"number": 31153, "title": "Quantization aware training does not add weights quantization and fake quantization nodes to Conv2D layers without bias and followed by non-addition ops", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.1/7\r\n- GPU model and memory: GTX 1080ti/12G\r\n\r\n**Describe the current behavior**\r\nCalling `tf.contrib.quantize.create_training_graph()` or `tf.contrib.quantize.create_eval_graph()` will not add weight quantization and activation quantization nodes to Conv2D layers with `use_bias=False, activation=None` and without following batch normalization layer.\r\n\r\n**Describe the expected behavior**\r\nA weights quantization node and an activation quantization node should be added to this layer, even it does not have biases, activation function, and batch normalization.\r\n\r\nThe issue seems to be related to [THIS](https://github.com/tensorflow/tensorflow/blob/dfe0d543aa6777b780f42f12fd503aebc71839d7/tensorflow/contrib/quantize/python/quantize.py#L529-L561) part\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nslim = tf.contrib.slim\r\n\r\nif __name__ == '__main__':\r\n    with tf.Session() as sess:\r\n        input = tf.ones((2, 5, 5, 10))\r\n\r\n        conv_bias = tf.keras.layers.Conv2D(\r\n            15,\r\n            kernel_size=3,\r\n            dilation_rate=(1, 1),\r\n            activation=None,\r\n            use_bias=True,\r\n            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\r\n            kernel_regularizer=slim.l2_regularizer(0.00005),\r\n            kernel_constraint=tf.keras.constraints.UnitNorm(axis=2))\r\n\r\n        conv_no_bias = tf.keras.layers.Conv2D(\r\n            15,\r\n            kernel_size=3,\r\n            dilation_rate=(1, 1),\r\n            activation=None,\r\n            use_bias=False,\r\n            kernel_initializer=tf.truncated_normal_initializer(stddev=0.01),\r\n            kernel_regularizer=slim.l2_regularizer(0.00005),\r\n            kernel_constraint=tf.keras.constraints.UnitNorm(axis=2))\r\n\r\n        x_2 = conv_bias(input)\r\n        x_1 = conv_no_bias(input)\r\n\r\n        output = tf.concat((x_1, x_2), axis=3)\r\n        tf.contrib.quantize.create_training_graph(quant_delay=0)\r\n        writer = tf.summary.FileWriter('./bug_report', sess.graph)\r\n\r\n```\r\n**Other info / logs**\r\nGraph from tensorboard:\r\n\r\n![image](https://user-images.githubusercontent.com/18194209/62093263-810e9600-b22d-11e9-87d5-17703fa7542e.png)\r\n", "comments": ["I could reproduce the issue with TF1.14.0. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/301d8c4a1c743e49639bd46161106fe9/tf31153.ipynb). Thanks!", "I also encounter this problem, how do you solve this?@protossw512@jvishnuvardhan @ pavithrasv \r\nThanks very much!", "@pianogGG Can you please try `tf-nightly` and share a simple standalone code to reproduce the issue? Thanks!", "@protossw512 Can you please try `tf-nightly` and share the updated code. I think there are no more updates to `contrib`. So it is better to try `TF2.x` or `tf-nightly`. Thanks!", "> \r\n> \r\n> @pianogGG Can you please try `tf-nightly` and share a simple standalone code to reproduce the issue? Thanks!\r\n\r\nDo you mean using tensorflow_model_optimization package to quantize ,right?", "@jvishnuvardhan Do you mean using tensorflow_model_optimization package to quantize ,right?\r\n", "Conv with relu6 but without bias  can insert fakequant node", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31153\">No</a>\n"]}, {"number": 31152, "title": "Fix compile error on MacOS", "body": "The test case for optimized contraction kernels depends on some hard-coded Eigen kernels for 8- and 16-bit data types. These kernels are implemented in `Eigen/CXX11/src/FixedPoint/MatMatProductAVX2.h` as hard-coded specializations of the templated class `TensorContractionBlocking`. At the point that these specializations are declared, the underlying `TensorContractionBlocking` class is not in scope. This missing forward declaration causes `clang` to exit with an error, which prevents the target `//tensorflow/core/kernels:eigen_mkldnn_contraction_kernel_test` from compiling on MacOS.\r\n\r\nThis PR adds an additional `#include` to the test file that ensures that `TensorContractionBlocking` has been defined at the point that `MatMatProductAVX2.h` specializes it. This change fixes the compilation problem on my Mac environment.", "comments": []}, {"number": 31151, "title": "Pulic API for name_scope & convert_to_tensor", "body": "Replace ops.name_scope by tf.name_scope", "comments": ["@I-Hong  Could you please check failed build errors? Thanks!", "Thanks @gbaned. I replaced it back, but there is an import/copybara error, could you please retrigger the test?  \r\nAlso, I'm not sure whether replacing a public API is good at this point. Do you have any comments? ", "I think this should get a review from the API owners if it is replacing public API", "This does not work. You cannot `import tensorflow as tf` from a python file inside tensorflow itself.", "Thanks @alextp, I got it. "]}, {"number": 31150, "title": "DO NOT MERGE THIS.. TEST CHERRYPICK", "body": "Test Cherrypick", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31150) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 31149, "title": "Feature Request: Capability to fix the seed for all keras kernel initializers", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0-beta\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIf you create a Keras model, the only way to make your model reproducible is to repeat setting the seed for the `kernel_initializer` in every layer. This makes the code bulky and more prone to error. For example, a reproducible code looks like this:\r\n\r\n```\r\nnp.random.seed(1)\r\ntf.set_random_seed(1)\r\n# FEATURE REQUEST for another parameter like the ones above that we can set\r\n# here and avoid repeating initializer seed in each and every layer below\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1), input_shape=[1]))\r\nmodel.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))\r\nmodel.add(tf.keras.layers.Dense(1, activation='linear', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))\r\n```\r\nI wonder if you can add a default seed parameter that we can set and it affects all the initializers. \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who likes clean code and would like to create reproducible models.\r\n", "comments": ["@mhaghighat Sorry for the late response. \r\nLook like you are looking for the following feature.\r\nhttps://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism\r\n\r\nIf you think the above is not the feature you are looking, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31148, "title": "FailedPreconditionError when training BERT on Colab", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: N/A\r\n- TensorFlow installed from (source or binary): Not sure\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: Python 3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWe are trying to train a BERT model on Google Colab in keras with tensorflow hub.  The codes are largely copied from https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b with slight modification.  \r\nThe model after compilation failed to generate the prediction. I continue to get the error message 'FailedPreconditionError' despite 'with tf.Session() as sess:\r\n     sess.run(tf.global_variables_initializer())'\r\n\r\n**Describe the expected behavior**\r\nWe expect the model to run and build predictions. \r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```def build_model(max_seq_length): \r\n    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\r\n    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\r\n    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\r\n    \r\n    bert_inputs = [in_id, in_mask, in_segment]\r\n    \r\n    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"sequence_output\")(bert_inputs)\r\n    \r\n    print(bert_output)\r\n    \r\n    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\r\n    \r\n    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\r\n    \r\n    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\r\n    \r\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n    \r\n    model.summary()\r\n    \r\n    return model\r\n\r\ndef initialize_vars(sess):\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(tf.tables_initializer())\r\n    K.set_session(sess)\r\n\r\nsess = tf.Session()\r\n\r\nmodel = build_model(max_seq_length)\r\n\r\nwith tf.Session() as sess:\r\n     sess.run(tf.global_variables_initializer())\r\n\r\nmodel.fit(\r\n    [train_input_ids, train_input_masks, train_segment_ids], \r\n    train_labels,\r\n    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\r\n    epochs=1,\r\n    batch_size=32\r\n)```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```Tensor(\"bert_layer_12/bert_layer_12_module_apply_tokens/bert/encoder/Reshape_13:0\", shape=(?, ?, 768), dtype=float32)\r\nModel: \"model_12\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_ids (InputLayer)          [(None, 32)]         0                                            \r\n__________________________________________________________________________________________________\r\ninput_masks (InputLayer)        [(None, 32)]         0                                            \r\n__________________________________________________________________________________________________\r\nsegment_ids (InputLayer)        [(None, 32)]         0                                            \r\n__________________________________________________________________________________________________\r\nbert_layer_12 (BertLayer)       (None, None, 768)    108931396   input_ids[0][0]                  \r\n                                                                 input_masks[0][0]                \r\n                                                                 segment_ids[0][0]                \r\n__________________________________________________________________________________________________\r\ndense_24 (Dense)                (None, None, 256)    196864      bert_layer_12[0][0]              \r\n__________________________________________________________________________________________________\r\ndense_25 (Dense)                (None, None, 1)      257         dense_24[0][0]                   \r\n==================================================================================================\r\nTotal params: 109,128,517\r\nTrainable params: 21,460,737\r\nNon-trainable params: 87,667,780\r\n__________________________________________________________________________________________________\r\nTrain on 65 samples, validate on 124 samples\r\n---------------------------------------------------------------------------\r\nFailedPreconditionError                   Traceback (most recent call last)\r\n<ipython-input-43-622dfa03de82> in <module>()\r\n     11     validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\r\n     12     epochs=1,\r\n---> 13     batch_size=32\r\n     14 )\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)\r\n   1456         ret = tf_session.TF_SessionRunCallable(self._session._session,\r\n   1457                                                self._handle, args,\r\n-> 1458                                                run_metadata_ptr)\r\n   1459         if run_metadata:\r\n   1460           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\nFailedPreconditionError: Error while reading resource variable bert_layer_12_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/bert_layer_12_module/bert/encoder/layer_10/attention/output/LayerNorm/gamma/N10tensorflow3VarE does not exist.\r\n\t [[{{node bert_layer_12/bert_layer_12_module_apply_tokens/bert/encoder/layer_10/attention/output/LayerNorm/batchnorm/mul/ReadVariableOp}}]]```\r\n", "comments": ["@sangurocactus Please refer to the following [solution](https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162) and let me know if it helps in resolving your issue. Thanks!", "@gowtham-kp Sorry - is there a link or an image with the solution following your message? I don't see any instruction apart from the one-line message you sent me. Thanks. ", "Yes @sangurocactus Just for the refence again, here's [some workaround](https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162) for your problem. Thanks!", "Closing this issue due to the lack of inactivity. Please add additional comments and we can open the issue again. Thanks!", "I went through this problem and I found the solution @gowthamkpr suggested did not work.\r\nInstead, I found the [solution](https://stackoverflow.com/questions/34001922/failedpreconditionerror-attempting-to-use-uninitialized-in-tensorflow) from stackoverflow helps. I hope my experience is helpful to anyone who happened to stuck with this issue."]}, {"number": 31147, "title": "Tensorflow 1.14 Keras functional API mixed with ops using placeholders throws InvalidArgumentError 'You must feed a value for placeholder tensor'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nKeras layers that have an input dependent on a Tensorflow placeholder will throw an InvalidArgumentError on the op creation step asking to feed a value for the placeholder. Specifically, this happens during the `base_layer_utils.create_keras_history(inputs)` step in the `__call__` function on the layer, where the inputs to the Keras layer are passed through a `GraphExecutionFunction` object made during `backend.function([], op_input)([])`. This exception is new in Tensorflow 1.14.0.\r\n\r\n**Describe the expected behavior**\r\nAs in previous versions of Tensorflow, I would not expect the InvalidArgumentError to be thrown when I am building the graph mixing Keras with Tensorflow.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef compile_errors():\r\n    tf_graph = tf.Graph()\r\n    with tf_graph.as_default():\r\n        image = tf.keras.Input(shape=[224, 224, 3], dtype=tf.float32, name=\"image\")\r\n        scale = tf.placeholder(dtype=tf.float32, shape=[], name=\"scale\")\r\n        scaled_image = image * scale\r\n        conv = tf.keras.layers.Conv2D(filters=32, kernel_size=3, name=\"conv2d\")(scaled_image)\r\n        # conv errors on __call__ here due to create_keras_history making a GraphExecutionFunction:\r\n        # tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'scale' with dtype float\r\n        # \t [[{{node scale}}]]\r\n\r\ndef compile_succeeds():\r\n    tf_graph = tf.Graph()\r\n    with tf_graph.as_default():\r\n        image = tf.keras.Input(shape=[224, 224, 3], dtype=tf.float32, name=\"image\")\r\n        scale = tf.placeholder(dtype=tf.float32, shape=[], name=\"scale\")\r\n        scaled_image = tf.keras.layers.Lambda(function=lambda tensors: tensors[0] * tensors[1])([image, scale])\r\n        conv = tf.keras.layers.Conv2D(filters=32, kernel_size=3, name=\"conv2d\")(scaled_image)\r\n        # this succeeds\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        compile_errors()\r\n        print(\"Functional API compilation succeeded.\")\r\n    except Exception as e:\r\n        print(\"Functional API compilation errored!\", e)\r\n    print()\r\n    compile_succeeds()\r\n    print(\"Explicit Keras Lambda Layer compilation succeeded.\")\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mbeissinger As mentioed in this [issue](https://stackoverflow.com/questions/50321139/keras-error-you-must-feed-a-value-for-placeholder-tensor) 'scale' is a  \"symbolic\" tensor.\r\nThis means, they have absolutely no data (or value) until the moment you start fitting or predicting.  \r\n\r\nBut in the 2nd case, you have created a lambda layer which doesn't expect the values immediately, You can feed in the values when you are fitting or predicting but in the 1st case its expecting the values immediately which is not possible.  ", "The example script is not running in eager execution mode and I am not expecting the symbolic tensor made by the placeholder to have or need any value at compile time. Tensorflow 1.13.2 did not throw this exception, why did the behavior change in 1.14 to expect placeholder values immediately and not just at session.run?", "Keras does not allow TensorFlow ops unless they are wrapped by a Lambda layer.\r\n@mbeissinger I think there is no bug here.", "@nairouz I have reproduced this issue. This looks like a bug as [TF 1.13](https://colab.sandbox.google.com/gist/gowthamkpr/0770971da52b163128be747f5f62c357/untitled64.ipynb) doesn't throw any error but it throws an error with [TF 1.14](https://colab.sandbox.google.com/gist/gowthamkpr/ed9f839bd3a574aefedcb1944cbce3fb/untitled85.ipynb) and [TF-Nightly](https://colab.sandbox.google.com/gist/gowthamkpr/238ff431df1edc6885c9188dc81295d1/untitled86.ipynb) too.", "This is fixed with latest version of TF nightly build '1.15.0-dev20190821'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31147\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31147\">No</a>\n"]}, {"number": 31146, "title": "[XLA:GPU][ROCm] AMDGPU-specific logic to produce HSA code objects for XLA.", "body": "Follow-up to #30326  .\r\n\r\n- rename `nvptx_backend_lib` to `gpu_backend_lib`.\r\n- introduce `xla::gpu::amdgpu::CompileToHsaco`.\r\n- AMDGPU-specific logic to drive LLVM / LLD to produce HSA code objects which are used on AMD ROCm platform.", "comments": ["@whchung \r\nWe need to resolve some internal build setup issue before merging this PR. Sorry to make you wait. I hope this won't take long.", "@thomasjoerg @chsigg any chance I can understand how this PR broke XLA for CUDA?", "@thomasjoerg / @chsigg ping. may I understand how this PR broke XLA for CUDA?", "An internal test assumed the file you renamed in this PR to contain the string 'ptx'. I've fixed it and the PR should be merged again shortly."]}, {"number": 31145, "title": "Fix duplicate node name in graph for tf.concat", "body": "This fix tries to address the issue raised in #31137 where\r\n`Duplicate node name in graph: 'concat'` shown up with the\r\nfollowing:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\ni = Input(shape=3)\r\nj = Input(shape=4)\r\ntry:\r\n    print(tf.concat([i, j], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat([i, j], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat([i], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat(i, axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\n```\r\n\r\nThe issue was that `identity` node passed the scope as the name.\r\nThis fix fixes the issue.\r\n\r\nThis fix fixes #31137.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @alextp for the review and suggestion. The PR has been updated. Please take a look.", "@alextp @rthadur one kokoro CI test was failing, because of the name change where 'concat:0' =>`concat/concat:0`. I have updated the PR and fixed the failed test.\r\n\r\nPlease take a look and sorry for the additional inconvenience."]}, {"number": 31144, "title": "Having issue with the input and output arrays. Here is my jupyter notebook. ", "body": "**System information**\r\nModel Name:\tMacBook Pro\r\nModel Identifier:\tMacBookPro11,3\r\nProcessor Name:\tIntel Core i7\r\nProcessor Speed:\t2.5 GHz\r\nNumber of Processors:\t1\r\nTotal Number of Cores:\t4\r\nL2 Cache (per Core):\t256 KB\r\nL3 Cache:\t6 MB\r\nHyper-Threading Technology:\tEnabled\r\nMemory:\t16 GB\r\nBoot ROM Version:\t153.0.0.0.0\r\nSMC Version (system):\t2.19f12\r\nSerial Number (system):\tC02NF1XAG3QD\r\nHardware UUID:\t411A542B-6D05-553C-A6D5-7C2EBA5068C6\r\n\r\ntensorflow version 2.0.0b1\r\npython version . 3.7.3\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Embedding,SpatialDropout1D\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.utils import to_categorical\r\nimport numpy as np \r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\nfrom keras.layers.recurrent import LSTM\r\nfrom keras.preprocessing.sequence import pad_sequences\r\nUsing TensorFlow backend.\r\nIn [2]:\r\ndata = {'email_subject': ['Good Morning','Blackberry','Really Nice Weather'], \r\n        'secondary_folder_name': ['Sync Issues','Execution Reports','First']}\r\ndf = pd.DataFrame(data)\r\nIn [3]:\r\nimport re\r\nfrom nltk.corpus import stopwords\r\nIn [4]:\r\ndf = df.reset_index(drop=True)\r\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\r\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\r\nSTOPWORDS = set(stopwords.words('english'))\r\n\r\ndef clean_text(text):\r\n    \"\"\"\r\n        text: a string\r\n        \r\n        return: modified initial string\r\n    \"\"\"\r\n    text = text.lower() # lowercase text\r\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\r\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \r\n    text = text.replace('x', '')\r\n#    text = re.sub(r'\\W+', '', text)\r\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\r\nIn [5]:\r\n# The maximum number of words to be used. (most frequent)\r\nMAX_NB_WORDS = 400\r\n# Max number of words in each\r\nMAX_SEQUENCE_LENGTH =40\r\n# This is fixed.\r\nEMBEDDING_DIM = 100\r\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\ntokenizer.fit_on_texts(df['email_subject'].values)\r\nword_index = tokenizer.word_index\r\nprint('Found %s unique tokens.' % len(word_index))\r\nFound 6 unique tokens.\r\nIn [6]:\r\nX = tokenizer.texts_to_sequences(df['email_subject'].values)\r\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\nprint('Shape of data tensor:', X.shape)\r\nShape of data tensor: (3, 40)\r\nIn [7]:\r\nY = pd.get_dummies(df['secondary_folder_name']).values\r\nprint('Shape of label tensor:', Y.shape)\r\nShape of label tensor: (3, 3)\r\nIn [8]:\r\nimport sklearn \r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 101)\r\nprint(X_train.shape,Y_train.shape)\r\nprint(X_test.shape,Y_test.shape)\r\n(2, 40) (2, 3)\r\n(1, 40) (1, 3)\r\nIn [9]:\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\r\nmodel.add(keras.layers.SpatialDropout1D(0.2))\r\nmodel.add(keras.layers.SimpleRNN(100, dropout=0.2, recurrent_dropout=0.2))\r\nmodel.add(keras.layers.Dense(3, activation='softmax'))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nmodel.summary()\r\nepochs = 40\r\nbatch_size = 64\r\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0729 14:14:34.209316 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0729 14:14:34.222216 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1628: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding (Embedding)        (None, 40, 100)           40000     \r\n_________________________________________________________________\r\nspatial_dropout1d (SpatialDr (None, 40, 100)           0         \r\n_________________________________________________________________\r\nsimple_rnn (SimpleRNN)       (None, 100)               20100     \r\n_________________________________________________________________\r\ndense (Dense)                (None, 3)                 303       \r\n=================================================================\r\nTotal params: 60,403\r\nTrainable params: 60,403\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nW0729 14:14:34.670812 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:454: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\nTrain on 1 samples, validate on 1 samples\r\nEpoch 1/40\r\n1/1 [==============================] - 0s 339ms/sample - loss: 1.4005 - acc: 0.0000e+00 - val_loss: 1.1219 - val_acc: 0.0000e+00\r\nEpoch 2/40\r\n1/1 [==============================] - 0s 16ms/sample - loss: 1.1713 - acc: 0.0000e+00 - val_loss: 1.1644 - val_acc: 0.0000e+00\r\nEpoch 3/40\r\n1/1 [==============================] - 0s 17ms/sample - loss: 1.4085 - acc: 0.0000e+00 - val_loss: 1.2018 - val_acc: 0.0000e+00\r\nEpoch 4/40\r\n1/1 [==============================] - 0s 18ms/sample - loss: 0.7954 - acc: 1.0000 - val_loss: 1.2390 - val_acc: 0.0000e+00\r\nIn [10]:\r\nfrom keras.models import load_model\r\nkeras_file =\"move.h5\"\r\nkeras.models.save_model(model,keras_file)\r\nfrom tensorflow import lite\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nW0729 14:14:35.465211 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0729 14:14:35.466186 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0729 14:14:35.466797 4525888960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0729 14:14:36.299986 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py:249: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0729 14:14:36.300852 4525888960 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0729 14:14:36.908327 4525888960 module_wrapper.py:136] From /anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py:163: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-d6b22a2e1927> in <module>\r\n      3 keras.models.save_model(model,keras_file)\r\n      4 from tensorflow import lite\r\n----> 5 converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)\r\n    833     _set_tensor_shapes(input_tensors, input_shapes)\r\n    834 \r\n--> 835     graph_def = _freeze_graph(sess, input_tensors, output_tensors)\r\n    836     return cls(\r\n    837         graph_def,\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/util.py in freeze_graph(sess, input_tensors, output_tensors)\r\n    247     output_arrays = [get_tensor_name(tensor) for tensor in output_tensors]\r\n    248     return tf_graph_util.convert_variables_to_constants(sess, graph_def,\r\n--> 249                                                         output_arrays)\r\n    250   else:\r\n    251     return sess.graph_def\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    322               'in a future version' if date is None else ('after %s' % date),\r\n    323               instructions)\r\n--> 324       return func(*args, **kwargs)\r\n    325     return tf_decorator.make_decorator(\r\n    326         func, new_func, 'deprecated',\r\n\r\n/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py in convert_variables_to_constants(sess, input_graph_def, output_node_names, variable_names_whitelist, variable_names_blacklist)\r\n    300         source_op_name = get_input_name(map_name_to_node[source_op_name])\r\n    301       if map_name_to_node[source_op_name].op != \"VarHandleOp\":\r\n--> 302         raise ValueError(\"Cannot find the variable that is an input \"\r\n    303                          \"to the ReadVariableOp.\")\r\n    304 \r\n\r\nValueError: Cannot find the variable that is an input to the ReadVariableOp.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@tensorflowbutler Need your expertise", "@sajagkc11 Please post this question on stack overflow as this is a support issue and github is mainly for bug, performance, feature request and documentation request issues. Thanks!", "@sajagkc11 I see you are using TF 2.0 \r\nCan you please try ```tf.lite.TFLiteConverter``` instead,\r\nSee https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite/TFLiteConverter"]}, {"number": 31143, "title": "CUDA Download link needs update", "body": "**Describe the current behavior**\r\n\r\nWhen CUDA is not installed, I get the following error\r\n\r\n```\r\n...\\tensorflow\\python\\platform\\self_check.py in preload_check()\r\n\r\nImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\n```\r\n\r\nThe link is for CUDA 9.0 rather than 10.0. \r\n\r\n**Describe the expected behavior**\r\n\r\nUse one of these links (whichever is appropriate): \r\n\r\n```\r\nhttps://developer.nvidia.com/cuda-10.0-download-archive\r\nhttps://developer.nvidia.com/cuda-10.1-download-archive\r\n```", "comments": ["btw: installed CUDA from `https://developer.nvidia.com/cuda-10.0-download-archive`, installed it, and restarted the computer. The path to the `%programfiles%\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin` directory was added in the system-wide path variable. \r\n\r\nWhent typing \r\n\r\n```cmd\r\nwhere cudart64_100.dll\r\n```\r\n\r\nI get \r\n\r\n```cmd\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudart64_100.dll\r\n```\r\n\r\nHowever, I still get the above error in my Jupyter Notebook. \r\n", "@MovGP0 ,\r\nJust to clarify did you add sub folders: bin and lib to the path?Thanks!", "@MovGP0 You are right. Thanks for reporting this.  What version of TF you are trying to install?", "@ymodak v1.14 GPU \r\n```\r\npip install tensorflow-gpu==1.14\r\n```", "@anush-o the bin and lib folders where added to the path variable during the Installation of CUDA. ", "@yifeif Can you please take a look? Thanks!", "> btw: installed CUDA from `https://developer.nvidia.com/cuda-10.0-download-archive`, installed it, and restarted the computer. The path to the `%programfiles%\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin` directory was added in the system-wide path variable.\r\n> \r\n> Whent typing\r\n> \r\n> ```batchfile\r\n> where cudart64_100.dll\r\n> ```\r\n> \r\n> I get\r\n> \r\n> ```batchfile\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudart64_100.dll\r\n> ```\r\n> \r\n> However, I still get the above error in my Jupyter Notebook.\r\n\r\n\r\n@MovGP0 This occurred to me. My setting was CUDA 10.0, cudNN 7.4.2, tensorflow 1.14. I had the same issue mutliple times. I have CUDA 9.0 installed too. In environment variables, I changed the ordering in \"Variable name\" \"path\". Moved up v10.0 path, libnvvp and moved the cuda 9.0 below. Restarted the system.\r\n\r\nIt worked. I have attached a screenshot in the link.\r\n[Image](https://drive.google.com/open?id=1QdBQ8paxecS-wNrAyd6uynxNv1ZDkKWe)", "The error message is fixed with TF 1.15 version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31143\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31143\">No</a>\n"]}, {"number": 31142, "title": "Tensor flow not found - spyder", "body": "Hello,\r\nI am new of python environment.\r\nI want to use spyder and tensor flow in windows PC, but I get the error: \r\n\r\n`no module named 'tensorflow' spyder`\r\n\r\nI simply downloaded anaconda from https://www.anaconda.com/distribution/#download-section.\r\nWhat's wrong? I already tried to look for similar problems on internet but I am quite confused to the procedure. Can you provide me a step-by-step guide on how to install the module?\r\n\r\nThank you so much,\r\nA", "comments": ["Could you post your code snippet here?", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I am using a Windows 7 i7 pc, and the Anaconda version is \"Anaconda 2019.07 for Windows Installer\" that you can see in the link: https://www.anaconda.com/distribution/#download-section\r\nand then I installed the anaconda with \"Python 3.7 version\" (at left).\r\n\r\n[EDIT] But good news: now I write in the anaconda prompt:\r\n'pip install tensorflow' and now the code works without problem of 'no module named 'tensorflow' spyder'.\r\nTherefore thank you so much for your help but I think that it is already solved (I don't know how exacly). According to that I close the issues. \r\nKindly regards!", "@AlbiBone ,\r\nGood to know that your issue is resolved. If I understand it correctly, you were trying to \r\n`import tensorflow` without running `pip install tensorflow`. Now that you have run `pip install tensorflow`, you could `import` it without any error.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31142)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31142)\r\n"]}, {"number": 31141, "title": "Tensorflow script that runs perfectly well on Windows 10 laptop gives error on ubuntu 18 cloud vps. How can I solve this problem? ", "body": "Here is the error message thrown:\r\n\r\n(base) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$ ls\r\nhyperopt_v2.py  xbot.py\r\n(base) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$ screen -r\r\ner\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 2143, in build\r\n    constraint=self.kernel_constraint)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 384, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 663, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    variable_shape = tensor_shape.TensorShape(shape)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 774, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 774, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 716, in as_dimension\r\n    return Dimension(value)\r\n  File \"/home/ps1/anaconda3/envs/tf_cpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 185, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not '_ListWrapper'\r\n(tf_cpu) ps1@ubuntu-s-1vcpu-2gb-nyc1-01:~/xbot_hyperopt$\r\n\r\n", "comments": ["I just saw that accidental change I made was responsible for this. Please ignore it and close it. "]}, {"number": 31140, "title": "[go] Include MetaGraphDef in SavedModel for go library", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, the MetaGraphDef is not included in the SavedModel struct (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/saved_model.go). The underlying cause is that the protobufs are not pregenerated for go.\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, but It should be a backwards compatible change - the new exposed MetaGraphDef field will be added to SavedModel struct.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who runs inference using go and needs to dynamically load the model's meta graph. Currently, the meta graph /  signature needs to be hardcoded in the code, and it may be fragile if input or output tensor names are change in the python code that generated the saved model. \r\n\r\n**Any Other info.**\r\nPlease see the similar issue for java here: https://github.com/tensorflow/tensorflow/issues/19441", "comments": ["Do you need this for the Signature field? That has been added since this was filed.", "Yes, I needed this for the signature field. Perfect, thank you for letting me know. Closing this issue then."]}, {"number": 31139, "title": "ps ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 31138, "title": "[TF 2.0] gRPC error, in TPUStrategy experimental_distribute_dataset", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): TF 2.0 Beta 1\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRaises this error.\r\n```\r\nInternalError: Failed copying input tensor from /job:worker/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:1/device:CPU:0 in order to run ExperimentalAutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1564422681.083878500\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:ExperimentalAutoShardDataset]\r\n```\r\n**Describe the expected behavior**\r\nWork without any error\r\n**Code to reproduce the issue**\r\nRunning the following on Colab produces the error.\r\n`!pip3 install tensorflow==2.0.0b1 &> /dev/null`\r\n```python3\r\nimport tensorflow as tf\r\nimport os\r\ncluster = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"grpc://%s\" % os.environ[\"COLAB_TPU_ADDR\"])\r\ntf.config.experimental_connect_to_host(cluster.get_master())\r\ntf.tpu.experimental.initialize_tpu_system(cluster)\r\nstrategy = tf.distribute.experimental.TPUStrategy(cluster)\r\ndataset = tf.data.Dataset.range(100).batch(16)\r\ndistributed_dataset = strategy.experimental_distribute_dataset(dataset)\r\n```\r\nCC:\r\n@srjoglekar246 \r\n@vbardiovskyg \r\n\r\n", "comments": ["This fixed the issue.\r\nDownloading TF Nightly 2.0 and disabling autosharding helped fixed the issue.\r\n```python3\r\ndataset = tf.data.Dataset.range(100).batch(16)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard = False\r\ndataset.with_options(options)\r\ndistributed_dataset =  strategy.experimental_distribute_dataset(dataset)\r\n```", "Hi @captain-pool, thanks your report. A couple of questions:\r\n\r\n1) Which version of Cloud TPUs are you using?\r\n2) Are you using a single TPU or a slice of a Cloud TPU pod (i.e. was it a v2/v3-8 or was it a v2-32 / v3-32 or above)?", "@frankchn I'm using single TPU, v2-8, installed with nightly build.", "To clarify, you selected the \"nightly\" version when creating the TPU in the console and/or created the TPU with `ctpu --version=nightly` or `gcloud compute tpus create --version=nightly`? ", "Yep\n\nOn Tue, 30 Jul 2019, 11:53 am Frank Chen, <notifications@github.com> wrote:\n\n> To clarify, you selected the \"nightly\" version when creating the TPU in\n> the console and/or created the TPU with ctpu --version=nightly or gcloud\n> compute tpus create --version=nightly?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31138?email_source=notifications&email_token=ADKYRWK5F6U6JVUQ2JBVOGDQB7M6JA5CNFSM4IHUQRT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3C4XVQ#issuecomment-516279254>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ADKYRWOQHHW3AUZLYKKX55LQB7M6JANCNFSM4IHUQRTQ>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31138\">No</a>\n"]}, {"number": 31137, "title": "tf.concat throws error after another call of tf.concat if values is a single Tensor or a list of length 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\n\r\nAssume that I have already made a previous call of `tf.concat`. When calling again `tf.concat` with a `values` argument which is either a list of `Tensor` objects of length 1 or a single `Tensor` object, I get the following error:\r\n\r\n```Duplicate node name in graph: 'concat'```\r\n\r\nIt seems to be a naming conflict.\r\n\r\n**Describe the expected behavior**\r\n\r\nFrom [tf.concat documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/concat?hl=en):\r\n> `values`: A list of `Tensor` objects or a single `Tensor`.\r\n\r\n`tf.concat` should not throw an error when `values` is a list of length 1 or a single `Tensor` object and no naming conflict should rise.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input\r\n\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\ni = Input(shape=3)\r\nj = Input(shape=4)\r\ntry:\r\n    print(tf.concat([i, j], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat([i, j], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat([i], axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\ntry:\r\n    print(tf.concat(i, axis=-1))\r\nexcept Exception as e:\r\n    print(type(e))\r\n    print(e)\r\n```\r\n\r\nwhich outputs:\r\n```\r\nUsing Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7)\r\nTensor(\"concat:0\", shape=(None, 7), dtype=float32)\r\nTensor(\"concat_1:0\", shape=(None, 7), dtype=float32)\r\n<class 'ValueError'>\r\nDuplicate node name in graph: 'concat'\r\n<class 'ValueError'>\r\nDuplicate node name in graph: 'concat'\r\n```\r\n\r\nIf I comment both `print(tf.concat([i, j], axis=-1))` lines then `print(tf.concat([i], axis=-1))` does not fail, but `print(tf.concat(i, axis=-1))` do. If I also comment `print(tf.concat([i], axis=-1))` then `print(tf.concat(i, axis=-1))` resolves without error.\r\n\r\n**Other info / logs**\r\n\r\nFull error log:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1550   try:\r\n-> 1551     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1552   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Duplicate node name in graph: 'concat'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-bd15a2b20363> in <module>\r\n----> 1 print(tf.concat(i, axis=-1))\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n   1282           dtype=dtypes.int32).get_shape().assert_is_compatible_with(\r\n   1283               tensor_shape.scalar())\r\n-> 1284       return identity(values[0], name=scope)\r\n   1285   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n   1286 \r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    178     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    179     try:\r\n--> 180       return target(*args, **kwargs)\r\n    181     except (TypeError, ValueError):\r\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in identity(input, name)\r\n     84       return copied\r\n     85   else:\r\n---> 86     ret = gen_array_ops.identity(input, name=name)\r\n     87     # Propagate handle data for happier shape inference for resource variables.\r\n     88     if hasattr(input, \"_handle_data\"):\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py in identity(input, name)\r\n   4251   # Add nodes to the TensorFlow graph.\r\n   4252   _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 4253         \"Identity\", input=input, name=name)\r\n   4254   _result = _op.outputs[:]\r\n   4255   _inputs_flat = _op.inputs\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    786         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\r\n    787                          input_types=input_types, attrs=attr_protos,\r\n--> 788                          op_def=op_def)\r\n    789       return output_structure, op_def.is_stateful, op\r\n    790 \r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in create_op(***failed resolving arguments***)\r\n    463     return super(FuncGraph, self).create_op(\r\n    464         op_type, inputs, dtypes, input_types, name, attrs, op_def,\r\n--> 465         compute_device=compute_device)\r\n    466 \r\n    467   def capture(self, tensor, name=None):\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    505                 'in a future version' if date is None else ('after %s' % date),\r\n    506                 instructions)\r\n--> 507       return func(*args, **kwargs)\r\n    508 \r\n    509     doc = _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(***failed resolving arguments***)\r\n   3294           input_types=input_types,\r\n   3295           original_op=self._default_original_op,\r\n-> 3296           op_def=op_def)\r\n   3297       self._create_op_helper(ret, compute_device=compute_device)\r\n   3298     return ret\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1712           op_def, inputs, node_def.attr)\r\n   1713       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\r\n-> 1714                                 control_input_ops)\r\n   1715 \r\n   1716     # Initialize self._outputs.\r\n\r\n~/Documents/beta1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1552   except errors.InvalidArgumentError as e:\r\n   1553     # Convert to ValueError for backwards compatibility.\r\n-> 1554     raise ValueError(str(e))\r\n   1555 \r\n   1556   return c_op\r\n\r\nValueError: Duplicate node name in graph: 'concat'\r\n```\r\n", "comments": ["Added a PR #31145 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31137\">No</a>\n"]}, {"number": 31136, "title": "Inconsistent gradient", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.4.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Colab GPU\r\n\r\n**Describe the current behavior**\r\nI have a very simple Keras model and I want to compute the gradient of the different layers using TensorFlow. I start by creating the computational graph in the first cell of a Jupyter notebook. Here is the code of the computational graph:\r\n\r\n    import tensorflow as tf\r\n    import tensorflow.keras as keras\r\n    import tensorflow.keras.backend as K\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Dense, Input, Layer\r\n    from tensorflow.keras.models import Model\r\n    input_tensor = Input(shape=(20,), name=\"input\")\r\n    print(input_tensor.name)\r\n    hidden = Dense(100, activation='relu')(input_tensor)\r\n    out1 = Dense(10, activation='relu', name=\"out1\")(hidden)\r\n    model = Model(inputs=input_tensor, outputs=[out1])\r\n    grad = []\r\n    for i in range(4):\r\n       grad.append(tf.gradients(out1, model.trainable_weights[i]))\r\n    model.compile(loss={\"out1\": \"mse\"}, \r\n    optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\n\r\n    np.random.seed(0)\r\n    X = np.random.random((3, 20)).astype(np.float32)\r\n    Y = np.random.random((3, 10)).astype(np.float32)\r\n    model.fit(x={'input' : X}, y={'out1' : Y}, batch_size=1, epochs=10)\r\n\r\nThen each time I run the tf.gradients operator, I get a different gradient vector (the gradient changes). This result is not reasonable. Where is the problem in my code?\r\n\r\nAnd here is the code for the created Session:\r\n\r\n    with tf.Session() as sess:\r\n       sess.run(tf.global_variables_initializer())\r\n       out_grad = sess.run(grad, feed_dict={'input:0':X})\r\n       print(out_grad)\r\n\r\n**Describe the expected behavior**\r\nThe same gradient each time I run the `out_grad = sess.run(grad, feed_dict={'input:0':X})`\r\n\r\n", "comments": ["I solved this problem by using `tf.keras.backend.get_session()`.\r\nI think that the problem is that `tf.global_variables_initializer()` reinitialize the training weights each time it is called.\r\nWhen I run this code multiple times, I get consistent results.\r\n\r\n     sess = tf.keras.backend.get_session({'input:0':X})\r\n     out_grad = sess.run(grad, feed_dict={'input:0':X})\r\n     print(out_grad[0][0])"]}, {"number": 31135, "title": "[TF 2.0] How to globally force CPU?", "body": "In TF 1.x it was possible to force CPU only by using:\r\n\r\n```\r\nconfig = tf.ConfigProto(device_count = {'GPU': 0})\r\n```\r\nHowever, `ConfigProto` doesn't exist in TF 2.0 and changing a OS environment variable seems very clunky.\r\n\r\nWhat's the TF 2.0 way of doing this?\r\n\r\n", "comments": ["There might be a better way, but you can use `tf.config.experimental.set_visible_devices([], 'GPU')` to hide any GPU (they can still be listed using `tf.config.experimental.list_physical_devices('GPU')` and restored using the first function with different arguments, but will no longer appear when running `tf.config.experimental.list_logical_devices('GPU')` and should therefore not be used to place operations).", "This can help you set all operations on CPU;\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n# Set CPU as available physical device\r\nmy_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\r\ntf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')\r\n\r\n# To find out which devices your operations and tensors are assigned to\r\ntf.debugging.set_log_device_placement(True)\r\n\r\n# Create some tensors and perform an operation\r\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\nc = tf.matmul(a, b)\r\n\r\nprint(c)\r\n```\r\nOutput\r\n```python\r\n2.0.0-beta1\r\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0\r\ntf.Tensor(\r\n[[22. 28.]\r\n [49. 64.]], shape=(2, 2), dtype=float32)\r\n```", "That works, thank you.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "for anyone who is using tf 2.1, the above comment does not seems to work.\r\n\r\nI use ``tf.config.set_visible_devices([], 'GPU')`` seems to do the job correctly", "@henrysky this is indeed part of the breaking changes introduced in 2.1 (as documented in the release notes), which promoted some former experimental functions to the public scope; thank you for pointing the new correct syntax out :)", "tf.config.set_visible_devices([], 'GPU')\r\nWorks for tf 2.2", "I managed to limit cpus by using `psutils` lib.\r\nI provided this the beginning of the function\r\n```\r\npid = psutil.Process(os.getpid())\r\npid.cpu_affinity([0, 1])\r\n```\r\nThe later call of `model.fit` utilized exactly 2 cpus"]}, {"number": 31134, "title": "Can not run  ./build_ios_universal_lib.sh", "body": "According to https://www.tensorflow.org/lite/guide/build_ios, I am running tensorflow/lite/tools/make/build_ios_universal_lib.sh. However, when I run it, it gives me the error: \r\n\r\nUndefined symbols for architecture x86_64:\r\n  \"tflite::ResourceVariable::~ResourceVariable()\", referenced from:\r\n      tflite::Interpreter::~Interpreter() in benchmark-lib.a(interpreter.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/Users/broccoli/Downloads/tensorflow-master/tensorflow/lite/tools/make/gen/ios_x86_64/bin/benchmark_model] Error 1\r\n\r\n\r\nIs there any way to fix this error?", "comments": ["I had the same problem", "any news?", "I'm getting a similar issue with the build_ios_universal_lib.sh:\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"tflite::Interpreter::AllocateTensors()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::Interpreter::Invoke()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::Interpreter::~Interpreter()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::FlatBufferModel::~FlatBufferModel()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::InterpreterBuilder::~InterpreterBuilder()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::InterpreterBuilder::operator()(std::__1::unique_ptr<tflite::Interpreter, std::__1::default_delete<tflite::Interpreter> >*)\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::DefaultErrorReporter()\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::PrintInterpreterState(tflite::Interpreter*)\", referenced from:\r\n      _main in minimal.o\r\n  \"tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()\", referenced from:\r\n      _main in minimal.o\r\n  \"vtable for tflite::MutableOpResolver\", referenced from:\r\n      _main in minimal.o\r\n  NOTE: a missing vtable usually means the first non-inline virtual member function has no definition.\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```", "Sorry for the late update to this issue.\r\nThe `build_ios_universal.sh` is being deprecated, and the new recommended way of building the TFLite for iOS is using bazel instead.\r\n\r\nCould you follow the [recently updated build guide](https://www.tensorflow.org/lite/guide/build_ios) and see if that works for you?", "@yyoon Thanks I will look into it!", "Let me close the bug for now. Please feel free to reopen or file a new issue if the problem persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31134\">No</a>\n"]}, {"number": 31133, "title": "[ROCm] Fix for the broken `--config=rocm` build", "body": "The following PR/commit breaks the `--config=rocm` build\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/30997\r\n\r\nIt introduces a declaration of the variable \"data_layout\" , within only the #else section of a #if-else block, and then adds a reference to \"data_layout\" outside of that #if-else block. Since the ROCm build takes #if path, the declaration for \"data_layout\" is missing, leading to the compile error on the reference to it.\r\n\r\nThe fix is add the corresponding declaration for \"data_layout\" in the #if section.\r\n\r\n------------------------------------------------------------------\r\n\r\n@tatianashp @whchung @chsigg \r\n", "comments": ["The failure in the `Linux GPU` run does not seem to be related to the changes in this PR", "@tatianashp can you please review this PR ?"]}, {"number": 31132, "title": "TPU support for training with scheduled sampling", "body": "Scheduled sampling in `tf.contrib.seq2seq` is implemented using form of `where` that is not supported on TPU. This PR modifies scheduled sampling helper so that it can be used to train on any device.", "comments": ["Feedback (and hopefully merge) on this PR would be really appreciated. So far we have to extend `tf_contrib.seq2seq.ScheduledEmbeddingTrainingHelper` in our project (https://github.com/sciforce/phones-las/blob/master/utils/training_helper.py#L47) to add these changes and be able to enjoy faster training times on TPU.", "Can one of the admins verify this patch?", "Please, merge this PR. It is really useful for any project that uses attention APIs and training on TPUs.", "@tensorflowbutler  Sorry for delay. I'll post an update in next few days.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "closing this PR as contrib folder will be deprecated in 2.0, thank you.\r\nCC @mihaimaruseac"]}, {"number": 31131, "title": "Unable to save model when using Mirrored Strategy with multiple GPUs", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9 (stretch) (Linux 4.9.0-9-amd64)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-7260-gd4bc7d98c6 2.0.0-dev20190729\r\n- Python version: 3.7.2\r\n- GPU model and memory: 2 X Tesla V100 (24 gig RAM)\r\n\r\n**Describe the current behavior**\r\nAn error is thrown when saving a keras model that has been compiled in using a Mirrored Strategy using more than one GPU.\r\n\r\n**Describe the expected behavior**\r\nShould be able to save and reload models that have been saved under Mirror Strategy.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```import tensorflow as tf\r\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\n\r\n\r\ndef get_model():\r\n    with mirrored_strategy.scope():\r\n        model = InceptionV3(input_shape=(299, 299, 3),\r\n                                 include_top=True,\r\n                                 weights='imagenet',\r\n                                 pooling='avg')\r\n\r\n        model.compile(loss='sparse_categorical_crossentropy',\r\n                      optimizer=tf.keras.optimizers.Adam(),\r\n                      metrics=['accuracy'])\r\n\r\n    return model\r\n\r\n\r\nmodel = get_model()\r\nmodel.save(\"my_model\")\r\n```\r\n\r\n**Other info / logs**\r\nSee attached.\r\n[logs.txt](https://github.com/tensorflow/tensorflow/files/3442574/logs.txt)\r\n\r\n", "comments": ["You need to define the model function inside `strategy.scope()` or remove the function and just use \r\n```\r\n        with mirrored_strategy.scope():\r\n        model = InceptionV3(input_shape=(299, 299, 3),\r\n                             include_top=True,\r\n                             weights='imagenet',\r\n                             pooling='avg')\r\n\r\n        model.compile(loss='sparse_categorical_crossentropy',\r\n                      optimizer=tf.keras.optimizers.Adam(),\r\n                      metrics=['accuracy'])`\r\n", "@richriley ,\r\nCan you please let us know if solution provided by @rishabhsahrawat worked? Thanks!", "It seems that my original code now works on 2.0.0-dev20190809. I can confirm that @rishabhsahrawat's example also works.\r\n\r\nThanks for your help.", "Closing since the issue is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31131\">No</a>\n", "Closing since the issue is resolved.Thanks!"]}, {"number": 31130, "title": "KerasClassifier.score: Support `weighted_metrics`", "body": "**Background:**\r\n\r\n`tf.keras.wrappers.scikit_learn.KerasClassifier` is a wrapper for `tf.keras.Model`.\r\n\r\n\r\n**Problem:**\r\n\r\nWhen [`tf.keras.Model.compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) is called with `weighted_metrics`,\r\nit automatically [adds a `'weighted_'` prefix](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/engine/training_utils.py#L796) to the metric's name.\r\nI.e. `'acc'` becomes `'weighted_acc'`.\r\nEven when the metric's `name` is set manually via its class initializer.\r\n\r\n`KerasClassifier.score` on the other hand cares only for the metrics `'accuracy'` or `'acc'`,\r\nbut sometimes `tf.keras.Model.compile` is called only with `weighted_metrics` and not `metrics`.\r\nAnd so these metrics are not found and an error is raised.\r\n\r\n\r\n**Proposed solution:**\r\n\r\n`KerasClassifier.score` should look for `'weighted_accuracy'` and `'weighted_acc'` too.", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\nCC @mihaimaruseac", "@eliadl Please make this PR against master and if you also want the changes in the 1.14 branch you can make a cherry-pick against it. The window for cherry-picks for 1.14.1 patch release is just until Wednesday."]}, {"number": 31129, "title": "Introduce the concept of Frontend Attributes.", "body": "Summary:\r\n    Frontend Attributes can be set by the user or the frontend and\r\n    are passed through to the XLA backend as a dictionary of strings where they can\r\n    be used to modify the way the HLO instructions are executed.\r\n\r\nXLA Development discussion:\r\n    https://groups.google.com/d/msg/xla-dev/9TM0-1N_JlM/Q2R8o2RgBwAJ\r\n\r\nTest Plan:\r\n    bazel test returned:\r\n    INFO: Executed 522 out of 522 tests: 522 tests pass.\r\n    INFO: There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n    INFO: Build completed successfully, 4027 total actions\r\n    SUCCESS!", "comments": ["Maybe I'm missing the context, but was there a reason why we couldn't reuse backend_config?", "You've got more context in the XLA-dev thread here : https://groups.google.com/d/msg/xla-dev/9TM0-1N_JlM/Q2R8o2RgBwAJ\r\n\r\nBasically we do store the attributes as part of our backend config once we're on the XLA backend side but before that the backend config seems to already be used to store other information. ", "What other information was stored in backend_config before the backend compiler starts? I only know the situation where the XLA client sets backend_config for the custom call string. Also, don't you have the same problem when moving some of the front-end config to the back-end config if it was already being used?\r\n\r\nI'm fine with approving this change, but the frontend_attribute added in this PR seemed to mean almost identical to backend_config, with the same level of guarantee (i.e., we do not guarantee to preserve this during compiler transformations), and also the email discussion was about reusing the  backend_config. ", "From a quick look through the code base it's being used by the `gpu` and the `cudnn` backends and also by the `custom_calls` since the `opaque` attribute was removed.\r\nSeems risky to re-purpose an attribute which is already used in several places for different things.", "Moreover, the backend config is not an uninterpreted map from strings to strings; it is a serialized proto where the type of the proto is backend dependent.", "@hyouklee WDYT?  Do you find the comments made here (for not using the already existing `backend_config`) convincing?", "Oh, I thought this PR was already merged in and closed. Yes, I'm fine with adding the front-end attribute.", "Can one of the admins verify this patch?", "FYI: Don't know if it's intentional but it looks like the CI doesn't run or build `//tensorflow/compiler/xla/client:all`", "@AnthonyBarbier there are some conflicts , can you please resolve ", "> @AnthonyBarbier there are some conflicts , can you please resolve\r\n\r\nSure. Done. ", "Did someone accidentally merge this PR ? \r\nLooks like it wasn't the latest version though that got merged.", "> Did someone accidentally merge this PR ?\r\n> Looks like it wasn't the latest version though that got merged.\r\n\r\nI merged it yesterday with some minor build fixes.  Can you please create a new PR if there are things missing in what got merged?", "Closing this PR as this has been merged internally , @AnthonyBarbier please open a new PR things which are missing, sorry about that."]}, {"number": 31128, "title": "What is the right way to use intra_op_parallelism_threads and inter_op_parallelism_threads?", "body": "Hi,\r\n  I create a `Session` with `tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)`. \r\nWhen I run the `Session`, I use the `top` command to observe the situations. But I found the program still use 1700% CPU. Why did this happen? What's the right way to control the number of cores/threads used by tensorflow?\r\nthx!\r\n\r\n", "comments": ["Could you set:\r\n```\r\nexport OMP_NUM_THREADS=1\r\n```\r\nWhat's your cpu utilization now?\r\n", "> Could you set:\r\n> \r\n> ```\r\n> export OMP_NUM_THREADS=1\r\n> ```\r\n> \r\n> What's your cpu utilization now?\r\n\r\nI set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.", "> > Could you set:\r\n> > ```\r\n> > export OMP_NUM_THREADS=1\r\n> > ```\r\n> > \r\n> > \r\n> > What's your cpu utilization now?\r\n> \r\n> I set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.\r\n\r\nCould you share some code snippet you used?", "> > > Could you set:\r\n> > > ```\r\n> > > export OMP_NUM_THREADS=1\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > What's your cpu utilization now?\r\n> > \r\n> > \r\n> > I set this in the shell and then run the same python program. The CPU utilization is still more than 1000%.\r\n> \r\n> Could you share some code snippet you used?\r\n\r\nSure. I was doing a unit test in python with my model. The model is converted to TensorFlow from another framework through ONNX.\r\n\r\n   \r\n    config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n    __sess = tf.Session(graph=graph, config=config)\r\n    input_node = \"input:0\"\r\n    ivector_node = \"ivector:0\"\r\n    inputs = np.loadtxt(\"inputs.txt\", dtype=np.float32)\r\n    ivector = np.loadtxt(\"ivector.txt\", dtype=np.float32)\r\n    __feed_dict = {input_node: [inputs], ivector_node: [ivector]}\r\n    output_node = \"output.affine:0\"\r\n    __out_tensor = __sess.graph.get_tensor_by_name(output_node)\r\n    output = __sess.run(__out_tensor, __feed_dict)\r\n\r\nWhen I run this snippet, the CPU usage is about 160% when it goes into session.run.\r\nWhen I run the session in a loop like: \r\n\r\n    for _ in range(50):    \r\n       _ = __sess.run(__out_tensor, __feed_dict)\r\n\r\nThe CPU usage will rise to near 1600%.\r\n\r\nThanks for your relay!  If there is anything else that needs to be provided, just tell me.", "@JiayiFu\r\nI try setting OMP_NUM_THREADS, intra_op_parallelism_threads and inter_op_parallelism_threads to 1.  And the cpu utilization is almost 100%.\r\n\r\nIs it possible you have more than one sessions(async) are running, such as data_session which makes your CPU usage to 160%?", "@Leslie-Fang \r\nI try to simplify the code and it just creates a session and runs once like the followed snippet. The CPU usage still raises to more than 100% and when I do session.run in a loop, the usage raised to more than 1000%\r\n\r\n    config = tf.ConfigProto(intra_op_parallelism_threads=1,\r\n                            inter_op_parallelism_threads=1)\r\n    sess = tf.Session(graph=graph, config=config)\r\n    output_node = \"output.affine:0\"\r\n    out_tensor = sess.graph.get_tensor_by_name(output_node)\r\n    _ = sess.run(out_tensor, feed_dict)\r\n    sess.close()\r\n\r\nIn your case, what's kind of model you use? Have you tried to do the session.run in a loop? and is there any difference in the CPU usage?\r\nBtw, what is data_session you mentioned?", "@JiayiFu ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!", "> @JiayiFu ,\r\n> In order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks!\r\n\r\nThanks for your advice! This is the complete code snippet. It just loads a model and then runs many times. If you need the model, I would like to upload it. But I guess the model shouldn't be the reason that the CPU usage raised to more than 100%. \r\n    \r\n    import os\r\n    \r\n    import numpy as np\r\n    import tensorflow as tf\r\n\r\n    def __test_cnn(pb_path):\r\n      graph_def = tf.GraphDef()\r\n      with tf.gfile.GFile(pb_path, \"rb\") as f:\r\n        graph_def.ParseFromString(f.read())\r\n        tf.import_graph_def(graph_def, name='')\r\n      \r\n      os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n      os.environ[\"OMP_NUM_THREADS\"] = \"1\"\r\n      config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\n      with tf.Session() as sess:\r\n        input_node = \"input:0\"\r\n        ivector_node = \"ivector:0\"\r\n        output_name = \"output.affine:0\"\r\n        output_tensor = sess.graph.get_tensor_by_name(output_name)\r\n        input_value = np.random.rand(1,79, 43)\r\n        ivector_value = np.random.rand(1,79, 100)\r\n        feed_dict = {input_node: [input_value], ivector_node: [ivector_value]}\r\n        _ = sess.run(out_tensor, feed_dict)\r\n       \r\n        for _ in range(50):\r\n          _ = sess.run(out_tensor, feed_dict)\r\n        \r\nMy OS version is Ubuntu 16.04. The TF is 1.14. The Python version is 3.5.2.", "Is it possible that the effects of `inter` and `intra` depend on the installation way? by pip or building from source?", "I tried these on another server and reinstalled the TensorFlow of it. It seems that everything is back to normal...Maybe there is something wrong in my envriment."]}]