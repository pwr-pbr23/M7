[{"number": 50915, "title": "loading a keras model on raspberry pi", "body": "```\r\nTraceback (most recent call last):\r\n  File \"/home/pi/Desktop/testting/client.py\", line 21, in <module>\r\n    modelLSTM = load_model('/home/pi/Desktop/ANN_model.keras')\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 168, in load_model_from_hdf5\r\n    custom_objects=custom_objects)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/serialization.py\", line 106, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 263, in deserialize_keras_object\r\n    config, module_objects, custom_objects, printable_module_name)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\", line 234, in class_and_config_for_serialized_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\nValueError: Unknown layer: Functional\r\n```\r\n\r\n> I get this error when running a client code from the socket communication module. The role of the client is to receive inputs from the server code in a NumPy array format fed to a saved Keras model where I saved first on my laptop and transferred to my raspberry pi. so a brief:\r\n\r\n`import tensorflow as tf\r\nfrom random import randint\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras.layers import Activation, Dense, Input\r\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam, Ftrl\r\nfrom tensorflow.keras.metrics import categorical_crossentropy, mean_squared_error\r\nfrom tensorflow.keras.models import model_from_json\r\nimport pandas as pd\r\nimport numpy as np\r\nimport xlrd\r\nimport matplotlib.pyplot as plt\r\nimport openpyxl\r\nfrom math import sqrt\r\nimport sklearn\r\n\r\n\r\n\r\ntrain_inputs = []\r\ntrain_outputs = []\r\ntest_inputs = []\r\ntest_outputs = []\r\ntest_time = []\r\n\r\ntrain_inputs = np.array(train_inputs)\r\ntrain_outputs = np.array(train_outputs)\r\ntest_inputs = np.array(test_inputs)\r\ntest_outputs = np.array(test_outputs)\r\ntest_time = np.array(test_time)\r\n\r\noutput = pd.read_excel(r'K:\\BachelorThesis\\Data\\Training_data\\Mix_Data_outputs.xlsx')\r\ntrain_outputs = output.to_numpy()\r\n#print(type(train_outputs[-1][0]))\r\n\r\ninput = pd.read_excel(r'K:\\BachelorThesis\\Data\\Training_data\\Mix_Data_inputs.xlsx')\r\ntrain_inputs = input.to_numpy()\r\n#print(type(train_inputs[-1][0]))\r\n\r\ntemp_output = pd.read_excel(r'K:\\BachelorThesis\\Data\\TestingData\\Mix_Data_outputs.xlsx')\r\ntest_outputs = temp_output.to_numpy()\r\n#print(type(train_outputs[-1][0]))\r\n\r\ntemp_input = pd.read_excel(r'K:\\BachelorThesis\\Data\\TestingData\\Mix_Data_inputs.xlsx')\r\ntest_inputs = temp_input.to_numpy()\r\n#print(type(train_inputs[-1][0]))\r\n\r\ntime = pd.read_excel(r'K:\\BachelorThesis\\Data\\TestingData\\Nuneaton_RDE_Track\\DataNuneaton_time.xlsx')\r\ntest_time = time.to_numpy()\r\n\r\nscaler_i = MinMaxScaler()\r\nscaler_o = MinMaxScaler()\r\nscaler_i.fit(train_inputs)\r\nnormalized_inputs = scaler_i.transform(train_inputs)\r\nscaler_o.fit(train_outputs)\r\nnormalized_outputs = scaler_o.transform(train_outputs)\r\n\r\n\r\ntf.random.set_seed(16)\r\n\r\nlayer1 = Input(shape = (8, ))\r\nlayer2 = Dense(2048, activation='tanh')(layer1)\r\nlayer3 = Dense(1024, activation='tanh')(layer2)\r\nlayer4 = Dense(512, activation='tanh')(layer3)\r\nlayer5 = Dense(512, activation='tanh')(layer4)\r\nlayer6 = Dense(256, activation='tanh')(layer5)\r\nlayer7 = Dense(256, activation='tanh')(layer6)\r\nlayer8 = Dense(512, activation='tanh')(layer7)\r\nlayer9 = Dense(512, activation='tanh')(layer8)\r\nlayer10 = Dense(1024, activation='tanh')(layer9)\r\nlayer11 = Dense(2048, activation='tanh')(layer10)\r\noutput = Dense(8, )(layer11)\r\nmodel = Model(inputs = layer1, outputs = output)\r\n\r\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mean_squared_error'])\r\n\r\nhistory = model.fit(x=normalized_inputs, y=normalized_outputs, validation_split=0.3, epochs=30, verbose=2)#, batch_size = 400, validation_batch_size= 100)\r\n\r\nhistory.history.keys()\r\n\r\nplt.plot(history.history['loss'])\r\nplt.plot(history.history['mean_squared_error'])\r\nplt.title('training')\r\nplt.ylabel('loss & mse')\r\nplt.xlabel('epochs')\r\nplt.legend(['loss','mse'],loc='upper left')\r\nplt.show()\r\n\r\nplt.plot(history.history['val_loss'])\r\nplt.plot(history.history['val_mean_squared_error'])\r\nplt.title('validation')\r\nplt.ylabel('val_loss & val_mse')\r\nplt.xlabel('epochs')\r\nplt.legend(['val_loss','val_mse'],loc='upper left')\r\nplt.show()\r\n\r\nscaler_ti = MinMaxScaler()\r\nscaler_to = MinMaxScaler()\r\nscaler_ti.fit(test_inputs)\r\nnormalized_test_inputs = scaler_ti.transform(test_inputs)\r\nscaler_to.fit(test_outputs)\r\nnormalized_test_outputs = scaler_to.transform(test_outputs)\r\n\r\nmodel.evaluate(normalized_test_inputs, normalized_test_outputs)\r\n\r\nprediction = model.predict(normalized_test_inputs)\r\ninverse = scaler_to.inverse_transform(prediction)\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,0])\r\nax.plot(test_time, test_outputs[:,0], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,1])\r\nax.plot(test_time, test_outputs[:,1], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,2])\r\nax.plot(test_time, test_outputs[:,2], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,3])\r\nax.plot(test_time, test_outputs[:,3], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,4])\r\nax.plot(test_time, test_outputs[:,4], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,5])\r\nax.plot(test_time, test_outputs[:,5], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,6])\r\nax.plot(test_time, test_outputs[:,6], linestyle = 'dashed')\r\nplt.show()\r\n\r\nfig, ax = plt.subplots(figsize=(30,10))\r\nax.plot(test_time, inverse[:,7])\r\nax.plot(test_time, test_outputs[:,7], linestyle = 'dashed')\r\nplt.show()\r\n\r\nmodel.save('K:\\BachelorThesis\\code testing\\TireForces.LSTM\\ANN_model.keras')\r\n\r\n#model_json = model.to_json()\r\n#with open(\"K:\\BachelorThesis\\code testing\\tireForcesANN/modelANN.json\", \"w\") as json_file:\r\n #   json_file.write(model_json)\r\n# serialize weights to HDF5\r\n#model.save_weights(\"K:\\BachelorThesis\\code testing\\tireForcesANN/modelANN.h5\")\r\n#print(\"Saved model to disk\")`\r\n\r\n> This code on my laptop was used to train a model and save it in my laptop then the saved model was transferred using WinSCP to my raspberry pi.\r\n\r\n`import socket\r\nimport numpy as np\r\nimport pandas as pd\r\nimport sklearn\r\nfrom sklearn.preprocessing import MinMaxScaler\r\n\r\nscaler_ti = MinMaxScaler()\r\ntest_inputs = []\r\ntest_inputs = np.array(test_inputs)\r\ntemp_in = pd.read_excel(r'K:\\BachelorThesis\\Data\\TestingData\\Mix_Data_inputs.xlsx')\r\ntest_inputs = temp_in.to_numpy()\r\nrows = test_inputs.shape[0]\r\nscaler_ti.fit(test_inputs)\r\nnormalized_test_inputs = scaler_ti.transform(test_inputs)\r\n\r\n\r\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\nhost = ''\r\nport = 62402\r\ns.bind((host,port))\r\ns.listen(5)\r\n\r\nwhile True:\r\n    \r\n    clientsocket, address = s.accept()\r\n    print(f\"connection from {address} has been established!\")\r\n    strg = test_inputs\r\n    temp = strg.tobytes()\r\n    clientsocket.send(temp)\r\n    clientsocket.close()\r\n        \r\n\r\n`\r\n\r\n> This code is the server code on my laptop to feed the client code with the inputs.\r\n\r\n`import socket\r\nimport numpy as np\r\n#import pandas as pd\r\n#import sklearn\r\n#from sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import load_model\r\nimport tensorflow as tf\r\nfrom random import randint\r\nimport h5py\r\n#import argparse\r\n\r\n#ap = argparse.ArgumentParser()\r\n#ap.add_argument(\"-d\", \"--dataset\", required=True,\r\n\t#help=\"path to input dataset\")\r\n#ap.add_argument(\"-m\", \"--LSTM_model.h5\", required=True, help=\"/home/pi/Desktop\")\r\n#args = vars(ap.parse_args())\r\n\r\ni = 0\r\nfinal = []\r\nfinal = np.array(final)\r\nmodelLSTM = load_model('/home/pi/Desktop/ANN_model.keras')\r\n#json_file = open('LSTM_model.json', 'r')\r\n#loaded_model_json = json_file.read()\r\n#json_file.close()\r\n#loaded_model = model_from_json(loaded_model_json)\r\n\r\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\nhost = '192.168.1.24'\r\nport = 62402\r\ns.connect((host, port))\r\n\r\nwhile True:\r\n    if i in range(65533):\r\n        i = i + 1\r\n        msg = s.recv(64)\r\n        out = np.frombuffer(msg)\r\n        #out = out.reshape(1,8)\r\n        #out = out.reshape(1,1,8)\r\n        #prediction = modelLSTM.predict(out)\r\n        #inverse = scaler_ti.inverse_transform(prediction.reshape(1,8))\r\n        #print(prediction)\r\n        #print(inverse)\r\n        #final = np.vstack(inverse) \r\n        print(out)\r\n        if len(msg) <= 0:\r\n            break\r\n#print (final)\r\n\r\n`\r\n\r\n> This code is the client code on my raspberry pi where I get the error every time I run. \r\n\r\n", "comments": ["@karimanis \r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\n", "@karimanis Code is too long to debug. Any (TF/Keras) model needs to be converted to TFlite model to run on Raspberry Pi. Did you convert the model? \r\n\r\nPlease check the instructions here https://www.tensorflow.org/lite/tutorials", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50915\">No</a>\n"]}, {"number": 50914, "title": "[XLA:GPU] BatchNorm lowering overhaul for cudnn fast fp16 kernels and act+side input fusion", "body": "**Problem Definition:**\r\n1. Currently, BatchNorm, when calling into cuDNN, forces the data layout to be NCHW (not by transforms but by squashing physical dimensions) irrespective of their original layout. This leads to calls into suboptimal BatchNorm kernels in cuDNN resulting in drastic performance regression for NHWC models. Note that latest cuDNN versions have capability of calling into fast semi-persistent NHWC BatchNorm fp16 kernels. \r\n2.  In addition, cuDNN allows fusion of BN+Act and BN+Add+Act (implemented in this PR) that improves BN Training performance (over BatchNorm expansion).  BN Backward fusion has not been implemented yet.\r\nNote, whether or not XLA calls into fused cuDNN BN depends on whether Grappler remaps FusedBatchNormV3 to FusedBatchNormEx. CuDNN FusedBatchNormEx API (which allows fusion)  is only valid\r\na) NHWC datatype,\r\nb) DT_HALF,\r\nc) only ReLU activation,\r\nd) Channel dim has to be a multiple of 4, and\r\ne) BatchNormSpatial Persistent mode.\r\n\r\n**Implementation:**\r\nCalling into the fast semi-persistent  and fused kernels require an overhaul of the BatchNorm lowering since these require additional reserve space (transfer bits from the forward to the backward BatchNorm op) and work space (scratch space).\r\nThis PR modifies BatchNorm lowering from TF2XLA bridge to rewriter optimization to thunks in order to make sure that the performant kernels get called.\r\n", "comments": ["@AyanmoI  Can you please resolve conflicts? Thanks!", "@AyanmoI Any update on this PR? Please. Thanks!", "@AyanmoI Any update on this PR? Please. Thanks!", "@AyanmoI Any update on this PR? Please. Thanks!", "@AyanmoI  Any update on this PR? Please. Thanks!"]}, {"number": 50913, "title": "Mention intro-to-graphs in tf.function docstring.", "body": "Making it more discoverable.", "comments": ["+ @mdanatg "]}, {"number": 50912, "title": "Ensure non-empty compressed input in tf.raw_ops.UncompressElement", "body": "PiperOrigin-RevId: 383955815\nChange-Id: I072a84fd02738dd2f51b3f42836ed80067dba4a8", "comments": []}, {"number": 50911, "title": "Ensure non-empty compressed input in tf.raw_ops.UncompressElement", "body": "PiperOrigin-RevId: 383955815\nChange-Id: I072a84fd02738dd2f51b3f42836ed80067dba4a8", "comments": []}, {"number": 50910, "title": "Ensure non-empty compressed input in tf.raw_ops.UncompressElement", "body": "PiperOrigin-RevId: 383955815\nChange-Id: I072a84fd02738dd2f51b3f42836ed80067dba4a8", "comments": []}, {"number": 50909, "title": "Validate num_elements input in tf.raw_ops.TensorListReserve", "body": "PiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f", "comments": []}, {"number": 50908, "title": "Validate num_elements input in tf.raw_ops.TensorListReserve", "body": "PiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f", "comments": []}, {"number": 50907, "title": "Validate num_elements input in tf.raw_ops.TensorListReserve", "body": "PiperOrigin-RevId: 383954564\nChange-Id: I454bd78eff85bc4f16ddb7e608596971cca47f8f", "comments": []}, {"number": 50906, "title": "Fix null ptr deref in tf.raw_ops.RaggedTensorToTensor", "body": "PiperOrigin-RevId: 384257511\nChange-Id: I0484ad285039d132d6c41b284a7fcdd2b774a38e", "comments": []}, {"number": 50905, "title": "Fix null ptr deref in tf.raw_ops.RaggedTensorToTensor", "body": "PiperOrigin-RevId: 384257511\nChange-Id: I0484ad285039d132d6c41b284a7fcdd2b774a38e", "comments": []}, {"number": 50904, "title": "Fix null ptr deref in tf.raw_ops.RaggedTensorToTensor", "body": "PiperOrigin-RevId: 384257511\nChange-Id: I0484ad285039d132d6c41b284a7fcdd2b774a38e", "comments": []}, {"number": 50903, "title": "Fix accessing possible nullptr in tensorflow::data::CompressElement and UncompressElement which are used in tf.data.service.", "body": "PiperOrigin-RevId: 373920841\nChange-Id: Ia88d78aee09fa19bb53a0f163fd19620d0c68743", "comments": []}, {"number": 50902, "title": "Fix accessing possible nullptr in tensorflow::data::CompressElement and UncompressElement which are used in tf.data.service.", "body": "PiperOrigin-RevId: 373920841\nChange-Id: Ia88d78aee09fa19bb53a0f163fd19620d0c68743", "comments": []}, {"number": 50901, "title": "Fix accessing possible nullptr in tensorflow::data::CompressElement and UncompressElement which are used in tf.data.service.", "body": "PiperOrigin-RevId: 373920841\nChange-Id: Ia88d78aee09fa19bb53a0f163fd19620d0c68743", "comments": []}, {"number": 50900, "title": "Disallow division by zero FPE in tf.raw_ops.SparseDenseCwiseDiv", "body": "PiperOrigin-RevId: 383959809\nChange-Id: Ibe88458bdf66a686c93e354b8255dec94285c560", "comments": []}, {"number": 50899, "title": "Disallow division by zero FPE in tf.raw_ops.SparseDenseCwiseDiv", "body": "PiperOrigin-RevId: 383959809\nChange-Id: Ibe88458bdf66a686c93e354b8255dec94285c560", "comments": []}, {"number": 50898, "title": "Disallow division by zero FPE in tf.raw_ops.SparseDenseCwiseDiv", "body": "PiperOrigin-RevId: 383959809\nChange-Id: Ibe88458bdf66a686c93e354b8255dec94285c560", "comments": []}, {"number": 50897, "title": "Predict data from a collection to determine a single object with the most used data.", "body": "**System information**\r\n- Linux Ubuntu 16.04\r\n- TensorFlow Lite JS\r\n\r\n**Description** \r\nI need to start from a set of objects, define a single object, with the most used data among the javascript object collection informed.\r\n\r\n**Example of set of objects:**\r\n```javascript\r\n[{\r\n    name: \"Pedro\",\r\n    product: \"Caneta\"\r\n}, {\r\n    name: \"Ana\",\r\n    product: \"Caderno\"\r\n}, {\r\n    name: \"Maria\",\r\n    product: \"Boracha\"\r\n}, {\r\n    name: \"Pedro\",\r\n    product: \"Caneta\"\r\n}, {\r\n    name: \"Pedro\",\r\n    product: \"Caneta\"\r\n}]\r\n```\r\n\r\n**Single object with the most used values in the above collections:**\r\n```javascript\r\n{\r\n    name: \"Pedro\",\r\n    product: \"Caneta\"\r\n}\r\n```\r\n\r\nThat's what I need, based on the javascript collections array you can see that the most used information for the fields (name and product), were `name: \"Pedro\"` and `product: \"Caneta\"`.\r\nThat kind of analysis I need.\r\n\r\nWhat function or how could I do this using tensorflow javascript, if you have any examples I'm very grateful, because I'm new to this.", "comments": ["@GlauberF ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code, dataset and tensorflow version to reproduce the issue reported here.Thanks!", "@tilakrayal \r\nit wouldn't be a problem, I would just like to know how to implement using tensorflow javascript, a solution that solves what I mentioned above.\r\nI know it might not be the right place here, but I'm new around here.\r\nIf you have an example with tensorflow javascript that I can use to create the solution I need.", "Hi Glauber,\r\n\r\nYou might want to raise issues in https://github.com/tensorflow/tfjs for TFJS related issues.\r\n\r\nThanks,\r\nTiezhen"]}, {"number": 50896, "title": "Could not find org.tensorflow:tensorflow-lite-support:0.1.0-rc1", "body": "Hello everybody :)\r\n\r\nI am converting a transformer model for text classification to TFLite. Conversion works. When I integrate it in the Android Studio template for text classification ([https://www.tensorflow.org/lite/examples/text_classification/overview](url)) I use lib_interpreter and obtain the following error:\r\n\r\nExecution failed for task ':app:checkInterpreterDebugAarMetadata'.\r\n> Could not resolve all files for configuration ':app:interpreterDebugRuntimeClasspath'.\r\n   > Could not find org.tensorflow:tensorflow-lite-support:0.1.0-rc1.\r\n     Searched in the following locations:\r\n       - https://dl.google.com/dl/android/maven2/org/tensorflow/tensorflow-lite-support/0.1.0-rc1/tensorflow-lite-support-0.1.0-rc1.pom\r\n       - https://repo.maven.apache.org/maven2/org/tensorflow/tensorflow-lite-support/0.1.0-rc1/tensorflow-lite-support-0.1.0-rc1.pom\r\n       - http://oss.sonatype.org/content/repositories/snapshots/org/tensorflow/tensorflow-lite-support/0.1.0-rc1/tensorflow-lite-support-0.1.0-rc1.pom\r\n     Required by:\r\n         project :app > project :lib_interpreter\r\n\r\nI would like to know if this is a problem related to the conversion of the model or what is going on here. Thank you very much! ", "comments": ["Could you file a bug at https://github.com/tensorflow/tflite-support ?", "Solved in [https://github.com/tensorflow/tflite-support/issues/616](url)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50896\">No</a>\n"]}, {"number": 50895, "title": "Added Python 3.9 support for \"Building TensorFlow Lite Standalone Pip\"", "body": "Add support for Python 3.9 to the Standalone Pip build of TensorFlow Lite by Bazel.", "comments": ["Hi Terry, \r\n\r\nAny thoughts on this PR?\r\n\r\nThanks,\r\nTiezhen"]}, {"number": 50894, "title": "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FE83356310> and will run it as-is.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 20H2 19042.985\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):Binary (Anconda + Tensorflow.org + Nvidia-CUDA)\r\n- TensorFlow version (use command below):2.4.0\r\n- Python version:3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: GeForce GTX 1650 computeCapability: 7.5  4098 Mib\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n**CMD Runtime Log\r\n2021-07-22 15:36:43.493521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-07-22 15:36:44.947073: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-07-22 15:36:44.953523: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-07-22 15:36:45.498789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.515GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\r\n2021-07-22 15:36:45.507262: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-07-22 15:36:45.517511: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-07-22 15:36:45.521634: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-07-22 15:36:45.530173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-07-22 15:36:45.537579: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-07-22 15:36:45.550524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-07-22 15:36:45.559310: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-07-22 15:36:45.565102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-07-22 15:36:45.568619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\nFound 28709 images belonging to 7 classes.\r\nFound 7178 images belonging to 7 classes.\r\n2021-07-22 15:36:46.904080: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-22 15:36:46.916424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.515GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s\r\n2021-07-22 15:36:46.925351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-07-22 15:36:46.929340: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-07-22 15:36:46.933846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-07-22 15:36:46.940659: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-07-22 15:36:46.944810: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-07-22 15:36:46.949295: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-07-22 15:36:46.954932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-07-22 15:36:46.958866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-07-22 15:36:46.963356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-07-22 15:36:47.561268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-22 15:36:47.566554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-07-22 15:36:47.582880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-07-22 15:36:47.586730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2907 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-07-22 15:36:47.596419: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-07-22 15:36:50.406265: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001FA29D673A0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function Model.make_predict_function.<locals>.predict_function at 0x000001FA29D673A0>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2021-07-22 15:36:50.483496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-07-22 15:36:51.687993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-07-22 15:36:51.696841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-07-22 15:36:54.689521: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-07-22 15:36:54.823788: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0", "comments": ["@Frost-fusion \r\n\r\nCould you please share the Colab gist to analyse the issue better.\r\nalso refer the similar issues [link1](https://github.com/tensorflow/tensorflow/issues/37144) and [link2](https://stackoverflow.com/questions/62931610/warningtensorflowautograph-could-not-transform-function-format-example-at). Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50894\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50894\">No</a>\n"]}, {"number": 50893, "title": "Enable '--distinct_host_configuration' for arm builds", "body": "This option is needed to cross compilation\r\n\r\nPiperOrigin-RevId: 383284957\r\nChange-Id: Ic49c3807c050ea5c71c31995e5a38ae6f06044a8", "comments": ["This cherry-pick resolves #50826 issue."]}, {"number": 50892, "title": "Tensorflow could not build with `config=asan`  flag", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: master\r\nPython version: 3.8.8\r\nInstalled using virtualenv? pip? conda?: cinda\r\nBazel version (if compiling from source): 3.7.2\r\nGCC/Compiler version (if compiling from source): clang-11\r\nCUDA/cuDNN version: CUDA 11.3, CuDNN 8\r\nGPU model and memory: GTX 1080 Ti\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI am trying to build TF with Address Sanitizer enabled. \r\n\r\nTF could not built with the flag `--config asan`.\r\n\r\nI tried to use the following command:\r\n\r\n```Batch\r\nCC=/usr/lib/llvm-11/bin/clang TMP=~/tmp bazel build --config asan --config=dbg --per_file_copt=//tensorflow/core/grappler/./.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\nand it failed with the following\r\n\r\n```Log\r\ntensorflow git:(master) \u2717 CC=/usr/lib/llvm-11/bin/clang TMP=~/tmp bazel build --config asan --config=dbg --per_file_copt=//tensorflow/core/grappler/./.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=149\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/user/anaconda3/envs/tfenv/bin/python3 --action_env PYTHON_LIB_PATH=/home/user/anaconda3/envs/tfenv/lib/python3.8/site-packages --python_path=/home/user/anaconda3/envs/tfenv/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.3 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1,6.1,6.1,6.1 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-11/bin/clang --config=cuda_clang\r\nINFO: Reading rc options for 'build' from /home/user/tensorflow/.bazelrc:\r\n  'build' options: --verbose_failures\r\nINFO: Found applicable config definition build:short_logs in file /home/user/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/user/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/user/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/user/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:asan in file /home/user/tensorflow/.bazelrc: --strip=never --copt -fsanitize=address --copt -DADDRESS_SANITIZER --copt -g --copt -O3 --copt -fno-omit-frame-pointer --linkopt -fsanitize=address\r\nINFO: Found applicable config definition build:dbg in file /home/user/tensorflow/.bazelrc: -c dbg --per_file_copt=+.*,-tensorflow.*@-g0 --per_file_copt=+tensorflow/core/kernels.*@-g0 --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD\r\nINFO: Found applicable config definition build:linux in file /home/user/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/user/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/toolchains/archive/d781e89e2ee797ea7afd0c8391e761616fc5d50d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c362c993e4cca885ba6afef155d864a2dfd21f85.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/bcf6f641acdbeb208ea07a9e8ded37cd5b796d26.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/highwayhash/BUILD.bazel:23:11: undeclared inclusion(s) in rule '@highwayhash//:arch_specific':\r\nthis rule is missing dependency declarations for the following files included by 'highwayhash/highwayhash/arch_specific.cc':\r\n  '/usr/lib/llvm-11/lib/clang/11.1.0/share/asan_blacklist.txt'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 0.680s, Critical Path: 0.40s\r\nINFO: 17 processes: 17 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAlso tried the command of `--copt -fsanitize=address --linkopt -fsanitize=address`:\r\n\r\n```Batch\r\n CC=/usr/lib/llvm-11/bin/clang TMP=~/tmp bazel build --copt -fsanitize=address --linkopt -fsanitize=address --config=dbg --per_file_copt=//tensorflow/core/grappler/./.*\\.cc@-g //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n\r\nand get the similar error.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n```Batch\r\n$env\r\nUSER=user\r\nLOGNAME=user\r\nHOME=/home/user\r\nPATH=/home/user/bin:/usr/local/cuda-11.3/bin:/home/user/anaconda3/envs/tfenv/bin:/home/user/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\nSHELL=/usr/bin/zsh\r\nTERM=xterm-256color\r\nXDG_SESSION_ID=99\r\nXDG_RUNTIME_DIR=/run/user/1000\r\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus\r\nXDG_SESSION_TYPE=tty\r\nXDG_SESSION_CLASS=user\r\nMOTD_SHOWN=pam\r\nLANG=en_US.UTF-8\r\nLC_NUMERIC=zh_CN.UTF-8\r\nLC_TIME=zh_CN.UTF-8\r\nLC_MONETARY=zh_CN.UTF-8\r\nLC_PAPER=zh_CN.UTF-8\r\nLC_NAME=zh_CN.UTF-8\r\nLC_ADDRESS=zh_CN.UTF-8\r\nLC_TELEPHONE=zh_CN.UTF-8\r\nLC_MEASUREMENT=zh_CN.UTF-8\r\nLC_IDENTIFICATION=zh_CN.UTF-8\r\nSSH_CLIENT=10.249.174.114 49269 22\r\nSSH_CONNECTION=10.249.174.114 49269 10.239.44.39 22\r\nSSH_TTY=/dev/pts/3\r\nSHLVL=1\r\nPWD=/home/user/tensorflow\r\nOLDPWD=/home/user\r\nZSH=/home/user/.oh-my-zsh\r\nPAGER=less\r\nLESS=-R\r\nLSCOLORS=Gxfxcxdxbxegedabagacad\r\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\r\nLD_PRELOAD=/home/user/anaconda3/envs/dlrm/lib/libiomp5.so\r\nCONDA_EXE=/home/user/anaconda3/bin/conda\r\n_CE_M=\r\n_CE_CONDA=\r\nCONDA_PYTHON_EXE=/home/user/anaconda3/bin/python\r\nCONDA_SHLVL=2\r\nCONDA_PREFIX=/home/user/anaconda3/envs/tfenv\r\nCONDA_DEFAULT_ENV=tfenv\r\nCONDA_PROMPT_MODIFIER=(tfenv)\r\nLD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64\r\nCONDA_PREFIX_1=/home/user/anaconda3\r\n_=/usr/bin/env\r\n```\r\n", "comments": ["Building with `gcc` will trigger a different error. The third_party/jpeg package could not pass the `asan` check.\r\n\r\nIt gives the error :\r\n\r\n\r\n```\r\nERROR: /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/external/libjpeg_turbo/BUILD.bazel:238:8: Executing genrule @libjpeg_turbo//:simd_x86_64_assemblage23 failed (Exit 1): bash failed: error executing command\r\n  (cd /home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-11.3/lib64 \\\r\n    PATH=/home/user/bin:/usr/local/cuda-11.3/bin:/home/user/anaconda3/envs/tfenv/bin:/home/user/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for out in bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jccolor-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jccolor-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jcgray-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jcgray-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jchuff-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jcphuff-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jcsample-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jcsample-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdcolor-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdmerge-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdsample-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jdsample-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jfdctflt-sse.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jfdctfst-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jfdctint-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jidctflt-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jidctfst-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jidctint-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jidctint-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jidctred-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jquantf-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jquanti-avx2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jquanti-sse2.o bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/simd/x86_64/jsimdcpu.o; do\r\n  bazel-out/k8-dbg/bin/external/nasm/nasm -f elf64    -DELF -DPIC -D__x86_64__    -I $(dirname bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/jconfig.h)/    -I $(dirname bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo/jconfigint.h)/    -I $(dirname external/libjpeg_turbo/simd/nasm/jsimdcfg.inc.h)/    -I $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/    -o $out    $(dirname external/libjpeg_turbo/simd/x86_64/jccolext-sse2.asm)/$(basename ${out%.o}.asm)\r\ndone')\r\nExecution platform: @local_execution_config_platform//:platform\r\n\r\n=================================================================\r\n==479927==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 5888 byte(s) in 32 object(s) allocated from:\r\n    #0 0x7f0adb6bcbc8 in malloc (/lib/x86_64-linux-gnu/libasan.so.5+0x10dbc8)\r\n    #1 0x55d56e9d76d1 in nasm_malloc (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1d96d1)\r\n    #2 0x55d56e9b9c2e in do_directive (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1bbc2e)\r\n    #3 0x55d56e9c5d60 in pp_getline (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1c7d60)\r\n    #4 0x55d56e99d13f in assemble_file.constprop.8 (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x19f13f)\r\n    #5 0x55d56e97b275 in main (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x17d275)\r\n    #6 0x7f0adb2020b2 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x270b2)\r\n\r\nDirect leak of 768 byte(s) in 192 object(s) allocated from:\r\n    #0 0x7f0adb6bcbc8 in malloc (/lib/x86_64-linux-gnu/libasan.so.5+0x10dbc8)\r\n    #1 0x55d56e9d76d1 in nasm_malloc (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1d96d1)\r\n    #2 0x55d56e9ac010 in new_Token.constprop.9 (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1ae010)\r\n    #3 0x55d56e9c706d in pp_getline (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x1c906d)\r\n    #4 0x55d56e99d13f in assemble_file.constprop.8 (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x19f13f)\r\n    #5 0x55d56e97b275 in main (/home/user/.cache/bazel/_bazel_user/7562dd0a46bcb6cada6ae19650738275/execroot/org_tensorflow/bazel-out/k8-dbg/bin/external/nasm/nasm+0x17d275)\r\n    #6 0x7f0adb2020b2 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x270b2)\r\n```", "@Stonepia \r\nCuda 11.3 is not tested configuration, please refer to this page and used [tested build configurations](https://www.tensorflow.org/install/source#gpu) and let us know if you still face any issues.", "@Saduf2019 \r\nWell, I test on CUDA 11.2 and clang (ver. 10.0.0-4ubuntu1), this problem still happens, and the error message is identical.\r\n\r\nIt does not work on gcc as well.\r\n", "Currently we don't officially support an OSS ASAN build, although one is in the long term roadmap.\r\n\r\nIf you want to submit PRs for issues you discover or PRs to enable the ASAN build, please add me to them and I will gladly review and get them in. Thank you in advance for future work in this area.", "Thank you very much for your reply. Could I ask if there is anyway of bypassing the ASAN check for third party build? For tensorflow's building process, I could add the `tags = [\"noasan\"]` in BUILD file to bypass specific project.\r\n\r\nHowever, according to my logs, this happens in third_party library, i.e.  `libjpeg` , for which file should I add this noasan tag?\r\n", "The tag only works for tests and only in projects that do `bazel test --test_filter=-noasan ...`. It does not work by default and for any target.", "Thanks for the reply. Got this.  I'll close this issue for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50892\">No</a>\n"]}, {"number": 50890, "title": "converting tensorflow model to mlir", "body": "I saved tf model using model.save().Then I tried to convert it to mlir using tensorflow/bazel-bin/tensorflow/compiler/mlir/tf-mlir-translate --graphdef-to-mlir /data/aruna/tf_models/saved_model.pb -o /data/aruna/tf_models/convolution.mlir  but I am getting error as  tensorflow/compiler/mlir/tensorflow/translate/tf_mlir_translate.cc:163] SavedModel import failed: Failed precondition: Could not restore saved variable: Adam/conv2d/kernel/m\r\nAm I giving correct flag to convert tf to mlir?", "comments": ["@ArunaKote ,\r\n\r\nPlease  take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/31501) with similar error.Also  this is not a TensorFlow bug or feature request, it is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) or tensorflow discussion [forum](https://discuss.tensorflow.org/). There is a larger community that reads questions there. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50889, "title": "48584629.0\u8fd9\u4e2a\u6d6e\u70b9\u6570\u6570\u636e\u7c7b\u578b\u8f6c\u6362\u7cbe\u5ea6\u4e22\u5931", "body": "tf.cast(48584629.0,tf.float64)  \u8f6c\u6362\u540e\u53d8\u621048584628", "comments": ["@HelloWorld19930113 ,\r\n\r\nLooks like this is duplicate of issue #50888.Can you please close this issue, since it is already being tracked there? Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50889\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50889\">No</a>\n"]}, {"number": 50888, "title": "tf.cast(48584629,tf.float32) \u7cbe\u5ea6\u4e22\u5931", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@HelloWorld19930113 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code, dataset  and tensorflow version to reproduce the issue reported here.", "@HelloWorld19930113 ,\r\n\r\nI was able to execute the mentioned code without any error.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/aa1f3b8b48c220bb1b66a11c2d9145fe/untitled350888.ipynb).Also please go take a look at [link](https://www.tensorflow.org/api_docs/python/tf/cast) for more information on tf.cast.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50888\">No</a>\n"]}, {"number": 50886, "title": "[Question] TfLiteTensor is supposed to be deleted/released with TfLiteTensorFree?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version (use command below): master branch\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**TfLiteTensor is supposed to be deleted/released with TfLiteTensorFree?**\r\nI found the function TfLiteTensorFree and TfLiteTensorReset in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h#L608 ,  however, it is not clarified in the documentation and not seen in examples that TfLiteTensor(s) are supposed to be freed.\r\n\r\n**Describe the expected behavior**\r\nClarify whether TfLiteTensor(s) are supposed to be freed.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi Carlos,\r\n\r\nNormally the TfLite interpreter would take care of tensor's lifecycle. But if you created the tensor yourself (instead of getting the tensor from the interpreter), you can use TfLiteTensorFree to free the field inside TFLite tensor.\r\n\r\nThanks,\r\nTiezhen", "Hi Tiezhen @wangtz,\r\n\r\nthanks for your reply. \r\n\r\nI am using the following arrays of pointers within an Obj structure to pass the input tensor and get the output tensors\r\n```\r\nconst TfLiteTensor* tfliteInputTensors[1];\r\nconst TfLiteTensor* tfliteOutputTensors[4];\r\n```\r\nThen, I get the tensors from the model using the corresponding TfLiteInterpreterGetInputTensor and TfLiteInterpreterGetOutputTensor APIs\r\n```\r\npObj->tfliteInputTensors[0] = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n\r\npObj->tfliteOutputTensors[0] = TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\npObj->tfliteOutputTensors[1] = TfLiteInterpreterGetOutputTensor(interpreter, 1);\r\npObj->tfliteOutputTensors[2] = TfLiteInterpreterGetOutputTensor(interpreter, 2);\r\npObj->tfliteOutputTensors[3] = TfLiteInterpreterGetOutputTensor(interpreter, 3);\r\n\r\n```\r\nSince they are arrays of pointers and you mentioned that the TfLite interpreter would take care of the tensor's lifecycle, then there is no need to free the arrays if the interpreter is deleted with the TfLiteInterpreterDelete API, right?", "Yes. Since these tensors were created and managed by the interpreter. You don't need to do deallocate them manually. Also you might want to set pObj->tfliteInputTensors[0] = nullptr after the interpreter is deallocated. Otherwise they may point to invalid memory addresses.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50886\">No</a>\n"]}, {"number": 50884, "title": "[TF-TRT] Refactoring Top Level __init__ file for TF-TRT", "body": "- Builds on #50782\r\n\r\nApplies final cleaning and refactoring on TRT version checking\r\n\r\n@bixia1 : for review", "comments": ["@bixia1 the refactoring commit is the following: 5960df254d80b98f9be99789b4fe0ac92cb73048", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan  Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "Please do not close", "@bixia1 all comments have been addressed. Please review", "Please update the PR description, and describe that this PR does.", "@bixia1 I had to add this line: https://github.com/tensorflow/tensorflow/pull/50884/files#diff-0de5f087b61e9a3572b30a6a3ac6f7cbc88f96de81c733334784077646b9f198R51\r\n\r\nOtherwise CPU builds would fail. Please re-approve", "@DEKHTIARJonathan Can you please check @bixia1's comments and keep us posted ? Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!", "@DEKHTIARJonathan Any update on this PR? Please. Thanks!"]}, {"number": 50883, "title": "tf.math.sign() gives inconsistent output for the same input values for small complex numbers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: X\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source): X\r\n- GCC/Compiler version (if compiling from source): X\r\n- CUDA/cuDNN version: X\r\n- GPU model and memory: X\r\n\r\n**Describe the current behavior**\r\nThe function `tf.math.sign()` gives inconsistent results for `tf.complex64` values with small magnitudes.  When passing `tf.math.sign()` two slices of the same Tensor, one of which wholly includes the elements of the other, the same numerical inputs result in different values.\r\n\r\nReferencing the code example below:\r\ntf.math.sign(z[23:26]) -> [1.+0.j, 1.+0.j, 1.+0.j]\r\ntf.math.sign(z[23:27]) -> [nan+nanj, nan+nanj, nan+nanj, nan+nanj]\r\n\r\nI've also demonstrated this behavior when using the docker image `tensorflow/tensorflow:2.5.0`\r\n\r\n**Describe the expected behavior**\r\nReferencing the code example below:\r\ntf.math.sign(z[23:26]) -> [1.+0.j, 1.+0.j, 1.+0.j]\r\ntf.math.sign(z[23:27]) -> [1.+0.j, 1.+0.j, 1.+0.j, 1.+0.j (or some other value)]\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nx = tf.constant([10 ** -n for n in range(47)], dtype=tf.float32)\r\nz = tf.complex(x, 0.0)\r\n\r\nq0 = tf.math.sign(z[23:26])  # result: [1.+0.j, 1.+0.j, 1.+0.j]\r\nq1 = tf.math.sign(z[23:27])  # result: [nan+nanj, nan+nanj, nan+nanj, nan+nanj]\r\n```", "comments": ["@pawlick2  I can able to replicate the issue in TF 2.5 version but in TF -Nightly 2.7 I am getting different output instead of nan. Please check it once.\r\n\r\n", "Yes, looks like this doesn't happen when using 2.7 nightly.", "@pawlick2 Seems the issue is resolved in TF nightly version. Please feel free to re-open the issue if needed . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50883\">No</a>\n"]}]