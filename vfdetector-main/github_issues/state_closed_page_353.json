[{"number": 43492, "title": "failed to build pip windows 10", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source(master)\r\n- TensorFlow version: source(master)\r\n- Python version: 3.8.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.1\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: 1060super\r\n\r\n```\r\n ERROR: G:/tensorflow/tensorflow/core/grappler/BUILD:168:1: C++ compilation of rule '//tensorflow/core/grappler:grappler_item_builder' failed (Exit 2): python.exe failed: error executing command\r\n  cd F:/users/sonfire/_bazel_sonfire/ic7qrvhc/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LD_LIBRARY_PATH=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.0lib\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe\r\n    SET PYTHON_LIB_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=F:\\Users\\sonfire\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=F:\\Users\\sonfire\\AppData\\Local\\Temp\r\n```\r\n\r\n\r\n`Target //tensorflow/tools/pip_package:build_pip_package failed to build`\r\n", "comments": ["Are you using python 64 bit?", "@bhack yes", "Are you using the official guide https://www.tensorflow.org/install/source_windows?", "@bhack yes", "Error for `bazel build //tensorflow/tools/pip_package:build_pip_package`", "So do you have executed all the other steps before `bazel build`? \r\nAre you using the correct Bazel version https://www.tensorflow.org/install/source_windows#tested_build_configurations?", "Yes, Im used Bazel 3.1.0\r\n![image](https://user-images.githubusercontent.com/26105224/94038454-e33a3b00-fddf-11ea-9d13-aeea5fa23d88.png)\r\n\r\n", "@sonfire86,\r\nCould you please provide the complete error log and mention the exact sequence of commands / steps that you executed before running into the problem? Thanks!", "```\r\nG:\\tensorflow>bazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from g:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe\r\nINFO: Reading rc options for 'build' from g:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from g:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe --action_env PYTHON_LIB_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/lib/site-packages --python_path=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.0lib --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file g:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file g:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file g:\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file g:\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file g:\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file g:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file g:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/a90d72127a814ea242227456be711aca07cc83a6.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (401 packages loaded, 32080 targets configured). INFO: Found 1 target...\r\nINFO: Deleting stale sandbox base F:/users/sonfire/_bazel_sonfire/ic7qrvhc/sandbox\r\nERROR: G:/tensorflow/tensorflow/python/autograph/impl/testing/BUILD:8:1: Linking of rule '//tensorflow/python/autograph/impl/testing:pybind_for_testing.so' failed (Exit 1120): link.exe failed: error executing command\r\n  cd F:/users/sonfire/_bazel_sonfire/ic7qrvhc/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LD_LIBRARY_PATH=C:Program FilesNVIDIA GPU Computing ToolkitCUDAv11.0lib\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe\r\n    SET PYTHON_LIB_PATH=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=F:\\Users\\sonfire\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=F:\\Users\\sonfire\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Enterprise/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/python/autograph/impl/testing/pybind_for_testing.so-2.params\r\nExecution platform: @local_execution_config_platform//:platform\r\n   \u0421\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430 bazel-out/x64_windows-opt/bin/tensorflow/python/autograph/impl/testing/pybind_for_testing.so.if.lib \u0438 \u043e\u0431\u044a\u0435\u043a\u0442 bazel-out/x64_windows-opt/bin/tensorflow/python/autograph/impl/testing/pybind_for_testing.so.if.exp\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyMem_Free \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::detail::instance::deallocate_layout(void)\" (?deallocate_layout@instance@detail@pybind11@@QEAAXXZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyMem_Calloc \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::detail::instance::allocate_layout(void)\" (?allocate_layout@instance@detail@pybind11@@QEAAXXZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyType_IsSubtype \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: bool __cdecl pybind11::detail::type_caster_generic::load_impl<class pybind11::detail::type_caster_generic>(class pybind11::handle,bool)\" (??$load_impl@Vtype_caster_generic@detail@pybind11@@@type_caster_generic@detail@pybind11@@QEAA_NVhandle@2@_N@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyType_Ready \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_Repr \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class pybind11::str __cdecl pybind11::repr(class pybind11::handle)\" (?repr@pybind11@@YA?AVstr@1@Vhandle@1@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_Str \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: class pybind11::str __cdecl pybind11::str::format<class pybind11::handle &,class pybind11::handle &>(class pybind11::handle &,class pybind11::handle &)const \" (??$format@AEAVhandle@pybind11@@AEAV12@@str@pybind11@@QEBA?AV01@AEAVhandle@1@0@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_GetAttrString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: class pybind11::class_<class autograph::TestClassDef> & __cdecl pybind11::class_<class autograph::TestClassDef>::def<class pybind11::object (__cdecl autograph::TestClassDef::*)(void)const >(char const *,class pybind11::object (__cdecl autograph::TestClassDef::*&&)(void)const )\" (??$def@P8TestClassDef@autograph@@EBA?AVobject@pybind11@@XZ$$V@?$class_@VTestClassDef@autograph@@$$V@pybind11@@QEAAAEAV01@PEBD$$QEAP8TestClassDef@autograph@@EBA?AVobject@1@XZ@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_SetAttrString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_HasAttrString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"bool __cdecl pybind11::hasattr(class pybind11::handle,char const *)\" (?hasattr@pybind11@@YA_NVhandle@1@PEBD@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_SetAttr \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::detail::accessor<struct pybind11::detail::accessor_policies::obj_attr>::operator=<class pybind11::cpp_function &>(class pybind11::cpp_function &)&& \" (??$?4AEAVcpp_function@pybind11@@@?$accessor@Uobj_attr@accessor_policies@detail@pybind11@@@detail@pybind11@@QEHAAXAEAVcpp_function@2@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_ClearWeakRefs \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"void __cdecl pybind11::detail::clear_instance(struct _object *)\" (?clear_instance@detail@pybind11@@YAXPEAU_object@@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__Py_Dealloc \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::class_<class autograph::TestClassDef>::class_<class autograph::TestClassDef><>(class pybind11::handle,char const *)\" (??$?0$$V@?$class_@VTestClassDef@autograph@@$$V@pybind11@@QEAA@Vhandle@1@PEBD@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__PyType_Lookup \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 pybind11_meta_getattro.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__PyObject_GetDictPtr \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"void __cdecl pybind11::detail::clear_instance(struct _object *)\" (?clear_instance@detail@pybind11@@YAXPEAU_object@@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_Malloc \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct _object * __cdecl pybind11::detail::make_new_python_type(struct pybind11::detail::type_record const &)\" (?make_new_python_type@detail@pybind11@@YAPEAU_object@@AEBUtype_record@12@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyBytes_Size \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"private: bool __cdecl pybind11::detail::string_caster<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,0>::load_bytes<char>(class pybind11::handle)\" (??$load_bytes@D@?$string_caster@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@$0A@@detail@pybind11@@AEAA_NVhandle@2@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyBytes_AsString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"private: bool __cdecl pybind11::detail::string_caster<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,0>::load_bytes<char>(class pybind11::handle)\" (??$load_bytes@D@?$string_caster@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@$0A@@detail@pybind11@@AEAA_NVhandle@2@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyBytes_AsStringAndSize \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::str::operator class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >(void)const \" (??Bstr@pybind11@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyUnicode_FromString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::str::str(char const *)\" (??0str@pybind11@@QEAA@PEBD@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyUnicode_FromFormat \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct _object * __cdecl pybind11::detail::make_new_python_type(struct pybind11::detail::type_record const &)\" (?make_new_python_type@detail@pybind11@@YAPEAU_object@@AEBUtype_record@12@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyUnicode_AsEncodedString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: bool __cdecl pybind11::detail::string_caster<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,0>::load(class pybind11::handle,bool)\" (?load@?$string_caster@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@$0A@@detail@pybind11@@QEAA_NVhandle@3@_N@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyUnicode_DecodeUTF8 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class pybind11::tuple __cdecl pybind11::make_tuple<1,char const * const &>(char const * const &)\" (??$make_tuple@$00AEBQEBD@pybind11@@YA?AVtuple@0@AEBQEBD@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyUnicode_AsUTF8String \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::str::operator class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >(void)const \" (??Bstr@pybind11@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyTuple_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::detail::simple_collector<1>::simple_collector<1><>(void)\" (??$?0$$V@?$simple_collector@$00@detail@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyTuple_Size \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyTuple_GetItem \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: static class pybind11::object __cdecl pybind11::detail::accessor_policies::tuple_item::get(class pybind11::handle,unsigned __int64)\" (?get@tuple_item@accessor_policies@detail@pybind11@@SA?AVobject@4@Vhandle@4@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyTuple_SetItem \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::detail::accessor<struct pybind11::detail::accessor_policies::tuple_item>::operator=<struct _object * &>(struct _object * &)&& \" (??$?4AEAPEAU_object@@@?$accessor@Utuple_item@accessor_policies@detail@pybind11@@@detail@pybind11@@QEHAAXAEAPEAU_object@@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyList_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::list::list(unsigned __int64)\" (??0list@pybind11@@QEAA@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyList_Size \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: void __cdecl pybind11::detail::generic_type::initialize(struct pybind11::detail::type_record const &)\" (?initialize@generic_type@detail@pybind11@@IEAAXAEBUtype_record@23@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyList_GetItem \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: static class pybind11::object __cdecl pybind11::detail::accessor_policies::list_item::get(class pybind11::handle,unsigned __int64)\" (?get@list_item@accessor_policies@detail@pybind11@@SA?AVobject@4@Vhandle@4@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyList_Append \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: static void __cdecl pybind11::detail::loader_life_support::add_patient(class pybind11::handle)\" (?add_patient@loader_life_support@detail@pybind11@@SAXVhandle@3@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::dict::dict(void)\" (??0dict@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_Next \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::detail::generic_iterator<class pybind11::detail::iterator_policies::dict_readonly>::generic_iterator<class pybind11::detail::iterator_policies::dict_readonly>(class pybind11::handle,__int64)\" (??0?$generic_iterator@Vdict_readonly@iterator_policies@detail@pybind11@@@detail@pybind11@@QEAA@Vhandle@2@_J@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_Size \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_Copy \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_GetItemString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyDict_DelItemString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCFunction_NewEx \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: void __cdecl pybind11::cpp_function::initialize_generic(struct pybind11::detail::function_record *,char const *,class type_info const * const *,unsigned __int64)\" (?initialize_generic@cpp_function@pybind11@@IEAAXPEAUfunction_record@detail@2@PEBDPEBQEBVtype_info@@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyInstanceMethod_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: void __cdecl pybind11::cpp_function::initialize_generic(struct pybind11::detail::function_record *,char const *,class type_info const * const *,unsigned __int64)\" (?initialize_generic@cpp_function@pybind11@@IEAAXPEAUfunction_record@detail@2@PEBDPEBQEBVtype_info@@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::capsule::capsule(void const *,void (__cdecl*)(void *))\" (??0capsule@pybind11@@QEAA@PEBXP6AXPEAX@Z@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_GetPointer \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"private: static void __cdecl <lambda_57bff18ff75e63c44eefc978be789c02>::<lambda_invoker_cdecl>(struct _object *)\" (?<lambda_invoker_cdecl>@<lambda_57bff18ff75e63c44eefc978be789c02>@@CAXPEAU_object@@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_GetName \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::capsule::operator<struct pybind11::detail::internals *> struct pybind11::detail::internals * *(void)const \" (??$?BPEAUinternals@detail@pybind11@@@capsule@pybind11@@QEBAPEAPEAUinternals@detail@1@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_GetContext \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"private: static void __cdecl <lambda_57bff18ff75e63c44eefc978be789c02>::<lambda_invoker_cdecl>(struct _object *)\" (?<lambda_invoker_cdecl>@<lambda_57bff18ff75e63c44eefc978be789c02>@@CAXPEAU_object@@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_SetContext \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::capsule::capsule(void const *,void (__cdecl*)(void *))\" (??0capsule@pybind11@@QEAA@PEBXP6AXPEAX@Z@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThread_tss_alloc \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThread_tss_create \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThread_tss_set \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThread_tss_get \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThreadState_New \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThreadState_Clear \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::gil_scoped_acquire::dec_ref(void)\" (?dec_ref@gil_scoped_acquire@pybind11@@QEAAXXZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThreadState_DeleteCurrent \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::gil_scoped_acquire::dec_ref(void)\" (?dec_ref@gil_scoped_acquire@pybind11@@QEAAXXZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyThreadState_Get \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyGILState_Ensure \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl `struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)'::`2'::gil_scoped_acquire_local::gil_scoped_acquire_local(void)\" (??0gil_scoped_acquire_local@?1??get_internals@detail@pybind11@@YAAEAUinternals@23@XZ@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyGILState_Release \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl `struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)'::`2'::gil_scoped_acquire_local::~gil_scoped_acquire_local(void)\" (??1gil_scoped_acquire_local@?1??get_internals@detail@pybind11@@YAAEAUinternals@23@XZ@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyGILState_GetThisThreadState \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__PyThreadState_UncheckedGet \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyWeakref_NewRef \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::weakref::weakref(class pybind11::handle,class pybind11::handle)\" (??0weakref@pybind11@@QEAA@Vhandle@1@0@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_SetString \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_Occurred \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl pybind11::detail::error_string(void)\" (?error_string@detail@pybind11@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_Clear \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: class pybind11::class_<class autograph::TestClassDef> & __cdecl pybind11::class_<class autograph::TestClassDef>::def<class pybind11::object (__cdecl autograph::TestClassDef::*)(void)const >(char const *,class pybind11::object (__cdecl autograph::TestClassDef::*&&)(void)const )\" (??$def@P8TestClassDef@autograph@@EBA?AVobject@pybind11@@XZ$$V@?$class_@VTestClassDef@autograph@@$$V@pybind11@@QEAAAEAV01@PEBD$$QEAP8TestClassDef@autograph@@EBA?AVobject@1@XZ@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_Fetch \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::error_already_set::error_already_set(void)\" (??0error_already_set@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_Restore \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: virtual __cdecl pybind11::error_already_set::~error_already_set(void)\" (??1error_already_set@pybind11@@UEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_NormalizeException \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl pybind11::detail::error_string(void)\" (?error_string@detail@pybind11@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyException_SetTraceback \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl pybind11::detail::error_string(void)\" (?error_string@detail@pybind11@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyErr_Format \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 PyInit_pybind_for_testing.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyModule_Create2 \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::module::module(char const *,char const *)\" (??0module@pybind11@@QEAA@PEBD0@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_Py_GetVersion \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 PyInit_pybind_for_testing.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyEval_GetBuiltins \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyEval_SaveThread \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::~gil_scoped_acquire(void)\" (??1gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyEval_InitThreads \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyEval_AcquireThread \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::gil_scoped_acquire::gil_scoped_acquire(void)\" (??0gil_scoped_acquire@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_CallObject \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: class pybind11::object __cdecl pybind11::detail::object_api<class pybind11::detail::accessor<struct pybind11::detail::accessor_policies::str_attr> >::operator()<1>(void)const \" (??$?R$00$$V@?$object_api@V?$accessor@Ustr_attr@accessor_policies@detail@pybind11@@@detail@pybind11@@@detail@pybind11@@QEBA?AVobject@2@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_GetItem \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: static class pybind11::object __cdecl pybind11::detail::accessor_policies::generic_item::get(class pybind11::handle,class pybind11::handle)\" (?get@generic_item@accessor_policies@detail@pybind11@@SA?AVobject@4@Vhandle@4@0@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_SetItem \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: void __cdecl pybind11::detail::accessor<struct pybind11::detail::accessor_policies::generic_item>::operator=<class pybind11::capsule>(class pybind11::capsule &&)&& \" (??$?4Vcapsule@pybind11@@@?$accessor@Ugeneric_item@accessor_policies@detail@pybind11@@@detail@pybind11@@QEHAAX$$QEAVcapsule@2@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PySequence_Tuple \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::tuple::tuple(class pybind11::object const &)\" (??0tuple@pybind11@@QEAA@AEBVobject@1@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyObject_IsInstance \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 pybind11_meta_setattro.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyBuffer_Release \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: __cdecl pybind11::buffer_info::~buffer_info(void)\" (??1buffer_info@pybind11@@QEAA@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyFrame_GetLineNumber \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl pybind11::detail::error_string(void)\" (?error_string@detail@pybind11@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyType_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyBaseObject_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__Py_NoneStruct \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"public: class pybind11::class_<class autograph::TestClassDef> & __cdecl pybind11::class_<class autograph::TestClassDef>::def<class pybind11::object (__cdecl autograph::TestClassDef::*)(void)const >(char const *,class pybind11::object (__cdecl autograph::TestClassDef::*&&)(void)const )\" (??$def@P8TestClassDef@autograph@@EBA?AVobject@pybind11@@XZ$$V@?$class_@VTestClassDef@autograph@@$$V@pybind11@@QEAAAEAV01@PEBD$$QEAP8TestClassDef@autograph@@EBA?AVobject@1@XZ@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__Py_NotImplementedStruct \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__Py_FalseStruct \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class pybind11::detail::type_caster<bool,void> __cdecl pybind11::detail::load_type<bool>(class pybind11::handle const &)\" (??$load_type@_N@detail@pybind11@@YA?AV?$type_caster@_NX@01@AEBVhandle@1@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp__Py_TrueStruct \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class pybind11::detail::type_caster<bool,void> __cdecl pybind11::detail::load_type<bool>(class pybind11::handle const &)\" (??$load_type@_N@detail@pybind11@@YA?AV?$type_caster@_NX@01@AEBVhandle@1@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCFunction_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: void __cdecl pybind11::cpp_function::initialize_generic(struct pybind11::detail::function_record *,char const *,class type_info const * const *,unsigned __int64)\" (?initialize_generic@cpp_function@pybind11@@IEAAXPEAUfunction_record@detail@2@PEBDPEBQEBVtype_info@@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyModule_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct _object * __cdecl pybind11::detail::make_new_python_type(struct pybind11::detail::type_record const &)\" (?make_new_python_type@detail@pybind11@@YAPEAU_object@@AEBUtype_record@12@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyInstanceMethod_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: void __cdecl pybind11::cpp_function::initialize_generic(struct pybind11::detail::function_record *,char const *,class type_info const * const *,unsigned __int64)\" (?initialize_generic@cpp_function@pybind11@@IEAAXPEAUfunction_record@detail@2@PEBDPEBQEBVtype_info@@_K@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyCapsule_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"bool __cdecl pybind11::isinstance<class pybind11::capsule,0>(class pybind11::handle)\" (??$isinstance@Vcapsule@pybind11@@$0A@@pybind11@@YA_NVhandle@0@@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyProperty_Type \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"struct pybind11::detail::internals & __cdecl pybind11::detail::get_internals(void)\" (?get_internals@detail@pybind11@@YAAEAUinternals@12@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_BufferError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 pybind11_getbuffer.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_ImportError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 PyInit_pybind_for_testing.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_IndexError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 __catch$?translate_exception@detail@pybind11@@YAXVexception_ptr@std@@@Z$6.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_MemoryError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 __catch$?translate_exception@detail@pybind11@@YAXVexception_ptr@std@@@Z$2.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_RuntimeError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > __cdecl pybind11::detail::error_string(void)\" (?error_string@detail@pybind11@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_SystemError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 __catch$?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z$5.\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_TypeError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \"protected: static struct _object * __cdecl pybind11::cpp_function::dispatcher(struct _object *,struct _object *,struct _object *)\" (?dispatcher@cpp_function@pybind11@@KAPEAU_object@@PEAU3@00@Z).\r\npybind_for_testing.obj : error LNK2019: \u0441\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0439 \u0432\u043d\u0435\u0448\u043d\u0438\u0439 \u0441\u0438\u043c\u0432\u043e\u043b __imp_PyExc_ValueError \u0432 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 __catch$?translate_exception@detail@pybind11@@YAXVexception_ptr@std@@@Z$7.\r\nbazel-out\\x64_windows-opt\\bin\\external\\local_config_python\\python38.lib : warning LNK4272: \u0442\u0438\u043f \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0430 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \"x86\" \u043a\u043e\u043d\u0444\u043b\u0438\u043a\u0442\u0443\u0435\u0442 \u0441 \u0442\u0438\u043f\u043e\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0433\u043e \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0430 \"x64\"\r\nbazel-out\\x64_windows-opt\\bin\\tensorflow\\python\\autograph\\impl\\testing\\pybind_for_testing.so : fatal error LNK1120: \u043d\u0435\u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u043d\u044b\u0445 \u0432\u043d\u0435\u0448\u043d\u0438\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432: 97\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1301.731s, Critical Path: 113.53s\r\nINFO: 146 processes: 146 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "You need to use python 64bit as it is requested in the official installation guide.\r\n\r\n", "@bhack Yes. Im used Python Windows x86-64 ", "No you are using 32bit:\n\npython_path=F:/Users/sonfire/AppData/Local/Programs/Python/Python38-32/python.exe ", "@bhack upss... Ok. I reinstall python and try again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43492\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43492\">No</a>\n"]}, {"number": 43491, "title": "tf.dynamic_stitch gives wrong shape and outputs raw memory", "body": "**System information**\r\n- Linux Ubuntu 20.04\r\n- TensorFlow 2.3.0 installed from binary\r\n- Python version 3.8.2 (default, Jul 16 2020, 14:00:26) [GCC 9.3.0]\r\n- no GPU available\r\n\r\n**Describe the current behavior**\r\nWhen stitching empty partitions, the output shape is wrong.\r\n```python\r\n>>> tf.dynamic_stitch([1], [1.2])\r\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.4e-44, 1.2e+00], dtype=float32)>\r\n```\r\nThe `2.4e-44` is from uninitialized memory and will typically vary per execution. There is nothing special about 1.2\r\n\r\n**Describe the expected behavior**\r\nFrom documentation: `merged[indices[m], ...] = data[m][...]` so we would expect:\r\n```python\r\n>>> tf.dynamic_stitch([1], [1.2])\r\n<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.2], dtype=float32)>\r\n```\r\n\r\n**Extra**\r\nI don't know if it's a security concern, but the amount of data read can be arbitrarily large:\r\n```python\r\nsize = 8\r\n>>> tf.dynamic_stitch([1], [tf.zeros([1, size])])[0]\r\n<tf.Tensor: shape=(8,), dtype=float32, numpy=\r\narray([-2.8330522e-19,  4.5886920e-41, -2.8330522e-19,  4.5886920e-41,\r\n        6.6383240e-07,  1.6802996e-04,  1.7299214e-04,  4.3915941e-05],\r\n      dtype=float32)>\r\n```\r\nEdit: simplified the code", "comments": ["@shlapfish \r\nI ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fc1c1931e6dfb74a5f49117b369b89d4/untitled418.ipynb), and let us know if it confirms your issue.", "> @shlapfish\r\n> I ran the code shared, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fc1c1931e6dfb74a5f49117b369b89d4/untitled418.ipynb), and let us know if it confirms your issue.\r\n\r\nThat does confirm my issue.", "It took me a while to figure out a workaround. If it's any help:\r\n```python\r\nfrom math import prod\r\n\r\ndef my_stitch(indices, data):\r\n    shapes = [tf.shape(idx) for idx in indices]\r\n    tmp = tf.concat([tf.reshape(idx, [-1]) for idx in indices], axis=0)\r\n    args = tf.argsort(tmp, direction='ASCENDING')\r\n    tmp = tf.scatter_nd(tf.expand_dims(args, axis=-1), tf.range(tmp.shape[0]), shape=[tmp.shape[0]])\r\n    \r\n    indices = tf.split(tmp, [prod(s) for s in shapes])\r\n    indices = [tf.reshape(idx, s) for idx, s in zip(indices, shapes)]\r\n    return tf.dynamic_stitch(indices, data)\r\n```", "I'm so sorry. I misread the documentation :facepalm: ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43491\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43491\">No</a>\n"]}, {"number": 43489, "title": "map_and_batch() is way faster than map().batch() ???", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu 2.2.0\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n- CUDA/cuDNN version:  CUDA Version: 10.1 , libcudnn7-dev 7.6.5.32-1\r\n- GPU model and memory: NVIDIA Tesla V100, Memory 32480MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen training ResNet-50 with ImageNet (in TFRecord), the time per epoch with \"map_and_batch()\" is 3 times faster than using \".map().batch()\".\r\nI know \"map_and_batch()\" will be depracated (https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch?hl=en). \r\nBut if this is faster and working just fine, then why ??\r\n\r\n**Describe the expected behavior**\r\nAccoridng to some previous posts, these two API should have similar performance.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nOn the same machine, when i use \r\ndataset = dataset.apply(tf.data.experimental.map_and_batch(\r\n    lambda value: parse_record_fn(value, is_training, dtype),\r\n    batch_size=batch_size))\r\nthe results are:\r\nEpoch 1/30\r\n2859/2859 [==============================] - 2638s 923ms/step - accuracy: 0.5294 - loss: 2.9772 - val_accuracy: 0.6064 - lr: 0.1000 - val_loss: 2.6385\r\nEpoch 2/30\r\n1700/2859 [================>.............] - ETA: 17:26 - accuracy: 0.6093 - loss: 2.6254\r\n\r\nHowever, when we use\r\ndataset = dataset.map(lambda value: parse_record_fn(value, is_training, dtype))\r\ndataset = dataset.batch(batch_size=batch_size)\r\nthe results are:\r\nEpoch 1/30\r\n2859/2859 [==============================] - 6732s 2s/step - loss: 2.9723 - accuracy: 0.5304 - lr: 0.1000 - val_loss: 2.6656 - val_accuracy: 0.5948\r\nEpoch 2/30\r\n2110/2859 [=====================>........] - ETA: 27:56 - loss: 2.6110 - accuracy: 0.6128", "comments": ["Take a look at https://github.com/tensorflow/tensorflow/issues/20059", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43488, "title": "[tflite] fix missing labels.txt in model maker tutorial", "body": "fix the problem of missing `labels.txt` in #43463 and #41470", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/43488\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n See visual diffs & provide feedback on Jupyter Notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com/?utm_source=gh'>ReviewNB</a></i>", "Please hold on this PR. We hope to make with_metadata=True in the colab. Will update the colab description instead.", "@ziyeqinghan  Any update on this PR? Please. Thanks!", "@ziyeqinghan Any update on this PR? Please. Thanks!", "We have updated the colab to describe with_metadata more clear."]}, {"number": 43486, "title": "CMSIS-NN: Dynamic allocation of conv per-channel quant params", "body": "Fixes #42748 for convolution\r\n\r\nFor models with channels greater than 256, out of bounds memory\r\nwrite happens resulting in undefined behaviour. This PR addresses\r\nthis issue by dynamically allocating the per-channel params.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "https://github.com/tensorflow/tensorflow/issues/42883 is the ticket for the issue on kernel_utils.cc\r\n"]}, {"number": 43485, "title": "CMSIS-NN: Partial fix for failing conv unit test cases", "body": "Fixes #43475 when combined with PR that fixes #42951\r\n\r\nFix #42951 ensures that the reference implementation is invoked\r\nwhen dilation is not equal to 1 as that case does not have an\r\noptimized implementation.\r\n\r\nThis PR addresses the issue that zero point arguments are\r\nuninitialized when invoking the reference implementation of\r\nconvolution resulting in failing test cases.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 43484, "title": "CMSIS-NN: Fix incorrect flags for non-DSP/MVE processors", "body": "Fixes #42951\r\n\r\nCalculateOpData() was not invoked for target Arm Cortex-M\r\nprocessors without DSP or MVE extension like Cortex-M3 or Cortex-M0.\r\n\r\nThe fix is to remove the usage of feature flags __ARM_FEATURE_DSP\r\nand __ARM_FEATURE_MVE and replace them with an appropriate\r\n'if' condition where applicable. It is also part of the general\r\nwork to move the processor feature flags completely to CMSIS-NN.\r\n\r\nTwo reinterpret_casts are replaced with static casts along the\r\narea where the feature flag change is done.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain It was a merge commit and not a rebase that was done. But, for some reason, I couldn't push it and got a reject from the server. Hence the force push. I guess, at the end of the day it might as well as have been a rebase because the history is lost because of the force push. Will contact the team next time I encounter this for a better resolution. "]}, {"number": 43482, "title": "Add transform to fuse activations into TFL pooling ops", "body": "TFLite average and max pool operations support fused activation functions similar to the convolution ops. This PR adds a MLIR transform to the TFLite converter to fuse activations functions into preceding pooling operations.", "comments": ["@joker-eph @abattery do you mind giving this PR a quick review?", "@abattery Thanks for approving the PR!\r\n\r\nWould you also be able to take a look at #41790 which is a crutial bug fix for #39572 and touches similar files?"]}, {"number": 43481, "title": "Uplampling2D args:interpolation='nearest' isn't supported in TFLite or TFLite converter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):2.2.0\r\n\r\nTensorflow installed using pip3 insall tensorflow==2.2.0\r\n$pip3 show tensorflow\r\n```\r\nName: tensorflow\r\nVersion: 2.2.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/nymble/.local/lib/python3.6/site-packages\r\nRequires: wrapt, google-pasta, gast, protobuf, six, opt-einsum, absl-py, astunparse, tensorboard, scipy, keras-preprocessing, grpcio, wheel, tensorflow-estimator, numpy, termcolor, h5py\r\n\r\n```\r\nModel code :\r\n```\r\nimport tensorflow as tf\r\nfrom  tensorflow.keras import models\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.optimizers import SGD, Adam\r\n#-----------------------------------------------------##\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(224, 224, 3)))\r\n\r\nmodel.add(layers.Conv2D(32, (3, 3), padding='same'))\r\n#model.add(layers.BatchNormalization())\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\n\r\nmodel.add(layers.UpSampling2D((2, 2), interpolation='nearest'))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(1, activation='sigmoid'))\r\n\r\nmodel.summary()\r\n\r\n```\r\nModel's summary looks like this: \r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 224, 224, 32)      896       \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 224, 224, 32)      9248      \r\n_________________________________________________________________\r\nactivation (Activation)      (None, 224, 224, 32)      0         \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \r\n_________________________________________________________________\r\nup_sampling2d (UpSampling2D) (None, 224, 224, 32)      0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 1605632)           0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 1605633   \r\n=================================================================\r\nTotal params: 1,615,777\r\nTrainable params: 1,615,777\r\nNon-trainable params: 0\r\n__________________________\r\n```\r\n\r\nTrained this model and saved it as sample_model.h5. \r\nLoaded this model again in a separate code using:\r\n```\r\n\r\nmodel_dir = '/home/nymble'\r\nnymble_model = models.load_model(os.path.join(model_dir,'sample_model.h5'))\r\nprint(\"Model loaded\")\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(nymble_model)\r\n#converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflmodel = converter.convert()\r\n\r\n```\r\nRunning this code gives error at 6th line : \r\n```\r\n\r\nException: /home/nymble/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:865:9: error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n        ^\r\n/home/nymble/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:959:5: note: called from\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n    ^\r\n/home/nymble/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:435:5: note: called from\r\n    concrete_func = func.get_concrete_function()\r\n    ^\r\n/media/nymble/Nymble_Storage/ML_Data/my_passport/keras_data/convert_keras_to_tflite.py:20:1: note: called from\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(nymble_model)\r\n^\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.\r\n```\r\n\r\nAlso if i use 'bilinear' interpolation parameter in Upsampling2D, Tflconverter doesn't raises this error.\r\n\r\nAttached is the sample_mdoel.h5 graph to look into.\r\n\r\n[sample_model.zip](https://github.com/tensorflow/tensorflow/files/5267661/sample_model.zip)\r\n", "comments": ["The above ResizeNearestNeighbor op can be supported via Select TF ops. You can follow the below guides to add the Select TF ops:\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select#convert_a_model\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "@pranshugo,\r\nI was able to convert the model without any issues on TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2eda98b48197481eca438e46f7ef0cd6/43481.ipynb).\r\n\r\nCould you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!\r\n", "Yes, please try the latest version of TF, as I believe this op was updated recently.", "Yeah that solves my confusion. \r\nGot it working.", "@Pranshu195,\r\nThank you for the update. Marking this issue as closed, as it is fixed. Please feel free to re-open if necessary."]}, {"number": 43480, "title": "[RNN] Invoke the tflite model for inference with dynamic batchsize", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tf-nightly(2.4.0.dev20200826)\r\n- Python version:  3.7.7\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: GTX1050/2G\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n\r\nWhen I want to invoke the tflite_model for inference(with dynamic batchsize)\uff0c\r\nAn error occurred\uff1a\r\n\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.\r\nNode number 10 (WHILE) failed to invoke.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nInference with dynamic batchsize\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n**_Here is the link to my Colab to reproduce the issue:_**\r\n\r\n[https://colab.research.google.com/drive/13fr-C53JjRxIKFC9d96H9iwwexkGeH2O?usp=sharing](url)\r\n\r\nAlso here is the code segment where issure occured:\r\n```\r\n# Run the model with TensorFlow Lite\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nPREDICT_BATCH_SIZE = 13\r\n\r\n# resize the input tensor\r\ninterpreter.resize_tensor_input(input_details[0]['index'], (PREDICT_BATCH_SIZE,28,28))\r\ninterpreter.allocate_tensors()\r\n\r\ninterpreter.set_tensor(input_details[0][\"index\"], x_test[0:PREDICT_BATCH_SIZE, :, :])\r\ninterpreter.invoke()\r\nresult = interpreter.get_tensor(output_details[0][\"index\"])\r\n\r\nprint(result)\r\n\r\ninterpreter.reset_all_variables()\r\n```\r\n\r\nHere is the error occured:\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.\r\nNode number 10 (WHILE) failed to invoke.\r\n```\r\n\r\n**Other info / logs**  the whole error:\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-28-6198c0bfcdb3> in <module>()\r\n     13 \r\n     14 interpreter.set_tensor(input_details[0][\"index\"], x_test[0:PREDICT_BATCH_SIZE, :, :])\r\n---> 15 interpreter.invoke()\r\n     16 result = interpreter.get_tensor(output_details[0][\"index\"])\r\n     17 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    537     \"\"\"\r\n    538     self._ensure_safe()\r\n--> 539     self._interpreter.Invoke()\r\n    540 \r\n    541   def reset_all_variables(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/concatenation.cc:76 t->dims->data[d] != t0->dims->data[d] (13 != 1)Node number 23 (CONCATENATION) failed to prepare.\r\nNode number 10 (WHILE) failed to invoke.\r\n```\r\n", "comments": ["How about specifying the batch size when you create a TF model?", "> How about specifying the batch size when you create a TF model?\r\n\r\nThank you for your reply. I have tried this, and when I specify the batch size as 13(for example) while creating my TF model , the batch size for inference must also be set to13, or the same error will occur.\r\nSince the number of items in each batch is dynamic in my project, I hope that the batch size can be set dynamically before inference.", "Can you try set batch size to 1?\r\n\r\nThen inference one by one?", "> Can you try set batch size to 1?\r\n> \r\n> Then inference one by one?\r\n\r\nThank you for your answer. I also tried this method, but this method of cyclically calling inference will cause serious calculation delays. We know that the mobile terminal has high requirements for speed, and we hope that items can be placed in a batch for parallel calculation to reduce inference time.", "well, i new on here, but the test seem good, so far ", "> > Can you try set batch size to 1?\r\n> > Then inference one by one?\r\n> \r\n> Thank you for your answer. I also tried this method, but this method of cyclically calling inference will cause serious calculation delays. We know that the mobile terminal has high requirements for speed, and we hope that items can be placed in a batch for parallel calculation to reduce inference time.\r\n\r\nTFL rnn/lstm kernel is stateful (the states are maintained internally), so it's hard to change batch_size during inference time.\r\n\r\nIf you're fine with binary size, maybe it's possible to have multiple models with different batch_size.", "> > > Can you try set batch size to 1?\r\n> > > Then inference one by one?\r\n> > \r\n> > \r\n> > Thank you for your answer. I also tried this method, but this method of cyclically calling inference will cause serious calculation delays. We know that the mobile terminal has high requirements for speed, and we hope that items can be placed in a batch for parallel calculation to reduce inference time.\r\n> \r\n> TFL rnn/lstm kernel is stateful (the states are maintained internally), so it's hard to change batch_size during inference time.\r\n> \r\n> If you're fine with binary size, maybe it's possible to have multiple models with different batch_size.\r\n\r\nI see, thank you for your answer. Since dynamic batchsize can be supported when applying inference in TF, does TFlite have plans to support dynamic batchsize when applying inference too?", "> > > > Can you try set batch size to 1?\r\n> > > > Then inference one by one?\r\n> > > \r\n> > > \r\n> > > Thank you for your answer. I also tried this method, but this method of cyclically calling inference will cause serious calculation delays. We know that the mobile terminal has high requirements for speed, and we hope that items can be placed in a batch for parallel calculation to reduce inference time.\r\n> > \r\n> > \r\n> > TFL rnn/lstm kernel is stateful (the states are maintained internally), so it's hard to change batch_size during inference time.\r\n> > If you're fine with binary size, maybe it's possible to have multiple models with different batch_size.\r\n> \r\n> I see, thank you for your answer. Since dynamic batchsize can be supported when applying inference in TF, does TFlite have plans to support dynamic batchsize when applying inference too?\r\n\r\nTFL hasn't modeled resource variables yet, so we don't have any near-term plan. Sorry about that.", "> > > > > Can you try set batch size to 1?\r\n> > > > > Then inference one by one?\r\n> > > > \r\n> > > > \r\n> > > > Thank you for your answer. I also tried this method, but this method of cyclically calling inference will cause serious calculation delays. We know that the mobile terminal has high requirements for speed, and we hope that items can be placed in a batch for parallel calculation to reduce inference time.\r\n> > > \r\n> > > \r\n> > > TFL rnn/lstm kernel is stateful (the states are maintained internally), so it's hard to change batch_size during inference time.\r\n> > > If you're fine with binary size, maybe it's possible to have multiple models with different batch_size.\r\n> > \r\n> > \r\n> > I see, thank you for your answer. Since dynamic batchsize can be supported when applying inference in TF, does TFlite have plans to support dynamic batchsize when applying inference too?\r\n> \r\n> TFL hasn't modeled resource variables yet, so we don't have any near-term plan. Sorry about that.\r\n\r\nOk I see, thank you very much for your answers and help!"]}, {"number": 43479, "title": "Installing Tensorflow", "body": "(tensoflow) C:\\Users\\Compaq>python\r\nPython 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Compaq\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@LanceGinxzy \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Thanks!", "Yes it is on 32 bits but my system type is 64-bit operating system and x64-based processor", "@LanceGinxzy \r\nTensorflow is tested and supported only for 64 bit. Please, make sure your python and OS is 64 bit.Thanks!", "Hi I am trying to call tensflow in jupyter notebook using the command:\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nI use anaconda to use jupyter. I run into the following error:\r\n\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-0ba7c349fd63> in <module>\r\n      1 import numpy as np\r\n      2 import matplotlib.pyplot as plt\r\n----> 3 import tensorflow as tf\r\n      4 from tensorflow import keras\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Shilpi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nAny leads would be helpful. I am using windows 64 bit. I have checked the software and hardware installation requirements as mentioned in the previous comments.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43479\">No</a>\n"]}, {"number": 43478, "title": "tf2.3 keras.models.load_model setting compile=False fails to load saved_model but tf2.0 works.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nUse tensorflow Addons\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac (Can be reproduced on colab)\r\n- TensorFlow installed from (source or binary):\r\npip\r\n- TensorFlow version (use command below):\r\n2.3 fails 2.0works\r\n- Python version:\r\npython3\r\n\r\n**Describe the current behavior**\r\n\r\nI use F1score from addons as the metric. After training, I  use `keras.models.load_model`  to  load the saved_model  and also set `compile=False`.  I got an error. \r\n```\r\nValueError: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\r\n```\r\nThis **happens with tf2.3**, but **works with tf2.0**.\r\n\r\n**Describe the expected behavior**\r\n\r\nIf `compile=False` is set, it shouldn't check the metrics or losses.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n- CODE\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nprint(tf.__version__)\r\n\r\n_input = tf.keras.layers.Input(shape=(500), name=\"fbank\") # B*T*F*c\r\nout = tf.keras.layers.Dense(50, activation=\"tanh\")(_input)\r\nprobabilities = tf.keras.layers.Dense(2, activation=\"softmax\")(out)\r\nmodel = tf.keras.Model(inputs=_input, outputs=probabilities)\r\n\r\nmodel.compile(optimizer=\"sgd\", loss=tf.keras.losses.CategoricalCrossentropy(), \r\n              metrics= [\"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\")])\r\n\r\nmodel.summary()\r\n\r\nx=np.random.rand(300,500)\r\ny=np.random.rand(300,2)\r\nmodel.fit(x,y,batch_size=100, epochs=2)\r\n\r\npath = 'saved_model/'\r\nmodel.save(path, save_format='tf')\r\n\r\ndel model\r\nmodel = tf.keras.models.load_model('saved_model', compile=False)\r\n\r\n```\r\n\r\n- OUTPUT\r\n\r\n```\r\n2.3.0\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nfbank (InputLayer)           [(None, 500)]             0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 50)                25050     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 2)                 102       \r\n=================================================================\r\nTotal params: 25,152\r\nTrainable params: 25,152\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nEpoch 1/2\r\n3/3 [==============================] - 0s 4ms/step - loss: 0.7292 - accuracy: 0.5033 - f1_score: 0.0000e+00\r\nEpoch 2/2\r\n3/3 [==============================] - 0s 4ms/step - loss: 0.7192 - accuracy: 0.5200 - f1_score: 0.0000e+00\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: saved_model/assets\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-9ff0edc2f186> in <module>()\r\n     23 \r\n     24 del model\r\n---> 25 model = tf.keras.models.load_model('saved_model', compile=False)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/load.py in revive_custom_object(identifier, metadata)\r\n    844                      'and `from_config` when saving. In addition, please use '\r\n    845                      'the `custom_objects` arg when calling `load_model()`.'\r\n--> 846                      .format(identifier))\r\n    847 \r\n    848 \r\n\r\nValueError: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements `get_config`and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\r\n```\r\n\r\n**colab**\r\n\r\nhttps://colab.research.google.com/drive/17DI2N1L9EKSJ8-Ua88mcSnkmRT5adna3?usp=sharing\r\n\r\n\r\n", "comments": ["Please unlock the colab access permissions.", "> Please unlock the colab access permissions.\r\n\r\nhttps://colab.research.google.com/drive/17DI2N1L9EKSJ8-Ua88mcSnkmRT5adna3?usp=sharing\r\n", "Can you try to load with `custom_objects={\"F1Score\": tfa.metrics.F1Score}`?\r\nI've used: `!pip install --upgrade tensorflow tensorflow_addons` in your colab\r\n\r\n/cc @marload", "@Liu-Da,\r\nAs suggested by @bhack, by adding the `custom_objects={\"F1Score\": tfa.metrics.F1Score}` argument I was able to load the model without any issues. Please check [this gist](https://colab.research.google.com/gist/amahendrakar/ae7bdd21b2917d2ec57757c648f0572a/43478.ipynb) for reference. Thanks!", "> @Liu-Da,\r\n> As suggested by @bhack, by adding the `custom_objects={\"F1Score\": tfa.metrics.F1Score}` argument I was able to load the model without any issues. Please check [this gist](https://colab.research.google.com/gist/amahendrakar/ae7bdd21b2917d2ec57757c648f0572a/43478.ipynb) for reference. Thanks!\r\n\r\nThanks @bhack and @amahendrakar. This does make the model load successfully.  \r\n\r\nBut the key to the problem is why the same code behaves differently in the tf2.0 and tf2.3.  \r\n\r\nIn addition, if I set `compile=False`,  why should we still care about the custom metric?", "I also found that the model trained and saved using tf2.0 can be loaded correctly by tf2.3. (set compile=False and dont need set custom_objects)", "I don't know if tf2.0 was going to save this custom object metrics. With the current code a think that the internal load doesn't care about the `compile` argument https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/load.py#L119-L120", "> I don't know if tf2.0 was going to save this custom object metrics. With the current code a think that the internal load doesn't care about the `compile` argument https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/saved_model/load.py#L119-L120\r\n\r\nAs the model trained and saved using tf2.0 can be loaded correctly by tf2.3 without set any custom_objects, this situation may not be caused by tf.keras.model.load but by Model.save.", "> I don't know if tf2.0 was going to save this custom object metrics. \r\n\r\nSo if you don't save custom object metrics you don't have problem on load or not? As you was asking about loading with `compile=False`", "E.g. If you save and load a non compiled model also with TF2.3 is ok you don't need to care about the custom_metrics:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nprint(tf.__version__)\r\n\r\n_input = tf.keras.layers.Input(shape=(500), name=\"fbank\") # B*T*F*c\r\nout = tf.keras.layers.Dense(50, activation=\"tanh\")(_input)\r\nprobabilities = tf.keras.layers.Dense(2, activation=\"softmax\")(out)\r\nmodel = tf.keras.Model(inputs=_input, outputs=probabilities)\r\n\r\n#model.compile(optimizer=\"sgd\", loss=tf.keras.losses.CategoricalCrossentropy(), \r\n#              metrics= [\"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\")])\r\n\r\nmodel.summary()\r\n\r\nx=np.random.rand(300,500)\r\ny=np.random.rand(300,2)\r\n#model.fit(x,y,batch_size=100, epochs=2)\r\n\r\npath = 'saved_model/'\r\nmodel.save(path, save_format='tf')\r\n\r\ndel model\r\nmodel = tf.keras.models.load_model('saved_model', compile=False)\r\n```", "> E.g. If you save and load a non compiled model also with TF2.3 is ok you don't need to care about the custom_metrics:\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> import tensorflow_addons as tfa\r\n> \r\n> print(tf.__version__)\r\n> \r\n> _input = tf.keras.layers.Input(shape=(500), name=\"fbank\") # B*T*F*c\r\n> out = tf.keras.layers.Dense(50, activation=\"tanh\")(_input)\r\n> probabilities = tf.keras.layers.Dense(2, activation=\"softmax\")(out)\r\n> model = tf.keras.Model(inputs=_input, outputs=probabilities)\r\n> \r\n> #model.compile(optimizer=\"sgd\", loss=tf.keras.losses.CategoricalCrossentropy(), \r\n> #              metrics= [\"accuracy\", tfa.metrics.F1Score(num_classes=2, average=\"micro\")])\r\n> \r\n> model.summary()\r\n> \r\n> x=np.random.rand(300,500)\r\n> y=np.random.rand(300,2)\r\n> #model.fit(x,y,batch_size=100, epochs=2)\r\n> \r\n> path = 'saved_model/'\r\n> model.save(path, save_format='tf')\r\n> \r\n> del model\r\n> model = tf.keras.models.load_model('saved_model', compile=False)\r\n> ```\r\n\r\nThis code runs perfectly. \r\n\r\n> > I don't know if tf2.0 was going to save this custom object metrics.\r\n> \r\n> So if you don't save custom object metrics you don't have problem on load or not? As you was asking about loading with `compile=False`\r\n\r\nYes, this code runs perfectly.   ", "![image](https://user-images.githubusercontent.com/24472521/94018474-3bbf0780-fde3-11ea-8ad7-1e08dbc7f464.png)\r\n\r\nThe official document says that if we use Savedmodel, we don't need to consider the issue of custom_object.\r\n\r\nModel should been successfully loaded by `model =tf.keras.models.load_model('saved_model')`.\r\n\r\nEven if there is a custom_object, it should not affect the loading model.\r\n\r\nWhen we compile and train a model and then distribute it to others, we hope that the model can be successfully loaded without any source data sharing and inference can be performed correctly.", "I don't know if the exception is (/cc @k-w-w)\r\nhttps://github.com/tensorflow/tensorflow/blob/0650101e05ef1ad56bfaadcd63ab775b25ecdd16/tensorflow/python/keras/saving/saved_model/metric_serialization.py#L44-L45", "If someone only wants to load the model for prediction and inference, **without retraining**, I found a workaround solution.\r\nUse this code before `model.save`\r\n```\r\nmodel.optimizer = None\r\nmodel.compiled_loss = None\r\nmodel.compiled_metrics = None\r\n```\r\n", "> I don't know if the exception is (/cc @k-w-w)\r\n> https://github.com/tensorflow/tensorflow/blob/0650101e05ef1ad56bfaadcd63ab775b25ecdd16/tensorflow/python/keras/saving/saved_model/metric_serialization.py#L44-L45\r\n\r\nCan anyone familiar with this part of the code provide some help?", "It is referencing an internal ticket. I've already mentioned @k-w-w ", "Was able to reproduce the issue. \r\n\r\nCode works with TF v2.0, throws an error stating \r\n`ValueError: Unable to restore custom object of type _tf_keras_metric currently. Please make sure that the layer implements get_config and from_config when saving. In addition, please use the custom_objects arg when calling load_model().` with TF v2.3. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/906edc47c0ce6ee03c40d8f3b8f0dc01/untitled0.ipynb#scrollTo=Hcdmx695RtxT). Thanks! ", "> If someone only wants to load the model for prediction and inference, **without retraining**, I found a workaround solution.\r\n> Use this code before `model.save`\r\n> \r\n> ```\r\n> model.optimizer = None\r\n> model.compiled_loss = None\r\n> model.compiled_metrics = None\r\n> ```\r\n@Liu-Da I'm experiencing this problem as well. I'm using `from tensorflow.keras.callbacks import ModelCheckpoint` to save the model. Do you see an adjustment of your work-around when using `ModelCheckpoint`?", "I submitted a fix for this issue in #45278, but it is still waiting for review.\r\nI don't think there is an easy workaround for this currently other than only saving the weights in `ModelCheckpoint` or by adapting the callback itself to only save the model without the metrics.", "@lgeiger I'm thinking about using @Liu-Da workaround together with `ModelCheckpoint`:\r\n\r\n```python\r\n...\r\ncallbacks = [\r\n    ModelCheckpoint(\r\n        filepath=arguments['model_output_path'],\r\n        monitor=arguments['early_stopping_monitor'],\r\n        verbose=0,\r\n        save_best_only=True,\r\n        save_weights_only=False,\r\n        mode='max'\r\n    )\r\n]\r\nmodel.fit(\r\n    x=train_generator,\r\n    steps_per_epoch=np.ceil(num_images_train / arguments['batch_size_train']),\r\n    epochs=arguments['epochs'],\r\n    validation_data=val_generator,\r\n    validation_steps=np.ceil(num_images_val / arguments['batch_size_val']),\r\n    callbacks=callbacks\r\n)\r\n\r\nmodel.optimizer = None\r\nmodel.compiled_loss = None\r\nmodel.compiled_metrics = None\r\nmodel.save(arguments['model_output_path'])\r\n```\r\nHave I understood `ModelCheckpoint` correctly, that after `model.fit()`, the variable `model` will be the best model (because `save_best_only=True` in `ModelCheckpoint`)?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43478\">No</a>\n", "@Liu-Da Just ran in to this issue where I trained a model with a tensorflow_addon metric. In the file where I load the saved model, import both tensorflow and tensorflow_addons, then the saved model loads without issue.", "For anyone subclassing a metric, see my [response](https://github.com/tensorflow/tensorflow/issues/34068#issuecomment-948208134) on another thread. Having to set the `custom_objects` keys to exactly the same name as the class isn't limited to just saving H5 models, but also TF models."]}, {"number": 43477, "title": "upd callbacks.py to fix keras.model.fit() progbar output collision", "body": "The suggested edit ensures that `keras.model.fit()` progbar output is complete before any output from custom callbacks appears on epoch end. Basically this edit puts the standard `ProgbarLogger` first in the list of callbacks. \r\n\r\n**Current behavior:**\r\nwhen training Model using Model.fit() with verbose=1 (for progress bar) and keras.callbacks.Callback() with on_epoch_end() function I observe that custom callback output happens BEFORE completion of the epoch progress output, like this:\r\n```\r\nEpoch 1/2\r\n 63/125 [==============>...............] - ETA: 0s - loss: 43.2549 - mean_absolute_error: 4.6003\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n125/125 [==============================] - 0s 794us/step - loss: 37.6690 - mean_absolute_error: 4.6575\r\n\r\n```\r\n**Expected behavior:**\r\ncustom callback output should happen AFTER completion of the epoch progress output:\r\n```\r\nEpoch 1/2\r\n125/125 [==============================] - 0s 900us/step - loss: 36.9897 - mean_absolute_error: 4.8264\r\nCALLBACK MESSAGE ON END EPOCH 0\r\n```\r\nOriginally discussed at [tf github](https://github.com/tensorflow/tensorflow/issues/43184) with problem description and gist examples. Latest colab example is [here](https://colab.research.google.com/gist/poedator/3630b1cdec32ff6ef6ccdf6b63c4957a/custom_callback.ipynb).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43477) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43477) for more info**.\n\n<!-- ok -->", "@poedator  Can you please resolve conflicts? Thanks!", "I would like to close this PR and resubmit 2 separate ones to avoid confusion. "]}, {"number": 43475, "title": "CMSIS-NN: conv unit test cases fail for non-unity dilation cases", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): 712fd5cfe4a5de420a4226c874664423eba4cb1a\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): stm32f4\r\n\r\n**Describe the problem**\r\nThe unit test cases which have dilation parameters not equal to 1 fail for cmsis-nn.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1. Remove conv_test in exclusion filter list in target Makefile for stm32f4.\r\n\r\n2. make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=stm32f4 test_kernel_fully_connected_test\r\n\r\n", "comments": ["I'll provide the fix for this.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43475\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43475\">No</a>\n"]}, {"number": 43474, "title": "avoid using TRUE and FALSE in enum", "body": "this addresses a recent failed-to-build problem on Mac.\r\nsee #43421", "comments": ["Can you please solve conflicts?", "close this, since this was addressed by 74027b89402720e598d5512dddf3ea7737df7ffe, that's why there are conflicts"]}, {"number": 43472, "title": "Missing TRANSPOSE Op Kernel", "body": "**System information**\r\n- Linux Ubuntu 20.04:\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (or github SHA if from source):  tf-nightly==2.4.0.dev20200917\r\n\r\nI'm attempting to use a TFLite converted model, which was created and trained using TF2 + Keras.  The converter successfully created the TFLite file, and I've loaded it into a micro-controller app, as a flatbuffer cpp + h file.\r\n\r\nI'm unable to share the model at this time due to confidentiality, however the model contains Conv2D, BatchNormalization, ReLu, MaxPooling2D, Permute, Dropout, Flatten, Dense and Softmax.\r\n\r\nAfter conversion, the model is loaded into an Arduino sketch, but upon loading the model, an error is reported.\r\n\r\n```txt\r\n8 bytes lost due to alignment. To avoid this loss, please make sure the tensor_arena is 16 bytes aligned.\r\nDidn't find op for builtin opcode 'TRANSPOSE' version '2'\r\n\r\nFailed to get registration from op code TRANSPOSE\r\n \r\nFailed starting model allocation.\r\n```\r\n\r\nGiven that this operation was chosen from the Builtin operation set, I believe this is a fault/bug.  Can you please advise?\r\n\r\ntype registration\r\n``` \r\nconst tflite::Model* model = nullptr;\r\ntflite::MicroInterpreter* interpreter = nullptr;\r\nTfLiteTensor* input = nullptr;\r\nTfLiteTensor* output = nullptr;\r\nalignas(16) uint8_t tensor_arena[kTensorArenaSize]\r\n```\r\n\r\nAllOps missing TRANSPOSE\r\n```cpp\r\nstatic tflite::AllOpsResolver resolver;  // NO TRANSPOSE kernel registration\r\n```\r\n\r\nMicroMutableOpResolver missing TRANSPOSE registration\r\n```cpp\r\ntflite::MicroMutableOpResolver<6> resolver;\r\nresolver.AddConv2D();    \r\nresolver.AddDepthwiseConv2D();\r\nresolver.AddFullyConnected();\r\nresolver.AddReshape();\r\nresolver.AddSoftmax();\r\nresolver.AddBuiltin(tflite::BuiltinOperator_MAX_POOL_2D    \r\n    ,tflite::ops::micro::Register_MAX_POOL_2D()); \r\n// resolver.AddBuiltin(tflite::BuiltinOperator_TRANSPOSE,\r\n//     ,tflite::ops::micro::Register_TRANSPOSE); //BuiltinOperator_TRANSPOSE exists, but no Register_TRANSPOSE exists\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\n def model_to_tflite(self, features_path = None, tflite_path = None):\r\n        '''Converts a Keras model into a TFLite model'''\r\n        assert self.model is not None, 'TFLite conversion requires the model be loaded'\r\n        assert self.x_data is not None and self.y_data is not None, 'Sample data must be loaded'\r\n\r\n        if os.path.exists(tflite_path):\r\n            logging.warning(f'TFLite file already exists: {tflite_path}')\r\n\r\n        logging.info(f'Found {len(self.x_data)} features')\r\n\r\n        # Construction of a representative dataset\r\n        def representative_dataset():\r\n            for i in range(len(self.x_data)):\r\n                yield([self.x_data[i:i+1,:,:,:]])\r\n\r\n        # Construction of a TFLite converter\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\r\n        converter.representative_dataset = representative_dataset\r\n        \r\n        converter.optimizations = [ tf.lite.Optimize.OPTIMIZE_FOR_LATENCY ]\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n        \r\n        converter.inference_input_type = tf.int8\r\n        converter.inference_output_type = tf.int8\r\n        \r\n        tflite_model = converter.convert()\r\n        bytes_written = open(tflite_path, 'wb').write(tflite_model)\r\n\r\n        return bytes_written\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n/home/ian/Documents/source/acdnet_pipeline/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2289: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\r\n2020-09-23 14:34:59.654363: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n/home/ian/Documents/source/acdnet_pipeline/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1376: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`layer.updates` will be removed in a future version. '\r\n2020-09-23 14:35:02.599832: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-23 14:35:02.600152: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-09-23 14:35:02.600313: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-23 14:35:02.600650: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-09-23 14:35:02.600775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-23 14:35:02.601273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-09-23 14:35:02.601398: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:02.601464: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:02.601493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-23 14:35:02.601521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-23 14:35:02.601531: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-23 14:35:02.601622: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:02.601715: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:02.601745: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-09-23 14:35:02.894277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-23 14:35:02.894345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-23 14:35:02.894368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-23 14:35:02.919186: I tensorflow/core/platform/profile_utils/cpu_utils.cc:108] CPU Frequency: 2599990000 Hz\r\n2020-09-23 14:35:02.982735: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:872] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 5.462ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n\r\n2020-09-23 14:35:03.203503: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-09-23 14:35:03.203549: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n2020-09-23 14:35:03.469971: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-09-23 14:35:03.470367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-23 14:35:03.471405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1060 computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 10 deviceMemorySize: 5.94GiB deviceMemoryBandwidth: 178.99GiB/s\r\n2020-09-23 14:35:03.471730: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:03.471928: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:03.471981: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-23 14:35:03.472028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-23 14:35:03.472067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-23 14:35:03.472238: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:03.472414: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\r\n2020-09-23 14:35:03.472450: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-09-23 14:35:03.472494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-23 14:35:03.472519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-23 14:35:03.472540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n\r\n```", "comments": ["@victorromeo \r\nI ran the code shared on 2.3 and do not face any errors,please find the [gist here](url). Please share a colab gist with the erro reported.\r\nWith respect to the error,please refer to [link](https://stackoverflow.com/questions/62321919/edge-tpu-compiler-error-didnt-find-op-for-builtin-opcode-resize-nearest-neigh)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Bump, as I'm still authoring a TRANSPOSE operation for the micro interpreter.", "> @victorromeo\r\n\r\nAs informed above we do not face any errors in the code shared, please confirm and if issue exist share a colab gist with the error or move the issue to closed status if resolved.Thanks!", "This is an issue of a missing C++ micro kernel operation, not a core kernel operation and as such is not appropriate for a python colab gist. As mentioned earlier, I\u2019m working on this as a custom c++ operation.  Does Colab support C++11 with Bezal compilation?\r\n\r\nA Keras TF2 model, when converted to TFLite **micro**, includes an operation which is not yet supported called TRANSPOSE. This already exists as a TFLite operation (But when used on an ARM microcontroller, this is simply not available). ", "@jdduke May I please get some pointers if possible to confirm the approach for an appropriate Transpose operation? There are guides for TFLite custom operations, however not for missing micro operations or custom micro operations.\r\n\r\nI'm planning on reusing the TransposeContext, from Transpose.cc but dropping the KernelType\r\n```c++\r\nstruct TransposeContext {\r\n    TransposeContext(TfLiteContext* context, TfLiteNode* node) {\r\n        input = GetInput(context, node, 0);\r\n        perm = GetInput(context, node, 1);\r\n        output = GetOutput(context, node, 0);\r\n    }\r\n    const TfLiteTensor* input;\r\n    const TfLiteTensor* perm;\r\n    TfLiteTensor* output;\r\n};\r\n```\r\n\r\nI'm thinking of basing the micro implementation off of ComputePermutation in tfl_ops.cc\r\n```c++\r\n// Computes the permutation of a constant `input_tensor` according to `perm`.\r\n// The function recursively traverses the dimensions of the output tensor in\r\n// a row-major order and writes the value in the output tensor into\r\n// `new_values`.\r\nvoid ComputePermutation(ElementsAttr input_tensor, ArrayRef<int32_t> perm,\r\n                        ArrayRef<int64_t> output_shape, int num_dimensions,\r\n                        int output_axis, std::vector<uint64_t> *input_indices,\r\n                        std::vector<Attribute> *new_values) {\r\n  // Refer to the implementation of `Transpose` function in\r\n  // tensorflow/lite/kernels/internal/reference/reference_ops.h\r\n  assert(output_axis < num_dimensions);\r\n  const int input_axis = perm[output_axis];\r\n  for (int i = 0; i < output_shape[output_axis]; ++i) {\r\n    // Update the input indices on `input_axis`.\r\n    input_indices->at(input_axis) = i;\r\n    // Write the value from `input_tensor` if it is the last axis or\r\n    // recurse into the next axis.\r\n    const bool is_last_axis = output_axis == num_dimensions - 1;\r\n    if (is_last_axis) {\r\n      new_values->push_back(input_tensor.getValue(*input_indices));\r\n    } else {\r\n      ComputePermutation(input_tensor, perm, output_shape, num_dimensions,\r\n                         output_axis + 1, input_indices, new_values);\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nFinally, I'm hoping that the current /lite/transpose_test.cc are appropriate for reuse, with generic implementations but specific tests for int8 quantization.\r\n\r\nHere's my work in progress.\r\n[Forked micro_transpose_op branch](https://github.com/victorromeo/tensorflow/commit/0b604d65d421109b730d6b4448bd437d8ff92065)", "@advaitjain or @petewarden can you advise on the recommended process for porting a Lite kernel to Micro?", "@victorromeo, as you have found out, TFLM supports a subset of the TfLite ops.\r\n\r\nWe will be adding a guide for porting from Lite to as there are subtleties around not having dynamic memory allocation as well as other differences between Lite and Micro.\r\n\r\nI would recommend to start by sharing more details about the actual model that you are trying to use, based on our [contribution guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#reference-kernel-implementations) to help motivate the need for this additional Op in Micro. We can then give you some specific pointers on how to proceed.\r\n\r\n\r\nOne example of an Op (along with motivation for why it is worth adding to Micro at this time) that is currently being ported via community contributions is PR #43384 and #43381\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks @advaitjain \r\n\r\nThe model is currently being used to prepare an audio classifier, which fits onto a micro-controller.  The Transpose operation has been shown to be a valuable kernel layer toward a greater goal of effectively switching from time domain to frequency domain and back.  As the model is currently being developed for research papers, I have been advised I am unable to share the entire architecture.  The architecture doesn't use FFT preprocessing.  The origin of the issue is due to the Keras.Permute operation being converted into TFLite.Transpose. I appreciate it is hard to fully appreciate the benefit of adding the operation, without evidence, but this will be in the paper.\r\n\r\nThe model is planned to run on Cortex M4 devices such as the moderately sized Arduino Nano 33 BLE Sense.\r\n\r\nSide note, it would be great to see a flag to check compatibility of builtin micro operations for use during TFLite conversion, as I had to circle a while to deduce that Transpose was the issue.", "Thanks for the additional context, I understand that you're not able to share model architecture pre-publication.\r\n\r\nLet's wait for around a week for me to put together a guide for porting ops from lite to micro and I'll link to it from this issue as well.\r\n\r\nIn the meantime, if you want to send a PR with flatbuffer changes for transpose (similar to #43384), that would be great.\r\n\r\nSince the transpose op does not have any BuilinOptions, the parsing function will be something along the lines of https://github.com/tensorflow/tensorflow/blob/e0ed4b42ee845ab63a714e1cf26abf54198c61b5/tensorflow/lite/core/api/flatbuffer_conversions.cc#L1124-L1130\r\n\r\nand\r\nhttps://github.com/tensorflow/tensorflow/blob/e0ed4b42ee845ab63a714e1cf26abf54198c61b5/tensorflow/lite/core/api/flatbuffer_conversions.cc#L212-L214\r\n", "Thanks for your additional feedback.  I've further implemented all compatible unit tests for Transpose in my fork [here](https://github.com/victorromeo/tensorflow/tree/micro_transpose_op) and these are now compiling.  Its worth noting my implemetnation is not optimal, but does achieve 1D-4D transpose support.  Acheived using a port of existing TFLite core Transpose, using micro frameworks and static arrays. Onto final stages of testing now.  Cheers.", "@advaitjain \r\n> Let's wait for around a week for me to put together a guide for porting ops from lite to micro and I'll link to it from this issue as well.\r\nAny luck yet?\r\nCould you please update if you have guide ready for porting a TF Lite kernel to TF Lite Micro?\r\nIt is badly needed for real world practical projects.\r\n\r\n@victorromeo\r\nWould you be happy to share you Transpose implementation for micro? It might help others or you can get some valuable support from others as well. \r\nThanks", "The most stable implementation of Transpose is https://github.com/victorromeo/tensorflow/tree/v2.3.1_transpose\r\n\r\nI'm going to rebase this off master, then create a pull request for consideration from the team.", "I created a PR [48192](https://github.com/tensorflow/tensorflow/pull/48192) that should solve this issue.\r\n\r\nThank you @victorromeo for your contribution. I refactor some of your code cherry-picking it, in a way your name appears. Having your name associated with the PR, Google now asks for both of our signature in their Contributor License Agreement. It would be really nice if you could quickly sign it so the PR will be pushed forward :)\r\n\r\nThank you!", "@victorromeo,\r\n\r\nWe can see that the PR that you submitted has been merged. Can you kindly confirm if we can close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43470, "title": "Bad Loss History, Callback vs. Progressbar Loss Mismatch", "body": "**System information**\r\n- Have I written custom code - Yes\r\n- OS Platform and Distribution - RHEL7\r\n- Mobile device - None\r\n- TensorFlow installed from - pip\r\n- TensorFlow version v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: \r\ncudatoolkit               10.1.243             h6bb024c_0  \r\ncudnn                     7.6.5                cuda10.1_0  \r\n- GPU model and memory: 4x Nvidia TITAN X (Maxwell)\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am working with a GAN, using code by subclassing tf.keras.Model in the style of: https://keras.io/examples/generative/dcgan_overriding_train_step/\r\n\r\nI attempted to make a plot of the generator loss and the discriminator loss using the history object from the result of model.fit(), and noticed the values in the history object were all constants, despite obvious variability as displayed by the progress bar.  I dug deeper, and added a callback to print the `d_loss` and `g_loss` after `on_epoch_end` and `on_train_batch_end`, switched the model.compile() to include `run_eagerly=True`, added a print statement at the end of `train_step()` and I noticed that they do not match the values being printed by the progress bar.\r\n\r\nExample terminal output:\r\n```\r\nEpoch 00001: LearningRateScheduler reducing learning rate to 0.01.\r\nEpoch 1/4\r\n\r\nTrain Step:  tf.Tensor(0.72807, shape=(), dtype=float32) tf.Tensor(0.46352494, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.7280700206756592, 'g_loss': 0.4635249376296997}\r\n  1/100 [..............................] - ETA: 0s - d_loss: 0.7281 - g_loss: 0.4635\r\n\r\nTrain Step:  tf.Tensor(0.7460346, shape=(), dtype=float32) tf.Tensor(0.36776978, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.7460346221923828, 'g_loss': 0.3677697777748108}\r\n  2/100 [..............................] - ETA: 12s - d_loss: 0.7371 - g_loss: 0.4156\r\n\r\nTrain Step:  tf.Tensor(0.67379105, shape=(), dtype=float32) tf.Tensor(0.6921332, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.6737910509109497, 'g_loss': 0.6921331882476807}\r\n  3/100 [..............................] - ETA: 15s - d_loss: 0.7160 - g_loss: 0.5078\r\n```\r\n\r\nAs you can see, on the first batch, all the values match precisely - the print statement at the end of `train_step`, the callback for `on_train_batch_end` and the progressbar output all match.  By the 2nd batch, `train_step` and `on_train_batch_end` are reporting a value of 0.746 for d_loss by the progressbar is printing a value of .7371, and g_loss is .3677 vs .4156.  \r\n\r\nOriginally, I suspected the distribute_strategy might be involved, so I commented that out and I re-ran with `CUDA_VISIBLE_DEVICES=0`, and got the same value mismatching.  I have this issue regardless of whether I run with `run_eagerly=True` or False.\r\n\r\nMy actual `train_step` for reference in case I'm just doing something stupid:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.nn import sigmoid_cross_entropy_with_logits\r\n\r\ndef train_step(self, train_step_data):\r\n        \r\n        high_res_imgs = train_step_data[1]\r\n        \r\n        low_res_imgs = train_step_data[0] \r\n        \r\n        with tf.GradientTape(persistent=True) as tape:\r\n           \r\n            generated_images = self.generator(low_res_imgs)\r\n    \r\n            logits_on_generated = self.discriminator(generated_images)\r\n            \r\n            logits_on_real = self.discriminator(high_res_imgs)\r\n            \r\n            d_real_loss = sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_on_real), logits=logits_on_real)\r\n            \r\n            d_gen_loss = sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(logits_on_generated), logits=logits_on_generated)\r\n            \r\n            d_loss = 0.5*(d_real_loss + d_gen_loss)\r\n            \r\n            \r\n            g_adv_loss = sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_on_generated), logits=logits_on_generated)\r\n            \r\n            \r\n            g_loss = g_adv_loss\r\n\r\n\r\n        ################################################################################\r\n        # Now backpropagate discriminator based on the losses\r\n        ################################################################################\r\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\r\n        \r\n        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\r\n        \r\n        ################################################################################\r\n        # Now backpropagate generator based on the losses\r\n        ################################################################################\r\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\r\n        \r\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\r\n\r\n\r\n        #print(d_loss.shape)\r\n        #print(d_loss.numpy())\r\n        print(\"\\nTrain Step: \", d_loss[0,0], g_loss[0,0])\r\n        return {\"d_loss\": d_loss[0,0], \"g_loss\": g_loss[0,0]}\r\n```\r\n\r\nI still have this issue regardless of whether I just return d_loss and g_loss or 'd_loss[0,0]' and g_loss[0,0].\r\n\r\nAnd my callback:\r\n```\r\nclass ModelPrinter(tf.keras.callbacks.Callback):\r\n        \r\n        def __init__(self, model):\r\n            \r\n            self.model = model\r\n            \r\n        def on_epoch_end(self, epoch, logs=None):\r\n            \r\n            print(self.model)\r\n            print(\"\\nEpoch End: \", logs)\r\n            \r\n        def on_train_batch_end(self, step_num, logs=None):\r\n            \r\n            print(\"\\nBatch End: \", logs)\r\n```", "comments": ["Can you have something very, very minimal code snippet that we could copy paste and run to reproduce this. If you have dummy input it is better so we don't need to copy files.\r\n\r\nAlso if you can share a minimal running Colab could be useful.", "Self-contained example script:\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.nn import sigmoid_cross_entropy_with_logits\r\nfrom tensorflow.keras.losses import mean_squared_error\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.applications import VGG19\r\n\r\nfrom tensorflow.keras.layers import Dense, \\\r\n                                    Activation, \\\r\n                                    Input, \\\r\n                                    Flatten, \\\r\n                                    Dropout, \\\r\n                                    LSTM, \\\r\n                                    Conv2D, \\\r\n                                    Convolution2DTranspose, \\\r\n                                    UpSampling2D, \\\r\n                                    Reshape, \\\r\n                                    BatchNormalization, \\\r\n                                    MaxPooling2D, \\\r\n                                    PReLU, \\\r\n                                    LeakyReLU, \\\r\n                                    ReLU, \\\r\n                                    Add\r\n                                    \r\nfrom tensorflow.keras.models import Sequential, Model\r\n\r\nimport numpy\r\n\r\n        \r\n        \r\n\r\ndef create_generator(input_shape):\r\n    \r\n    gen_input = Input(shape=input_shape, name=\"Generator_Input_Layer\")\r\n    \r\n    model = Conv2D(filters=256, \r\n                   kernel_size=9, \r\n                   strides=1, \r\n                   padding=\"same\")(gen_input)\r\n    \r\n    model = PReLU(alpha_initializer='zeros', \r\n                  alpha_regularizer=None, \r\n                  alpha_constraint=None, \r\n                  shared_axes=[1,2])(model)\r\n    \r\n    model = Convolution2DTranspose(filters=64,\r\n                                   kernel_size=3,\r\n                                   strides=2,\r\n                                   padding='same')(model)\r\n    model = BatchNormalization()(model)\r\n    \r\n    model = ReLU()(model)\r\n    \r\n    \r\n                                   \r\n    model = Convolution2DTranspose(filters=16,\r\n                                   kernel_size=3,\r\n                                   strides=2,\r\n                                   padding='same')(model)\r\n    model = BatchNormalization()(model)\r\n    \r\n    model = ReLU()(model)\r\n    \r\n    \r\n                                   \r\n    model = Convolution2DTranspose(filters=4,\r\n                                   kernel_size=3,\r\n                                   strides=2,\r\n                                   padding='same')(model)\r\n    model = BatchNormalization()(model)\r\n    \r\n    model = ReLU()(model)\r\n    \r\n    \r\n                                   \r\n    model = Convolution2DTranspose(filters=1,\r\n                                   kernel_size=3,\r\n                                   strides=2,\r\n                                   padding='same')(model)\r\n    model = BatchNormalization()(model)\r\n    \r\n    model = ReLU()(model)\r\n    \r\n    generator_model = Model(inputs=gen_input, outputs=model)\r\n    \r\n    return generator_model\r\n\r\n\r\n\r\n\r\n\r\ndef discriminator_block(model, filters, kernel_size, strides):\r\n    \r\n    model = Conv2D(filters=filters, \r\n                   kernel_size=kernel_size, \r\n                   strides=strides, \r\n                   padding=\"same\")(model)\r\n                   \r\n    model = BatchNormalization(momentum = 0.5)(model)\r\n    \r\n    model = LeakyReLU(alpha = 0.2)(model)\r\n    \r\n    return model\r\n    \r\n\r\ndef create_discriminator(image_shape):\r\n    \r\n        \r\n    input = Input(shape=image_shape, name=\"Discriminator_Input_Layer\")\r\n    \r\n    model = Conv2D(filters=64, \r\n                   kernel_size=3, \r\n                   strides=1, \r\n                   padding=\"same\")(input)\r\n    \r\n    model = LeakyReLU(alpha = 0.2)(model)\r\n    \r\n    model = discriminator_block(model, 64, 3, 2)\r\n    model = discriminator_block(model, 64, 3, 2)\r\n    model = discriminator_block(model, 64, 3, 2)\r\n    \r\n    \r\n    model = Flatten()(model)\r\n    model = Dense(256)(model)\r\n    model = LeakyReLU(alpha = 0.2)(model)\r\n   \r\n    model = Dense(1)(model)\r\n    model = Activation('sigmoid')(model) \r\n    \r\n    discriminator_model = Model(inputs=input, outputs=model)\r\n    \r\n    return discriminator_model\r\n\r\n\r\n\r\n\r\n\r\nclass TestGAN(tf.keras.Model):\r\n    \r\n    def __init__(self,\r\n                 high_res_shape,\r\n                 low_res_shape,\r\n                 discrim_lr, \r\n                 gen_lr):\r\n        \r\n        super(TestGAN, self).__init__()\r\n        \r\n        self.noise_shape = low_res_shape\r\n        \r\n        #self.output_shape = high_res_shape\r\n        \r\n        self.discriminator = create_discriminator(high_res_shape)\r\n        \r\n        self.generator = create_generator(low_res_shape)\r\n        \r\n        self.d_optimizer = Adam(learning_rate=discrim_lr)\r\n        \r\n        self.g_optimizer = Adam(learning_rate=gen_lr)\r\n        \r\n        \r\n    \r\n    def train_step(self, train_step_data):\r\n        \r\n        low_res_imgs = train_step_data[0] \r\n\r\n        high_res_imgs = train_step_data[1]\r\n        \r\n        #Need persistent=True because we call the tape.gradient() twice\r\n        with tf.GradientTape(persistent=True) as tape:\r\n            \r\n            generated_images = self.generator(low_res_imgs)\r\n    \r\n            logits_on_generated = self.discriminator(generated_images)\r\n            \r\n            logits_on_real = self.discriminator(high_res_imgs)\r\n            \r\n            \r\n            \r\n            ################################################################################\r\n            \r\n            real_labels = tf.ones_like(logits_on_real)\r\n            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\r\n            \r\n            generated_labels = tf.zeros_like(logits_on_generated)\r\n            generated_labels += 0.05 * tf.random.uniform(tf.shape(generated_labels))\r\n    \r\n            d_real_loss = sigmoid_cross_entropy_with_logits(labels=real_labels, logits=logits_on_real)\r\n            \r\n            d_gen_loss = sigmoid_cross_entropy_with_logits(labels=generated_labels, logits=logits_on_generated)\r\n            \r\n            d_loss = 0.5*(d_real_loss + d_gen_loss)\r\n            \r\n            \r\n            \r\n            g_loss = sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_on_generated), logits=logits_on_generated)\r\n            \r\n            \r\n        ################################################################################\r\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\r\n        \r\n        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\r\n        \r\n        ################################################################################\r\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\r\n        \r\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\r\n\r\n\r\n        #print(d_loss.shape)\r\n        #print(d_loss.numpy())\r\n        print(\"\\nTrain Step: \", d_loss[0,0], g_loss[0,0])\r\n        return {\"d_loss\": d_loss[0,0], \"g_loss\": g_loss[0,0]}\r\n    \r\n    \r\n    \r\n    def call(self, inputs):\r\n        \r\n        print(\"Call, inputs: \", inputs.shape)\r\n        \r\n        return self.generator(inputs)\r\n    \r\n\r\nclass ModelPrinter(tf.keras.callbacks.Callback):\r\n        \r\n    def __init__(self, model):\r\n    \r\n        self.model = model\r\n    \r\n    def on_epoch_end(self, epoch, logs=None):\r\n    \r\n        print(\"\\nEpoch End: \", logs)\r\n    \r\n    def on_train_batch_end(self, step_num, logs=None):\r\n    \r\n        print(\"\\nBatch End: \", logs)\r\n\r\n\r\nclass FakeDataGenerator():\r\n    def __init__(self, batch_size, high_res_shape, low_res_shape):\r\n        self.batch_size = batch_size\r\n        self.high_res_shape = high_res_shape\r\n        self.low_res_shape = low_res_shape\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def next(self):\r\n        \r\n        return self.__next__()\r\n\r\n    def __next__(self):\r\n\r\n        high_res = numpy.random.rand(self.batch_size, \r\n                                     self.high_res_shape[0],\r\n                                     self.high_res_shape[1],\r\n                                     1)\r\n\r\n        low_res = numpy.random.rand(self.batch_size,\r\n                                    self.low_res_shape[0],\r\n                                    self.low_res_shape[1],\r\n                                    1)\r\n\r\n        return low_res, high_res\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n\r\n    batch_size = 4\r\n\r\n    high_res_shape = (256, 256, 1)\r\n\r\n    low_res_shape = (16, 16, 1)\r\n\r\n    data_gen = FakeDataGenerator(batch_size,\r\n                                 high_res_shape,\r\n                                 low_res_shape)\r\n\r\n    model = TestGAN(high_res_shape,\r\n                    low_res_shape,\r\n                    1.0e-4,\r\n                    1.0e-4)\r\n\r\n    model.compile(run_eagerly=True)\r\n\r\n\r\n    callbacks_list = []\r\n\r\n    callbacks_list.append(ModelPrinter(model))\r\n        \r\n    history = model.fit(data_gen, \r\n                            steps_per_epoch=100, \r\n                            epochs=3,\r\n                            callbacks=callbacks_list, \r\n                            initial_epoch=0, \r\n                            verbose=1)\r\n\r\n```\r\n\r\nSame problem in output (note I added some newlines in the output below to make the trains steps easier to separate visually):\r\n```\r\nEpoch 1/3\r\n\r\nTrain Step:  tf.Tensor(0.70294505, shape=(), dtype=float32) tf.Tensor(0.47407043, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.7029450535774231, 'g_loss': 0.4740704298019409}\r\n  1/100 [..............................] - ETA: 0s - d_loss: 0.7029 - g_loss: 0.4741\r\n\r\nTrain Step:  tf.Tensor(0.6781775, shape=(), dtype=float32) tf.Tensor(0.4736327, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.6781774759292603, 'g_loss': 0.47363269329071045}\r\n  2/100 [..............................] - ETA: 16s - d_loss: 0.6906 - g_loss: 0.4739\r\n\r\nTrain Step:  tf.Tensor(0.65583074, shape=(), dtype=float32) tf.Tensor(0.47412047, shape=(), dtype=float32)\r\nBatch End:  {'d_loss': 0.6558307409286499, 'g_loss': 0.47412046790122986}\r\n  3/100 [..............................] - ETA: 21s - d_loss: 0.6790 - g_loss: 0.4739\r\n\r\n```\r\n\r\n", "Cause the progress bar logic is different.\r\nIf you try to insert a `print(values)` in the progressbar update method you will see the same values as in your Train Step/Batch End:\r\nhttps://github.com/tensorflow/tensorflow/blob/f3462c311d2b5839a59f155ad82d3f3b3759da61/tensorflow/python/keras/utils/generic_utils.py#L530-L547\r\n\r\nThen follow the code to see how `info` is composed.\r\n", "Oh I think I see\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L650\r\n\r\nIt looks like its displaying a running mean instead of the raw value.  Maybe this just needs to be updated to a documentation update somewhere?  ", "It Is not so explicit but It is described in params at https://www.tensorflow.org/api_docs/python/tf/keras/utils/Progbar\n\nIf you want you can close this and open a documentation improvement ISSUE or directly a PR on Doc", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43469, "title": "reduce_sum and reduce_prod Permuting order of columns on Power9 CPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary - community build\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: V100  32gb\r\n\r\n\r\n**Describe the current behavior**\r\ntf.reduce_sum is permuting the columns when producing the result.  Stand-alone code below produces a tensor [3.,4.,1.,2.]\r\nThis only happens when computation is on CPU and we've only been able to reproduce it on our Power9 machines.\r\n\r\n**Describe the expected behavior**\r\ntf.reduce_sum should preserve the order of columns.  It should produce [1.,2.,3.,4.]\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.constant(np.array([[1.0,2.0,3.0,4.0],\r\n       [0.0,0.0,0.0,0.0]]),\r\n      dtype=tf.float32)\r\n\r\nout = tf.reduce_sum(x, axis=0)\r\nprint(out)\r\n```\r\n\r\nPrints:\r\n\r\n> tf.Tensor([3. 4. 1. 2.], shape=(4,), dtype=float32)\r\n<img width=\"469\" alt=\"Screen Shot 2020-09-22 at 6 57 33 PM\" src=\"https://user-images.githubusercontent.com/11985639/93945751-8407f800-fd05-11ea-8008-793b0c72084e.png\">\r\n\r\ntf.reduce_prod is producing similar results:\r\n<img width=\"470\" alt=\"Screen Shot 2020-09-22 at 6 59 50 PM\" src=\"https://user-images.githubusercontent.com/11985639/93945902-d9dca000-fd05-11ea-8496-e878adc64b19.png\">\r\n\r\n\r\nWe notice the same thing in more complicated examples breaking the UnitNorm constraint, as it's using reduce_sum when computing the norm and then dividing vectors by a permutation of the relevant lengths.", "comments": ["@ajyeager \r\nI ran the code shared above and get the expected output, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/8ef665342db639e5f5af3746e04df85a/untitled.ipynb), could you please share a colab gist with the error reported.", "@tfboyd Do you still have any Power team contact for this?", "@Saduf2019  it's only happening on our Power based machines - I don't think it'll be reproducible on colab.", "@ajyeager Looks like this is related to your `Power9` machines. I tried [colab](https://colab.research.google.com/gist/jvishnuvardhan/78015a8265a1fcafd4796e9f1ead5df6/untitled.ipynb), windows, and Mac and I could not reproduce the issue. \r\n\r\nThis is clearly not a bug or performance related issue related to TF. Thanks!", "@ajyeager I've tried your example with Docker/Qemu but It Is not working well with the official IBM image https://hub.docker.com/r/ibmcom/tensorflow-ppc64le", "I'm going to look into getting IBM involved, as I just tested and it happens in their official image too.", "@jayfurmanek   Jason was my contact in the past.", "There is a TF2.2 version in WML CE that doesn't have this problem:\r\nhttps://public.dhe.ibm.com/ibmdl/export/pub/software/server/ibm-ai/conda-early-access/\r\n\r\nThere are no patches in that build for this problem. The ppc64le community builds recently moved to the manylinux2014 compliant quay.io container, which has a devtoolset-8 compiler toolchain. The WMLCE one above uses Anaconda's 7.3 based toolchain. \r\n\r\nI've not proven it, but this problem is likely because of that devtoolset-8 compiler..", "@ajyeager Is this still an issue? \r\n\r\nPlease close the issue If this was already resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43469\">No</a>\n"]}, {"number": 43466, "title": "Fix casting condition in Embedding layer", "body": "As it appears, the current condition for casting is flawed due to using `and` operator, i.e. the `inputs` cannot have both `int32` and `int64` data-type at the same time hence making the condition always evaluating to `True` and doing unnecessary work when the `inputs` is already of type integer.", "comments": []}, {"number": 43465, "title": "Is there any user data that is either being collected or Tracked by the tensorflow", "body": "As part of the iOS 14 /AppTracking Transparency framework, would like to know if any kind of the user data is collected or Tracked by the tensorflow.\r\n\r\nAs mentioned in the Apple (Ref: https://developer.apple.com/support/app-privacy-on-the-app-store/),\r\nbelow were some of the examples for Tracking:\r\n\r\n- Sharing device location data or email lists with a data broker.\r\n- Placing a third-party SDK in our app that combines user data from the app with user data from other developers\u2019 apps to target advertising or measure advertising efficiency, even if we don\u2019t use the SDK for these purposes. For example, using a login SDK that repurposes the data it collects from the app to enable targeted advertising in other developers\u2019 apps.\r\n\r\nThe following situations are not considered tracking:\r\n\r\n- When the data is linked solely on the end-user\u2019s device and is not sent off the device in a way that can identify the end-user or device.\r\n- When the data broker uses the data shared with them solely for fraud detection or prevention or security purposes, and solely on our behalf.", "comments": ["> Is there any user data that is either being collected or Tracked by the tensorflow?\r\n\r\nNo. Absolutely not."]}, {"number": 43464, "title": "'Tensor' object has no attribute 'numpy'", "body": "I'll just post this as bug issue because if I post other issues you guys would just tell me to post at stackoverflow every time while stackoverflow never helps and I actually tried many methods they had posted before I open this issue. \r\n\r\nSo I try to print the output of each layer and got this error. `tf.executing_eagerly` and `model.run_eagerly` are all TRUE. I've checked most issues on the github and this situation is rare so those just don't help. Here is the code(very simple). \r\n[model_print_data.txt](https://github.com/tensorflow/tensorflow/files/5263474/model_print_data.txt)\r\nThis may provide some details. Much appreciated.\r\n", "comments": ["Before we start to investigate this we need a very, very minimal standalone example to reproduce your issue that we could just copy, paste and run or a colab (if possible without filesystem dependencies as we prefer dummy inputs).", "Hi, @bhack Thanks! I'll attach the files. Thanks so much for not ignoring me again. This is supposed to be run on bash with venv. \r\n[test.zip](https://github.com/tensorflow/tensorflow/files/5263587/test.zip)\r\n**The step to install enviroment**\r\n\r\n```\r\npip install virtualenv\r\nvirtualenv -p python3 ./venv\r\nsource ./venv/bin/activate\r\npip install --upgrade pip\r\npip install tensorflow==2.3.0\r\npip install pydot\r\npip install pydotplus\r\nsudo apt-get install graphviz\r\npip install graphviz\r\npip install frozendict\r\npip install numpy\r\npip install absl-py\r\n```\r\n\r\nJust run \r\n```\r\npython -m kws_test.train.model_print_data \\\r\n--data_dir ./data_test \\\r\n--train_dir ./kws_print \\\r\n--window_size_ms 40.0 \\\r\n--window_stride_ms 20.0 \\\r\n--mel_num_bins 80 \\\r\n--dct_num_features 30 \\\r\n--resample 0.15 \\\r\n--time_shift_ms 100 \\\r\n--train 0 \\\r\n--debug 1 \\\r\n--generate 0 \\\r\n--print 1 \\\r\n--kernel_size '(3,1)' \\\r\n--first_kernel '(1,1)' \\\r\n--channels '36, 36, 36, 36, 36, 36, 36' \\\r\n--groups 1 \\\r\n--mobile 0 \\\r\n--residual 0 \\\r\n--dropout 0.0 \\\r\n&> ./kws_print.txt\r\n\r\n```\r\nat the `test` folder and you can reproduce 100% the same.", "@tu1258 I think it is too articulated. You really need **to minimize** the code surface to let use to reproduce your problem. If not we need to route you on the Stackoverflow channel.", "Hi, @bhack I minimized the code. There is only `model_print_data.py`. `model` folder and `best_weights` are being loaded to predict. `yes_data.csv` is the test data.\r\n[test.zip](https://github.com/tensorflow/tensorflow/files/5265791/test.zip)\r\nyou just run \r\n```\r\npython model_print_data.py \\\r\n&> ./kws_print.txt\r\n\r\n```\r\nIf you comment the 2 line below in `model_print_data.py`, the code can be run without error.\r\n```\r\nfor i in range(len(model.layers)):\r\n  print(model.get_layer(index=i).output.numpy) \r\n```", "@tu1258 \r\n\r\nPlease, see similar issue #27519 and [SO link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy) and see if it helps you.\r\n\r\nCan you try with  `tf.compat.v1.disable_eager_execution() ` ans see if the issue still persists. Thanks!", "@ravikyram I've read 27519 countless times. \r\n\r\n> Can you try with tf.compat.v1.disable_eager_execution()\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/training/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/training/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 96, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 38, in main\r\n    print_test_data(flags)\r\n  File \"/home/training/r08943133/kws_test/train/model_print_data.py\", line 72, in print_test_data\r\n    predictions = model.predict(test_fingerprints)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\", line 992, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 714, in predict\r\n    callbacks=callbacks)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 386, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3825, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/training/r08943133/venv/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\r\n\t [[{{node feature_layer/data_frame/frame/Reshape}}]]\r\n  (1) Invalid argument: Input to reshape is a tensor with 16000 values, but the requested shape has 2048000\r\n\t [[{{node feature_layer/data_frame/frame/Reshape}}]]\r\n\t [[reshape_1/_187]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```", "I have tried in colab with TF version 2.3 and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/12c94eb10b1e53716332933e222a4647/untitled385.ipynb).Thanks!", "@tu1258 You code is not correct as you cannot retrieve the output layer in this way.\r\nIf you need to get layers outputs please use something like https://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction", "At first, thanks for the both reply. @bhack I've tried the method you mention above (you could actually see the comment line just  below those failed print code) and it still fails.\r\n\r\nNoted that those commented codes are not exactly what I run. I've tried with code below and got error `TypeError: call() got an unexpected keyword argument 'outputs'`\r\n```\r\n  intermediate_layer_model = model(inputs=model.input,\r\n                                              outputs=model.get_layer(index=0).output)\r\n  intermediate_output = intermediate_layer_model(test_fingerprints)\r\n  print(intermediate_output)\r\n```\r\nMuch appreciated!\r\n\r\nEdit: Solved. Shame on me. Just replace `model(inputs=model.input, outputs=model.get_layer(index=0).output)` with `tf.keras.Model(inputs=model.input, outputs=model.get_layer(index=0).output)`\r\nBig thanks for both guy, I've struggled with this for 2 weeks.", "@tu1258 Can you close this ticket now?\r\n\r\nAlso about your initial claim\r\n> I'll just post this as bug issue because if I post other issues you guys would just tell me to post at stackoverflow every time while stackoverflow never helps and I actually tried many methods they had posted before I open this issue.\r\n\r\nI think that it is hard also for stackoverflow if you open questions with a not very very minimized code/use case. \r\nGenerally you to need to investigate on your side and minimize your code cause most of the time you can reproduce/redouce your problem with very few lines of code (as in this case you have still too much code distraction in your zip). Any other additional line generate noise and it requires more time for Stackoverflow contributors or ticket triaging so you will have lower probability to recieve support.", "I'll close it. \r\n\r\n> I think that it is hard also for stackoverflow if you open questions with a not very very minimized code/use case.\r\n\r\nTo be honest, I didn't plan to post code in a regular issue. This is the first time since I think there are already lots of same issue and I still couldn't solve it after reading them. (it turns out that I seem to have some misunderstand though, I apologize) I understand the pain of looking into code written by others. That's why I only attach that `model_print_data.py` because I didn't expect you guys to reproduce the error. I only expect to get some advice. And if we look back from now, the issue can be solved based on what I attached if I explained it a bit. I am sorry that I didn't explain my code is just an \"load model\" thing.\r\n\r\nAbout stackoverflow, I think many issue (that are recommended to post on stackoverflow, usually usage problem) are because of lack of document. Not saying tf is bad, because building complete document is impossible. We all know those are not bug report or feature request and not improving the tensorflow framework. But what can they do what if stackoverflow can't solve problems? Users can only come here for help. I believe people who are willing to spend time opening an issue have already searched for answer on google/stackoverflow.\r\n\r\n> Generally you to need to investigate on your side and minimize your code\r\n\r\nThanks for help and suggestion, I'll do it next time. Hopefully there is no next time \ud83d\ude04 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43464\">No</a>\n"]}, {"number": 43463, "title": "Flower_Classification_with_TFLite_Model_Maker: missing labels.txt", "body": "The Tutorial and the Notebook state that I need a labels.txt file that should be generated along with the model.tflite but is not:\r\n\r\n> After this simple 4 steps, we can now download the model and label files\r\n\r\nhttps://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/ml/Flower_Classification_with_TFLite_Model_Maker.ipynb\r\n\r\n>Copy the TensorFlow Lite model model.tflite and label.txt that you trained earlier to assets\r\n\r\nhttps://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android\r\n\r\nI tried\r\n\r\n`model.export(export_dir='/content/export/tflite_quant', label_filename='labels.txt', tflite_filename='model_quant.tflite', quantization_config=config)`\r\n\r\nand it says `INFO:tensorflow:Label file is inside the TFLite model with metadata.` in the log.\r\n\r\nSo is this a new feature? Is the labels.txt no longer required? What changes need to be made?\r\n\r\nI manually created the labels.txt for the app and it worked, but I am not sure if it works without.\r\n\r\n\r\n---\r\n\r\nAlso\r\n\r\nlite/codelabs/flower_classification/start/app/src/main/assets/\r\n\r\nchanged to\r\n\r\nlite/codelabs/flower_classification/android/start/app/src/main/assets\r\n\r\nin https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#4\r\n", "comments": ["Perhaps this can help with the [codelab approach](https://github.com/tensorflow/examples/blob/master/lite/codelabs/flower_classification/android/finish/app/src/main/assets/labels.txt)\r\nI see there are multiple examples for implementing flowers recognition problem such as using [codelab](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#2) or [ModelMaker](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification) approach.\r\nSee https://www.tensorflow.org/lite/models/image_classification/overview#customize_model", "@ymodak I know it is not difficult to write a labels.txt . I did not find any information why the labels.txt file is no longer created.\r\nSame issue and workaround: https://github.com/tensorflow/tensorflow/issues/41470\r\n\r\nIt would be nice if the tutorial/colab would be updated or the labels creation be fixed.", "Thanks for the report I see it fails to generate labels file along with the tflite file. ", "labels are embedded in tflite metadata.\r\nuse `model.export(export_dir='.', with_metadata=False)` instead of `model.export(export_dir='.')` to get `labels.txt`\r\nCheck the source code for why, https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/core/task/custom_model.py#L130-L177\r\n", "@freedomtan Thanks, in this case, the Colab notebook should be adjusted accordingly.\r\n", "@natowi I think originally the Jupyter notebook was correct. The behavior of model maker changed. ", "@freedomtan Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43463\">No</a>\n"]}, {"number": 43462, "title": "Tensorflow developer certificate", "body": "Can we refer some resources like stackover flow(for any errors) or our noted material while taking the exam?", "comments": ["@gharshini \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "I have seen a similar question in github and it was answered. Can you please answer this"]}, {"number": 43461, "title": "g++ compilation results in undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal - collect2: error: ld returned 1 exit status", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 on docker (image: `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.3\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: RTX 2080 Super - 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm compiling  a very simple C++ program and linking it to the tensorflow_cc library. However i get a compilation error`\r\n\r\nCompiling the source file using the following:\r\n\r\n```\r\n# g++ -I /opt/tensorflow/lib/include/ -L /opt/tensorflow/lib/ -ltensorflow_cc -ltensorflow_framework -lstdc++ -L/usr/local/lib/ -L /usr/lib/x86_64-linux-gnu/ -o /object_detection/load_saved_model /object_detection/load_saved_model.cpp\r\n```\r\n\r\nresults in:\r\n\r\n```\r\n/tmp/ccz16e32.o: In function `tensorflow::core::RefCounted::~RefCounted()':\r\nload_saved_model.cpp:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0xf4): undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\nload_saved_model.cpp:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0x11c): undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccz16e32.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\nload_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x33): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\nload_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x5d): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\nload_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x7b): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\nload_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x8a): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\nload_saved_model.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0xad): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. `git clone https://github.com/tensorflow/tensorflow.git `\r\n2. `git checkout r2.3`\r\n3. `git checkout r2.3`\r\n4. `bazel clean`\r\n5. \r\n```\r\nroot@167c90dbc0fe:/tensorflow# ./configure\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.6/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]\r\n/usr/local/lib/python3.6/dist-packages\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: N\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.1/targets/x86_64-linux/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 3.5, 7.0, 7.5\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: N\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n6. \r\n```\r\nbazel build --jobs=8 --config=v2 --copt=-O3 --copt=-m64 --copt=-march=native --verbose_failures //tensorflow:install_headers //tensorflow:tensorflow //tensorflow:tensorflow_cc //tensorflow:tensorflow_framework //tensorflow/tools/lib_package:libtensorflow\r\n```\r\n\r\n`Target //tensorflow:libtensorflow_cc.so up-to-date:\r\n  bazel-bin/tensorflow/libtensorflow_cc.so\r\nINFO: Elapsed time: 3582.131s, Critical Path: 175.91s\r\nINFO: 11806 processes: 11806 local.\r\nINFO: Build completed successfully, 16884 total actions\r\n`\r\n\r\n7. \r\n```\r\n g++ -I /opt/tensorflow/lib/include/ -L /opt/tensorflow/lib/ -ltensorflow_cc -ltensorflow_framework -lstdc++ -L/usr/local/lib/ -L /usr/lib/x86_64-linux-gnu/ -o /object_detection/load_saved_model /object_detection/load_saved_model.cpp\r\n```\r\n\r\n**Any other info / logs**\r\nload_saved_model.cpp\r\n\r\n```\r\n#include <stdlib.h>\r\n#include <stdio.h>\r\n#include <string>\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n\r\nint  main(int argc, char* argv[]){\r\n    printf(\"Will load and serve a saved model...\");\r\n}\r\n```", "comments": ["you should put library related flags after source file to make linker happy.\r\n\r\n```\r\ng++ -I /opt/tensorflow/lib/include/ -o /object_detection/load_saved_model /object_detection/load_saved_model.cpp -L /opt/tensorflow/lib/ -ltensorflow_cc -ltensorflow_framework -lstdc++ -L/usr/local/lib/ -L /usr/lib/x86_64-linux-gnu/\r\n```", "@freedomtan That was it, thanks a ton!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43461\">No</a>\n"]}, {"number": 43459, "title": "ModuleNotFoundError: No module named 'tensorflow'", "body": "-using anaconda python v3.8\r\n-working with jupyter notebook \r\n-try to add on cmd system --- conda install tensorflow but loading old version tensorflow and than give to me this error code:\r\n\r\n```\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n```\r\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-62c45ef75a16> in <module>\r\n----> 1 from tensorflow.keras.models import Sequential\r\n      2 #modelleri olu\u015fturmak i\u00e7in\r\n      3 from tensorflow.keras.layers import Dense\r\n      4 #katmanlar\u0131 da b\u00f6yle olu\u015ftururuz\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n\r\n", "comments": ["We don't directly support Anaconda here.\r\nPlease use our official install guide https://www.tensorflow.org/install/pip\r\n\r\nAnaconda has a third party support:\r\nhttps://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\r\n", "@PrivetMerhaba \r\nThis could ocuur due to different reasons, as the error log has not been shared, please refer to [below issues](https://stackoverflow.com/questions/46568913/tensorflow-import-error-no-module-named-tensorflow):\r\n#42761 - verify gast related issue.\r\n#42233 - verify if python/cpu is not 32 bit it should be 64 bits.\r\n#42367 - as you have python 3.8 ensure your tf version is 2.2 or above. [#27935 please verify your tf version]\r\n[jupyter](https://www.heatonresearch.com/2019/09/03/tf-no-module-jupyter.html) related \r\n\r\n\r\n\r\n", "You could be using a different installation of Python. Make sure you installed tensor flow to the correct spot and that you don't have multiple installations of Python.\r\n\r\nYou could consider using a venv to install TensorFlow and run your code using said venv.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43459\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43459\">No</a>\n"]}, {"number": 43458, "title": "XLA: Print nicer message when trying to traverse HLO computation", "body": "Sometimes when developing a new pass/optimization with a new computation being built some instructions from other computations might incorrectly end up in the graph traversal.\r\nPrint a nicer error message when that happens.", "comments": []}, {"number": 43457, "title": "Unexpected accuracy output from tf.keras.Model.evaluate called on a saved and loaded model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: N/A (run with x86_64 CPU)\r\n\r\n\r\n**Describe the current behavior**\r\nA model is trained for a multiclass classification task. Labels are a class vector. Method [evaluate](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) called on the trained model outputs an accuracy of 0.9579. After saving the model and loading the saved model, calling method `evaluate` on the loaded model outputs an accuracy of 0.0997.\r\n\r\n**Describe the expected behavior**\r\nI would have expected the same accuracy output (0.9579) on the loaded model.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1L4vgn3H5PquUJEgBY3PCrlGEHSXd6Rtc\r\n\r\n**Other info / logs**\r\n", "comments": ["It is a know issue and it is fixed in `tf-nightly` https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-686769008\r\nIf you don't want to use nightly there is a workaround at https://github.com/tensorflow/tensorflow/issues/42045#issuecomment-674232499 ", "Thank you @bhack, and sorry I was not able to find that thread. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43457\">No</a>\n"]}, {"number": 43455, "title": "A commit message should be \"unit64\"", "body": "In the commit message: https://github.com/tensorflow/tensorflow/commit/4be466a87efc152a8581febe7c1deaae562465af#diff-f0675f2568ff9470bcfc1f2bc79c5386, \r\n\r\nI find that it said \"int64\". But when I check the API documentation, it seems that it should be \"unit64\". Please look at parameter description: TF2.3:https://www.tensorflow.org/api_docs/python/tf/Variable#__mod__\r\nTF2.2:  https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/Variable#__mod__", "comments": ["Hi @zjzh, I'm having trouble trying to figure out what exactly you are talking about. Are you trying to say that the function `bincount` should not support non-negative ints as a `dtype`?", "Hi  @Harsh188,  thanks for your reply. \r\nYes, it should support. But the commit message say: \"and support int64.\", which seems that it does not support the int64 in past. But if we check the documentation changes: TF2.3:https://www.tensorflow.org/api_docs/python/tf/Variable#__mod__\r\nTF2.2: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/Variable#__mod__. We can see that the \"x\" parameter description add \" uint64\". The information is inconsistent with API documentation. If the information of commit message is wrongly writes \"and support int64\", it should be \"and support uint64\".", "@zjzh Sorry for my delayed response. \r\n\r\nDon't worry too much about the details of a commit message. I just checked `bincount`, it still raises an error if you input an array which contains negative elements.\r\n\r\nThe message does not try to indicate that arrays with negative values will be supported. I think this issue can be closed.", "Thanks. If the docs are accurate then we can close this.", "@Harsh188, @lamberta, Thanks for your explanation. It makes me understand that developers could make it support more data types but does not mean that it support negative elements.\r\n\r\nHowever, I am confused about whether there is the  inconsistency between commits and documentation, commits and code changes. Specifically, you know, the difference of TF 2.2 [\"tf.Variable.__mod__\"](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/Variable#__mod__) and TF 2.3[\"tf.Variable.__mod__\"](https://www.tensorflow.org/api_docs/python/tf/Variable#__mod__) is that \"x\" parameter description is add \"uint64\". When I check the commit messages about the change, I localize the [commit ](https://github.com/tensorflow/tensorflow/commit/4be466a87efc152a8581febe7c1deaae562465af#diff-f0675f2568ff9470bcfc1f2bc79c5386)  that shows that  the documentation added the description \"uint64\". However, the commit message make me confuse because the commit message is \"int64\". Therefore, I want to know that whether the description of documentation is inconsistent with the commit message or or something."]}, {"number": 43454, "title": "Do not create unnecessary intermediate lists in tf.nest", "body": "`tf.nest` creates intermediate lists for iterating over elements. This PR changes the list comprehensions to generator expresions in places where they are only used to iterate over once inside the same function. This makes the iteration lazy and removes the need for creation of intermediate lists.", "comments": []}, {"number": 43453, "title": "InaccessibleTensorError: The tensor cannot be accessed here: it is defined in another function or code block.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04 LTS (GNU/Linux 4.4.0-18362-Microsoft x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI was trying to create a custom model using the Subclassing API by subclassing the tf.keras.models.Model class. After creating the model and compiling, while training the model the following issue was raised\r\n`InaccessibleTensorError: The tensor 'Tensor(\"mul:0\", shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=139645200248064); accessed from: FuncGraph(name=train_function, id=139645180620608).`\r\n\r\n**Describe the expected behavior**\r\nI have tried using the Subclassing API before but i haven't seen such an error raised. Maybe some new updates in TF.\r\n\r\n**Standalone code to reproduce the issue**\r\nLink to the gist\r\n[https://gist.github.com/Gokul-S-Kumar/4834f19e78a3415caf8612327812b599](url)\r\n\r\n**Other info/logs** \r\nLink to the error description, in case useful:\r\n[https://gist.github.com/Gokul-S-Kumar/8d36b7fd40aded0fbc7a7a3c3a3a1baa](url)\r\n\r\nPS:- I may be committing a simple mistake, if yes please help in pointing it out so that I can rectify it.", "comments": ["@Gokul-S-Kumar \r\n\r\nPlease, refer #32477 and see if it helps you.Thanks!\r\n\r\n", "@Gokul-S-Kumar You code example it is not runnable:\r\n`NameError: name 'train_test_split' is not defined`\r\n\r\nWe need a very minimal standalone or colab example that we could copy, paste and run (so please prefer \"dummy inputs\" that doesn't rely on filesystem) to reproduce your issue.\r\n", "> @Gokul-S-Kumar You code example it is not runnable:\r\n> `NameError: name 'train_test_split' is not defined`\r\n> \r\n> We need a very minimal standalone or colab example that we could copy, paste and run (so please prefer \"dummy inputs\" that doesn't rely on filesystem) to reproduce your issue.\r\n\r\n@bhack Sorry for the error as some of the imports were missing. I have updated the gist (link given below):\r\n[https://gist.github.com/Gokul-S-Kumar/4834f19e78a3415caf8612327812b599](url)", "> @Gokul-S-Kumar\r\n> \r\n> Please, refer #32477 and see if it helps you.Thanks!\r\n\r\n@ravikyram Thanks for the response but this reference isn't of much help as I couldn't figure out any solution to my case from it. Maybe because the said issue revolves around BatchNormalization( ) being used in custom layers whereas mine doesn't employ BN layers at all.", "@Gokul-S-Kumar Who is calling your build?\r\nRemember that:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L325-L328", "> @Gokul-S-Kumar Who is calling your build?\r\n> Remember that:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L325-L328\r\n\r\n@bhack That works!! Thanks a lot for letting me know the details as I am a beginner whos finding his way around the framework :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43453\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43453\">No</a>\n", "I am facing the same issue, so what is the solution? Could you explain it?"]}]