[{"number": 19354, "title": "building tensorflow r1.4  RelWithDebInfo on Window 10 Fails", "body": "Hi Guys,\r\n\r\nI was able to build Tensorflow r1.4 in Release on Windows 10 with the following successfully.\r\nPython 3.5\r\nCuda 8.0, Cudnn 6.0\r\nVisual Studio 2015\r\n\r\nHowever, I clone the Tensorflow repository, checked out r1.4 and try to build RelWithDebInfo with the following commands:\r\n\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=\"C:\\\\swigwin-3.0.12\\\\swig.exe\" -DPYTHON_EXECUTABLE=\"C:\\\\Python35\\\\python.exe\" -DPYTHON_LIBRARIES=\"C:\\\\Python35\\\\libs\\\\python35.lib\" -DPYTHON_INCLUDE_DIR=\"C:\\\\Python35\\\\include\" -DNUMPY_INCLUDE_DIR=\"C:\\\\Python35\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\\\cudnn_80_60\"\r\n\r\nMSBuild /p:Configuration=RelWithDebInfo tf_tutorials_example_trainer.vcxproj\r\n\r\nCmake was able to generate the visual studio files.\r\n\r\nHowever, I ended up with the following compilation error:\r\n\"C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\" (default target) (1) ->\r\n\"C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj\" (default target) (114) ->\r\n(ClCompile target) ->\r\n  C:\\tensorflow_1_4\\tensorflow\\tensorflow\\core\\kernels\\training_ops.cc(3248): error C2471: cannot update program database 'C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\nuild\\tf_core_kernels.dir\\RelWithDebInfo\\tf_core_kernels.pdb' [C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n\r\nCan I please get some help on this?  Thank you very much in advance!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi @tensorflowbutler and @shivaniag,\r\n\r\nPlease see below for the requested information.  Please let me know if you have any further question on how to recreate the issue.  Thank you!\r\n\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.4\r\nBazel version: N/A (used cmake and VS2015)\r\nCUDA/cuDNN version: Cuda 8, cuDNN 6.0\r\nGPU model and memory: GPU model Titan Xp, memory 64GB\r\n\r\nExact Commands:\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=\"C:\\swigwin-3.0.12\\swig.exe\" -DPYTHON_EXECUTABLE=\"C:\\Python35\\python.exe\" -DPYTHON_LIBRARIES=\"C:\\Python35\\libs\\python35.lib\" -DPYTHON_INCLUDE_DIR=\"C:\\Python35\\include\" -DNUMPY_INCLUDE_DIR=\"C:\\Python35\\Lib\\site-packages\\numpy\\core\\include\" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\cudnn_80_60\"\r\n\r\nThis was fine,  however, the build command failed:\r\nMSBuild /p:Configuration=RelWithDebInfo tf_tutorials_example_trainer.vcxproj\r\n\r\nPlease see below for the error message:\r\n\"C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\" (default target) (1) ->\r\n\"C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj\" (default target) (114) ->\r\n(ClCompile target) ->\r\nC:\\tensorflow_1_4\\tensorflow\\tensorflow\\core\\kernels\\training_ops.cc(3248): error C2471: cannot update program database 'C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\b\r\nuild\\tf_core_kernels.dir\\RelWithDebInfo\\tf_core_kernels.pdb' [C:\\tensorflow_1_4\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj]\r\n\r\n\r\nThank you very much for your help!\r\n", "I have the same issue with 1.7.1...\r\n\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.7.1\r\nBazel version: N/A (used cmake and VS2017)\r\nCUDA/cuDNN version: Cuda 9, cuDNN 7.1.3\r\nGPU model and memory: 1070 MaxQ\r\n\r\nCommands...\r\ncmake .. -A x64 -G \"Visual Studio 15 2017\" -T host=x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo ^\r\n-Dtensorflow_ENABLE_GPU=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=ON -Dtensorflow_BUILD_SHARED_LIB=ON ^\r\n-Dtensorflow_CUDA_VERSION=9.0 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 ^\r\n-DCMAKE_INSTALL_PREFIX=\"D:\\TensorFlow\\tensorflow-1.7.1\\tensorflow\\contrib\\cmake\\build\\install\" ^\r\n-DSWIG_EXECUTABLE=\"C:\\Program Files\\swigwin-3.0.12\\swig.exe\" ^\r\n-DPYTHON_EXECUTABLE=\"C:\\Python36\\python.exe\" ^\r\n-DPYTHON_LIBRARIES=\"C:\\Python36\\libs\\python36.lib\" ^\r\n-DPYTHON_INCLUDE_DIR=\"C:\\Python36\\include\" ^\r\n-DNUMPY_INCLUDE_DIR=\"C:\\Python36\\Lib\\site-packages\\numpy\\core\\include\" ^\r\n-DCUDA_HOST_COMPILER=\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.11.25503\\bin\\HostX64\\x64\\cl.exe\"\r\n\r\nMSBuild /p:Configuration=RelWithDebInfo /p:Platform=x64 /m:6 INSTALL.vcxproj /p:PreferredToolArchitecture=x64\r\n\r\nThe RelWithDebInfo build fails with the error:\r\n\u201ctraining_ops.cc(3508): error C2471: cannot update program database \u2026 tf_core_kernels.pdb\u201d\r\n\r\n(I tried adding CXXFLAGS=/FS but this didn't appear to solve the problem)\r\n\r\nRelease builds fine. Any help with debug support would be much appreciated!\r\n\r\n", "This is broken for Tensorflow 1.5 as well.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.5\r\nBazel version: N/A\r\nCUDA/cuDNN version: CUDA 9.0, cuDNN 7.1.2\r\nGPU model and memory:  GPU model Titan Xp, memory 64GB\r\nExact command to reproduce:\r\n\r\nCmake:\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DSWIG_EXECUTABLE=\"C:\\\\swigwin-3.0.12\\\\swig.exe\" -DPYTHON_EXECUTABLE=\"C:\\\\Python35\\\\python.exe\" -DPYTHON_LIBRARIES=\"C:\\\\Python35\\\\libs\\\\python35.lib\" -DPYTHON_INCLUDE_DIR=\"C:\\\\Python35\\\\include\" -DNUMPY_INCLUDE_DIR=\"C:\\\\Python35\\\\Lib\\\\site-packages\\\\numpy\\\\core\\\\include\" -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\\\cudnn_90_712\"\r\n\r\nBuild:\r\nMSBuild /p:Configuration=RelWithDebInfo tf_tutorials_example_trainer.vcxproj\r\n\r\nBuild fails with the following error:\r\n\"C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\" (default target) (1) ->\r\n\"C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_stream_executor.vcxproj\" (default target) (120) ->\r\n  C:\\tensorflow_1_5\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc(137): warning C4068: unknown pragma [C:\\tensorflow_1_5\\tensorflow\\tensorflow\\co\r\nntrib\\cmake\\build\\tf_stream_executor.vcxproj]\r\n  C:\\tensorflow_1_5\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc(138): warning C4068: unknown pragma [C:\\tensorflow_1_5\\tensorflow\\tensorflow\\co\r\nntrib\\cmake\\build\\tf_stream_executor.vcxproj]\r\n  C:\\tensorflow_1_5\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc(224): warning C4068: unknown pragma [C:\\tensorflow_1_5\\tensorflow\\tensorflow\\co\r\nntrib\\cmake\\build\\tf_stream_executor.vcxproj]\r\n  C:\\tensorflow_1_5\\tensorflow\\tensorflow/stream_executor/lib/statusor.h(191): warning C4180: qualifier applied to function type has no meaning; ignored (com\r\npiling source file C:\\tensorflow_1_5\\tensorflow\\tensorflow\\stream_executor\\plugin_registry.cc) [C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\t\r\nf_stream_executor.vcxproj]\r\n\r\n\r\n\"C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_tutorials_example_trainer.vcxproj\" (default target) (1) ->\r\n\"C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernels.vcxproj\" (default target) (118) ->\r\n(ClCompile target) ->\r\n  C:\\tensorflow_1_5\\tensorflow\\tensorflow\\core\\kernels\\training_ops.cc(3500): error C2471: cannot update program database 'C:\\tensorflow_1_5\\tensorflow\\tenso\r\nrflow\\contrib\\cmake\\build\\tf_core_kernels.dir\\RelWithDebInfo\\tf_core_kernels.pdb' [C:\\tensorflow_1_5\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_kernel\r\ns.vcxproj]\r\n\r\n    20057 Warning(s)\r\n    1 Error(s)\r\n\r\n\r\nThank you very much for your help!", "@gunan could you take a look into this.", "As far as I know, this has never worked.\r\n@mrry @guschmue may be able to explain why.", "RelWithDebInfo used to work fine, it was the most practical way for debugging.\r\nLooks it doesn't work anymore because multiple builds of tf_core_kernels try to update the same pdb.\r\nSomething wrong with the dependencies I assume.\r\nI can see if I can find some time to look at it.", "Hi, gentle prompt... is there a work around that can be used in the build? Many thanks.", "i have tried changing and building with /FS option, it shows the same behavior", "if i set all cc files to produce different pdb the error does not happen. also if /mp option is disabled everything is fine, the build time is slower", "I have disabled the /MP flag which was a clue, got the error again. Seems to me that the pdb generated is just too big, because it is around 2GB\r\ntf_core_kernels fails, did not check other libraries", "Seems to me that splitting the kernels library will be good solution, also looks to me that on windows we are in a desperate need for precompiled headers, building on 8 core machine takes too much time also.", "So i have split tf_core_kernels in two, the errors are gone. Other tools now fail like transform_graph. But the problem is obvious. Tensorflow in this shape is a monster. Need to be split in several dlls to reduce these issues. Will see more these days, but the transform graph pdb is 3gb on my machine", "actually i wanted to compile it and debug one simple inference example and now i got this two week effort to build a pdb :). ", "so i have split tf_core_kernels  into 6 libraries, they compile fine, but after that tensorflow.pdb cannot be generated, because it gets above 3GB. So removing the pdb for now will work. I have discovered also that there is one script for version_info.cc, which is executed on every build and should be executed only once from a build machine. putting it in make,sh or something similar will be better.", "@kingofthebongo2008 if you only want to compile it and debug one simple inference example, you can simply comment out all the \"\\tensorflow\\core\\kernels\\training_ops.cc\" file. That fix worked for me!\r\n\r\n", "@vincentlabonte sadly not for me,  I still get subsequent errors\r\ne.g ...\\range_coder.cc(375): error C2471: cannot update program database ....\\tf_core_kernels.pdb\r\n\r\nWhat build options did you use? thanks!\r\n", "as a work around for now, just disable the debug info for tf_core_kernels\nmanually in visual studio\nIn general this library needs to be split in multiple dll's. Still not sure\nabout its dependencies.\n\nOn Sat, 14 Jul 2018 at 21:35, jrs-synth <notifications@github.com> wrote:\n\n> @vincentlabonte <https://github.com/vincentlabonte> sadly not for me, I\n> still get subsequent errors\n> e.g ...\\range_coder.cc(375): error C2471: cannot update program database\n> ....\\tf_core_kernels.pdb\n>\n> What build options did you use? thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19354#issuecomment-405041717>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cW1mx4z1iibGteVLHEYm4laM2qfCks5uGjoPgaJpZM4UDGvI>\n> .\n>\n", "@jrs-synth You are right my work around only works with -Dtensorflow_ENABLE_GPU=OFF... ", "I am working in a branch to split some tensorflow in parts in dlls.\n\nSo far the kernels are registered in a GlobalRegistry which is a multimap.\nThis can go in a dll. If it goes there, then every kernel can be made as a\ndll and will have own pdb and the problem will disappear.\n\nFor this to happen.\n\n1. proto files must be in a different dll like tf_proto.dll\n2. tf_kernel_registry.dll must be made\n3. tf_core_kernels must be split.\n\n\n\n\n\n\nOn Thu, 19 Jul 2018 at 23:19, Vincent Labont\u00e9 <notifications@github.com>\nwrote:\n\n> @jrs-synth <https://github.com/jrs-synth> You are right my work around\n> only works with -Dtensorflow_ENABLE_GPU=OFF...\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19354#issuecomment-406400577>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5ccbLcK0Fs5xrk9sQ54ThJ_46gD4Hks5uIOnbgaJpZM4UDGvI>\n> .\n>\n", "Actually we are also planning to do that.\r\nPlease see the design here:\r\nhttps://github.com/tensorflow/community/pull/2", "This is quite, good. Hope will come soon, In the mean time i will fix it\nlocally.\n\nOn Fri, 20 Jul 2018 at 10:05, Gunhan Gulsoy <notifications@github.com>\nwrote:\n\n> Actually we are also planning to do that.\n> Please see the design here:\n> tensorflow/community#2 <https://github.com/tensorflow/community/pull/2>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19354#issuecomment-406507860>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABF5cYAjm9Ubk6oCpRKZxJbMIzDZutURks5uIYEqgaJpZM4UDGvI>\n> .\n>\n", "I think splitting into multiple dlls is the only way to handle the large pdbs. \r\nOne issue that might come with this is the symbol exports from the dll done by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tools/create_def_file.py ... it is good enough to handle kernels in _lstm_ops.dll  but I'm not sure if it exports enough symbols to keep all kernels happy. In case additional symbols need to be exported, there is a 64k limit on the number of symbols and for gpu builds 59k are used today ... not much room to grow.\r\nIn case you need a workaround to just debug a specific issue: I used to have the issue with the pdb size during the port of the cuda to windows and could get around it by generating symbols selective in cmake for the modules I needed. Or alternatively you can delete the pdb's you don't care about and re-link with  /p:BuildProjectReferences=false ... total hack of course.", "@kingofthebongo2008 I believe you mean in the tf_core_kernels project,  \"Configuration Properties > Linker > Debugging> Generate Debug Info\" set to No.  However, I do not see the Linker tab in the \"tf_core_kernels\" project. Am I missing something?", "Nagging Assignees @gunan, @shivaniag: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for not being able to resolve this.\r\nCMake files we had were becoming too confusing to maintain.\r\nThey have been deprecated in favor of bazel on windows.\r\nI am sorry to close this issue as obsolete."]}, {"number": 19353, "title": "CMAKE+CPU on Ubuntu 18.04 build failure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master 51ef16057b4625e0a3e2943a9f1bbf856cf098ca\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.13.0 (Irrelevant)\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\n$ mkdir build && cd build\r\n$ cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n$ make -j 4\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhile trying to build tensorflow with cmake (CPU only) it looks like build is broken. From the log it seems that there are some eager dependency issues. \r\n\r\n### Source code / logs\r\n\r\n```\r\n# mkdir build && cd build\r\n# cmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\n# make -j 4\r\n\r\n[ 99%] Building CXX object CMakeFiles/summarize_graph.dir/home/ubuntu/tensorflow/tensorflow/tools/graph_transforms/summarize_graph_main.cc.o\r\n[ 99%] Linking CXX executable summarize_graph\r\nCMakeFiles/tf_core_cpu.dir/home/ubuntu/tensorflow/tensorflow/core/common_runtime/eager/eager_operation.cc.o: In function `tensorflow::EagerOperation::AddInput(tensorflow::TensorHandle*)':\r\neager_operation.cc:(.text+0x23b): undefined reference to `tensorflow::AttrBuilder::NumInputs(int)'\r\nCMakeFiles/tf_core_cpu.dir/home/ubuntu/tensorflow/tensorflow/core/common_runtime/eager/execute.cc.o: In function `tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::gtl::InlinedVector<tensorflow::TensorHandle*, 2>*, int*)':\r\nexecute.cc:(.text+0x2bf4): undefined reference to `tensorflow::AttrBuilder::CacheKey(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\nexecute.cc:(.text+0x32f4): undefined reference to `tensorflow::AttrBuilder::BuildNodeDef()'\r\nexecute.cc:(.text+0x4e3a): undefined reference to `tensorflow::OpDefForOp(char const*, tensorflow::OpDef const**)'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/summarize_graph.dir/build.make:2164: recipe for target 'summarize_graph' failed\r\nmake[2]: *** [summarize_graph] Error 1\r\nCMakeFiles/Makefile2:2214: recipe for target 'CMakeFiles/summarize_graph.dir/all' failed\r\nmake[1]: *** [CMakeFiles/summarize_graph.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n```\r\n", "comments": ["Duplicate of #19127 . It is because eager dependency on c api and current cmake code doesn't handle that. ", "Nagging Assignee @skye: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I tried to build with CMAKE again on Ubuntu 18.04 and now it works. Thanks!"]}, {"number": 19352, "title": "Failed to build version 1.8", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel build --config=opt//tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nTrying to build tensorflow v1.8 from sources got linking error.\r\n\r\nERROR: /home/alexey/tensorflow-1.8.0/tensorflow/python/BUILD:1582:1: Executing genrule //tensorflow/python:sparse_ops_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/python/gen_sparse_ops_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/python/gen_sparse_ops_py_wrappers_cc: undefined symbol: _ZN10tensorflow8str_util8EndsWithENS_11StringPieceES1_\r\n\r\n### Steps to reproduce\r\n1) Go to https://github.com/tensorflow/tensorflow\r\n2) Click \"Branch\" button, select \"Tags\" tab and choose v1.8.0 tag\r\n3) Click \"Clone or download\" > Download ZIP\r\n4) Unpack the archive to home folder and cd to tensorflow-1.8.0\r\n5) Run configure answering all options NO, except to jmalloc and python version/path (see the logs below)\r\n6) Build CPU version of tensorflow with \"bazel build --config=opt//tensorflow/tools/pip_package:build_pip_package\" command\r\n7) Wait for 2400 seconds and get the linking error mentioned above\r\n\r\nP.S. configure's inputs:\r\n\r\nalexey@Inspiron-7559:~/tensorflow-1.8.0$ ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.11.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n", "comments": ["The issue is still taking place on the latest code from git. Is there any way to fix that?", "The issue is still taking place on the latest code from git. Is there any way to fix that?\r\n\r\n", "Will you be OK with using Anaconda https://anaconda.org/anaconda/tensorflow-mkl? This already enables TensorFlow with Intel MKL DNN library acceleration.", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19351, "title": "Bidirectional beam search tensorflow implmentation", "body": "###  Problem Description :\r\n\r\nI read the paper named \"Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning\"  [Paper-Link](https://arxiv.org/pdf/1705.08759.pdf).\r\nI checked the official Tensorflow website for a feature similar to this one but didn't find any.\r\nSo is it possible we add this feature to Tensorflow or are there already similar implemented ones?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 19350, "title": "negative condition in tf.cond gets evaluated", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n2.7.14\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: none\r\n\r\n### Describe the problem\r\nwhen the output tensor of tf.cond is evaluated, it evaluates the tensors returned from both true and false conditions. This creates an unwanted behavior when used in combination with tf.data.Iterator.get_next. In such a situation both true and false tensors get iterated to the next element. \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\n\r\niterate = tf.placeholder(tf.bool, shape=())\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(range(1, 10))\r\niterator = dataset.make_one_shot_iterator()\r\n\r\ndata = iterator.get_next()\r\n\r\nempty_data = tf.constant(0)\r\n\r\ninput_data = tf.cond(iterate,\r\n                     true_fn=lambda: data,\r\n                     false_fn=lambda: empty_data)\r\n\r\nwith tf.Session() as sess:\r\n    print sess.run([input_data], feed_dict={iterate: True})\r\n    print sess.run([input_data], feed_dict={iterate: False})\r\n    print sess.run([input_data], feed_dict={iterate: False})\r\n    print sess.run([input_data], feed_dict={iterate: True})\r\n    print sess.run([input_data], feed_dict={iterate: False})\r\n    print sess.run([input_data], feed_dict={iterate: False})\r\n    print sess.run([input_data], feed_dict={iterate: False})\r\n    print sess.run([input_data], feed_dict={iterate: True})\r\n```\r\nOutput\r\n```\r\n[1]\r\n[0]\r\n[0]\r\n[4]\r\n[0]\r\n[0]\r\n[0]\r\n[8]\r\n```\r\nExpected Output\r\n```\r\n[1]\r\n[0]\r\n[0]\r\n[2]\r\n[0]\r\n[0]\r\n[0]\r\n[3]\r\n```\r\n", "comments": ["reading the docs again, it states that \r\n> Since z is needed for at least one branch of the cond, the tf.multiply operation is always executed, unconditionally. Although this behavior is consistent with the dataflow model of TensorFlow, it has occasionally surprised some users who expected a lazier semantics.\r\n\r\nThis covers the problem I am facing. Closing this issue."]}, {"number": 19349, "title": "Add mirror for nasm in workspace.bzl", "body": "The two specified URLs do not work, see discussion here #16862. This PR adds a working URL to the list of mirrors.\r\n\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Please make sure you sign a CLA before we can work on this PR. Thanks!", "I signed the CLA :)", "CLAs look good, thanks!\n\n<!-- ok -->", "As far as I can tell these tests failed for unrelated reasons"]}, {"number": 19348, "title": "Improve java documentation by explaining \"magic\" constants like \"\"save/control_dependency\"\"", "body": "In your examples [here](https://github.com/tensorflow/models/blob/master/samples/languages/java/training/src/main/java/Train.java) there is a statement:\r\n```\r\nsess.runner()\r\n     .feed(\"save/Const\", checkpointPrefix)\r\n     .addTarget(\"save/control_dependency\")\r\n     .run();\r\n```\r\nCan you explain somewhere in documentation what is the \"magic\" constants used there and list all the variants of them somewhere?", "comments": ["These aren't special constants, these are names printed by the script that generates the graph (see [code](https://github.com/tensorflow/models/blob/493d7a2e233fa874415062061fe0ce1aa62f2d3e/samples/languages/java/training/model/create_graph.py#L24)).\r\n\r\nSent https://github.com/tensorflow/models/pull/4297, hopefully that helps."]}, {"number": 19347, "title": "Segmentation fault (core dumped) on tf.Session()", "body": "I just installed TensorFlow and to test the installation, I tried the following code and as soon as I initiate the TF Session, I am getting the ***Segmentation fault (core dumped)*** error.\r\n\r\n    bafhf@remote-server:~$ python\r\n    Python 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \r\n    [GCC 7.2.0] on linux\r\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n    >>> import tensorflow as tf\r\n    /home/bafhf/anaconda3/envs/ismll/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n      from ._conv import register_converters as _register_converters\r\n    >>> tf.Session()\r\n    2018-05-15 12:04:15.461361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1349] Found device 0 with properties: \r\n    name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n    pciBusID: 0000:04:00.0\r\n    totalMemory: 11.17GiB freeMemory: 11.10GiB\r\n    Segmentation fault (core dumped)\r\n\r\nMy **nvidia-smi** is:\r\n\r\n    Tue May 15 12:12:26 2018       \r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  Tesla K80           On   | 00000000:04:00.0 Off |                    0 |\r\n    | N/A   38C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n    |   1  Tesla K80           On   | 00000000:05:00.0 Off |                    2 |\r\n    | N/A   31C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n    +-----------------------------------------------------------------------------+\r\n    | Processes:                                                       GPU Memory |\r\n    |  GPU       PID   Type   Process name                             Usage      |\r\n    |=============================================================================|\r\n    |  No running processes found                                                 |\r\n    +-----------------------------------------------------------------------------+\r\n\r\nAnd **nvcc --version** is:\r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n\r\n```\r\n\r\nAlso **gcc --version** is:\r\n\r\n    gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n    Copyright (C) 2015 Free Software Foundation, Inc.\r\n    This is free software; see the source for copying conditions.  There is NO\r\n    warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\nFollowing is my **PATH**:\r\n\r\n    /home/bafhf/bin:/home/bafhf/.local/bin:/usr/local/cuda/bin:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib:/home/bafhf/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n\r\nand the **LD_LIBRARY_PATH**:\r\n\r\n    /usr/local/cuda/bin:/usr/local/cuda/lib:/usr/local/cuda/extras/CUPTI/lib\r\n\r\n<br>I am running this on a server and I don't have root privileges. Still I managed to install everything as per the instructions on the official website.<br>\r\n\r\nSeems like the GPU is allocating memory for the process for a second and then the core segmentation dumped error is thrown.\r\n\r\n![terminal output](https://user-images.githubusercontent.com/34688139/40171744-7a623402-59cc-11e8-857a-f1c0b2286f2a.gif)\r\n\r\nI downgraded my tensorflow version from v1.8 to v1.5. The issue still remains.\r\n\r\n<br>Is there any way address or debug this issue?\r\n", "comments": ["How did you install tensorflow? From sources, using bazel/cmake?", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I first installed tensorflow in an anaconda env using conda. It gave me the **Segmentation fault** error. So dumped that conda env, created a new conda env and then used bazel to build and generate the the whl package to install it. It still gives me the same error. I followed the same process for version r1.5. Each time I tried something new, I removed the old conda env just to make sure I don't pick up anything residual from earlier process. And every time I am getting the same error.", "**Have I written custom code**: N/A\r\n**OS Platform and Distribution**: Ubuntu 16.04.4 LTS (GNU/Linux 4.4.0-116-generic x86_64)\r\n**TensorFlow installed from**: Both using conda as well as bazel build whl package inside conda env\r\n**Bazel version**: 0.12 and 0.11 in individual tries\r\n**CUDA/cuDNN version**: CUDA 9.0 cuDNN 1.7\r\n**GPU model and memory**: 2 Tesla K80's (see the nvidia-smi output shown in the question)\r\n**Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\ntf.Session()\r\n```\r\ntf.Session() causes the error\r\n\r\nHope this information is adequate. ", "I am confused. Are you using cuda 9 or 9.1? In the original post you posted LD_LIBRARY_PATH as \r\n\r\n```\r\n\r\n> and the LD_LIBRARY_PATH:\r\n> \r\n> /home/bafhf/cuda-9.1/lib64:/home/bafhf/cuda-9.1/extras/CUPTI/lib64:\r\n\r\n```\r\n\r\nYou can get a stacktrace by running your python file using gdb. \r\n\r\n1. gdb python\r\n2. run file.py\r\n3. bt\r\n", "I am sorry for the mixup, I should have corrected that.\r\n\r\nI tried first with CUDA 9.1, got the error. So switched to CUDA 9.0 to check whether that's the issue. But seems like error is still there with CUDA 9.0 also. So its at least not a CUDA problem. Right now I am sticking with CUDA 9.0.\r\n\r\nI have corrected the original post now.", "I have got the stacktrace.\r\n\r\n```\r\n>>> tf.Session()\r\n[New Thread 0x7fff95a22700 (LWP 6916)]\r\n[New Thread 0x7fff98223700 (LWP 6917)]\r\n[New Thread 0x7fff9aa24700 (LWP 6918)]\r\n[New Thread 0x7fff9d225700 (LWP 6919)]\r\n[New Thread 0x7fffe3a42700 (LWP 6920)]\r\n[New Thread 0x7fffe1241700 (LWP 6921)]\r\n[New Thread 0x7fffdea40700 (LWP 6922)]\r\n[New Thread 0x7fffde23f700 (LWP 6923)]\r\n[New Thread 0x7fffd9a3e700 (LWP 6924)]\r\n[New Thread 0x7fffd723d700 (LWP 6925)]\r\n[New Thread 0x7fffd4a3c700 (LWP 6926)]\r\n[New Thread 0x7fffd423b700 (LWP 6927)]\r\n[New Thread 0x7fffd3a3a700 (LWP 6928)]\r\n[New Thread 0x7fffd3239700 (LWP 6929)]\r\n[New Thread 0x7fffcca38700 (LWP 6930)]\r\n[New Thread 0x7fffca237700 (LWP 6931)]\r\n[New Thread 0x7fffc7a36700 (LWP 6932)]\r\n[New Thread 0x7fffc5235700 (LWP 6933)]\r\n[New Thread 0x7fffc2a34700 (LWP 6934)]\r\n[New Thread 0x7fffc0233700 (LWP 6935)]\r\n[New Thread 0x7fffbda32700 (LWP 6936)]\r\n[New Thread 0x7fffbb231700 (LWP 6937)]\r\n[New Thread 0x7fffb8a30700 (LWP 6938)]\r\n[New Thread 0x7fffb622f700 (LWP 6939)]\r\n[New Thread 0x7fffb3a2e700 (LWP 6940)]\r\n[New Thread 0x7fffb122d700 (LWP 6941)]\r\n[New Thread 0x7fffaea2c700 (LWP 6942)]\r\n[New Thread 0x7fffac22b700 (LWP 6943)]\r\n[New Thread 0x7fffa9a2a700 (LWP 6944)]\r\n[New Thread 0x7fffa7229700 (LWP 6945)]\r\n[New Thread 0x7fffa4a28700 (LWP 6946)]\r\n[New Thread 0x7fffa0227700 (LWP 6947)]\r\n[New Thread 0x7fff9fa26700 (LWP 6948)]\r\n[New Thread 0x7fff59e9d700 (LWP 6949)]\r\n[New Thread 0x7fff5969c700 (LWP 6950)]\r\n[New Thread 0x7fff58e9b700 (LWP 6951)]\r\n[New Thread 0x7ffecbfff700 (LWP 6952)]\r\n[New Thread 0x7ffecb7fe700 (LWP 6953)]\r\n[New Thread 0x7ffecaffd700 (LWP 6954)]\r\n[New Thread 0x7ffeca7fc700 (LWP 6955)]\r\n[New Thread 0x7ffec9ffb700 (LWP 6956)]\r\n[New Thread 0x7ffec97fa700 (LWP 6957)]\r\n[New Thread 0x7ffec8ff9700 (LWP 6958)]\r\n[New Thread 0x7ffeaffff700 (LWP 6959)]\r\n2018-05-17 23:27:46.350913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1349] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n[New Thread 0x7ffeaf7fe700 (LWP 6960)]\r\n[New Thread 0x7ffeaeffd700 (LWP 6961)]\r\n\r\nThread 86 \"python\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7ffeaeffd700 (LWP 6961)]\r\n0x00007fff67a448d5 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n(gdb) bt\r\n#0  0x00007fff67a448d5 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#1  0x00007fff67b94914 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#2  0x00007fff67b30e80 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n#3  0x00007ffff7bc16ba in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#4  0x00007ffff78f741d in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n```\r\n", "Does it work outside conda enviornment?\r\n\r\n1. sudo pip install tensorflow-gpu\r\n\r\n2. \r\npython\r\nimport tensorflow as tf\r\ntf.Session()", "@achalshah20 No it doesn't work outside conda as well", "@BenisonSam \r\n\r\nLook at your nvidia-smi output - the second GPU has an ECC code of 2. This error manifests itself irrespective of a CUDA version of TF version error, and usually as a segfault, and sometimes, with the `CUDA_ERROR_ECC_UNCORRECTABLE` flag in the stack trace.\r\n\r\n> \"Uncorrectable ECC error\" usually refers to a hardware failure. ECC is Error Correcting Code, a means to detect and correct errors in bits stored in RAM. A stray cosmic ray can disrupt one bit stored in RAM every once in a great while, but \"uncorrectable ECC error\" indicates that several bits are coming out of RAM storage \"wrong\" - too many for the ECC to recover the original bit values.\r\n> \r\n> This could mean that you have a bad or marginal RAM cell in your GPU device memory.\r\n> \r\n> Marginal circuits of any kind may not fail 100%, but are more likely to fail under the stress of heavy use - and associated rise in temperature.\r\n\r\nYour best option is to change your GPU machine -- this points to a hardware level failure.\r\n\r\nSee https://stackoverflow.com/questions/11839555/cuda-uncorrectable-ecc-error-encountered", "@BenisonSam Sometimes reboot just take away \"Uncorrectable ECC error\". If it doesn't go then it may be a hardware failure as @viksit suggested.", "I agree with @achalshah20 because these GPUs are shared GPUs. I am student and this is a shared machine in our university. Other students whose accounts were created and configured by our former admin is up and working and they are still able to use the GPU.\r\n\r\nI am configuring this new account of mine for a project and am facing this issue. My over all setup is same as theirs, but I don't know what I am doing differently which makes it not work for me.\r\n\r\nAlso since there are multiple K80 nodes, is there a need for OpenMPI/NCCL setup of any kind?", "Nagging Assignee @poxvoculi: It has been 23 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am also see similar issue on Tensorflow 1.8.0-gpu, any resolution for this issue.", "After restarting the server as @achalshah20 mentioned, the ECC error was gone which @viksit pointed out. Then I was able to use both my K80 cards for my tensorflow implementations. Thanks for all your help!", "In case anyone still interested in, I happened to had the same issue, with \"Volatile Uncorr. ECC\" output. My problem was incompatible versions as shown below:\r\n\r\n```\r\nLoaded runtime CuDNN library: 7.1.1 but source was compiled with: 7.2.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nSegmentation fault\r\n```\r\n\r\nAfter I upgrade CuDNN library to 7.3.1 (which is greater than 7.2.1), segmentation fault error disappeared. To upgrade I did the following (as also documented in [here](https://github.com/williamFalcon/tensorflow-gpu-install-ubuntu-16.04)).\r\n\r\n1. Download CuDNN library from [NVIDIA website](https://developer.nvidia.com/rdp/cudnn-download)\r\n2. sudo tar -xzvf [TAR_FILE]\r\n3. sudo cp cuda/include/cudnn.h /usr/local/cuda/include\r\n4. sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\r\n5. sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*"]}, {"number": 19346, "title": "Add getter to TFLite tensor dims", "body": "Add a public getter function of tensor dims in tflite interpreter. It is useful for users to fully aware the shapes of tensors, for processing dynamic shape outputs and for debugging.", "comments": ["Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 46 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19345, "title": "Branch 196939548", "body": "Merging internal changes.", "comments": []}, {"number": 19344, "title": "compile error with CUDA8.0", "body": "\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.7\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.13\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n\r\n### Describe the problem\r\nmet compile error with:\r\ntensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)4> \") is not allowed\r\n\r\ntensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)5> \") is not allowed\r\n\r\ntensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)6> \") is not allowed\r\n\r\ntensorflow/core/kernels/gather_nd_op_gpu.cu.cc(45): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherSliceOpKernel< ::std::complex<double> , long long, (int)7> \") is not allowed\r\n\r\n80 errors detected in the compilation of \"/tmp/tmpxft_0000502d_00000000-7_gather_nd_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/carmark/github/tensorflow/tensorflow/core/kernels/BUILD:690:1: output 'tensorflow/core/kernels/_objs/gather_nd_op_gpu/tensorflow/core/kernels/gather_nd_op_gpu.cu.pic.o' was not created\r\nERROR: /home/carmark/github/tensorflow/tensorflow/core/kernels/BUILD:690:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1050.472s, Critical Path: 77.58s\r\nINFO: 4023 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nGPU model and memory\nExact command to reproduce", "Duplicate of #19203."]}, {"number": 19343, "title": "AttributeError: module 'tensorflow.contrib' has no attribute 'distribute'", "body": "When I include mirroring strategy using following code -\r\n\r\n# Build the Estimator\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nmodel = tf.estimator.Estimator(model_fn,config=config)\r\n\r\nI get the error - AttributeError: module 'tensorflow.contrib' has no attribute 'distribute'\r\n\r\nI tried this with both Tensorflow 1.7 and 1.8 - I get the same issue. I am using Python 3.6 https://colab.research.google.com/ .\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I am using something like this with TF 1.8 (GPU version) with no issue:\r\n```\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\n```", "Colab reasearch has v1.7. It has no module called distribute, so you should use that code with 1.8 and it works(tested).\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.7/tensorflow/contrib/distribute", "Thank you for your comments! This was the issue happening only on Colab server. When I moved to my P2.8xlarge instance on AWS, it works fine there. Thank you for all your help and comments!"]}, {"number": 19342, "title": "Install of Tensorflow Lite error - :app:compiledebugjavawithjavac", "body": "Hi,\r\nI am trying to test out the Tensorflow Lite Android demo on Android Studio, but I am always getting the build error: :app:compiledebugjavawithjavac. I set the JDK path and rebuilt the project. I have done everything the instructions have said. I recently installed the latest Android Studio 3.1.2 just before importing the Tensorflow Lite demo. So how can I fix this?\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows - on Android Studio\r\n- **TensorFlow installed from (source or binary)**: Android Studio Demo Code\r\n- **TensorFlow version (use command below)**: NA\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 9.0 / 7.0\r\n- **GPU model and memory**: GeForce GTX 750\r\n- **Exact command to reproduce**: NA", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I've updated the fields.", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Could you describe how you are building tf lite?", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19341, "title": "Eager: Variable collection for training specific varibles", "body": "### System information\r\n- **TensorFlow version (use command below)**: 1.6.0v GPU\r\n- **Python version**: 3.6.4\r\n\r\n### Describe the problem\r\nWhen exploring the eager execution mode, I built neural network model with `tf.layers.Dense` API under the `tf.variable_scope(\"MLP\")`. \r\n\r\nOnce I would like to get variables with `tf.get_collection` to train specific variables, a ValueError was raised. \r\n\r\n> ValueError: When Eager Execution is enabled, variable collections are not supported.\r\n\r\nIt means that the `var_list` could not be collected to train specific variables, although fine-tune is quite common in the application of neural network model. Is there any solution?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@Harry-Up could you find solution to this problem?"]}, {"number": 19340, "title": "[TfLite]how can i run test for tflite(contrib/lite/testing/)?", "body": "if i did some optimization work for tflite kernels, for example, i modify the Conv() operation.\r\nHow can i test it? i find that there is a \"testing\" in contrib/lite/, and there is something related to test, but there there is no document for what the testing is and how to compile and run it.\r\nIs there any instructions and document available? if i really make some improvement to tflite kernel and want to contribute, i want to make it be fully tested.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n now. No change, it is a question for how to test tflite kernels.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc-5.4.0\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nit is a question for how to use testing in tflite. \r\n\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "For running all the tests:\r\n  `bazel test tensorflow/contrib/lite/testing/...`\r\nAlso, there are unittests in tensorflow/contrib/lite/kernerls.\r\n\r\n\r\n"]}, {"number": 19339, "title": "Failed to load the native TensorFlow runtime.", "body": "CPU: 3.1 GHz Intel Core i7\r\nGPU: Intel Iris Graphics 6100 1536 MB\r\nPython Version: 2.7\r\n\r\nHello, I hope someone will be able to help me with this, I am interested in seeing what Tensorflow \r\n could do, but while attempting to run a test program for Tensorflow I receive this message: \r\n\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 46, in <module>\r\n    import tensorflow as tf\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 51, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 10): no suitable image found.  Did find:\r\n\t/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: mach-o, but wrong architecture\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code:N/A\r\nOS Platform and Distribution: OS X El Capitan\r\nTensorFlow installed from: Tensorflow Github - https://github.com/tensorflow/models \r\nTensorFlow version: Unknown\r\nBazel version: Unknown\r\nCUDA/cuDNN version: Unknown\r\nGPU model and memory: Intel Iris Graphics 6100 1536 MB\r\nExact command to reproduce: python classify_image.py", "Unfortunately see [the Mac install pages](https://www.tensorflow.org/install/install_mac#common_installation_problems) Mac OS is only supported from Sierra 10.12.6 onwards."]}, {"number": 19338, "title": "Make tf.clip_by_value not crash on empty tensors", "body": "Also rearrange the code to remove duplication.  No tests yet; I'll leave\r\nrefactoring the test cases for empty tensor coverage to someone else.\r\n\r\nFixes #19337.", "comments": ["Whoever wants to go the rest of the way and write the tests is free to use this patch or not.", "@girving Thanks for the fix!\r\n\r\nI tried with v1.8.0 and the most recent master branch (nightly). On 1.8.0 the crash happens. Interestingly, on nightly there is no crash. I haven't figure out the changes recently between master and v1.8.0 that prevent the crash. I added small test cases in PR #19396.\r\n\r\nIt is always better to change the empty tensor case early on, and this PR reduces the code duplication significantly \ud83d\udc4d , so this PR looks good to me.", "Great!  Should we merge this one and then the test case PR separately, or do you want to combine them first?", "@girving The two PRs could be merged separately. Since this PR has been approved, I would think someone will merge fairly soon."]}, {"number": 19337, "title": "tf.clip_by_value crashes for empty tensors on GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: `b'v1.8.0-0-g93bc2e2' 1.8.0`\r\n- **Python version**: `Python 3.6.1 :: Continuum Analytics, Inc.`\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7.0.5\r\n- **GPU model and memory**: Tesla K80, 12 GB\r\n- **Exact command to reproduce**: `./bug.py`\r\n\r\n### Describe the problem\r\n`tf.clip_by_value` has four different kernel code paths for different shape inputs.  None of them check for empty inputs, which is necessary before calling into a cuda kernel since the infinite wisdom of cuda is that zero length loops should crash horribly.  One of the four places a check is necessary:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_clip.cc#L48\r\n\r\nIt ran across this in real code, but it's slightly delicate to reproduce this crash.  Therefore, I'm not sure that the check is technically necessary before all of the paths.  But it's necessary in at least one, as exhibited by the code below.  Also the code should probably be restructured: currently the same error code is repeated three times.  With that restructuring, there would also only need to be one emptiness check.\r\n\r\n### Source code / logs\r\n\r\nThis code reproduces the bug for me.  Note that it runs fine on CPU, since zero length loops on CPUs do not crash horribly:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nz = tf.placeholder(dtype=tf.float32, shape=None)\r\nx = tf.clip_by_value(z, 1, z)\r\nwith tf.Session():\r\n  print(x.eval(feed_dict={z: np.zeros((7,0))}).shape)\r\n```\r\n\r\nResulting output:\r\n\r\n```\r\nroot@azero-bug:~/tmp$ ./bug.py \r\n/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-05-16 22:07:46.191147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 4761:00:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.09GiB\r\n2018-05-16 22:07:46.191226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-16 22:07:46.514559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-16 22:07:46.514640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-05-16 22:07:46.514661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-05-16 22:07:46.514923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 4761:00:00.0, compute capability: 3.7)\r\n2018-05-16 22:07:46.631461: F ./tensorflow/core/util/cuda_launch_config.h:127] Check failed: work_element_count > 0 (0 vs. 0)\r\nAborted (core dumped)\r\n```", "comments": ["Writing a quick patch.  I'm probably going to write just the `ClipOp` part of the patch and leave it to someone else to write the tests, since for my own use I'm going to have to work around this at the graph level regardless (since I want to keep using TF 1.8.0).", "@dave-andersen A shame you never got general op fuzzing up and running. :)"]}, {"number": 19336, "title": "Fallback to dynamic loader even if HADOOP_HDFS_HOME is not defined", "body": "Prior to this commit HadoopFileSystem required HADOOP_HDFS_HOME to be\r\ndefined to initialize the filesystem, even if libhdfs.so is located\r\noutside of the standard location. This limitation is unnecessary and\r\ncan be safely removed.\r\n\r\nAs a nice side-effect, the error message is now more informative.\r\n\r\nBefore:\r\n\r\n    Environment variable HADOOP_HDFS_HOME not set\r\n\r\nAfter:\r\n\r\n    libhdfs.so: cannot open shared object file: No such file or directory\r\n\r\nChange-Id: Ief6a8679d7ef353003aa387f7767ebaa8ef290ce", "comments": []}, {"number": 19335, "title": "fix iris example to work with python3", "body": "iris.py did not work with python3 as urllib.urlopen is not in python3.\r\nSwitched to urlretrive from six. Same was done in:\r\ntensorflow/examples/image_retraining/retrain.py", "comments": ["Nagging Assignee @xiejw: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19334, "title": "Fixing ELU and SELU GradGrad", "body": "Changing EluGradGrad and SeluGradGrad to match ReluGradGrad\r\n\r\nFixes #19333", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@zero-impact are you still working on this PR? ", "Ostensibly yes, but unfortunately I haven't really put aside the time to\nwork on it and implement the tests.\n\nIf someone else would like to patch it up that's fine by me, but otherwise\nI can try and get it closed off in the next few weeks.\n\nOn Fri, Aug 10, 2018 at 2:31 PM Yifei Feng <notifications@github.com> wrote:\n\n> @zero-impact <https://github.com/zero-impact> are you still working on\n> this PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19334#issuecomment-412211118>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtrbu4y_S186fWQWNQtz6vHQxpsEZLpks5uPfufgaJpZM4UB854>\n> .\n>\n", "hi @zero-impact, have you had any time?", "Nope sorry. Deadlines on other projects..\n\nEarliest I could do is give it a shot mid October.\n\nOn Thu, Oct 4, 2018 at 11:27 PM Vijay Vasudevan <notifications@github.com>\nwrote:\n\n> hi @zero-impact <https://github.com/zero-impact>, have you had any time?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/19334#issuecomment-427234640>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtrbtVLGUYJl7z1XirYygMBtIt4gqgwks5uhtG0gaJpZM4UB854>\n> .\n>\n", "@zero-impact  Any update ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "Looking into this again after forgetting about it and looks like it has been fixed: https://github.com/mark0725/tensorflow/pull/1447/commits/84c5a4551e2e71d854932cb389c359db11cfa2b1"]}, {"number": 19333, "title": "Bug in EluGradGrad", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary - latest pip tf_nightly\r\n- **TensorFlow version (use command below)**: v1.8.0-1674-gd8fac4cb80 1.9.0-dev20180515\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA Version 9.1.85\r\n- **GPU model and memory**: GeForce GTX 970\r\n- **Exact command to reproduce**: See script attached\r\n\r\n### Describe the problem\r\n\r\nThere seems to be an issue with EluGradGrad that I have uncovered. Please see the post here: https://github.com/renmengye/tensorflow-forward-ad/issues/2#issuecomment-389321546\r\n\r\n### Source code / logs\r\n\r\nRunning this script will demonstrate the incorrect values:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef fwd_gradients(ys, xs, d_xs):\r\n    dummy = tf.zeros_like(ys)\r\n    g = tf.gradients(ys, xs, grad_ys=dummy, name=\"gradients\")\r\n    return tf.gradients(g, dummy, grad_ys=d_xs, name=\"jvp\")\r\n\r\ndef my_elu(x):\r\n    return tf.where(x >= 0.0, x, tf.exp(x) - 1.0)\r\n\r\ndef main():\r\n    print(tf.__version__)\r\n\r\n    sess = tf.InteractiveSession()\r\n    init = tf.global_variables_initializer()\r\n    \r\n    # activation = my_elu # Works correctly tf.nn.relu (or any other non-elu activation)\r\n    activation = tf.nn.elu\r\n\r\n    x_size = 3\r\n    y_size = x_size\r\n\r\n    # Single ELU or RELU op\r\n    X = tf.placeholder(tf.float64, shape=[x_size]) # Input\r\n    Y = activation(X) # Output\r\n\r\n    # Define vjp and jvp\r\n    Vx = tf.placeholder(tf.float64, shape=[x_size]) # Jac-vector product input V\r\n    Vy = tf.placeholder(tf.float64, shape=[y_size]) # vector-jac product input V\r\n    jvp = fwd_gradients(Y, X, d_xs=Vx)\r\n    vjp = tf.gradients(Y, X, grad_ys=Vy)\r\n\r\n    # Compute jacobians\r\n    x = np.ones(x_size) - 1.5 # Bug only occurs in x < 0 region\r\n    # x = np.random.normal(-1, 1, x_size)\r\n    tf_jac, numeric_jac = tf.test.compute_gradient(X, [x_size], Y, [y_size], x_init_value=x)\r\n    vjp_jac = np.array([sess.run(vjp, feed_dict={X: x, Vy: v})[0] for v in np.identity(y_size)])\r\n    jvp_jac = np.array([sess.run(jvp, feed_dict={X: x, Vx: v})[0] for v in np.identity(x_size)])\r\n\r\n    # Print results as maximum absolute error\r\n    print(\"Numeric jac:\", numeric_jac)\r\n    print(\"jvp jac:\", jvp_jac)\r\n    print(\"tf error:\", np.max(np.abs(numeric_jac - tf_jac)))   # ~0.0\r\n    print(\"vjp error:\", np.max(np.abs(numeric_jac - vjp_jac))) # ~0.0\r\n    print(\"jvp error:\", np.max(np.abs(numeric_jac - jvp_jac))) # LARGE! for ELU\r\n\r\n    sess.close()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThe solution is to edit the implementation of `_EluGradGrad` [here](https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/python/ops/nn_grad.py#L364).\r\n\r\nI've created a pull request that references this issue.", "comments": ["Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@zero-impact Thank you for submitting PR #19334! "]}, {"number": 19332, "title": "[INTEL MKL] Upgrading MKL DNN to v.14", "body": "It also includes fixes to handle empty input tensors in MKLConcat and MKLAvgPooling. MKL DNN v0.14 does more sanity checks on empty input tensors and throws \"invalid parameter\" exceptions.", "comments": ["Closing this due to problems with merging. opening a new one."]}, {"number": 19331, "title": "Undefined symbol when importing tensorflow", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from**: usual pip install\r\n- **TensorFlow version**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Exact command to reproduce**: python -c \"import tensorflow\"\r\n\r\n### Describe the problem\r\nI'm installing Tensorflow CPU version via the usual pip command:\r\n```bash\r\npip install tensorflow\r\n```\r\nAnd when I try to import tensorflow I get the following error message:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow34_CallableOptions_default_instance_E\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow34_CallableOptions_default_instance_E\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n[1]    15355 segmentation fault (core dumped)  python -c \"import tensorflow\"\r\n```\r\nAny idea of what is going wrong?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Something seems incorrect with the installation, that error would make sense if the shared library is from an older version.\r\n\r\nCould you try uninstalling and re-installing first?\r\n```sh\r\npip3 uninstall tensorflow tensorflow-gpu\r\npip3 install tensorflow\r\n```\r\n\r\nAnd if that doesn't help, could you run the following:\r\n\r\n```sh\r\nmd5sum /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nmd5sum /home/jplu/.local/lib/python3.6/site-packages/tensorflow/libtensorflow_framework.sh\r\nldd /home/jplu/.local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\n\r\nThat might give some clue as to what is messed up about the installation. Not quite sure how it ended up in this state.", "Ok, indeed it was refering to a wrong version of `libtensorflow_framework.so`. I have the Go framework installed with the C backend in 1.7.0, and when I have updated the python package through pip the link was done with the wrong version.\r\n\r\nI did update the Go version and the C backend to 1.8.0 and now it works like a charm :)\r\n\r\nThanks a lot!!!", "Hi jplu,\r\n\r\nI am trying to install tensorflow on Odroid XU4. It got installed using bazel. But now while importing, I am having an error like your's. Can you be more specific about how you solved it.\r\n\r\nThe detailed error message is the following:\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\n\r\n", "> Hi jplu,\r\n> \r\n> I am trying to install tensorflow on Odroid XU4. It got installed using bazel. But now while importing, I am having an error like your's. Can you be more specific about how you solved it.\r\n> \r\n> The detailed error message is the following:\r\n> \r\n> > > > import tensorflow\r\n> > > > Traceback (most recent call last):\r\n> > > > File \"\", line 1, in \r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 24, in \r\n> > > > from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 49, in \r\n> > > > from tensorflow.python import pywrap_tensorflow\r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in \r\n> > > > raise ImportError(msg)\r\n> > > > ImportError: Traceback (most recent call last):\r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in \r\n> > > > from tensorflow.python.pywrap_tensorflow_internal import *\r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in \r\n> > > > _pywrap_tensorflow_internal = swig_import_helper()\r\n> > > > File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n> > > > _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n> > > > ImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n> \r\n> See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n> \r\n> for some common reasons and solutions. Include the entire stack trace\r\n> above this error message when asking for help.\r\n> \r\n> > > >\r\n\r\nI have exactly the same issue using 1.6.0"]}, {"number": 19330, "title": "[XLA] Don't do int64 tests for devices which do not support int64", "body": "If a device does not specify that it supports int64, then do not run tests which hard-code int64 types into themselves.\r\n\r\n", "comments": ["Nagging Assignee @jpienaar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@zheng-xq perhaps then assignee is no longer with the Tensorflow project?\r\n", "thanks :)", "Does someone have to tell the CI system run run the unit tests, so that it will be merged?", "Some python lint errors to fix:\r\ntensorflow/compiler/tests/binary_ops_test.py:690: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 8\r\ntensorflow/compiler/tests/binary_ops_test.py:694: [C0301(line-too-long), ] Line too long (83/80)", "thanks - will do that ASAP", "Done.  :)\r\n", "Hi @jpienaar \r\n\r\nsorry to trouble you again, i did the python linting update but I think it needs to be reviewed and marked for testing again.\r\n\r\ncheers\r\n", "Nagging Assignee @jpienaar: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "thanks dude \ud83d\udc4d ", "I'm guessing this test which has not completed is ok to ignore?\r\n"]}, {"number": 19329, "title": "MPI enabled builds fail for r1.8", "body": "There are two problems: \"endl\" not being part of std, and undefined logging macros.", "comments": ["Amit, are you ok with this going into 1.8? I assume that branch is closed now.\r\n\r\nWe're going to cut 1.9 soon anyway, so perhaps we should hold off.", "There is no point putting this in 1.8. The branch is closed unless we do a cherrypick and 1.9 is being cut soon anyways."]}, {"number": 19328, "title": "Fix issue in Keras model complie with float64 mode", "body": "This fix tries to address the issue raised in #19318 where Keras model complie for `model.compile('rmsprop', 'mse')` does not work in float64 mode.\r\n\r\nThe issue comes from `placeholder_with_default([1.]...`, which returns dtype float32 by default (as `[1.]` was inteprated as float32). Since placeholder_with_default does not have a output_dtype to pass, this fix converts `[1.]` to float64 first before passing in to address the issue. A test case has been added to cover the changes.\r\n\r\nThis fix fixes #19318.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["The Sanity failure was unrelated and has be fixed in PR #19327.", "Please rebase to fix the merge conflicts.", "Thanks @fchollet for the review. The PR has been rebased with conflict resolved. Please take a look.", "Looks like this is causing `//py_test_dir/tensorflow/python/estimator:keras_test` to fail on Windows\r\n```\r\nERROR: test_multi_inputs_multi_outputs (__main__.TestKerasEstimator)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_ujqw31nk\\runfiles\\org_tensorflow\\py_test_dir\\tensorflow\\python\\estimator\\keras_test.py\", line 388, in test_multi_inputs_multi_outputs\r\n    keras_model=model, config=self._config)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 534, in model_to_estimator\r\n    keras_weights)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 434, in _save_first_checkpoint\r\n    custom_objects)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\estimator\\keras.py\", line 300, in _clone_and_build_model\r\n    model = models.clone_model(keras_model, input_tensors=input_tensors)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 263, in clone_model\r\n    return _clone_functional_model(model, input_tensors=input_tensors)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 156, in _clone_functional_model\r\n    **kwargs))\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 703, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 711, in call\r\n    return self.function(inputs, **arguments)\r\n  File \"C:/Python36/lib/site-packages/tensorflow/python/ops/gen_parsing_ops.py\", line 1284, in string_to_number\r\n    name=name)\r\nSystemError: unknown opcode\r\n```\r\nhttps://source.cloud.google.com/results/invocations/616850d6-7bf1-477c-9f74-ff7db78ba6e6/log\r\n\r\n@yongtang Can you take a look?", "@meteorcloudy I am still working on #19583 to setup windows + bazel. Though for `SystemError: unknown opcode` error, I am wondering if this might be caused by mismatch of the python version? ", "@yongtang I don't think it's a mismatch of python version. I'm seeing this failure in both Python35 and Python36 build."]}, {"number": 19327, "title": "Fix pylint sanity error for CI build", "body": "\r\nThe CI build is failing, caused by:\r\n```\r\n53 FAIL: Found 4 non-whitelited pylint errors:\r\n54 tensorflow/python/ops/sparse_ops.py:87: [C0301(line-too-long), ] Line too long (94/80)\r\n55\r\n56 tensorflow/python/ops/sparse_ops.py:594: [C0301(line-too-long), ] Line too long (92/80)\r\n57\r\n58 tensorflow/python/ops/array_ops.py:2622: [C0301(line-too-long), ] Line too long (92/80)\r\n59\r\n60 tensorflow/python/ops/array_ops.py:2623: [C0301(line-too-long), ] Line too long (98/80)\r\n```\r\n\r\nThis fix fixes the sanity pylint error.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19326, "title": "Remove redundant header includes in mpi_utils.h", "body": "In `mpi_utils.h` the header `\"tensorflow/core/platform/logging.h\"` was included twice.\r\n\r\nThis fix removes redundant header includes.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["The sanity build failure is not related to this PR. It should be fixed once PR #19327 is merged.", "All tests passed now."]}, {"number": 19325, "title": "Alternative to image_tensor_mapped in Tensoreflow-Lite", "body": "Hi there, For my iOS app using Tensorflow the pre-processing was done by using image_tensor_mapped. i.e.\r\n\r\n```\r\nauto image_tensor_mapped = image_tensor.tensor<float, 4>();\r\n\r\nuint32_t *pixels = (uint32_t *) calloc(width * height, sizeof(uint32_t));\r\n\r\nuint8_t *bufptr = (uint8_t *)pixels;\r\nfor (i = 0; i < height; i++){\r\n   for (j = 0; j < width; j++){\r\n        blue = bufptr[1] / 255.0;\r\n        green = bufptr[2] / 255.0;\r\n        red = bufptr[3] / 255.0;\r\n            \r\n        image_tensor_mapped(0, i, j, 0) = blue;\r\n        image_tensor_mapped(0, i, j, 1) = green;\r\n        image_tensor_mapped(0, i, j, 2) = red;\r\n            \r\n        bufptr += 4;\r\n    }\r\n}\r\n```\r\nWhat will be the alternative for image_tensor_mapped in Tensorflow-lite(tflite)\r\n\r\n\r\nHave I written custom code - No\r\nOS Platform and Distribution - MacOS\r\nTensorFlow installed from Pod\r\nTensorFlow version - r1.8\r\nTensorFlowLite version - 0.1.7\r\nBazel version - 0.11.0\r\nCUDA/cuDNN version  - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code - No\r\nOS Platform and Distribution - MacOS\r\nTensorFlow installed from Pod\r\nTensorFlow version - r1.8\r\nTensorFlowLite version - 0.1.7\r\nBazel version - 0.11.0\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A"]}]