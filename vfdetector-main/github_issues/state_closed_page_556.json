[{"number": 37033, "title": "[tflite] make gpu delegate build", "body": "This resolves https://github.com/tensorflow/tensorflow/issues/36967.\r\n\r\nIt seems this line was forgotten when adding the space_to_depth kernel.\r\nWithout this, all the programs use the GPU delegeate don't build.\r\n\r\n@impjdi: it seems you checked in the space_to_depth kernel.", "comments": ["Sorry for the breakage and thanks for the fix!"]}, {"number": 37032, "title": "Do you have a plug-in for model encryption?Before generation\uff1f", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 37031, "title": "[Intel MKL] support MKL Quantized Matmul With Bias and Dequantize Op and DNNL 1.0", "body": "**Add Support for MKL QuantizedMatMulWithBiasAndDequantize OP and DNNL1.0 Support**\r\n\r\n***Need:***\r\nRequired for models such as BERT\r\n\r\n***Manual Testing:***\r\n\r\nbazel --output_base=/localdisk/niroop/bert2/temp/build_test --output_user_root=/localdisk/niroop/bert2/temp/build_output_dir test --cache_test_results=no --verbose_failures --copt -mfma --copt -mavx2 --copt -O3 --copt -march=broadwell -s -c opt //tensorflow/tools/api/tests:api_compatibility_test   ***(Passed)***\r\n\r\nbazel --output_base=/localdisk/niroop/bert2/temp/build_test --output_user_root=/localdisk/niroop/bert2/temp/build_output_dir test --cache_test_results=no --verbose_failures --config=mkl --copt -mfma --copt -mavx2 --copt -O3 --copt -march=broadwell -s -c opt //tensorflow/core/kernels:mkl_qmatmul_op_test  ***(Passed)***", "comments": ["@penpornk This should be a straight forward one. Let me know and thank you.", "@nammbash Thank you for the PR! Does this need to be in release 2.2?", "@penpornk Yes, Internally we had planned on customers based on BERT model improvement. Hence.\r\n\r\nMore relevant to TF2.2 (DNNL1.X) ones will come soon. But pushing this smaller/straight forward one will be highly beneficial.\r\n\r\nyou can prioritize as less important than the other ones but still important enough for release 2.2 if possible.Hoping it is alright. :) and clear", "@penpornk Thank you for the quick review! Appreciate it. Fixed brackets based on comments.", "@alextp @gbaned Any error that I should resolve from my side? or is it just API review", "> @alextp @gbaned Any error that I should resolve from my side? or is it just API review\r\n\r\n@nammbash Nothing left to do here at the moment. It is waiting for API review. If all goes well, it will be automerged, otherwise we will report back. Thank you!."]}, {"number": 37030, "title": "[Intel MKL] Change the test that will only run in mkl version", "body": "", "comments": []}, {"number": 37028, "title": "jupyter notebook kernel dies when running a UNet with tensorflow=2.1.0", "body": "**System information** \r\n- OS - Linux RedHat 8\r\n- conda install tensorflow-gpu - tensorflow version = 2.1.0\r\n- Python version: 3.7.4\r\n- Jupyter Notebook \r\n- CUDA/cuDNN version: - 10.2\r\n- CPU: AMD Threadripper 3960x\r\n- GPU: 2x RTX Titans\r\n- Memory: 128 GB Corsair 3200MHz\r\n\r\nI recently upgraded to tensorflow=2.1.0 from tensorflow=2.0.0 and now my code will not run. Before the first epoch ends I get a message saying 'The kernel appears to have died. It will restart automatically.'. The code is using MirroredStrategy() to distribute the UNet to the GPUs. I don't get this error when I remove MirroredStrategy() and run on a single-gpu. \r\nOriginally I was running my code with tensorflow=2.0.0 using jupyter notebook and it would worked fine except that when I would try to load my model and make a prediction I get an error at `model.predict()` saying `AttributeError: 'Model' object has no attribute 'loss'`. I called `model.summary()` after loading the model and it showed the model was empty. So I upgraded to tensorflow=2.1.0 and called `model.summary()` this time it showed a readout. However I still get the same error `AttributeError: 'Model' object has no attribute 'loss'`\r\nIs there currently an incompatibility with MirroredStrategy() and tensorflow=2.1.0?\r\n\r\n```\r\ndef get_model(optimizer, loss_metric, metrics, lr=1e-4):\r\n    with tf.device('/job:localhost/replica:0/task:0/device:GPU:0'):\r\n        inputs = Input((sample_width, sample_height, sample_depth, 1))\r\n        conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\r\n        conv1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv1)\r\n        pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv1)\r\n        drop1 = Dropout(0.5)(pool1)\r\n        conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(drop1)\r\n        conv2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv2)\r\n        pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\r\n        drop2 = Dropout(0.5)(pool2)\r\n        conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(drop2)\r\n        conv3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv3)\r\n        pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv3)\r\n        drop3 = Dropout(0.3)(pool3)\r\n        conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(drop3)\r\n        conv4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv4)\r\n        pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)\r\n        drop4 = Dropout(0.3)(pool4)\r\n        conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(drop4)\r\n        conv5 = Conv3D(512, (3, 3, 3), activation='relu', padding='same')(conv5)\r\n    with tf.device('/job:localhost/replica:0/task:0/device:GPU:1'): \r\n        up6 = concatenate([Conv3DTranspose(256, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv5), conv4], axis=4)\r\n        conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(up6)\r\n        conv6 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(conv6)\r\n        up7 = concatenate([Conv3DTranspose(128, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv6), conv3], axis=4)\r\n        conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(up7)\r\n        conv7 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(conv7)\r\n        up8 = concatenate([Conv3DTranspose(64, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv7), conv2], axis=4)\r\n        conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(up8)\r\n        conv8 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(conv8)\r\n        up9 = concatenate([Conv3DTranspose(32, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv8), conv1], axis=4)\r\n        conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(up9)\r\n        conv9 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(conv9)\r\n        conv10 = Conv3D(1, (1, 1, 1), activation='sigmoid')(conv9)\r\n        model = Model(inputs=[inputs], outputs=[conv10])\r\n        model.compile(optimizer=optimizer(lr=lr), loss=loss_metric, metrics=metrics)\r\n        return model\r\n\r\nsmooth = 1.\r\ndef dice_coef(y_true, y_pred):\r\n    y_true_f = K.flatten(y_true)\r\n    y_pred_f = K.flatten(y_pred)\r\n    intersection = K.sum(y_true_f * y_pred_f)\r\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\r\ndef dice_coef_loss(y_true, y_pred):\r\n    return -dice_coef(y_true, y_pred)\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\nwith mirrored_strategy.scope():\r\n    model = get_model(optimizer=Adam, loss_metric=dice_coef_loss, metrics=[dice_coef], lr=1e-4)\r\nobserve_var = 'dice_coef'\r\nstrategy = 'max' # greater dice_coef is better\r\nmodel_checkpoint = ModelCheckpoint('{epoch:04}model', monitor=observe_var, save_best_only=True)\r\nmodel.fit(train_x, train_y, batch_size = 2, epochs= 1000, verbose=1, shuffle=True, validation_split=.2, callbacks=[model_checkpoint])\r\nmodel.save( 'finalmodel')\r\n```\r\nmodel.summary():\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         multiple                  0         \r\n_________________________________________________________________\r\nconv3d (Conv3D)              multiple                  896       \r\n_________________________________________________________________\r\nconv3d_1 (Conv3D)            multiple                  27680     \r\n_________________________________________________________________\r\nmax_pooling3d (MaxPooling3D) multiple                  0         \r\n_________________________________________________________________\r\ndropout (Dropout)            multiple                  0         \r\n_________________________________________________________________\r\nconv3d_2 (Conv3D)            multiple                  55360     \r\n_________________________________________________________________\r\nconv3d_3 (Conv3D)            multiple                  110656    \r\n_________________________________________________________________\r\nmax_pooling3d_1 (MaxPooling3 multiple                  0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          multiple                  0         \r\n_________________________________________________________________\r\nconv3d_4 (Conv3D)            multiple                  221312    \r\n_________________________________________________________________\r\nconv3d_5 (Conv3D)            multiple                  442496    \r\n_________________________________________________________________\r\nmax_pooling3d_2 (MaxPooling3 multiple                  0         \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          multiple                  0         \r\n_________________________________________________________________\r\nconv3d_6 (Conv3D)            multiple                  884992    \r\n_________________________________________________________________\r\nconv3d_7 (Conv3D)            multiple                  1769728   \r\n_________________________________________________________________\r\nmax_pooling3d_3 (MaxPooling3 multiple                  0         \r\n_________________________________________________________________\r\ndropout_3 (Dropout)          multiple                  0         \r\n_________________________________________________________________\r\nconv3d_8 (Conv3D)            multiple                  3539456   \r\n_________________________________________________________________\r\nconv3d_9 (Conv3D)            multiple                  7078400   \r\n_________________________________________________________________\r\nconv3d_transpose (Conv3DTran multiple                  1048832   \r\n_________________________________________________________________\r\nconcatenate (Concatenate)    multiple                  0         \r\n_________________________________________________________________\r\nconv3d_10 (Conv3D)           multiple                  3539200   \r\n_________________________________________________________________\r\nconv3d_11 (Conv3D)           multiple                  1769728   \r\n_________________________________________________________________\r\nconv3d_transpose_1 (Conv3DTr multiple                  262272    \r\n_________________________________________________________________\r\nconcatenate_1 (Concatenate)  multiple                  0         \r\n_________________________________________________________________\r\nconv3d_12 (Conv3D)           multiple                  884864    \r\n_________________________________________________________________\r\nconv3d_13 (Conv3D)           multiple                  442496    \r\n_________________________________________________________________\r\nconv3d_transpose_2 (Conv3DTr multiple                  65600     \r\n_________________________________________________________________\r\nconcatenate_2 (Concatenate)  multiple                  0         \r\n_________________________________________________________________\r\nconv3d_14 (Conv3D)           multiple                  221248    \r\n_________________________________________________________________\r\nconv3d_15 (Conv3D)           multiple                  110656    \r\n_________________________________________________________________\r\nconv3d_transpose_3 (Conv3DTr multiple                  16416     \r\n_________________________________________________________________\r\nconcatenate_3 (Concatenate)  multiple                  0         \r\n_________________________________________________________________\r\nconv3d_16 (Conv3D)           multiple                  55328     \r\n_________________________________________________________________\r\nconv3d_17 (Conv3D)           multiple                  27680     \r\n_________________________________________________________________\r\nconv3d_18 (Conv3D)           multiple                  33        \r\n=================================================================\r\nTotal params: 22,575,329\r\nTrainable params: 22,575,329\r\nNon-trainable params: 0\r\n```\r\nPrediction Code:\r\n```\r\nnorm_images='imagedata\\'\r\nmodel = load_model('finalmodel',compile = False)\r\nprint(model.summary())\r\nprediction = model.predict(norm_images, verbose = 1)\r\n```\r\nAttribute Error:\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-06140bdcec2b> in <module>\r\n     75 \r\n     76 print('Predicting the labels')\r\n---> 77 prediction = model.predict(norm_images, verbose = 1)\r\n     78 # prediction = tf.keras.Model.predict(norm_images)\r\n     79 \r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n   1011         max_queue_size=max_queue_size,\r\n   1012         workers=workers,\r\n-> 1013         use_multiprocessing=use_multiprocessing)\r\n   1014 \r\n   1015   def reset_metrics(self):\r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in predict(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    496         model, ModeKeys.PREDICT, x=x, batch_size=batch_size, verbose=verbose,\r\n    497         steps=steps, callbacks=callbacks, max_queue_size=max_queue_size,\r\n--> 498         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\r\n    499 \r\n    500 \r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in _model_iteration(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    424           max_queue_size=max_queue_size,\r\n    425           workers=workers,\r\n--> 426           use_multiprocessing=use_multiprocessing)\r\n    427       total_samples = _get_total_number_of_samples(adapter)\r\n    428       use_sample = total_samples is not None\r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    644     standardize_function = None\r\n    645     x, y, sample_weights = standardize(\r\n--> 646         x, y, sample_weight=sample_weights)\r\n    647   elif adapter_cls is data_adapter.ListsOfScalarsDataAdapter:\r\n    648     standardize_function = standardize\r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2358     is_compile_called = False\r\n   2359     if not self._is_compiled and self.optimizer:\r\n-> 2360       self._compile_from_inputs(all_inputs, y_input, x, y)\r\n   2361       is_compile_called = True\r\n   2362 \r\n\r\n~\\.conda\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in _compile_from_inputs(self, all_inputs, target, orig_inputs, orig_target)\r\n   2609     self.compile(\r\n   2610         optimizer=self.optimizer,\r\n-> 2611         loss=self.loss,\r\n   2612         metrics=self._compile_metrics,\r\n   2613         weighted_metrics=self._compile_weighted_metrics,\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\n", "comments": ["Can you remove the device scope in the `get_model` function and give a complete reproducible example?\r\nCan you also clarify the issues you had with the respective TF versions?\r\nIn TF 2.0 - you had issues with calling model.predict?\r\nIn TF 2.1 - you had a problem with the kernel dying? Where were you running this?", "I a having the same exact problem with this, as there is no get_model. \r\nIt is jupyter notebook and TF 2.1.\r\n\r\ndef build_model2():\r\n    model = Sequential([\r\n      Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),\r\n      Dense(17, kernel_initializer='normal', activation='relu'),\r\n      Dense(4, kernel_initializer='normal', activation='relu'),\r\n      Dense(1, kernel_initializer='normal')])\r\n    model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])\r\n    return model\r\n\r\nstrategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:2\"])\r\n\r\nwith strategy.scope():\r\n    parallel_model = build_model2()\r\n    parallel_model.compile(loss='mean_absolute_error', optimizer='adam')\r\n\r\n--> Kernel is dying at this cell and I am not able to run the below cells.\r\n\r\nparallel_model.summary()\r\nparallel_model.fit(x, y, epochs=20, batch_size=256)\r\n", "I solved my problem by downgrading to tensorflow-gpu=2.0.0 and had to add the `.h5` extension when saving my final model and when saving my models at my checkpoint.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37028\">No</a>\n", "> I a having the same exact problem with this, as there is no get_model.\r\n> It is jupyter notebook and TF 2.1.\r\n> \r\n> def build_model2():\r\n> model = Sequential([\r\n> Dense(n, input_dim=n, kernel_initializer='normal', activation='relu'),\r\n> Dense(17, kernel_initializer='normal', activation='relu'),\r\n> Dense(4, kernel_initializer='normal', activation='relu'),\r\n> Dense(1, kernel_initializer='normal')])\r\n> model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mae', 'mse'])\r\n> return model\r\n> \r\n> strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:2\"])\r\n> \r\n> with strategy.scope():\r\n> parallel_model = build_model2()\r\n> parallel_model.compile(loss='mean_absolute_error', optimizer='adam')\r\n> \r\n> --> Kernel is dying at this cell and I am not able to run the below cells.\r\n> \r\n> parallel_model.summary()\r\n> parallel_model.fit(x, y, epochs=20, batch_size=256)\r\n\r\n@BanuSelinTosun I had the same issue and I was solve it by moving 'with strategy.scope():' line into the function, just like how @rzaratx did, but I am not sure why it doesn't work in their case.\r\n\r\nBest,\r\nEmre"]}, {"number": 37027, "title": "tf-nightly not updated?", "body": "Just curious, tf-nightly is PyPI is not updated since Feb. 19. What's going on? Can we expect the update any time soon?\r\n\r\nThank you!", "comments": ["@wuhy08,\r\nNot sure what caused the delay, although I see that the latest nightly for [2020/02/25](https://pypi.org/project/tf-nightly/2.2.0.dev20200225/) has been released 3 hours ago.\r\n\r\nPlease feel free to close the issue if this resolves your query. Thanks!", "Thank you!"]}, {"number": 37026, "title": "Linearly increasing memory with use_multiprocessing and Keras Sequence", "body": "**System information**\r\nOS: Ubuntu 19.04\r\nCPU: Ryzen 2700X\r\nGPUs: 2 X GTX 1080ti\r\nRAM: 32GB\r\n \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\nYes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nUbuntu 19.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\nN/A\r\n\r\n- TensorFlow installed from (source or binary): \r\nbinary\r\n\r\n- TensorFlow version (use command below): \r\nCurrent Install: tf-nightly 2.2.0.dev20200218\r\nOccurs On: >= tf 2.0\r\n\r\n- Python version: \r\n3.6.8\r\n\r\n- Bazel version (if compiling from source):\r\nN/A\r\n\r\n- GCC/Compiler version (if compiling from source): \r\nN/A\r\n\r\n- CUDA/cuDNN version: \r\nCUDA: 10.1\r\ncuDNN: 7.6.5\r\n\r\n- GPU model and memory:\r\n2 X GTX 1080ti\r\n\r\n**Describe the current behavior**\r\nWhen using tf.keras.Sequence base class with 'use_multiprocessing' argument, the memory usage increases linearly every epoch until the program fails with a resource allocation error. I initially noticed this when upgrading to TF2.0. I have identified that the KerasSequenceAdapter has the `should_recreate_iterator` function returning True. If you set this function to return False the memory still increases over epochs but at a drastically slower rate. I plan to continue to debug this behavior and identify the root cause of the problem so that the memory does not increase at every epoch.\r\n\r\nObserved Behavior:\r\n- With `should_recreate_iterator` returning True:\r\n    - Workers are not killed after each epoch leaving the total workers at the end of training worker_count**epoch_count\r\n    - Memory usage doubles each epoch at the re-initialization of the KerasSequenceAdapter\r\n- With `should_recreate_iterator` returning False:\r\n    - Workers are reused correctly for each epoch\r\n    - Memory still increases every epoch but at a much lower rate (5-10%) of total memory allocation\r\n\r\n**Describe the expected behavior**\r\nX workers should be instantiated to fill a queue of size Y. Once an epoch is complete, those workers should either be re-used for the next epoch or killed and replaced with new workers. The memory should reach its' maximum fill (worker_count * queue_size * batch_size * data_size * 4) and remain at this allocation for the duration of training.\r\n\r\n**Standalone code to reproduce the issue** \r\nI will create a minimal reproducible example in the near future\r\n\r\n**Other info / logs**\r\nI have no logs at the moment but am happy to produce them in the near future if it is desired\r\n\r\n**Notes**\r\n- I am aware that the expected action for TF.Keras users is to switch to the tf.Dataset API for parallel data processing. I intend to do this at a later date, however, my current data pipeline took a while to get set up and I am not ready to completely re-write it. I am hoping to find a solution for multiprocessing and tf.keras.Sequence in the near term to bridge the gap for people in situations similar to me.\r\n- I chose to open this issue in the TF repo because I am using the TF version of Keras and have no idea if this happens on pure Keras\r\n\r\n***My Questions***\r\n- Is there a reason KerasSequenceAdapter's `should_recreate_iterator` function returns True?\r\n", "comments": ["I have found the following unresolved issues on this topic in the original Keras repo:\r\n- https://github.com/keras-team/keras/issues/8668\r\n- https://github.com/keras-team/keras/issues/5835", "@sam-ulrich1, Could you post the sample standalone code to replicate the reported issue. Thanks!", "```python\r\nimport numpy as np\r\nnp.random.seed(1)\r\nimport tensorflow as tf\r\ntf.random.set_seed(2)\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\n\r\ntrain_dir = \"data/train\"\r\ntest_dir = \"data/test\"\r\ntotal_classes = 200\r\n\r\n\r\n# Define simple image model\r\nmodel_input = Input((32, 32, 3))\r\n\r\nx = Conv2D(128, (7, 7))(model_input)\r\nx = Conv2D(64, (3, 3))(x)\r\nx = Conv2D(32, (3, 3))(x)\r\nx = Flatten()(x)\r\nx = Dense(400, activation=\"relu\")(x)\r\nx = Dense(total_classes, activation=\"softmax\")(x)\r\n\r\nmodel = Model(inputs=model_input, outputs=x)\r\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\r\n\r\n# Define data prep\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1./255,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_dir,\r\n    color_mode='rgb',\r\n    target_size=(32, 32),\r\n    batch_size=128,\r\n    class_mode='categorical')\r\n\r\ntest_generator = test_datagen.flow_from_directory(\r\n    test_dir,\r\n    color_mode='rgb',\r\n    target_size=(32, 32),\r\n    batch_size=128,\r\n    class_mode='categorical')\r\n\r\nmodel.fit(\r\n    train_generator,\r\n    validation_data=test_generator,\r\n    validation_freq=1,\r\n    epochs=10,\r\n    workers=6,\r\n    use_multiprocessing=True\r\n)\r\n```\r\n\r\nWith this example, the memory increase is less pronounced but still plainly apparent. The memory allocation on my system starts at ~1.8GB at epoch 1 and increases to ~2.15GB by epoch 10. If you watch the child processes being spawned you can see that each epoch 6 more processes are spawned and the pre-existing processes remain alive.\r\n\r\nIf you set `should_recreate_iterator` to return False for KerasSequenceAdapter, the memory starts at ~1.8GB at epoch 1 and increases to ~1.89GB by epoch 10 (much more reasonable). If you watch the child processes being spawned you can see that each epoch the prior 6 processes are killed and a new 6 processes are spawned leaving only 6 processes alive at any given time.\r\n\r\nP.S.\r\nIs there a reason that KerasSequenceAdapter is set to be re-created each epoch by default?\r\nI am noticing no detrimental effects by setting `should_recreate_iterator` to False\r\n@gadagashwini ", "In 2.x, we would recommend using tf.data.Datasets, which can handle multiprocessing of input pipelines. See https://www.tensorflow.org/guide/data for more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37026\">No</a>\n", "@karmel As I mentioned earlier, I have created a quite complex data pipeline and I'm not able to completely re-write my pipeline at the moment. Considering there are a number of people in a similar position as me and the fix seems very simple, I believe this warrants more consideration than simply closing the issue. \r\n\r\nI think it would be reasonable to explore such a simple solution in pursuit of backwards compatibility with TF1.X. Please consider further reviewing the material I presented in the interest of the many people experiencing similar issues. ", "> ```python\r\n> ```python\r\n> import numpy as np\r\n> np.random.seed(1)\r\n> import tensorflow as tf\r\n> tf.random.set_seed(2)\r\n> \r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.layers import *\r\n> from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n> \r\n> \r\n> train_dir = \"data/train\"\r\n> test_dir = \"data/test\"\r\n> total_classes = 200\r\n> \r\n> \r\n> # Define simple image model\r\n> model_input = Input((32, 32, 3))\r\n> \r\n> x = Conv2D(128, (7, 7))(model_input)\r\n> x = Conv2D(64, (3, 3))(x)\r\n> x = Conv2D(32, (3, 3))(x)\r\n> x = Flatten()(x)\r\n> x = Dense(400, activation=\"relu\")(x)\r\n> x = Dense(total_classes, activation=\"softmax\")(x)\r\n> \r\n> model = Model(inputs=model_input, outputs=x)\r\n> model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\r\n> \r\n> # Define data prep\r\n> train_datagen = ImageDataGenerator(\r\n>     rescale=1./255,\r\n>     shear_range=0.2,\r\n>     zoom_range=0.2,\r\n>     horizontal_flip=True)\r\n> \r\n> test_datagen = ImageDataGenerator(rescale=1./255)\r\n> \r\n> train_generator = train_datagen.flow_from_directory(\r\n>     train_dir,\r\n>     color_mode='rgb',\r\n>     target_size=(32, 32),\r\n>     batch_size=128,\r\n>     class_mode='categorical')\r\n> \r\n> test_generator = test_datagen.flow_from_directory(\r\n>     test_dir,\r\n>     color_mode='rgb',\r\n>     target_size=(32, 32),\r\n>     batch_size=128,\r\n>     class_mode='categorical')\r\n> \r\n> model.fit(\r\n>     train_generator,\r\n>     validation_data=test_generator,\r\n>     validation_freq=1,\r\n>     epochs=10,\r\n>     workers=6,\r\n>     use_multiprocessing=True\r\n> )\r\n> ```\r\n> \r\n> \r\n> With this example, the memory increase is less pronounced but still plainly apparent. The memory allocation on my system starts at ~1.8GB at epoch 1 and increases to ~2.15GB by epoch 10. If you watch the child processes being spawned you can see that each epoch 6 more processes are spawned and the pre-existing processes remain alive.\r\n> If you set should_recreate_iterator to return False for KerasSequenceAdapter, the memory starts at ~1.8GB at epoch 1 and increases to ~1.89GB by epoch 10 (much more reasonable). If you watch the child processes being spawned you can see that each epoch the prior 6 processes are killed and a new 6 processes are spawned leaving only 6 processes alive at any given time.\r\n> P.S.\r\n> Is there a reason that KerasSequenceAdapter is set to be re-created each epoch by default?\r\n> I am noticing no detrimental effects by setting should_recreate_iterator to False\r\n> @gadagashwini\r\n> ```\r\n\r\nCould you please tell how / where are you setting `should_recreate_iterator ` ? Im having the same issue. Thank you", "@stringcode86 In the KerasSequenceAdapter class there is a function named `should_recreate_iterator` that simply returns `True`. Change the return value to `False`\r\n\r\nYou can find the function here in the current master: https://github.com/tensorflow/tensorflow/blob/56f4edbb5a78ae9d29d797f39e107a28c9bcf6e5/tensorflow/python/keras/engine/data_adapter.py#L954", "Found another walkaround:\r\n\r\nYou can wrap your Sequence inside an OrderedEnqueuer. I dug into the source code an see actually keras use OrderedEnqueuer as a wrapped for the sequence. The OrderedEnqueuer itself doesn't leak so I guess the leak is hidden in the adapter level which is related to tf. So I came up a way below\r\n```\r\nseq = MySequence(...)\r\nenq = OrderedEnqueuer(self, use_multiprocessing=True)\r\nenq.start(workers=10, max_queue_size=20)\r\ngen = enq.get()\r\n\r\nmodel.fit(\r\n                gen,\r\n                epochs=300,\r\n                steps_per_epoch=steps_per_epoch,\r\n                shuffle=False,\r\n                callbacks=callbacks,\r\n                validation_steps=valid_steps_per_epoch,\r\n                validation_data=valid_dataset,\r\n                initial_epoch=0,\r\n                use_multiprocessing=False,\r\n                max_queue_size=10,\r\n                workers=1,\r\n            )\r\n```\r\nMake sure you set the use_multiprocessing in fit to False and workers to 1. Tested with tf-2.1.0 and I didn't see memory leak comparing to directly using sequence. \r\n\r\n", "> @stringcode86 In the KerasSequenceAdapter class there is a function named `should_recreate_iterator` that simply returns `True`. Change the return value to `False`\r\n> \r\n> You can find the function here in the current master:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/56f4edbb5a78ae9d29d797f39e107a28c9bcf6e5/tensorflow/python/keras/engine/data_adapter.py#L954\r\n\r\nJust a heads up for anyone tempted to use this workaround:\r\n\r\n```\r\nimport tensorflow.python.keras.engine\r\ntensorflow.python.keras.engine.data_adapter.KerasSequenceAdapter.should_recreate_iterator = lambda _: False\r\n```\r\n\r\nThis did not adequately address the memory growth issue for me (training + validation = 820GB), and it did have an effect on the model.  In particular, the trajectory of the validation loss function over epochs differed by 3 orders of magnitude from anything I'd ever seen before.", "https://fantashit.com/linearly-increasing-memory-with-use-multiprocessing-and-keras-sequence/#comment-254237"]}, {"number": 42785, "title": "Portuguese version", "body": "Is it possible to correct some Portuguese notebooks, as they have some typos? I'm a Portuguese native speaker.", "comments": ["Yes, these are community-provided translations\u2014please join as a contributor or a reviewer and send a pull request.\r\n\r\nMore info available here: https://www.tensorflow.org/community/contribute/docs#community_translations\r\nand in README docs in this repo: https://github.com/tensorflow/docs-l10n/tree/master/site\r\n\r\nIf you add your github username to the REVIEWERS file: https://github.com/tensorflow/docs-l10n/blob/master/site/pt/REVIEWERS\r\nyou'll get notified on new pull requests for Portuguese. Thanks!"]}, {"number": 37025, "title": "Stateful LSTMLite conversion", "body": "**System information**\r\n- Red Hat Enterprise Linux Server release 7.7:\r\n- TensorFlow installed from binary:\r\n- TensorFlow version 1.15:\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, LOGISTIC, UNIDIRECTIONAL_SEQUENCE_LSTM, UNPACK. Here is a list of operators for which you will need custom implementations: Assign.\r\nTraceback (most recent call last):\r\n  File \"/home/xyz/anaconda3/envs/tf15/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/xyz/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/xyz/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/xyz/anaconda3/envs/tf15/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/xyz/anaconda3/envs/tf15/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/xyz/anaconda3/envs/tf15/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, LOGISTIC, UNIDIRECTIONAL_SEQUENCE_LSTM, UNPACK. Here is a list of operators for which you will need custom implementations: Assign.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport os\r\nos.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\r\nimport tensorflow as tf\r\nfrom tensorflow_core.python.keras.models import Model, Sequential\r\nfrom tensorflow_core.python.keras.layers.core import Dense, Activation, Lambda, Reshape\r\nfrom tensorflow_core.python.keras.engine.input_layer import Input\r\nfrom tensorflow_core.python.keras.layers.recurrent import RNN, StackedRNNCells\r\nfrom tensorflow_core.lite.experimental.examples.lstm.rnn_cell import TFLiteLSTMCell, TfLiteRNNCell\r\nfrom tensorflow_core.lite.experimental.examples.lstm.rnn import dynamic_rnn\r\nfrom tensorflow_core.python.ops.rnn_cell_impl import LSTMStateTuple\r\nfrom tensorflow_core.lite.python.interpreter import Interpreter\r\nfrom tensorflow_core.python.ops.rnn_cell_impl import MultiRNNCell\r\n\r\ndef get_state_variables(batch_size, cell):\r\n    # For each layer, get the initial state and make a variable out of it\r\n    # to enable updating its value.\r\n    state_variables = []\r\n    for state_c, state_h in cell.zero_state(batch_size, tf.float32):\r\n        state_variables.append(tf.contrib.rnn.LSTMStateTuple(\r\n            tf.Variable(state_c, trainable=False),\r\n            tf.Variable(state_h, trainable=False)))\r\n    # Return as a tuple, so that it can be fed to dynamic_rnn as an initial state\r\n    return tuple(state_variables)\r\n\r\n\r\ndef get_state_update_op(state_variables, new_states):\r\n    # Add an operation to update the train states with the last state tensors\r\n    update_ops = []\r\n    for state_variable, new_state in zip(state_variables, new_states):\r\n        # Assign the new state to the state variables on this layer\r\n        update_ops.extend([state_variable[0].assign(new_state[0]),\r\n                           state_variable[1].assign(new_state[1])])\r\n    # Return a tuple in order to combine all update_ops into a single operation.\r\n    # The tuple's actual value should not be used.\r\n    return tf.tuple(update_ops)\r\n\r\n\r\ndef buildMultiCell(cells):\r\n    return MultiRNNCell(cells)\r\n\r\n\r\ndef buildRNNLayer(inputs, rnn_cells, initial_state=None):\r\n  \"\"\"Build the lstm layer.\r\n\r\n  Args:\r\n    inputs: The input data.\r\n    num_layers: How many LSTM layers do we want.\r\n    num_units: The unmber of hidden units in the LSTM cell.\r\n  \"\"\"\r\n  # Assume the input is sized as [batch, time, input_size], then we're going\r\n  # to transpose to be time-majored.\r\n  transposed_inputs = tf.transpose(inputs, perm=[1, 0, 2])\r\n  outputs, new_state = dynamic_rnn(\r\n      rnn_cells,\r\n      transposed_inputs,\r\n      initial_state=initial_state,\r\n      dtype='float32',\r\n      time_major=True)\r\n  unstacked_outputs = tf.unstack(outputs, axis=0)\r\n  # update_op = get_state_update_op(initial_state, new_state)\r\n  return unstacked_outputs[-1], new_state\r\n\r\n\r\ndef build_rnn_lite(model, state=False):\r\n    tf.reset_default_graph()\r\n    # Construct RNN\r\n    cells = []\r\n    for layer in range(3):\r\n        if model == 'LSTMLite':\r\n            cells.append(TFLiteLSTMCell(192, name='lstm{}'.format(layer)))\r\n        else:\r\n            cells.append(TfLiteRNNCell(192, name='rnn{}'.format(layer)))\r\n\r\n    rnn_cells = Lambda(buildMultiCell, name='multicell')(cells)\r\n    states = get_state_variables(1, rnn_cells)\r\n    if state:\r\n        spec_input = Input(shape=(5, 64,), name='rnn_in', batch_size=1)\r\n        x, new_states = Lambda(buildRNNLayer, arguments={'rnn_cells': rnn_cells, 'initial_state': states}, name=model.lower())(spec_input)\r\n        updated_states = Lambda(get_state_update_op, arguments={'new_states': new_states}, name='update_state')(states)\r\n    else:\r\n        spec_input = Input(shape=(5, 64,), name='rnn_in')\r\n        x, new_states = Lambda(buildRNNLayer, arguments={'rnn_cells': rnn_cells}, name=model.lower())(spec_input)\r\n        updated_states = Lambda(get_state_update_op, arguments={'new_states': states}, name='update_state')(states)\r\n\r\n    out = Dense(64, activation='sigmoid', name='fin_dense')(x)\r\n    return Model(inputs=spec_input, outputs=[out, updated_states])\r\n\r\nmodel = build_rnn_lite('LSTMLite', True)\r\n```\r\nThis now creates a stateful LSTM model which returns the updated states of the LSTM.\r\n```python\r\n###### TF LITE CONVERSION\r\nsess = tf.keras.backend.get_session()\r\ninput_tensor = sess.graph.get_tensor_by_name('rnn_in:0')\r\noutput_tensor = []\r\noutput_tensor.append(sess.graph.get_tensor_by_name('fin_dense/Sigmoid:0'))\r\nall_tensors = [tensor for op in tf.get_default_graph().get_operations() for tensor in op.values()]\r\n# output_tensor = sess.graph.get_tensor_by_name('fin_dense/Sigmoid:0')\r\n\r\n# imp_tensor = []\r\nfor ten in all_tensors:\r\n    if 'update_state/Assign' in ten.name:\r\n        output_tensor.append(ten)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, [input_tensor], output_tensor)\r\n# Note: It will NOT work without enabling the experimental converter!\r\n# `experimental_new_converter` flag.\r\nconverter.experimental_new_converter = True\r\ntflite_seg = converter.convert()\r\n```\r\nThe output tensor list throws an error, if only the `fin_dense/Sigmoid:0` tensor is used the model does get converted but behaves as a stateless model therefore the `update_state` needs to be included in the output tensor list.\r\nI am trying to create a stateful LSTMLite model, I was able to get the model to behave as a stateful model as described here [https://stackoverflow.com/questions/59962348/how-to-create-a-stateful-tensorflowlite-rnn-model-in-tf1-15](url) but am unable to convert the model to TFLite.\r\n", "comments": ["@razor1179 Can you please provide a simple standalone code to reproduce the issue? I tried but got an error `NameError: name 'MultiRNNCell' is not defined`. Please check the [gist here](\r\nhttps://colab.sandbox.google.com/gist/jvishnuvardhan/dd69c6005d2afdd176df3870697c070b/untitled865.ipynb#scrollTo=GQGYzikS4Hok). Thanks!", "> @razor1179 Can you please provide a simple standalone code to reproduce the issue? I tried but got an error `NameError: name 'MultiRNNCell' is not defined`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/dd69c6005d2afdd176df3870697c070b/untitled865.ipynb#scrollTo=GQGYzikS4Hok). Thanks!\r\n\r\n@jvishnuvardhan Please add line below\r\n`from tensorflow_core.python.ops.rnn_cell_impl import MultiRNNCell`\r\n[updated gist here](https://colab.research.google.com/gist/razor1179/8bbac5d7eb3758615a839a2b5d5380ba/untitled865.ipynb)", "@razor1179 When I enable custom_ops as mentioned by the error description, I was able to convert the model. \r\nI added this line just before the conversion. \r\n`converter.allow_custom_ops = True `\r\n\r\nAdded this line to write the tf_lite_model.\r\n`open(\"my_tflitemodel.tflite\",'wb').write(tflite_seg)`\r\n\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/3bf9170ef111f9bb0b3aaaf2ab8bb2a8/untitled865.ipynb). \r\n\r\nAnother thing I want to point out is Importing using `tensorflow_core.` is not a suggested way for importing any module as mentioned [here](https://github.com/tensorflow/tensorflow/issues/32957#issuecomment-543819065) and [another resource](https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546). Thanks!\r\n\r\nPlease let us know how it progresses. Thanks!", "@razor1179 If possible, can you please try `TF2.1` or `tf-nightly`? I suspect statefull LSTM was not part of `TF1.x`. Thanks!", "@jvishnuvardhan sure will try and get back to you.", "@razor1179 Any update from your side. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37024, "title": "Convolution operations (such as Conv2d) does not detect corner case 'kernel_size = 0', which leads to an unexpected result.", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: -\r\n- TensorFlow installed from (source or\r\nbinary):Binary\r\n- TensorFlow version (use command below): 1.15.0 -cpu\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):-\r\n- GCC/Compiler version (if compiling from\r\nsource): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory:-\r\n\r\n**Describe the current behavior**\r\nWhen I use convolution-related operations, Tensorflow doesn't seem to handle the corner case  'kernel_size = 0' well. The problem can be divided into two parts: \r\n\r\n1) When 'padding = same' is set, **SeparableConv2D \\ DepthwiseConv2D\\Conv2D \\ Conv2DTranspose** takes kernel_size = 0 as a normal value to calculate the padding/crop shape, and finally reports an **Negative-shape error** . For example,  `Conv2DTranspos` reports a  `ValueError: crops cannot be negative for 'conv2d_transpose_1/atrous_conv2d_transpose/BatchToSpaceND' (op: 'BatchToSpaceND') with input shapes: [9,10,10,2], [2], [2,2] and with computed input tensors: input[1] = <3 3>, input[2] = <[-2 0][-2 0]>.`  at `tensorflow_core\\python\\framework\\ops.py line 1610`. The negative value is actually created  in `tensorflow_core\\python\\ops\\nn_ops.py`. For example, the negative value which leads to the padding error of `DepthwiseConv2D` is calculated around line 619 of nn_ops.py). See the following snapshot of `nn_ops.py`. \r\n\r\n![pic1](https://user-images.githubusercontent.com/61384431/75172507-573c3400-5768-11ea-9747-dcd52835118a.jpg)\r\n\r\n\r\n2) When  'padding=valid' is set, the situation is even worse. **`DepthwiseConv2D` and `SeparableConv2D` can build the model and  even predict. But the outputs are the all-zero matrices. `Conv2D` can also build and save the model, but it seems to get stuck in an infinite loop when predicting. It takes a lot of time and eventually gets no result. **Only `Conv2DTranspose` behaves normally and  reports the following error in `tensorflow_core\\python\\client\\session.py line1470`\r\n\r\n``` yaml\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Conv2DCustomBackpropInput: Size of out_backprop doesn't match computed: actual = 16, computed = 17 spatial_dim: 1 input: 16 filter: 0 output: 16 stride: 1 dilation: 1\r\n\t [[{{node conv2d_transpose_1_1/atrous_conv2d_transpose/Conv2DBackpropInput}}]]\r\n```\r\n\r\nFrom the description above, we conclude that **Tensorflow seems to lack a critical check whether `kernel_size` is  0 when conducting convolution-related operations, which is a dangerous corner case. This illegal parameter should not be brought into the calculation**, but Tensorflow uses it to build a layer and even uses this layer to process the input and get all zero matrices as output. This should be a logical bug.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n``` python\r\nimport os\r\nimport numpy as np\r\nimport keras.layers as L\r\nimport keras.backend as K\r\nimport importlib\r\nfrom keras.engine import Model, Input\r\n\r\n## Using TensorFlow as Keras backend.\r\n## Input dtype default is float32\r\n\r\n'''kwargs={'filters': 8, \r\n  'kernel_size': 0,\r\n  'padding': 'same',\r\n  'strides': 2, \r\n  'dilation_rate': 1,\r\n  'data_format': 'channels_last'}'''# Conv2d/SeparableConv2D\r\n\r\nkwargs={'kernel_size': 0,\r\n  'padding': 'same',\r\n  'strides': 2, \r\n  'dilation_rate': 1,\r\n  'data_format': 'channels_last'}# DepthwiseConv2d\r\ninput = (10 * np.random.random((1,32,32,16)))\r\nlayer = L.convolutional.DepthwiseConv2D(**kwargs)# you can use Conv2D\\SeparableConv2D\\Conv2DTranspose instead of DepthwisConv2D\r\n#layer = L.convolutional.SeparableConv2D(**kwargs)\r\n#layer=L.convolutional.Conv2D(**kwargs)\r\nx = Input(batch_shape=input.shape)\r\ny = layer(x)\r\nbk_model = Model(x, y)\r\nmodel_path = os.path.join('./', 'model.h5')\r\nbk_model.save(model_path, bk_model)\r\n'''from keras.models import load_model\r\nmodel = load_model(model_path)\r\noutput = model.predict(input)'''\r\nprint('finish')\r\n```\r\n", "comments": ["@Jup11ter, Thanks for reporting this issue.\r\nI tried replicating the issue with TF 1.15. but getting different error. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/032c54ca5347603ded5e772c0858f476/untitled422.ipynb) and confirm. Thanks!", "> @Jup11ter, Thanks for reporting this issue.\r\n> I tried replicating the issue with TF 1.15. but getting different error.\r\n> Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/032c54ca5347603ded5e772c0858f476/untitled422.ipynb) and confirm. Thanks!\r\n\r\nThank you for your help!\r\nThe code expression in the previous version is not clear enough, which brings inconvenience to your reproduction. **Now it has been changed to a better version, and the same result as the issue described above can also be obtained on colab** (`kernel_size = 0` can save the model  (Without reporting an error). Hoping to have a positive effect on solving the problem.\r\nThank you again\r\n![image](https://user-images.githubusercontent.com/61384431/76085155-7484c400-5fec-11ea-89b9-f3920521fe65.png)\r\n\r\n", "@Jup11ter, Tried with update code to replicate issue but code executed without any error. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/9468015fd6292e82bf0741187aaa0b1c/untitled432.ipynb). Thanks", "> @Jup11ter, Tried with update code to replicate issue but code executed without any error.\r\n> Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/9468015fd6292e82bf0741187aaa0b1c/untitled432.ipynb). Thanks\r\n\r\nThank you for your help! \r\nYes, this is exactly the problem I described above. **Using the dangerous parameter `kernel_size = 0`, TensorFlow can still generate and save the model**. It seems to lack a detection mechanism for this illogical parameters.\r\n", "Check for zero `kernel_size` in conv layers has been added. You should have it in tf-nightly version.\r\nhttps://github.com/tensorflow/tensorflow/blob/84eb083bb5328912dde064b8b0f61d28c6edbe43/tensorflow/python/keras/layers/convolutional.py#L132\r\ncommit 1e102f63964365d82d7f22402b7ba21e0e0e64fe", "> Check for zero `kernel_size` in conv layers has been added. You should have it in tf-nightly version.\r\n> https://github.com/tensorflow/tensorflow/blob/84eb083bb5328912dde064b8b0f61d28c6edbe43/tensorflow/python/keras/layers/convolutional.py#L132\r\n> \r\n> \r\n> commit [1e102f6](https://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe)\r\n\r\nI will try this version, Thank you very much for your help! ", "This should have been fixed. Closing it for now. Let us know if any other issues come up", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37024\">No</a>\n"]}, {"number": 37023, "title": "RNN unrolled, wrong quantization graph", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (or github SHA if from source):\r\nversion is 2.2.0-dev20200218, git version is v1.12.1-25080-gca585e7\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThe problem has appeared on much bigger model, but I can demonstrate it on the MNIST example.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\"version is {}, git version is {}\".format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\n\r\nfrom model import Model\r\n\r\nclass SimpleModel(Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.model_name = \"mnist\"\r\n        self.train_data = None\r\n        self.test_data = None\r\n        self.calib_data = None\r\n        self.num_calib = 1000\r\n        # (data preprocessing) Normalize the input image so that\r\n        # each pixel value is between 0 to 1.\r\n        self.pre_process = lambda x: x / 255.0\r\n\r\n        self._load_data()\r\n        self._set_path()\r\n\r\n    def _load_data(self):\r\n        # Load MNIST dataset\r\n        mnist = tf.keras.datasets.mnist\r\n\r\n        # _data: (images, labels)\r\n        self.train_data, self.test_data = mnist.load_data()\r\n        self.calib_data = self.pre_process(\r\n            self.train_data[0][0 : self.num_calib].astype(np.float32)\r\n        )\r\n\r\n    def train(self):\r\n        cell = tf.keras.layers.GRUCell(3)\r\n\r\n        model = tf.keras.models.Sequential([\r\n            tf.keras.layers.Input(shape=(28, 28), name='input'),\r\n            #tf.keras.layers.LSTM(32),\r\n            tf.keras.layers.RNN(cell, unroll=True),\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')\r\n        ])\r\n        model.summary()\r\n\r\n        train_images = self.pre_process(self.train_data[0])\r\n        train_labels = self.train_data[1]\r\n        test_images = self.pre_process(self.test_data[0])\r\n        test_labels = self.test_data[1]\r\n        # Train the digit classification model\r\n        model.compile(\r\n            optimizer=\"adam\",\r\n            loss=\"sparse_categorical_crossentropy\",\r\n            metrics=[\"accuracy\"],\r\n        )\r\n        model.fit(\r\n            train_images,\r\n            train_labels,\r\n            epochs=1,\r\n            validation_data=(test_images, test_labels),\r\n        )\r\n        # dump SavedModel - ANOTHER BUG HERE!\r\n        #model.save(str(self.savedModel_dir))\r\n\r\n        return model\r\n\r\n    def eval(self, tflite_model_path: str):\r\n        interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\r\n        interpreter.allocate_tensors()\r\n\r\n        input_index = interpreter.get_input_details()[0][\"index\"]\r\n        output_index = interpreter.get_output_details()[0][\"index\"]\r\n\r\n        # (data preprocessing) Normalize the input image so that\r\n        # each pixel value is between 0 to 1.\r\n        test_images = self.pre_process(self.test_data[0])\r\n        test_labels = self.test_data[1]\r\n        # Run predictions on every image in the \"test\" dataset.\r\n        prediction_digits = []\r\n        for test_image in test_images:\r\n            # Pre-processing: add batch dimension and convert to float32 to match with\r\n            # the model's input data format.\r\n            test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\r\n            interpreter.set_tensor(input_index, test_image)\r\n\r\n            # Run inference.\r\n            interpreter.invoke()\r\n\r\n            # Post-processing: remove batch dimension and find the digit with highest\r\n            # probability.\r\n            output = interpreter.tensor(output_index)\r\n            digit = np.argmax(output()[0])\r\n            prediction_digits.append(digit)\r\n\r\n        # Compare prediction results with ground truth labels to calculate accuracy.\r\n        accurate_count = 0\r\n        for index, _ in enumerate(prediction_digits):\r\n            if prediction_digits[index] == test_labels[index]:\r\n                accurate_count += 1\r\n        accuracy = accurate_count * 1.0 / len(prediction_digits)\r\n\r\n        return accuracy\r\n\r\n    def _get_calib_data_func(self):\r\n        def representative_data_gen():\r\n            for input_value in self.calib_data:\r\n                input_value = np.expand_dims(input_value, axis=0).astype(np.float32)\r\n                yield [input_value]\r\n\r\n        return representative_data_gen\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    temp = SimpleModel()\r\n    model = temp.train()\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = temp._get_calib_data_func()\r\n\r\n    # save INT8 tflite\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n        tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.experimental_new_converter = True\r\n    tflite_model_INT8 = converter.convert()\r\n    open(\"lstm_unrolled_int8.tflite\", \"wb\").write(tflite_model_INT8)\r\n\r\n    print(\"INT8 model eval results: {:f}\".format(temp.eval(\"lstm_unrolled_int8\")))\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/elezhe01/work/discovery/int16_crosstest/model/SimpleModelLSTM_self.py\", line 132, in <module>\r\n    print(\"INT8 model eval results: {:f}\".format(temp.eval(\"lstm_unrolled_int8\")))\r\n  File \"/home/elezhe01/work/discovery/int16_crosstest/model/SimpleModelLSTM_self.py\", line 69, in eval\r\n    interpreter.allocate_tensors()\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 242, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/elezhe01/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:129 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 3 (FULLY_CONNECTED) failed to prepare.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nAnother bug: The model with unrolled RNN cell can't be saved - see above\r\n```\r\n\r\n**Failure details**\r\n\r\nThe error above means that the scale of bias is not equal to the scale of activations * the scale of weights as it should be. As far as I can see, after unroll operation the bias is shared, but it should be quantized for each layer separately. \r\n", "comments": ["Hi @jianlijianli ! Sorry to bother you, but could you please take a look ?\r\nThe main problem here is that for an unrolled RNN cell the bias is shared and it has the same quantization for all layers, but it must have different quantizations for different layers: param scale should be scale of activations * scale of weights. Is it a known issue ? Are there plans to fix it ? \r\nWe have a bunch of models that hit this problem/exceptions.\r\nThanks!", "@wwwind  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37023\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37023\">No</a>\n"]}, {"number": 37022, "title": "[MLIR][XLA] Polish LHLO copy operation removal", "body": "In this PR, we\r\n- Created a separate file for lhlo-copy-removal pass.\r\n- Created a separate test file with dedicated test cases for lhlo-copy-removal pass.\r\n- Adapted Hlo-To-Lhlo-Legalization tests.\r\n\r\nWe addressed all the comments we received in this [PR](https://github.com/tensorflow/tensorflow/pull/36335)", "comments": ["@dfki-ehna Can you please fix the build failures? Thanks!", "This PR was commited as https://github.com/tensorflow/tensorflow/commit/efff893c709b585da9771cb3a2997321efef36ae.  I am not sure why was it not automatically closed.", "Changes have been merged into master by commit **efff893**. So closing the PR."]}, {"number": 37021, "title": "There are no documentations related to decode_predictions() and preprocess_input() in any keras.applications", "body": "## URL(s) with the issue:\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/applications?version=nightly](https://www.tensorflow.org/api_docs/python/tf/keras/applications?version=nightly)\r\n\r\n\r\n## Description of issue (what needs changing):\r\nThe doc corresponding to these two functions must be added in each application model doc.\r\n\r\nAre you planning to also submit a pull request to fix the issue? \r\nYes, Will mention these issue soon in those PRs.\r\n", "comments": []}, {"number": 37020, "title": "Modified metric_utils.py to fix conflicts with floatx and float32", "body": "Fixes #36790. @pavithrasv, @MarkDaoust  Please review this PR.", "comments": ["@ashutosh1919 Can you please check MarkDaoust's comments and keep us posted? Thanks!", "@gbaned , sure. I am working on it.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@ashutosh1919 Any update on this, please. Thanks!", "No action in a while. Let's close.  \r\n\r\n@ashutosh1919 feel free to reopen or make a new PR if you figure out those issues."]}, {"number": 37019, "title": "Tensorflow 2.1.0 :  TypeError: __new__() got an unexpected keyword argument 'serialized_options'", "body": "I am facing an error while importing tensorflow-2.1.0 (CPU) with Python 3.6 in Ubuntu 18.\r\n protobuf Version: 3.11.3\r\nTried many solutions provided in the same Tensorflow GitHub issue pages..But none were useful. \r\nI dont have any NVIDIA, CUDA etc.. (I dont want to install them as i would like to go with CPU)\r\n\r\nFollowing errors appeared.\r\n---------------------------------\r\n[Mon Feb 24 11:44:52.926212 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372] mod_wsgi (pid=5520): Exception occurred processing WSGI script '/var/www/testnig/service.wsgi'.\r\n[Mon Feb 24 11:44:52.927298 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372] Traceback (most recent call last):\r\n[Mon Feb 24 11:44:52.927347 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/service.wsgi\", line 9, in <module>\r\n[Mon Feb 24 11:44:52.927352 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from service import app as application\r\n[Mon Feb 24 11:44:52.927358 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/service.py\", line 11, in <module>\r\n[Mon Feb 24 11:44:52.927362 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     import classify\r\n[Mon Feb 24 11:44:52.927367 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/classify.py\", line 3, in <module>\r\n[Mon Feb 24 11:44:52.927370 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     import tensorflow as tf\r\n[Mon Feb 24 11:44:52.927376 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow/__init__.py\", line 98, in <module>\r\n[Mon Feb 24 11:44:52.927379 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow_core import *\r\n[Mon Feb 24 11:44:52.927384 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/__init__.py\", line 42, in <module>\r\n[Mon Feb 24 11:44:52.927388 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from . _api.v2 import audio\r\n[Mon Feb 24 11:44:52.927393 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/_api/v2/audio/__init__.py\", line 10, in <module>\r\n[Mon Feb 24 11:44:52.927397 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n[Mon Feb 24 11:44:52.927402 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_audio_ops.py\", line 11, in <module>\r\n[Mon Feb 24 11:44:52.927406 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow.python.eager import context as _context\r\n[Mon Feb 24 11:44:52.927411 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py\", line 29, in <module>\r\n[Mon Feb 24 11:44:52.927414 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow.core.protobuf import config_pb2\r\n[Mon Feb 24 11:44:52.927420 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/core/protobuf/config_pb2.py\", line 16, in <module>\r\n[Mon Feb 24 11:44:52.927423 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow.core.framework import cost_graph_pb2 as tensorflow_dot_core_dot_framework_dot_cost__graph__pb2\r\n[Mon Feb 24 11:44:52.927436 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/core/framework/cost_graph_pb2.py\", line 16, in <module>\r\n[Mon Feb 24 11:44:52.927440 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\r\n[Mon Feb 24 11:44:52.927445 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]   File \"/var/www/testnig/venv/lib/python3.6/site-packages/tensorflow_core/core/framework/tensor_shape_pb2.py\", line 23, in <module>\r\n[Mon Feb 24 11:44:52.927449 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote 127.0.0.1:34372]     serialized_pb=_b('\\\\n,tensorflow/core/framework/tensor_shape.proto\\\\x12\\\\ntensorflow\\\\\"z\\\\n\\\\x10TensorShapeProto\\\\x12-\\\\n\\\\x03\\\\x64im\\\\x18\\\\x02 \\\\x03(\\\\x0b\\\\x32 .tensorflow.TensorShapeProto.Dim\\\\x12\\\\x14\\\\n\\\\x0cunknown_rank\\\\x18\\\\x03 \\\\x01(\\\\x08\\\\x1a!\\\\n\\\\x03\\\\x44im\\\\x12\\\\x0c\\\\n\\\\x04size\\\\x18\\\\x01 \\\\x01(\\\\x03\\\\x12\\\\x0c\\\\n\\\\x04name\\\\x18\\\\x02 \\\\x01(\\\\tBq\\\\n\\\\x18org.tensorflow.frameworkB\\\\x11TensorShapeProtosP\\\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\\\xf8\\\\x01\\\\x01\\\\x62\\\\x06proto3')\r\n[Mon Feb 24 11:44:52.927464 2020] [wsgi:error] [pid 5520:tid 140127778391808] [remote **127.0.0.1:34372] **TypeError: __new__() got an unexpected keyword argument *emphasized text*'serialized_options'****\r\n\r\nSteps to reproduce : (I am using Apache2, Flask & python3 for NSFW detection). However following are the simplest ways to reproduce. \r\n 1. In Ubuntu 18, install python 3.6 and create a Virtualenv. Activate Virtualenv\r\n 2. pip3 install tensorflow\r\n 3. Clone or Download GIT --> github.com/minto5050/NSFW-detection \r\n4. python3 classify.py test_image.jpg\r\n\r\nPlease let me know if you need any other information", "comments": ["@votesappteam \r\nplease refer to this [link](https://github.com/tensorflow/models/issues/3995) related to protobuf, let us know if it helps resolve your issue.", "@Saduf2019 \r\nI have tried all of the solution you mentioned.\r\nPlease understand those were all Tensorflow version less than 2.\r\nNone of the solution helped.\r\nPlease help.", "@votesappteam \r\nI tried replicating the code shared by you and face a different error,please refer to [the gist](https://colab.sandbox.google.com/gist/Saduf2019/967ba100b6072abcfc5b11fdaff93f04/untitled62.ipynb) for the same", "Hi Sadu,\r\nThe original code Github is not compatible with Tensorflow version 2.\r\nPlease replace code in classify.py with below:\r\n\r\nimport os, sys\r\n\r\nimport tensorflow as tf\r\n#https://github.com/minto5050/NSFW-detection\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\ntf.config.optimizer.set_jit(True)\r\n\r\n    # change this as you see fit\r\nimage_path = img\r\n\r\n    # Read in the image_data\r\nimage_data = tf.io.gfile.GFile(image_path, 'rb').read()\r\n\r\n    # Loads label file, strips off carriage return\r\nlabel_lines = [line.rstrip() for line\r\n                   in tf.io.gfile.GFile(\"retrained_labels.txt\")]\r\n\r\n    # Unpersists graph from file\r\nwith tf.io.gfile.GFile(\"retrained_graph.pb\", 'rb') as f:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    tf.import_graph_def(graph_def, name='')\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n        # Feed the image_data as input to the graph and get first prediction\r\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\r\n\r\n    predictions = sess.run(softmax_tensor, \\\r\n                               {'DecodeJpeg/contents:0': image_data})\r\n\r\n        # Sort to show labels of first prediction in order of confidence\r\n    top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\r\n    scoreDict = []\r\n    for node_id in top_k:\r\n        human_string = label_lines[node_id]\r\n        score = predictions[0][node_id]\r\n        scoreDict.append(human_string+\":\"+str(score))\r\n        print('%s (score = %.5f)' % (human_string, score))", "@votesappteam \r\ncould you please provide us with simple standalone indented code to replicate your issue in our local or share a gist of the error, if possible in colab for us to analyse it.", "@votesappteam\r\ncould you please update as per above comment", "I will respond\n\nOn Wed, Mar 18, 2020 at 12:53 PM, Saduf2019 <notifications@github.com>\nwrote:\n\n> @votesappteam <https://github.com/votesappteam>\n> could you please update as per above comment\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/37019#issuecomment-600464239>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOUTBXQNKO2G6G7SS4ZUIA3RIBZF5ANCNFSM4K2JDJIA>\n> .\n>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I'm having the same issue with tensorflow version 2.3.1 is there any solution? \r\nThank you in advance "]}, {"number": 37018, "title": "Bugfix: keras custom train config serialize", "body": "fix the issue that Keras cannot correctly serialize custom train config (loss, metrics) for Model.\r\n\r\nFixes #36259", "comments": []}, {"number": 37017, "title": "Added information", "body": "Added information about initial_state for Bidirectional RNNs", "comments": ["Note: just added comments. No code changes.", "@gavinlive Can you please check reviewer comments and keep us posted. Thanks!", "@gavinlive Can you please address Ubuntu Sanity errors? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37016, "title": "[TfLite] SPLIT_V quantization support for int8", "body": "This PR adds int8 quantization for SPLIT_V operator. See issue https://github.com/tensorflow/tensorflow/issues/36965\r\nI added tests as for split_v for int8 by refactoring the existing one, so that they are not Copy&Paste.\r\n\r\n", "comments": ["We need to bump the op version for this.\r\n\r\nTo do this, you can add [logic in GetBuiltinOperatorVersion to detect the input/output types](https://github.com/tensorflow/tensorflow/blob/306dee4096e97520b1218d78c23b1a72941a9508/tensorflow/lite/tools/versioning/op_version.cc), and use an incremented version, update the op version to TFlite version mapping [here](https://github.com/tensorflow/tensorflow/blob/306dee4096e97520b1218d78c23b1a72941a9508/tensorflow/lite/toco/tflite/op_version.cc), and then update the max version where the builtin is registered [here](https://github.com/tensorflow/tensorflow/blob/306dee4096e97520b1218d78c23b1a72941a9508/tensorflow/lite/kernels/register.cc#L43). ", "Hi @jdduke I bumped a version for this operator as requested. Please take a look. Thanks for the review!", "@gbaned Can it be merged?", "Hi @rthadur ! Can this PR be merged ? It has been marked as 'ready_to_pull' many days ago. Thanks!", "@wwwind it got stuck internally , @jdduke can you please help merge this PR internally."]}, {"number": 37015, "title": "Error with mkl flag", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 18363.657\r\n- TensorFlow installed from (source or binary):  TF from git 2.1\r\n- Python version: 3.8.1\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.29\r\n- CUDA/cuDNN version: 10.2/7.6.5\r\n- GPU model and memory: GF 1070 ti\r\n- CPU model: Intel\u00ae Core\u2122 i7-9700K\r\n- VS: 2019\r\n\r\n\r\n\r\n**Describe the problem**\r\nAll settings as on the site https://www.tensorflow.org/install/source_windows?hl=ru\r\nMy command:\r\nbazel build --config=mkl --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nAt the end of the build process I have the following error\r\nERROR: C:/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: \u2550\u0445 \u044d\u0440\u0449\u0444\u0445\u044d \u0454\u044a\u0440\u0447\u0440\u044d\u044d\u221a\u0449 \u044c\u044e\u0444\u0454\u044b\u2116.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\Build\\AppData\\Local\\Temp\\Bazel.runfiles_ykxci5xn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: \u2550\u0445 \u044d\u0440\u0449\u0444\u0445\u044d \u0454\u044a\u0440\u0447\u0440\u044d\u044d\u221a\u0449 \u044c\u044e\u0444\u0454\u044b\u2116.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/tensorflow/tensorflow/tools/pip_package/BUILD:223:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nINFO: Elapsed time: 11991.974s, Critical Path: 9750.03s\r\nINFO: 6217 processes: 6217 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nWithout the mkl option, the build completes successfully.\r\nwhat could be the problem?", "comments": ["@Expert73 \r\nCould you please check this [link](https://github.com/tensorflow/tensorflow/issues/36167) and [link](https://github.com/tensorflow/tensorflow/issues/11326) on a similar issue and let us know if it helps. Thanks!", "All critical dll in path: cudnn64_7, mkl, msvcp140*. But I have an error.\r\nwithout mkl in bild command all works super fine. But mkl does not allow you to complete the build.\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal\r\nHow do I understand which dll?\r\n", "I tried to build with Mkl on another computer (other hardware without GPU), another operating system (Windows 8.1) - still get the same error. If you look at the latest messages on the forum, several people have already encountered the mkl problem. Can someone authoritatively look at the problem?", "The build with the mkl flag before the error lasts from 6 to 8 hours. Build without this parameter takes 1 hour.\r\nA lot of identical files with very long paths are created. For example, such paths with mkl*.dll I counted more than 10:\r\n\r\n\r\n\\\\?\\C:\\tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\execroot\\org_tensorflow\\output_dir\\execroot\\org_tensorflow\\external\\mkl_windows\\lib\r\n\r\n\\\\?\\C:\\tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\mkl_windows\\lib\r\n\r\n\\\\?\\C:\\tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\external\\org_tensorflow\\output_dir\\execroot\\org_tensorflow\\output_dir\\external\\mkl_windows\\lib", "I also tried to install all the libraries from the INTEL\u00ae SOFTWARE DEVELOPMENT PRODUCTS series. The same error still occurs - it is specified in the first message. Long paths are also allowed in operating systems. But that doesn't help either.", "I even tried using mkl*.dll already compiled by myself and specified them in the TF_MKL_ROOT environment. An error still occurs.", "I tried to build with different parameters:\r\nbazel build --config=mkl --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel build --config=mkl --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel build --config=mkl --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nbazel --output_base=output_dir build --config=mkl --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nFor different CPU isntructions: AVX, AVX2\r\n\r\nI still get the same error.", "Can someone from Intel Corporation look at this problem?", "Hi,\r\nI'll take a look at this issue.", "The build of the software code was excellent for the release version 2.1. The problem was in Path. You can close the problem. Problem solved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37015\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37015\">No</a>\n"]}, {"number": 37014, "title": "Added reduce_variance for RaggedTensor with extra optimization", "body": "Fixes #37000. Please take a look at [this proof](https://math.stackexchange.com/a/755344) for the formula used. So, it is good to modify this if  it's optimization as suggested in the issue.", "comments": ["Just to clarify, the purpose of this suggestion was not to improve performance, but for compatibility with ragged tensors. In fact, the current method is 2x mean, 1x square, 1x diff, while the proposed one is 2x mean, 2x square, 2x diff, making it one op more. So it would probably be best to only apply this method to ragged tensors.", "@ashutosh1919 Any update on this PR, please. Thanks!", "@mihaimaruseac , Added condition to check instance of `RaggedTensor`.", "@ashutosh1919 Can you please address Ubuntu Sanity errors? Thanks!", "@gbaned , I don't think this error is because of the code that I changed in this PR. But I will rebase master so that If it has to do something with that then it will be resolved.", "@ashutosh1919 Still, Ubuntu Sanity errors appearing, Can you fix those ? Thanks!", "@gbaned, as I mentioned in the previous comment that this error is not because of the code which I added. And if it is because of that then I am not able to see the error messages corresponding to that.", "Actually, the error is from this PR, it is a pylint error.\r\n\r\n```\r\nFAIL: Found 2 non-whitelisted pylint errors:\r\ntensorflow/python/ops/math_ops.py:2116: [C0330(bad-continuation), ] Wrong continued indentation (add 7 spaces).\r\n\r\ntensorflow/python/ops/math_ops.py:2117: [C0330(bad-continuation), ] Wrong continued indentation (add 7 spaces).\r\n```\r\n\r\n![1](https://user-images.githubusercontent.com/323199/76779778-2187db00-6769-11ea-9030-112b37798d74.png)\r\n", "Apologies @mihaimaruseac and @gbaned , I didn't notice that silly mistake. I have resolved that. Please run ci. Let's check if it works now.", "There are slightly under 4000 tests failing internally with this PR. We will have to import it manually", "@mihaimaruseac , Please suggest the chnages. I am facing this issue for the first time and that's why don't know much about it.", "There are internal tests which depend on the numerical precision of the old code. So far I don't think you have to do anything", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "@mihaimaruseac Any update on this PR, please. Thanks!", "Instead of adding a special case in reduce_variance in math_ops.py, this should be implemented by defining a `reduce_variance` method in `python/ops/ragged/ragged_math_ops.py`, and registering it for type-based dispatch in `python/ops/ragged/ragged_dispatch.py`.  (You can search for `reduce_mean` in those two files, and it should be fairly clear what needs to be done.)  Ideally, you should also add tests in `ragged_reduce_op_test.py` and `ragged_dispatch_test.py` as well.", "@ashutosh1919 Can you please check @edloper's comments and keep us posted ? Thanks!", "@ashutosh1919 Any update on this PR? Please. Thanks!", "@edloper , @mihaimaruseac - I have added reduce_variance and its tests as per previous comments by @edloper . Please review.", "@edloper , apologies. Actually i was running the bazel tests in my local in parallel. But it failed at the end and I noticed that today morning. \r\nI have done changes that you mentioned and now bazel tests are passing in my local.\r\nPlease review.", "Resolved doctest failiure. But there was also error shown as below.\r\n```\r\nAttributeError: module 'tensorflow.compat.v2' has no attribute 'reduce_variance'\r\n```\r\n\r\n@bhack , can you please run the kokoro?", "isn't `tf.math.reduce_variance`?", "@bhack , Thanks for noticing the error. Done changes. Hope it'll work now.", "@mihaimaruseac , can you please approve again? Fixed doctest", "@mihaimaruseac , can you please look at 3 failed builds? Looks like it is not because of this PR.", "@edloper , can you please approve again? I changed `gen_math_ops` to `math_ops`"]}, {"number": 37013, "title": "TensorFlow tf.data.Dataset API extremely slow for 3D-shaped pipeline", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): binary pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.7.6\r\n- CUDA/cuDNN version: 7.6.4.38-1\r\n- GPU model and memory: Quadro RTX 6000 22752 MB\r\n\r\n**Describe the current behavior**\r\nIn the data pipeline given below, the performance is extremely low. The pipeline produces output of the shape `((128, None, 128), (128, None, 5))` and if either dimension 0 (batching) or dimension 1 (windowing) is removed, the performance is much higher. Unfortunately both batching and windowing are required for efficient and meaningful training.\r\n\r\nAdditionally, the iteration of the Dataset does not work as expected with 3D output. The shape of the output tensor (loop body at the end: `print([x.shape for x in e])`) is not printed at all. When removing either windowing or batching, the shape is printed as expected. From this I suspect this issue could also be a bug.\r\n\r\nTimings for comparison (note: number of time steps / dim 1 (`None`) is avg. 312.5 = 40000 / 128):\r\n- 1x Shape `((128, None, 128), (128, None, 5))`:\r\n**546.8976650238037 s**\r\n- 1000x Shape `((128, 128), (128, 5))` (no windowing):\r\n8.29618239402771 s * 312.5 / 1000 = **2.592556998133659375 s**\r\nSpeedup: 210x faster\r\n- 1000x Shape `((None, 128), (None, 5))` (no batching):\r\n65.43172574043274 * 128 / 1000 = **8,37526089477539072 s**\r\nSpeedup: 65x faster\r\n\r\n**Describe the expected behavior**\r\nIt is expected for the performance to scale linearly with the amount of data. The shape of the output data should not have a big negative effect on performance.\r\n\r\n**Standalone code to reproduce the issue** \r\n```\r\nimport time\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow {}\".format(tf.__version__))\r\n\r\nnum_features = 128\r\nnum_labels = 5\r\nbatch_size = 128\r\nwin_size = 2048\r\nlen_max = 600000\r\n\r\n# create some sample .tfrecord files with different length\r\n\r\nl1 = len_max - 44260\r\nX1, Y1 = (np.ones((l1, num_features + 2)), np.zeros((l1, num_labels)))\r\nl2 = len_max - 121340\r\nX2, Y2 = (np.ones((l2, num_features + 2)), np.zeros((l2, num_labels)))\r\n\r\ndef write_tfrecord(X, Y, fn):\r\n    with tf.Graph().as_default():\r\n        ds = tf.data.Dataset.from_tensor_slices((X, Y))\r\n\r\n        sstring = ds.map(lambda *x: \r\n           tf.reshape(tf.py_function(lambda *v:\r\n               tf.train.Example(features=tf.train.Features(feature={\r\n                   \"features\": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),\r\n                   \"label\": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),\r\n               })).SerializeToString(), x, tf.string\r\n           ), ())\r\n        )\r\n\r\n        writer = tf.data.experimental.TFRecordWriter(fn)\r\n        writer_op = writer.write(sstring)\r\n\r\n        sess = tf.compat.v1.Session()\r\n        sess.run(tf.compat.v1.global_variables_initializer())\r\n        sess.run(writer_op)\r\n        sess.close()\r\n\r\nfiles_base = [\"./temp1.tfrecord\", \"./temp2.tfrecord\"]\r\n\r\nwrite_tfrecord(X1, Y1, files_base[0])\r\nwrite_tfrecord(X2, Y2, files_base[1])\r\n\r\n# take 200 random files as dataset\r\n\r\nfiles = random.choices(files_base, k=200)\r\n\r\nds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)\r\nfs_len = 0\r\nfor f in ds_fs:\r\n    fs_len += 1\r\nprint(\"Reading {} tfrecord files...\".format(fs_len))\r\n\r\n# create a StaticHashTable holding the length for trimming\r\n\r\nds_f_len_table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(\r\n        tf.constant(files_base),\r\n        tf.constant([random.randint(30000, 50000), random.randint(30000, 50000)], dtype=tf.int64)\r\n), -1)\r\n\r\n# prepare the Dataset\r\n\r\ndef prep_ds_file(file):\r\n    _ds = tf.data.TFRecordDataset(file)\r\n    _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {\r\n        \"features\": tf.io.FixedLenFeature([num_features + 2], tf.float32),\r\n        \"label\": tf.io.FixedLenFeature([num_labels], tf.float32),\r\n    }), num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    print(_ds)\r\n\r\n    _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[\"features\"][2:], v[\"label\"])))\r\n    print(_ds)\r\n\r\n    _trunc = ds_f_len_table.lookup(file)\r\n    _ds = _ds.take(_trunc)\r\n    print(_ds)\r\n    _num_tsteps = _trunc // batch_size\r\n\r\n    ####################################################################################################\r\n    # WINDOWING                                                                                        #\r\n    ####################################################################################################\r\n    _ds = _ds.window(size=_num_tsteps, shift=win_size//2, stride=1, drop_remainder=True)               #\r\n    print(_ds)                                                                                         #\r\n    _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(_num_tsteps), y.batch(_num_tsteps)))) #\r\n    print(_ds)                                                                                         #\r\n    ####################################################################################################\r\n\r\n    ##################################################\r\n    # BATCHING                                       #\r\n    ##################################################\r\n    _ds = _ds.batch(batch_size, drop_remainder=True) #\r\n    print(_ds)                                       #\r\n    ##################################################\r\n\r\n    return _ds\r\n\r\n\r\ndef prep_ds(files):\r\n    _ds = files.flat_map(prep_ds_file)\r\n    print(_ds)\r\n    return _ds\r\n\r\n\r\nds = prep_ds(ds_fs)\r\n\r\n# read/use the Dataset\r\n\r\nts = time.time()\r\nfor e in ds.take(1):\r\n    print([x.shape for x in e])\r\nte = time.time()\r\nprint(\"Duration: {} s\".format(te - ts))\r\n```\r\nOutput:\r\n```\r\nTensorFlow 2.1.0\r\nReading 200 tfrecord files...\r\n<ParallelMapDataset shapes: {features: (130,), label: (5,)}, types: {features: tf.float32, label: tf.float32}>\r\n<FlatMapDataset shapes: ((128,), (5,)), types: (tf.float32, tf.float32)>\r\n<TakeDataset shapes: ((128,), (5,)), types: (tf.float32, tf.float32)>\r\n<WindowDataset shapes: (DatasetSpec(TensorSpec(shape=(128,), dtype=tf.float32, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(5,), dtype=tf.float32, name=None), TensorShape([]))), types: (DatasetSpec(TensorSpec(shape=(128,), dtype=tf.float32, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(5,), dtype=tf.float32, name=None), TensorShape([])))>\r\n<FlatMapDataset shapes: ((None, 128), (None, 5)), types: (tf.float32, tf.float32)>\r\n<BatchDataset shapes: ((128, None, 128), (128, None, 5)), types: (tf.float32, tf.float32)>\r\n<FlatMapDataset shapes: ((128, None, 128), (128, None, 5)), types: (tf.float32, tf.float32)>\r\nDuration: 546.8976650238037 s\r\n```", "comments": ["I have tried on colab with TF version 2.1.0 ,2.2.0-dev20200218 and was able to reproduce the issue.Please,find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e06d28308eab617431c537750e6f6c5d/untitled671.ipynb). Thanks!", "Thank you for verifiying this. I just found out that it seems to be caused by the `shift` parameter in the `.window()` function. It seems the larger the shift value, the slower it gets. For larger values it doesn't work at all any more.", "TLDR: Your expectation that shape of the output data should not affect the performance is not grounded is reality. tf.data is a streaming API and the time it takes for \"batch\" transformations such as `window` and `batch` to produce an output is proportional to the \"batch\" dimension.\r\n\r\nBoth `window` and `batch` end up increasing the number of input elements needed to produce an output by a constant -- window size and batch size respectively. For instance, if it takes 1s to fetch a single output from `ds`, then -- assuming there is not asynchrony and parallelism within `ds` -- fetching a single output from `ds.window(size=10)` would be expected to take 10 seconds and fetching a single output from `ds.window(size=10).batch(batch_size=16)` would be expected to take 160 seconds. This is expected.\r\n\r\nIncreasing `shift` beyond `size` means that your input pipeline will need to read even more input elements per output. For example, to produce a single output from `ds.window(size=10, shift=20)`, you will need to consume 20 input elements per a single output element.\r\n\r\nI recommend taking a look at the tf.data performance analysis [guide](https://www.tensorflow.org/guide/data_performance_analysis) to help you understand the performance of your input pipeline. ", "Thank you for your response. I think or rather hope there is a misunderstanding. What I did to compensate the difference in dimensionality and the resulting difference in total size is multiplicate the result time by the missing factor:\r\n\r\nFirst, I stated the average size of the dimension 1 (number of time steps)\r\n\r\n> Timings for comparison (note: number of time steps / dim 1 (None) is avg. **312.5** = 40000 / 128):\r\n\r\nand consequently used it for the timing calculations.\r\n\r\n> - 1x Shape ((128, None, 128), (128, None, 5)):\r\n>  546.8976650238037 s\r\n> - 1000x Shape ((128, 128), (128, 5)) (no windowing):\r\n>  8.29618239402771 s * **312.5** / 1000 = 2.592556998133659375 s\r\n>  Speedup: 210x faster\r\n> - 1000x Shape ((None, 128), (None, 5)) (no batching):\r\n>  65.43172574043274 * **128** / 1000 = 8,37526089477539072 s\r\n>  Speedup: 65x faster\r\n\r\nThe first value of 546.9 seconds ist the reference with all dimensions.\r\n\r\nThe second value of 2.59 seconds is based on the benchmark without the \"time step\"-dimension taking 8.3 seconds. This \"raw\" value is multiplied by the average size of the missing dimension (312.5) and divided by the sample size 1000 (i.e. the loop ran 1000 rounds) to get to the final 2.59 second result.\r\n\r\nCorrespondingly, the third and final value of 8.38 seconds is computed with dimension 0 removed (batch size) and therefore, the \"raw\" value of 65.43 seconds is multiplied with the previously used batch size 128 for compensation. Again, the value is divided by 1000 (loop bound) which results in the final value.\r\n\r\nMaybe I am missing something here - I hope you can help me understand my mistake.\r\n\r\nI do understand, however, that the shift size can cause such behaviour. I maybe expected to see some more advanced caching mechanisms (after all the overlap of two windows contains the same data), but meanwhile I have found ways to pre-process the data differently and work around the issues described here.", "Sorry, I didn't realize you are normalizing the runtime by the missing shape. I think I found the problem, which is that your input pipeline with both `batch` and `window`, does not produce any elements. This is because each dataset will be truncated to `ds = ds.take(trunc)` elements. Let's say that this is 40k (which is the average). Your batch size is `128`, so your window size will be `40000 // 128 = 312`. So far so good. The problem is that you are using `shift = win_size // 2` and `win_size` is fixed to be 2048 (as opposed to being relative to the window size). Because of this, each window will be taken at 2048 offset and to create a batch of 128 windows, you would need at least 127 * 2048 + 312 elements, but you only have 40k. Because you are using `batch` with `drop_remainder=True`, partial batches will be dropped.\r\n\r\nIn other words, the performance you are listing as the baseline (using both `window` + `batch`), is the time needed to process all input data (and then throw it all away). You can validate my hypothesis by changing the loop bound of the baseline to an arbitrarily large number, which will not change the E2E time. Another way to validate this is to to set `drop_remainder=False`, which will result in only one partial batch being computed.", "Yes, that makes sense, I should have thought of that. Thank you for taking the time and figuring this out!"]}, {"number": 37012, "title": "Not able to perform tflite inferences for batch sizes beyond 1 (COCO SSD MobileNet v1)", "body": "\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0 binary\r\n\r\nBasically I imported the COCO SSD mobile net model. I want to perform inference for batch sizes more than 1.\r\n\r\nThis is my code \r\n\r\n```import numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"detect.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_shape=[100,300,300,3]\r\n\r\ninterpreter.resize_tensor_input(input_details[0]['index'],[100,300,300,3])\r\n\r\nprint(output_details[0])\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\r\nprint(\"INPUT SHAPE: \",input_data.shape)\r\ninterpreter.set_tensor(input_details[0]['index'],input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n``` \r\n\r\nI ended up getting this error\r\n\r\n```{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}\r\nTraceback (most recent call last):\r\n  File \"tflite.py\", line 20, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/hrishikesh/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py\", line 247, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/hrishikesh/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (433200 != 4332)Node number 36 (RESHAPE) failed to prepare.```\r\n\r\n", "comments": ["Was able to reproduce the issue with TF 2.1 and TF-nightly. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/cb02f279e0aecd0b2c2f97ae662a95e8/37012.ipynb). Thanks!", "@amahendrakar Thank you. I would love to have a solution to this problem. ", "Not all models inputs can be resized. The input should be [1,300,300,3]\r\nSee https://www.tensorflow.org/lite/models/object_detection/overview#input\r\nThe code executes successfully with following changes;\r\n```python\r\ninput_shape=[1,300,300,3]\r\ninterpreter.resize_tensor_input(input_details[0]['index'],[1,300,300,3])\r\n```\r\nThanks!", "You should resize the output tensor as well was the input tensor.\r\n\r\nSomething like :\r\n\r\n```\r\noutput_details = interpreter.get_output_details()\r\noutput_shape = output_details[0]['shape']\r\ninterpreter.resize_tensor_input(output_details[0]['index'],[100, output_shape[1], output_shape[2], output_shape[3]])\r\n```\r\nDon't forget to run allocate_tensors() after this resizing\r\n", "Hey @ymodak, @xross, your previous replies have been very informative!\r\n\r\n**System information**\r\n- OS Platform and Distribution: Amazon Linux version 2018.03\r\n- TensorFlow installed from: TFLite built from source in docker container\r\n- TensorFlow version: 2.2\r\n\r\nI am able to resize my input tensor to recognize a dynamic value for the batch dimension `(batch_size, 512, 512, 3)`, which was originally failing prior to resizing the tensor.  However, my **output values seemed to be fixed** at `(1, 512, 512, 3)` despite resizing and allocating as described above for both the input and outputs.\r\n\r\n**Original model input summary:**\r\n![image](https://user-images.githubusercontent.com/13341935/86292304-881cf180-bbbe-11ea-93f2-e02386cf7718.png)\r\n\r\n**Original model output summary:**\r\n![image](https://user-images.githubusercontent.com/13341935/86292396-a8e54700-bbbe-11ea-9f7c-849c14c946e3.png)\r\n\r\n**Sample running TFLite converted model:**\r\n```python\r\n# Concatenate the images to a single numpy array\r\nbatch_input = np.vstack(ARRAY_OF_NP_IMAGES) #(batch, 512, 512, 3)\r\n\r\n# Get input/output information\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Adjust graph input to handle batch tensor\r\ninterpreter.resize_tensor_input(input_details[0]['index'], batch_input.shape) #(batch_size, 512, 512, 3)\r\n\r\n# Adjust output #1 in graph to handle batch tensor\r\ninterpreter.resize_tensor_input(output_details[0]['index'], batch_input.shape) #(batch_size, 512, 512, 3)\r\n\r\n# Adjust output #2 in graph to handle batch tensor\r\noutput_shape = output_details[1]['shape']\r\ninterpreter.resize_tensor_input(output_details[1]['index'], (batch_input.shape[0], output_shape[1], output_shape[2], output_shape[3])) #(batch_size, 512, 512, 18)\r\n\r\n# Allocate for the resizing operations\r\ninterpreter.allocate_tensors()\r\n\r\n# Set input tensor\r\ninterpreter.set_tensor(input_details[0]['index'], batch_input)\r\n\r\n# Run\r\ninterpreter.invoke()\r\n\r\nprint(output_details[0]) # Shape and shape signature are always (1, 512, 512, 3)\r\nprint(output_details[1]) # Shape and shape signature are always (1, 512, 512, 18)\r\n```\r\n\r\nI tested the code above using an input array containing 2 images but always get a single result returned for both output tensors:\r\n```\r\n{'name': 'Identity', 'index': 244, 'shape': array([  1, 512, 512,   3], dtype=int32), 'shape_signature': array([  1, 512, 512,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n\r\n{'name': 'Identity_1', 'index': 288, 'shape': array([  1, 512, 512,  18], dtype=int32), 'shape_signature': array([  1, 512, 512,  18], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n```\r\n\r\nI also verified that the resizing shapes before callling allocate are correct `(2, 512, 512, 3)` and `(2, 512, 512, 18)`.\r\n\r\nAny suggestions or help would be greatly appreciated!", "@alfarok You should have your model converted again with supporting dynamic batch size. Looks like you specified static size during conversion.", "Thanks @kamathhrishi!\r\n\r\nI originally converted the model using python and did not specify the model `input_shape` or `output_shape`.  Since my inputs and outputs displayed correctly on the original model when using `model.summary()` I didn't think I would have to manually reset them during the conversion.\r\n\r\nRunning the CLI converter with just the `input_shape` flag seems to fix the issue as suggested:\r\n```\r\ntflite_convert --input_shape=None,512,512,3 --saved_model_dir=models/my_model --output_file=models/batch_model.tflite \r\n```\r\n\r\nOutput results with 2 input images (`shape_signature` now updates the dimension on the first axis to `-1`):\r\n```python\r\nprint(output_details[0])\r\n\r\n{'name': 'Identity', 'index': 312, 'shape': array([  1, 512, 512,   3], dtype=int32), 'shape_signature': array([ -1, 512, 512,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n\r\nprint(output_details[1])\r\n\r\n{'name': 'Identity_1', 'index': 360, 'shape': array([  1, 512, 512,  18], dtype=int32), 'shape_signature': array([ -1, 512, 512,  18], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\r\n```"]}, {"number": 37011, "title": "nanopb needs to be updated to pull in a security fix mentioned in GHSA-gcx3-7m76-287p", "body": "Nanopb is vulnerable to a denial of service, caused by a out of memory condition. By persuading a victim to open a specially crafted file, a remote attacker could exploit this vulnerability to cause the application to crash. Security Advisory link is https://github.com/nanopb/nanopb/security/advisories/GHSA-gcx3-7m76-287p.\r\n\r\nTensorflow has been using nanopb with commit hash \"f8ac463766281625ad710900479130c7fcb4d63b\" which is quite old. nanopb 0.3.6 is nearest release to this commit hash which is affected. \r\nThis bug is raised to address this issue which can be solved by two ways -\r\n1. If TF is tightly dependent on the existing commit of nanopb, we need a patch it with the patch created for 0.3 series mentioned in the advisory. \r\n2. Update nanopb to the higher version which has the fix.\r\n", "comments": ["Thank you for the issue. I'm going to work on this asap", "This should be fixed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37011\">No</a>\n"]}, {"number": 37009, "title": " Here is a list of operators for which you will need custom implementations: RandomUniform.", "body": "**System information**\r\n- OS Platform and Distribution (MacOS 10.15.3):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (1.15.2):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, BIDIRECTIONAL_SEQUENCE_LSTM, CAST, FULLY_CONNECTED, GATHER, GREATER_EQUAL, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\nTraceback (most recent call last):\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/lidayuan/opt/anaconda3/envs/py371/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, BIDIRECTIONAL_SEQUENCE_LSTM, CAST, FULLY_CONNECTED, GATHER, GREATER_EQUAL, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(\"../model\", tag_set=[tf.saved_model.tag_constants.SERVING])\r\n# converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n#                         tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\nopen(\"./result/edison_model.tflite\", \"wb\").write(tflite_model)\r\n", "comments": ["Thanks", "@yuanlida,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/35449#issuecomment-570318723) comment on a similar issue and let us know if it works. Thanks!", "I think I have resolve this problem.\r\nI added an initial value to the dropout parameter:seed.\r\n        self.word_embeddings = tf.nn.dropout(word_embeddings, self.dropout_pl, seed=time.time())\r\n", "This is the logs:\r\n2020-02-24 16:00:00.602358: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 47 operators, 115 arrays (0 quantized)\r\n2020-02-24 16:00:00.602705: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 47 operators, 115 arrays (0 quantized)\r\n2020-02-24 16:00:00.602838: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"dropout_1/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603229: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"bi-lstm/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603455: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 22 operators, 67 arrays (0 quantized)\r\n2020-02-24 16:00:00.603598: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"bi-lstm/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603642: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"dropout_1/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603741: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 21 operators, 66 arrays (0 quantized)\r\n2020-02-24 16:00:00.603833: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"dropout_1/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603875: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"bi-lstm/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.603994: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 3: 20 operators, 64 arrays (0 quantized)\r\n2020-02-24 16:00:00.604102: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"bi-lstm/dropout/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.604145: W tensorflow/lite/toco/graph_transformations/resolve_constant_random_uniform.cc:88] RandomUniform op outputting \"dropout_1/random_uniform/RandomUniform\" is truly random (using /dev/random system entropy). Therefore, cannot resolve as constant. Set \"seed\" or \"seed2\" attr non-zero to fix this\r\n2020-02-24 16:00:00.604242: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 20 operators, 64 arrays (0 quantized)\r\n2020-02-24 16:00:00.604376: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Group bidirectional sequence lstm/rnn pass 1: 16 operators, 58 arrays (0 quantized)\r\n2020-02-24 16:00:00.604559: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 16 operators, 58 arrays (0 quantized)\r\n2020-02-24 16:00:00.604838: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 537600 bytes, theoretical optimal value: 537600 bytes.\r\n2020-02-24 16:00:00.604929: I tensorflow/lite/toco/toco_tooling.cc:439] Estimated count of arithmetic ops: 653248 ops, equivalently 326624 MACs\r\n2020-02-24 16:00:00.604935: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 3807621", "Thanks.", "@yuanlida,\r\nIs this still an issue? Please feel free to close the issue if it is resolved. Thanks!"]}, {"number": 37008, "title": "Here is a list of operators for which you will need custom implementations: RandomUniform.", "body": "**System information**\r\n- OS Platform and Distribution (MacOS 10.15.3 (19D76)):\r\n- TensorFlow installed from ( binary):\r\n- TensorFlow version (1.15.2)\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ARG_MAX, BIDIRECTIONAL_SEQUENCE_LSTM, CAST, FULLY_CONNECTED, GATHER, GREATER_EQUAL, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["@yuanlida, Looks duplicate of [#37009](https://github.com/tensorflow/tensorflow/issues/37009). Can we close this issue. Will track resolution there. Thanks", "@yuanlida, You may try converting your model using Select TensorFlow Ops\r\nPlease take a look at similar [#35605(comment)](https://github.com/tensorflow/tensorflow/issues/35605#issuecomment-571737987) issue comment. Thanks!", "@yuanlida, Were you able to resolve this?", "Closing this issue since the resolution is provided. \r\nPlease feel free to open if still issue persists. Thanks "]}, {"number": 37007, "title": "Unable to implement \"Hello_world\" example on esp32", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from : installed on esp-idf using `pip install tensorflow`\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform : esp32-devKitC-V4\r\n\r\n**Describe the problem**\r\nI have followed [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/README.md#deploy-to-esp32) guide step-by-step so far\r\nI managed to install the esp-idf and also checked that the env. variables are in PATH (following the guide) but unable to proceed further\r\n\r\n**Exact sequence of commands/steps when you ran into the problem**\r\nafter doing `pip install tensorflow` (on esp-idf cmd prompt), I entered this command:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project`\r\n\r\nand the output is this:\r\n`'make' is not recognized as an internal or external command,\r\noperable program or batch file.`\r\n\r\nI also tried this command:\r\n`cmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project`\r\n\r\nand this was the error:\r\n`CMake Error: The source directory \"C:/Users/kkulk/Desktop/esp-idf/generate_hello_world_esp_project\" does not exist.\r\nSpecify --help for usage, or press the help button on the CMake GUI.`\r\n\r\n**I am a complete newbie to esp32, tensorflow and github as well, please let me know what the problem seems to be and if I am overlooking something completely obvious or did something wrong**", "comments": ["@kunalkul looks like you do not have `make` installed!\r\n\r\ndoing `cmake` in place of `make` is NOT same thing.", "@kunalkul , It seems you have installed tensorflow with `pip install tensorflow`, that should be done when you want to use tensorflow functions python code.\r\nFor being able to compile the [tensorflow/lite/micro/examples](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples) and flash the binaries on the respective devices, you need to download the source code as mentioned [here](https://www.tensorflow.org/install/source_windows#download_the_tensorflow_source_code)\r\n\r\nAlso,\r\n>  'make' is not recognized as an internal or external command, operable program or batch file.\r\n\r\nit seems you have not installed make utility on your machine , maybe [this](http://gnuwin32.sourceforge.net/packages/make.htm) could help.\r\n\r\nIn short you should proceed as follows\r\n1) Install source code of [tensorflow](https://github.com/tensorflow/tensorflow) ( clone this repository) as mentioned above\r\n2)  Install make if not already installed \r\n3) Note: When you successfully install all the required dependancies and try to build hello-world example with following command\r\n> make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n\r\nPlease note that it might print an error message, as currently there seems to be a build issuer which needs a fix, but even with the error message the build actually succeeds and you can go to example folder and compile the binary for ESP with \u2018idf.py build\u2019.", "I executed steps 1 and 2, and ran\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project`\r\n\r\ngot the following error : `C:\\Users\\kkulk\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\Makefile:2: *** \"Requires make version 3.82 or later (current is 3.81)\".  Stop.`\r\n\r\nI proceeded ahead by running `idf.py build`\r\nand got this error : `Checking Python dependencies...\r\nPython requirements from C:\\Users\\kkulk\\Desktop\\esp-idf\\requirements.txt are satisfied.\r\nExecuting action: all (aliases: build)\r\nCMakeLists.txt not found in project directory c:\\users\\kkulk\\tensorflow\\tensorflow\\lite\\micro\\examples\\hello_world`\r\n\r\n**Edit**\r\n\r\nI updated the Make version using the 'chocolately' installer [here](https://stackoverflow.com/a/54086635) and tried to execute the `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project` command again, but got this **error** : \r\n`process_begin: CreateProcess(NULL, uname -m, ...) failed.\r\ntensorflow/lite/micro/tools/make/Makefile:25: pipe: No error\r\n-m was unexpected at this time.\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\nprocess_begin: CreateProcess(NULL, bash C:\\Users\\kkulk\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\download_and_extract.sh https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip 7e8191b24853d75de2af87622ad293ba tensorflow/lite/micro/tools/make/downloads/gemmlowp, ...) failed.\r\nmake (e=2): The system cannot find the file specified.\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:261: tensorflow/lite/micro/tools/make/downloads/gemmlowp] Error 2`\r\n\r\n**despite this**, I tried to proceed with the next step in the tutorial `cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf` \r\nand the output was : `The system cannot find the path specified.`   ", "@kunalkul The errors are arising because of faulty setup of tensorflow. Please do not go to next step unless previous step is completed as it is surely not gonna work.\r\nPlase see that you have successfully cloned [tensorflow](https://github.com/tensorflow/tensorflow/). \r\nplease change you have make version > 3.81 as recommended.\r\nIf you get errors such as cannot find file, then it seems you have not cloned the repository correctly", "I was able to clone tensorflow succesfully, as I am able to navigate to the directory on my PC both through file explorer and cmd (This was after I updated my make version to 4.3, as I have already mentioned in the **edit** comment above).\r\n\r\nI am not sure what's causing this error : `process_begin: CreateProcess(NULL, uname -m, ...) failed. tensorflow/lite/micro/tools/make/Makefile:25: pipe: No error -m was unexpected at this time. tensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp process_begin: CreateProcess(NULL, bash C:\\Users\\kkulk\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\download_and_extract.sh https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip 7e8191b24853d75de2af87622ad293ba tensorflow/lite/micro/tools/make/downloads/gemmlowp, ...) failed. make (e=2): The system cannot find the file specified. make: *** [tensorflow/lite/micro/tools/make/Makefile:261: tensorflow/lite/micro/tools/make/downloads/gemmlowp] Error 2`", "I assume you have setup all the things on windows correctly as mentioned [here](https://www.tensorflow.org/install/source_windows#setup_for_windows), If yes and still you have the problem then maybe you could look into [this issue](https://github.com/dariomanesku/cmft/issues/28), I personally do not use windows for development so if that is an issue related with windows I would not be the right person to comment, Maybe looking at other build issues in [tensorflow/issues](https://github.com/tensorflow/tensorflow/issues) might help.", "I am having this issue too. I am going insane. I've installed make but the default is set to 3.81 and upgrading it to 4.1 ", "Hello everyone,\r\n\r\nI would also like to learn to know tensorflow and therefore I'm trying to use the Hello World example.\r\nThat fails with errors either in MinGW or cygwin. I have two output lines that express their opinion (Yeah sadly they both speak german only on my PC):\r\n\r\nHere comes MinGW:\r\n```\r\nC:\\Versionsverwaltung\\tensorflow>C:\\MinGW\\bin\\mingw32-make.exe -v\r\nGNU Make 3.82.90\r\nBuilt for i686-pc-mingw32\r\nCopyright (C) 1988-2012 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\nC:\\Versionsverwaltung\\tensorflow>C:\\MinGW\\bin\\mingw32-make.exe -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\nprocess_begin: CreateProcess(NULL, uname -m, ...) failed.\r\n\"-m\" kann syntaktisch an dieser Stelle nicht verarbeitet werden.\r\nFIND: Parameterformat falsch\r\nmingw32-make: *** No rule to make target 'generate_hello_world_esp_project'.  Stop.\r\n\r\nC:\\Versionsverwaltung\\tensorflow>\r\n```\r\n\r\n\r\nThis is cygwin:\r\n```\r\nxxx@XXX/cygdrive/c/Versionsverwaltung/tensorflow\r\n$ make -v\r\nGNU Make 4.3\r\nGebaut f\u00fcr x86_64-pc-cygwin\r\nCopyright (C) 1988-2020 Free Software Foundation, Inc.\r\nLizenz GPLv3+: GNU GPL Version 3 oder sp\u00e4ter <http://gnu.org/licenses/gpl.html>\r\nDies ist freie Software: Sie k\u00f6nnen sie nach Belieben \u00e4ndern und weiter verteilen.\r\nSoweit es die Gesetze erlauben gibt es KEINE GARANTIE.\r\n\r\nxxx@XXX/cygdrive/c/Versionsverwaltung/tensorflow\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\ntensorflow/lite/micro/tools/make/Makefile:297: Warnung: Die Befehle f\u00fcr das Ziel \u201etensorflow/lite/micro/tools/make/downloads/ruy\u201c werden \u00fcberschrieben\r\ntensorflow/lite/micro/tools/make/Makefile:297: Warnung: Alte Befehle f\u00fcr das Ziel \u201etensorflow/lite/micro/tools/make/downloads/ruy\u201c werden ignoriert\r\ntensorflow/lite/micro/tools/make/Makefile:297: Warnung: Die Befehle f\u00fcr das Ziel \u201etensorflow/lite/micro/tools/make/downloads/person_model_grayscale\u201c werden \u00fcberschrieben\r\ntensorflow/lite/micro/tools/make/Makefile:297: Warnung: Alte Befehle f\u00fcr das Ziel \u201etensorflow/lite/micro/tools/make/downloads/person_model_grayscale\u201c werden ignoriert\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh: Zeile 16: $'\\r': Kommando nicht gefunden.\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh: Zeile 24: $'\\r': Kommando nicht gefunden.\r\n: Ung\u00fcltige Optioncro/tools/make/download_and_extract.sh: Zeile 25: set: -\r\nset: Aufruf: set [-abefhkmnptuvxBCHP] [-o Option] [--] [Argument ...]\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh: Zeile 26: $'\\r': Kommando nicht gefunden.\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh: Zeile 28: Syntaxfehler beim unerwarteten Wort `$'{\\r''\r\n'ensorflow/lite/micro/tools/make/download_and_extract.sh: Zeile 28: `patch_am_sdk() {\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:297: tensorflow/lite/micro/tools/make/downloads/gemmlowp] Fehler 2\r\n\r\nxxx@XXX/cygdrive/c/Versionsverwaltung/tensorflow\r\n```\r\n", "Hi @TobiasBoeschMiele this is because you have Windows style line ending.\r\nCan you please run following command and try again:\r\n`dos2unix tensorflow/lite/micro/tools/make/download_and_extract.sh`\r\n", "Thank @vikramdattu for your suggestion.\r\n\r\n_I use tensorflow lite at master at https://github.com/tensorflow/tensorflow/commit/89bd08149c9d179bef02a5baf866a20b2fc71fc1_\r\n\r\nMinGW has the dos2unix program so I used that. Here is what it puts out when failling again:\r\n```\r\nC:\\Versionsverwaltung\\tensorflow>C:\\MinGW\\bin\\mingw32-make.exe -v\r\nGNU Make 3.82.90\r\nBuilt for i686-pc-mingw32\r\nCopyright (C) 1988-2012 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\n\r\nC:\\Versionsverwaltung\\tensorflow>C:\\MinGW\\msys\\1.0\\bin\\dos2unix.exe tensorflow/lite/micro/tools/make/download_and_extract.sh\r\ndos2unix: converting file tensorflow/lite/micro/tools/make/download_and_extract.sh to Unix format...\r\n\r\nC:\\Versionsverwaltung\\tensorflow>C:\\MinGW\\bin\\mingw32-make.exe -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\nprocess_begin: CreateProcess(NULL, uname -m, ...) failed.\r\n\"-m\" kann syntaktisch an dieser Stelle nicht verarbeitet werden.\r\nFIND: Parameterformat falsch\r\nFIND: Parameterformat falsch\r\nmingw32-make: *** No rule to make target 'generate_hello_world_esp_project'.  Stop.\r\n```", "The link which you have mentioned does not exists anymore, please follow [this](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level) link and try the `Hello world` example.\r\nFeel free to open new issue if the issue still persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37007\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37007\">No</a>\n"]}, {"number": 37006, "title": "Resolves symbolic links for CUDA paths", "body": "Resolves #35122.", "comments": ["Per b7796f3c856965f68699b2b527fc2872e2aa71ad perhaps you'd like to update https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/gpus/find_cuda_config.py.gz.base64 (possibly with help from @buchgr).\r\n\r\nAlso I would suggest using the `realpath` function from https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/remote_config/common.bzl#L254 in `third_party/gpus/cuda_configure.bzl`, which plays nice with remote builds. ", "In `find_cuda_config.py` it seems better to change https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/gpus/find_cuda_config.py#L438-L443 from `normpath` to `realpath` to ensure you normalize the path lists in the end, instead of checking at the beginning (it actually fails in my settings, complaining not able to find `cuda.h`).", "> Per [b7796f3](https://github.com/tensorflow/tensorflow/commit/b7796f3c856965f68699b2b527fc2872e2aa71ad) perhaps you'd like to update https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/gpus/find_cuda_config.py.gz.base64 (possibly with help from @buchgr).\r\n> \r\n> Also I would suggest using the `realpath` function from\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/remote_config/common.bzl#L254\r\n> in `third_party/gpus/cuda_configure.bzl`, which plays nice with remote builds.\r\n\r\nThese suggestions have been applied. I also updated `find_cuda_config.py.gz.base64` using `compress_find_cuda_config.py`.", "LGTM. Thanks @byronyi and @njzjz. Apologies for the gzip compression step. This should go away soon!", "Looks good to me, but I'll defer to @byronyi for the review. Thanks for your work on this!", "> Looks good to me, but I'll defer to @byronyi for the review. Thanks for your work on this!\r\n\r\n@angerson LGTM, but I am not a committer to approve this :) Mind to approve?", "Looks like the `realpath` function does not work if the path contains spaces.\r\nhttps://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/remote_config/common.bzl#L268\r\n\r\nI think we should add double quotes\r\n```py\r\n return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip() \r\n```", "> Looks like the `realpath` function does not work if the path contains spaces.\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/a83686471143b3f3234f23637c85c6b0cab3c3ef/third_party/remote_config/common.bzl#L268\r\n> \r\n> I think we should add double quotes\r\n> \r\n> ```python\r\n>  return execute(repository_ctx, [bash_bin, \"-c\", \"realpath \\\"%s\\\"\" % path]).stdout.strip() \r\n> ```\r\n\r\nMind to fix this? I believe it\u2019s the reason why CI fails on Windows.", "Ping @buchgr; mind to kick off CI?", "On Windows it fails, but it does not seem related to this PR anymore.\r\n\r\nPing @gbaned to approve.", " I believe I can't trigger a build. I am not a member of the Tensorflow team.", "What is `import/copybara \u2014 An error happened while migrating the change`? Does anyone know?", "@njzjz it means it\u2019s processing internally inside Google and we just need to be patient as external contributors."]}, {"number": 37005, "title": "fix the range arg explanation and improve readability", "body": "", "comments": []}, {"number": 37004, "title": "Add checking of kwonlyargs in keras has_arg", "body": "I ran into an issue with `tensorflow.keras.utils.generic_utils.has_arg` when using in conjunction with `functools.partial`. Minimum reproducible example:\r\n```python3\r\nfrom functools import partial\r\n\r\nfrom tensorflow.python.keras.utils.generic_utils import has_arg\r\n\r\n\r\ndef f(a, b, c):\r\n      return a + b + c\r\n\r\npartial_f = partial(f, b=1)\r\n\r\nassert has_arg(partial_f, 'c', accept_all=True)\r\n```\r\n\r\nI would expect `c` to be in the args, even though it has been converted to a keyword-only argument by partial due to its position after a keyword argument (`b`). It appears that `has_arg` does not check keyword-only arguments. I'm assuming this is not by design but rather a bug, based on what I see in keras-team/keras#7035. I'm submitting this PR as a tentative fix, and I added the above code snippet to the tests. There are much more extensive tests in `tensorflow/tensorflow/python/util/tf_inspect_test.py`; it might be worth checking some of the edge cases there in `has_arg` tests (for a future PR).", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37004) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37004) for more info**.\n\n<!-- ok -->", "@pavithrasv it looks like the Python2 failure is a real failure (i.e., Python 2's `argspec` does not like what I did with `functools.partial` in that test). Is this a blocker, or is it okay since Python 2 is no longer supported (as far as I understand)?", "@rthadur @pavithrasv one option would be to skip this test for Python 2. At least that way this fix will be applied to external TF users, but it isn't breaking anything for you internally that wasn't already broken/not supported. Let me know if you would like me to implement that.", "@pavithrasv, Can you please take a look on above comments from  @adriangb ? Thanks!", "@pavithrasv  Any update on this PR? Please. Thanks!", "Yes, i think it should be ok since Python 2 is not supported.", "@adriangb Can you please check @pavithrasv's comments and resolve conflicts?. Thanks!\r\n", "> @adriangb Can you please check @pavithrasv's comments and resolve conflicts?. Thanks!\r\n\r\nDone and done.\r\n\r\nI obviously know close to nothing about Google's internal use of Python 2, but I don't think this change will _break_ any existing Python 2 functionality, so like @pavithrasv said it should be fine!"]}, {"number": 37002, "title": "[Doc] Laplacian for Deep Dream 2.0", "body": "Hi, i will skip the complete description of the issue because it ' s obvious thing. In the original Deep Dream code you managed to do a very complex code of splits and merges, this code tuned for much better the Deep Dream results. I dont see it in the 2.0 version. \r\nAlso, iam aware of Lucid project, the \"laplace pyramid\" present in Lucid does not seems to resemble the old Deep Dream code. \r\n\r\nMy question, Is there anyone who tryied to implement this sucessfully for tf 2.0 ? at last something that can talk with the notebook of Deep Dream for TF 2.0 ? \r\n\r\nThank you.", "comments": ["@Uiuran, Here is the [link](https://www.tensorflow.org/tutorials/generative/deepdream) for Tensorflow DeepDream tutorial which uses Tf 2.x. Thanks\r\n", "@gadagashwini exactly this that iam using to do my version, whoever the laplacian is not there, and it 's fairly visible the difference, the deep dream from older versions (before 2.x) is far better in generating images. \r\n\r\nMy question reside in, there is any implementation of that laplacian near from the logic of eager tf.functions for Tensorflow ? i was not able translate that Laplacian code of deep dream to 2.0.\r\n\r\n(many error surged, one of them is that recommends an initial_scope, but this didnt works. Plus i tried to put it on eager mode too with tf.functions but it poped up an Error saying that i was trying to access a Tensor that was not directly connected to the graph).\r\n\r\nBut since the code is too big and ugly, there is no small snippet of code to reproduce that ... or i dont know how to reproduce. ", "I found a way, or perhaps a way to be tried. I will try it in the eager mode of tf 2.0.\r\n\r\nThe point, to me, is that even with tf.function that uses AutoGraph to build an optimized comp graph, there is some Neural Network algorithms that are simply too hard to build all the graph first then execute in a session. For example the Laplacian. \r\n\r\nAutograph still cant build 2 graphs in the same scope, and then rewire them coherently (that is, bearing in mind that the output to be rewired to the node op has to be the same dimensionality and so on ...).\r\n\r\nThis makes hard to build a model with Keras, from another in the zoo, and them build a fresh Laplacian graph and rewire ... Although the older Deep Dream is not in eager mode afaik. \r\n\r\nHere i dont know if it's more me dont knowing to build comp graphs or if it seems to be a feature that AutoGraph would be nice to have.\r\n\r\nps: when using tf.function i was getting a lot of \" this Tensor is not connected to the graph in the scope\", what points to what i mentioned.", " \" I found a way, or perhaps a way to be tried. I will try it in the eager mode of tf 2.0. \"\r\n\r\nIt worked in eager mode, i only plugged the old code and deleted all tf.functions that builds the comp graph and exec in non eager mode.\r\n\r\nBut the part:\r\n\" Autograph still cant build 2 graphs in the same scope, and then rewire them coherently (that is, bearing in mind that the output to be rewired to the node op has to be the same dimensionality and so on ...) \"\r\n\r\nStill relevant, but maybe it is a feature rather than docs ?  \r\n\r\nIf you think its not relevant for Autograph to be able to rewire two graphs, then the problem to me is finished and i just close ...", "Hi @Uiuran, github is best for reporting bugs. If you're trying to better understand how to use TF2 and migrate TF1 code, then Stack Overflow is a better option. There's a wider community to read and answer questions there. If you think you've found a bug, please provide reproducible code so we can troubleshoot. Thanks!"]}]