[{"number": 41913, "title": "Support outputting tree leaves in BoostedTreesClassifier", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.14 and would like to migrate to 2.3\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the `BoostedTreesClassifier` canner estimator does not support outputting the activated leaves of the tree on output, instead only supporting outputting the natural output of the classifier. The feature request I am making is to also support outputting the leaves, such as what the `CoreGradientBoostedDecisionTreeEstimator` in TF 1.14 supported via the flag `output_leaf_index=True`. With this flag the estimator would also provide an output on which leaves were output on an evaluation.\r\n\r\n**Will this change the current api? How?**\r\nThe estimator will gain an initialization argument of `output_leaf_index`. When set to true, the estimator would add leaf indices as one of its predict keys which will be output to the user on evaluation.\r\n\r\n**Who will benefit with this feature?**\r\nOne common use case of boosted trees models is in stacked architectures where the output of the tree model is fed into a deep neural network. For this use case the leaf indices are oftentimes more useful as an input to the DNN than the raw prediction of the tree model. This use case is supported in TF 1 but not in TF 2 so it prevents migrating code for this use case to TF 2.", "comments": ["@mafhcow ,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras/) and [tf.data](https://www.tensorflow.org/guide/data) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? \r\n\r\nThanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41912, "title": "Should this be \"ZerosLike\" ?", "body": "https://github.com/tensorflow/tensorflow/blob/05632ed9bad5bf9eee3edd57ade3d8250d580019/tensorflow/c/eager/gradients.cc#L104", "comments": ["@saxenasaurabh @alextp ", "Indeed. Can you fix?", "Closing this issue since its fixed with [7fafd21](https://github.com/tensorflow/tensorflow/commit/7fafd216eb116a092a85d35d1e56d486c36d8727#diff-90a2d1c5b6dd65802ebf77e266d4d2e5). Thanks for the fix!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41912\">No</a>\n"]}, {"number": 41911, "title": "[INTEL MKL] pooling ops build fix.", "body": "Fixes a build errors introduced by a recent PR in mkl_pooling_ops", "comments": []}, {"number": 41910, "title": "Only support ksizes across space, when using tf.image.extract_patches", "body": "System info:\r\nOs: MacOs catalina (10.15.5 )\r\nTensorflow: 2.0.0 installed over anaconda navigator (1.9.12 python 2.7) enviroment\r\n\r\nI was **trying to split my image through 4 patches** when I came through the following error: \r\n\r\n```UnimplementedError: Only support ksizes across space``` \r\n\r\n```python\r\niterator = tf.compat.v1.data.make_one_shot_iterator(parsed_dataset) \r\nimage,label = iterator.get_next()\r\nimage_height = image.shape[0]\r\nimage_width = image.shape[1]\r\n# Since the expected type is (batch,height,width,channels), i have tryied to expand my image that have\r\n# dimensions: (800,344,3) to (1,800,344,3) but didn't solved the error.\r\n#image = tf.expand_dims(image ,0)\r\nimages = list(image)\r\nextracted_patches = tf.image.extract_patches(images=images,\r\n                                             sizes=[1,int(0.25*image_height),int(0.25*image_width),3],\r\n                                             strides=[1,int(0.25*image_height),int(0.25*image_width),3],\r\n                                             rates=[1,1,1,1],\r\n                                             padding=\"SAME\")\r\n```\r\n\r\n### Traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-64-23c2aff4c306> in <module>()\r\n     17                                              strides=[1,int(0.25*image_height),int(0.25*image_width),3],\r\n     18                                              rates=[1,1,1,1],\r\n---> 19                                              padding=\"SAME\")\r\n     20 \r\n     21 \r\n\r\n/Users/lucianoaraujo/anaconda2/lib/python2.7/site-packages/tensorflow_core/python/ops/array_ops.pyc in extract_image_patches_v2(images, sizes, strides, rates, padding, name)\r\n   4657   \"\"\"\r\n   4658   return gen_array_ops.extract_image_patches(images, sizes, strides, rates,\r\n-> 4659                                              padding, name)\r\n   4660 \r\n   4661 \r\n\r\n/Users/lucianoaraujo/anaconda2/lib/python2.7/site-packages/tensorflow_core/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)\r\n   2542       else:\r\n   2543         message = e.message\r\n-> 2544       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n   2545   # Add nodes to the TensorFlow graph.\r\n   2546   if not isinstance(ksizes, (list, tuple)):\r\n\r\n/Users/lucianoaraujo/anaconda2/lib/python2.7/site-packages/six.pyc in raise_from(value, from_value)\r\n    735 else:\r\n    736     def raise_from(value, from_value):\r\n--> 737         raise value\r\n    738 \r\n    739 \r\n\r\nUnimplementedError: Only support ksizes across space. [Op:ExtractImagePatches]\r\n```", "comments": ["Changing:\r\n```python \r\nsizes=[1,int(0.25*image_height),int(0.25*image_width),3]\r\nstrides=[1,int(0.25*image_height),int(0.25*image_width),3]\r\n```\r\nTo:\r\n```python \r\nsizes=[1,int(0.25*image_height),int(0.25*image_width),1]\r\nstrides=[1,int(0.25*image_height),int(0.25*image_width),1]\r\n```\r\nand using ```image = tf.expand_dims(image ,0)``` to reshape input appears to resolve the problem.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41910\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41910\">No</a>\n"]}, {"number": 41909, "title": "Tensor Flow 2.2/CUDA 10.1 Could not load dynamic library", "body": "I believe my PATH is set up correctly and these files are present in the bin\r\n\r\n2020-07-30 16:17:56.453813: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-07-30 16:17:56.453948: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2020-07-30 16:17:58.729263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-30 16:17:58.751682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2020-07-30 16:17:58.752751: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-07-30 16:17:58.753611: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-07-30 16:17:58.756415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-30 16:17:58.757327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-30 16:17:58.763641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-30 16:17:58.764657: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-07-30 16:17:58.765503: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-07-30 16:17:58.765627: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-30 16:17:58.766078: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-30 16:17:58.772467: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18e96ef2a40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 16:17:58.772617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-30 16:17:58.772770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 16:17:58.772873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]\r\n\r\n[code.txt](https://github.com/tensorflow/tensorflow/files/5003637/code.txt)\r\n\r\n\r\n![path](https://user-images.githubusercontent.com/54596770/88971045-59f40580-d281-11ea-9c06-b094b10a3f31.PNG)\r\n![bin](https://user-images.githubusercontent.com/54596770/88971048-5a8c9c00-d281-11ea-8ba3-a271f86e23b6.PNG)\r\n\r\n\r\n\r\n\r\n", "comments": ["@VincentFSU \r\nPlease Search cudart64_101.dll  files are placed in the folder C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin (path) If the path is customized, it needs to be changed to the actual path) folder.\r\n\r\n\r\n#36111 [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install)", "@Saduf2019\r\n> Please Search cudart64_101.dll files are placed in the folder C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin (path) If the path is customized, it needs to be changed to the actual path) folder.\r\n> \r\n> #36111 [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install)\r\n\r\nThey are indeed located in that path, see the image attached to the original post.\r\n", "@VincentFSU \r\nIn images, I seen most of the file names is end with \"*_10.*\" instead of  \"*_101.*\" (try this: change 10 to 101)\r\nFor example: \r\n1> cublas64_10.dll --> cublas64_101.dll\r\n2> cufftw64_10.dll --> cufftw64_101.dll\r\n3> nppif64_10.dll --> nppif64_101.dll\r\n\r\nNote: Although this is a trick for a temporary purpose. In the end, you need to reinstall Cuda and TensorFlow.", "@VincentFSU\r\n\r\n> > Please Search cudart64_101.dll files are placed in the folder C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin (path) If the path is customized, it needs to be changed to the actual path) folder.\r\n> > #36111 [link](https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install)\r\n> \r\n> They are indeed located in that path, see the image attached to the original post.\r\n\r\nAs explained above and in the link,  \u200b\u200bcudart64_100.dll Orcudart64_102.dll (may not be these two names, depending on the version of CUDA you install)rename to cudart64_101.dll and let us know", "Looks like this issue is similar to [this issue](https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-670224813). Similar issues were reported for GTX GPUs. Please check the suggestion mentioned there in that issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41909\">No</a>\n", "Got the same error. Any updates? ", "what I got from cudnn is `cudnn64_8.dll` rather than `cudnn64_7.dll`", "I also got the same error with\r\n\r\n**Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found**\r\n\r\nI am using \r\n\r\ntensorflow 2.4.0\r\ncuda 11.0\r\ncudnn 8.0.4 for cuda 11.0", "updating Nvidia GeForce worked for me"]}, {"number": 41908, "title": "Forwarding non-transactional API to transactional API in FileSystem", "body": "This PR adds implementations for non-transactional filesystem API to transactional api. This removes pure virtual methods. Follow up PRs will move implementations of FileSytem to transactional API, remove virtual property of non-transactional api, and convert transactional api to pure virtuals where needed.", "comments": ["Posting again since I posted in wrong place. \r\nYes. This is what I meant in the body of the PR. If I do it now, I need to change all filesystems at once since they don't implement transaction API yet."]}, {"number": 41907, "title": "Hadoop filesystem helper", "body": "@mihaimaruseac \r\nThis PR adds some helper functions and class port from `core/platform/hadoop`.", "comments": ["There might be an issue with the `functional` header but let's try to import while I look if that is allowed", "I don't change anything at all ( just `status` to `TF_Status` ) so I hope it will be fine", "Oh, then it should be easy to go around potential restrictions.", "I had to do a few minor changes internally after import, but nothing major. It should land in a few hours (currently running tests)", "Actually, in the next PR, I will remove `libhdfs()` because it causes a memory leak. So if there is a problem with `libhdfs()`, please let me know and I will push a commit to address that problem.", "Hmm, I think the issue was not there. I didn't keep a list of the manual changes I did, unfortunately"]}, {"number": 41906, "title": "How to disable Cudnn for reproducible results", "body": "I use the setting in [https://github.com/NVIDIA/framework-determinism](https://github.com/NVIDIA/framework-determinism), But I found my model still get random result each time (1% mIOU different)\r\n\r\nIs there any way in Tensorflow provides the feature like Pytorch:\r\n```\r\ntorch.backends.cudnn.enabled = False\r\n```\r\nI also found there is an enviroment variable \"TF_USE_CUDNN\" but it does not work.", "comments": ["@edwardyehuang Non-determinism for a priori deterministic operations come from concurrent (multi-threaded) implementations.\r\nDespite constant progress on that front, TensorFlow does not currently guarantee determinism for all of its operations. After a quick search on the internet, it seems that the situation is similar with the other major toolkits.\r\nDuring training, unless you are debugging an issue, it is OK to have fluctuations between runs. Uncertainty is in the nature of training, and it is wise to measure it and take it into account when comparing results \u2013 even when toolkits eventually reach perfect determinism in training.\r\nYou can also look at the complete explanation [here](https://stackoverflow.com/questions/50744565/how-to-handle-non-determinism-when-training-on-a-gpu)\r\n\r\nYou can also take a look at this [video](https://developer.nvidia.com/gtc/2019/video/S9911). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 41905, "title": "TensorMap: Update TensorMap Gradients and Gradient Tests", "body": "Update gradients and gradient tests for the TensorMap feature to be more comprehensive.\r\n1. Add gradient for the erase op.\r\n2. Implement ZerosLike and BinaryAdd kernels, which will be used in certain gradients.\r\n3. Add gradient tests to cover more cases, including gradients for more function combinations, replace, erase, and non-scalar values.\r\n@mdanatg @saxenasaurabh @dynamicwebpaige ", "comments": []}, {"number": 41904, "title": "Windows import library is missing sufficient symbols to use C++ API", "body": "**System information**\r\n- OS Platform and Distribution **Windows 10**\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version: **2.3.0**\r\n- Python version: **3.8.5**\r\n- Bazel version (if compiling from source): **3.3.1**\r\n- GCC/Compiler version (if compiling from source): **Visual Studio 2019**\r\n\r\n**Expected Result**\r\n\r\nThe generated Windows Binaries should export sufficient symbols to compile and link a basic program using the C++ API\r\n\r\n**Actual Result**\r\n\r\nCompiling and linking a basic Tensorflow C++ program with the Windows Binaries results in undefined symbols.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nExample from [Tensorflow 2.3.0 API Reference](https://www.tensorflow.org/api_docs/cc/class/tensorflow/client-session)\r\n\r\n`main.cpp`\r\n\r\n```.cpp\r\n\r\n#include <tensorflow/cc/framework/scope.h>\r\n#include <tensorflow/cc/client/client_session.h>\r\n#include <tensorflow/cc/ops/array_ops.h>\r\n#include <tensorflow/cc/ops/math_ops.h>\r\n\r\n#include <vector>\r\n\r\nnamespace tf = tensorflow;\r\nusing namespace tf::ops;\r\n\r\nint main() {\r\n    tf::Scope root = tf::Scope::NewRootScope();\r\n    auto a = Placeholder(root, tf::DT_INT32);\r\n    auto c = Add(root, a, {41});\r\n\r\n    tf::ClientSession session(root);\r\n    std::vector<tf::Tensor> outputs;\r\n\r\n    auto status = session.Run({{a, {1}}}, {c}, &outputs);\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n`CMakeLists.txt`\r\n\r\n```\r\ncmake_minimum_required(VERSION 2.8)\r\nproject(tensorflow_example)\r\n\r\nset(CMAKE_VERBOSE_MAKEFILE TRUE)\r\nadd_executable(example main.cpp)\r\n\r\ntarget_link_libraries(example PRIVATE \"${TENSORFLOW_ROOT}/tensorflow_cc.lib\")\r\ntarget_compile_definitions(example PRIVATE NOMINMAX)\r\ntarget_include_directories(example PRIVATE\r\n    \"${TENSORFLOW_ROOT}/include\"\r\n    \"${TENSORFLOW_ROOT}/include/src\"\r\n)\r\n```\r\n\r\n```\r\npython configure.py\r\n<Accept defaults>\r\nbazel build --config=opt //tensorflow:tensorflow_cc\r\nbazel build --config=opt //tensorflow:tensorflow_cc_dll_import_lib\r\nbazel build --config=opt //tensorflow:install_headers\r\n```\r\n\r\n```\r\nmkdir build\r\ncd build\r\ncmake .. -DTENSORFLOW_ROOT=<path to bazel-bin/tensorflow>\r\ncmake --build .\r\n```\r\n\r\nThe compilation succeeds but linking fails with the following missing externals: \r\n```\r\nMicrosoft (R) Build Engine version 16.6.0+5ff7b0c9e for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::ClientSession(class tensorflow::Scope const &)\" (??0ClientSession@tensorflow@@QEAA@AEBVScope@1@@Z) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::~ClientSession(void)\" (??1ClientSession@tensorflow@@QEAA@XZ) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::ClientSession::Run(class std::unordered_map<class tensorflow::Output,struct tensorflow::Input::Initializer,struct tensorflow::OutputHash,struct std::equal_to<class tensorflow::Output>,class std::allocator<struct std::pair<class tensorflow::Output const ,struct tensorflow::Input::Initializer> > > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)const \" (?Run@ClientSession@tensorflow@@QEBA?AVStatus@2@AEBV?$unordered_map@VOutput@tensorflow@@UInitializer@Input@2@UOutputHash@2@U?$equal_to@VOutput@tensorflow@@@std@@V?$allocator@U?$pair@$$CBVOutput@tensorflow@@UInitializer@Input@2@@std@@@7@@std@@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@5@PEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@5@@Z) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Placeholder::Placeholder(class tensorflow::Scope const &,enum tensorflow::DataType)\" (??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nmain.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ops::Add::Add(class tensorflow::Scope const &,class tensorflow::Input,class tensorflow::Input)\" (??0Add@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@1@Z) referenced in function main [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\nC:\\Users\\plane\\projects\\tf_test\\build\\Debug\\example.exe : fatal error LNK1120: 9 unresolved externals [C:\\Users\\plane\\projects\\tf_test\\build\\example.vcxproj]\r\n```\r\n\r\n", "comments": ["I'm new to Bazel but from what I can tell, the root cause is that, unfiltered, the Windows dll would contain more than 64K symbols, which is the maximum number of symbols that can be exported by a Windows Dll (because each symbol is indexed by a 16bit integer).\r\n\r\nTo cirvcumvent this, Bazel builds a 'dummy' dll called `tf_custom_op_library_additional_deps.dll` in order to generate a `.def` file containing the symbols to export:\r\n\r\nFrom https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/BUILD :\r\n\r\n```.bzl\r\n # add win_def_file for tensorflow_cc\r\n    win_def_file = select({\r\n        # We need this DEF file to properly export symbols on Windows\r\n        \"//tensorflow:windows\": \":tensorflow_filtered_def_file\",\r\n        \"//conditions:default\": None,\r\n    }),\r\n```\r\n\r\nHowever, this filtered def file contains only the symbols required to build custom ops. It does *not* contain the symbols required to actually use the C++ API as defined in the reference.\r\n\r\nFrom https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/tensorflow.bzl :\r\n\r\n```.bzl\r\n# A list of targets that contains the implemenation of\r\n# tf_custom_op_library_additional_deps. It's used to generate a DEF file for\r\n# exporting symbols from _pywrap_tensorflow.dll on Windows.\r\ndef tf_custom_op_library_additional_deps_impl():\r\n    return [\r\n        \"@com_google_protobuf//:protobuf\",\r\n        \"@nsync//:nsync_cpp\",\r\n        # for //third_party/eigen3\r\n        clean_dep(\"//third_party/eigen3\"),\r\n        # for //tensorflow/core:framework_headers_lib\r\n        clean_dep(\"//tensorflow/core:framework\"),\r\n        clean_dep(\"//tensorflow/core:reader_base\"),\r\n    ]\r\n```\r\nNote that the `//tensorflow/core:ops` target is missing here.\r\n\r\n\r\n\r\n\r\n\r\n", "A related issue is https://github.com/tensorflow/tensorflow/issues/30246\r\n\r\nI have an attempt to fix here: https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96, could you please test if that helps?", "The basic idea of meteorcloudy@4b8e7be is to let Bazel parse all symbols from object files of a cc_library (unlike cc_binary, it will not include its transitive dependencies) and generate a DEF file that could be filtered (overcome the 64K symbol limit) and used for export symbols in tensorflow.dll.", "> A related issue is #30246\r\n> \r\n> I have an attempt to fix here: [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), could you please test if that helps?\r\n\r\nI'm afraid not. It fails with the following unresolved externals: \r\n```\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) referenced in function \"public: __cdecl tensorflow::Input::Input(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z)\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ) referenced in function main\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ) referenced in function main\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::ClientSession(class tensorflow::Scope const &)\" (??0ClientSession@tensorflow@@QEAA@AEBVScope@1@@Z) referenced in function main\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::ClientSession::~ClientSession(void)\" (??1ClientSession@tensorflow@@QEAA@XZ) referenced in function main\r\nC:\\Users\\plane\\projects\\tf_test\\out\\build\\x64-Release\\main.cpp.obj : error LNK2019: unresolved external symbol \"public: class tensorflow::Status __cdecl tensorflow::ClientSession::Run(class std::unordered_map<class tensorflow::Output,struct tensorflow::Input::Initializer,struct tensorflow::OutputHash,struct std::equal_to<class tensorflow::Output>,class std::allocator<struct std::pair<class tensorflow::Output const ,struct tensorflow::Input::Initializer> > > const &,class std::vector<class tensorflow::Output,class std::allocator<class tensorflow::Output> > const &,class std::vector<class tensorflow::Tensor,class std::allocator<class tensorflow::Tensor> > *)const \" (?Run@ClientSession@tensorflow@@QEBA?AVStatus@2@AEBV?$unordered_map@VOutput@tensorflow@@UInitializer@Input@2@UOutputHash@2@U?$equal_to@VOutput@tensorflow@@@std@@V?$allocator@U?$pair@$$CBVOutput@tensorflow@@UInitializer@Input@2@@std@@@7@@std@@AEBV?$vector@VOutput@tensorflow@@V?$allocator@VOutput@tensorflow@@@std@@@5@PEAV?$vector@VTensor@tensorflow@@V?$allocator@VTensor@tensorflow@@@std@@@5@@Z) referenced in function main\r\n```\r\n\r\nIn addition the `SavedModel` interfaces as described in the [Documentation](https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c)  are not available (ie, attempting to use them results in linking errors). This is resolved in the linked PR.", "I see, https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96 basically offers an example to get the DEF file from cc_library instead of cc_binary. The DEF file will not contains symbols from the transitive cc_libraries, so it makes it easier to not exceed the symbol limits (64K). I think if you add the corresponding cc_libraries for the missing symbols in \r\n```\r\nfilegroup(\r\n    name = \"cc_ops_def_file\",\r\n    srcs = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:array_ops\",\r\n        \"//tensorflow/cc:const_op\",\r\n        \"//tensorflow/cc:math_ops\",\r\n    ],\r\n    output_group = \"def_file\",\r\n)\r\n```\r\nIt will probably work.", "I had the same problem as @planetmarshall. Applying the patch of [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), i have a linking problem for ClientSession .\r\nAfter applying [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), I added some lines in \"tensorflow/BUILD\" so I had this:\r\n```\r\nfilegroup(\r\n    name = \"cc_ops_def_file\",\r\n    srcs = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:array_ops\",\r\n        \"//tensorflow/cc:const_op\",\r\n        \"//tensorflow/cc:math_ops\",\r\n        \"//tensorflow/cc:scope\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/cc/profiler\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n    output_group = \"def_file\",\r\n)\r\n```\r\nThen i rebuild the .lib.\r\n\r\nNow I have no more linking problems, but the Add operation returns an error during runtime:\r\n`Unhandled exception at 0x00007FFCB07BA719 in ConsoleApplication1.exe: Microsoft C++ exception: std::bad_alloc at memory location 0x0000005151BEEAF0.`\r\n", "> Now I have no more linking problems, but the Add operation returns an error during runtime:\r\n> `Unhandled exception at 0x00007FFCB07BA719 in ConsoleApplication1.exe: Microsoft C++ exception: std::bad_alloc at memory location 0x0000005151BEEAF0.`\r\n\r\nI've just resolved the _bad_alloc_ problem. I switch from Debug to Release in Microsoft Visual Studio.\r\nNow i'm able to execute the example from Tensorflow 2.3.0 API Reference.\r\n\r\nThanks for your help!", "> I had the same problem as @planetmarshall. Applying the patch of [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), i have a linking problem for ClientSession .\r\n> After applying [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), I added some lines in \"tensorflow/BUILD\" so I had this:\r\n> \r\n> ```\r\n> filegroup(\r\n>     name = \"cc_ops_def_file\",\r\n>     srcs = [\r\n>         \"//tensorflow/cc:cc_ops\",\r\n>         \"//tensorflow/cc:array_ops\",\r\n>         \"//tensorflow/cc:const_op\",\r\n>         \"//tensorflow/cc:math_ops\",\r\n>         \"//tensorflow/cc:scope\",\r\n>         \"//tensorflow/cc:client_session\",\r\n>         \"//tensorflow/cc/profiler\",\r\n>         \"//tensorflow/core:tensorflow\",\r\n>     ],\r\n>     output_group = \"def_file\",\r\n> )\r\n> ```\r\n> \r\n> Then i rebuild the .lib.\r\n> \r\n> Now I have no more linking problems, but the Add operation returns an error during runtime:\r\n> `Unhandled exception at 0x00007FFCB07BA719 in ConsoleApplication1.exe: Microsoft C++ exception: std::bad_alloc at memory location 0x0000005151BEEAF0.`\r\n\r\nHi @andrea-garritano, I have same problem and do like meteorcloudy@4b8e7be, but it's raise some of error: \r\n\"\"\"\r\nERROR: C:/users/me/documents/tensorflow/tensorflow/BUILD:642:11: in cmd attribute of genrule rule //tensorflow:tensorflow_filtered_def_file: label '//tensorflow:cc_ops_def_file' in $(locations) expression expands to no files\r\nERROR: Analysis of target '//tensorflow:tensorflow_cc.dll' failed; build aborted: Analysis of target '//tensorflow:tensorflow_filtered_def_file' failed; build aborted\r\nINFO: Elapsed time: 0.734s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded, 2365 targets configured)\r\n\"\"\"\r\nHave you meet this error?", "> > I had the same problem as @planetmarshall. Applying the patch of [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), i have a linking problem for ClientSession .\r\n> > After applying [meteorcloudy@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7bef11040c465453e973eb0c6ad43cfd3c96), I added some lines in \"tensorflow/BUILD\" so I had this:\r\n> > ```\r\n> > filegroup(\r\n> >     name = \"cc_ops_def_file\",\r\n> >     srcs = [\r\n> >         \"//tensorflow/cc:cc_ops\",\r\n> >         \"//tensorflow/cc:array_ops\",\r\n> >         \"//tensorflow/cc:const_op\",\r\n> >         \"//tensorflow/cc:math_ops\",\r\n> >         \"//tensorflow/cc:scope\",\r\n> >         \"//tensorflow/cc:client_session\",\r\n> >         \"//tensorflow/cc/profiler\",\r\n> >         \"//tensorflow/core:tensorflow\",\r\n> >     ],\r\n> >     output_group = \"def_file\",\r\n> > )\r\n> > ```\r\n> > \r\n> > \r\n> > Then i rebuild the .lib.\r\n> > Now I have no more linking problems, but the Add operation returns an error during runtime:\r\n> > `Unhandled exception at 0x00007FFCB07BA719 in ConsoleApplication1.exe: Microsoft C++ exception: std::bad_alloc at memory location 0x0000005151BEEAF0.`\r\n> \r\n> Hi @andrea-garritano, I have same problem and do like [meteorcloudy/tensorflow@4b8e7be](https://github.com/meteorcloudy/tensorflow/commit/4b8e7be), but it's raise some of error:\r\n> \"\"\"\r\n> ERROR: C:/users/me/documents/tensorflow/tensorflow/BUILD:642:11: in cmd attribute of genrule rule //tensorflow:tensorflow_filtered_def_file: label '//tensorflow:cc_ops_def_file' in $(locations) expression expands to no files\r\n> ERROR: Analysis of target '//tensorflow:tensorflow_cc.dll' failed; build aborted: Analysis of target '//tensorflow:tensorflow_filtered_def_file' failed; build aborted\r\n> INFO: Elapsed time: 0.734s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (2 packages loaded, 2365 targets configured)\r\n> \"\"\"\r\n> Have you meet this error?\r\n\r\nI'm sorry, but I never met this error.", "I did like @andrea-garritano suggested and tensorflow_cc builds successfully but I had 2 linking errors when I build the test code:\r\n\r\n```\r\nError\tLNK2001\tsimbolo esterno \"public: __cdecl tensorflow::Operation::Operation(class tensorflow::Node *)\" (??0Operation@tensorflow@@QEAA@PEAVNode@1@@Z) non risolto\tcnn_tf_test\tC:\\Users\\DESKTOP-SERVER2\\Desktop\\cnn_tf_test\\cnn_tf_test\\main.obj\t1\t\r\nError\tLNK2001\tsimbolo esterno \"public: __cdecl tensorflow::Input::Initializer::Initializer(class std::initializer_list<struct tensorflow::Input::Initializer> const &)\" (??0Initializer@Input@tensorflow@@QEAA@AEBV?$initializer_list@UInitializer@Input@tensorflow@@@std@@@Z) non risolto\tcnn_tf_test\tC:\\Users\\DESKTOP-SERVER2\\Desktop\\cnn_tf_test\\cnn_tf_test\\main.obj\t1\r\n```\t\r\nAny suggestions? \r\nThank you,\r\nBest regards\r\n", "I solved putting `TF_EXPORT` macro in ops.h in front of the requested class methods `Operation::Operation` and `Input::Initializer`, and adding this include at the beginning of ops.h include section:\r\n`#include \"tensorflow/core/platform/macros.h\"`", "I have now another linking error when I try to include the freeze_saved_model module in the BUILD file as below:\r\n\r\n```\r\nfilegroup(\r\n    name = \"cc_ops_def_file\",\r\n    srcs = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:array_ops\",\r\n        \"//tensorflow/cc:const_op\",\r\n        \"//tensorflow/cc:math_ops\",\r\n        \"//tensorflow/cc:scope\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/cc/profiler\",\r\n        \"//tensorflow/core:tensorflow\",\r\n\t\"//tensorflow/cc/tools:freeze_saved_model\",\r\n    ],\t\r\n    output_group = \"def_file\",\r\n)\r\n```\r\n\r\nAdding the last src causes a lot of linker errors when compiling tensorflow_cc, some of them reported below. Does anyone have some suggestions? \r\nThank you\r\n\r\n```\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: __cdecl std::pair<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> *,bool>::pair<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> *,bool><struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> * &,bool,0>(struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> * &,bool &&)\" (??$?0AEAPEAU?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@_N$0A@@?$pair@PEAU?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@_N@std@@QEAA@AEAPEAU?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@1@$$QEA_N@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: __cdecl std::_List_node_emplace_op2<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > >::_List_node_emplace_op2<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > ><struct std::piecewise_construct_t const &,class std::tuple<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &>,class std::tuple<> >(class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > &,struct std::piecewise_construct_t const &,class std::tuple<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &> &&,class std::tuple<> &&)\" (??$?0AEBUpiecewise_construct_t@std@@V?$tuple@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@1@V?$tuple@$$V@1@@?$_List_node_emplace_op2@V?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@std@@@std@@QEAA@AEAV?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@1@AEBUpiecewise_construct_t@1@$$QEAV?$tuple@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@1@$$QEAV?$tuple@$$V@1@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: __cdecl std::_Compressed_pair<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> >,class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > >,1>::_Compressed_pair<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> >,class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > >,1><class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > const &>(struct std::_One_then_variadic_args_t,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > const &)\" (??$?0AEBV?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@$$V@?$_Compressed_pair@V?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@std@@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@2@$00@std@@QEAA@U_One_then_variadic_args_t@1@AEBV?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@1@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: __cdecl std::_Compressed_pair<class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > >,class std::_Vector_val<struct std::_Simple_types<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > >,1>::_Compressed_pair<class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > >,class std::_Vector_val<struct std::_Simple_types<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > >,1><class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > const &>(struct std::_One_then_variadic_args_t,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > const &)\" (??$?0AEBV?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@$$V@?$_Compressed_pair@V?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@V?$_Vector_val@U?$_Simple_types@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@@2@$00@std@@QEAA@U_One_then_variadic_args_t@1@AEBV?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@1@@Z) non risolto\r\n...\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: class tensorflow::NodeDef * const & __cdecl std::unordered_map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::NodeDef *,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > >::at(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?at@?$unordered_map@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@@std@@QEBAAEBQEAVNodeDef@tensorflow@@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: unsigned __int64 __cdecl std::_Hash<class std::_Umap_traits<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::NodeDef *,class std::_Uhash_compare<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> >,0> >::bucket(class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &)const \" (?bucket@?$_Hash@V?$_Umap_traits@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@V?$_Uhash_compare@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@$0A@@std@@@std@@QEBA_KAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@2@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: unsigned __int64 __cdecl std::_Hash<class std::_Umap_traits<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::NodeDef *,class std::_Uhash_compare<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> >,0> >::bucket_count(void)const \" (?bucket_count@?$_Hash@V?$_Umap_traits@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@V?$_Uhash_compare@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@$0A@@std@@@std@@QEBA_KXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: void __cdecl std::_Hash<class std::_Umap_traits<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::NodeDef *,class std::_Uhash_compare<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> >,0> >::clear(void)\" (?clear@?$_Hash@V?$_Umap_traits@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@V?$_Uhash_compare@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@$0A@@std@@@std@@QEAAXXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: void __cdecl std::list<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > >::clear(void)\" (?clear@?$list@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@@std@@QEAAXXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: class google::protobuf::RepeatedPtrField<class tensorflow::TensorInfo> const & __cdecl tensorflow::TensorInfo_CompositeTensor::components(void)const \" (?components@TensorInfo_CompositeTensor@tensorflow@@QEBAAEBV?$RepeatedPtrField@VTensorInfo@tensorflow@@@protobuf@google@@XZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: static void __cdecl std::_Default_allocator_traits<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > >::deallocate(class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > &,struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> * const,unsigned __int64)\" (?deallocate@?$_Default_allocator_traits@V?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@std@@@std@@SAXAEAV?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@2@QEAU?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@2@_K@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: void __cdecl std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> >::deallocate(struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> * const,unsigned __int64)\" (?deallocate@?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@std@@QEAAXQEAU?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@2@_K@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: void __cdecl std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > >::deallocate(class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > * const,unsigned __int64)\" (?deallocate@?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@QEAAXQEAV?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@2@_K@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: float __cdecl std::_Hash<class std::_Umap_traits<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class tensorflow::NodeDef *,class std::_Uhash_compare<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> >,0> >::max_load_factor(void)const \" (?max_load_factor@?$_Hash@V?$_Umap_traits@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@V?$_Uhash_compare@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@2@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@$0A@@std@@@std@@QEBAMXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: static unsigned __int64 __cdecl std::_Default_allocator_traits<class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > >::max_size(class std::allocator<struct std::_List_node<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,void *> > const &)\" (?max_size@?$_Default_allocator_traits@V?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@std@@@std@@SA_KAEBV?$allocator@U?$_List_node@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@PEAX@std@@@2@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: static unsigned __int64 __cdecl std::_Default_allocator_traits<class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > >::max_size(class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > const &)\" (?max_size@?$_Default_allocator_traits@V?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@@std@@SA_KAEBV?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@2@@Z) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: unsigned __int64 __cdecl std::_Hash_vec<class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > >::max_size(void)const \" (?max_size@?$_Hash_vec@V?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@@std@@QEBA_KXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: unsigned __int64 __cdecl std::list<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *>,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > >::max_size(void)const \" (?max_size@?$list@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@V?$allocator@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@2@@std@@QEBA_KXZ) non risolto\r\nlibtensorflow_cc.dll.exp : error LNK2001: simbolo esterno \"public: unsigned __int64 __cdecl std::_Hash_vec<class std::allocator<class std::_List_unchecked_iterator<class std::_List_val<struct std::_List_simple_types<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class tensorflow::NodeDef *> > > > > >::size(void)const \" (?size@?$_Hash_vec@V?$allocator@V?$_List_unchecked_iterator@V?$_List_val@U?$_List_simple_types@U?$pair@$$CBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVNodeDef@tensorflow@@@std@@@std@@@std@@@std@@@std@@@std@@QEBA_KXZ) non risolto\r\n```\r\n", "Hi @Shadowman82.\r\n\r\nDid you ever get anywhere with this? If I build dll just with the ops \r\n\r\n\"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:array_ops\",\r\n        \"//tensorflow/cc:const_op\",\r\n        \"//tensorflow/cc:math_ops\",\r\n\r\nit works and I get a dll and lib. When I link with  my application, however it complains about ClientSession missing symbols.\r\n\r\nI note that the file client_session.cc is NOT even built unless it is added to the list above.\r\n\r\nI tried adding some garbage text to the file and the dll still compiled. If I add \r\n\r\n\"//tensorflow/cc:client_session\",\r\n\r\nto the list above and build, it tries to build client_session.cc", "Continuing from above. If I add all of:\r\n\r\nfilegroup(\r\n    name = \"cc_ops_def_file\",\r\n    srcs = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:array_ops\",\r\n        \"//tensorflow/cc:const_op\",\r\n        \"//tensorflow/cc:math_ops\",\r\n        \"//tensorflow/cc:scope\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/cc/profiler\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n    output_group = \"def_file\",\r\n)\r\n\r\nI can't even build the dll. Loads of missing symbols similar to those reported by @Shadowman82 ", "@philipeccles try and check this out I ended up getting it to work https://dkjoi.medium.com/tf2-4-dll-with-gpu-support-for-3090-in-windows-305126bc0d17?source=your_stories_page-------------------------------------\r\n\r\nI also have one simple with linking it to a standard model and doing inference.", "I finally got it to build and link with my application.\r\nCPU for now, doubtless GPU will cause another level of hassle.\r\n\r\nOne of the problems is I was using tensorflow.dll instead of tensorflow_cc.dll. The latter automatically builds client_session, the former doesn't. The only way I could get client_session to build was adding it in the filegroup in BUILD file as above. But building tensorflow.dll failed. If I build tensorflow_cc.dll I don't have to add client_session to build file and it all works.\r\n\r\nI will show my entire patch file at end of this message.\r\n\r\n**What exactly is the major difference between  tensorflow.dll instead of **tensorflow_cc.dll?**** I see no documentation.\r\n\r\nJust to make the point. Tensorflow is open source so much gratitude to everyone who has contributed. However building it is on a different level entirely in terms of difficulty, compared to other open source projects I have built. I have spent a week at least getting to this point. Don't assume it's going to be easy!\r\n\r\nSo here is my patch to tensorflow-2.3.0\r\n\r\n", "\r\n[build.patch.txt](https://github.com/tensorflow/tensorflow/files/6553552/build.patch.txt)\r\n", "@Shadowman82 , do you know the difference between tensorflow.dll and tensorflow_cc.dll? In the BUILD file they seem to be built in a very similar way.", "@philipeccles , I managed to solve it in the same way like you. I also had the same issue because I was using tensorflow.dll instead of tensorflow_cc.dll. Looking at the BUILD file the tensorflow_cc.dll I didn't find a reason for that", "I recently succeeded in building 2.6 (both tensorflow.dll and tensorflow_cc.dll) with GPU on Windows but linking it is a different challenge altogether to make a simple Matmul operation work and honestly, I am struggling. I arrived at this discussion after searching a lot for answers. Just as @philipeccles has stated above, I would like to express my gratitude to the tf team for making it accessible. However unless the accessibility of C++ API is improved particularly on Windows, it is a huge problem. Mere making something work should not feel like an achievement but currently this is how it feels with tensorflow C++ API. I am requesting all the good people in tf team to make it easy and intuitive to add symbols such as a file that list all the symbols perhaps that could be read by tf executable during compile time to determine what should be included and excluded (60k symbols limit will force us to make that choice). TF_EXPORT is tedious. C++ API is not usable in its current form on Windows for building graps. \r\nIs there a good documentation to build tf C++  API from source for Linux  Debia/ Ubuntu 20.04? I'd be very grateful. Thank you!", "Hi @planetmarshall !\r\nWe are checking to see whether you still need help in this issue . Please create a new issue if the issue is replicating in Latest version TF 2.6 . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41904\">No</a>\n"]}, {"number": 41903, "title": "Add test S3 ReadLargeFile", "body": "@mihaimaruseac \r\nThis PR adds test agains large file for `s3`. More tests will come in next PR.", "comments": []}, {"number": 41902, "title": "Relax constraint on `numpy` dependency", "body": "In https://github.com/tensorflow/tensorflow/commit/79518facb4b857af9d9d5df2da463fdbf7eb0e3e the constraint of the `numpy` dependency was bounded to `numpy < 1.19.0` from `numpy < 2.0` due to breaking ABI changes from https://github.com/numpy/numpy/pull/15355.\r\n\r\nThe next quarterly release of pip, 20.3, in October 2020 will change the default dependency resolution behavior to ensure that this constraint is more strictly followed. Specifically, it means that installing the latest version of TensorFlow will have the effect of asking the user to downgrade to `numpy < 1.19.0` if they have `numpy >= 1.19.0` installed.\r\n\r\nIt's likely that there are currently many TensorFlow users who are also using `numpy >= 1.19.0` without issue, who will start to experience this behavior when the new version of pip is released. This is already being reported by users who have opted-in to the new version of the resolver (https://github.com/pypa/pip/issues/8076#issuecomment-664161421).\r\n\r\nI couldn't find an issue about resetting this constraint to `numpy < 2.0` and migrating to the new ABI (sorry if I missed it) so I wanted to create this to capture that, and also give you some early warning about how the change pip's behavior will affect users of this project. Thanks!\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41902\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41902\">No</a>\n"]}, {"number": 41901, "title": "Could not import PIL.Image. The use of `array_to_img` requires PIL", "body": "**System information**\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution : ubuntu 18.04 LTS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson Nano\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): tensorflow==1.15.2+nv20.6\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: CUDA 10.2 / cuDNN 8.00\r\nGPU model and memory: NVIDIA Tegra X1 (nvgpu)/integrated and 4GB memory\r\nKeras: 2.0.5\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhile executing an application, i got following exception\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/2168986/88942166-83c02300-d2a7-11ea-8948-551074acb48d.png)\r\n\r\n\r\nAfter that, I installed pillow-7.2.0. But i am getting same error message **ImportError: Could not import PIL.Image. The use of array_to_img requires PIL**.\r\n\r\nCould you please help me to resolve the issue?\r\n", "comments": ["@suresh-s \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41901\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41901\">No</a>\n"]}, {"number": 41900, "title": "TFLu r2.3: Broken memory allocation for micro interpreter object with TF_LITE_STATIC_MEMORY enabled", "body": "Compiling TFLu r2.3 with TF_LITE_STATIC_MEMORY gives an incorrect memory allocation for the micro interpreter object. \r\n\r\n**System information**\r\n- TFLu r2.3\r\n- OS Platform: Linux Ubuntu 16.04 or Windows 10 64 bit\r\n- Target: Cortex M4F\r\n\r\n**Current behavior**\r\nIn my case the size of the network input data is\r\ninterpreter->input(0)->bytes = 1920\r\n\r\nAfter calling for memory allocation\r\nTfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n\r\nthe memory address of the input data is\r\ninterpreter->input(0)->data = 0x8012174\r\nand memory address of the input tensor structure is\r\ninterpreter->input(0) = 0x8012570\r\n\r\nThis gives only 1020 bytes memory space for the data which is 1920 btes. As a consequence, the input tensor structure is overwritten when copying the input data.\r\n\r\n**Expected behavior**\r\nAfter removing TF_LITE_STATIC_MEMORY from the CCFLAGS/CXXFLAGS the memory allocation leaves enough space between the data and the input tensor structure:\r\ninterpreter->input(0) = 0x80124d0\r\ninterpreter->input(0)->data = 0x80103c0\r\n\r\n**Other info**\r\nUsing TFLu r2.2 with TF_LITE_STATIC_MEMORY does not show the problem. It gives correct memory layout. Starting with commit fbf407383c93774d10bd7c45cd66788a070b0e07 (mid of June '20) the memory layout is broken.\r\n\r\nI'm not sure if this is a bug or if it is intended behavior and I better compile without TF_LITE_STATIC_MEMORY.\r\n", "comments": ["It looks like this may be related to some of our memory optimization efforts. I have filed an internal bug to track this and will update when we have a resolution.", "Thx for looking into this. As indicated in my bug description, temporary workarounds are\r\n- using r2.2\r\n- using r2.3 without TF_LITE_STATIC_MEMORY in the CCFLAGS/CXXFLAGS", "This is a case of our making changes that are not backwards compatible coupled with the fact that Tensorflow Lite Micro doesn't currently conform to the release cadence of the broader Tensorflow project.\r\n\r\nMy recommendation here is to use TFLM from source. Since we no longer support using TFLM without TF_LITE_STATIC_MEMORY (and have thus made it the default in our Makefile).\r\n\r\nI'm going to close the current issue but please open a new one if you are seeing issues when building from source.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41900\">No</a>\n", "I just found the reason for the observation on my side: the application project includes header files from TFLM to instantiate the interpreter. I missed to define TF_LITE_STATIC_MEMORY when including such header files.\r\n\r\nIs not a bug in TFLM. Sorry for the confusion.", "@ml-0 Hi, I'm experiencing a similar issue. Could you clarify which header files you are referring to? thanks in advanced.\r\n", "I think what @ml-0 is referring to is that any external code that includes TFLM headers should be built with `-DTF_LITE_STATIC_MEMORY`, or have a `#define TF_LITE_STATIC_MEMORY` before the headers are included.\r\n\r\nApologies for not being very clear with my previous reply - what I was emphasizing is that using TFLM without TF_LITE_STATIC_MEMORY defined isn't really supported anymore, even on x86.\r\n", "Hi @COTASPAR, my application project comes with its own Makefile (not part of TFLM) and links against the libtensorflow_microlite .a. To instantiate the TFLM interpreter I have to include\r\n```\r\n#include \"tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"tensorflow/lite/micro/all_ops_resolver.h\"\r\n#include \"tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"tensorflow/lite/micro/micro_interpreter.h\"\r\n#include \"tensorflow/lite/version.h\" \r\n#include \"tensorflow/lite/c/common.h\"\r\n```\r\nAs the last include (common.h) heavily uses TF_LITE_STATIC_MEMORY this macro has to be defined in the application project as well. If not, the application uses a different definition for TfLiteTensor as the library does.\r\n\r\nThis problem does not appear in all the examples that come with TFLM because they compile TFLM + the example code with the same settings (single Makefile).\r\n\r\n@advaitjain Do you see the chance to avoid such problems by inverting the definition of TF_LITE_STATIC_MEMORY? The code we should use does not require it and it has to be defined only for special purposes?", "Unfortunate as it is, its best to use TFLM with  TF_LITE_STATIC_MEMORY. It has basically become a way to separate the TFLM build graph from the TfLite build graph.\r\n\r\nWithout this define, the performance is degraded due to allocations of TfLiteTensors from the temp space at each Eval. More importantly, its not a path that we test or support anymore so there can definitely be bugs.\r\n\r\nWe would like to get to a point where TFLM does not need this additional define at all, but the code sharing with TfLite (via common.h as you pointed out and more) makes it something that will take some time to disentangle."]}, {"number": 41899, "title": "Compilation Failure when using tf.keras.layers.UpSampling2D() with colab TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS, Mojave 10.14.6 (Running in Colab)\r\n- TensorFlow version (use command below):2.2.0\r\n- Python version: Python 3.6.9\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhile using tf.keras.layers.Upsampling2D() in Colab TPU, The following error occurs:\r\n\r\n```UnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-34-7125d086c152> in <module>()\r\n     10   model.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\r\n     11 model.summary()\r\n---> 12 history = model.fit(training_dataset, epochs=10, validation_data=validation_dataset, steps_per_epoch=steps_per_epoch)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnimplementedError: {{function_node __inference_train_function_66860}} Compilation failure: CustomCall is not supported to have a dynamic dimension\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_7048177641045036345/_5}}]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf I use tf.keras.UpSampling2D((2,2)), I should expect the tensor to be resized by a factor of 2.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\ndef create_model():\r\n    \r\n    pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)\r\n    pretrained_model.trainable = True\r\n\r\n    model = tf.keras.Sequential([\r\n        pretrained_model,\r\n        tf.keras.layers.MaxPooling2D((2,2)),\r\n        tf.keras.layers.UpSampling2D((2,2)),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        #tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(5, activation='softmax', dtype=tf.float32) # the float32 is needed on softmax layer when using mixed precision\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer='adam',\r\n        loss = 'categorical_crossentropy',\r\n        metrics=['accuracy']\r\n    )\r\n\r\n    return model\r\nwith strategy.scope():\r\n  model = create_model()\r\nmodel.summary()\r\nstart_time = time.time()\r\nhistory = model.fit(training_dataset, validation_data=validation_dataset,\r\n                    steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])\r\n\r\nfinal_accuracy = history.history[\"val_accuracy\"][-5:]\r\nprint(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))\r\nprint(\"TRAINING TIME: \", time.time() - start_time, \" sec\")\r\n```\r\n\r\nThis is the [link](https://colab.research.google.com/drive/1yABsMbLguooJutACqH3splUq3lMkc38N?usp=sharing) to the Colab Notebook\r\n\r\n**Other info / logs** \r\nThe training dataset is a publicly available TFRecord Dataset", "comments": ["@linkun-1998 \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/38234#issuecomment-609482622) and let us know if it helps resolve your issue.[[link]](https://stackoverflow.com/questions/57658114/how-to-solve-propagation-of-dynamic-dimension-failed-error-in-tf-keras-with-tp)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41899\">No</a>\n", "If @Saduf2019 's answer doesn't solve it for you, there might be two different solutions to this error. \r\n\r\nTPU requires explicitly defined shape of input in your data pipeline. First, make sure you're using tf.data pipeline since it offers best compatibility with TPUs. \r\n\r\nAs the shape of input image/label looks something like (batch_size, height, weight, channels), all of them have to be stated explicitly and statically for the TPU. \r\n\r\n1. For h, w, c,  you should use tf.reshape when reading tf.Example and extracting image, label, etc. Example code below for reading TFRecords. Note that tf.reshape() has to be used even if it just preserves the original shape.\r\n\r\n```python\r\ndef read_tfrecord(record, dim):\r\n    LABELED_TFREC_FORMAT = {\r\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\r\n        \"label\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\r\n    }\r\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\r\n    image = tf.io.decode_image(example['image'], dtype=tf.float32, channels=3)\r\n    >>>image = tf.reshape(image, (dim, dim, 3))<<<\r\n  \r\n    label = tf.io.decode_image(example['label'], dtype=tf.float32, channels=1)\r\n    >>>label = tf.reshape(label, (dim, dim, 1))<<<\r\n \r\n    return image, label # returns a dataset of (image, label) pairs\r\n```\r\n\r\n2. Now, we also need to make sure batch size is constant. This can be ensured by using tf.data.Dataset.batch(batch_size, **drop_remainder=True**). Second argument is crucial here since without it, batch size would be dynamic in order to include the remainder."]}, {"number": 41898, "title": "Keras Model, Functional API, Multi-input, Efficient allreduce is not supported for n IndexedSlices", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CUDA Version 10.1.243\r\n- GPU model and memory: (8x) Tesla K80 - 11441MiB - Driver Version: 410.72\r\n\r\n\r\n**Describe the current behavior**\r\nSame issue using TF v2.2.0.\r\nI am using Keras functional API to train a model with more than one input. As a simplified example geting the same problem,\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef build_model_():\r\n\r\n\tinput_a_size = 20\r\n\tinput_b_size = 4\r\n\tnum_classes = 2\r\n\tlen_embedding = 256\r\n\r\n\tinput_a = tf.keras.layers.Input(shape=(input_a_size,), name='input_a', dtype=np.uint8)\r\n\tinput_b = tf.keras.layers.Input(shape=(input_b_size,), name='input_b', dtype=np.float32)\r\n\r\n\tx = tf.keras.layers.Embedding(len_embedding, 100)(input_a)\r\n\tx = tf.keras.layers.Conv1D(128, 4, activation='relu')(x)\r\n\tx = tf.keras.layers.MaxPooling1D(4)(x)\r\n\tx = tf.keras.layers.Flatten()(x)\r\n\tbranch_a = tf.keras.layers.Dense(64, activation='relu')(x)\r\n\r\n\tx = tf.keras.layers.Dense(32, activation='relu')(input_b)\r\n\tbranch_b = tf.keras.layers.Dense(32, activation='relu')(x)\r\n\r\n\tconcat = tf.keras.layers.Concatenate()([\r\n\t\t\t\t                            branch_a,\r\n\t\t\t\t                            branch_b,\r\n\t\t\t\t                           ])\r\n\r\n\tx = tf.keras.layers.Dense(512, activation = 'relu')(concat)\r\n\toutput = tf.keras.layers.Dense(num_classes, name='output', activation='softmax')(x)\r\n\r\n\tmodel = tf.keras.models.Model(inputs=[\r\n\t\t\t\t                          input_a,\r\n\t\t\t\t                          input_b,\r\n\t\t\t\t                         ],\r\n\t\t\t\t                  outputs=[output])\r\n\r\n\treturn model\r\n\r\nstrategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1'])\r\nwith strategy.scope():\r\n    model = build_model_()\r\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n\r\ny_train = True\r\ny_train = tf.keras.utils.to_categorical(y_train, 2)\r\n\r\ndataset = tf.data.Dataset.from_tensors(\r\n    (\r\n        {\"input_a\": [[1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.]], \r\n         \"input_b\": [[1.], [1.], [1.], [1.]],}, \r\n        {\"output\": y_train},\r\n    )\r\n).repeat(1000000).batch(256)\r\n\r\nhistory = model.fit(\r\n    x = dataset,\r\n    epochs=10,\r\n    verbose = 1,\r\n)\r\n```\r\n\r\nWhen starting training I get this warning,\r\n`WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices`\r\nWhen training a model with 3 inputs I get, ...not supported for 2 IndexedSlices, so, I'm getting \r\n`WARNING:tensorflow:Efficient allreduce is not supported for n-1 IndexedSlices`, being `n` the number of inputs to the net.\r\n\r\nThe performance is not scaling using multiple GPUs. It gets slower with 2 GPUs vs 1 GPUs, and worst case using 8 GPUs.\r\n", "comments": ["I am running into this same issue with a different model", "@ratovarius \r\n\r\nLooks like code is incomplete. I am seeing name '`Input'` is not defined in the given code. Request you to provide  reproducible code in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nI updated the code, now it sould be reproducible", "Hi @ratovarius, right now MirroredStrategy.reduce will do a concatenation of IndexedSlices on one device, and broadcast the result back to all GPUs. This is not efficient, hence the warning. This is a known limitation and the current suggestion is to use MultiWorkerMirroredStrategy, which has a slightly better implementation for handling IndexedSlices. Despite its name, MultiWorkerMirroredStrategy can be used with a single worker without additional setup.", "Thanks for your answer @nikitamaia \r\nIs there a way to configure witch GPUs to use with MultiWorkerMirroredStrategy ? By default it uses all GPUs, right? \r\nI want to make some performance measurements.\r\n\r\nI tested your suggestion, using MultiWorkerMirroredStrategy removes the warning, but it is taking twice time for training using \r\n`strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # Default -> 8GPUs`\r\ncompared to,\r\n`strategy = tf.distribute.MirroredStrategy(['/gpu:0'])`\r\n\r\nI'm not sure about the concept of IndexedSlices. Does it get generated when using multi-input model and not when using a single input one?", " You can use `tf.config.set_visible_devices` to specify which devices are visible to the runtime. I'm curious to know what the performance is when you try a smaller number of GPUs.\r\n", "This issue mirrors some issues I am facing, the algorithm scales up when using 4 gpus under mirrored strategy, but there is no speed increase in how long it takes to get to the end of an epoch (after scaling the batch size by the number of gpus) when I go to 8 gpus. I do not believe the input pipeline is the bottleneck as it is pretty simple and has sufficient prefetch buffer size and parallel map calls\r\n\r\nWIth 8 gpus: \r\n- The gpu utilization often goes down to 0 for 1 to 4 seconds on each gpu \r\n- the gpu utilization appears to go from 100 to 0 on gpu 1, then gpu 2 etc in a sequential manner\r\n- I get the following warning WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices however the model does not return any sparse tensors, there is 1 sparse tensor created internally that is converted to a dense tensor", "I increased a little the size of the model in the original issue to get more representative timing.\r\nNow I'm sizing the Batch_Size according to number og GPUs used.\r\n```\r\nBATCH_SIZE_PER_REPLICA = 1024\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\ndataset = tf.data.Dataset.from_tensors(\r\n    (\r\n        {\"input_a\": [[1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.], [1.]], \r\n         \"input_b\": [[1.], [1.], [1.], [1.]],}, \r\n        {\"output\": y_train},\r\n    )\r\n).repeat(1000000).batch(GLOBAL_BATCH_SIZE)\r\n```\r\nTiming comparison:\r\n\r\n1. `tf.distribute.MirroredStrategy`\r\n\r\n- `strategy = tf.distribute.MirroredStrategy(['/gpu:0'])`\r\n\r\n```\r\nEpoch 1/10\r\n977/977 [==============================] - 17s 17ms/step\r\nEpoch 2/10\r\n977/977 [==============================] - 16s 17ms/step\r\n```\r\n- `strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1']) # (2 GPUs)`\r\n```\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nEpoch 1/10\r\n489/489 [==============================] - 12s 25ms/step\r\nEpoch 2/10\r\n489/489 [==============================] - 12s 25ms/step\r\n```\r\n- `strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1','/gpu:2', '/gpu:3']) # (4 GPUs)`\r\n```\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nEpoch 1/10\r\n245/245 [==============================] - 11s 46ms/step\r\nEpoch 2/10\r\n245/245 [==============================] - 11s 46ms/step\r\n```\r\n- `strategy = tf.distribute.MirroredStrategy() # (8 GPUs)`\r\n```\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nEpoch 1/10\r\n123/123 [==============================] - 12s 95ms/step\r\nEpoch 2/10\r\n123/123 [==============================] - 11s 89ms/step\r\n```\r\n\r\n\r\n1. `tf.distribute.experimental.MultiWorkerMirroredStrategy()`. Configuring number of GPUs with `tf.config.set_visible_devices`. Performance Warning disappears.\r\n\r\n- 1 GPU\r\n```\r\nEpoch 1/10\r\n977/977 [==============================] - 17s 17ms/step\r\nEpoch 2/10\r\n977/977 [==============================] - 16s 17ms/step\r\n```\r\n- 2 GPUs\r\n```\r\nEpoch 1/10\r\n489/489 [==============================] - 11s 23ms/step\r\nEpoch 2/10\r\n489/489 [==============================] - 11s 22ms/step\r\n```\r\n- 4 GPUs\r\n```\r\nEpoch 1/10\r\n245/245 [==============================] - 7s 30ms/step\r\nEpoch 2/10\r\n245/245 [==============================] - 7s 30ms/step\r\n```\r\n- 8 GPUs\r\n```\r\nEpoch 1/10\r\n123/123 [==============================] - 6s 49ms/step\r\nEpoch 2/10\r\n123/123 [==============================] - 5s 44ms/step\r\n```\r\nSo, I have performance scaling with GPUs, thanks @nikitamaia .", "Glad to hear it! When you just have one machine `MirroredStrategy` is better tested and definitely preferred. However, for this performance issue with IndexedSlices `MultiWorkerMirroredStrategy` is a potential workaround. I can update this thread when there is a change to how `MirroredStrategy` handles IndexedSlices.\r\n\r\n@r-wheeler can you open a separate issue with some more information about your model and the GPUs you're using? Sometimes when there's a drop like that it can be a network issue. Did you try experimenting with `MultiWorkerMirroredStrategy` ?\r\n", "Closing this thread now since it's a known limitation with a workaround. I will update here if anything changes.", "@nikitamaia\r\nSame issue here with tf-2.4. \r\n`MultiWorkerMirroredStrategy` is optimized for distributed training on distributed computers (networked environment)\r\n`MirroredStrategy` is optimized for multi-gpu on a single computer\r\nIs this right?\r\n\r\nIf that's the case, I think the issue should not be closed, as the workarround for distributed training with multi-input model is not optimized.", "@nikitamaia I have a similar issue https://github.com/tensorflow/tensorflow/issues/46800, the MultiWorkerMirroredStrategy \r\n work-around is not helping.  Is there a plan to address this in the upcoming releases?\r\n\r\nThank you!\r\n", "> @nikitamaia\r\n> Same issue here with tf-2.4.\r\n> `MultiWorkerMirroredStrategy` is optimized for distributed training on distributed computers (networked environment)\r\n> `MirroredStrategy` is optimized for multi-gpu on a single computer\r\n> Is this right?\r\n> \r\n> If that's the case, I think the issue should not be closed, as the workarround for distributed training with multi-input model is not optimized.\r\n\r\n@nikitamaia Though `MultiWorkerMirroredStrategy ` won't throw the IndexedSlices warning, it takes a long time to start training, much longer than `MirroredStrategy `. \r\n\r\nIs there any progress about this issue?", "Any update?  Should we open a new issue to track a solution?", "No one cares it seems :-("]}, {"number": 41897, "title": "Allow black box ops (with custom_gradient) in @tf.function", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): No (don't know how)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhen decorating a function with `@tf.function` it would be great to be able to include untraceable ops (e.g., because they contain compiled numba code), as long as they are decorated with  `@tf.custom_gradient` and somehow their signature and dtype is well specified.\r\n\r\nUnless I'm missing something major, once a function is decorated with `@custom_gradient`, it shouldn't be necessary for AutoGraph to trace it. It should be \"pluggable\" in the computational graph as a block.\r\n\r\n**Will this change the current api? How?**\r\nPossibly (perhaps optional flags in `tf.function`).\r\n\r\n**Who will benefit with this feature?**\r\nUsers who want to use custom/compiled code compatibly with `tf.function`/autograph (which sometimes happens by default, e.g. when using `Model.fit()` in keras).\r\n\r\n", "comments": ["For Python interop, [py_function](https://www.tensorflow.org/api_docs/python/tf/py_function) is effectively a black box. Beware however that the overhead of calling back into the Python runtime is not negligible. `py_function` is also not portable and is not saved by saved_model. `py_function` itself doesn't allow a custom gradients, but you can always write a wrapper function that has a @custom_gradient.\r\n\r\nFor C++ interop, there is the alternative of registering a custom op kernel.\r\n\r\nFeel free to reopen the issue if the options above don't work for you."]}, {"number": 41896, "title": "Get deadlock after Restoring SavedModelBundle(cuda10.1, cudnn7.6.3, trt6.0, Tesla T4 GPU)", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux centos 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r2.1\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: cuda 10.1.168 / cudnn 7.6.3\r\n- GPU model and memory: Tesla T4\r\n\r\nI have build my own Service  (TFServing Like) to run tensorflow for model inferencing, but get stucked after load first saved model bundle.  it's fine in cpu mode, so i tracked the stack, and got some information below.\r\n\r\nobviously, it's in async execute, and waiting for something\r\n\r\n```\r\nThread 33 (Thread 0x7fefe47b5700 (LWP 68)):\r\n#0  0x00007feff6106ba9 in syscall () from /usr/lib64/libc.so.6\r\n#1  0x00007ff018372402 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from ./lib/libtensorflow_framework.so.2\r\n#2  0x00007ff018371a49 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#3  0x00007ff01836f06b in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#4  0x00007ff01836f543 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#5  0x00007ff002a7c49c in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, long long) () from ./lib/libtensorflow_cc.so.2\r\n#6  0x00007ff002a7c56d in tensorflow::DirectSession::WaitForNotification(tensorflow::Notification*, tensorflow::DirectSession::RunState*, tensorflow::CancellationManager*, long long) () from ./lib/libtensorflow_cc.so.2\r\n#7  0x00007ff002a8ccee in tensorflow::DirectSession::RunInternal(long long, tensorflow::RunOptions const&, tensorflow::CallFrameInterface*, tensorflow::DirectSession::ExecutorsAndKeys*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&) () from ./lib/libtensorflow_cc.so.2\r\n#8  0x00007ff002a8fb97 in tensorflow::DirectSession::RunCallable(long long, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tensorflow::thread::ThreadPoolOptions const&) () from ./lib/libtensorflow_cc.so.2\r\n#9  0x00007ff002a7a760 in tensorflow::DirectSession::RunCallable(long long, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from ./lib/libtensorflow_cc.so.2\r\n#10 0x00007ff017746302 in tensorflow::(anonymous namespace)::RunOnce(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, tensorflow::Session*) [clone .constprop.473] () from ./lib/libtensorflow_framework.so.2\r\n#11 0x00007ff017746b7c in tensorflow::(anonymous namespace)::RunRestore(tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::string_view, absl::string_view, std::vector<tensorflow::AssetFileDef, std::allocator<tensorflow::AssetFileDef> > const&, tensorflow::Session*) () from ./lib/libtensorflow_framework.so.2\r\n#12 0x00007ff0177472e0 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) () from ./lib/libtensorflow_framework.so.2\r\n\r\n```\r\n\r\nwhat does it exactly waiting for...\r\n\r\n```\r\nThread 95 (Thread 0x7fef50d2b700 (LWP 170)):\r\n#0  0x00007feff6106ba9 in syscall () from /usr/lib64/libc.so.6\r\n#1  0x000000000101a1a1 in absl::lts_2020_02_25::synchronization_internal::Futex::WaitUntil (t=..., val=0, v=0x7fefac0bb150) at external/com_google_absl/absl/synchronization/internal/waiter.cc:107\r\n#2  absl::lts_2020_02_25::synchronization_internal::Waiter::Wait (this=this@entry=0x7fefac0bb150, t=t@entry=...) at external/com_google_absl/absl/synchronization/internal/waiter.cc:151\r\n#3  0x000000000101a0e2 in AbslInternalPerThreadSemWait (t=...) at external/com_google_absl/absl/synchronization/internal/waiter.h:99\r\n#4  0x00007ff018536fbd in absl::Mutex::Block(absl::base_internal::PerThreadSynch*) () from ./lib/libtensorflow_framework.so.2\r\n#5  0x00007ff01772e04a in absl::Mutex::LockSlowWithDeadline(absl::MuHowS const*, absl::Condition const*, absl::synchronization_internal::KernelTimeout, int) [clone .cold.36] () from ./lib/libtensorflow_framework.so.2\r\n#6  0x00007ff01772e05e in absl::Mutex::LockSlow(absl::MuHowS const*, absl::Condition const*, int) () from ./lib/libtensorflow_framework.so.2\r\n#7  0x00007ff018535e03 in absl::Notification::WaitForNotification() const () from ./lib/libtensorflow_framework.so.2\r\n#8  0x00007ff018365f90 in stream_executor::(anonymous namespace)::BlockOnThreadExecutor(tensorflow::thread::ThreadPool*) () from ./lib/libtensorflow_framework.so.2\r\n#9  0x00007ff01836b2b9 in stream_executor::StreamExecutor::SynchronizeAllActivity() () from ./lib/libtensorflow_framework.so.2\r\n#10 0x00007ff017e3e5da in tensorflow::GPUUtil::SyncAll(tensorflow::Device*) () from ./lib/libtensorflow_framework.so.2\r\n#11 0x00007ff017e29121 in tensorflow::BaseGPUDevice::Sync() () from ./lib/libtensorflow_framework.so.2\r\n#12 0x00007ff017e75ab1 in tensorflow::Device::Sync(std::function<void (tensorflow::Status const&)> const&) () from ./lib/libtensorflow_framework.so.2\r\n#13 0x00007ff017e88bcc in tensorflow::(anonymous namespace)::ExecutorState::Finish() () from ./lib/libtensorflow_framework.so.2\r\n#14 0x00007ff017e912f4 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) () from ./lib/libtensorflow_framework.so.2\r\n#15 0x00007ff017e91daf in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(absl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8ul, std::allocator<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode> >*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#2}>::_M_invoke(std::_Any_data const&) () from ./lib/libtensorflow_framework.so.2\r\n#16 0x00007ff017f62463 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from ./lib/libtensorflow_framework.so.2\r\n#17 0x00007ff017f5fbb3 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from ./lib/libtensorflow_framework.so.2\r\n#18 0x00007feff66aed9f in std::execute_native_thread_routine (__p=0xacf3cb0) at ../../../.././libstdc++-v3/src/c++11/thread.cc:80\r\n#19 0x00007feff7269e65 in start_thread () from /usr/lib64/libpthread.so.0\r\n#20 0x00007feff610c88d in clone () from /usr/lib64/libc.so.6\r\n```\r\n\r\n```\r\nThread 80 (Thread 0x7fefb66d7700 (LWP 154)):\r\n#0  0x00007feff6106ba9 in syscall () from /usr/lib64/libc.so.6\r\n#1  0x00007ff003165e31 in absl::synchronization_internal::Waiter::Wait(absl::synchronization_internal::KernelTimeout) () from ./lib/libtensorflow_cc.so.2\r\n#2  0x00007ff003165cc2 in AbslInternalPerThreadSemWait () from ./lib/libtensorflow_cc.so.2\r\n#3  0x00007ff00316718d in absl::Mutex::Block(absl::base_internal::PerThreadSynch*) () from ./lib/libtensorflow_cc.so.2\r\n#4  0x00007ff00316810d in absl::Mutex::AwaitCommon(absl::Condition const&, absl::synchronization_internal::KernelTimeout) () from ./lib/libtensorflow_cc.so.2\r\n#5  0x00007ff00316816d in absl::Mutex::Await(absl::Condition const&) () from ./lib/libtensorflow_cc.so.2\r\n#6  0x00007ff00278fbfb in stream_executor::host::HostStream::WorkLoop() () from ./lib/libtensorflow_cc.so.2\r\n#7  0x00007feff66aed9f in std::execute_native_thread_routine (__p=0x9ede390) at ../../../.././libstdc++-v3/src/c++11/thread.cc:80\r\n#8  0x00007feff7269e65 in start_thread () from /usr/lib64/libpthread.so.0\r\n#9  0x00007feff610c88d in clone () from /usr/lib64/libc.so.6\r\n\r\nThread 85 (Thread 0x7fefadaf1700 (LWP 160)):\r\n#0  0x00007feff6106ba9 in syscall () from /usr/lib64/libc.so.6\r\n#1  0x00007ff018372402 in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) () from ./lib/libtensorflow_framework.so.2\r\n#2  0x00007ff018371a49 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#3  0x00007ff01836f06b in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#4  0x00007ff01836f543 in nsync::nsync_cv_wait_with_deadline(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*, timespec, nsync::nsync_note_s_*) () from ./lib/libtensorflow_framework.so.2\r\n#5  0x00007ff017f2e153 in tensorflow::EventMgr::PollLoop() () from ./lib/libtensorflow_framework.so.2\r\n#6  0x00007ff017f62463 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from ./lib/libtensorflow_framework.so.2\r\n#7  0x00007ff017f5fbb3 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from ./lib/libtensorflow_framework.so.2\r\n#8  0x00007feff66aed9f in std::execute_native_thread_routine (__p=0xacf3410) at ../../../.././libstdc++-v3/src/c++11/thread.cc:80\r\n#9  0x00007feff7269e65 in start_thread () from /usr/lib64/libpthread.so.0\r\n#10 0x00007feff610c88d in clone () from /usr/lib64/libc.so.6\r\n```\r\n\r\n```\r\nGPU Info\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:08.0 Off |                    0 |\r\n| N/A   56C    P0    28W /  70W |  14333MiB / 15079MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\ntf logs\r\n\r\n```\r\n2020-07-30 20:07:18.210474: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2020-07-30 20:07:18.225309: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2494140000 Hz\r\n2020-07-30 20:07:18.226300: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fc1180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 20:07:18.226319: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-30 20:07:18.228940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-30 20:07:18.374126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.375556: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fc0700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-30 20:07:18.375588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n2020-07-30 20:07:18.375783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.377080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:00:08.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2020-07-30 20:07:18.377634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 20:07:18.379342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-30 20:07:18.380984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-30 20:07:18.381417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-30 20:07:18.383041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-30 20:07:18.383863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-30 20:07:18.387237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-30 20:07:18.387324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.388741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.389989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-07-30 20:07:18.390018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-30 20:07:18.391498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-30 20:07:18.391514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-07-30 20:07:18.391524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-07-30 20:07:18.391652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.392929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-30 20:07:18.394207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:08.0, compute capability: 7.5)\r\n2020-07-30 20:07:18.402115: I tensorflow/cc/saved_model/loader.cc:203] Restoring SavedModel bundle.\r\n```\r\n\r\ncuda threads bt\r\n```\r\nThread 81 (Thread 0x7fefaeaf3700 (LWP 155)):\r\n#0  0x00007feff610dd1f in accept4 () from /usr/lib64/libc.so.6\r\n#1  0x00007fefb4f6b38a in ?? () from /usr/lib64/libcuda.so.1\r\n#2  0x00007fefb4f5d68d in ?? () from /usr/lib64/libcuda.so.1\r\n#3  0x00007fefb4f6ca58 in ?? () from /usr/lib64/libcuda.so.1\r\n#4  0x00007feff7269e65 in start_thread () from /usr/lib64/libpthread.so.0\r\n#5  0x00007feff610c88d in clone () from /usr/lib64/libc.so.6\r\n\r\nThread 84 (Thread 0x7fefacaef700 (LWP 159)):\r\n#0  0x00007feff6101bed in poll () from /usr/lib64/libc.so.6\r\n#1  0x00007fefb4f6a3e3 in ?? () from /usr/lib64/libcuda.so.1\r\n#2  0x00007fefb4ff830d in ?? () from /usr/lib64/libcuda.so.1\r\n#3  0x00007fefb4f6ca58 in ?? () from /usr/lib64/libcuda.so.1\r\n#4  0x00007feff7269e65 in start_thread () from /usr/lib64/libpthread.so.0\r\n#5  0x00007feff610c88d in clone () from /usr/lib64/libc.so.6\r\n\r\n```\r\n\r\nother eigen threads are just in pthread_cond_wait, 3ks", "comments": ["i delete one line code below, and every thing is being ok Now \ud83d\ude02\r\n\r\ndoes it will cause any critical failure ?\r\n\r\n``` \r\n// stream_executor_pimpl.cc:568\r\n\r\n bool StreamExecutor::SynchronizeAllActivity() {\r\n...\r\n  // This should all be quick and infallible work, so we can perform the\r\n  // synchronization even in the case of failure.\r\n  BlockOnThreadExecutor(background_threads_.get()); // <- delete this line, the deadlock is disappear\r\n...\r\n}\r\n\r\n```\r\n\r\ndose it possible that `n.Notify()` will be invoked before `n.WaitForNotification()`\r\n\r\n~~~\r\n 58 // Make sure the executor is done with its work; we know (because this isn't\r\n 59 // publicly visible) that all enqueued work is quick.\r\n 60 void BlockOnThreadExecutor(port::ThreadPool *executor) {\r\n 61   absl::Notification n;\r\n 62   executor->Schedule([&n]() { n.Notify(); });\r\n 63   n.WaitForNotification();\r\n 64 }\r\n\r\n~~~", "@zenyuhao,\r\n\r\nWe are checking to see if this is still an issue, Can you try updating your TF to latest stable version i.e `2.6.0` and let us know if the issue still persists? You can take a look at this [link](https://www.tensorflow.org/install/source#gpu) to know about tested build configurations.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41896\">No</a>\n"]}, {"number": 41895, "title": "ImportError: DLL load failed: The specified module could not be found", "body": "\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Source (branch r2.1)\r\n- Python version: 3.6\r\n- Bazel version :0.29.1\r\n- GCC/Compiler version :\r\n- Memory: 8GB\r\n\r\n**Describe the problem**\r\n Compiling Tensorflow with mkl on Windows ends up in \r\n\"ERROR: C:/users/dm212/documents/devi/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\". \r\nI followed the steps from **https://www.tensorflow.org/install/source_windows**. Also checked few solutions in similar issues, but didn't help.\r\n\r\n **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel --output_base=\"C:/Users/dm212/Documents/TF\"  build --config=mkl --config=opt  --config=v2                            --define=no_tensorflow_py_deps=true   //tensorflow/tools/pip_package:build_pip_package \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: C:/users/dm212/documents/devi/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dm212\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dm212\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"\\\\?\\C:\\Users\\dm212\\AppData\\Local\\Temp\\Bazel.runfiles_8o1_a1ke\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\dm212\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\dm212\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: C:/users/dm212/documents/devi/tensorflow/tensorflow/tools/pip_package/BUILD:114:1 Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)\r\nINFO: Elapsed time: 24724.095s, Critical Path: 20460.98s\r\nINFO: 5794 processes: 5794 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@manidevi \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "> @manidevi\r\n> \r\n> What is make/model of your cpu?\r\n> I suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\n> Make sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n> .Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n> \r\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\n> Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n> Thanks!\r\n\r\nI checked all the requirements you mentioned.\r\n- CPU has AVX and AVX2 support\r\n- Microsoft 2015-2019 redistributable has been installed\r\n- Python is of 64 bits\r\n`Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32` \r\n\r\nI get the error with the above setup only.", "@manidevi \r\n\r\nPlease, check the below comments \r\n\r\n1.You need to install the MSVC 2019 redistributable\r\n2.There is a library that is in a different location/not installed on your system that cannot be loaded.", "@ravikyram \r\n\r\n  Installed the lastest MSVC redistributable from [here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads). Still the ImportError exists.\r\n Is there a way to find the missing library or libraries required by tensorflow?  \r\n", "You may use https://www.dependencywalker.com/ to find missing dll files.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41895\">No</a>\n"]}, {"number": 41892, "title": "Tensorflow 2.3 doesn't recognize GeForce MX130 GPU", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.4\r\n- TensorFlow installed from: binary as per instructions on [tf website](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101)\r\n- TensorFlow version: 2.3\r\n- Python version: 3.6\r\n- Installed using : pip\r\n- CUDA/cuDNN version:  CUDA 10.1, cuDNN 7\r\n- GPU model and memory: GeForce MX130, 2GB\r\n\r\n\r\n**Problem Description:**\r\nTensorflow 2.3 doesn't recognize my GPU.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter installation I run the following:\r\n```\r\npython3 -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n```\r\nI get this error message:\r\n```\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n```\r\n\r\nOn Tensorflow 2.2, I don't get the error.\r\n", "comments": ["Tensorflow 2.3 also does not work with TPU: https://github.com/tensorflow/tensorflow/issues/41542", "@elishafer \r\n\r\nCan you please check if tensorflow sees your GPU by running below code.\r\n```\r\npip install tensorflow-gpu\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n```\r\nThanks!", "I get the same error on GeForce 840M, compute capability 5.0\r\nDowngrading to TF 2.2 solves it, but it would be nice to understand why 2.3 fails though", "CC @chsigg @sandeepngupta @manivaradarajan \r\n\r\nThis is likely because starting TF 2.3 we don't ship PTX for older compute capabilities to reduce the size of the TF pip package.\r\n\r\nThe simplest solution for you is to build the TF pip from source (this will take a while but it is a one-time cost) with the compute capabilities you need included.", "@ravikyram \r\nI ran what you requested:\r\n```\r\npip install tensorflow-gpu\r\n```\r\n\r\n```\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-08-02 09:48:16.786563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-08-02 09:48:16.838980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 09:48:16.839502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce MX130 computeCapability: 5.0\r\ncoreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n2020-08-02 09:48:16.839577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-08-02 09:48:16.844172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-08-02 09:48:16.847447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-08-02 09:48:16.849151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-08-02 09:48:16.854795: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-08-02 09:48:16.858097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-08-02 09:48:16.869744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-08-02 09:48:16.870083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 09:48:16.870725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-08-02 09:48:16.871176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\nNum GPUs Available:  1\r\n```\r\nBTW, maybe the error `Status: device kernel image is invalid` should be mentioned in the installation documentation because I found very little mention of this error anywhere and it took me several hours to figure out the cause and solution.", "> BTW, maybe the error `Status: device kernel image is invalid` should be mentioned in the installation documentation\r\n\r\nThat's a good idea.  @chsigg Can you please add this error message to https://www.tensorflow.org/install/gpu?", "Please see https://github.com/tensorflow/docs/pull/1642 which updates GPU.md.", "I'm closing this issue, since my question has been answered.\r\nI'm impressed by your speedy response and work.\r\nCheers!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41892\">No</a>\n", "> @ravikyram\r\n> I ran what you requested:\r\n> \r\n> ```\r\n> pip install tensorflow-gpu\r\n> ```\r\n> \r\n> ```\r\n> >>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n> 2020-08-02 09:48:16.786563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n> 2020-08-02 09:48:16.838980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-08-02 09:48:16.839502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\n> pciBusID: 0000:01:00.0 name: GeForce MX130 computeCapability: 5.0\r\n> coreClock: 1.189GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s\r\n> 2020-08-02 09:48:16.839577: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> 2020-08-02 09:48:16.844172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n> 2020-08-02 09:48:16.847447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n> 2020-08-02 09:48:16.849151: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n> 2020-08-02 09:48:16.854795: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n> 2020-08-02 09:48:16.858097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n> 2020-08-02 09:48:16.869744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n> 2020-08-02 09:48:16.870083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-08-02 09:48:16.870725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2020-08-02 09:48:16.871176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n> Num GPUs Available:  1\r\n> ```\r\n> \r\n> BTW, maybe the error `Status: device kernel image is invalid` should be mentioned in the installation documentation because I found very little mention of this error anywhere and it took me several hours to figure out the cause and solution.\r\n\r\nHey, I'm using the TensorFlow 2.2.0 from pip, and I get `Num GPUs Available:  0`. Did you have to do anything special to make it work?", "Please open a new issue instead of piggybacking on a closed issue."]}, {"number": 41891, "title": "CUDA_ERROR_ILLEGAL_ADDRESS in toy training example", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Inside virtual container:\r\nuname -a\r\nLinux 3558c7dc300b 4.15.0-74-generic #84-Ubuntu SMP Thu Dec 19 08:06:28 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: Python 3.6.8\r\n- CUDA/cuDNN version: \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n- GPU model and memory:\r\n\r\nNVIDIA DGX-2\r\n16x NVIDIA Tesla V100 (32GB)\r\n2x Intel Xeon Platinum 8168 2.7GHz 24C/48T\r\n1.5TB RAM\r\n30TB Internal NVME SSD\r\n\r\n**Describe the current behavior**\r\n\r\nCrashes. Same happens in [Colab](https://colab.research.google.com/drive/1akS92A3mbh_L-c978OsTHvTFAxnv22vV?usp=sharing) and in tf-nightly. \r\n\r\nSee crash report in colab:\r\n\r\n2020-07-30 08:06:02.330777: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should work, as it does for other values of the batch and repeat parameters.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\n\r\n# This is a simplified version of https://www.tensorflow.org/guide/distributed_training\r\n\r\nm_training_data = tf.constant([[1.1,1.]], dtype=tf.double)\r\ndataset = tf.data.Dataset.from_tensor_slices((m_training_data,)).repeat(8*1024).batch(1024)\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy()\r\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\r\n\r\ndef train_step(input):\r\n    A = tf.matmul(input[0], tf.linalg.matrix_transpose(input[0]))\r\n    return tf.linalg.det(A)\r\n\r\n@tf.function\r\ndef distributed_train_step(dist_inputs):\r\n    loss = mirrored_strategy.run(train_step, args=(dist_inputs,))\r\n    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, loss,\r\n                         axis=None)\r\n\r\nfor dist_inputs in dist_dataset:\r\n    print(distributed_train_step(dist_inputs))\r\n\r\n```\r\n\r\n**Other info / logs** \r\n[crash_1002.txt](https://github.com/tensorflow/tensorflow/files/5000057/crash_1002.txt)\r\n", "comments": ["On running the Colab notebook linked, I was able to reproduce the issue with TF v2.2, TF v2.3 and TF-nightly. \r\n\r\nCode runs without any issues for `repeat(8*1024).batch(1024)`, but when changed to `repeat(256).batch(256)` session crashes. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5b23114e3bf0b6840fe601351026f527/41891-tf-nightly.ipynb#scrollTo=LbLfQOflzywD). Thanks!", "Changing labels as I don't think this is a dist-strat specific issue. The crash also occurs with Default Strategy.\r\n", "Hi @fachu000, can you clarify where you are seeing the crash? Is it for the `repeat(8*1024).batch(1024)` example or the `repeat(256).batch(256)` example? Like @amahendrakar I seem to get the crash in colab only with `repeat(256).batch(256)`\r\n\r\nI can also see the crash in colab with the following reduced code on a GPU runtime\r\n```\r\nm_training_data = tf.constant([[1,1]], dtype=tf.double)\r\ndataset = tf.data.Dataset.from_tensor_slices((m_training_data,)).repeat(256).batch(256)\r\n\r\n\r\nfor input in dataset:\r\n    A = tf.matmul(input[0], tf.linalg.matrix_transpose(input[0]))\r\n    det = tf.linalg.det(A)\r\n```", "Hi @nikitamaia, \r\n\r\nColab crashes with .repeat(256).batch(256)\r\n\r\nIn the server I use (system described above), it works with .repeat(256).batch(256) but crashes with .repeat(8*1024).batch(1024)\r\n\r\nOne difference may be that I use 4 GPUs whereas Colab uses 1. \r\n\r\ndaniel", "Thanks for the clarification. I'm seeing different behavior based on where I run the code and whether I use MirroredStrategy or the Default strategy.\r\nI am able to run `.repeat(256).batch(256)` on GCP with 2 GPUs with MirroredStrategy, but `.repeat(8*1024).batch(1024)` crashes. I see the reverse behavior with 1 GPU in Colab.\r\n\r\nI also tried running on GPU with 1 GPU and both cases worked.\r\nHowever, with 2 GPUs, and using the Default strategy I get a crash on the  `.repeat(256).batch(256)` but `.repeat(8*1024).batch(1024)`  works.", "@rmlarsen here cuSolverDN is failing with `CUSOLVER_STATUS_EXECUTION_FAILED`:\r\n\r\n```\r\n...\r\nW tensorflow/core/framework/op_kernel.cc:1773] OP_REQUIRES failed at determinant_op.cc:228 : Internal: tensorflow/core/util/cuda_solvers.cc:466: cuSolverDN call failed with status =6\r\nW tensorflow/core/framework/op_kernel.cc:1773] OP_REQUIRES failed at determinant_op.cc:228 : Internal: tensorflow/core/util/cuda_solvers.cc:466: cuSolverDN call failed with status =6\r\nE tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:725] failed to record completion event; therefore, failed to create inter-stream dependency\r\nI tensorflow/stream_executor/stream.cc:4952] [stream=0x402c0130,impl=0x402be810] did not memcpy host-to-device; source: 0x7f5089649fc0\r\nE tensorflow/stream_executor/stream.cc:338] Error recording event in stream: Error recording CUDA event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\nE tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n```\r\n\r\nAny idea how to debug this further?  Is it a user error?  If so, probably TF should be returning a friendlier error message.", "I tried the code in colab with TF v2.5 & TF v2.6.0-dev20210606 .I didn't face the issue reported  ,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/063363b65702fec5dc1e1e7e5c3ce8a7/untitled282.ipynb)..Thanks!", "Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41891\">No</a>\n", "Is this fix in 2.5.0 or 2.6.0?"]}, {"number": 41890, "title": "the info need me to tell you", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution win10 18363:\r\n- TensorFlow installed from conda:\r\n- TensorFlow version 2.3.0-rc0:\r\n- Python 3.7:\r\n- CUDA/cuDNN 10.2:\r\n- 1080  16G:\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nEpoch 1/5\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000013933B3A798> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000013933B3A798> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'arguments' object has no attribute 'posonlyargs'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n235/235 [==============================] - 0s 1ms/step - loss: 0.7901 - accuracy: 0.7434\r\nEpoch 2/5\r\n235/235 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.8110\r\nEpoch 3/5\r\n235/235 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.8239\r\nEpoch 4/5\r\n235/235 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.8309\r\nEpoch 5/5\r\n235/235 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.8352\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\r\nx_train = x_train / 255.0\r\nx_test = x_test / 255.0\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer=tf.keras.optimizers.SGD(0.1),\r\n              loss = 'sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train,y_train,epochs=5,batch_size=256)\r\n\r\nand now it`s happen\r\n\r\n\r\n", "comments": ["@erquren \r\nI ran the shared code and do not face any warnings,please find [gist here for 2.2](https://colab.research.google.com/gist/Saduf2019/89f7c21c1ee3b3fcb6f45e2c1c7b3cf8/untitled305.ipynb) and [2.3 as reported](https://colab.research.google.com/gist/Saduf2019/580d82953532a58f3e8db7cee73c2315/untitled305.ipynb).\r\n\r\nYou can safely ignore the warning log as its intended to debug logging in AutoGraph issues. Thanks !\r\n\r\nYou may refer to resolved issues with same error:\r\n#37144 [link](https://github.com/tensorflow/tensorflow/issues/38947#issuecomment-620707630) [link1](https://github.com/tensorflow/tensorflow/issues/41120#issuecomment-654268902) [link2](https://github.com/tensorflow/tensorflow/issues/32377#issuecomment-529893477)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41890\">No</a>\n", "This looks like a gist version mismatch. If the issue still reproduces in tf-nightly, please re-open the bug. Thanks!"]}, {"number": 41889, "title": "Fix environment for protobuf compilation", "body": "The invocation environment for protobuf compilation is ignoring environment variables like LD_LIBRARY_PATH which were used when building `protoc`. This leads to failures due to e.g. mismatches in the `libstdc++.so` versions.\r\n\r\n\r\nThis fixes #41857 by using the patch from upstream https://github.com/grpc/grpc/pull/23664", "comments": []}, {"number": 41888, "title": "Resource exhausted: OOM when allocating tensor with shape[256] ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution : ubuntu 18.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  Jetson Nano\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tensorflow==1.15.2+nv20.6\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.2 / cuDNN 8.00\r\n- GPU model and memory: NVIDIA Tegra X1 (nvgpu)/integrated and 4GB memory\r\nKeras: 2.0.5\r\n\r\n**Describe the current behavior**\r\n**Error :**\r\n\r\n**tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at assign_op.h:117 : Resource exhausted: OOM when allocating tensor with shape[256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**\r\n\r\n![image](https://user-images.githubusercontent.com/2168986/88894424-7b46f880-d265-11ea-98bb-55ea40271009.png)\r\n\r\n\r\n**Source code**\r\n\r\n![image](https://user-images.githubusercontent.com/2168986/88894492-9154b900-d265-11ea-8799-58202d7338b0.png)\r\n\r\n**Refer the Logs, generated while start executing**\r\n\r\n**\u201cadding visible gpu devices: 0\u201d**\r\n\r\n**is GPU not allocated for the job?**\r\n![image](https://user-images.githubusercontent.com/2168986/88894707-ee506f00-d265-11ea-9cac-b9d1d29c13bc.png)\r\n\r\n\r\n", "comments": ["@suresh-s \r\n\r\nPlease, share colab link or simple standalone code with supporting files with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "ravikyram,\r\nI am testing the following github project.   Could you use this code repository to check? \r\nhttps://github.com/foamliu/Car-Recognition", "@suresh-s \r\nLimiting gpu memory can help resolve OOM error.\r\nSee https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth.\r\nThanks!", "I have improved the performance by specifying configs per_process_gpu_memory_fraction =0.2, allow_growth =True and visible_device_list=\"0\".  But at one point in time, Swap memory reaches \"100%\" and System freezed.   Could you suggest to resolve this issue?", "@suresh-s \r\n\r\nCan you please try setting hard limit on the GPU as shown in the second method under `Limiting gpu memory growth` section\r\nhttps://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\nThanks!", "I have tried hard limit on the GPU. I have created swap file of size 6 GB. But again i got Out of memory exception on **Jetson Nano**.   I am using [Stanford Cars Dataset ](https://ai.stanford.edu/~jkrause/cars/car_dataset.html) \r\n\r\nthis is the source code https://github.com/suresh-s/Car-Recognition-Phd/blob/master/train.py\r\n\r\nPlease help me to solve this issue\r\n", "@suresh-s First of all please use CUDA 10.0 with cuDNN 7.4 as shown in the [doc](https://www.tensorflow.org/install/source#gpu).\r\nAlso a few recommendations:\r\n1. try to run the same script on a bigger machine with more memory and see if it fixes it.\r\n2. try reducing the batch size (although the batchsize you are using here is only 2) and see if it fixes it.\r\n\r\nI think its a memory issue from your end, try running your code on Colab gpu and let me know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41887, "title": "module 'tensorflow.python.framework.ops' has no attribute 'register_tensor_conversion_function'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@wkr-ux,\r\nPlease fill in the template and clearly describe the issue you are facing.\r\n\r\nAlso in in order to reproduce the issue reported here, please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41886, "title": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "With: \r\n\r\n#### load MNIST dataset\r\nmnist = tf.keras.datasets.mnist\r\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n\r\nprint(X_train.shape, y_train.shape)\r\nprint(X_train.min(), X_train.max(), y_train.min(), y_train.max())\r\n\r\nX_train, X_test = X_train/255., X_test/255.\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n    tf.keras.layers.Dense(100, activation='relu'),\r\n    tf.keras.layers.Dense(10),\r\n    ])\r\n\r\npredictions = model(X_train.astype('float32')[:1]).numpy()\r\nprint(predictions)\r\n\r\ny_pred = tf.nn.softmax(predictions).numpy()\r\nprint(y_pred)\r\nprint(y_pred.sum())\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(X_train, y_train, epochs=15)\r\n\r\nmodel.evaluate(X_test, y_test, verbose=2)\r\n\r\nmodel = tf.keras.models.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\nmodel.add(tf.keras.layers.MaxPool2D((2, 2)))\r\nmodel.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\r\nmodel.add(tf.keras.layers.MaxPooling2D((2, 2)))\r\nmodel.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\r\n\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\r\nmodel.add(tf.keras.layers.Dense(10))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n             metrics=['accuracy'])\r\n             \r\nhistory = model.fit(X_train[:, :, :, np.newaxis], y_train, epochs=10,\r\n                    validation_data=(X_test[:, :, :, np.newaxis], y_test))\r\n\r\n###################\r\n**I get:**\r\n###################\r\n\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-20-0e520e098f8a> in <module>\r\n      4 \r\n      5              # fit\r\n----> 6 history = model.fit(X_train[:, :, :, np.newaxis], y_train, epochs=10,\r\n      7                     validation_data=(X_test[:, :, :, np.newaxis], y_test))\r\n      8 \r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    642         # Lifting succeeded, so variables are initialized and we can run the\r\n    643         # stateless function.\r\n--> 644         return self._stateless_fn(*args, **kwds)\r\n    645     else:\r\n    646       canon_args, canon_kwds = \\\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1659       `args` and `kwargs`.\r\n   1660     \"\"\"\r\n-> 1661     return self._call_flat(\r\n   1662         (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1743         and executing_eagerly):\r\n   1744       # No tape is watching; skip to running the function.\r\n-> 1745       return self._build_call_outputs(self._inference_function.call(\r\n   1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    591       with _InterpolateFunctionError(self):\r\n    592         if cancellation_manager is None:\r\n--> 593           outputs = execute.execute(\r\n    594               str(self.signature.name),\r\n    595               num_outputs=self._num_outputs,\r\n\r\n~/miniconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     57   try:\r\n     58     ctx.ensure_initialized()\r\n---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node sequential_1/conv2d/Conv2D (defined at <ipython-input-20-0e520e098f8a>:6) ]] [Op:__inference_train_function_59155]\r\n\r\nFunction call stack:\r\ntrain_function`\r\n\r\n####################\r\n**My specs:**\r\n####################\r\n\r\nNAME=\"elementary OS\"\r\nVERSION=\"5.1.6 Hera\"\r\nID=elementary\r\nID_LIKE=ubuntu\r\nPRETTY_NAME=\"elementary OS 5.1.6 Hera\"\r\nLOGO=distributor-logo\r\nVERSION_ID=\"5.1.6\"\r\nHOME_URL=\"https://elementary.io/\"\r\nSUPPORT_URL=\"https://elementary.io/support\"\r\nBUG_REPORT_URL=\"https://github.com/elementary/os/issues/new\"\r\nPRIVACY_POLICY_URL=\"https://elementary.io/privacy-policy\"\r\nVERSION_CODENAME=hera\r\nUBUNTU_CODENAME=bionic\r\n\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 18.04.4 LTS\r\nRelease:\t18.04\r\nCodename:\tbionic\r\n\r\nGPU drivers (nvidia-smi)\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 207...  Off  | 00000000:09:00.0  On |                  N/A |\r\n|  0%   51C    P8    25W / 215W |   7915MiB /  7979MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n \r\n\r\n(All installed with conda install)\r\n**Tensorflow-gpu version 2.2.0**\r\ncudnn                     7.6.5                cuda10.1_0  \r\ncudatoolkit               10.1.243             h6bb024c_0  \r\n                                                                             \r\nI tried installing driver version 435 and it worked but my screen stops being recognized, therefore I cannot readjust the resolution\r\n\r\n\r\n\r\n", "comments": ["@nicgar20 \r\nI ran the code shared and do not face any issues, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/13d41c917309a9d495a6fefbaad57f92/untitled302.ipynb).\r\n\r\nCan you please try setting allow_growth option at the top of your code:\r\n\r\n```\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\n\r\n# your code \r\n```\r\n\r\nCan you please refer to below resolved issues with same error[as per which this error occurs due to mismatch of cudaa/cudnn with your tensorflow version]:\r\n#41146 [link](https://github.com/tensorflow/tensorflow/issues/36025#issuecomment-628145158) [link1](https://stackoverflow.com/questions/53698035/failed-to-get-convolution-algorithm-this-is-probably-because-cudnn-failed-to-in)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41886\">No</a>\n"]}, {"number": 41885, "title": "Peraid131-White", "body": "\r\n![prs-p-1](https://user-images.githubusercontent.com/68693282/88882575-8559fe80-d247-11ea-95a6-e46664fcf92d.jpg)\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 41884, "title": "K-fold Cross Validation error in Tensorflow sklearn", "body": "I am using following code for semantic segmentation (image, and mask), this code was working fine with simple training and testing, but when i tried to implement k-fold cross validation. this code has shown error, please check my code and let me know what is wrong, and how i can fix this!\r\n\r\n````\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\r\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom sklearn.model_selection import KFold\r\nimport numpy as np\r\n\r\ndef tf_dataset(x, y, batch=8):\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.map(tf_parse)\r\n    dataset = dataset.batch(batch)\r\n    dataset = dataset.repeat()\r\n    return dataset\r\n\r\ntrain_dataset = tf_dataset(train_x, train_y, batch=batch_size)\r\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=batch_size)\r\n\r\nnum_folds = 10\r\n\r\n# Define per-fold score containers\r\nacc_per_fold = []\r\nloss_per_fold = []\r\n\r\n# Define the K-fold Cross Validator\r\nkfold = KFold(n_splits=num_folds, shuffle=True)\r\n\r\n# K-fold Cross Validation model evaluation\r\nfold_no = 1\r\n\r\nfor train, valid in kfold.split(train_dataset, valid_dataset):\r\n  \r\n  optimizer = tf.keras.optimizers.Adam(lr)\r\n  metrics = ['accuracy']\r\n  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\r\n\r\n  callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\r\n              EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)]\r\n\r\n  train_steps = len(train_x) // batch_size\r\n  valid_steps = len(valid_x) // batch_size\r\n\r\n  if len(train_x) % batch_size != 0:\r\n    train_steps += 1\r\n  if len(valid_x) % batch_size != 0:\r\n    valid_steps += 1\r\n\r\n\r\n  # Generate a print\r\n  print('------------------------------------------------------------------------')\r\n  print(f'Training for fold {fold_no} ...')\r\n\r\n  model.fit(train_dataset[train], valid_dataset[train],\r\n            epochs=epochs,\r\n            steps_per_epoch=train_steps,\r\n            validation_steps=valid_steps,\r\n            callbacks=callbacks)\r\n\r\n  # Generate generalization metrics\r\n  scores = model.evaluate(train_dataset[valid], valid_dataset[valid], verbose=0)\r\n  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\r\n  acc_per_fold.append(scores[1] * 100)\r\n  loss_per_fold.append(scores[0])\r\n\r\n  # Increase fold number\r\n  fold_no = fold_no + 1\r\n\r\n# == Provide average scores ==\r\nprint('------------------------------------------------------------------------')\r\nprint('Score per fold')\r\nfor i in range(0, len(acc_per_fold)):\r\n  print('------------------------------------------------------------------------')\r\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\r\nprint('------------------------------------------------------------------------')\r\nprint('Average scores for all folds:')\r\nprint(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\r\nprint(f'> Loss: {np.mean(loss_per_fold)}')\r\nprint('------------------------------------------------------------------------')\r\n```\r\n\r\n**Error:**\r\n\r\n> \r\n> --------------------------------------------------------------------------- TypeError Traceback (most recent call last) in () 12 # K-fold Cross Validation model evaluation 13 fold_no = 1 ---> 14 for train, valid in kfold.split(train_dataset, valid_dataset): 15 16 optimizer = tf.keras.optimizers.Adam(lr)\r\n> \r\n> 4 frames /usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py in _num_samples(x) 150 if len(x.shape) == 0: 151 raise TypeError(\"Singleton array %r cannot be considered\" --> 152 \" a valid collection.\" % x) 153 # Check that shape is returning an integer or default to len 154 # Dask dataframes may not return numeric shape[0] value\r\n> \r\n> TypeError: Singleton array array(<RepeatDataset shapes: ((None, 224, 224, 3), (None, 224, 224, 1)), types: (tf.float64, tf.float64)>, dtype=object) cannot be considered a valid collection.", "comments": ["@ahmedeqbal,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 41883, "title": "Import TensorFlow Failed", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 x64\r\n- TensorFlow installed from (source or binary): anaconda prompt\r\n- TensorFlow version: tensorflow2.3.0 only cpu\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: from anaconda prompt- [pip install tensorflow]\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: I dont have a GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm new to tensorflow. Sorry if it's a dummy question.\r\nI downloaded the tensorflow from anaconda prompt as I listed above. Then I only run \"import tensorflow as tf\" as the turtorial said but it goes wrong as below. I check the path and there's no such a dll file. Can anyone help me?\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nrunfile('C:/Users/13926/zzz/untitled1.py', wdir='C:/Users/13926/zzz')\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\n\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\13926\\zzz\\untitled1.py\", line 8, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\13926\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@liyy0 \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/36683#issuecomment-585097726) of resolved issue and let us know.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41883\">No</a>\n"]}, {"number": 41882, "title": "When i run code by python2 but it read python3 library files erro. What should i do?", "body": "When i run code by python2 but it read python3 library files all time.\r\nAs shown below.\r\n![image](https://user-images.githubusercontent.com/31792429/88876372-bb9e7a80-d255-11ea-8ad3-ffb226e50c26.png)\r\n\r\n\r\n", "comments": ["@Dolphinzzx \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]