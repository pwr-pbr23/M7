[{"number": 1360, "title": "Tensorflow Cifar-10 evaluation error: Enqueue operation was cancelled", "body": "Operating System: Mac Os X 10.9.5\n\nCurrent tensorflow version 0.7.1\n\nI am trying to get the Cifar-10 tutorial running on my own data set (so using other images, but the same code). Training works fine. However, the evaluation seems to have a problem. Whenever I run the evaluation, I get the following message:\n\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe504a20 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe403eb0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe206b10 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbe16130 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e6bd80 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbcca1e0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbbb5e30 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8daee90 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe304d50 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fbcb9570 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e01660 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fba2a1a0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e357a0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe207e90 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fe2089b0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6fa6bfe20 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomp                                                                                                                     onents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/ta                                                                                                                     sk:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div, Cast)]]\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fc6f8e689f0 Compute status                                                                                                                     : Cancelled: Enqueue operation was cancelled\n         [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tc                                                                                                                     omponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/c                                                                                                                     pu:0\"](input_producer, input_producer/RandomShuffle)]]\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\n\nRight before this message, if I let the code run long enough, I get the output for a precision (precision @ 1 = xxx). However, the message still irritates me, I looks like an error message...\n\nI've googled this error, and so far I've only found some posts talking about that one should first initialize placeholders before starting the queuerunners. However, since there are no placeholders used anywhere in the code before, and I am very new to tensorflow/machine learning in general, I have no idea where one would put these placeholders, or what they would be. \n", "comments": ["Duplicate of #1341, essentially.\n\nI think this is superfluous logging.  I tried to get rid of this but apparently these logs are useful for debugging deadlocks. @jmchen-g \n\nMaybe we should just write to log files instead of spamming users.\n"]}, {"number": 1359, "title": " run time error:could not find cudnnConvolutionBackwardData_v2 in cudnn DSO", "body": "Operating System:ubuntu 14.04LTS\npip install 0.6 and  sudo pip install --upgrade .. to 0.7\nThe output from python -c \"import tensorflow; print(tensorflow.**version**)\" is:\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.7.1\n\nbelow is errors when running cifar10_train.py:\n tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x13047a0000 extends to 0x15a8b24400\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:220] could not find cudnnConvolutionBackwardFilter_v2 in cudnn DSO; dlerror: /usr/local/lib/libcudnn.so: undefined symbol: cudnnConvolutionBackwardFilter_v2\n", "comments": ["What version of cudnn do you have?  The pip package at 0.7.1 is built assuming you have cudnn r4.  You have to build from sources if you want to support earlier versions.\n", "Hi,\nI have cuda installed folders such as: cuda, cuda-7.0, cuda-7.5, all they under this path /usr/local/. I copy cudnn r2, r3, r4 in all folders:cuda, cuda-7.0, cuda-7.5, but this did not work? how to check my tensorflow actually use which version of cudnn and cuda?\n", "Hi, \nfinally sovled, cudnn lib64 files placed in /usr/local/cuda/lib64 according to the install tutorial:\ntar xvzf cudnn-6.5-linux-x64-v2.tgz\nsudo cp cudnn-6.5-linux-x64-v2/cudnn.h /usr/local/cuda/include\nsudo cp cudnn-6.5-linux-x64-v2/libcudnn\\* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/lib64/libcudnn*\nbut actually my program search in /usr/local/lib, thank you very much\n", "In addtion i use cudnn-7.0-linux-x64-v4.0-rc.tgz this cudnn, so tutorial above cudnn-6.5\\* will be replaced by cudnn-7.0*, hope this maybe helps someone has the same problem\n", "@lfwin  I still have the same error after placing cudnn lib 64 files in usr/local/cuda/lib64. what should i do? \n", "@kibtes I encountered the same problem today and after replacing the cuddn v2 stuff with v3 and it works. Try to make sure that the cuda dir is in your loading path.\n", "@kibtes just correctly provide path to cudnn libraries. For example in .bashrc file put line: \n`export LD_LIBRARY_PATH=\"/path_to_your_cudnn/cuda/lib64\":$LD_LIBRARY_PATH`\nThen logout and login. Check path with command: `env | grep cundd`.\n", "I met this problem before. Error log is \"F tensorflow/stream_executor/cuda/cuda_dnn.cc:220] could not find cudnnConvolutionBackwardData_v2 in cudnn DSO; dlerror: /usr/local/cuda-7.0/lib64/libcudnn.so: undefined symbol: cudnnConvolutionBackwardData_v2\".\nSo I tried to update cuDNN libraries to the latest version. So I download cudnn-7.0-linux-x64-v4.0-prod.tgz then export LD_LIBRARY_PATH, it works. Thank you!\n", "It sounds like this issue is caused by system irregularities which I'm not sure we'll be able to work around on our end, so closing for now.\n"]}, {"number": 1358, "title": "Mean of cross entropy vs sum in summaries tutorial", "body": "The `mnist_with_summaries.py` and `mnist_softmax.py` tutorials compute the sum of the cross_entropy over the data points in the batch. This seems to deviate from the more common approach of taking the mean, such as in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py#L99 \nThe current form works as a loss function, but might confuse newcomers who are figuring out how gradients are aggregated in TensorFlow (as experienced by me, see http://stackoverflow.com/questions/35731506/unaggregated-gradients-gradients-per-example-in-tensorflow :-))\n", "comments": ["Can one of the admins verify this patch?\n", "Perhaps one reason that reduce_sum was used is that reduce_mean is not yet implemented on GPU.\n", "Fair enough. Do you think it's a good idea to make a note about this in the example code or the [documentation](https://www.tensorflow.org/versions/r0.7/api_docs/python/math_ops.html#reduce_mean) to clear this up? Looking at some other frameworks, taking a mean over cross entropy seems to be customary[1,2], so I can imagine this could potentially confuse others as well. \n- [1] https://github.com/Lasagne/Lasagne/blob/master/README.rst\n- [2] https://github.com/mila-udem/blocks/blob/58994253579d30fb63a2cfa53499a68b1237c3c0/blocks/bricks/cost.py#L59\n", "Just pushed b3dfff2a23b435f14cca7377f8e0e5ad7583b45e which added reduce_mean support for GPU, so that will no longer be the reason.  @vincentvanhoucke does this look good?\n", "Does this converge to the same result though? Our learning rate is not scale invariant.\n", "@colah: friendly ping?\n", "@basveeling I agree that switching from a sum to a mean across datapoints is better. Can you:\n- Make the corresponding changes to the tutorial text https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/tutorials/mnist\n- Confirm that the model converges to the same result after making this change. (You will probably need to multiply the learning rate by the batch size of 100.)\n", "@Sohl-Dickstein: done. I determined the new learning rate for the softmax tutorial by cross validation:\nhttps://gist.github.com/basveeling/0b43d5d9fe533e9aaf710d13356931f4\nI haven't validated the results for the convolutional mnist network in the pros tutorial due to lack of gpu, but someone can verify this with https://gist.github.com/basveeling/80b11bf657891c0c8f253c899a4be99d .\n", "Thank you for the submission @basveeling.\n\nSee the inline requests for minor text changes. Also, both tutorials reference a performance of the naive model of 91%. Thanks to your cross validation, this is now 92% -- so you should make that change too.\n\nOtherwise it looks good. Thanks again.\n", "@Sohl-Dickstein Thanks, should be good now. I left the performance at 91% in the docs so users wouldn't feel bad when they incidentally get a lower score :-), but updated it to 92 now. \n", "Jenkins, test this please.\n", "Thanks!\n"]}, {"number": 1357, "title": "Retraining Example with Notebook", "body": "The code from the retraining demo morphed into a IPython Notebook to make it easier to use interactively.\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLA has been signed for kmader\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@kmader: I like this, but right now we don't have a good way to test ipython notebooks, which means it's very hard to keep updated if we make changes.  The udacity notebooks already often break, and I'm worried about adding more notebooks at this point, but I could be convinced otherwise.  @martinwicke \n", "@vrv not sure how @googlebot works but we have jenkins setup to [extract the python code](https://ipython.org/ipython-doc/1/interactive/nbconvert.html) from the notebooks and make sure it executes without throwing any errors. Alternatively it is fairly easy to construct the notebooks from annotated python code, so maybe that is a more viable option. The biggest issue there is the `FLAGS` in TF needs to be reworked.\n\n``` bash\nipython nbconvert --to python notebook.ipynb notebook.py\n```\n", "Flags is just a thin wrapped around argparse, in what way would it have to\nbe reworked?\n\nI'm quite excited by the idea of tested ipython notebooks, it's been\nsomething we've been thinking about for our tutorials (although we've gone\nwith regular tests for now).\nOn Sat, Mar 5, 2016 at 12:00 Kevin Mader notifications@github.com wrote:\n\n> @vrv https://github.com/vrv not sure how @googlebot\n> https://github.com/googlebot works but we have jenkins setup to extract\n> the python code\n> https://ipython.org/ipython-doc/1/interactive/nbconvert.html from the\n> notebooks and make sure it executes without throwing any errors.\n> Alternatively it is fairly easy to construct the notebooks from annotated\n> python code, so maybe that is a more viable option. The biggest issue there\n> is the FLAGS in TF needs to be reworked.\n> \n> ipython nbconvert --to python notebook.ipynb notebook.py\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1357#issuecomment-192720472\n> .\n", "friendly ping -- what needs to be reworked?\n\nAlso, @craigcitro for general advice on migrating our tutorials to testable notebooks rather than having copies that will inevitably diverge.\n", "Is this PR still active? \n", "We want to move our tutorials to notebooks, but I guess we're not ready now.  Thanks for this PR -- we'll try to get to this for all of our tutorials eventually.\n"]}, {"number": 1356, "title": "Compile error ", "body": "MacOS, master branch pull minutes ago.\nI intended to try this `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\n### Bazel version:\n\nBuild label: 0.1.5\nBuild target: bazel-out/local_darwin-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 21 12:53:53 2016 (1453380833)\nBuild timestamp: 1453380833\nBuild timestamp as int: 1453380833\nI've also tried on 0.1.4, same error as below.\n### gcc --version:\n\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/c++/4.2.1\nApple LLVM version 7.0.2 (clang-700.1.81)\nTarget: x86_64-apple-darwin15.3.0\nThread model: posix\n### Logs:\n\nERROR: /Users/elin/gitHub_e-lin/tensorflow/tensorflow/contrib/linear_optimizer/BUILD:14:1: Linking of rule '//tensorflow/contrib/linear_optimizer:python/ops/_sdca_ops.so' failed: osx_gcc_wrapper.sh failed: error executing command \n  (cd /private/var/tmp/_bazel_elin/c89d25ef263422ce5293ac2a19ea0c36/tensorflow && \\\n  exec env - \\\n  external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh -shared -o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/python/ops/_sdca_ops.so -Wl,-all_load bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/kernels/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/ops/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/core/libframework_internal.pic.lo bazel-out/local_darwin-opt/bin/tensorflow/core/liblib_internal.pic.a bazel-out/local_darwin-opt/bin/external/jpeg_archive/libjpeg.pic.a bazel-out/local_darwin-opt/bin/external/png_archive/libpng.pic.a bazel-out/local_darwin-opt/bin/external/re2/libre2.pic.a bazel-out/local_darwin-opt/bin/tensorflow/core/libprotos_all_cc.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf_lite.pic.a -Wl,-noall_load -Wl,-Bsymbolic -lm -ldl -lm -ldl -lz -pthread -lpthread -lstdc++ -undefined dynamic_lookup -no-canonical-prefixes): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: osx_gcc_wrapper.sh failed: error executing command \n  (cd /private/var/tmp/_bazel_elin/c89d25ef263422ce5293ac2a19ea0c36/tensorflow && \\\n  exec env - \\\n  external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh -shared -o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/python/ops/_sdca_ops.so -Wl,-all_load bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/kernels/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/_objs/python/ops/_sdca_ops.so/tensorflow/contrib/linear_optimizer/ops/sdca_ops.pic.o bazel-out/local_darwin-opt/bin/tensorflow/core/libframework_internal.pic.lo bazel-out/local_darwin-opt/bin/tensorflow/core/liblib_internal.pic.a bazel-out/local_darwin-opt/bin/external/jpeg_archive/libjpeg.pic.a bazel-out/local_darwin-opt/bin/external/png_archive/libpng.pic.a bazel-out/local_darwin-opt/bin/external/re2/libre2.pic.a bazel-out/local_darwin-opt/bin/tensorflow/core/libprotos_all_cc.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf.pic.a bazel-out/local_darwin-opt/bin/google/protobuf/libprotobuf_lite.pic.a -Wl,-noall_load -Wl,-Bsymbolic -lm -ldl -lm -ldl -lz -pthread -lpthread -lstdc++ -undefined dynamic_lookup -no-canonical-prefixes): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nld: unknown option: -Bsymbolic\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 3.198s, Critical Path: 2.04s\n", "comments": ["fix as: #1352 \n", "@blueegg Thanks! This works for me! I close this case.\n"]}, {"number": 1355, "title": "Resource exhausted", "body": "This error happens when training several models in a for loop:\n\n`W tensorflow/core/common_runtime/executor.cc:1076] 0x18ef02bb0 W tensorflow/core/common_runtime/executor.cc:1076] 0x18ef02bb0 Compute status: Resource exhausted: OOM when allocating tensor with shape\n     [[Node: sub_2518 = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_2518/x, Variable_1955/read)]]\nTraceback (most recent call last):\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py\", line 398, in <module>\n    fited = model.fit(Xtrain, ytrain, nb_epoch=n_epochs, batch_size=batch_size, show_accuracy=True, shuffle=True, validation_split=0.35)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 581, in fit\n    shuffle=shuffle, metrics=metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 239, in _fit\n    outs = f(ins_batch)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 336, in __call__\n    updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 3200 } dim { size: 2000 }\n     [[Node: mul_4305 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_1954/read, Variable_1956/read)]]\n     [[Node: add_3103/_9711 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_423_add_3103\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'mul_4305', defined at:\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py\", line 387, in <module>\n    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 440, in compile\n    train_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/optimizers.py\", line 262, in get_updates\n    m_t = (self.beta_1 * m) + (1 - self.beta_1) * g\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 452, in <lambda>\n    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 467, in _RunOp\n    return getattr(ops.Tensor, operator)(a._AsTensor(), b)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 426, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 728, in mul\n    return _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n: OOM when allocating tensor with shape\n     [[Node: sub_2518 = Sub[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](sub_2518/x, Variable_1955/read)]]\nTraceback (most recent call last):\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py\", line 398, in <module>\n    fited = model.fit(Xtrain, ytrain, nb_epoch=n_epochs, batch_size=batch_size, show_accuracy=True, shuffle=True, validation_split=0.35)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 581, in fit\n    shuffle=shuffle, metrics=metrics)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 239, in _fit\n    outs = f(ins_batch)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 336, in __call__\n    updated = session.run(self.outputs + self.updates, feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shapedim { size: 3200 } dim { size: 2000 }\n     [[Node: mul_4305 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable_1954/read, Variable_1956/read)]]\n     [[Node: add_3103/_9711 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_423_add_3103\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'mul_4305', defined at:\n  File \"/media/konet/01D15611945D72C0/Pycharm_ubuntu/deep_neural_network_binary/dnn_127_models_binary.py\", line 387, in <module>\n    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n  File \"/usr/local/lib/python2.7/dist-packages/keras/models.py\", line 440, in compile\n    train_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/keras/optimizers.py\", line 262, in get_updates\n    m_t = (self.beta_1 * m) + (1 - self.beta_1) * g\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 452, in <lambda>\n    setattr(Variable, operator, lambda a, b: Variable._RunOp(operator, a, b))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 467, in _RunOp\n    return getattr(ops.Tensor, operator)(a._AsTensor(), b)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 426, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 728, in mul\n    return _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n`\n\nAny advice on how to solve this?\n\nThank you\n", "comments": ["It sounds like you are running out of memory\n", "Yes, this sounds like an out-of-memory issue.  Try reducing your model size / number of concurrent active models.  Note that we will continue to try to improve our GPU memory utilization, so the situation may get better over time.\n", "Hi, there is no number of concurrent models. \nWhat I do is right after training/testing one model, I start training/testing other in a for loop.\nThere is a limited number of models I can try consecutively due to this error.\n", "I've the same issue.\nI'm using Keras with Tensorflow as backend, if I do KFold cross validation I run out of memor. Same model and same number of KFolds runs with no issues on Theano backend.\nIt seems like the GPU memory utilization needs to be improved\n", "@vrv It seems that calling multiple fit inside a loop has some cumulative alloc without reuse or free GPU memory resources.\n", "I think we need a small representative example to reproduce the problem -- we shouldn't have any cumulative allocations since we reference count everything, and the only state that persists across run calls (in the non-partial run case) is stored in Variables\n", "We have tested again with TF master but we got the same issue. \n@vrv Do you think that make sense to leak check with cuda tools?\nIf not we need to deep investigate how keras pilot tensorflow in the backend with this cycles.\n", "Maybe worth trying cuda tools but:\n\n1) We should dump out of memory information to LOG(INFO) when we OOM: https://github.com/tensorflow/tensorflow/blob/cc9bfbf8ef4a3dea6514ad939d238f7442188247/tensorflow/core/common_runtime/bfc_allocator.cc#L639\n\n2) Try setting https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/core/framework/log_memory.cc#L25 to true and see if you get more information ?\n\nI haven't looked at Keras code, perhaps it is using a single session and adding/initializing new variables to the session for each new model?  Variables are persisted through the lifetime of a session, so it would be wise to use a different session for each \"model\", if creating that model requires creating variables.\n", "@vrv Mhhh.. Quite sure that we are incurring in https://github.com/fchollet/keras/issues/2102\n", "Drastically reducing batch-size to 5-10 solved the problem in my case.", "Yes, reducing batch size works.", "reducing batch-size from13 to 10 solved the problem in my case", "Reducing batch size of course reduces the memory usage but that is not the key issue here. The key is why running multiple models consecutively caused GPU blow-up. \r\n\r\nI had a similar problem. When run model training using single GPU worked fine. But when I duplicated the model onto multiple GPUs (with same batch size per GPU) blew up. I was using Keras' new multi_gpu_model API call.", "I think the solution is to clean  the session after training a model", "I came across the same problem. I shut down all the anaconda prompt windows and cleared all the python tasks. Reopened an Anaconda prompt window and executed the train.py file. It worked for me the next time. The Anaconda and Python terminals were taking up the memory which doesn't leave space for the training process. Hope this helps \ud83d\udc4d ", "Try clear_session() if you running the model consecutively.\r\n\r\n`from keras import backend as K`\r\n`K.clear_session()`\r\nyou can find more in the following link:\r\n[https://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/8](url)\r\n"]}, {"number": 1354, "title": "How to decrease memory usage on GPU card", "body": "When I run example like mnist on GPU card, I see that it consumes almost all device memory of that card. It seems like the tensorflow reserves as much memory as possible for memory pool at the start of process.\nHowever, we need to run the tensor flow on a shared GPU card, on which runs other users' processes. So we can't occupy all device memory of it. Is there any switch to turn this memory pool off, or decrease the memory pool to a user defined size?\nThanks a lot in advance!\n", "comments": ["I guess you are considering tensorflow for your online service. We intend to use it for models in our search engine as well, but the GPU usage is really an annoying problem. I am looking forward to positive feedback ;)\n", "http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n"]}, {"number": 1353, "title": "AttributeError: 'module' object has no attribute 'truncated_normal'", "body": "Operating System: **Ubuntu 14.04 LTS**\nEnvironment: **Python 2.7.11 :: Continuum Analytics, Inc.**\nIDE: **Spyder and IPython**\nTensorFlow Version: **0.7.1**\nPip Package File: **tensorflow-0.7.1-cp27-none-linux_x86_64.whl 64-Bit CPU ONLY**\nPip Package File from: [https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl](https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl)\n\nTrying to follow tutorial from Kaggle to implement Convolutional NN. `import tensorflow` seems to execute without returning an error.\n\nCode to reproduce:\n\n```\nIn [74]: \ndef weight_variable(shape):\n     initial = tf.truncated_normal(shape, stddev=0.1) #Outputs random values from truncated normal distribution.\n     return tf.Variable(initial)\n\nIn [75]: \nW_convl = weight_variable([5,5,1,32])\n\nTraceback (most recent call last):\n\n  File \"<ipython-input-75-65b9a0522cb2>\", line 1, in <module>\n    W_convl = weight_variable([5,5,1,32])\n\n  File \"<ipython-input-74-ae39e79c161a>\", line 2, in weight_variable\n    initial = tf.truncated_normal(shape, stddev=0.1) #Outputs random values from truncated normal distribution.\n\nAttributeError: 'module' object has no attribute 'truncated_normal'\n```\n\nI have tried:\n\n`import tensorflow as tf`\n\nAnd:\n\n`from tensorflow import truncated_normal`\n\nNot sure what is going on.\n", "comments": ["That's really strange -- can you try installing in a virtualenv from scratch, to isolate the installation?\n", "Also, if `import tensorflow` works, can you report the output of `dir(tensorflow)`?\n", "@vrv and @mrry \n\nThanks for your reply guys! I have it working now. It seems like I had to again just pip uninstall tensorflow and protobuf and then pip install tensorflow. I'm not sure why but if it works it works!\n", "Glad to hear it's working, and sorry for the installation trouble.\n", "I was wondering if it was wrong to pass your parameters, The tf.truncated_normal(shape,mean,stddev...)", "Just replace tf with tf.compat.v1. It should work fine.", "> Just replace tf with tf.compat.v1. It should work fine.\r\n\r\nthank you@vkmanojk, it works for me", "> @vrv and @mrry\r\n> \r\n> Thanks for your reply guys! I have it working now. It seems like I had to again just pip uninstall tensorflow and protobuf and then pip install tensorflow. I'm not sure why but if it works it works!\r\n\r\nIf we are using `tensorflow-gpu`, we can't install TensorFlow alongside, right?", "replaces \"tf.truncated_normal\" by \"tf.random.truncated_normal\" in Tensorflow Core v2.x\r\n"]}, {"number": 1352, "title": "fix build bug come from 8041c54", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System: OSX\n\nIf installed from sources, provide the commit hash: 8041c54\n\n**tensorflow/contrib/linear_optimizer/BUILD**\n\n```\nlinkopts = [\n     \"-Wl,-Bsymbolic\",\n     \"-lm\",\n],\n```\n\n**should change to:**\n\n```\nlinkopts = select({\n    \"//conditions:default\": [\n     \"-Wl,-Bsymbolic\",\n     \"-lm\",\n    ],\n    \"//tensorflow:darwin\": [],\n}),\n```\n", "comments": ["Thanks for the report, we'll get in a change tomorrow!\n", "Fixed in c3c40b7a8950bba876b92456824e5b6a9505c0a7.  Still one more error to fix.\n"]}, {"number": 1351, "title": "Removed a broken link", "body": "A broken link referred to CIFAR-10 learning rate decay which was a broken link. So, I removed the link and the references to the link.\n", "comments": ["Can one of the admins verify this patch?\n", "https://www.tensorflow.org/versions/r0.7/tutorials/deep_cnn/index.html\n\nthe image is there -- don't rely on github's .md files to look perfect :(\n"]}, {"number": 1350, "title": "nan gradients with low memory gpu", "body": "### Environment info\n\nOperating System: \n`3.12.9-201.fc19.x86_64 #1 SMP Wed Jan 29 15:44:35 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux`\n\nGPU card info from `lspci -v | grep gtx -i`\n\n```\nNVIDIA Corporation GK110 [GeForce GTX Titan] (rev a1)\n```\n\nIf installed from binary pip package, provide:\n1. pip package installed:\n   `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally                                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally                                                                              \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                              \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                             \n0.7.1                                                                        \n```\n### Steps to reproduce\n1. Increase batch size, so that you see the following warning message\n2. check the gradients, and there are `nan` values\n3. decrease batch size, and do the same check. Everything is normal if I don't see the following warning message\n### What have you tried?\n1. reduce batch size\n### Logs or other output that would be helpful\n\n```\nW tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:131] Ran out of memory trying to allocate 1.85GiB. The caller indicates that this is not a failure, but may mean that th\nere could be performance gains if more memory is available.\n```\n", "comments": ["It's possible this is not a bug -- changing the batch size often requires changing the learning rate.\n", "I didn't apply the optimizer at all. Just check the gradients from the beginning.\n", "I think we would need real code to reproduce this, and very strong evidence to suggest there is a bug to make it worth our time to investigate (we do have 276 open bugs and growing...)\n", "Sure. I will create a test case soon.\n", "I will reopen this when I have a solid example.\n", "I believe this may be happening to me as well. I am running on a GeForce 770 with only 2GB memory (1.8GB free) and tensorflow claims to allocate 1.6GiB of memory. I'm not sure about the gradients, but when I output my loss I get nan. My model doesn't perform any operations that would produce nan (only additions, subtractions, and multiplications).\n", "The problem occurs only with my model. I tried my best to reproduce using the examples, but everything was fine. @vladfi1 can you figure out a simple example to reproduce?\n", "I read [here](http://stackoverflow.com/questions/34514324/error-using-tensorflow-with-gpu) that Tensorflow always reserves all but 200MB of the GPU's memory, so the memory usage is always going to be very high. I've drastically reduced my model down in size and am still getting the nan outputs (as well as other nonsensical values such as negative norms), so I'm not sure if memory use is the culprit.\n", "I now suspect that the issue has to do with reduce_sum (on gpu). I kept simplifying my original program until I got to something trivial:\n\n``` python\nimport tensorflow as tf\n\ntest_input = tf.placeholder(tf.float32, [2])\ntest_output = tf.reduce_sum(test_input)\n\nwith tf.Session() as sess:\n  [out] = sess.run([test_output], feed_dict = {test_input :  [1.0, 2.0]})\n  print(out)\n```\n\nExecuting on my gpu machine I get the following output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.4 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 770\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.137\npciBusID 0000:02:00.0\nTotal memory: 2.00GiB\nFree memory: 1.46GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:763] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 1.26GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x4202380000 extends to 0x425335a000\n0.0\n```\n\nThe correct output should be 3.0, not 0.0. It is interesting that If I use a shape-`[1]` array instead then I get the correct output.\n", "Tested this and my original model on a different machine (with a 660) and it seems to work just fine...\n", "I'm getting the same problem with the newest Tensorflow (0.8.0). For a bigger batch size I get NaN:s even when limiting gradients (could not reproduce with batch size of 1). Following the min and max of all learned variables does not show any explosions. Running the same program on CPU (CUDA_VISIBLE_DEVICES=\"\") does not give the NaNs.\nIt might even have something to do with GPU memory allocation as updating to the newest Tensorflow started giving me out of memory errors, even if it worked without such errors with an older version.\nI'm getting correct 3.0 from the above example, though.\nI have a 4 GB NVidia GPU: NVIDIA Corporation GM204 [GeForce GTX 970](rev a1)\n\nSeems like the problem was solved by updating the kernel to the newest Ubuntu kernel, the CUDA, the CUDNN and the Tensorflow.\n", "@keskival Not sure if this is the issue, but the 970 has a strange architecture with 3.5GB of fast memory and 0.5GB of slow memory - from what I've read it isn't recommended to go over the 3.5GB when running cuda programs. This might interact poorly with tensorflow's default allocation of all available GPU memory, although I don't see why you would get NaNs.\n"]}, {"number": 1349, "title": "Question about back propagation in time-delayed recurrent models", "body": "The full question is available here: http://stackoverflow.com/questions/35733302/back-propagation-in-tensorflow\n\nThe question concerns whether I would have to implement a custom backward step in order to create something similar to the [clockwork model](http://jmlr.org/proceedings/papers/v32/koutnik14.pdf). In the event that I would, I was wondering if there is reference code for how I might properly go about it.\n\nThank you in advance for your support.\n", "comments": ["Please don't cross post stack overflow questions as Github issues.\n", "@girving just trying to get it in front of the only people who will have the answer.\n"]}, {"number": 1348, "title": "Error when running User Defined Op Tutorial", "body": "For bugs/issues, please fill in the following.  The more information you\nprovide, the more likely we can help you.\n### Environment info\n\nOperating System:  Tried both on OS X and Rhel7\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n0.7.1\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Follow instructions from https://www.tensorflow.org/versions/master/how_tos/adding_an_op/index.html\n2. Attempt to build zero out: \n\n```\ng++ -v -std=c++11 -shared zero_out.cc -o zero_out.so \\\n-I $TF_INC -l tensorflow_framework -L $TF_LIB \\\n-fPIC -Wl,-rpath $TF_LIB\n```\n1.  Get Error:\n\n```\n g++ -v -std=c++11 -shared zero_out.cc -o zero_out.so -I $TF_INC -l tensorflow_framework -L $TF_LIB -fPIC -Wl,-rpath /usr/local/lib/python2.7/site-packages/\nApple LLVM version 7.0.2 (clang-700.1.81)\nTarget: x86_64-apple-darwin14.5.0\nThread model: posix\n \"/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/clang\" -cc1 -triple x86_64-apple-macosx10.10.0 -Wdeprecated-objc-isa-usage -Werror=deprecated-objc-isa-usage -emit-obj -mrelax-all -disable-free -disable-llvm-verifier -main-file-name zero_out.cc -mrelocation-model pic -pic-level 2 -mthread-model posix -mdisable-fp-elim -masm-verbose -munwind-tables -target-cpu core2 -target-linker-version 253.9 -v -dwarf-column-info -resource-dir /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.0.2 -I /usr/local/lib/python2.7/site-packages/tensorflow/include -stdlib=libc++ -std=c++11 -fdeprecated-macro -fdebug-compilation-dir /usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/user_ops -ferror-limit 19 -fmessage-length 158 -stack-protector 1 -mstackrealign -fblocks -fobjc-runtime=macosx-10.10.0 -fencode-extended-block-signature -fcxx-exceptions -fexceptions -fmax-type-align=16 -fdiagnostics-show-option -fcolor-diagnostics -o /var/folders/pg/fbr4jqk15zz3wx4b2h1pgzlr002cz4/T/zero_out-31d965.o -x c++ zero_out.cc\nclang -cc1 version 7.0.2 based upon LLVM 3.7.0svn default target x86_64-apple-darwin14.5.0\nignoring nonexistent directory \"/usr/include/c++/v1\"\n#include \"...\" search starts here:\n#include <...> search starts here:\n /usr/local/lib/python2.7/site-packages/tensorflow/include\n /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1\n /usr/local/include\n /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../lib/clang/7.0.2/include\n /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include\n /usr/include\n /System/Library/Frameworks (framework directory)\n /Library/Frameworks (framework directory)\nEnd of search list.\nIn file included from zero_out.cc:1:\nIn file included from /usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op.h:22:\n/usr/local/lib/python2.7/site-packages/tensorflow/include/tensorflow/core/framework/op_def.pb.h:9:10: fatal error: 'google/protobuf/stubs/common.h' file not\n      found\n#include <google/protobuf/stubs/common.h>\n         ^\n1 error generated.\n```\n", "comments": ["Dupicate of #1270\n", "The protobuf error was coming from a bad installation, and there was a previous error around the eigen that is now referenced in #1270 .\n"]}, {"number": 1347, "title": "Embedding Lookup - shared weights: Dimensions are not compatible error", "body": "### Environment info\n\nOperating System: Linux\n\nInstalled from sources, commit hash: 7b5fef17806e2138ef16e8399ef5b1078d404dbe\nPython 2.7.11 |Anaconda 2.3.0 (64-bit)| (default, Dec  6 2015, 18:08:32)\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\n\nvocab_size = 1337\nembedding_size = 100\nbatch_size = 512\nQmax = 19\nTmax = 17\n\nW = tf.Variable(\n    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=\"W\")\n\n#W2 = tf.Variable(\n#    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=\"W2\")\n\n_q = tf.placeholder(\"int32\", [batch_size, Qmax])\n_t = tf.placeholder(\"int32\", [batch_size, Tmax])\n\nA = tf.reduce_mean(tf.nn.embedding_lookup(W, _q), [1])\n\n# DOES NOT WORK\nB = tf.reduce_mean(tf.nn.embedding_lookup(W, _t), [1])\n\n# WORKS\n#B = tf.reduce_mean(tf.nn.embedding_lookup(W2, _t), [1])\n\ncost = tf.reduce_mean(tf.add(A, B))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n```\n### What have you tried?\n1. Commented lines seem to mitigate the problem, but then the embedding matrix is not shared, which is desired for my use case.\n### Logs or other output that would be helpful\n\n```\njd@jd-cuda:~$ python tensor_fail.py \nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nTraceback (most recent call last):\n  File \"tensor_fail.py\", line 28, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 416, in gradients\n    aggregation_method)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 638, in _AggregatedGrads\n    for x in out_grad]), out_grad[0].dense_shape)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 309, in concat\n    name=name)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 70, in _concat\n    name=name)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1836, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1476, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 364, in _ConcatShape\n    concat_dim + 1:].merge_with(value_shape[concat_dim + 1:])\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 530, in merge_with\n    new_dims.append(dim.merge_with(other[i]))\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 119, in merge_with\n    self.assert_is_compatible_with(other)\n  File \"/home/jd/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 94, in assert_is_compatible_with\n    % (self, other))\nValueError: Dimensions Dimension(19) and Dimension(17) are not compatible\n```\n\nI would be grateful for any help.\n\nBest regards,\n## \n\nJacek Dabrowski\n", "comments": ["``` python\nimport tensorflow as tf\n\nvocab_size = 1337\nembedding_size = 100\nbatch_size = 512\nQmax = 19\nTmax = 17\n\nW = tf.Variable(\n    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name=\"W\")\n\n_x = tf.placeholder(\"int32\", [batch_size, Qmax+Tmax])\n\nE = tf.nn.embedding_lookup(W, _x)\n\nA = tf.reduce_mean(tf.slice(E, [0, 0, 0], [-1, Qmax, -1]), [1])\nB = tf.reduce_mean(tf.slice(E, [0, Qmax, 0], [-1, Tmax, -1]), [1])\n\ncost = tf.reduce_mean(tf.add(A, B))\n\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n```\n\nSeems to be a functional workaround.\n", "Yeah, I think this is the only workaround for now -- it looks like during gradient construction, we have to aggregate the gradients to 'W' from the two different sources with different shapes, and it's not easy to know that one of them needs to be zero-padded.  Slicing provides this extra bit of information.\n\nI'm not sure of a straightforward way to make this easy, so I think your workaround is probably the right thing to do.\n", "Understood & agreed. Might be beneficial to document this behavior for `embedding_lookup` though - sharing a dictionary over multiple input fields is a pretty common use case.\n"]}, {"number": 1346, "title": "CUDA compilation on ArchLinux fails", "body": "## Overview\n\nBuild fails with \n\n```\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n     return (char *) memcpy (__dest, __src, __n) + __n;\n```\n## Specific information:\n- ArchLinux system\n- Compute Capability 3.0 necessary\n- CuDNN 4, CUDA 7.5\n- Bazel 0.2.0\n#### Followed the installation from source instructions:\n- CuDNN 4, CUDA 7.5 are installed in /opt/cuda\n- `git clone -b v0.7.1 --recurse-submodules https://github.com/tensorflow/tensorflow`\n- `cd tensorflow`\n- `./configure` Details as attached. Building with cuda support and compute capability 3.0\n- `~/bin/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\n  This results in the following error:\n\n```\n\n/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(36): error: identifier \"__builtin_ia32_monitorx\" is undefined\n\n/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(42): error: identifier \"__builtin_ia32_mwaitx\" is undefined\n\n2 errors detected in the compilation of \"/tmp/tmpxft_00006c12_00000000-7_avgpooling_op_gpu.cu.cpp1.ii\".\n```\n\nThis is probably due to cuda requiring gcc 4.9, while I am using 5.3. I fixed this by editing `crosstool_wrapper_driver_is_not_gcc` in `tensorflow/third_party/gpus/crosstool/clang/bin`, changing line 49 to the explicit\n`GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')`\nThis allows compilation to continue; however, it then fails with \n\n```\n/usr/include/string.h: In function 'void* __mempcpy_inline(void*, const void*, size_t)':\n/usr/include/string.h:652:42: error: 'memcpy' was not declared in this scope\n   return (char *) memcpy (__dest, __src, __n) + __n;\n                                          ^\nERROR: /home/RED/src/tensorflow/tensorflow/core/BUILD:334:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/aggregate_ops_gpu.cu.o' was not created.\nERROR: /home/RED/src/tensorflow/tensorflow/core/BUILD:334:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nAdditional information:\n- I had been able to compile 0.6.0 (though with CUDA 7.0 and CuDNN 2). This also necessitated switching GCC.\n- CPU-Only compilation works.\n- Wheel installation works, too, but is obviously unable to use my CC3.0 device.\n\nThanks!\n\nAttachments:\n- [tensorflow_configure.txt](https://github.com/tensorflow/tensorflow/files/153447/tensorflow_configure.txt)\n- [tensorflow_output.txt](https://github.com/tensorflow/tensorflow/files/153446/tensorflow_output.txt)\n", "comments": ["I am not sure either.  It looks like maybe the failure is during nvcc compiling aggregate_ops_gpu.cu.cc, but I don't understand why it works for us at CUDA 7.0 and not for you.\n", "The `__builtin_ia32_monitorx`/`__builtin_ia32_mwaitx` error is a fairly common issue. You can try disabling `mwaitx` when using nvcc (append this to your nvcc flags: `-Xcompiler -mno-mwaitx`). Forcing `__STRICT_ANSI__` may help as well, although that's something I only had to use on my own code since it relies on Boost 1.60, and it seemed to be related to missing `__float128` support in nvcc (cf [this](https://github.com/arrayfire/arrayfire/issues/1210)).\n\nAs for your second issue, I had the same problem just a few days ago in my own code. In my case, CMake used `/usr/lib/hardening-wrapper/bin/cc` rather than `gcc` directly. That wrapper adds some extra compiler options (e.g. `_FORTIFY_SOURCE`) and can usually be used without even thinking about it, but apparently the source of the `memcpy` error was in one of these extra flags. I didn't try to pinpoint the exact source of the problem, but that's probably what you should be looking at.\n", "> I am not sure either. It looks like maybe the failure is during nvcc compiling aggregate_ops_gpu.cu.cc, but I don't understand why it works for us at CUDA 7.0 and not for you.\n\nFor reference, I have also used CUDA 7.0 (and CuDNN 4). Same error.\n\n> You can try disabling mwaitx when using nvcc (append this to your nvcc flags: -Xcompiler -mno-mwaitx). \n\nThis was one of the other things I had experimented with before (following #1066). I ran into the second error, too.\n\n> As for your second issue, I had the same problem just a few days ago in my own code. In my case, CMake used /usr/lib/hardening-wrapper/bin/cc rather than gcc directly. That wrapper adds some extra compiler options (e.g. _FORTIFY_SOURCE) and can usually be used without even thinking about it, but apparently the source of the memcpy error was in one of these extra flags. I didn't try to pinpoint the exact source of the problem, but that's probably what you should be looking at.\n\nNo `hardening-wrapper` installed, but looking at the flags might be a good start.\n\nFor now, I am going to roll back to 0.6.0, reproduce the exact steps I needed to get it to run (since I know it once worked) and try to apply them to 0.7.1. I'll try to keep this issue updated.\n\nThanks, both of you!\n", "@e14159 can you try with `-D_FORCE_INLINES`? I just had the same issue with pcl, I checked the `string.h` header, and using that preprocessor directive skips the block where the `memcpy` error appears. There's probably a cleaner workaround though...\n", "It worked!\nSpecifically, I've inserted `cxx_flag: \"-D_FORCE_INLINES\"` into `tensorflow/third_party/gpus/crosstool/CROSSTOOL`:48.\nI have compiled it successfully (despite some complaints due to it being both inlined and outlined) both with CUDA7.0/CuDNN2 (only with the test `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`) and CUDA7.5/CuDNN4 (both the test and the package compilation). I have installed the latter, and ran it with an example of mine. It used the gpu, and it trained. I'm therefore, for now, going to assume that it also works correctly.\n\nThank you very much!\n\nClosing this issue; I hope it'll help others running into the same issue.\n", "@bchretien oh!!! it worked! i am so excited! thank you very much!!\n"]}, {"number": 1345, "title": "random.sample cannot take np.array. fixed", "body": "Otherwise there will be an error on my machine.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "[Related fix.](https://github.com/tensorflow/tensorflow/pull/1335)\n"]}, {"number": 1344, "title": "op.device no longer showing which device an op is actually placed on", "body": "Using the latest builds, it appears that `op.device` now returns the device that an op is supposed to be placed on, i.e. what the user specifies, as opposed to what it is actually placed on. I believe this is true because when I set `allow_soft_placement` to `False` when starting a `Session`, some ops, for example `global_step`, cannot be placed on the GPU. But if I assign them to the GPU and then allow soft placement, `op.device` returns `/gpu:0` for ops that were previously not placeable on the GPU.\n\nThis is a break from previous behavior, which showed the actual device an op is placed on. The new behavior makes it very difficult to debug device placement. Is there are any way to probe the actual device that an op is placed on?\n\nP.S. I'm using a device function for my placement logic. Something like:\n\n```\ndef _device_function(op):\n    if complicated_logic_true:\n        return '/cpu:0'\n    else:\n        return '/gpu:0'\n```\n", "comments": ["op.device reflects what the user specified, because it is part of the GraphDef that is sent to TensorFlow.\n\nIf you use log_device_placement in the ConfigProto, you will get a log of what the assigned device is (after applying all soft_placement rules, etc).  I don't think op.device ever could reflect what the runtime actually did.\n", "I may have been mistaken about the previous behavior. I do know about `log_device_placement`, but prefer `op.device` as it's easier to work with it programmatically, especially with very large unfolded graphs that have hundreds of thousands of nodes. If there's some way to provide an alternative mechanism to probe actual device placement programmatically from within python that would be fantastic.\n", "An update here: you can call `Session.run()` passing a `RunOptions` protobuf that requests a trace, and fills in a `RunMetadata` protobuf with details of all the ops that ran, on which device they ran, and how long they took. I think this gives you the programmatic access that you're looking for. Here's an example:\n\nhttps://github.com/tensorflow/tensorflow/blob/6f90ede2496134777a948fea872b17a67b4f6ef2/tensorflow/python/client/session_test.py#L913\n", "Thanks, will take a look.\n", "Is there a way to check the device using an op.device? Note that after using the with tf.device(device_name) and then checking op.device all I observe is an empty string.\r\n\r\nIs there a way to check device placement without referring to the logs (but rather through attributes of the operation(s))  ?\r\n\r\nNote I am using Tensorflow v1.1.0 Thanks ", "@atinzad No, the `op.device` property is never filled in with the true device on which an op ran (and technically the op could be placed on multiple different devices, depending on how your session is configured). Using `RunOptions` and `RunMetadata` to extract the partitioned subgraphs [as I suggested above](https://github.com/tensorflow/tensorflow/issues/1344#issuecomment-210641516) is the best way to get this information programmatically."]}, {"number": 1343, "title": "sigmoid cross entropy doesn't verify shapes", "body": "Hello,\n\nI have found a misleading behaviour in `sigmoid_cross_entropy_with_logits` function. If I pass a tensor of shape `(n, 1)` as logits and a tensor of shape `(n,)` as targets it outputs a tensor of `(n, n)` which is obviously different from tensor `(n,)` if both tensors have the same shape `(n,)`. Although api docs say that logits and targets must have the same shape, this example doesn't fail properly nor broadcast correctly.\n\nOS: Fedora 21\nTF v0.7.1 compiled from sources (tagged commit).\n", "comments": ["That is indeed a bug.  We can add the proper shape validation code in that function to prevent the unexpected broadcasting.\n"]}, {"number": 1342, "title": "download link of pdf version of the documentation/get_startred/tutorials etc", "body": "Tried a lot of searching, but I am still not able to find a pdf version of the tensorflow docs. Kindly provide the same.\n", "comments": ["I don't think we have any plans to do this. In the absence of much interest, I'm going to close this.\n"]}, {"number": 1341, "title": "cifar10_eval.py stop problem", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   : tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   : 0.7.1\n### Steps to reproduce\n1. cd git/tensorflow/tensorflow/models/image/cifar10/\n2. python cifar10_eval.py\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.3545\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.56GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\n\nand stop this line.\n\nbut sometimes not stop this line when run same as above. and\n\n==========check3.25==========\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 5.27GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x706400000 extends to 0x857ab6000\n==========check3.5==========\n==========check3.75==========\n==========check3.25==========\n\n.\n.\n.\n\n==========check3.25==========\n==========check3.5==========\n==========check3.75==========\n2016-03-01 15:41:09.935657: precision @ 1 = 0.801\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb37405e7b0 Compute status: Cancelled: Enqueue operation was cancelled\n     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\nI tensorflow/core/kernels/queue_base.cc:286] Skipping cancelled enqueue attempt\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb350035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36403ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3223f5240 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb35c03ada0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb338035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb36c00c600 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb354035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb340035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb34c035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3480095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33000c470 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb370011050 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb328010190 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb3600095e0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb358035000 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\nW tensorflow/core/common_runtime/executor.cc:1102] 0x7fb33c05b3b0 Compute status: Aborted: RandomShuffleQueue '_1_shuffle_batch/random_shuffle_queue' is closed.\n     [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT, DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, Div/_206, Cast/_208)]]\n### What have you tried?\n1.  Add some instruction for checking stop point.\n\ncifar10_eval.py\n\ndef eval_once(saver, summary_writer, top_k_op, summary_op):\n\n```\n# Start the queue runners.\ncoord = tf.train.Coordinator()\ntry:\n  threads = []\n  for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n    threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                     start=True))\n  num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n  true_count = 0  # Counts the number of correct predictions.\n  total_sample_count = num_iter * FLAGS.batch_size\n  step = 0\n  while step < num_iter and not coord.should_stop():\n    print(\"==========check3.25==========\")\n    predictions = sess.run([top_k_op])\n    print(\"==========check3.5==========\")\n    true_count += np.sum(predictions)\n    step += 1\n    print(\"==========check3.75==========\")\n\n  # Compute precision @ 1.\n  precision = true_count / total_sample_count\n  print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n\n  summary = tf.Summary()\n  summary.ParseFromString(sess.run(summary_op))\n  summary.value.add(tag='Precision @ 1', simple_value=precision)\n  summary_writer.add_summary(summary, global_step)\nexcept Exception as e:  # pylint: disable=broad-except\n  coord.request_stop(e)\n\ncoord.request_stop()\ncoord.join(threads, stop_grace_period_secs=10)\n```\n\nlook like #389 issue but different. and many time stop \n\npredictions = sess.run([top_k_op])\n\nthis line. \n", "comments": ["@vrv: Was this supposed to be closed by your commit?\n", "Yes, sort of.  We no longer log those warnings, because they were just superfluous warnings.\n"]}, {"number": 1340, "title": "Error in installing the tensorflow on 32 bit Ubuntu 14.04 LTS", "body": "Hi Developers:\n\nPlease, guide me to fix the issue and help me to install the tensorflow on the 32bit Ubuntu 14.04 LTS.\n\nError i am getting :\nhunter@hunter:~$ sudo pip install --upgrade \n![screenshot from 2016-03-01 13 00 32](https://cloud.githubusercontent.com/assets/9070249/13420607/b4ccc094-dfad-11e5-9e32-fd9880f4056f.png)\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\ntensorflow-0.7.1-cp27-none-linux_x86_64.whl is not a supported wheel on this platform.\nStoring debug log for failure in /home/hunter/.pip/pip.log\n\nThanks you for your time in advance.\n", "comments": ["Unfortuately we don't have a pip package for 32-bit linux right now.  You'd have to build from sources, and I'm not even sure it's supported.\n", "Aha, I have met the problem these days.The problem is caused by the failure of getting the root right. To solve the problem you can input the code 'sudo su -' to get the right of root. Good luck\n"]}, {"number": 1339, "title": "Fix typo in README", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "LG, can you rebase to master?\n", "Nevermind, there's no conflict\n", "Merged\n"]}, {"number": 1338, "title": "Master", "body": "translate.py\u306e\u4e2d\u306e\noutputs=[int(np.argmax(logit,axis=1))for logit in output_logits]\n\u306a\u3044\u306benumerate\u95a2\u6570\u3092\u7d44\u307f\u8fbc\u3080\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "\u3053\u3093\u306b\u3061\u306f,\n\nThis pull request appears to be empty, consisting of just two merges. Also, the description is in Japanese\u2014did you intend to send it in its current form? If so, please rewrite to English :-)\n"]}, {"number": 1337, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Merged\n"]}, {"number": 1336, "title": "pip install/upgrade version 0.7.1 issue -- ImportError: No module named protobuf", "body": "Had a previously working version of tensorflow 0.6\n\nExecuted single command which appeared to proceed without incident:\n\n`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl`\n\nHowever, now protobuf not found during python runtime though pip seems to have installed protobuf==3.0.0b2 \n### Environment info\n\nOperating System: Centos 7\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n\n`[root@localhost ~]# sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\nCollecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n\n/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:315: \n\nSNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.\n  SNIMissingWarning\n\n/usr/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py:120: \nInsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.\n  InsecurePlatformWarning\n\nRequirement already up-to-date: six>=1.10.0 in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)\n\nRequirement already up-to-date: protobuf==3.0.0b2 in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)\n\nRequirement already up-to-date: wheel in /usr/lib/python2.7/site-packages (from tensorflow==0.7.1)\n\nRequirement already up-to-date: numpy>=1.8.2 in /usr/lib64/python2.7/site-packages (from tensorflow==0.7.1)\n\nRequirement already up-to-date: setuptools in /usr/lib/python2.7/site-packages (from protobuf==3.0.0b2->tensorflow==0.7.1)\n\nInstalling collected packages: tensorflow\n\nSuccessfully installed tensorflow-0.7.1\n`\n1. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n`Traceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/usr/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named protobuf\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n`\n", "comments": ["Can you try:\n\npip uninstall tensorflow\npip uninstall protobuf\n\nthen just reinstall tensorflow ?\n\nSee: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#cannot-import-name-descriptor\n", "Thanks that worked...closing issue.\n"]}, {"number": 1335, "title": "Fix word2vec_basic.py example to be python 3 compatible", "body": "Fixed word2vec_basic.py example to be both Python 2 & 3 compatible.\n", "comments": ["Can one of the admins verify this patch?\n", "I think I like this one more than #1345.  can you rebase?\n", "Nm, the icon they switched to looks like the conflict icon.  @tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 1334, "title": "Compile issue", "body": "Ubuntu 14, master branch pull minutes ago.\n\nERROR: /home/ggg/000/tensorflow/tensorflow/core/kernels/BUILD:712:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depthwise_conv_op':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/depthwise_conv_op.cc':\n  '/home/ggg/000/tensorflow/tensorflow/core/common_runtime/device.h'\n  '/home/ggg/000/tensorflow/tensorflow/core/graph/graph.h'\n  '/home/ggg/000/tensorflow/tensorflow/core/graph/edgeset.h'\n  '/home/ggg/000/tensorflow/tensorflow/core/graph/types.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n", "comments": ["I have the same issue as well as a similar one, depending on which parallel bazel task completes first:\n\nERROR: /home/karenbre/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:158:1: undeclared inclusion(s) in rule '//tensorflow/core/distributed_runtime/rpc:grpc_remote_master':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc':\n  'bazel-out/local_linux-opt/genfiles/tensorflow/core/protobuf/master_service.grpc.pb.h'.\nTarget //tensorflow/python:zero_out_op_test failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 3.716s, Critical Path: 3.43s\n", "I'm fixing the first issue, will try to figure out why the second one is happening.\n", "You seem to be building the target `//tensorflow/python:zero_out_op_test`. `bazel query tensorflow/python/...` doesn't show that it exists. Is that something that you added? What does your rule look like?\n", "I am trying to follow the tutorial for building a user operator since it was reported that those issues are now fixed. \n", "I can reproduce the issue when compiling the pip package `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`. Sometimes it even succeeds (after trying multiple times) but then `bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg` fails with `error: can't copy 'tensorflow/python/training/gen_training_ops.py': doesn't exist or not a regular file`\n", ":+1: \nSame error after running  `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\n", "The above commit probably fixes some issues, and maybe some others, but leaving it open until we fix any other issues that have cropped up recently\n", "Have issue compiling on Mac OS X 10.10, pulled from master branch couple minutes ago.\n\nERROR: /Users/michael/tensorflow/tensorflow/contrib/linear_optimizer/BUILD:14:1: Linking of rule '//tensorflow/contrib/linear_optimizer:python/ops/_sdca_ops.so' failed: osx_gcc_wrapper.sh failed: error executing command external/bazel_tools/tools/cpp/osx_gcc_wrapper.sh -shared -o bazel-out/local_darwin-opt/bin/tensorflow/contrib/linear_optimizer/python/ops/_sdca_ops.so -Wl,-all_load ... (remaining 23 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nclang: warning: argument unused during compilation: '-pthread'\nld: warning: option -noall_load is obsolete and being ignored\nld: unknown option: -Bsymbolic\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\n", "my issue is fixed.\n", "I have the same issue as michaelnhw has.\n", "I pushed c3c40b7a8950bba876b92456824e5b6a9505c0a7 to fix the first problem, but now there's one more linker problem.  I just fixed that internally and will be pushing it out momentarily.\n", "Okay, I think the builds are going to be green (or at least, not broken).\n", "pip package still does not build:\n\nERROR: /home/karenbre/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:158:1: undeclared inclusion(s) in rule '//tensorflow/core/distributed_runtime/rpc:grpc_remote_master':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc':\n  'bazel-out/local_linux-opt/genfiles/tensorflow/core/protobuf/master_service.grpc.pb.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n", "@keveman, @mrry, any ideas?\n", "Trying to build from source raise the same error as @kbrems .\n\n```\nERROR: /home/tom/Work/prog/github/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:131:1: undeclared inclusion(s) in rule '//tensorflow/core/distributed_runtime/rpc:grpc_worker_service':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc':\n'bazel-out/local_linux-py3-opt/genfiles/tensorflow/core/protobuf/worker_service.grpc.pb.h'.\nTarget //tensorflow/core/distributed_runtime/rpc:grpc_worker_service failed to build\n```\n\nAnd similar failure building `//tensorflow/core/distributed_runtime/rpc:grpc_master_service`.\n", "I don't see these errors when building `//tensorflow/tools/pip_package:build_pip_package` on a Ubuntu 14.04 machine. What Bazel version are you using? What OS version?\n", "I am on an archlinux distribution and running bazel 0.2.0-1.\n\nedit: The package compile. Apparently, it is unsafe to use Keyboard Interrupt with bazel build as it can break latter restart. Sorry for that.\n", "I pulled master today and I still have the same issue building pip package. I am on Ubuntu 14.04. \nI have not interrupted bazel. \nNote, a similar issue referred to a stale config file build_config.bzl, but that file is modified by ./configure, so it always appears to have been locally modified:\n\ndiff --git a/tensorflow/core/platform/default/build_config.bzl b/tensorflow/core/platform/default/build_config.bzl\nindex 633441f..7e6549b 100644\n--- a/tensorflow/core/platform/default/build_config.bzl\n+++ b/tensorflow/core/platform/default/build_config.bzl\n@@ -4,9 +4,9 @@ load(\"//google/protobuf:protobuf.bzl\", \"cc_proto_library\")\n load(\"//google/protobuf:protobuf.bzl\", \"py_proto_library\")\n\n # configure may change the following lines to '.X.Y' or similar\n-CUDA_VERSION = \"\"\n+CUDA_VERSION = \".7.5\"\n\n-CUDNN_VERSION = \"\"\n+CUDNN_VERSION = \".4\"\n\n # Appends a suffix to a list of deps.\n def tf_deps(deps, suffix):\n\n---\n\nBuild error:\nERROR: /home/karenbre/tensorflow/tensorflow/core/distributed_runtime/rpc/BUILD:158:1: undeclared inclusion(s) in rule '//tensorflow/core/distributed_runtime/rpc:grpc_remote_master':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc':\n  'bazel-out/local_linux-opt/genfiles/tensorflow/core/protobuf/master_service.grpc.pb.h'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 344.224s, Critical Path: 149.03s\n", "OK, I figured out the issue. Something changed in protobuf. That does not come with a git pull.\nI ran \ngit submodule update --init\nthen bazel clean --expunge\n\nThen I can build. \n", "This fixed the issue for me as well.  Thanks!\n", "I tried `git submodule update --init` but I still get the error:\n`error: can't copy 'external/protobuf/python/google/protobuf/wrappers_pb2.py': doesn't exist or not a regular file`\n", "@bernardopires \r\n\r\ni have the same error\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg fails with error: can't copy 'tensorflow/python/training/gen_training_ops.py': doesn't exist or not a regular file.\r\n\r\nHow do i fix it?", "above  solution by @mattisback fixes a problem I had as well, but keep getting:\r\n\r\nexecroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/image/_objs/python/ops/_distort_i\r\nmage_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.d\r\n\r\nWhich is referenced here, but posted because of possible relationship between the two:\r\nhttps://github.com/tensorflow/tensorflow/issues/15565\r\n\r\n\r\n", "Any work around for this right now, or do we have to wait until you release this update? If I do it is broken until then.... :("]}, {"number": 1333, "title": "Problem building label_image", "body": "Hi all, \nI tried to build the label_image example (I have tensor flow version 0.7 and I installed bazel) but when I build the example with\n`../../bin/bazel build tensorflow/examples/label_image/...`\nI get:\n`INFO: Found 2 targets...\nERROR: /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).\n/home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow\n/tmp/tmp.dPQmFTp3hh /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow`\n\nAt the end of this, in bazel-bin/tensorflow/examples/label_image I have no executable file, only a manifest and a param file.\nWhat's wrong?\n", "comments": ["Can you try building with --verbose_failures and repaste your logs?\n", "Thanks for reply vrv!\nNew output is this:\n`INFO: Found 2 targets...\nERROR: /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command (cd /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow && \\  exec env - \\\n    PATH=/home/informatica/tensorflow/bin:/home/informatica/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \\* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/local_linux-fastbuild/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;'): bash failed: error executing command \n  (cd /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow && \\ exec env - \\\n    PATH=/home/informatica/tensorflow/bin:/home/informatica/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \\* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/local_linux-fastbuild/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;').\n/home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow\n/tmp/tmp.k02Q7lIQJp /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow/external/png_archive/libpng-1.2.53 /home/informatica/.cache/bazel/_bazel_informatica/91a9bba81c77fb078ff5ab9ac13df451/tensorflow\n\nchecking for a BSD-compatible install... /usr/bin/install -c\n\nchecking whether build environment is sane... yes\n\nchecking for a thread-safe mkdir -p... /bin/mkdir -p\n\nchecking for gawk... no\n\nchecking for mawk... mawk\n\nchecking whether make sets $(MAKE)... yes\n\nchecking whether make supports nested variables... yes\n\nchecking whether to enable maintainer-specific portions of Makefiles... no\n\nchecking for gcc... gcc\n\nchecking whether the C compiler works... yes\n\nchecking for C compiler default output file name... a.out\n\nchecking for suffix of executables... \n\nchecking whether we are cross compiling... no\n\nchecking for suffix of object files... o\n\nchecking whether we are using the GNU C compiler... yes\n\nchecking whether gcc accepts -g... yes\n\nchecking for gcc option to accept ISO C89... none needed\n\nchecking whether gcc understands -c and -o together... yes\n\nchecking for style of include used by make... GNU\n\nchecking dependency style of gcc... gcc3\n\nchecking build system type... x86_64-unknown-linux-gnu\n\nchecking host system type... x86_64-unknown-linux-gnu\n\nchecking for a sed that does not truncate output... /bin/sed\n\nchecking for grep that handles long lines and -e... /bin/grep\n\nchecking for egrep... /bin/grep -E\n\nchecking for fgrep... /bin/grep -F\n\nchecking how to print strings... printf\n\nchecking for ld used by gcc... /usr/bin/ld\n\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\n\nchecking how to run the C preprocessor... gcc -E\n\nchecking for sed... /bin/sed\n\nchecking whether ln -s works... yes\n\nchecking whether make sets $(MAKE)... (cached) yes\n\nchecking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n\nchecking the name lister (/usr/bin/nm -B) interface... BSD nm\n\nchecking the maximum length of command line arguments... 1572864\n\nchecking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... \n\nfunc_convert_file_noop\n\nchecking how to convert x86_64-unknown-linux-gnu file names to toolchain format... \n\nfunc_convert_file_noop\n\nchecking for /usr/bin/ld option to reload object files... -r\n\nchecking for objdump... objdump\n\nchecking how to recognize dependent libraries... pass_all\n\nchecking for dlltool... dlltool\n\nchecking how to associate runtime and link libraries... printf %s\\n\n\nchecking for ar... ar\n\nchecking for archiver @FILE support... @\n\nchecking for strip... strip\n\nchecking for ranlib... ranlib\n\nchecking command to parse /usr/bin/nm -B output from gcc object... ok\n\nchecking for sysroot... no\n\nchecking for a working dd... /bin/dd\n\nchecking how to truncate binary pipes... /bin/dd bs=4096 count=1\n\nchecking for mt... mt\n\nchecking if mt is a manifest tool... no\n\nchecking for ANSI C header files... yes\n\nchecking for sys/types.h... yes\n\nchecking for sys/stat.h... yes\n\nchecking for stdlib.h... yes\n\nchecking for string.h... yes\n\nchecking for memory.h... yes\n\nchecking for strings.h... yes\n\nchecking for inttypes.h... yes\n\nchecking for stdint.h... yes\n\nchecking for unistd.h... yes\n\nchecking for dlfcn.h... yes\n\nchecking for objdir... .libs\n\nchecking if gcc supports -fno-rtti -fno-exceptions... no\n\nchecking for gcc option to produce PIC... -fPIC -DPIC\n\nchecking if gcc PIC flag -fPIC -DPIC works... yes\n\nchecking if gcc static flag -static works... yes\n\nchecking if gcc supports -c -o file.o... yes\n\nchecking if gcc supports -c -o file.o... (cached) yes\n\nchecking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n\nchecking dynamic linker characteristics... GNU/Linux ld.so\n\nchecking how to hardcode library paths into programs... immediate\n\nchecking whether stripping libraries is possible... yes\n\nchecking if libtool supports shared libraries... yes\n\nchecking whether to build shared libraries... no\n\nchecking whether to build static libraries... yes\n\nchecking for ANSI C header files... (cached) yes\n\nchecking malloc.h usability... yes\n\nchecking malloc.h presence... yes\n\nchecking for malloc.h... yes\n\nchecking for stdlib.h... (cached) yes\n\nchecking for string.h... (cached) yes\n\nchecking for strings.h... (cached) yes\n\nchecking for an ANSI C-conforming const... yes\n\nchecking for size_t... yes\n\nchecking whether struct tm is in sys/time.h or time.h... time.h\n\nchecking for working strtod... yes\n\nchecking for memset... yes\n\nchecking for pow... no\n\nchecking for pow in -lm... yes\n\nchecking for zlibVersion in -lz... no\n\nconfigure: error: zlib not installed\n\nINFO: Elapsed time: 47.026s, Critical Path: 14.45s`\n\nI hope it's useful\n", "Try installing zlib.\n\n```\nsudo apt-get install zlib1g-dev -y\n```\n", "I have also the same issue,\ncan't be found in zlib\nBut I have install zlib in my machine\n[@sjs_88_31 tensorflow]# sudo yum install zlib1g-dev -y\nLoaded plugins: fastestmirror, langpacks\nLoading mirror speeds from cached hostfile\n- epel: mirrors.neusoft.edu.cn\n  No package zlib1g-dev available.\n  Error: Nothing to do\n  [@sjs_88_31 tensorflow]# yum install zlib\n  Loaded plugins: fastestmirror, langpacks\n  Loading mirror speeds from cached hostfile\n- epel: mirrors.neusoft.edu.cn\n  Package zlib-1.2.7-13.el7.x86_64 already installed and latest version\n  Nothing to do\n", "Closing since it looks like the problem is not having installed zlib.\n"]}, {"number": 1332, "title": "Fixing a still-broken link to artifact in README.md", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks, can you update to master?\n", "Merged\n"]}, {"number": 1331, "title": "replaced an unused parameter with `_`", "body": "replaced an unused parameter with `_` as the Python convention to stop a linter warning\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks, can you update to master?\n", "@tensorflow-jenkins: test this please\n", "Flaky test failed.  Merging.\n"]}]