[{"number": 24567, "title": "docs:generate can't find numpy headers", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\n\r\narrayobject.h and other numpy headers not found when building api docs from source (*bazel run //tensorflow/tools/docs:generate -- --output_dir=/tmp/master_out* per https://www.tensorflow.org/community/documentation)\r\n\r\nNumpy and most of other libraries are installed using *pip3 install --user*, so they are not in the */usr/lib/python3/dist-packages*, but rather in *~/.local/lib/python3.6/site-packages*.\r\n\r\nThe goal of this exercise is to build a local set of api docs for offline use (think of \"[HTML+zip]\" docs downloadable from https://docs.scipy.org/doc/ or *make html* for scikit-learn docs).\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n*PATH=/home/username/tmp/bazel/bin:$PATH ./configure*\r\n\r\n*/usr/bin/python3* \r\n\r\n*/usr/lib/python3/dist-packages*\r\nOR \r\n*/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy*\r\nwith the same result\r\n\r\n*PATH=/home/username/tmp/bazel/bin:$PATH bazel run //tensorflow/tools/docs:generate -- --output_dir=/tmp/master_out*\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\noutput of the *docs:generate* above:\r\n\r\n> ...\r\n> ERROR: /home/username/tmp/tensorflow-1.12.0/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':\r\n> this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':\r\n>   'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'\r\n> ...\r\n> \r\n\r\n*cat .tf_configure.bazelrc*\r\n\r\n> build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> build --action_env PYTHON_LIB_PATH=\"/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy\"\r\n> build --python_path=\"/usr/bin/python3\"\r\n> build:ignite --define with_ignite_support=true\r\n> build:xla --define with_xla_support=true\r\n> build --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\n> build --action_env TF_NEED_ROCM=\"0\"\r\n> build --action_env TF_NEED_CUDA=\"0\"\r\n> build --action_env TF_DOWNLOAD_CLANG=\"0\"\r\n> build:opt --copt=-march=native\r\n> build:opt --host_copt=-march=native\r\n> build:opt --define with_default_optimizations=true\r\n> build:v2 --define=tf_api_version=2\r\n\r\nAttempts to list multiple include paths like *PYTHON_LIB_PATH=\"/usr/lib/python3/dist-packages:/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy\"* result in the following error:\r\n\r\n> ERROR: /home/username/tmp/tensorflow-1.12.0/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n> \tFile \"/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl\", line 308\r\n> \t\t_create_local_python_repository(repository_ctx)\r\n> \tFile \"/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl\", line 270, in _create_local_python_repository\r\n> \t\t_check_python_lib(repository_ctx, python_lib)\r\n> \tFile \"/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl\", line 213, in _check_python_lib\r\n> \t\t_fail((\"Invalid python library path: %...))\r\n> \tFile \"/home/username/tmp/tensorflow-1.12.0/third_party/py/python_configure.bzl\", line 28, in _fail\r\n> \t\tfail((\"%sPython Configuration Error:%...)))\r\n> Python Configuration Error: Invalid python library path: /usr/lib/python3/dist-packages:/home/username/.local/lib/python3.6/site-packages/numpy/core/include/numpy\r\n>  and referenced by '//third_party/python_runtime:headers'\r\n> ERROR: Analysis of target '//tensorflow/tools/docs:generate' failed; build aborted: Analysis failed\r\n\r\n\r\n", "comments": ["Added a PR #24569 for the fix."]}, {"number": 24566, "title": "leaky_relu - buggy when alpha > 1 or alpha < 0", "body": "tf.nn.leaky_relu() is implemented as math_ops.maximum(alpha * features, features, name=name). This won't work if alpha > 1 or alpha < 0. Although this is not expected, there is nothing that prevents users from setting alpha parameter as such.\r\nA better implementation could be nn.relu(features) - alpha*nn.relu(-features) which won't suffer from the above issue.", "comments": ["@jvishnuvardhan  Hi\uff0cI want to know when this problem will be solved. Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24566\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24566\">No</a>\n"]}, {"number": 24565, "title": "build_pip_package failed to build", "body": "**System information**\r\n-macos mojave 10.14.1 :\r\n-build source code\r\n- 1.12\r\n- python3.6\r\n- Bazel 0.19 and 0.20, 0.21\r\n**logs**\r\n`executing command \r\n  (cd /private/var/tmp/_bazel_Ryan/4520a5bb283b0ab95531a31cb29b1a66/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH=/Users/Ryan/.yarn/bin:/Users/Ryan/Library/Android/sdk/platform-tools/:/usr/local/Cellar/gcc/8.2.0/bin:/usr/local/Cellar/python/3.6.5_1/bin:/usr/local/Cellar/python@2/2.7.15_1/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/go/bin:/usr/local/mysql/bin \\\r\n    PYTHON_BIN_PATH=/usr/local/Cellar/python/3.6.5_1/bin/python3.6 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n    XCODE_VERSION_OVERRIDE=10.1.0 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote external/com_google_absl -iquote bazel-out/darwin-opt/genfiles/external/com_google_absl -iquote bazel-out/darwin-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/darwin-opt/genfiles/external/bazel_tools -iquote bazel-out/darwin-opt/bin/external/bazel_tools -MD -MF bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.d '-frandom-seed=bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -isysroot __BAZEL_XCODE_SDKROOT__ n -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_absl/absl/base/dynamic_annotations.cc -o bazel-out/darwin-opt/bin/external/com_google_absl/absl/base/_objs/dynamic_annotations/dynamic_annotations.o)\r\nclang: error: no such file or directory: 'n'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1.994s, Critical Path: 1.17s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1 process: 1 local.\r\nFAILED: Build did NOT complete successfully`\r\n", "comments": ["Can you please give a try to bazel version 0.15? TF build is tested against bazel 0.15. You are trying to install cpu version of TF right?", "@ymodak  sor,tensorflow required latest bazel version is 0.19.", "Apologies for the delay in response. Can you please mention the steps you followed to [build TF from source](https://www.tensorflow.org/install/source)?\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24564, "title": "tf_python_api  build failed, ImportError: No module named '_pywrap_tensorflow_internal'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  windows 10\r\n- TensorFlow installed from: source \r\n- TensorFlow version: r1.8\r\n- Python version: 3.5\r\n- win10 + cmake +vs2015 source build, Debug X64\r\n-  installed conda tensorflow 1.8\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0 / 7.1\r\n- GPU model and memory: GTX 1070\r\n\r\n**Describe the problem**\r\nI build 275 projects with Gpu-option, pywrap_tensorflow_internal.dll and tensorflow.dll is build out;  \r\nAll successful  except tf_python_api .  build error info :  ImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002ImportError: No module named '_pywrap_tensorflow_internal'.\r\n\r\nI  try to fix it by copying pywrap_tensorflow_internal.dll and pywrap_tensorflow_internal.pyd into system32 dir, but it don't take effect.\r\n\r\nDetail info:\r\n259>------ \u5df2\u542f\u52a8\u751f\u6210: \u9879\u76ee: tf_python_api, \u914d\u7f6e: Debug x64 ------\r\n259>  Generating __init__.py files for Python API.\r\n259>  Traceback (most recent call last):\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n259>      return importlib.import_module(mname)\r\n259>    File \"D:\\AppProgram\\Anaconda2\\envs\\py3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n259>      return _bootstrap._gcd_import(name[level:], package, level)\r\n259>    File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n259>    File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n259>    File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n259>    File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n259>    File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n259>    File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n259>    File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n259>  ImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n259>\r\n259>  During handling of the above exception, another exception occurred:\r\n259>\r\n259>  Traceback (most recent call last):\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n259>      from tensorflow.python.pywrap_tensorflow_internal import *\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n259>      _pywrap_tensorflow_internal = swig_import_helper()\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n259>      return importlib.import_module('_pywrap_tensorflow_internal')\r\n259>    File \"D:\\AppProgram\\Anaconda2\\envs\\py3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n259>      return _bootstrap._gcd_import(name[level:], package, level)\r\n259>  ImportError: No module named '_pywrap_tensorflow_internal'\r\n259>\r\n259>  During handling of the above exception, another exception occurred:\r\n259>\r\n259>  Traceback (most recent call last):\r\n259>    File \"E:/TensorBuilds/tensorflow/tensorflow/contrib/cmake/build/tf_python/tensorflow/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n259>      from tensorflow.python.util import tf_decorator\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n259>      from tensorflow.python import pywrap_tensorflow\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n259>      raise ImportError(msg)\r\n259>  ImportError: Traceback (most recent call last):\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n259>      return importlib.import_module(mname)\r\n259>    File \"D:\\AppProgram\\Anaconda2\\envs\\py3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n259>      return _bootstrap._gcd_import(name[level:], package, level)\r\n259>    File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n259>    File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n259>    File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n259>    File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n259>    File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n259>    File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n259>    File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n259>  ImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n259>\r\n259>  During handling of the above exception, another exception occurred:\r\n259>\r\n259>  Traceback (most recent call last):\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n259>      from tensorflow.python.pywrap_tensorflow_internal import *\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n259>      _pywrap_tensorflow_internal = swig_import_helper()\r\n259>    File \"E:\\TensorBuilds\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n259>      return importlib.import_module('_pywrap_tensorflow_internal')\r\n259>    File \"D:\\AppProgram\\Anaconda2\\envs\\py3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n259>      return _bootstrap._gcd_import(name[level:], package, level)\r\n259>  ImportError: No module named '_pywrap_tensorflow_internal'\r\n259>\r\n259>\r\n259>  Failed to load the native TensorFlow runtime.\r\n259>\r\n259>  See https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n259>\r\n259>  for some common reasons and solutions.  Include the entire stack trace\r\n259>  above this error message when asking for help.\r\n259>C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \u201ccmd.exe\u201d\u5df2\u9000\u51fa\uff0c\u4ee3\u7801\u4e3a 1\u3002\r\n========== \u751f\u6210: \u6210\u529f 0 \u4e2a\uff0c\u5931\u8d25 1 \u4e2a\uff0c\u6700\u65b0 0 \u4e2a\uff0c\u8df3\u8fc7 258 \u4e2a ==========\r\n\r\n\r\n\r\n", "comments": ["cmake python version is wrong."]}, {"number": 24563, "title": "bazel build failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 1809 Build 17763.195\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): compile from source\r\n- TensorFlow version: master\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): Microsoft Build Tools 2015 Update 3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Nvidia RTX 2080 TI, 16GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuilt as in: https://www.tensorflow.org/install/source_windows\r\n-->console output for configuration and build process see below\r\nBuild process failed with \"C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:fusion_merger' failed (Exit 2): python.exe failed: error executing command\"\r\n\r\n**Any other info / logs**\r\nconsole output:\r\n[buildconsole.txt](https://github.com/tensorflow/tensorflow/files/2708791/buildconsole.txt)\r\n\r\n", "comments": ["Compiling Tensorflow-GPU in Windows seems to have some problems with XLA enabled.I also encountered the same issue when compiling Tensorflow-GPU in a similar environment.But disabling XLA can solve this issue.", "@striker890 Did you get a chance to try @rootkitchao 's suggestion? Is this still an issue for you?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24562, "title": "Failed to load the native TensorFlow runtime.", "body": "I am new to tensorflow.I have installed tensorflow without GPU as my machine won't support gpu version.\r\nI have installed it in portable winpython.\r\n\r\n![image](https://user-images.githubusercontent.com/46132760/50423282-63351900-0879-11e9-9d74-4753a482e0c0.png)\r\n\r\nHere is my stack trace\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\JetBrains\\PyCharm 5.0.6\\helpers\\pydev\\pydev_monkey_qt.py\", line 71, in patched_import\r\n    return original_import(name, *args, **kwargs)\r\n  File \"D:\\Life_In_IEM\\Project\\Predicting_Riot\\Mine_Data\\WPy-3670\\python-3.6.7.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Life_In_IEM\\Project\\Predicting_Riot\\Mine_Data\\WPy-3670\\python-3.6.7.amd64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Life_In_IEM\\Project\\Predicting_Riot\\Mine_Data\\WPy-3670\\python-3.6.7.amd64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Life_In_IEM\\Project\\Predicting_Riot\\Mine_Data\\WPy-3670\\python-3.6.7.amd64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Hi @DevDaring, as it is not clear to find the root-cause of the issue, could you fill the following build/installation template.  Please report the installation process and commands used.  Here is the [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Hello Team,\r\nThanks for your cooperation.\r\nI found the root cause of the issue.\r\nI was using portable python package.I installed tensorflow in that package.It created a long path of some files that same path was replicated in local cache.Due to long path name in cache,it was not properly created hence the issue occurred.\r\nI just put my package outside of folder,directly in drive and it worked.You can close the ticket.", "@DevDaring Great! I am happy that you found the root cause of the issue. I am closing this issue."]}, {"number": 24561, "title": " Tensorflow c++ compile error on linux collect2: error: ld returned 1 exit status", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\nr1.8\r\n- Installed using :\r\nbazel 0.16.1\r\n- GCC/Compiler version (if compiling from source):\r\n5.4.0\r\n- CUDA/cuDNN version:\r\n9.0 / 7.4.2\r\n- GPU model and memory:\r\nGPU\r\n**Describe the problem**\r\n\r\ncompile TensorFlow C++ \r\nbazel build --config=monolithic tensorflow:libtensorflow_cc.so\r\nuse \r\ng++ -std=c++11 -o tf_example \\\r\n-I/usr/local/include/tf \\\r\n-I/usr/local/include/eigen3 \\\r\n-g -Wall -D_DEBUG -Wshadow -Wno-sign-compare -w  \\\r\n-L/usr/local/lib/libtensorflow_cc \\\r\n`pkg-config --cflags --libs protobuf` -ltensorflow_cc test.cpp\r\n\r\nto run test.cpp \r\nhave error \uff1a\r\n/tmp/ccPa2mDH.o: In function `main':\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::NewRootScope()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:11: undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:11: undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::ops::MatMul::MatMul(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input, tensorflow::ops::MatMul::Attrs const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:15: undefined reference to `tensorflow::Scope::~Scope()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::ClientSession(tensorflow::Scope const&)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::LogMessage(char const*, int, int)'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::~ClientSession()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::~Scope()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:13: undefined reference to `tensorflow::Scope::~Scope()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:19: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:21: undefined reference to `tensorflow::internal::LogMessage::~LogMessage()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:17: undefined reference to `tensorflow::ClientSession::~ClientSession()'\r\n/usr/Data/newdataset/mask-rcnn/tf_c/tensorflow-1.8.0/test.cpp:9: undefined reference to `tensorflow::Scope::~Scope()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::TfCheckOpHelper[abi:cxx11](tensorflow::Status, char const*)':\r\n/usr/local/include/tf/tensorflow/core/lib/core/status.h:127: undefined reference to `tensorflow::TfCheckOpHelperOutOfLine[abi:cxx11](tensorflow::Status const&, char const*)'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::TensorShapeRep::~TensorShapeRep()':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:510: undefined reference to `tensorflow::TensorShapeRep::DestructorOutOfLine()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::core::RefCounted::~RefCounted()':\r\n/usr/local/include/tf/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/usr/local/include/tf/tensorflow/core/lib/core/refcount.h:79: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::Tensor::operator=(tensorflow::Tensor const&)':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:170: undefined reference to `tensorflow::Tensor::CopyFromInternal(tensorflow::Tensor const&, tensorflow::TensorShape const&)'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::Input::Input(tensorflow::Output const&)':\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:204: undefined reference to `tensorflow::Tensor::Tensor()'\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:204: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::Input::Initializer::~Initializer()':\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:105: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::Input::~Input()':\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:97: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::TensorShape::TensorShape(std::initializer_list<long long>)':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:291: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::TensorShapeBase(std::initializer_list<long long>)'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::Input::Initializer::Initializer<float, void>(std::initializer_list<float> const&)':\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:138: undefined reference to `tensorflow::Tensor::Tensor()'\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::Tensor(tensorflow::DataType, tensorflow::TensorShape const&)'\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:141: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/usr/local/include/tf/tensorflow/cc/framework/ops.h:138: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<unsigned long, unsigned long>(unsigned long const&, unsigned long const&, char const*)':\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::TTypes<float, 2ul, long>::Tensor tensorflow::Tensor::tensor<float, 2ul>()':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:569: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'\r\n/tmp/ccPa2mDH.o: In function `tensorflow::TTypes<float, 1ul, long>::Tensor tensorflow::Tensor::shaped<float, 1ul>(tensorflow::gtl::ArraySlice<long long>)':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:666: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType) const'\r\n/tmp/ccPa2mDH.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizes<2>() const':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:463: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'\r\n/tmp/ccPa2mDH.o: In function `void tensorflow::Tensor::FillDimsAndValidateCompatibleShape<1ul>(tensorflow::gtl::ArraySlice<long long>, std::array<long, 1ul>*) const':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:630: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor.h:636: undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'\r\n/tmp/ccPa2mDH.o: In function `Eigen::DSizes<long, 2> tensorflow::TensorShape::AsEigenDSizesWithPadding<2>() const':\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:470: undefined reference to `tensorflow::TensorShape::CheckDimsAtLeast(int) const'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:473: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:474: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(int) const'\r\n/usr/local/include/tf/tensorflow/core/framework/tensor_shape.h:476: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dims() const'\r\n/tmp/ccPa2mDH.o: In function `void std::_Destroy<tensorflow::Tensor>(tensorflow::Tensor*)':\r\n/usr/include/c++/5/bits/stl_construct.h:93: undefined reference to `tensorflow::Tensor::~Tensor()'\r\n/tmp/ccPa2mDH.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long long, long long>(long long const&, long long const&, char const*)':\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:186: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:187: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\n/usr/local/include/tf/tensorflow/core/platform/default/logging.h:184: undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'\r\ncollect2: error: ld returned 1 exit status\r\n", "comments": ["Got the same problem!", "Could you resovel it?", "I have a feeling this is due to missing `-D_GLIBCXX_USE_CXX11_ABI=1` when building `test.cpp`.\r\nBut looks like I completely missed this issue.\r\n\r\nI will close this issue now, but please file a new issue if you still see this happening with 2.2, or master."]}, {"number": 24560, "title": "TFLite Android example: obtain input tensor shape from tflite model", "body": "To free classifier from hard-coded constants, such that when switching from inception to mobilenet one doesn't need to update associated constants. Makes the class more reusable.\r\n\r\nRequires pull request #24256 or equivalent change to work in Android Studio", "comments": ["Nagging Reviewer @andrewharp: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@zaichang could you please resolve the conflicts? Thanks !", "It seems that the conflicts are due to the example having been moved to the new TensorFlow examples repo at [https://github.com/tensorflow/examples](https://github.com/tensorflow/examples) along with major changes in the classes involved.\r\n\r\nDisappointingly, the new code still relies on hard-coded values instead of inferring them from the tflite model itself, so for my purpose (dynamic switching of model/weights) it's far from ideal.\r\n\r\nIn any case closing this as it's no longer applicable."]}, {"number": 24559, "title": "[Perfomance]Dilated/Atrous Conv implementation with cudnn", "body": "**System information**\r\n- TensorFlow version (TF 1.12.0):\r\n- Are you willing to contribute it (Yes):\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe [doc](https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d) and the [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L947) shows that dilated conv is implemented with traival ops(space_to_batch->regular conv->batch_to_space in python level. And the python code does not pass dilations parameters to lower level code. However both the registered [dilated conv ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L265), [gpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2901) and [cpu code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions.h#L1568) are ready to handle with dilatied conv. It is easy to implement dialted conv with cudnn by modifying the python level code.\r\n\r\n**Will this change the current api? How?**\r\nNO.\r\n\r\n**Who will benefit with this feature?**\r\nwho deploy the model which has dilated conv\r\n\r\n**Any Other info.**\r\ntime of my model reduces from 350ms to 216ms on 1080ti.\r\n", "comments": ["I am not the correct assignee for this. Please redirect to performance team.", "I will commit the code if you approve the request.", "@MlWoo,\r\nSorry for the delayed response. [This comment](https://github.com/tensorflow/tensorflow/pull/25025#issuecomment-473283030) from the PR states:\r\n\r\n> I am unable to see exactly what the gain is from doing this. Currently convolution supports dilation as a parameter but this code would not have a backwards pass which works on a CPU.\r\n\r\nAlso, no one from the Developer Community seems to be interested in this feature. Can you please confirm if we can close this issue? Thanks!"]}, {"number": 24558, "title": "Unable to build tensorflow from source on windows,cuda 10,cudnn 7.3.1,python 3.6,compute 6.1", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.11\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.17.2\r\n- GCC/Compiler version (if compiling from source): MSVC 2015\r\n- CUDA/cuDNN version: cuda 10.0,cudnn 7.3.1,compute 6.1\r\n- GPU model and memory: GTX 1060 6GB GDDR6\r\n\r\n**Describe the problem**\r\n\r\nMy GPU is only recognized by CUDA 10 that is the reason i am trying to build it from source but every time i get the same error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. python ./configure.py\r\n2. Do you wish to build TensorFlow with nGraph support? [y/N]: n\r\n3. Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n4. Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0\r\n5. Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.3.1\r\n6. Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5, 7.0]: 6.1\r\n7. Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n8. Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n\r\nDidn't added the steps related to path of python and cuda.\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nI have also followed the steps from the below article but in my case whenever i add the patch file it says \r\n\r\nno such package \u2018@eigen_archive//\u2019: Traceback (most recent call last):\r\n File \u201cC:/tensorflow-build/tensorflow/third_party/repo.bzl\u201d, line 106\r\n _apply_patch(ctx, ctx.attr.patch_file)\r\n\r\nBelow is the link of the article that i have referred.\r\n\r\nhttps://medium.com/@amsokol.com/update-1-how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-and-c2e86fec9ef2\r\n\r\nERROR: C:/tensorflow-build/tensorflow/tensorflow/core/kernels/BUILD:4567:1: C++ compilation of rule '//tensorflow/core/kernels:multinomial_op_gpu' failed (Exit 1): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/biswa/_bazel_biswa/j7bi4x5j/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/tensorflow-v1.1/Scripts/python.exe\r\n    SET PYTHON_LIB_PATH=C:/tensorflow-v1.1/Lib/site-packages\r\n    SET TEMP=C:\\Users\\biswa\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_CUDA_VERSION=10.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\biswa\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -x cuda -DGOOGLE_CUDA=1 -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true -DGOOGLE_CUDA=1 -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/multinomial_op_gpu/multinomial_op_gpu.cu.o /c tensorflow/core/kernels/multinomial_op_gpu.cu.cc\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(164): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/util/Memory.h(179): warning: calling a __host__ function from a __host__ __device__ function is not allowed\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/arch/CUDA/Half.h(212): error: more than one instance of overloaded function \"__hadd\" matches the argument list:\r\n            function \"__hadd(int, int)\"\r\n            function \"__hadd(__half, __half)\"\r\n            argument types are: (const Eigen::half, const Eigen::half)\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/products/Parallelizer.h(20): warning: variable \"m_maxThreads\" was set but never used\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\eigen\\src/Core/ArrayWrapper.h(94): warning: __declspec attributes ignored\r\n\r\nexternal/com_google_absl\\absl/strings/string_view.h(501): warning: expression has no effect\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(55): warning: integer conversion resulted in a change of sign\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(309): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/arena_impl.h(310): warning: integer conversion resulted in a change of sign\r\n\r\nexternal/protobuf_archive/src\\google/protobuf/map.h(1025): warning: invalid friend declaration\r\n\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(34): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(38): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=float, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=float]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsHalf.h(42): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::VALUE]\"\r\n(2096): here\r\n            instantiation of \"Eigen::internal::igamma_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(120): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::DERIVATIVE]\"\r\n(2102): here\r\n            instantiation of \"Eigen::internal::igamma_der_a_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::igamma_der_a(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(135): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(651): warning: missing return statement at end of non-void function \"Eigen::internal::igammac_cf_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igammac_cf_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(855): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/SpecialFunctionsImpl.h(712): warning: missing return statement at end of non-void function \"Eigen::internal::igamma_series_impl<Scalar, mode>::run [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n          detected during:\r\n            instantiation of \"Scalar Eigen::internal::igamma_series_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(863): here\r\n            instantiation of \"Scalar Eigen::internal::igamma_generic_impl<Scalar, mode>::run(Scalar, Scalar) [with Scalar=double, mode=Eigen::internal::SAMPLE_DERIVATIVE]\"\r\n(2108): here\r\n            instantiation of \"Eigen::internal::gamma_sample_der_alpha_retval<Eigen::internal::global_math_functions_filtering_base<Scalar, void>::type>::type Eigen::numext::gamma_sample_der_alpha(const Scalar &, const Scalar &) [with Scalar=double]\"\r\nc:\\users\\biswa\\_bazel_biswa\\j7bi4x5j\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\src/SpecialFunctions/arch/CUDA/CudaSpecialFunctions.h(154): here\r\n\r\n1 error detected in the compilation of \"C:/Users/biswa/AppData/Local/Temp/nvcc_inter_files_tmp_dir/multinomial_op_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 452.097s, Critical Path: 198.16s\r\nINFO: 1906 processes: 1906 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["duplicate #24233 ", "The issue has been resolved.Looks like there was a problem with the eigen patch file.Also i have to upgrade the version of bazel.\r\nAfter doing this i was able to create tensorflow pip package.\r\nClosing this issue.\r\nThanks!", "Good to hear that you were able to solve the issue. What version of bazel did you use?", "I have used bazel version 0.17.2\r\nThanks.\r\n\r\n"]}, {"number": 24557, "title": "Tensorflow autograph problem in creating AdamOptimizer", "body": "## env\r\nwin7 x64\r\ntf.version >= 1.12, cpu only\r\npython 3.6.6\r\n## code\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python import autograph\r\n\r\n\r\n@autograph.convert()\r\ndef prepare_for_cost(ys, ps, limit=tf.constant(.9)):\r\n    l = len(ys)\r\n    nys = []\r\n    nps = []\r\n    for i in range(l):\r\n        y = ys[i]\r\n        p = ps[i]\r\n        if p[1] > limit:\r\n            nys.append(y)\r\n            nps.append(p)\r\n        else:\r\n            nys.append([0.0, 1.0])\r\n            nps.append([1.0, 0.0])\r\n    nys_list = autograph.stack(nys, tf.float32)\r\n    nps_list = autograph.stack(nps, tf.float32)\r\n    return nys_list, nps_list\r\n\r\n\r\nwith tf.Graph().as_default() as g, tf.Session(graph=g) as sess:\r\n    xs = tf.placeholder(tf.float32, [None, 2])\r\n    w = tf.Variable(tf.random_normal([2, 2]))\r\n    ps = tf.matmul(xs, w)\r\n    ys = tf.placeholder(tf.float32, [None, 2])\r\n\r\n    nys, nps = prepare_for_cost(ys, ps)\r\n    nys.set_shape(tf.TensorShape((None, 2)))\r\n    nps.set_shape(tf.TensorShape((None, 2)))\r\n\r\n    cost = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits_v2(labels=nys, logits=nps))\r\n    train_op = tf.train.AdamOptimizer(0.01).minimize(cost)\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run([train_op], feed_dict={ys: [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0]], xs: [[.2, .8], [.7, .3], [.1, .92]]})\r\n```\r\n\r\nerror stacks below.  what's more, there's a little bug when using autograph.tensor_list to create a list, it's about none check like ```if tensor: ...```\r\n\r\nthe key tensor with variant type is:\r\nTensor(\"gradients/prepare_for_cost/while/Switch_2_grad/b_switch:0\", shape=(), dtype=variant)\r\n\r\n```\r\nFile \"D:/project/myshpy/test/myshtest/common/CommonPy.py\", line 36, in <module>\r\n        train_op = tf.train.AdamOptimizer(0.01).minimize(cost)\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 400, in minimize\r\n        grad_loss=grad_loss)\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 519, in compute_gradients\r\n        colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 630, in gradients\r\n        gate_gradients, aggregation_method, stop_gradients)\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 858, in _GradientsHelper\r\n        loop_state.PostProcessing()\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 1404, in PostProcessing\r\n        grad_val = constant_op.constant(0, dtype=dtype, shape=shape)\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 208, in constant\r\n        value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n      File \"D:\\soft\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 477, in make_tensor_proto\r\n        (dtype, nparray.dtype, values))\r\n    TypeError: Incompatible types: <dtype: 'variant'> vs. int32. Value is 0\r\n```", "comments": ["The same problem happens to me.", "Side note: when replicating the bug, I found a strange error caused by `limit=tf.constant(.9)`. Python mechanics can lead to unexpected behavior using placing non-primitive values for default arguments, in this case a constant created outside the default graph.", "It seems like the way we handle empty lists doesn't play well with `AdamOptimizer`. Until we get this sorted, please use this alternate way to initialize `nys` and `nps`:\r\n\r\n    nys = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    nps = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n\r\nThat removed the error in my tests - could you verify if it works in yours?", "Well, I have turned to dynamic graph using \"Eager Execution\" and have my job done.\r\nI've tried your way and meet a new error, which raised before autograph code when I create a placehoder using tf.placeholder :\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_1' with dtype float and shape [?,2]\r\n```\r\n", "Yes, eager will work too, especially for smaller models!\r\n\r\nI didn't see an error due to placeholders in my tests - anyway, here's the code I ran:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import autograph\r\n\r\n\r\n@autograph.convert()\r\ndef prepare_for_cost(ys, ps, limit):\r\n    l = len(ys)\r\n    nys = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    nps = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\r\n    for i in range(l):\r\n        y = ys[i]\r\n        p = ps[i]\r\n        if p[1] > limit:\r\n            nys.append(y)\r\n            nps.append(p)\r\n        else:\r\n            nys.append([0.0, 1.0])\r\n            nps.append([1.0, 0.0])\r\n    nys_list = autograph.stack(nys)\r\n    nps_list = autograph.stack(nps)\r\n    return nys_list, nps_list\r\n\r\n\r\nwith tf.Graph().as_default() as g, tf.Session(graph=g) as sess:\r\n    xs = tf.placeholder(tf.float32, [None, 2])\r\n    w = tf.Variable(tf.random_normal([2, 2]))\r\n    ps = tf.matmul(xs, w)\r\n    ys = tf.placeholder(tf.float32, [None, 2])\r\n\r\n    nys, nps = prepare_for_cost(ys, ps, limit=tf.constant(.9))\r\n    nys.set_shape(tf.TensorShape((None, 2)))\r\n    nps.set_shape(tf.TensorShape((None, 2)))\r\n\r\n    cost = tf.reduce_mean(\r\n        tf.nn.softmax_cross_entropy_with_logits_v2(labels=nys, logits=nps))\r\n    train_op = tf.train.AdamOptimizer(0.01).minimize(cost)\r\n    sess.run(tf.global_variables_initializer())\r\n    \r\n    print('Before: ', sess.run(w))\r\n    \r\n    sess.run([train_op], feed_dict={ys: [[0.0, 1.0], [1.0, 0.0], [0.0, 1.0]], xs: [[.2, .8], [.7, .3], [.1, .92]]})\r\n    \r\n    print('After: ', sess.run(w))\r\n```"]}, {"number": 24556, "title": "AlibabaCloud OSS support", "body": "AlibabaCloud Object Storage Service(OSS) is one of the most widely used cloud stroage services in the world. Could I ask for OSS support with tensorflow?", "comments": ["@oss-developer The file systems and frameworks are now supported by TensorFlow's SIG IO and discussions are mostly done through:\r\nGoogle Group: https://groups.google.com/a/tensorflow.org/forum/#!forum/io\r\nRepository: https://github.com/tensorflow/io", "@yongtang , May I know what difference between https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/platform and https://github.com/tensorflow/io supports external-filesystems?", "@oss-developer The https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/platform consists of a very minimal set of file systems, some are for historical reasons. Most of the file systems and streaming frameworks used to be provided through `tf.contrib` (e.g., Ignite File System, Kafka Streaming, etc).\r\n\r\nStarting TF 2.0, `tf.contrib` will be shifted to different SIGs for maintenance. SIG I/O (https://github.com/tensorflow/io) focus on existing and future file systems and streaming frameworks support.\r\n\r\nI have moved the issue to https://github.com/tensorflow/io/issues/41, so will close this issue now.\r\n\r\nDiscussion could be continued in SIG IO (https://github.com/tensorflow/io/issues/41) and SIG IO Google Group (https://groups.google.com/a/tensorflow.org/forum/#!forum/io).\r\n\r\nThanks for the interests and support!\r\n\r\n\r\n"]}, {"number": 24555, "title": "Error loading Keras model with custom layer in JSON format", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI was following the Keras tutorial on TF website and I was trying out my custom layer, which was basically the same as in the tutorial.\r\n```\r\nclass MyLayer(layers.Layer):\r\n\r\n  def __init__(self, output_dim, **kwargs):\r\n    self.output_dim = output_dim\r\n    super(MyLayer, self).__init__(**kwargs)\r\n\r\n  def build(self, input_shape):\r\n    shape = tf.TensorShape((input_shape[1], self.output_dim))\r\n    # Create a trainable weight variable for this layer.\r\n    self.kernel = self.add_weight(name='kernel',\r\n                                  shape=shape,\r\n                                  initializer='uniform',\r\n                                  trainable=True)\r\n    # Be sure to call this at the end\r\n    super(MyLayer, self).build(input_shape)\r\n\r\n  def call(self, inputs):\r\n    return tf.matmul(inputs, self.kernel)\r\n\r\n  def compute_output_shape(self, input_shape):\r\n    shape = tf.TensorShape(input_shape).as_list()\r\n    shape[-1] = self.output_dim\r\n    return tf.TensorShape(shape)\r\n\r\n  def get_config(self):\r\n    base_config = super(MyLayer, self).get_config()\r\n    base_config['output_dim'] = self.output_dim\r\n    return base_config\r\n\r\n  @classmethod\r\n  def from_config(cls, config):\r\n    return cls(**config)\r\n\r\n\r\nmodel = tf.keras.Sequential([\r\n    MyLayer(10),\r\n    layers.Activation('softmax')])\r\n\r\n# The compile step specifies the training configuration\r\nmodel.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ndataset = get_dataset(*get_train_data()) # custom functions\r\n\r\nval_dataset = get_dataset(*get_val_data())\r\n\r\ntest_dataset = get_dataset(*get_test_data())\r\n\r\n# Trains for 5 epochs.\r\nmodel.fit(dataset, steps_per_epoch=30, epochs=5, callbacks=callbacks, validation_data=val_dataset)\r\nmodel.evaluate(test_dataset, steps=30)\r\n\r\nmodel.save_weights('./weights/model1')\r\njson_string = model.to_json()\r\n\r\npprint(json.loads(json_string))\r\n\r\nfresh_model = tf.keras.models.model_from_json(json_string, custom_objects={'MyLayer':MyLayer})\r\nfresh_model.load_weights('./weights/model1')\r\n\r\nfresh_model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\r\n              loss='categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nfresh_model.evaluate(test_dataset, steps=30)\r\n```\r\nHowever, I got the following error when I tried to run it and I could not find any related issues online.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-67ef4d13fb1e>\", line 1, in <module>\r\n    runfile('/Users/ray/WorkSpace/machinelearning/tensorflow-learn/custom_layer.py', wdir='/Users/ray/WorkSpace/machinelearning/tensorflow-learn')\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/Users/ray/WorkSpace/machinelearning/tensorflow-learn/custom_layer.py\", line 86, in <module>\r\n    metrics=['accuracy'])\r\n  File \"/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 600, in compile\r\n    skip_target_weighing_indices)\r\n  File \"/Users/ray/miniconda3/envs/ml/lib/python2.7/site-packages/tensorflow/python/keras/engine/training.py\", line 134, in _set_sample_weight_attributes\r\n    self.output_names, sample_weight_mode, skip_target_weighing_indices)\r\nAttributeError: 'Sequential' object has no attribute 'output_names'\r\n```\r\n\r\n**Describe the expected behavior**\r\nIf the tutorial was correct, the code should run without any bugs and the evaluation results should be the same after loading.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@larry0123du Can you provide the minimal full code sample that could reproduce the error? The code you pasted is not complete with `get_dataset` and `callbacks` missing.", "Sure, my get_dataset is as follows:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef get_train_data():\r\n    # training data\r\n    data = np.random.random((1000, 32))\r\n\r\n    data = tf.cast(data, tf.float32)\r\n\r\n    labels = np.random.random((1000, 10))\r\n\r\n    labels = tf.cast(labels, tf.float32)\r\n\r\n    return data, labels\r\n\r\ndef get_val_data():\r\n    # validation data\r\n    val_data = np.random.random((100, 32))\r\n\r\n    val_data = tf.cast(val_data, tf.float32)\r\n\r\n    val_labels = np.random.random((100, 10))\r\n\r\n    val_labels = tf.cast(val_labels, tf.float32)\r\n\r\n    return val_data, val_labels\r\n\r\ndef get_test_data():\r\n    # test data\r\n    test_data = np.random.random((100, 32))\r\n\r\n    test_data = tf.cast(test_data, tf.float32)\r\n\r\n    test_labels = np.random.random((100, 10))\r\n\r\n    test_labels = tf.cast(test_labels, tf.float32)\r\n\r\n    return test_data, test_labels\r\n\r\ndef get_dataset(data, labels):\r\n    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\r\n    dataset = dataset.batch(32)\r\n    dataset = dataset.repeat()\r\n    return dataset\r\n```\r\nand the callback is\r\n```python\r\ncallbacks = [\r\n    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss')\r\n]\r\n```\r\nLet me know if you need anything else!", "Hi @larry0123du, this issue should be fixed in the latest nightly, I'm able to run the code successfully"]}, {"number": 24554, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64-bit version: 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0 (gpu)\r\n- Python version: 3.6.8 64-bit\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0.130/cuDNN 7.4.2.24\r\n- GPU model and memory: Nvidia GeForce 940MX 4GB\r\n- GPU Drivers: 417.35\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen importing tensorflow in python interpreter ImportError is encountered.\r\n`CUDA toolkit 10.0` is installed & tested on visual studio using sample project builds.\r\n`cuDNN 7.4.2.24` is installed & extracted in `C:\\tools\\` with `C:\\tools\\cuda\\bin` containing `cudnn64_7.dll`\r\n```cmd\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%\r\n```\r\nI haven't installed the following as they were required for installation from source rather than binary, although the newer versions of both (2017) are installed\r\n```\r\nMicrosoft Visual C++ 2015 Redistributable Update 3\r\nMicrosoft Build Tools 2015 Update 3\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```py\r\nimport tensorflow as tf\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Danis\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["#22872 seems to bear the closest resemblance but I'd still like confirmation for tensorflow 1.12 as it was built against CUDA 10.0 (or maybe it's 1.13 onwards)?", "Based on Nvidia's [Developer Guide](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html) for cuDNN:\r\n\r\nCopy `<installpath>\\cuda\\bin\\cudnn64_7.dll` to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin`.\r\nCopy `<installpath>\\cuda\\ include\\cudnn.h` to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\include`.\r\nCopy `<installpath>\\cuda\\lib\\x64\\cudnn.lib` to `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\lib\\x64`.\r\n\r\nwhere `<installpath>` was `C:\\tools\\` in my case, while in default case `cudnn-10.0-windows10-x64-v7.4.2.24`\r\n\r\n```cmd\r\nSET CUDA_PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\r\n```\r\n\r\nEven this didn't help resolve the issue though", "Just to confirm if the issue was with `tensorflow-gpu`==1.12.0, I uninstalled `tensorflow-gpu` from `pip` & installed `tensorflow`==1.12.0 instead to test if that worked. There were no `ImportError`s or any runtime errors from running sample code from site.", "Apparently `tensorflow-gpu`==1.13 [ as on date of comment `tf-nightly-gpu`==1.13.0dev20181225] seems to work without `ImportError` for `CUDA toolkit` 10.0 with `cuDNN` 7.4.2.24. While `tensorflow-gpu`==1.12.0 seems to expect `CUDA` toolkit 9.0 or 9.2 [Have read about `CUDA` 9.0 being supported since `tensorflow`==1.5+ but haven't actually tested it]. \r\n\r\nThere seems to be a lot of depreciated code in what I was using, though no such warning came when running `tensorflow`==1.12.0. The Example was an XLA compile API based example.\r\n\r\nFor anyone stumbling along later:\r\n\r\ncheck for \r\n - `nvcuda.dll` in your `c:\\windows\\system32` &/or `c:\\windows\\sysWOW64`. Should be present beforehand if you've installed the [`GPU drivers`](https://www.geforce.com/drivers) & your GPU supports `CUDA`.\r\n - `msvcp140.dll` in your `c:\\windows\\system32` &/or `c:\\windows\\sysWOW64`. Obtained from [`Microsoft Visual C++ 2015 Redistributable Update 3`](https://visualstudio.microsoft.com/vs/older-downloads/) though I had a recent version installed beforehand (2017) which works as well. \r\n -  `cudart64_100.dll`[for CUDA v10.0] in your CUDA installation directory `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin`. Obtained from [`CUDA Toolkit`](https://developer.nvidia.com/cuda-downloads) installation.\r\n - `cudnn64_7.dll` in your CUDA installation directory `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin`. Obtained from [`cuDNN`](https://developer.nvidia.com/rdp/form/cudnn-download-survey) installation."]}, {"number": 24553, "title": "Fix ValueError in tf.nn.nce_loss when weights and biases are float64", "body": "This fix tries to fix the issue raised in #24547 where ValueError was thrown when weights and biases are tf.float64\r\n\r\nThis fix fixes #24547\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24552, "title": "Removed .combat.v1. method in the example.", "body": "As a resolution to #24524 , old code is removed to get the correct results as show in the tensorflow [tutorial](https://www.tensorflow.org/tutorials/estimators/cnn#load_training_and_test_data)", "comments": ["Nagging Reviewer @xiejw: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@karmel do you know who owns the tutorial update? I had little knowledge about the new API in v2 module. ", "This particular tutorial has been auto-updated to 2.0-compatible code, so the compat.v1 is correct; you can try running with the 2.0 preview to fix the error linked above (`pip install --upgrade tf-nightly-2.0-preview`). That said, I believe this particular example will eventually be removed, as it is no longer current. Consider taking a look at the [official MNIST](https://github.com/tensorflow/models/tree/master/official/mnist#mnist-in-tensorflow)."]}, {"number": 24551, "title": "Close code environment in docstring for adjoint", "body": "This PR adds the delimiter for the code environment in the docs for tf.linalg.adjoint. Without this, the docs were rendered inappropriately as shown below:\r\n\r\n![image](https://user-images.githubusercontent.com/23639302/50403262-2d355d80-07c3-11e9-80af-9de4b0c83bff.png)\r\n", "comments": []}, {"number": 24550, "title": "Add python API for some quantized operations.", "body": "", "comments": []}, {"number": 24549, "title": "Building from source with bazel: no such package '@local_config_cuda//cuda'", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13 \r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.21.0\r\n- CUDA/cuDNN version: 10.0/7.4.2\r\n- GPU model and memory: GTX 960M\r\n\r\n\r\n**Describe the problem**\r\nI followed the instructions for building Tensorflow from source on the website. But the command 'bazel build...' gives me these errors:\r\n\r\n> C:\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow/.bazelrc\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for Visual C++ through registry\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Looking for default Visual C++ installation directory\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: 'PROGRAMFILES(X86)' environment variable is not set, using 'C:\\Program Files (x86)' as default\r\nDEBUG: C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\")\r\n        File \"C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 220, in find_msvc_tool\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\")\r\n        File \"C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 220, in find_msvc_tool\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 1,004s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\nI'm new to Tf so I'm kind of lost.\r\nIs there anything I can do ? \r\n\r\nThanks.\r\n", "comments": ["Tensorflow 1.13 does not seem to have been officially released, I am not sure that it can be successfully compiled with CUDA10.0.According to your error log, it seems that the location of the MSVC compiler is not properly configured.In addition, as far as I know, using MSVC2017 is likely to encounter some problems, it is recommended to use MSVC2015 Update3.It is recommended to read the guide (https://www.tensorflow.org/install/source_windows) carefully and ensure that all required software is properly installed.\r\nWhat I can tell you is that after disabling XLA I successfully compiled the master branch and used CUDA10.In addition, the bazel version I used is 0.20.0.I wish you success, thank you.\r\n", "I had actually tried also with MSVC2015 Update 3, with the same result.\r\nI tried again with **MSVC2015 Update 3** and **bazel 0.20.0**. The same error seems to occur.\r\nHere is the output, with the configuration I made:\r\n\r\n> C:\\tensorflow>python ./configure.py\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nnul\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nINFO: Invocation ID: 2e4f3e6d-1ac4-4089-8040-6cc98c820303\r\nYou have bazel 0.20.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python36-32\\python.exe]:\r\nFound possible Python library paths:\r\n  C:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages]\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0\r\nPlease specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apacha Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n> C:\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\tensorflow/.bazelrc\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\nINFO: Invocation ID: d1c653f2-e12e-4a46-b29e-3d46940aff08\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in C:/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\")\r\n        File \"C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 230, in find_msvc_tool\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': in C:/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1556\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1395, in _create_local_cuda_repository\r\n                find_cc(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 239, in find_cc\r\n                _get_msvc_compiler(repository_ctx)\r\n        File \"C:/tensorflow/third_party/gpus/cuda_configure.bzl\", line 156, in _get_msvc_compiler\r\n                find_msvc_tool(repository_ctx, vc_path, \"cl.exe\")\r\n        File \"C:/users/maxim/_bazel_maxim/xv6zejqw/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 230, in find_msvc_tool\r\n                repository_ctx.path((vc_path + \"\\\\Tools\\\\MSVC\")).readdir()\r\nC:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)\r\nINFO: Elapsed time: 12,161s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n    Fetching @local_config_cuda; fetching\r\n\r\nDoes anyone have an idea of what's going wrong ?\r\n\r\nThanks.", "CUDA 10 will be officially supported with TensorFlow 2.0 \r\nI would recommend you to switch to cuda 9.0 which is current official supported version if possible.\r\n", "So I tried with CUDA 9.0, and it's still not working. \r\nI have the same errors: \r\n> Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in C:/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda'\r\n\r\n> error loading package 'tensorflow/tools/pip_package': in C:/tensorflow/tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda'\r\n\r\nWeird thing though, even after going back to MSVC 2015 Update 3, the error message still says:\r\n> C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC (No such file or directory)\r\n\r\nI tried again with MSVC 2017 to be sure, still the exact same error.\r\n\r\nI don't know what's going on.\r\nDoes anyone have a clue ?\r\n\r\nThanks.", "Apologies for the delay in response. @Maxtoq  Is this still an issue for you? Can you share your progress till now? We can reopen this issue if required. Thanks!", "I still have the same issue. But I think the problem is that Bazel can't find the path of MSVC 2015 directories. \r\nIt was looking in the installation directory of Microsoft Visual Studio 2017. I uninstalled it and now Bazel just can't find any path for MSVC.\r\nI posted my issue on the Bazel Stackoverflow to see if they can help.\r\nI don't think we need to reopen this issue, unless someone has an idea. But I think the problem is with Bazel and MSVC, not Tensorflow.\r\nThanks", "I found the solution. The problem was that I had badly set the environment variable BAZEL_VS (or BAZEL_VC). I followed the examples given here: https://docs.bazel.build/versions/0.21.0/install-compile-source.html#bootstrap-windows-bootstrap.", "I see. Thanks for sharing your solution."]}, {"number": 24548, "title": "The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The current version of Tensorflow installed via pip uses the AVX instruction set at compile time.This means that your CPU needs to support the AVX instruction set. This instruction set is supported from the second generation of Intel Core CPUs (codenamed SandyBridge). You can compile a Tensorflow from the source that does not use the AVX instruction set. Or find an already compiled one on the internet.", "Closing this issue since @rootkitchao has answered the question correctly. Feel free to post if have any issues in building TF from sources. Thanks! ", "@rootkitchao I'd like to try out your suggestion, but could I get some more details please? I'm new to such environments, and I am getting the same error.\r\n- I have a GPU, and my TF is installed with GPU-support, so why is it trying to access/match instruction sets with my CPU?\r\n- If it's still necessary for me to fix this CPU issue, where can I find a pre-compiled TF with SSE2 or SSD4_1 instruction support? I couldn't find it via google...\r\nThanks!\r\n", "For others' reference, here is a useful repository of [custom TensorFlow builds](https://github.com/lakshayg/tensorflow-build). I tried installing one of their wheels, but then got \"import tf; Illegal Instruction\" so it's still a figuring-out-in-process, but maybe you'll have better luck with it", "I entered the same problem. The readme only says `pip install tensorflow`, but it simply does not work. \r\nAnd from this issue, I don't get the solution.\r\nCan you give clear install instructions (I'm on Linux, opensuse)?\r\n", "Try Anaconda. It has TensorFlow distribution for old CPU.", "FYI, Installing Anaconda works on my old computer.", "> The current version of Tensorflow installed via pip uses the AVX instruction set at compile time.This means that your CPU needs to support the AVX instruction set. This instruction set is supported from the second generation of Intel Core CPUs (codenamed SandyBridge). **You can compile a Tensorflow from the source that does not use the AVX instruction set. Or find an already compiled one on the internet.**\r\n\r\n@rootkitchao , could you tell me How do i can use already compiled tensorflow that dont used AVX instruction. \r\nMy cpu seems to not supporting AVX instruction, even it throws dll error while using 1.13.2 or lower version. \r\nThanks for the response ", "I was able to install TF with anaconda on an old Linux system (following the instructions at https://www.anaconda.com/tensorflow-in-anaconda/)\r\n```Shell\r\n$> conda create -n tensorflow_env tensorflow=2\r\n$> conda activate tensorflow_env\r\n```\r\n\r\n```Python\r\n>>> \r\n(tensorflow_env) mehdi@moneta:~> python\r\nPython 3.7.7 (default, Mar 26 2020, 15:48:22) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n.\r\n.\r\n.\r\n>>> msg = tf.constant(f\"Hello World from {tf.__version__}\")\r\n>>> print(msg)\r\ntf.Tensor(b'Hello World from 2.1.0', shape=(), dtype=string)\r\n```"]}, {"number": 24547, "title": "ValueError while using nce_loss with weights and biases of dtype tf.float64", "body": "ValueError while I'm trying to use nce_loss with weights and biases of dtype tf.float64\r\n\r\nmy pseudo code\r\n```\r\ntf.nn.nce_loss(\r\n    weights = nce_weights, (dtype=tf.float64)\r\n     biases = nce_biases, (dtype=tf.float64)\r\n      ......\r\n```\r\nthe error:\r\n```\r\n  ...(ignored)...\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1242, in nce_loss\r\n    name=name)\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1124, in _compute_sampled_logits\r\n    true_logits -= math_ops.log(true_expected_count)\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 970, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1014, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1104, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"##\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 947, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"nce_loss/Log:0\", shape=(?, 1), dtype=float32)'\r\n```\r\nAfter some tracing back, I located the problem at the function \"_compute_sampled_logits\" in the file \"nn_impl.py\".  The Problem came from these codes:\r\n```\r\n    ...\r\n    if subtract_log_q:\r\n      # Subtract log of Q(l), prior probability that l appears in sampled.\r\n      true_logits -= math_ops.log(true_expected_count)\r\n      sampled_logits -= math_ops.log(sampled_expected_count)\r\n    ...\r\n```\r\nWhere the `true_expected_count` was generated automatically with default dtype float32. But the `true_logits` has the same dtype float64 as my input. It came into crashes when trying the subtraction. Finally, I fixed this by adding somthing like these:\r\n```\r\n    if subtract_log_q:\r\n      # Subtract log of Q(l), prior probability that l appears in sampled.\r\n      if true_logits.dtype == dtypes.float64:\r\n        true_expected_count = math_ops.cast(true_expected_count, dtypes.float64)\r\n      true_logits -= math_ops.log(true_expected_count)\r\n      if sampled_logits.dtype == dtypes.float64:\r\n        sampled_expected_count = math_ops.cast(sampled_expected_count, dtypes.float64)\r\n      sampled_logits -= math_ops.log(sampled_expected_count)\r\n```\r\nI'm not very familiar with the TensorFlow\u2018s mechanism. But I noticed that the same logic is still exist in higher versions of TensorFlow. So I believe this bug still exists.\r\n\r\n**Environments**\r\n- tf version 1.8.0\r\n- python 3.6.0\r\n", "comments": ["Added a PR #24533 for the fix."]}, {"number": 24546, "title": "tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='g_b3') with bach size 1 error", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):anaconda3\r\n- TensorFlow version (use command below):1.9\r\n- Python version:3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):5.2\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nInternalError: cuDNN launch failure : input shape ([1,1,56,56])\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nInternalError: cuDNN launch failure : input shape ([1,1,56,56])\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ndef generator(z, batch_size, z_dim):\r\n    g_w1 = tf.get_variable('g_w1', [z_dim, 3136], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b1 = tf.get_variable('g_b1', [3136], initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g1 = tf.matmul(z, g_w1) + g_b1\r\n    g1 = tf.reshape(g1, [-1, 56, 56, 1])\r\n#     g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='g_b1')\r\n    g1 = tf.nn.relu(g1)\r\n\r\n    # Generate 50 features\r\n    g_w2 = tf.get_variable('g_w2', [3, 3, 1, z_dim/2], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b2 = tf.get_variable('g_b2', [z_dim/2], initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g2 = tf.nn.conv2d(g1, g_w2, strides=[1, 2, 2, 1], padding='SAME')\r\n    g2 = g2 + g_b2\r\n#     g2 = tf.contrib.layers.batch_norm(g2, epsilon=1e-5, scope='g_b2')\r\n    g2 = tf.nn.relu(g2)\r\n    g2 = tf.image.resize_images(g2, [56, 56])\r\n\r\n    # Generate 25 features\r\n    g_w3 = tf.get_variable('g_w3', [3, 3, z_dim/2, z_dim/4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b3 = tf.get_variable('g_b3', [z_dim/4], initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g3 = tf.nn.conv2d(g2, g_w3, strides=[1, 2, 2, 1], padding='SAME')\r\n    g3 = g3 + g_b3\r\n#     g3 = tf.contrib.layers.batch_norm(g3, epsilon=1e-5, scope='g_b3')\r\n    g3 = tf.nn.relu(g3)\r\n    g3 = tf.image.resize_images(g3, [56, 56])\r\n\r\n    # Final convolution with one output channel\r\n    g_w4 = tf.get_variable('g_w4', [1, 1, z_dim/4, 1], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b4 = tf.get_variable('g_b4', [1], initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g4 = tf.nn.conv2d(g3, g_w4, strides=[1, 2, 2, 1], padding='SAME')\r\n    g4 = g4 + g_b4\r\n    g4 = tf.sigmoid(g4)\r\n    \r\n    # Dimensions of g4: batch_size x 28 x 28 x 1\r\n    return g4\r\nz_dimensions = 100\r\nz_placeholder = tf.placeholder(tf.float32, [None, z_dimensions])\r\ngenerated_image_output = generator(z_placeholder, 1, z_dimensions)\r\n\r\nz_batch = np.random.normal(0, 1, [1, z_dimensions])\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    generated_image = sess.run(generated_image_output,\r\n                                feed_dict={z_placeholder: z_batch})\r\n    generated_image = generated_image.reshape([28, 28])\r\n    plt.imshow(generated_image, cmap='Greys')\r\n**Other info / logs**\r\nInternalError: cuDNN launch failure : input shape ([1,1,56,56])\r\n         [[Node: g_b1_1/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](g_b1_1/FusedBatchNorm-0-TransposeNHWCToNCHW-LayoutOptimizer, g_b1_1/Const, g_b1/beta/read, g_b1_1/Const_1, g_b1_1/Const_1)]]\r\n\r\nCaused by op 'g_b1_1/FusedBatchNorm', defined at:\r\n  File \"/home/humaolin/anaconda3/bin/ipython\", line 11, in <module>\r\n    sys.exit(start_ipython())\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 485, in mainloop\r\n    self.interact()\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 476, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-a711fc10a844>\", line 39, in <module>\r\n    generated_image_output = generator(z_placeholder, 1, z_dimensions)\r\n  File \"<ipython-input-3-a711fc10a844>\", line 6, in generator\r\n    g1 = tf.contrib.layers.batch_norm(g1, epsilon=1e-5, scope='g_b1')\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 183, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 650, in batch_norm\r\n    outputs = layer.apply(inputs, training=is_training)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 774, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 329, in __call__\r\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 703, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 158, in call\r\n    return super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 511, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 398, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 51, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\", line 54, in smart_cond\r\n    return true_fn()\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 384, in _fused_batch_norm_training\r\n    data_format=self._data_format)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py\", line 906, in fused_batch_norm\r\n    name=name)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3465, in _fused_batch_norm\r\n    is_training=is_training, name=name)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)\r\n  File \"/home/humaolin/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cuDNN launch failure : input shape ([1,1,56,56])\r\n         [[Node: g_b1_1/FusedBatchNorm = FusedBatchNorm[T=DT_FLOAT, data_format=\"NCHW\", epsilon=1.001e-05, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](g_b1_1/FusedBatchNorm-0-TransposeNHWCToNCHW-LayoutOptimizer, g_b1_1/Const, g_b1/beta/read, g_b1_1/Const_1, g_b1_1/Const_1)]]\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["The error arises due incorrect versions of cuda and cuDNN installed on your system.\r\nCan you please test against latest version of TensorFlow?\r\nNote: TF 1.12 comes with cuda 9 support and TF 1.13.rc1 supports cuda 10", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24545, "title": "Saver restore OOM with sharded=True", "body": "My model is saved with `tf.train.Saver(sharded=True)`. In predicting process, it has OOM problem when loading it in chief worker using `tf.train.Saver(sharded=True)`. However, it works fine using `tf.train.Saver(sharded=False)`.\r\ntf version: 1.12", "comments": []}, {"number": 24544, "title": "Error ValueError: Unknown field group_norm", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GTX 1060\r\n\r\n**Describe the current behavior**\r\nI have a Docker instance of Cuda and Tensorflow which I run with a nvidia container.\r\n\r\nThis container is being started with the following command: sudo docker run --runtime=nvidia -it -v $(pwd):/code --rm -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY -e QT_X11_NO_MITSHM=1 ce2d9c04faa2 bash\r\n\r\nWhen executing the following command in the container : python3 model_main.py --pipeline_config_path ./train_results/rcnn_inception3/faster_rcnn_inception_v2_coco.config --model_dir ./train_results/rcnn_inception3/ --logstderr\r\n\r\nI get the exception: \r\n\r\nTraceback (most recent call last):\r\n  File \"model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 354, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1207, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1234, in _train_model_default\r\n    input_fn, model_fn_lib.ModeKeys.TRAIN))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1075, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 1162, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/inputs.py\", line 488, in _train_input_fn\r\n    batch_size=params['batch_size'] if params else train_config.batch_size)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/builders/dataset_builder.py\", line 145, in build\r\n    num_parallel_calls=num_parallel_calls)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1040, in map\r\n    return ParallelMapDataset(self, map_func, num_parallel_calls)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2649, in __init__\r\n    use_inter_op_parallelism)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2611, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1860, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py\", line 864, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1794, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/builders/dataset_builder.py\", line 129, in process_fn\r\n    processed_tensors = transform_input_data_fn(processed_tensors)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/inputs.py\", line 465, in transform_and_pad_input_data_fn\r\n    model = model_builder.build(model_config, is_training=True)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/builders/model_builder.py\", line 122, in build\r\n    add_summaries)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/builders/model_builder.py\", line 389, in _build_faster_rcnn_model\r\n    frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\r\n  File \"/code/image_retrieval/tensorflow/models/research/object_detection/builders/hyperparams_builder.py\", line 218, in build\r\n    if hyperparams_config.HasField('group_norm'):\r\nValueError: Unknown field group_norm.\r\n\r\nMy config file is the default one, with changed pathes to fit the environment\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow training session being started\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I resolved this Error by deleting all my containers / images and creating a fresh docker container. Funny thing is, it worked a couple days ago. Seems like I broke something while trying things out."]}, {"number": 24543, "title": "g", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Closing this issue due to lack of information to work with. Please update the issue with all the information asked by the template and we will reopen it. Thanks!"]}, {"number": 24542, "title": "Step to Build for Centos 7", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Latest\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\nCan anyone give the steps for building this for Centos 7. ", "comments": ["Unfortunately TensorFlow does not provide official support for Centos. However you can install TensorFlow on Centos by reaching out to [community support](https://gist.github.com/thoolihan/28679cd8156744a62f88). "]}, {"number": 24541, "title": "How to resize segmentation map to the size of input image obtained from Deeplab?", "body": "Currently, Deeplab v3 return resized (small) image and its corresponding mask. But, I need segmentation mask for the original input image (large)? How can I achieve this?\r\n", "comments": ["@ahmedbilal \r\nPlease understand that Github is primarily for addressing bugs in installation and performance. StackOverflow (https://stackoverflow.com/) is better place to ask/discuss support questions like these. \r\nif there are any issues in the installation and performance, please fill the template for resolving. thank you\r\n\r\n", "@jvishnuvardhan Sorry. But, I got no response on Stackoverflow [How to resize segmentation mask obtained from Deeplab v3?](https://stackoverflow.com/questions/53910510/how-to-resize-segmentation-mask-obtained-from-deeplab-v3)", "Closing this support issue as GitHub is not a place for support. Thanks!", "> Currently, Deeplab v3 return resized (small) image and its corresponding mask. But, I need segmentation mask for the original input image (large)? How can I achieve this?\r\n\r\n@jvishnuvardhan I've been looking for the exact same thing.. were you able to resize the segmentation mask obtained from Deeplab v3?? Any leads would be helpful ..\r\n\r\n", "> > Currently, Deeplab v3 return resized (small) image and its corresponding mask. But, I need segmentation mask for the original input image (large)? How can I achieve this?\r\n> \r\n> @jvishnuvardhan I've been looking for the exact same thing.. were you able to resize the segmentation mask obtained from Deeplab v3?? Any leads would be helpful ..\r\n\r\nHi. I was able to resize it but don't recall how. Probably, I had used numpy resizing methods https://docs.scipy.org/doc/numpy/reference/generated/numpy.resize.html"]}, {"number": 24540, "title": "How to use the Indexdslice for sparse update \uff1f", "body": "Hi~everyone :)\r\nI\u2018m try to use  sparsed gradients to the apply_gradient func and I have turn my gradients to IndexdSlice:\r\n`grads[i] = tf.IndexedSlices(values = tf.gather_nd(grads[i], idx), indices = idx, dense_shape = tf.identity(grads[i].get_shape()))`\r\nand with vars_list,\r\n`vars_list = [v for _, v in grads_vars]\r\n....\r\ngrads_vars[i]  = (grads[i], vars_list[i])`\r\nthe I put the grads_vars into the apply_gradient func , but it didn't work, this is the error msg:\r\n`Traceback (most recent call last):\r\n  File \"12-13_linner.py\", line 147, in <module>\r\n    residual, new_grads_vars, grads, residual_mask, new_grads = grads_sparse(grads_vars, global_steps, residual_accum, sess)\r\n  File \"12-13_linner.py\", line 63, in grads_sparse\r\n    vars_list[i].scatter_sub(delta, use_locking=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 700, in scatter_sub\r\n    use_locking=use_locking)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1141, in scatter_sub\r\n    use_locking=use_locking, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1731, in __init__\r\n    control_input_ops)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1579, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shapes must be equal rank, but are 1 and 3 for 'ScatterSub' (op: 'ScatterSub') with input shapes: [1,10], [?,2], [?].\r\n`  \r\nthen I print the grads , \r\n`[IndexedSlicesValue(values=array([ 0.15668039,  0.14776216, -0.15483621, -0.15541458,  0.14010738,\r\n       -0.2145288 , -0.19674896], dtype=float32), indices=array([[0, 1],\r\n       [0, 2],\r\n       [0, 3],\r\n       [0, 5],\r\n       [0, 6],\r\n       [0, 8],\r\n       [0, 9]]), dense_shape=array([ 1, 10], dtype=int32)), \r\nIndexedSlicesValue(values=array([ 0.10107333, -0.17999038,  0.24159592, -0.18415265, -0.16449636,\r\n       -0.15763052,  0.24781035, -0.11305401], dtype=float32), indices=array([[0, 0],\r\n       [0, 1],\r\n       [0, 2],\r\n       [0, 3],\r\n       [0, 5],\r\n       [0, 6],\r\n       [0, 8],\r\n       [0, 9]]), dense_shape=array([ 1, 10], dtype=int32)), \r\nIndexedSlicesValue(values=array([-0.2975006 , -0.12903467, -0.1116452 , -0.15254416, -0.15824951,\r\n       -0.33808792, -0.10385308], dtype=float32), indices=array([[0, 0],\r\n       [2, 0],\r\n       [3, 0],\r\n       [6, 0],\r\n       [7, 0],\r\n       [8, 0],\r\n       [9, 0]]), dense_shape=array([10,  1], dtype=int32)), \r\nIndexedSlicesValue(values=array([-0.53333503], dtype=float32), indices=array([[0, 0]]), dense_shape=array([1, 1], dtype=int32))]`\r\nand var_list:\r\n`[array([[ 0.62123364,  0.8572454 ,  0.64919454,  1.4851073 ,  1.0997279 ,\r\n        -1.718374  ,  0.59419113, -1.4921925 ,  0.0046594 ,  1.0894274 ]],\r\n      dtype=float32), \r\n array([[0.1021007 , 0.03610747, 0.08513964, 0.08579567, 0.08281428,\r\n        0.05231547, 0.04030512, 0.11618505, 0.14989284, 0.06900915]],\r\n      dtype=float32), \r\n array([[-0.57928437],\r\n       [-2.0731926 ],\r\n       [-1.3741436 ],\r\n       [-0.32964215],\r\n       [ 0.69279206],\r\n       [ 0.22279441],\r\n       [ 1.3773512 ],\r\n       [-0.14758699],\r\n       [-0.3795929 ],\r\n       [ 1.3115551 ]], dtype=float32), \r\n array([[-0.03738503]], dtype=float32)]`\r\nit's seems that the have the same shape, can anyone help me how to fix it ? or tell me the right way to use the apply_gradients with the IndexdSlice, pls.\r\n\r\n", "comments": ["Hi, request you to post any support related questions in [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow). We encourage users to post only bug/feature request here. If you think it is a bug/feature, please fill [this](https://github.com/tensorflow/tensorflow/issues/new/choose) template and open a new issue. Thank you !"]}, {"number": 24538, "title": "[Intel MKL] Enable pad+fusedconv fusion", "body": "", "comments": ["@penpornk I have modified code based on your comments, after internal test passed, I will push the code for review, sorry about the delay.", "@penpornk your comments had been addressed, please help to review, sorry about this delay. ", "@penpornk thanks for you update, the changes you requested had been addressed, please help to review.", "@penpornk Can you help to check these failure, it seems did not related to my code, and others PR also had failure in these UT.", "pinging @caisq also", "@agramesh1 Roger. Starting the testing and merging process.", "@guizili0 @agramesh1 Sorry for my delay! Thank you, @caisq !", "Experiencing some glitches in the sync'ing mechanisms. Stay tuned. cc @yifeif ", "sync should be resolved, thanks for the patience! ", "@guizili0 do you mind resolving the merge conflicts before we proceed? Thank you!", "@yifeif conflict has been resolved, thanks.", "For some reason, I'm getting a message that there is a conflict again. @guizili0, could you please help take a look? Sorry for the inconvenience!", "@penpornk I cannot find the conflict, and the merge status is \"Merging can be performed automatically with 1 approving review.\" Can you help to check it?", "@guizili0 It's getting merged now. Sorry for the trouble! "]}, {"number": 24537, "title": "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[30003] and type half on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc", "body": "I am working on Sparse autoencoder model which have 15 convolution layers and 21 transpose convolution layers. I am running my code in a multi GPU system. This code is running well in the small dataset, but I am getting OMM resource exhausted error  when running on a huge dataset. I changed the batch size to 8 but still facing same error. Any help will be appreciated.\r\n\r\n**Traceback:**\r\n[[[[[Node: tower_1/DecodeRaw/_193 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_tower_1/DecodeRaw\", tensor_type=DT_HALF, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]](url)\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nException in thread QueueRunnerThread-tower_1/shuffle_batch/random_shuffle_queue-tower_1/shuffle_batch/random_shuffle_queue_enqueue:\r\nTraceback (most recent call last):\r\n  File \"/opt/rh/rh-python36/root/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/opt/rh/rh-python36/root/usr/lib64/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1205, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[30003] and type half on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[Node: tower_1/DecodeRaw = DecodeRaw[little_endian=true, out_type=DT_HALF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](tower_1/ReaderReadV2:1)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n         [[Node: tower_1/DecodeRaw/_193 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_15_tower_1/DecodeRaw\", tensor_type=DT_HALF, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.](url)\r\n](url)\r\n\r\n**System information**\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Centos 7)\r\n    TensorFlow installed from (source or binary): Binary\r\n    TensorFlow version (use command below): 1.10\r\n    Python version: 3.6.5_1\r\n    Bazel version (if compiling from source): N/A\r\n    GCC/Compiler version (if compiling from source): N/A\r\n    CUDA/cuDNN version: Cuda compilation tools, release 9.0, V9.0.176\r\n    GPU model and memory: NVIDIA TITAN V (4 GPUs)\r\n    Exact command to reproduce: (see above)", "comments": ["Hello, \r\nYour template looks quite OK and you are running from r1.10 binaries on Linux. Also try cuDNN 7.0 when you tweak the batch sizes, just to eliminate dependancies.\r\n\r\nOOM errors are generally associated with TensorFlow's tendency to greedily allocate all GPU memory to new sessions in the order they were created until exhaustion. You may need to configure TensorFlow as suggested by Scott in this Stack Overflow query:\r\n**_https://stackoverflow.com/questions/51310257/tensorflow-gpu-python-resource-exhausted-error-in-cluster_**\r\nPlease let us know your Sparse autoencoder's progress. Thanks.", "I already applied above suggestions but no improvement still facing this issue.\r\n\r\n`[InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'tower_0/input_producer/Assert/Assert': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU](url)`", "**My code snippet:**\r\n\r\n`config = tf.ConfigProto(allow_soft_placement=True,\r\n        log_device_placement=FLAGS.log_device_placement)\r\n    config.gpu_options.per_process_gpu_memory_fraction = 0.4\r\n    sess = tf.Session(config=config)\r\n    sess.run(init)\r\n`\r\n\r\n**Error**\r\n[Using TensorFlow backend.\r\nERROR:tensorflow:Exception in QueueRunner: Dst tensor is not initialized.\r\n\t [[Node: tower_0/per_image_standardization/_185 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_48_tower_0/per_image_standardization\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nERROR:tensorflow:Exception in QueueRunner: Dst tensor is not initialized.\r\n\t [[Node: tower_3/per_image_standardization/_223 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:3\", send_device_incarnation=1, tensor_name=\"edge_48_tower_3/per_image_standardization\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nERROR:tensorflow:Exception in QueueRunner: Dst tensor is not initialized.\r\n\t [[Node: tower_1/per_image_standardization/_191 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device_incarnation=1, tensor_name=\"edge_48_tower_1/per_image_standardization\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nERROR:tensorflow:Exception in QueueRunner: Dst tensor is not initialized.\r\n\t [[Node: tower_2/per_image_standardization/_207 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:2\", send_device_incarnation=1, tensor_name=\"edge_48_tower_2/per_image_standardization\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n](url)\r\n\r\nAny help will be highly appreciated. ", "Hello,\r\nThe other two errors are not aligned with the original OOM error. Please use Stack Overflow to investigate various errors and track down the solutions being thrown by the autoencoder code.\r\n_https://stackoverflow.com/search?q=tensorflow_\r\n"]}]