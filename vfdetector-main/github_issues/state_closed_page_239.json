[{"number": 47358, "title": "The comparing with memory by 1080 ti and 3090", "body": "- Unet by tensorflow.keras (not include tf.compat.v1 code)\r\n- Both of these are ubuntu18\r\n- 2.4.0 (A computer) 2.0.0 (B computer)\r\n- Both of these are python 3.6.12\r\n- 11.1/8.0.5 (A computer) 10.0/7.6.5 (B computer)\r\n- 3090 24265MiB (A computer) 1080 ti 11178MiB (B computer)\r\n- Driver version 460.39\r\n\r\n### current behavior\r\n\r\nI ran the code using Unet to do segmentation project.\r\nI found the using of memory is very diffecent.\r\nIt is the setting of model when I had trained the model:\r\n\r\nbatch_size = 20\r\nepochs = 2\r\n\r\nWhen I had trained the model, I inspect the htop and nvidia-smi:\r\n\r\nThe using resource by 3090 (A computer)\r\ncpu memory 4.4G -> 12G\r\ngpu memory 457MiB -> 18307MiB \r\n\r\nThe using resource by 1080 ti: (B computer)\r\ncpu memory 0.79G  -> 3.4G\r\ngpu memory 0MiB -> 10867MiB\r\n\r\n### expected behavior\r\n\r\nI expected 3090(tf2.4) should be faster than 1080(tf2.0).\r\n\r\nThank you to read this issue!", "comments": ["@leeivan1007 \r\n\r\nPlease, share the simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Sorry. I have to discuss with my boss if I share the code. The Unet had changed to some projects...\r\nBut I can test with official code:\r\n\r\n### Classification on mnist / tf2.4 / 3090\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\n```\r\nThese are utilize of memory:\r\n1. 6.5G -> 8.33G (memory) \r\n2. 549Mib -> 1218MiB (GPU memory)\r\n\r\n\r\n### Classification on mnist / tf2.0 / 1080\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = 0\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.Session(config=config)\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=10)\r\n```\r\nIt is an only different that setting of memory growth.\r\n\r\nThese are utilize of memory:\r\n1. 0.8G -> 2.18G (memory)\r\n2. 0Mib -> 283MiB (GPU memory)", "@leeivan1007 \r\n\r\nCan you please test the both using TF version 2.4 and see if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Het @leeivan1007, \r\n\r\nI am having the same issue. Did you find the reason behind this issue?\r\n\r\nsimilar threads: \r\n* https://github.com/NVIDIA/tensorflow/issues/23\r\n* https://forums.developer.nvidia.com/t/new-tensorrt-model-occupying-more-gpu-memory-as-compared-to-older-version/186373 "]}, {"number": 47357, "title": "tflite interpreter on android crashes on a model downloaded from tfhub", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android\r\n- TensorFlow installed from (source or binary): maven build\r\n- TensorFlow version (or github SHA if from source): ``org.tensorflow:tensorflow-lite:2.4.0``\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# N/A - as the model is downloaded from tfhub\r\n```\r\n\r\nI am trying the simple style-transfer application using tflite. I am downloading the ``style-predict`` and ``style-transfer`` models from tfhub: https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/fp16/predict/1\r\n\r\nThe problem is that when using the ``style transfer`` model, the app crashes with the following:\r\n\r\n```\r\nsignal 6 (SIGABRT), code -1 (SI_QUEUE), fault addr --------\r\nAbort message: '/buildbot/src/android/ndk-release-r18/external/libcxx/../../external/libcxxabi/src/abort_message.cpp:73: abort_message: assertion \"terminating with uncaught exception of type std::bad_alloc: std::bad_alloc\" failed'\r\n    eax 00000000  ebx 00001e3b  ecx 00001e3b  edx 00000006\r\n    edi e83c333e  esi ff88e7f0\r\n    ebp ea340ad0  esp ff88e798  eip ea340ad9\r\nbacktrace:\r\n      #00 pc 00000ad9  [vdso] (__kernel_vsyscall+9)\r\n      #01 pc 00092328  /apex/com.android.runtime/lib/bionic/libc.so (syscall+40) (BuildId: 76290498408016ad14f4b98c3ab6c65c)\r\n      #02 pc 000ad651  /apex/com.android.runtime/lib/bionic/libc.so (abort+193) (BuildId: 76290498408016ad14f4b98c3ab6c65c)\r\n      #03 pc 000adb88  /apex/com.android.runtime/lib/bionic/libc.so (__assert2+56) (BuildId: 76290498408016ad14f4b98c3ab6c65c)\r\n      #04 pc 002ba9a4  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #05 pc 002baab7  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #06 pc 002b82d9  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #07 pc 002b7c0e  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #08 pc 002b7b63  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #09 pc 002ba578  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #10 pc 000f7e76  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #11 pc 000fedd9  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #12 pc 000fd993  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #13 pc 000fc1e8  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #14 pc 000f1280  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #15 pc 000efe3b  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #16 pc 0012dfab  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #17 pc 0012c6a5  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #18 pc 000ee7d6  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #19 pc 0028acc0  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #20 pc 0028e326  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so\r\n      #21 pc 0004224f  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+63)\r\n      #22 pc 00144f67  /apex/com.android.runtime/lib/libart.so (art_quick_generic_jni_trampoline+71) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #23 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #24 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #25 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #26 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #27 pc 00684d03  /apex/com.android.runtime/lib/libart.so (MterpInvokeStatic+643) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #28 pc 001389a1  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_static+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #29 pc 00296a70  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+156)\r\n      #30 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #31 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #32 pc 0029603a  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n      #33 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #34 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #35 pc 00007b6e  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.runTransform+102)\r\n      #36 pc 006845ac  /apex/com.android.runtime/lib/libart.so (MterpInvokeDirect+1324) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #37 pc 00138921  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_direct+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #38 pc 00007bc0  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.StyleTransfer+16)\r\n      #39 pc 006845ac  /apex/com.android.runtime/lib/libart.so (MterpInvokeDirect+1324) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #40 pc 00138921  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_direct+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #41 pc 000080cc  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.MainActivity.lambda$configureFlutterEngine$0$MainActivity+236)\r\n      #42 pc 00681adc  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+1612) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #43 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #44 pc 00007944  [anon:dalvik-classes2.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk!classes2.dex] (com.companyName.mytflite_app.-$$Lambda$MainActivity$rQ-TPneqhLnrlPUSt3FsQ1A3iA8.onMethodCall+4)\r\n      #45 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #46 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #47 pc 001fc346  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage+34)\r\n      #48 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #49 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #50 pc 001f245a  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.embedding.engine.dart.DartMessenger.handleMessageFromDart+114)\r\n      #51 pc 006837bc  /apex/com.android.runtime/lib/libart.so (MterpInvokeInterface+1980) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #52 pc 00138a21  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_interface+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #53 pc 001f1128  [anon:dalvik-classes.dex extracted in memory from /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/base.apk] (io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage+8)\r\n      #54 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #55 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #56 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #57 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #58 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #59 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #60 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #61 pc 0055bc7a  /apex/com.android.runtime/lib/libart.so (art::InvokeVirtualOrInterfaceWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, char*)+474) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #62 pc 0040962f  /apex/com.android.runtime/lib/libart.so (art::JNI::CallVoidMethodV(_JNIEnv*, _jobject*, _jmethodID*, char*)+943) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #63 pc 003d8f44  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallMethodV(char const*, _JNIEnv*, _jobject*, _jclass*, _jmethodID*, char*, art::Primitive::Type, art::InvokeType)+1700) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #64 pc 003c53f9  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallVoidMethodV(_JNIEnv*, _jobject*, _jmethodID*, char*)+73) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #65 pc 0123628f  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #66 pc 012361c3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #67 pc 0123073e  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #68 pc 012b14a3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #69 pc 01255e05  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #70 pc 01258bf3  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #71 pc 01258b04  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #72 pc 0125f3fa  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #73 pc 0125f428  /data/app/com.companyName.mytflite_app-UCzFh7iFvc8i5Gni7pzrnA==/lib/x86/libflutter.so (BuildId: 8229a066e8159a187906d17af0b2738960128f78)\r\n      #74 pc 00018487  /system/lib/libutils.so (android::SimpleLooperCallback::handleEvent(int, int, void*)+39) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)\r\n      #75 pc 00019414  /system/lib/libutils.so (android::Looper::pollInner(int)+1044) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)\r\n      #76 pc 00018f4e  /system/lib/libutils.so (android::Looper::pollOnce(int, int*, int*, void**)+62) (BuildId: 288ba3aff5b46dbd7e74be954af88b83)\r\n      #77 pc 0013299b  /system/lib/libandroid_runtime.so (android::android_os_MessageQueue_nativePollOnce(_JNIEnv*, _jobject*, long long, int)+59) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)\r\n      #78 pc 002b86f8  /system/framework/x86/boot-framework.oat (art_jni_trampoline+136) (BuildId: ff6ec03dd8445d20788424c92ba8ea28ad0f54f4)\r\n      #79 pc 02001f56  /memfd:/jit-cache (deleted) (android.os.MessageQueue.next+230)\r\n      #80 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #81 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #82 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #83 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #84 pc 0068186d  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+989) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #85 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #86 pc 00319d8a  /system/framework/framework.jar (android.os.Looper.loop+130)\r\n      #87 pc 00684f6c  /apex/com.android.runtime/lib/libart.so (MterpInvokeStatic+1260) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #88 pc 001389a1  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_static+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #89 pc 0018945e  /system/framework/framework.jar (android.app.ActivityThread.main+194)\r\n      #90 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #91 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #92 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #93 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #94 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #95 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #96 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #97 pc 0055c32f  /apex/com.android.runtime/lib/libart.so (art::InvokeMethod(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jobject*, _jobject*, unsigned int)+1327) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #98 pc 004c9153  /apex/com.android.runtime/lib/libart.so (art::Method_invoke(_JNIEnv*, _jobject*, _jobject*, _jobjectArray*)+83) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #99 pc 000c6bf8  /system/framework/x86/boot.oat (art_jni_trampoline+168) (BuildId: 7913dbaef2e8d9971cb7619ef0d566987f8326a7)\r\n      #100 pc 0013e7d2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_stub+338) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #101 pc 00149a69  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+281) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #102 pc 00332502  /apex/com.android.runtime/lib/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+386) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #103 pc 0032c19c  /apex/com.android.runtime/lib/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+988) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #104 pc 0068186d  /apex/com.android.runtime/lib/libart.so (MterpInvokeVirtual+989) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #105 pc 00138821  /apex/com.android.runtime/lib/libart.so (mterp_op_invoke_virtual+33) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #106 pc 0034cd36  /system/framework/framework.jar (com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run+22)\r\n      #107 pc 002f8e0a  /apex/com.android.runtime/lib/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEbb.llvm.12194892193087984976+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #108 pc 002ffcc5  /apex/com.android.runtime/lib/libart.so (art::interpreter::EnterInterpreterFromEntryPoint(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*)+181) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #109 pc 0066fbd9  /apex/com.android.runtime/lib/libart.so (artQuickToInterpreterBridge+1209) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #110 pc 0014503d  /apex/com.android.runtime/lib/libart.so (art_quick_to_interpreter_bridge+77) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #111 pc 00998b08  /system/framework/x86/boot-framework.oat (com.android.internal.os.ZygoteInit.main+1816) (BuildId: ff6ec03dd8445d20788424c92ba8ea28ad0f54f4)\r\n      #112 pc 0013e9a2  /apex/com.android.runtime/lib/libart.so (art_quick_invoke_static_stub+418) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #113 pc 00149a7a  /apex/com.android.runtime/lib/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+298) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #114 pc 0055a513  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+99) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #115 pc 0055a1ae  /apex/com.android.runtime/lib/libart.so (art::InvokeWithVarArgs(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, char*)+430) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #116 pc 004305cd  /apex/com.android.runtime/lib/libart.so (art::JNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, char*)+893) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #117 pc 003d93bf  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallMethodV(char const*, _JNIEnv*, _jobject*, _jclass*, _jmethodID*, char*, art::Primitive::Type, art::InvokeType)+2847) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #118 pc 003c7509  /apex/com.android.runtime/lib/libart.so (art::(anonymous namespace)::CheckJNI::CallStaticVoidMethodV(_JNIEnv*, _jclass*, _jmethodID*, char*)+73) (BuildId: 895645e5113da057f27d9b2ec11eb3bf)\r\n      #119 pc 000b25fe  /system/lib/libandroid_runtime.so (_JNIEnv::CallStaticVoidMethod(_jclass*, _jmethodID*, ...)+62) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)\r\n      #120 pc 000b628a  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, android::Vector<android::String8> const&, bool)+794) (BuildId: 6ceb9761bceb97a18c92f8a4b7072247)\r\n      #121 pc 00003632  /system/bin/app_process32 (main+1490) (BuildId: b7a60bc7d078521421fd5a8d201915ae)\r\n      #122 pc 000898e8  /apex/com.android.runtime/lib/bionic/libc.so (__libc_init+120) (BuildId: 76290498408016ad14f4b98c3ab6c65c)\r\n```\r\n\r\nThe ``style-predict`` model works fine and generates the expected output. But when I try to use the input image and the ``bottleneck`` from the ``style-predict`` in the ``style-transfer`` model the app crashes. \r\n\r\nWhats interesting is that when I use the [other style transfer model that is not based on inceptionv3](https://tfhub.dev/google/lite-model/magenta/arbitrary-image-stylization-v1-256/fp16/transfer/1), the app works fine and everything works great. But I want to make the inception model work as its result is far superior.\r\n\r\nBelow is my code in java that I am using to make this work:\r\n\r\n```java\r\n    protected Interpreter predictInterpreter;\r\n    protected Interpreter transformInterpreter;\r\n    protected Interpreter.Options interpreterOptions = new Interpreter.Options();\r\n    private final static int IMAGE_MEAN = 0;\r\n    private final static int IMAGE_STD = 255;\r\n    private final static int STYLE_IMG_SIZE = 256;\r\n    private final static int CONTENT_IMG_SIZE = 384;\r\n    private final static int DIM_BATCH_SIZE = 1;\r\n    private final static int DIM_PIXEL_SIZE = 3;\r\n\r\n// non-relevant code removed for easy reading\r\n\r\ntry {\r\n            interpreterOptions.setNumThreads(4);\r\n            predictInterpreter = new Interpreter(loadModelFile(\"inceptionv3_fp16_predict_1.tflite\"), interpreterOptions);\r\n            transformInterpreter = new Interpreter(loadModelFile(\"inceptionv3_fp16_transfer_1.tflite\"), interpreterOptions);\r\n        } catch (Exception e) {\r\n            e.printStackTrace();\r\n        }\r\n\r\n// input contentImage is already downscaled to 384*384 and ``bottleneck`` is the output from the ``style-predict`` model.\r\nprivate Bitmap runTransform(Interpreter tflite, Bitmap contentImage, ByteBuffer bottleneck) {\r\n        TensorImage inputTensorImage = getInputTensorImage(tflite, contentImage);\r\n        Object[] inputs = new Object[2];\r\n        inputs[0] = inputTensorImage.getBuffer();\r\n        inputs[1] = bottleneck;\r\n\r\n        int[] outputShape =\r\n                new int[] {DIM_BATCH_SIZE, CONTENT_IMG_SIZE, CONTENT_IMG_SIZE, DIM_PIXEL_SIZE};\r\n        DataType outputDataType = tflite.getOutputTensor(/* outputTensorIndex */ 0).dataType();\r\n        TensorBuffer outputTensorBuffer = TensorBuffer.createFixedSize(outputShape, outputDataType);\r\n        Map<Integer, Object> outputs = new HashMap<>();\r\n        outputs.put(0, outputTensorBuffer.getBuffer());\r\n        tflite.runForMultipleInputsOutputs(inputs, outputs);    /// on this line app crashes with above error messages.\r\n        Bitmap outputBitmap = convertOutputToBmp(outputTensorBuffer.getFloatArray());\r\n        return outputBitmap;\r\n    }\r\n\r\nprivate TensorImage getInputTensorImage(Interpreter tflite, Bitmap inputBitmap) {\r\n        DataType imageDataType = tflite.getInputTensor(/* imageTensorIndex */0).dataType();\r\n        TensorImage inputTensorImage = new TensorImage(imageDataType);\r\n        inputTensorImage.load(inputBitmap);\r\n        ImageProcessor imageProcessor =\r\n                new ImageProcessor.Builder().add(new NormalizeOp(IMAGE_MEAN, IMAGE_STD)).build();\r\n        return imageProcessor.process(inputTensorImage);\r\n    }\r\n```\r\n\r\nI hope someone can help me figure out what the problem is please.", "comments": ["Hi,\r\n\r\nCan anyone help me with this please?", "Hi, can you use debug build to get more verbose stack track?\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_android#build_and_install\r\n```\r\nbazel build -c dbg --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  //tensorflow/lite/java:tensorflow-lite\r\n```", "I think I've seen this before, and I remember indeed there's one weird style transfer model.\r\n\r\n@khanhlvg , do you have any clue for this? I remember we've talked about it before.", "I don't have this exact problem, but I am experiencing crashes with a TFLite model with tflite built via bazel from master/nightly (1edc795). Building from tags/v2.4.1 works fine, though.\r\n\r\nModel was converted to tflite using 2.4.1.", "> Hi, can you use debug build to get more verbose stack track?\r\n> \r\n> https://www.tensorflow.org/lite/guide/build_android#build_and_install\r\n> \r\n> ```\r\n> bazel build -c dbg --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a \\\r\n>   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n>   //tensorflow/lite/java:tensorflow-lite\r\n> ```\r\n\r\nHi, sorry but I'm not familiar with bazel and I tried to follow the documentation and downloaded the ``bazel.exe`` and added it to my path. From where do I now call the ``bazel build`` command?", "Please follow steps on https://www.tensorflow.org/lite/guide/build_android#set_up_build_environment_without_docker\r\n\r\nBTW, I've tested the inception based models with https://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/android\r\nby updating https://github.com/tensorflow/examples/blob/master/lite/examples/style_transfer/android/app/download_model.gradle\r\nit works well for me.", "Hi, \r\n\r\nSo I have gotten something to update regarding this. \r\n\r\nI noticed that the app was crashing only on the x86 android emulator with Android Studio. But runs perfectly fine on a ``x86_64`` or ``arm64`` emulator. \r\n\r\nSince ``x86`` android phones only make up less than 1% of android phones, I can safely ignore this crash as it wont affect the 99% of android phones on the 64 bit architecture.\r\n\r\nDon't know if this is helpful in exploring the cause for this issue but felt to update for anyone also having this problem\r\n\r\n ", "I see. BTW, can you test this again?\r\nMy Mac has x86 emulator and it looks fine now.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47357\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47357\">No</a>\n"]}, {"number": 47356, "title": "[Kernel C API] Fix inplacement issue in C API", "body": "This commit is intended to fix two inplace issue in kernel C API:\r\n1. RefCountIsOne\r\n   tensor.RefCountIsOne() is the condition to decide whether the op can\r\n   do inplacement(e.g. forward_input), however, TF_Tensor retrieved through\r\n   TF_GetInput(), TF_AllocateOutput() makes the refcount always > 1, in\r\n   TF_TensorFromTensor, it will call tensor.CopyFrom() and increment the refcount\r\n\r\n2. TF_TensorBitcastFrom()\r\n   Currently, this API only modifies TF_Tensor's buf pointer, but the src c++ tensor it\r\n   copied from is not modified, that makes the TF_Tensor inconsistent with src c++ tensor,\r\n   see the following example, it allocated an output TF_Tensor and modified its buffer through\r\n   TF_TensorBitcastFrom(), however, src c++ tensor in Op_KernelContext is not modified.\r\n   ```c++\r\n    // Example:\r\n    TF_Tensor* a = TF_AllocateOutput(ctx, 0); // src c++ tensor: x\r\n    TF_Tensor* b = TF_AllocateTemp(ctx);\r\n    TF_TensorBitcastFrom(b, a); // a->buf_ = b_buf;\r\n    // src c++ tensor x's buf is not modified, thus x->buf_ != a->buf_\r\n   ```", "comments": ["@penpornk  This PR aims at solving two inplacement issue in kernel C API, please help to review, thanks very much.\r\n1. TF_Tensor's buffer refcount retrieved from TF_GetInput and TF_AllocatedOutput are always > 1, that will make TF_ForwardInputOrAllocateOutput, TF_TensorMaybeMove failed.\r\n2. TF_TensorBitcastFrom only replace TF_Tensor's buffer pointer, if TF_Tensor is copied from a proper's c++ tensor(TF_GetInput, TF_Allocateoutput), proper's c++ tensor will not be changed, that will make TF_Tensor inconsistent with src c++ tensor.\r\n\r\nwhat we changed: \r\n   1. we add a src_tensor_ptr_ in  TensorInterface which makes TF_Tensor be able to find its src c++ tensor.\r\n   2. when the TF_Tensor is retrieved from TF_GetInput, TF_AllocateOutput, that means TF_Tensor is used in kernel, its lifecycle is in kernel's compute function, so it no need to add reference count to src c++ tensor. We decrement the refcount after tensor.CopyFrom in TF_TensorFromTensor and increment the refcount by one before the delete of TensorInterface.. \r\n    ", "@jzhoulon Thank you for the PR! I think @saxenasaurabh owns this part. I'll request his review.\r\n\r\n@saxenasaurabh Would you mind helping review this PR? Or could you please suggest other reviewers? Thank you very much!", "cc: @allenlavoie ", "Thanks for finding and proposing a solution to this. `TF_Tensor` holding an additional reference to the `Tensor` buffer is definitely a problem. However the proposed changes seem a bit complex to me. I wonder if we can change the semantics of `TF_Tensor` to be `const Tensor&`. That might require a larger API change.\r\n\r\ncc: @rjpower ", "This is mostly just a problem for the C kernel APIs? Making TF_Tensor always a non-owning reference seems pretty drastic (who owns the result of TFE_TensorHandleResolve, and can I feed it into an op attribute which then owns it again?).\r\n\r\nWhat is the blocker for saying TF_Tensor is tensorflow::Tensor and just reinterpret_casting between the types at the C API boundaries? The TensorInterface abstraction we do to for TFRT tensors?", "@saxenasaurabh @allenlavoie  Thanks for the comments, I didn't try to make TF_Tensor always a non-owning reference since the TF_Tensor is also used for python tensor binding. In this PR, if TF_Tensor is get from (TF_GetInput, TF_AllocateOutput), that means TF_Tensor is used as Kernel C API, in this case, TF_Tensor will not increments the reference count of Tensor. In other case(such as TF_Tensor used for python tensor bind, ), the logic is not changed. ", "@allenlavoie Can you please take a look on the above comment from @jzhoulon. Thanks!", "@saxenasaurabh and I had a discussion with the team and we think adding \"explicit IncRef/DecRef/Borrow semantics on TF_Tensor\" might be more suitable for this. This would need an RFC and will miss TF 2.5. \r\n\r\n@jzhoulon Would you mind closing this PR for now? We can reopen it later if it becomes relevant. (Although I think that's unlikely given the RFC direction.)", "Sure, looking forward the RFC!"]}, {"number": 47355, "title": "Properly support complex in SparseTensorDenseMatMul", "body": "This adds gradient support for complex, and fixes a bug with adjoints (https://github.com/tensorflow/tensorflow/issues/47337).\r\n\r\n~My laptop can't build, so I haven't been able to properly test this yet. Hopefully CI will do it?~", "comments": ["I'm having trouble testing this locally because I can't build TF (I'm just on a little laptop :C), but CI seems to have run everything successfully at https://github.com/tensorflow/tensorflow/pull/47355/commits/0623c04b1e4385dc8cb85e037763f04b8686945d. I don't *think* the test update I've made since then should be a problem, but hard to know until it runs.", "Thanks! And thanks for the review :)"]}, {"number": 47354, "title": "ImportError traceback in VScode", "body": "ImportError                               Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n     17         try:\r\n---> 18             fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n     19         except ImportError:\r\n\r\nC:\\Program Files\\Python39\\lib\\imp.py in find_module(name, path)\r\n    295     else:\r\n--> 296         raise ImportError(_ERR_MSG.format(name), name=name)\r\n    297 \r\n\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     53     # use `dlopen()` for dynamic loading.\r\n---> 54     from tensorflow.python import pywrap_tensorflow\r\n     55 except ImportError:\r\n\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             import _pywrap_tensorflow\r\n     21             return _pywrap_tensorflow\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\nc:\\Users\\tarang mehta\\Desktop\\introtodeeplearning-master\\Deep_lizard\\First_ANN.py in \r\n----> 2 import tensorflow as tf\r\n      3 from tensorflow import keras\r\n      4 from tensorflow.keras.models import Sequential\r\n      5 from tensorflow.keras.layers import Activation, Dense\r\n      6 from tensorflow.keras.optimizers import Adam\r\n\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26 \r\n\r\n~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     58 please exit the tensorflow source tree, and relaunch your python interpreter\r\n     59 from there.\"\"\" % traceback.format_exc()\r\n---> 60   raise ImportError(msg)\r\n     61 \r\n     62 # Protocol buffers\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\tarang mehta\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Program Files\\Python39\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\tarang mehta\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\tarang mehta\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\tarang mehta\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n2 cells were canceled due to an error in the previous cell.", "comments": ["currently i have this version: \r\nRequirement already satisfied: tensorflow in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (0.12.0)\r\nRequirement already satisfied: numpy>=1.11.0 in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.20.1)\r\nRequirement already satisfied: six>=1.10.0 in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.15.0)\r\nRequirement already satisfied: protobuf==3.1.0 in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.1.0)\r\nRequirement already satisfied: wheel>=0.26 in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.36.2)\r\nRequirement already satisfied: setuptools in c:\\users\\tarang mehta\\appdata\\roaming\\python\\python39\\site-packages (from protobuf==3.1.0->tensorflow) (53.0.0)", "@tarangmehta1195 \r\n\r\nCould you please check if you are using 64-bit Python on your machine. TensorFlow is tested and supported on the following 64-bit systems. For more information, please check [this document](https://www.tensorflow.org/install#install-tensorflow-2).\r\n\r\nCan you please refer to these links and let us know if it helps: [link](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso), [link1](https://github.com/tensorflow/tensorflow/issues/7529),[link2](https://github.com/tensorflow/tensorflow/issues/8385), #40668", "i am using 64-bit Python", "@tarangmehta1195\r\nCan you also verify other requirement:\r\n\r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the Microsoft [Visual C++ Redistributable](https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0) package\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\nAlso, check this similar duplicate issue: #46124\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47354\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47354\">No</a>\n"]}, {"number": 47353, "title": "TensorFlow 2.4 Contains References to nocopts, which is No Longer Compatible with Bazel", "body": "In e5f8043742f927ed0e1711bb48f3e1a153b7a997 and ddde447e792231cdf83b435b0eeb59dd59bf4044, among other commits, support for `nocopts` was removed to enable the upgrade of Bazel to 1.0 and above.\r\n\r\nHowever, there are still a few references to `nocopts` in the r2.4 branch:\r\n```\r\n$ git grep nocopts\r\ntensorflow/lite/micro/testing/micro_test.bzl:        nocopts = \"\",\r\ntensorflow/lite/micro/testing/micro_test.bzl:        nocopts: list of gcc compilation flags to remove for this rule\r\ntensorflow/lite/micro/testing/micro_test.bzl:        nocopts = nocopts,\r\ntensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.\r\ntensorflow/tensorflow.bzl:    # -fno-exceptions in nocopts breaks compilation if header modules are enabled.\r\n```\r\n\r\nThese should probably be removed for consistency and to avoid confusion.", "comments": ["https://github.com/tensorflow/tensorflow/pull/47055 removed `tensorflow/lite/micro/testing/micro_test.bzl`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47353\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47353\">No</a>\n"]}, {"number": 47352, "title": "When will tensorflow be compatible with CUDA 11.2?", "body": "- TensorFlow version: 2.5 (nightly)\r\n- GPU: RTX 3070\r\n- CUDA version: 11.2\r\n- cuDNN version: 8\r\n\r\nRight now, tensorflow is unable to recognize my GPU, so I'm not able to take advantage of the hardware. I read that others have been having the same issue with the newer RTX cards and it's due to tensorflow not supporting the latest CUDA versions. Please make this update, so we can leverage these newer cards. They're pretty hard to get right now, so it was disappointing to find out I couldn't use it.\r\n", "comments": ["@AbeG13,\r\nSupport for CUDA 11.2 has already been requested in issue [#46093](https://github.com/tensorflow/tensorflow/issues/46093).\r\n\r\nCan we please close this issue since it is being tracked there? Thanks!", "Okay, closed. I hope the update comes out soon."]}, {"number": 47351, "title": "fix Windows debug build", "body": "PDB file format has internal 32-bit limits, which doesn't allow PDB files to grow beyond 4GB even on x64 builds\r\n\r\nthus use reduced debug symbols set in order not to exceed 4GB limit", "comments": []}, {"number": 47350, "title": "fix Windows build errors", "body": "fix some more build errors of the kind already addressed in #42676:\r\n\r\ncompiler doesn't get that LOG(FATAL) macro eventually calls std::abort() and complains with \"method doesn't return a value\", so call abort explicitely", "comments": ["This looks redundant to me: can we find a solution to make LOG(FATAL) work with MSVC? Either adding the abort more prominently there, or use an attribute or any annotation that would appease MSVC?\r\n\r\nAlternatively: can a flag be added to the MSVC configuration to disable this warning/error?", "> This looks redundant to me: can we find a solution to make LOG(FATAL) work with MSVC? Either adding the abort more prominently there, or use an attribute or any annotation that would appease MSVC?\r\n\r\nDue to the fact that the LOG macro expands to a subclass of std::ostringstream, I don't see a way to place abort() anywhere else than in the destructor and at the same time keep the semantics of the LOG macro.\r\n\r\n> Alternatively: can a flag be added to the MSVC configuration to disable this warning/error?\r\n\r\nIt's an error (I wouldn't have submitted this PR just to silence a false warning ;-) ), or rather it's tons of error C4716, for example:\r\n```\r\nC:\\tools\\vcpkg\\buildtrees\\.bzl\\_bazel_jgehweil\\e7zqzfy3\\execroot\\org_tensorflow\\tensorflow\\compiler\\xla\\literal.cc(1391) : error C4716: 'xla::`anonymous namespace'::BitcastBetweenNativeTypes<short,signed char>': must return a value\r\n```", "In the documentation of the MSVC compiler, I only found switches to handle warnings (suppress them or raise them like errors), but not errors. Nevertheless, I gave it a try to see if the suppress warning switch would also suppress an error, and added the according switch `/wd4716` to `./tensorflow/BUILD` like this:\r\n```\r\ntf_cc_shared_object(\r\n    name = \"tensorflow_cc\",\r\n    linkopts = select({\r\n        \"//tensorflow:macos\": [\r\n            \"-Wl,-exported_symbols_list,$(location //tensorflow:tf_exported_symbols.lds)\",\r\n        ],\r\n        \"//tensorflow:windows\": [],\r\n        \"//conditions:default\": [\r\n            \"-z defs\",\r\n            \"-Wl,--version-script,$(location //tensorflow:tf_version_script.lds)\",\r\n        ],\r\n    }) + select({\r\n        \"//tensorflow:msvc_cl_debug\": [\r\n            \"/DEBUG:FASTLINK\",\r\n            \"/wd4716\",\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    per_os_targets = True,\r\n```\r\nBut it kept failing with the C4716 error. So, in the end I don't see any better solution than mine.", "The MSVC [doc](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4716?view=msvc-160) suggests using `#pragma`; WDYT about putting that in the header that defines the `LOG` macro?", "> As per previous comment to try using `#pragma`.\r\n\r\nI tried to build it with the suggested `#pragma` solution, and it actually worked (through it just disables a warning and the compiler was reporting an error previously).\r\n\r\nI updated the PR accordingly."]}, {"number": 47349, "title": "fix Linux build error", "body": "fix some more issues of the kind already addressed in #42745", "comments": []}, {"number": 47348, "title": "Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: windows 10\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 2.4.0\r\n-   **Python version**: 3.8.8\r\n-   **Bazel version (if compiling from source)**: 3.1.0\r\n-   **GCC/Compiler version (if compiling from source)**: MSVC 2019\r\n-   **CUDA/cuDNN version**: 11.0.3/8.0.5\r\n-   **GPU model and memory**: GTX 1650 Ti, 4GB\r\n-   **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nBazel fails to download 1.7.336.tar.gz, then unable to unzip an empty simple_console_for_windows.zip (size is 0b).\r\n\r\n### Source code / logs\r\n```PS C:\\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package\r\nPS C:\\tensorflow> bazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Python38/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Python38/lib/site-packages --python_path=C:/Python38/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file c:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\n**WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found**\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (410 packages loaded, 26438 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package.exe\r\nINFO: Elapsed time: 6455.931s, Critical Path: 785.27s\r\nINFO: 10307 processes: 10307 local.\r\nINFO: Build completed successfully, 14648 total actions\r\nPS C:\\tensorflow> bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\nTue Feb 23 16:54:05 CEST 2021 : === Preparing sources in dir: /tmp/tmp.ZkbeuBDQsm\r\nUnzipping simple_console_for_windows.zip to create runfiles tree...\r\n[./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip]\r\n  End-of-central-directory signature not found.  Either this file is not\r\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n  latter case the central directory and zipfile comment will be found on\r\n  the last disk(s) of this archive.\r\nunzip:  cannot find zipfile directory in one of ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip or\r\n        ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.zip, and cannot find ./bazel-bin/tensorflow/tools/pip_package/simple_console_for_windows.zip.ZIP, period.```\r\n", "comments": ["@gastan81 \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/37802#issuecomment-603347330) on same issue and let us know if it helps.", "Adding --define=no_tensorflow_py_deps=true made it work. I had so many problems making it all work to this point, I totally overlooked the workaround already mentioned in the instructions, my bad, very sorry for that. And thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47348\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47348\">No</a>\n"]}, {"number": 47346, "title": "Proposal to use newer oneDNN version in the core", "body": "**System information**\r\n- TensorFlow version (you are using): v2.4.1/r2.4\r\n- Are you willing to contribute it: It depends on TF Community plans\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAt the moment TensorFlow employs [oneDNN v1.6.4](https://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/workspace.bzl#L172-L181) for core computationally intensive routines. Meanwhile oneDNN is approaching [to v2.2](https://github.com/oneapi-src/oneDNN/milestones) release. May you please clarify if there are any plans for the upcoming TF releases to move to a newer oneDNN version?\r\n\r\n**Will this change the current api? How?**\r\nIt should not change the current API, I built TensorFlow with oneDNN master and it worked fine.\r\n\r\n**Who will benefit with this feature?**\r\nNewer versions of oneDNN have better support for and performance on AArch64, the main benefits will be for the users running TensorFlow on AArch64-based machines.\r\n\r\n**Any Other info.**\r\nThis issue is mainly a question to understand TensorFlow Community plans for this topic.", "comments": ["This question may be relevant for @agramesh1 ", "It looks like the hope is to move to 2.2 with the TensorFlow 2.5 release: https://github.com/oneapi-src/oneDNN/issues/994#issuecomment-785426665\n?", "Yes, that is the plan.", "@agramesh1, many thanks for clarifying this."]}, {"number": 47345, "title": "support of TFlite of Snapdragon888 dsp", "body": "Hi @shuki-k \r\nSadly this is not possible.\r\nThese new chips uses a newer version of HVX that is different than what Hexagon delegate supports.\r\n\r\n_Originally posted by @karimnosseir in https://github.com/tensorflow/tensorflow/issues/47246#issuecomment-783807697_", "comments": ["when will Snapdragon 888 DSP be supported with TFlite? is it in the roadmap ?"]}, {"number": 47344, "title": "In-batch-negatives and distributed traning", "body": "I'm training a model using triplet loss. This means that my loss function aims to minimize the distance to a positive observation (let's say a sentence with the same semantic meaning) and maximize the distance to the negative observation. \r\nIm using [TripletHardLoss](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/losses/triplet.py#L348) from tensorflow-addons to do so. TripletHardLoss mines the batch to find the positive observation with the largest distance and the negative observation with the smallest distance in the batch and then applies the triplet loss.\r\n\r\nNow when I train this model using `tf.distribute.MirroredStrategy` on a batch of 128 sentences and 8 GPUs the batches on my devices will only contain 14 (not counting the positive observation and the observation itself) potential negatives for each datapoint in the batch.\r\n\r\nIs there any distribution strategy to concatenate all the 128 embeddings, run the triplet mining on this global batch, distribute the observation losses back to the devices and calculate the gradients (or even calculate the gradients on the device that runs the triplet mining) to utilize the negatives of the global batch?", "comments": ["If I understand it correctly, what you can do is before loss computation, use `tf.distribute.ReplicaContext.all_gather` to gather the logits from all the replicas and compute a loss based on the global logits.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47344\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47344\">No</a>\n"]}, {"number": 47343, "title": "Assert if y_pred and y_true have same shape in categorical_accuracy()", "body": "Refer this [issue](https://github.com/tensorflow/tensorflow/issues/46953) for details. ", "comments": ["@AdityaKane2001 Please check build failures, there seems to be a unittest that has `y_true` and `y_pred` such that their shapes are not equal.", "@amogh7joshi  Noted, but I don't know what to do with it. \ud83d\ude05 \r\nIf you see the aforementioned issue, you will see why one must assert that the shapes are equal. This is because the [`categorical_accuracy()` ](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/keras/metrics.py#L3250)function just uses a plain `argmax` function. Thus, if the shapes are unequal, there will be no errors but the results may be wrong. ", "Agreed. The likelihood is that there is simply a test case which contains certain incompatible values, and you simply need to edit the test case, but there is also the (albeit unlikely) situation in which the test case *cannot* be updated at which point this feature will not work.\r\n\r\nRegardless, you should check the build failures and update any necessary tests. Basically, click on \"details\" next to the failed test and scroll through the output log until you find the point where it says \"FAILED\" and something related to asserted shapes. Then, just update the test file which corresponds to it. Let me know if you have any other questions.", "@amogh7joshi \r\nI went through the logs. I found the module but I don't know what to update. Can you please guide a bit on the same? I am quite new to this...", "Following test is raising the `AssertionError`:\r\n` test_saving_with_sequence_features`\r\nSource code [here](https://github.com/tensorflow/tensorflow/blob/362f9fc585905592094db83e4f677a0fc7a65972/tensorflow/python/keras/feature_column/sequence_feature_column_test.py#L617).\r\n@chenmoneygithub  \r\nHow can I resolve these errors?\r\nThanks.", "Hi, sorry for the late reply. I tried your code by myself, and it will break the graph mode tests. The code below works:\r\n```python\r\ncheck_ops.assert_equal_v2(\r\n      array_ops.shape_v2(y_pred), array_ops.shape_v2(y_pred))\r\n```\r\n\r\nYou can copy & paste the code, and I will approve it. Thanks for your contribution to Keras!", "@chenmoneygithub \r\nDid you mean \r\n```\r\ncheck_ops.assert_equal_v2(\r\n      array_ops.shape_v2(y_pred), array_ops.shape_v2(y_true))\r\n```\r\n\r\nI have still commited this change for now.\r\n\r\n\r\n> ```python\r\n> check_ops.assert_equal_v2(\r\n>       array_ops.shape_v2(y_pred), array_ops.shape_v2(y_pred))\r\n> ```\r\n\r\n\r\n", "Yes, sorry for the typo. ", "@chenmoneygithub \r\nCommitted the required changes.\r\nThanks a lot for the support! ", "@AdityaKane2001 Can you please fix build failures ? Thanks!", "@gbaned\r\nCan you please tell how can I change the necessary test cases? \r\nI can locate the files which are causing the errors, but I am not able to get to the actual test cases.\r\n\r\nEdit: As you can see in the PR, we are asserting that the shapes of `y_true` and `y_pred` are the same. In a particular test case, the shapes are `(4,3)` and `(4,1)`. Is this intentional? If not, the test cases may be incorrect.\r\n\r\nEdit: Found and corrected the bug in this [commit](https://github.com/tensorflow/tensorflow/pull/47343/commits/02b016a9bcea40f9dc6e524ab2600ece32d4690c)", "@gbaned  @chenmoneygithub \r\nI believe this time around it's breaking for eager mode.\r\nAfter my last commit, there were no tests done for Windows and Ubuntu. \r\nThanks,\r\nAditya\r\n\r\n\r\nEdit: No tests were done after my last [commit](https://github.com/tensorflow/tensorflow/pull/47343/commits/02b016a9bcea40f9dc6e524ab2600ece32d4690c). I believe I have  resolved the bugs for `Ubuntu CPU` and `Ubuntu sanity check`, as they were caused by my code. `Windows Bazel` and `Windows Bazel GPU` builds are not yet resolved.  ", "@gbaned @chenmoneygithub \r\nThis PR was originally intended to assert the shapes in categorical accuracy function, as mentioned in the [issue](https://github.com/tensorflow/tensorflow/issues/46953). But, there are a few errors in `tensorflow/tensorflow/python/keras/layers/local_test.py` file, which causes 3 of the tests to fail. \r\nSo I intend to do the following:\r\n1. Use this PR to just change the `categorical_accuracy()` function. \r\n2. I'll create a fresh PR which takes care of the errors in the aforementioned file.\r\n\r\nI'll proceed to do the same if it's good by you.\r\n\r\nThanks again for the support,\r\nAditya Kane\r\n\r\nEdit: Alternatively, I could keep this PR as it is, as it is causing least errors, and continue debugging in a new PR.", "Thanks Aditya! "]}, {"number": 47342, "title": "How to convert a TF1 model with custom_getter to TF2", "body": "I need to do this very same calculation in TF2 which ends up by calculating `v1` and `v2` which are softmax output and the same softmax output but calculated using a \r\n`tf.train.ExponentialMovingAverage` `average()` method. The original example can be found [here](https://github.com/openai/baselines/blob/master/baselines/acer/acer.py) at line 88 but I simplified the code in the example here for readability.\r\n\r\n**TF1 code:**\r\n\r\n    import tensorflow as tf\r\n    \r\n    \r\n    with tf.variable_scope('my_model', reuse=tf.AUTO_REUSE):\r\n        step_model = StepModel(step_param)  # step_param is a placeholder\r\n        train_model = TrainModel(train_param)  # train_param is a placeholder\r\n    \r\n    params = tf.trainable_variables('my_model')\r\n    ema = tf.train.ExponentialMovingAverage(0.99)\r\n    ema_apply_op = ema.apply(params)\r\n    \r\n    def custom_getter(getter, *args, **kwargs):\r\n        return ema.average(getter(*args, **kwargs))\r\n    \r\n    with tf.variable_scope(\"my_model\", custom_getter=custom_getter, reuse=True):\r\n        avg_model = AveragingModel(avg_param)  # avg_param is a placeholder\r\n    \r\n    train_model_p = tf.nn.softmax(train_model.pi)\r\n    avg_model_p = tf.nn.softmax(avg_model.pi)\r\n    common_param = some_param\r\n    with tf.get_default_session() as sess:\r\n        feed_dict = {\r\n            train_model.train_param: common_param,\r\n            avg_model.avg_param: common_param,\r\n        }\r\n        ops = [train_model_p, avg_model_p]\r\n        v1, v2 = sess.run(ops, feed_dict)[1:]\r\n\r\nIf I want to keep a moving average of the trainable variables in TF2 without worrying about `v1` and `v2`, I'd do:\r\n\r\n\r\n    from tensorflow.keras.optimizers import Adam\r\n    from tensorflow_addons.optimizers import MovingAverage\r\n    from tensorflow.keras.models import Model\r\n\r\n\r\nand then I'd wrap the `tf.keras.optimizers.Optimizer()` being used:\r\n\r\n    \r\n    model = Model(...)\r\n    optim = MovingAverage(Adam())\r\n    model.compile(optimizer=optim)\r\n    model.fit(...)\r\n\r\nBut I need to get the values of `v1` and `v2` for further calculations.\r\n    ", "comments": ["@emadboctorx,\r\nPlease go through [this guide](https://www.tensorflow.org/guide/migrate) to migrate your code from TF 1.x to TF 2.x.\r\n\r\nAlso, this question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 47341, "title": "Give Grappler hints about function XLA compilation so it can selectively apply compatible optimizations", "body": "Some Grappler optimizations can cause problems for XLA. This was first discovered by the MemoryOptimizer pass creating TemporaryVariable ops without knowing if the later compiling XLA device implemented the TemporaryVariable OpKernel [[issue](https://github.com/tensorflow/tensorflow/issues/30580)].\r\n\r\nThe first fix for this was to disable Grappler for any functions called from an XlaLaunch node [[commit 1](https://github.com/tensorflow/tensorflow/commit/471b73c238709fb796929eb412f1dab763b3f8cc)].\r\nIt was found that this isn't sufficient, as some functions can be encapsulated out of the main XlaLaunch cluster function e.g. While ops, so a later fix was added to also disable Grappler for any functions transitively called by an XlaLaunch function [[commit 2](https://github.com/tensorflow/tensorflow/commit/e0bb74087f440394f6df00c5c7ff36e50d23132a)].\r\n\r\nHowever, a more general approach may be to introduce hints which Grappler can use to avoid making certain XLA-incompatible optimizations, allowing them to work together.\r\nThis PR introduces a GrapplerXlaHints struct which is populated by a GenerateXlaHints function, which allows individual Grappler optimizers to switch themselves off depending on the GrapplerItem's XLA hints.\r\nCurrently the PR disables the constant_folding, arithmetic_optimizer and the SchedulingPass of the MemoryOptimizer based on these hints.\r\n\r\nNote that this will cause a change in behaviour where the immediate XlaLaunch functions will now go through Grappler where they otherwise weren't before commit 1 (with appropriate passes selectively disabling themselves).\r\n\r\nI have added a test which passes locally that checks the hints are generated properly for an example call tree.\r\nI have also linted all files according to [this guide](https://www.tensorflow.org/community/contribute/code_style).", "comments": ["Is this when using the `jit_compile` (used to be `experimental_compile`) API?", "I believe so, or really anything that could result in XlaLaunch nodes being in the graph.", "Sorry I have a hard trouble following why do we need a whole struct with \"hints\"? Why is simple boolean \"are we inside a compiled cluster\" not sufficient?", "> Sorry I have a hard trouble following why do we need a whole struct with \"hints\"? Why is simple boolean \"are we inside a compiled cluster\" not sufficient?\r\n\r\nMy vision for this closer XLA<->Grappler integration is that we try to allow them to work together as much as is possible, and try to avoid unconditionally disabling entire grappler passes for rare XLA<->Grappler interactions.\r\nFor example, in this diff, I found the arithmetic optimizer pass only needs disabling when the XLA graph contains a subgraph that will be extracted out of it later:\r\n```\r\n// Do not rearrange arithmetic if the item will be compiled by XLA and\r\n// contains an extracted subgraph, since the rearrangement can interleave the\r\n// subgraphs in such a way as to cause a host<->device communication deadlock.\r\n```\r\nI think this is quite a rare situation, so disabling the arithmetic optimizer pass unconditionally for all XLA graphs just for this situation seems overkill.", "> My vision for this closer XLA<->Grappler integration is that we try to allow them to work together as much as is possible, and try to avoid unconditionally disabling entire grappler passes for rare XLA<->Grappler interactions\r\n\r\nBut why? Is there any purpose in doing the optimizations in grappler which can be done later in XLA?", "@markf-gc Can you please check @cheshire's comments and keep us posted ? Thanks!", "@markf-gc Any update on this PR? Please. Thanks!", "@markf-gc Any update on this PR? Please. Thanks!", "@markf-gc Any update on this PR? Please. Thanks!", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 47340, "title": "Calling model.test_on_batch after model.evaluate returns corrupted values for the loss and the metrics", "body": "**System information**\r\n- Google colab with tf 2.4.1 (v2.4.1-0-g85c8b2a817f )\r\n- with CPU or GPU runtimes, it does not matter\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `model.test_on_batch` after calling `model.evaluate` gives incorrect results.\r\n\r\n**Describe the expected behavior**\r\n\r\nCalling `model.test_on_batch` should  return the a value that does not depend on whether`model.evaluate`  was called before.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nLet's define a randomly initialized model evaluated on randomly generated data. If we call `model.test_on_batch` directly, everything is fine:\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nrng = np.random.RandomState(0)\r\nbatch_size = 32\r\nn_samples, n_features = batch_size * 10, 5\r\nX = rng.normal(size=(n_samples, n_features))\r\ny = rng.randint(low=0, high=2, size=X.shape[0])\r\n\r\nmodel = Sequential([Dense(1, input_shape=(n_features,),\r\n                          activation=\"sigmoid\")])\r\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nprint(\"model.test_on_batch without model.evaluate\")\r\nfor i in range(3):\r\n    loss, acc = model.test_on_batch(X[:batch_size], y[:batch_size])\r\n    print(loss, acc)\r\n```\r\n\r\noutput:\r\n\r\n```\r\nmodel.test_on_batch without model.evaluate\r\n6.294709205627441 0.5625\r\n6.294709205627441 0.5625\r\n6.294709205627441 0.5625\r\n```\r\n\r\nIf we then call `evaluate`, the fist call of `model.test_on_batch` return an incorrect value:\r\n\r\n```python\r\nnormal_loss, normal_acc = model.evaluate(\r\n    X, y, batch_size=batch_size, verbose=0)\r\n\r\nprint(\"model.test_on_batch *after* model.evaluate\")\r\nfor i in range(3):\r\n    loss, acc = model.test_on_batch(X[:batch_size], y[:batch_size])\r\n    print(loss, acc)\r\n```\r\n\r\noutput:\r\n\r\n```\r\nmodel.test_on_batch *after* model.evaluate\r\n6.585799694061279 0.4801136255264282\r\n6.294709205627441 0.5625\r\n6.294709205627441 0.5625\r\n```\r\n\r\nNote: this is a minimal reproducer of the report found in the comments of a similar standalone keras issue: https://github.com/keras-team/keras/issues/14086\r\n\r\nIt is probably also the cause or at least related to corrupted loss/metric values reported when using `evaluate_generator`:\r\nhttps://github.com/keras-team/keras/issues/13780", "comments": ["The problem also happens if the `batch_size` argument of evaluate equal or greater than `X.shape[0]`.", "@ogrisel \r\nI ran your code on tf nightly and there is not much of a difference can you please refer to[ this link ](https://colab.research.google.com/gist/Saduf2019/09d1df811c8d2fc688079f63fd18bcf8/untitled548.ipynb)and let us know.", "@Saduf2019 The difference is random because I did not seed the model parameters init. But the bug is still there in tf-nightly. For instance I re-ran your notebook and got:\r\n\r\n```\r\nmodel.test_on_batch *after* model.evaluate\r\n5.409824848175049 0.46022728085517883\r\n4.271170616149902 0.53125\r\n4.271170616149902 0.53125\r\n```", "Was able to reproduce this issue in TF v2.5,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/a7702376cf6b787efc866332bc967775/untitled144.ipynb?authuser=1)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/95e84ecec976207debf6513d1784ff8b/untitled166.ipynb?authuser=1) ..Thanks!", "@ogrisel Can you confirm that `evaluate` corrupts also `train_on_batch`?", "It seems I cannot reproduce this at https://github.com/keras-team/keras/pull/15342", "I can still reproduce on TF 2.6.0 on colab:\r\n\r\nhttps://colab.research.google.com/drive/1NgS57SegqurxwgAu0t3R9N3TPudWJpo9?usp=sharing\r\n", "@ogrisel Ok I can reproduce now on master at https://github.com/keras-team/keras/pull/15342", "Hi @ogrisel ! This issue is getting resolved in TF 2.7 . Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/6e36afd8ec501642367c885cc916cd11/tensorflow-gh-47340.ipynb#scrollTo=DyvDY7N5ipBo) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47340\">No</a>\n"]}, {"number": 47339, "title": "ModelCheckPoint callback creates \".png\" file instead of \".h5\" file ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memgory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nif we already have a saved model say \"model_name.h5\" in models directory and then we use ModelCheckPoint callback and provide the same name(\"model_name.h5\") to save the model. After the training is finished we get \"model_name.png\" file instead of \"model_name.h5\" file. \r\n\r\n**Describe the expected behavior**\r\nThe user should get \"model_name.h5\" file instead of \"model_name.png\" after training.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1rBD6SZwjU9wgxfsgNn5COeaDmCpGI9IQ#scrollTo=0qqWjf-v1lVw\r\n\r\nNote: run the training cell twice. first time to generate \"model_name.h5\" file then when you run the training cell second time, we can observe \"model_name.png\" is generated \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47339\">No</a>\n"]}, {"number": 47338, "title": "TFLM: Add FVP script to CI script", "body": "This is a fix for: https://github.com/tensorflow/tensorflow/issues/46829", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47337, "title": "SparseTensorDenseMatMul adjoint_a takes transpose, not adjoint", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nOn CPU (haven't checked GPU), the `adjoint_a` argument to `SparseTensorDenseMatMul` op (accessed via `tf.sparse.sparse_dense_matmul`) takes the transpose of the argument but not the conjugate.\r\n\r\nLooks like the issue is here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.h#L55; shouldn't that be taking a conjugate?\r\n\r\n**Describe the expected behavior**\r\n`adjoint_a` takes the adjoint of `a`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nx = tf.sparse.from_dense([[1j]])\r\ny = tf.constant([[1]], dtype=tf.complex128)\r\nprint(tf.sparse.sparse_dense_matmul(x, y, adjoint_a=True).numpy()) # Gives [[1j]]\r\n```\r\nSee gist here: https://colab.research.google.com/drive/1qRZo4q1qwwt_pt6EfxRJPVohXmDUrdvN?usp=sharing\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c29e066dc7630aff085c8e484bc0020c/47337.ipynb). Thanks!", "I'm attempting to fix this in https://github.com/tensorflow/tensorflow/pull/47355, although I haven't actually been able to test it since my laptop is weak <_<", "This is now fixed in tf-nightly: https://colab.research.google.com/drive/1qRZo4q1qwwt_pt6EfxRJPVohXmDUrdvN?usp=sharing", "Thanks for the fix, this should be a part of TF 2.5 release. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47337\">No</a>\n", "Thanks @ymodak ; any idea when we should expect TF 2.5 to be released?"]}, {"number": 47336, "title": "How to extend self attention in transformer with local attention", "body": "Hello everyone.. I'm trying (Vaswani et al, 2017) for machine translation task. Some of the recent findings (\"https://www.aclweb.org/anthology/P19-1295/\" and \"https://www.aclweb.org/anthology/D19-5622/\" ) show that number of heads can extract redundant features as original work by Vaswani et al, 2017 used global attention. These papers suggested to use local attention along with global attention. So, I'm trying to extent Vaswani et al, 2017 with local attention as suggested by \"https://www.aclweb.org/anthology/D19-5622/\". \r\nIn the paper, suggested global mask as follows:\r\n![global mask](https://user-images.githubusercontent.com/60576100/108811216-df9b2200-75d2-11eb-9ca4-55c3e4a398c2.png)\r\n\r\nand local mask as: \r\n![local mask](https://user-images.githubusercontent.com/60576100/108811141-b67a9180-75d2-11eb-89af-2aed43ace7a1.png)\r\n\r\n\r\n\r\nI have created global and local mask as follows: \r\n        seq_len = tf.shape(seq)[1]\r\n        global_mask = tf.zeros((tf.shape(seq)[0], tf.shape(seq)[1]))\r\n                \r\n        local_mask = global_mask\r\n        local_mask=tf.Variable(local_mask)\r\n        for i in range(0, tf.shape(seq)[0]):\r\n          for j in range(0,tf.shape(seq)[1]):\r\n            if not (i-w<=j and j<=i+w):\r\n              local_mask[i,j].assign(-1e9) \r\n        local_mask = tf.convert_to_tensor(local_mask)\r\n\r\n       global_mask=global_mask[:,tf.newaxis,tf.newaxis,:]\r\n       local_mask=local_mask[:,tf.newaxis,tf.newaxis,:]\r\n\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 47335, "title": "Slower on a Nvidia 1080TI GPU than on CPU ", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.4.1\r\n- Python version : 3.8\r\n- CUDA V 11.0/cuDNN version 8:\r\n- GPU model: Nvidia 1080TI 11gb RAM\r\n\r\nRunning same script is 2x slower on GPU than CPU \r\n\r\nSteps to reproduce the problem :\r\n\r\nobj = DeepFace.analyze(img_path = \"target.jpg\", actions = ['age', 'gender', 'race'])\r\n\r\nThis framework uses several models but in this case VGG Face was used for analysis .\r\n\r\nLog from GPU run  :\r\n\r\n2021-02-23 04:35:18.719513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-23 04:35:18.719590: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-02-23 04:35:19.596969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-23 04:35:19.597049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-02-23 04:35:19.597068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-02-23 04:35:19.597404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-23 04:35:19.598430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-23 04:35:19.599326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-02-23 04:35:19.600166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10271 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:00:05.0, compute capability: 6.1)\r\nAction: age:   0%|                                                                                                                     | 0/3 [00:00<?, ?it/s]2021-02-23 04:35:23.409541: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-02-23 04:35:23.410483: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199965000 Hz\r\n2021-02-23 04:35:23.581168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-02-23 04:35:24.535674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-02-23 04:35:24.910561: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\nAction: race: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:05<00:00,  1.80s/it]\r\n29  years old  asian     Woman\r\n10.368096351623535  seconds\r\n\r\n************** CPU RUn log :\r\n\r\n2021-02-22 21:37:27.379054: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-22 21:37:27.382967: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2021-02-22 21:37:27.385962: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-02-22 21:37:27.394354: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-101PVLV\r\n2021-02-22 21:37:27.398139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-101PVLV\r\n2021-02-22 21:37:27.400591: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-22 21:37:27.408451: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nAction: age:   0%|                                                                               | 0/3 [00:00<?, ?it/s]2021-02-22 21:37:31.366069: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nAction: race: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.63it/s]\r\n29  years old  asian     Woman\r\n5.869304418563843  seconds\r\n\r\n", "comments": ["@TriptonPt \r\n\r\nCan you please share standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47334, "title": "ERROR: Didn't find op for builtin opcode 'ADD' version '1'", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, with MCU Expresso\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.3.1\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33, etc.): IMX RT 1060\r\n\r\n**Describe the problem**\r\nThe application as per [this code](https://github.com/kavyaprasad22/MNV3) is saved as .pb file and converted to .tflite using [this convert.py](https://github.com/kavyaprasad22/MNV3/blob/main/conver.py). After uploading this to IMX RT 1060, the debugger throws an ```ERROR: Didn't find op for built-in opcode 'ADD' version '1'```. Is the opcode ADD failed to  be converted by the tflite or is it because of any missing header files in the client end?\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\npython train_RJ.py\r\npython conver.py\r\n``` \r\nafter this converted to *.h using ``` xxd -i model.tflite > model.h``` \r\nand uploaded with MCU Xpresso to IMX RT 1060 using tensorflow_lite_cifar_10 example provided under eiq folder from SDK 2.9.1 build from [this builder](https://mcuxpresso.nxp.com/en/builder?hw=EVK-MIMXRT1060).\r\n\r\n\r\n", "comments": ["Could you share which ops are being included in your model? At least, could you share at least the operation name of the problematic operation node?", "@abattery My opcode for 'ADD' seems to be causing the issue. All my layers for the model are defined in dpd_format.py in this [link](https://github.com/kavyaprasad22/MNV3). Please let me know if any additional information is needed.", "Is it possible to share your model to us? The reproducible steps to create a model also would be helpful.", "Hi @abattery, I was working with @kavyaprasad22. The code of the model is already there in the link provided by @kavyaprasad22  in the reply \r\n\r\n> @abattery My opcode for 'ADD' seems to be causing the issue. All my layers for the model are defined in dpd_format.py in this [link](https://github.com/kavyaprasad22/MNV3). Please let me know if any additional information is needed.\r\n\r\nDo you want the *.tflite file of the model?\r\n\r\nThanks,\r\nArjun.\r\n", "This [link](https://github.com/kavyaprasad22/MNV3/tree/main/tflite) has the files for running the model on client side (IMXRT 1060).", "Generally, there are two TF versions, one for executions and one for conversion. Could you make sure that the TF version used for executions should have the same TF version with the one for conversion or higher? We do not guarantee the compatibility that the TF for executions has a lower version than the TF for conversion.", "Hi @abattery  both training and conversion was done using TensorFlow 2.3.1 CPU versions.", "Is the installed TF version at IMX RT 1060 TF 2.3.1 or higher?\r\n\r\nTo make sure whether the converted model is correct or not, could you verify that the converted model is runnable  in the machine done at the training and conversion stage?", "Hi,\r\nWe downgraded the tensor flow in the host PC to 2.3.1 so that it matches with with the version mentioned in the SDK provided by the NXP for the Target IMX RT 1060 found [here](https://mcuxpresso.nxp.com/en/builder?hw=EVK-MIMXRT1060).\r\n\r\nThanks,\r\nArjun.", "Once all the TF versions is aligned, it will not encounter the versioning issues. After the alignment, is it possible to run the model on the device?", "If possible, could you share the converted model and the full error log messages to us for debugging?", "Hi, \r\n\r\nWe solved the error by adding the missing opcodes in the IMX microcontroller software part.\r\n\r\nIt was not the error of TensorFlow but of the target.\r\n\r\nThank you,\r\nArjun.", "@kavyaprasad22,\r\n\r\nAs @Arjun-NA has pointed out that the issue is solved and also its not from TF end, Can you confirm if we are good to close this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47334\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47334\">No</a>\n"]}, {"number": 47333, "title": "Refactor depthwise_conv to share code between reference and optimized kernels", "body": "Move shared structs / helper functions into depthwise_conv_common.cc\r\nClean up some of the existing code to directly call the reference implementations (made possible by the refactor of the helper functions).\r\n\r\nTested with:\r\nmake -j8 -f tensorflow/lite/micro/tools/make/Makefile test_kernel_depthwise_conv_test\r\n\r\nmake -j8 -f tensorflow/lite/micro/tools/make/Makefile OPTIMIZED_KERNEL_DIR=cmsis_nn TARGET=stm32f4 test_kernel_depthwise_conv_test\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_depthwise_conv_test\r\n\r\nProgress towards http://b/177457688", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 47332, "title": "[TFL] Support I32 for OptimizeSlice pattern", "body": "Somehow, some int32 `begin` will be generated. This PR extends the original pattern to support i32 `begin` because we have type constraint of `TFL_I32OrI64Tensor` on `begin` in ODS. Pattern is found in `tf.keras.layers.MultiHeadAttention`, for example.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlayer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\r\ntarget = tf.keras.Input(shape=[8, 16], batch_size=1)\r\nsource = tf.keras.Input(shape=[4, 16], batch_size=1)\r\noutput_tensor = layer(target, source, return_attention_scores=False)\r\nmodel = tf.keras.Model([target, source], output_tensor)\r\n```\r\n\r\nAlso fix:\r\n- when input has dynamic shape.\r\n- when shape has -1.\r\n\r\nIt turns out that we have to check if `shape` is the shape of `input`. Otherwise, given `input: tensor<?xf32>` and `output: tensor<?xf32>`, the original pattern is not going to work.", "comments": ["There is an error due after this new pattern. Please take a look.\r\n\r\n[Before]\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 12 : i32, producer = 685 : i32}}  {\r\n  func @main(%arg0: tensor<f32> {tf.device = \"/device:CPU:0\"}) -> tensor<?xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"Placeholder\", outputs = \"TensorListStack\"}} {\r\n    %cst = constant dense<0> : tensor<i32>\r\n    %cst_0 = constant dense<0.000000e+00> : tensor<4xf32>\r\n    %0:3 = \"tfl.while\"(%cst, %cst, %cst_0) ( {\r\n    ^bb0(%arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<?xf32>):  // no predecessors\r\n      %cst_1 = constant dense<4> : tensor<i32>\r\n      %1 = \"tfl.less\"(%arg2, %cst_1) : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n      \"tfl.yield\"(%1) : (tensor<i1>) -> ()\r\n    },  {\r\n    ^bb0(%arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<?xf32>):  // no predecessors\r\n      %cst_1 = constant dense<1.000000e+00> : tensor<f32>\r\n      %cst_2 = constant dense<1> : tensor<i32>\r\n      %cst_3 = constant dense<0> : tensor<1xi32>\r\n      %cst_4 = constant dense<-1> : tensor<1xi32>\r\n      %cst_5 = constant dense<1> : tensor<1xi32>\r\n      %1 = tfl.add %arg0, %arg0 {fused_activation_function = \"NONE\"} : tensor<f32>\r\n      %2 = tfl.add %1, %cst_1 {fused_activation_function = \"NONE\"} : tensor<f32>\r\n      %3 = tfl.add %arg2, %cst_2 {fused_activation_function = \"NONE\"} : tensor<i32>\r\n      %4 = \"tfl.reshape\"(%arg2, %cst_5) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n      %5 = \"tfl.slice\"(%arg3, %cst_3, %4) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n      %6 = \"tfl.reshape\"(%3, %cst_5) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n      %7 = \"tfl.slice\"(%arg3, %6, %cst_4) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n      %8 = \"tfl.reshape\"(%2, %cst_5) : (tensor<f32>, tensor<1xi32>) -> tensor<1xf32>\r\n      %9 = \"tfl.concatenation\"(%5, %8, %7) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<1xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n      %10 = tfl.add %arg1, %cst_2 {fused_activation_function = \"NONE\"} : tensor<i32>\r\n      \"tfl.yield\"(%10, %3, %9) : (tensor<i32>, tensor<i32>, tensor<?xf32>) -> ()\r\n    }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<4xf32>) -> (tensor<i32>, tensor<i32>, tensor<?xf32>)\r\n    return %0#2 : tensor<?xf32>\r\n  }\r\n}\r\n```\r\n\r\n[After]\r\n```\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 12 : i32, producer = 685 : i32}}  {\r\n  func @main(%arg0: tensor<f32> {tf.device = \"/device:CPU:0\"}) -> tensor<?xf32> attributes {tf.entry_function = {control_outputs = \"\", inputs = \"Placeholder\", outputs = \"TensorListStack\"}} {\r\n    %cst = constant dense<0> : tensor<i32>\r\n    %cst_0 = constant dense<0.000000e+00> : tensor<4xf32>\r\n    %0:3 = \"tfl.while\"(%cst, %cst, %cst_0) ( {\r\n    ^bb0(%arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<?xf32>):  // no predecessors\r\n      %cst_1 = constant dense<4> : tensor<i32>\r\n      %1 = \"tfl.less\"(%arg2, %cst_1) : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n      \"tfl.yield\"(%1) : (tensor<i1>) -> ()\r\n    },  {\r\n    ^bb0(%arg1: tensor<i32>, %arg2: tensor<i32>, %arg3: tensor<?xf32>):  // no predecessors\r\n      %cst_1 = constant dense<1.000000e+00> : tensor<f32>\r\n      %cst_2 = constant dense<1> : tensor<i32>\r\n      %cst_3 = constant dense<-1> : tensor<1xi32>\r\n      %cst_4 = constant dense<1> : tensor<1xi32>\r\n      %1 = tfl.add %arg0, %arg0 {fused_activation_function = \"NONE\"} : tensor<f32>\r\n      %2 = tfl.add %1, %cst_1 {fused_activation_function = \"NONE\"} : tensor<f32>\r\n      %3 = tfl.add %arg2, %cst_2 {fused_activation_function = \"NONE\"} : tensor<i32>\r\n      %4 = \"tfl.reshape\"(%3, %cst_4) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n      %5 = \"tfl.slice\"(%arg3, %4, %cst_3) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n      %6 = \"tfl.reshape\"(%2, %cst_4) : (tensor<f32>, tensor<1xi32>) -> tensor<1xf32>\r\n      %7 = \"tfl.concatenation\"(%arg3, %6, %5) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<1xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n      %8 = tfl.add %arg1, %cst_2 {fused_activation_function = \"NONE\"} : tensor<i32>\r\n      \"tfl.yield\"(%8, %3, %7) : (tensor<i32>, tensor<i32>, tensor<?xf32>) -> ()\r\n    }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<4xf32>) -> (tensor<i32>, tensor<i32>, tensor<?xf32>)\r\n    return %0#2 : tensor<?xf32>\r\n  }\r\n}\r\n```", "To correct the above case, the shape argument should be checked to verify that it covers the original shape.", "Looks like `IsTailOfShape` will allow dynamic dim, which should not happen in this pattern.", "@abattery Sorry for the delay. See if this is more readable. I follow the naming convention of `CanOptimizeIdentityGatherNdOrScatterNdOp` in the same file."]}, {"number": 47331, "title": "Adabound feature request ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n[Adabound](https://arxiv.org/abs/1902.09843) is a powerful optimizer for training models combining the advantages of SGD and Adam/momentum. Currently, there are no official TF/pytorch implementations available. Some projects in PYPI [keras-adabound](https://pypi.org/project/keras-adabound/) [torch-optimizer](https://pypi.org/project/torch-optimizer/) provides implementaions, but why not support an official Adabound and amsbound optimizer!\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nEveryone hoping to experiment adabound for accelerated training.\r\n**Any Other info.**\r\n", "comments": []}, {"number": 47330, "title": "memory increasing slowly and largely at the beginning of model.fit() in tf.keras", "body": "**System information**\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows\r\n    TensorFlow installed from (source or binary):github release\r\n    TensorFlow version (use command below):2.3.1\r\n    Python version:3.8.1\r\n    CUDA/cuDNN version:10.1, 7.6.\r\n    GPU model and memory: RTX2080Ti x4 \r\n**Describe the current behavior**\r\nIt takes to much time for memory increasing before keras logs come and sometimes shows W:\"multi-device function optimization failure\"\r\n**Describe the expected behavior**\r\nLess time for memory increasing\r\n\r\n**Code to reproduce the issue**\r\n\r\n> main.py:\r\n\r\n```python\r\n\r\n    import tensorflow as tf\r\n    import tensorflow_addons as tfa\r\n    import tensorflow_datasets as tfds\r\n    from tensorflow.keras.callbacks import TensorBoard\r\n    from model import VisionTransformer\r\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\n    ds = tfds.load(\"imagenet_resized/32x32\", as_supervised=True)\r\n    ds_train = (\r\n        ds[\"train\"]\r\n        .cache()\r\n        .shuffle(5 * 4096)\r\n        .batch(4096)\r\n        .prefetch(AUTOTUNE)\r\n    )\r\n    ds_test = (\r\n        ds[\"validation\"]\r\n        .cache()\r\n        .batch(4096)\r\n        .prefetch(AUTOTUNE)\r\n    )\r\n\r\n    strategy = tf.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n        model = VisionTransformer(\r\n            image_size=32,\r\n            patch_size=4,\r\n            num_layers=4,\r\n            num_classes=1000,\r\n            d_model=64,\r\n            num_heads=4,\r\n            mlp_dim=128,\r\n            channels=3,\r\n            dropout=0.1,\r\n        )\r\n        model.compile(\r\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n                from_logits=True\r\n            ),\r\n            optimizer=tfa.optimizers.AdamW(\r\n                learning_rate=3e-4, weight_decay=1e-4\r\n            ),\r\n            metrics=[\"accuracy\"],\r\n        )\r\n\r\n    model.fit(\r\n        ds_train,\r\n        validation_data=ds_test,\r\n        epochs=300,\r\n    )\r\n`\r\n\r\n> model.py:\r\n\r\n`\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\nfrom tensorflow.keras.layers import (\r\n    Dense,\r\n    Dropout,\r\n    LayerNormalization,\r\n)\r\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\r\n\r\nclass MultiHeadSelfAttention(tf.keras.layers.Layer):\r\n    def __init__(self, embed_dim, num_heads=8):\r\n        super(MultiHeadSelfAttention, self).__init__()\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        if embed_dim % num_heads != 0:\r\n            raise ValueError(\r\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\r\n            )\r\n        self.projection_dim = embed_dim // num_heads\r\n        self.query_dense = Dense(embed_dim)\r\n        self.key_dense = Dense(embed_dim)\r\n        self.value_dense = Dense(embed_dim)\r\n        self.combine_heads = Dense(embed_dim)\r\n\r\n    def attention(self, query, key, value):\r\n        score = tf.matmul(query, key, transpose_b=True)\r\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\r\n        scaled_score = score / tf.math.sqrt(dim_key)\r\n        weights = tf.nn.softmax(scaled_score, axis=-1)\r\n        output = tf.matmul(weights, value)\r\n        return output, weights\r\n\r\n    def separate_heads(self, x, batch_size):\r\n        x = tf.reshape(\r\n            x, (batch_size, -1, self.num_heads, self.projection_dim)\r\n        )\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs):\r\n        batch_size = tf.shape(inputs)[0]\r\n        query = self.query_dense(inputs)\r\n        key = self.key_dense(inputs)\r\n        value = self.value_dense(inputs)\r\n        query = self.separate_heads(query, batch_size)\r\n        key = self.separate_heads(key, batch_size)\r\n        value = self.separate_heads(value, batch_size)\r\n        attention, weights = self.attention(query, key, value)\r\n        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\r\n        concat_attention = tf.reshape(\r\n            attention, (batch_size, -1, self.embed_dim)\r\n        )\r\n        output = self.combine_heads(concat_attention)\r\n        return output\r\n\r\n\r\nclass TransformerBlock(tf.keras.layers.Layer):\r\n    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\r\n        super(TransformerBlock, self).__init__()\r\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\r\n        self.mlp = tf.keras.Sequential(\r\n            [\r\n                Dense(mlp_dim, activation=tfa.activations.gelu),\r\n                Dropout(dropout),\r\n                Dense(embed_dim),\r\n                Dropout(dropout),\r\n            ]\r\n        )\r\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\r\n        self.dropout1 = Dropout(dropout)\r\n        self.dropout2 = Dropout(dropout)\r\n\r\n    def call(self, inputs, training):\r\n        inputs_norm = self.layernorm1(inputs)\r\n        attn_output = self.att(inputs_norm)\r\n        attn_output = self.dropout1(attn_output, training=training)\r\n        out1 = attn_output + inputs\r\n        out1_norm = self.layernorm2(out1)\r\n        mlp_output = self.mlp(out1_norm)\r\n        mlp_output = self.dropout2(mlp_output, training=training)\r\n        return mlp_output + out1\r\n\r\nclass VisionTransformer(tf.keras.Model):\r\n    def __init__(\r\n        self,\r\n        image_size,\r\n        patch_size,\r\n        num_layers,\r\n        num_classes,\r\n        d_model,\r\n        num_heads,\r\n        mlp_dim,\r\n        channels=3,\r\n        dropout=0.1,\r\n    ):\r\n        super(VisionTransformer, self).__init__()\r\n        num_patches = (image_size // patch_size) ** 2\r\n        self.patch_dim = channels * patch_size ** 2\r\n        self.patch_size = patch_size\r\n        self.d_model = d_model\r\n        self.num_layers = num_layers\r\n\r\n        self.rescale = Rescaling(1.0 / 255)\r\n        self.pos_emb = self.add_weight(\r\n            \"pos_emb\", shape=(1, num_patches + 1, d_model)\r\n        )\r\n        self.class_emb = self.add_weight(\"class_emb\", shape=(1, 1, d_model))\r\n        self.patch_proj = Dense(d_model)\r\n        self.enc_layers = [\r\n            TransformerBlock(d_model, num_heads, mlp_dim, dropout)\r\n            for _ in range(num_layers)\r\n        ]\r\n        self.mlp_head = tf.keras.Sequential(\r\n            [\r\n                LayerNormalization(epsilon=1e-6),\r\n                Dense(mlp_dim, activation=tfa.activations.gelu),\r\n                Dropout(dropout),\r\n                Dense(num_classes),\r\n            ]\r\n        )\r\n\r\n    def extract_patches(self, images):\r\n        batch_size = tf.shape(images)[0]\r\n        patches = tf.image.extract_patches(\r\n            images=images,\r\n            sizes=[1, self.patch_size, self.patch_size, 1],\r\n            strides=[1, self.patch_size, self.patch_size, 1],\r\n            rates=[1, 1, 1, 1],\r\n            padding=\"VALID\",\r\n        )\r\n        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])\r\n        return patches\r\n\r\n    def call(self, x, training):\r\n        batch_size = tf.shape(x)[0]\r\n        x = self.rescale(x)\r\n        patches = self.extract_patches(x)\r\n        x = self.patch_proj(patches)\r\n\r\n        class_emb = tf.broadcast_to(\r\n            self.class_emb, [batch_size, 1, self.d_model]\r\n        )\r\n        x = tf.concat([class_emb, x], axis=1)\r\n        x = x + self.pos_emb\r\n\r\n        for layer in self.enc_layers:\r\n            x = layer(x, training)\r\n\r\n        # First (class token) is used for classification\r\n        x = self.mlp_head(x[:, 0])\r\n        return x\r\n`\r\n**Other info / logs**\r\n\r\n> Epoch 1/20\r\n2021-02-23 09:54:36.039640: W tensorflow/core/common_runtime/process_function_library_runtime.cc:675] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.\r\n```\r\nUsually it takes too much time from 'Epoch 1/20' log to progress bar log. By the way, could you tell me what things the increasing memory consists of?", "comments": ["Have you tried using [TensorFlow profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) tool to discover performance bottlenecks?\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47329, "title": "ImportError: cannot import name 'keras_modules_injection'", "body": "**System information**\r\n- Have I written custom code and  used `from tensorflow.python.keras.applications import keras_modules_injection` in Tensorflow 2.0, it worked, now I changed to Tensorflow 2.4, and facing error `ImportError: cannot import name 'keras_modules_injection'`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: Python 3.6.9\r\n- CUDA/cuDNN version: CUDA 11\r\n- GPU model and memory: Titan XP\r\n\r\nChecking TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n`2021-02-22 20:44:00.220774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nv2.4.0-49-g85c8b2a817f 2.4.1`\r\n\r\n\r\n**Describe the current behavior**\r\npython test_tf_imports.py \r\n2021-02-22 20:24:25.627138: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"test_tf_imports.py\", line 2, in <module>\r\n    from tensorflow.python.keras.applications import keras_modules_injection\r\nImportError: cannot import name 'keras_modules_injection'\r\n\r\n**Describe the expected behavior**\r\nWith the line of code `from tensorflow.python.keras.applications import keras_modules_injection` which was running in TF2.0, should run in TF2.4.x\r\n\r\n**Standalone code to reproduce the issue**\r\nfrom tensorflow.python.keras.applications import keras_modules_injection\r\nfrom tensorflow.python.util.tf_export import keras_export\r\n\r\n@keras_export('keras.applications.resnet50.ResNet50',\r\n              'keras.applications.ResNet50')\r\n@keras_modules_injection\r\ndef ResNet50(*args, **kwargs):\r\n  return resnet50.ResNet50(*args, **kwargs)\r\n", "comments": ["Was able to run the code without any issues with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/6ddd992c27768a534e5826cf4cf4f910/47329-2-0.ipynb) and TF v2.1. \r\n\r\nWhereas with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/fd545805499bcedb3a419e936951e206/47329.ipynb#scrollTo=pBdW4kdaERMT) and above, I'm facing an error stating `ImportError: cannot import name 'keras_modules_injection'`. Please check the linked gist for reference. Thanks!", "@rrklearn2020 `keras_modules_injection`  is just a `Decorator injecting tf.keras replacements for Keras modules.`. I think `keras_modules_injection` is not required for more recent versions starting from `v2.2`.\r\n\r\nPlease check https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/__init__.py from `master` branch and compare it with `2.0` branch https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/applications/__init__.py\r\n\r\nAre you noticing any error without this decorator when used with recent TF versions? Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47329\">No</a>\n"]}, {"number": 47328, "title": "tf.nn.depth_to_space with NCHW argument works in TF 2.3 but fails in TF 2.4", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Intel CPU, Macbook Pro 2019\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nIn TF 2.4, the tf.nn.depth_to_space command fails with any data_format parameter of \"NCHW\". \r\n\r\n```\r\npython test_d2l.py\r\n2021-02-22 15:37:05.145218: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-22 15:37:05.145510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nTraceback (most recent call last):\r\n  File \"test_d2l.py\", line 4, in <module>\r\n    tf.nn.depth_to_space(x, block_size=2, data_format=\"NCHW\")\r\n  File \"/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\", line 3922, in depth_to_space_v2\r\n    return gen_array_ops.depth_to_space(input, block_size, data_format, name=name)\r\n  File \"/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1625, in depth_to_space\r\n    return depth_to_space_eager_fallback(\r\n  File \"/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1661, in depth_to_space_eager_fallback\r\n    _result = _execute.execute(b\"DepthToSpace\", 1, inputs=_inputs_flat,\r\n  File \"/usr/local/anaconda3/envs/keras2onnx/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node DepthToSpace}} = DepthToSpace[T=DT_FLOAT, block_size=2, data_format=\"NCHW\"]\r\nAll kernels registered for op DepthToSpace:\r\n  device='CPU'; T in [DT_UINT64]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_INT64]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_UINT32]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_UINT16]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_INT16]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_UINT8]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_INT8]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_INT32]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_HALF]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_BFLOAT16]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_FLOAT]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_DOUBLE]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_COMPLEX64]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_COMPLEX128]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_BOOL]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_STRING]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_RESOURCE]; data_format in [\"NHWC\"]\r\n  device='CPU'; T in [DT_VARIANT]; data_format in [\"NHWC\"]\r\n [Op:DepthToSpace]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nCode block below runs without error in TF 2.3 (compiles cluster with XLA). \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nx =  np.random.rand(3, 4, 6, 8).astype(np.float32)\r\ntf.nn.depth_to_space(x, block_size=2, data_format=\"NCHW\")\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@acuskev It's because XLA:{CPU_GPU} device is not registered in 2.4.* by default.\r\n\r\nAs you can see in [here](https://colab.research.google.com/drive/1fnS3PvK_ralzf2jlMG73JMRXDKPApRvt?usp=sharing), with NCHW format, depth to space is running on XLA:CPU. To fix it, you can decorate the function with `tf.function(experimental_compile=True)` in 2.4.* and onwards.\r\n\r\n```python\r\n@tf.function(experimental_compile=True)\r\ndef func(x):\r\n  return tf.nn.depth_to_space(x, block_size=2, data_format=\"NCHW\")\r\n```", "Hi, \r\n\r\nThanks for the response but we are installing 2.4 through pip and cannot access the TF source code to change the function. Is there any approach to resolve just through changes in the model code? ", "@acuskev You don't need to build TF from source. You can use TF through `pip` and decorate the function as suggested by @WindQAQ .\r\n\r\nBased on @WindQAQ suggestion, you code works well with `func` as shown in this [gist](https://colab.research.google.com/gist/jvishnuvardhan/f0b499c954afbeeace1466b32d7e98ca/untitled90.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47328\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47328\">No</a>\n"]}]