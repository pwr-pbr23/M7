[{"number": 35607, "title": "Added usage example for tf.keras.callbacks.TensorBoard", "body": "I have added a usage example to tf.keras.callbacks.TensorBoard. I am participating in Google CodeIn 2019-20, and my GCI handle is jimmy_page. This is for the task Contribute to TensorFlow API Documentation.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35607) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35607) for more info**.\n\n<!-- ok -->", "Can you please expedite the review of this pull request, as I am doing it as part of Google CodeIn 19. I cannot progress and select another task unless this has been reviewed, please help.", "I have fixed the formatting errors, please review!", "I have incorporated the changes.", "I added the missing python keyword after triple backticks in the documentation."]}, {"number": 35606, "title": "log1pexp", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): -\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nPlease add `log1pexp(x)` function to math. This computes `log(1 + exp(x)` in a numerically stable way. Here is a Python implementation that I am currently using:\r\n\r\n```\r\n@tf.function\r\ndef log1pexp(x):\r\n    return tf.where(tf.less(x, 9),\r\n                    tf.math.log1p(tf.exp(x)),\r\n                    tf.where(tf.less(x, 16),\r\n                             x + tf.math.exp(-x),\r\n                             x))\r\n```\r\n\r\nBut a native C version will probably be faster.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis function is useful when programming RBM models, for instance. But it has many other uses.", "comments": ["@cossio\r\nyou can use `tf.math.softplus` function! \ud83c\udf89 \r\nhttps://www.tensorflow.org/api_docs/python/tf/math/softplus?version=stable", "@pshiko Thanks! I guess I was used to looking for this under the name `log1pexp`."]}, {"number": 35605, "title": "Request Adding Ops in Tensorflow Lite \uff1aRandomUniform", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, FLOOR, MAX_POOL_2D, MUL, RESHAPE, SPLIT, SUB, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: RandomUniform.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You may try converting your model using `Select TensorFlow Ops`. This should help you to avoid custom implementation for `RandomUniform`. \r\nSee https://www.tensorflow.org/lite/guide/ops_select#converting_the_model", "@aijiart Can you please share a standalone code to reproduce the issue. Did you try adding `converter.allow_custom_ops = True` to enable custom ops in the `tf.lite`? Thanks!", "> @aijiart Can you please share a standalone code to reproduce the issue. Did you try adding `converter.allow_custom_ops = True` to enable custom ops in the `tf.lite`? Thanks!\r\n\r\nThank. I have tried the step you point out, but I shouldn't test the tflite model directly in python. Because the tflite Interpreter can't process the tensorflow ops just now.", "> You may try converting your model using `Select TensorFlow Ops`. This should help you to avoid custom implementation for `RandomUniform`.\r\n> See https://www.tensorflow.org/lite/guide/ops_select#converting_the_model\r\n\r\nThanks a lot.  Problem sovled~"]}, {"number": 35604, "title": "module 'tensorflow' has no attribute 'function' .", "body": "Name: tensorflow\r\nVersion: 2.0.0 in Anaconda with Python v. 3.7.3", "comments": ["Can you share the code?\ud83d\ude0a", "@MrAbhishek319,\r\nCould you please provide the link for the document you are referring to? Can you please elaborate the context as well? Thanks!", "Any updates regarding this issue? Thanks!", "I run the code through google colaboratory and its working. I was running the code with Jupyter when I got the error."]}, {"number": 35603, "title": "[XLA] [TPU] It should not be possible to run out of vmem - please file a bug against XLA.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n```v1.12.1-21643-g03f1214 2.1.0-dev20200105```\r\n\r\n**Describe the current behavior**\r\nI modified this script: https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py.\r\n\r\n```\r\n2020-01-06 08:30:26.371423: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:75]Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n        TPU compilation failed\r\n         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nTraceback (most recent call last):\r\n  File \"train_eval.py\", line 482, in <module>\r\n    main()\r\n  File \"train_eval.py\", line 458, in main\r\n    global_step, tr_loss = train(args, model_class, tokenizer, config, strategy)\r\n  File \"train_eval.py\", line 129, in train\r\n    running_loss = smooth * running_loss + (1. - smooth) * float(loss)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 867, in __float__\r\n    return float(self._numpy())\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 918, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __inference_tpu_function_106662}} Compilation failure: Ran out of memory in memory space vmem. It should not be possible to run out of vmem - please file a bug against XLA.\r\n\r\nLargest program allocations in vmem:\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n  XLA label: %fusion.5840 = f32[512,1024]{1,0:T(8,128)} fusion(s32[512]{0:T(512)}, f32[512,1024]{1,0:T(8,128)}, f32[]{:T(256)}, f32[]{:T(256)}), kind=kCustom, calls=%fused_computation.5742\r\n  Allocation type: scoped\r\n\r\n        TPU compilation failed\r\n         [[{{node tpu_compile_succeeded_assert/_7693574632605057830/_9}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\n\r\nI debugged this error a bit and found that using the custom optimizer causes the error. That is\r\n\r\n```\r\noptimizer = optimization.create_optimizer(args.learning_rate,\r\n            num_steps_per_epoch * args.num_train_epochs, warmup_steps)\r\n```\r\n\r\nWith a default optimizer the program runs fine:\r\n```\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nCustom optimizers should work and the error message should be more specific.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI was not able to create a minimal example to reproduce a minimal example, but the two lines make the difference between a failed and successful compilation.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Sorry for the breakage. I don't think our generated report has enough info for us to reproduce and debug that. We have a PR to add more info in our OOM report system that will hopefully show up in the nightly version later this week. Perhaps then you can rerun your custom optimizer and we can reproduce the issue.\r\n\r\nIn the meanwhile, assigning this to @saberkun who may have a way to reproduce it internally, then our compiler team can debug it. ", "Thanks. I tried to create a self-contained reproduction but it's a little hard to debug on TPUs. The official script runs fine. I debugged it a little more and my current work-around is to comment out this line:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/official/nlp/optimization.py#L142\r\n\r\nGradient clipping is not that important for the final model performance. I will rerun with the future version of tf-nightly.", "Thanks, YLaCoon! \r\n\r\nThe better report should have been submitted into tf-nightly a couple of days ago. Would be great if you can retry your bug and copy-paste the new report -- once we have it it should be much easier for us to reproduce on compiler side. ", "Could you provide the code your modified? \r\nThe gradient clipping will cause all reduce and probably more memory cost.\r\nIf you reduce your batch size, could you make it work?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35603\">No</a>\n", "@yunxing I am having the same issue with the latest TF `2.3` version. Can you suggest any fixes for it? I tried using nightly version but they result in even more bizarre errors.\r\nIn my `model.fit()` function I am feeding it 2 numpy arrays. But If I feed a custom generator which takes the arrays in smaller batches then I get the `  (0) Unavailable: {{function_node __inference_train_function_19338}} failed to connect to all addresses` error", "@neel04,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n"]}, {"number": 35602, "title": "Census tutorial documentation refers to Python 2, but code uses Python 3", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tfx/tutorials/transform/census#python_check_imports_and_globals\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe text says \"First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need.\" but the code below indicates we need to use Python 3.\r\n\r\n### Clear description\r\n\r\nThe text states:\r\n\r\n> First we'll make sure that we're using Python 2, and then go ahead and install and import the stuff we need.\r\n\r\nThe code example says:\r\n\r\n```python\r\nimport sys\r\n\r\n# Confirm that we're using Python 3\r\nassert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\r\n```\r\n", "comments": ["@fdb \r\n\r\nThis issue is more suitable for TensorFlow Extended (TFX) repo.Please post it on TFX repo from [here](https://github.com/tensorflow/tfx/issues).Thanks!", "@fdb In the given URL of the issue only python 3 is mentioned. If I am wrong do mention the line number you are talking about, Thanks.", "@fdb \r\nCan we close this issue here and track the issue in TFX repo. Please, confirm. Thanks!", "@fdb \r\n\r\nAutomatically closing due to lack of recent activity. I believe you have raised issue in TFX repo and tracking the issue in TFX repo. Thanks!"]}, {"number": 35601, "title": "RuntimeError: Mixing different tf.distribute.Strategy objects: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f50aed5d160> is not <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f5230440748>", "body": "`def train_net():\r\n        mirror_strategy = tf.distribute.MirroredStrategy(\r\n                     cross_device_ops=config.cross_device_ops\r\n          )\r\n         with mirror_strategy.scope():\r\n\r\nfor i in range(1, config.num_step+1):\r\n            train_net(config, stage_cfg, args.fast_mode, merge_num=i)```", "comments": ["@Hukongtao ,\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here also error stack trace and TF version being used.Thanks!", "@Hukongtao ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35600, "title": "Avoid unnecessary copy of the closure for runner (repeat 8834e4f)", "body": "Hello, this is very small changes that add const to the closure for Clang-tidy.\r\nThese changes just repeat the previous changes 8834e4f that was made by @mrry.\r\n", "comments": ["@mrry Thank you for the review.\r\nI made this PR by using [DevReplay](https://www.npmjs.com/package/devreplay) that found recent similar changes from this repo.\r\nIf you have an interest in more potential tensorflow rules, please give me feedback.    \r\n\r\n```json\r\n[{\r\n  \"repository\": \"tensorflow/tensorflow\",\r\n  \"sha\": \"8834e4f86c8d5adc89b990249d2eb6532f2fc03f\",\r\n  \"author\": \"Derek Murray\",\r\n  \"created_at\": \"2020-01-05 02:56:38\",\r\n  \"condition\": [\r\n   \"[](std::function<void()> ${1:Identifier}) { ${1:Identifier}(); });\"\r\n  ],\r\n  \"consequent\": [\r\n   \"[](const std::function<void()>& ${1:Identifier}) { ${1:Identifier}(); });\"\r\n  ],\r\n  \"abstracted\": {\r\n   \"1\": \"f\"\r\n  }\r\n }]\r\n```"]}, {"number": 35599, "title": "support convert eager tensor to graph tensor", "body": "", "comments": ["> Why do you need this?\r\n> \r\n> When using tf.function capturing happens properly; mixing v1 graph mode and eager execution can be quite dangerous, so I'd strongly prefer if users added .numpy directly if they need it.\r\n\r\nThe tf.constant could transfer eager tensor to graph tensor. What's the difference between them,\r\n\r\n``` python\r\na=tf.constant(1)\r\nwith tf.compat.v1.Session() as sess:\r\n    b=tf.convert_to_tensor(a)\r\n    c=tf.constant(a)\r\n```\r\nAbove code could run on tensorflow1 but not tensorflow2, it's annoyed but not fatal.\r\nMaybe I should use constant rather than convert_to_tensor.", "The code still doesn't do the right thing after this change. If you want to be building graphs to use with sessions in tf2 you need to use with tf.compat.v1.Graph().as_default()"]}, {"number": 35598, "title": "dll load failed with errorcode 3221225501", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?:  installed using pip also tried wheel\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: No Gpu, only cpu, \r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen i import tensorflow it raise error\r\n`dll load failed with errorcode 3221225501`\r\nI tried several methods like\r\n1) Installing whl from [Tensorflow wheel](https://github.com/fo40225/tensorflow-windows-wheel/blob/master/2.0.0/py37/CPU/avx2/tensorflow-2.0.0-cp37-cp37m-win_amd64.whl )\r\n2) Installed visual c++ 2015 redistributable\r\n\r\nBut all failed,\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n>>>import tensorflow \r\n\r\n**Any other info / logs**\r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\",\r\n line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\r\n\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\ge\r\nn_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line\r\n 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import\r\n_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init\r\n__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap\r\n_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\SAMAN\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@razzOn2calculus \r\n\r\nCan you please refer the [link1](https://devtalk.nvidia.com/default/topic/1057009/error-code-3221225501-when-trying-to-run-tensorflow/) and [link2 ](https://stackoverflow.com/questions/57719942/whats-the-problem-of-error-code-3221225501-during-running-tensorflow-codes).Thanks!", "It didnt solved the problem.", "What is make/model of your cpu? \r\nI suspect your cpu model does not support AVX instructions sets.\r\nSee [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)", "@ymodak  Here are specification of my system,\r\n\r\nSystem model:  Inspiron N4030\r\nSystem Type:   x64-based pc\r\nprocessor: Intel(R) Core(TM) i3 CPU M 380 \r\n", "Your cpu does not support avx instruction sets. See https://ark.intel.com/content/www/us/en/ark/products/50178/intel-core-i3-380m-processor-3m-cache-2-53-ghz.html\r\n\r\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nYou can still use TensorFlow with the alternatives given below:\r\n* Try Google Colab to use TensorFlow.\r\n    * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.\r\n    * It has an added advantage since you can you easily switch to different hardware accelerators     \r\n      (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.\r\n* Try to build TF from sources by changing CPU optimization flags.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35598\">No</a>\n", "Thanks for response @ymodak . But , before tensorflow was working fine in my machine, i got this problem recently. week passed , I didnt find the solution.  ", "@razzOn2calculus What version were you using before? I suspect it was TF 1.5 or less.", "I think it was 2.0.0 but recently now it is 2.1..    . \r\nI downgraded to the old version available 1.13.2 but it is also not working.\r\n", "After 10 days, Problem solved after downgrading to \r\ntensorflow version <= 1.6.0\r\n\r\nSince as \r\nhttps://en.wikipedia.org/wiki/Advanced_Vector_Extensions#cite_note-36\r\nAvx instruction as added after tensorflow version 1.6.0 .\r\n\r\nI downloaded older package-lists of anaconda \r\nand installed . Thanks guys.", "i am getting same issue can you help me with that also", "I have the same problem. Could you help me how can I tackle this issue?", "yes text me at insta my id @_yash_kadam_\n\nOn Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi <notifications@github.com>\nwrote:\n\n> I have the same problem. Could you help me how can I tackle this issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ>\n> .\n>\n", "https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\n\n\nOn Wed, Jun 24, 2020 at 8:55 PM Yash Kadam <textyash20@gmail.com> wrote:\n\n> yes text me at insta my id @_yash_kadam_\n>\n> On Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi <notifications@github.com>\n> wrote:\n>\n>> I have the same problem. Could you help me how can I tackle this issue?\n>>\n>> \u2014\n>> You are receiving this because you commented.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ>\n>> .\n>>\n>\n", "On Wed, Jun 24, 2020 at 9:44 PM Yash Kadam <textyash20@gmail.com> wrote:\n\n>\n> https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\n>\n>\n> On Wed, Jun 24, 2020 at 8:55 PM Yash Kadam <textyash20@gmail.com> wrote:\n>\n>> yes text me at insta my id @_yash_kadam_\n>>\n>> On Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi <notifications@github.com>\n>> wrote:\n>>\n>>> I have the same problem. Could you help me how can I tackle this issue?\n>>>\n>>> \u2014\n>>> You are receiving this because you commented.\n>>> Reply to this email directly, view it on GitHub\n>>> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427>,\n>>> or unsubscribe\n>>> <https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ>\n>>> .\n>>>\n>>\n\nabsl-py==0.9.0\nastor==0.8.1\ncertifi==2019.11.28\nclick==7.1.1\nFlask==1.1.1\nFlask-Cors==3.0.8\ngast==0.3.3\ngoogle-pasta==0.2.0\ngrpcio==1.27.2\nh5py==2.10.0\nitsdangerous==1.1.0\nJinja2==2.11.1\nKeras==2.3.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nMarkdown==3.2.1\nMarkupSafe==1.1.1\nnumpy==1.18.2\nPillow==7.0.0\nprotobuf==3.11.3\nPyYAML==5.3.1\nscipy==1.4.1\nsix==1.14.0\ntensorboard==1.14.0\ntensorflow==1.14.0\ntensorflow-estimator==1.14.0\ntermcolor==1.1.0\nWerkzeug==1.0.0\nwrapt==1.12.1\n", "forget about last file\nuse this\n\nOn Wed, Jun 24, 2020 at 11:46 PM Yash Kadam <textyash20@gmail.com> wrote:\n\n>\n>\n> On Wed, Jun 24, 2020 at 9:44 PM Yash Kadam <textyash20@gmail.com> wrote:\n>\n>>\n>> https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\n>>\n>>\n>> On Wed, Jun 24, 2020 at 8:55 PM Yash Kadam <textyash20@gmail.com> wrote:\n>>\n>>> yes text me at insta my id @_yash_kadam_\n>>>\n>>> On Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi <notifications@github.com>\n>>> wrote:\n>>>\n>>>> I have the same problem. Could you help me how can I tackle this issue?\n>>>>\n>>>> \u2014\n>>>> You are receiving this because you commented.\n>>>> Reply to this email directly, view it on GitHub\n>>>> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427>,\n>>>> or unsubscribe\n>>>> <https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ>\n>>>> .\n>>>>\n>>>\n\nabsl-py==0.9.0\nastor==0.8.1\ncertifi==2019.11.28\nclick==7.1.1\nFlask==1.1.1\nFlask-Cors==3.0.8\ngast==0.3.3\ngoogle-pasta==0.2.0\ngrpcio==1.27.2\nh5py==2.10.0\nitsdangerous==1.1.0\nJinja2==2.11.1\nKeras==2.3.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nMarkdown==3.2.1\nMarkupSafe==1.1.1\nnumpy==1.18.2\nPillow==7.0.0\nprotobuf==3.11.3\nPyYAML==5.3.1\nscipy==1.4.1\nsix==1.14.0\ntermcolor==1.1.0\nWerkzeug==1.0.0\nwrapt==1.12.1\n", "> https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\r\n> [\u2026](#)\r\n> On Wed, Jun 24, 2020 at 8:55 PM Yash Kadam ***@***.***> wrote: yes text me at insta my id @_yash_kadam_ On Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi ***@***.***> wrote: > I have the same problem. Could you help me how can I tackle this issue? > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <[#35598 (comment)](https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427)>, > or unsubscribe > <https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ> > . >\r\n\r\nYou are a life saver!\r\nInstalled tf 1.15 from official pypi for windows 7.\r\n\r\nDidn't work, even though it was supposedly not AVX enabled. \r\n\r\nYours worked. Thank you!", "Thankss buddy\n\nOn Thu, Dec 30, 2021, 5:39 PM saiabinesh ***@***.***> wrote:\n\n>\n> https://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\n> \u2026 <#m_-9162152128464153793_>\n> On Wed, Jun 24, 2020 at 8:55 PM Yash Kadam *@*.*> wrote: yes text me at\n> insta my id @yash_kadam On Wed, Jun 24, 2020, 8:49 PM MasoumehVahedi @.*>\n> wrote: > I have the same problem. Could you help me how can I tackle this\n> issue? > > \u2014 > You are receiving this because you commented. > Reply to\n> this email directly, view it on GitHub > <#35598 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-648886427>>,\n> > or unsubscribe >\n> https://github.com/notifications/unsubscribe-auth/APGAJM4SRLVY3MHWEI3PSYLRYIKQ5ANCNFSM4KC7CDHQ\n> > . >\n>\n> You are a life saver!\n> Installed tf 1.15 from official pypi for windows 7.\n>\n> Didn't work, even though it was supposedly not AVX enabled.\n>\n> Yours worked. Thank you!\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35598#issuecomment-1003001969>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APGAJM42BKA6E2DKYRKFZVLUTRDZJANCNFSM4KC7CDHQ>\n> .\n> You are receiving this because you commented.Message ID:\n> ***@***.***>\n>\n"]}, {"number": 35597, "title": "\"EIGEN_ALWAYS_INLINE\" is not a class or function template in the current scope", "body": "**Configuration**\r\nWindows 10, Visual Studio 2017\r\nTensorflow: 1.14\r\nPython: 3.6.3\r\nCUDA: No\r\nLanguage: C++\r\nCPU: Yes (arch/AVX)\r\nRcom: No\r\nJIT: No\r\nOverride eigen inline for less compile time: Yes\r\n\r\nMy tensorflow cpu build is successful. I set the paths in C++ \"Include Path Directories\" at project properties in VS2017. While trying to execute the demo program I am getting int32, EIGEN_ALWAYS_INLINE errors. Below find the code below and the errors.\r\n\r\n**Error Screenshot**\r\n[https://drive.google.com/open?id=1xQfZVwQAmi30OMmrxl4BI-7pYvgW3BGi](url)\r\n\r\n**Sample code**\r\n```\r\n// tf demo 1.cpp : This file contains the 'main' function. Program execution begins and ends there.\r\n//\r\n\r\n#include <iostream>\r\n#include <Windows.h>\r\n#include <stdio.h>\r\n//#include <vector>\r\n\r\n//#include <eigen/Dense>\r\n\r\n// #include \"matmul.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\n\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/ops/math_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/protobuf/config.pb.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nint main() {\r\n\tusing namespace tensorflow;\r\n\tusing namespace tensorflow::ops;\r\n\tScope root = Scope::NewRootScope();\r\n\t//Matrix A = [3 2; -1 0]\r\n\tauto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\r\n\t// Vector b = [3 5]\r\n\tauto b = Const(root, { {3.f, 5.f} });\r\n\t// v = Ab^T\r\n\tauto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n\tstd::vector<Tensor> outputs;\r\n\tClientSession session(root);\r\n\t// Run and fetch v\r\n\tTF_CHECK_OK(session.Run({ v }, &outputs));\r\n\t// Expect outputs[0] == [19; -3]\r\n\tLOG(INFO) << outputs[0].matrix<float>();\r\n\treturn 0;\r\n}\r\n```", "comments": ["This is fixed. I did a fresh cpu version compile after cleaning the temporary files, prefetch. Closing the issue.\r\nConfiguration changes: Override eigen inline for less compile time: Yes"]}, {"number": 35596, "title": "Fix typo in optimizer_v2", "body": "", "comments": []}, {"number": 35595, "title": "model to json", "body": "added an option to directly convert a model to json", "comments": ["Please open against master."]}, {"number": 35594, "title": "[sparkfun-tensorflow-codelab] - Update Code", "body": "The codelab still links to the experimental folder for the makefile, which is incorrect.\r\n\r\nVisual:\r\n![image](https://user-images.githubusercontent.com/997157/71787651-3bc27180-2fe0-11ea-8318-424dc887efc5.png)\r\n\r\nUpdate the codelab at this link https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n\r\nCode should read:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\                                    \r\nTARGET=sparkfun_edge micro_speech_bin\r\n```\r\n\r\n## URL(s) with the issue:\r\nhttps://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n", "comments": ["@petewarden is this the right place for bugs for codelabs like that?", "If there's any way I can do a PR to fix this, let me know.", "The codelab is now updated with the changes mentioned upstream. Thanks!\r\nhttps://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3"]}, {"number": 35593, "title": "The report a mistake link does not work", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe \"Report a Mistake\" link at the bottom left does not work, and goes to a GitHub 404 instead.\r\n![image](https://user-images.githubusercontent.com/997157/71787589-711a8f80-2fdf-11ea-8c54-03f8fc68b8d0.png)\r\n\r\nCurrently it links to: https://github.com/tensorflow/tensorflow/issues/new/title=[sparkfun-tensorflow-codelab]:", "comments": ["@GantMan It was moved out of `tensorflow` GitHub repository to a new repository on GitHub. Link to the new repository is [here](https://github.com/googlecodelabs/tools/issues). Please file an issue there so that it will be resolved faster. You could also raise a PR to update the docs in that repository. Thanks!\r\n\r\nI am closing this issue as it was not related to Bug/Performance related to `Tensorflow` repository. Thanks!"]}, {"number": 35592, "title": "Unable to build hello_world for sparkfun_edge", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: a2cf331a2073d6f5e9a52802cbc2809f9e2667b8\r\n- Python version: Python 3.6.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): arm-none-eabi-gcc (15:6.3.1+svn253039-1build1) 6.3.1 20170620\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am unable to build hello_world for sparkfun_edge.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the instructions on https://www.tensorflow.org/lite/microcontrollers/library\r\nand issued the command:\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin`\r\n\r\nThis results in the following build error:\r\n```\r\narm-none-eabi-ar: creating tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a\r\narm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g -DTF_LITE_STATIC_MEMORY -fno-rtti -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -Wno-write-strings -Wno-sign-compare -fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive -nostdlib -ggdb -O3 -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/CMSIS_ext/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/utils/ -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_accelerometer/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_adc/ -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sine_model_data.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/output_handler.o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/constants.o  tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a -mthumb -mcpu=cortex-m4 -mfpu=fpv4-sp-d16 -mfloat-abi=hard -nostartfiles -static -Wl,--gc-sections -Wl,--entry,Reset_Handler -Wl,--start-group -lm -lc -lgcc -Wl,--end-group -fno-exceptions -nostdlib --specs=nano.specs -t -lstdc++ -lc -lnosys -lm -Wl,-T,tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/apollo3_evb/examples/hello_world/gcc_patched/apollo3evb.ld -Wl,-Map=tensorflow/lite/micro/tools/make/gen/sparkfun_edge.map,--cref tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp/gcc/bin/libam_bsp.a tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/hal/gcc/bin/libam_hal.a tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o -lm\r\narm-none-eabi-g++: error: tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: No such file or directory\r\ntensorflow/lite/micro/examples/hello_world/Makefile.inc:41: recipe for target 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world] Error 1\r\n```\r\n\r\n", "comments": ["@vanti  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version  2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35592\">No</a>\n"]}, {"number": 35591, "title": "GPU memory on Bahdanau Attention", "body": "**System information**\r\nGoogle Colab system (GPU mode) with TensorFlow 2.1-rc2 and Python 3.\r\n\r\n**Describe the current behavior**\r\nBahdanau attention get a lot gpu memory, it's right? Even bahdanau attention using concatenation approach, Luong attention get 2.5gb of memory while bahdanau over 15gb. (using scale=False, batch_size=128, units=512 both)\r\n\r\n**Describe the expected behavior**\r\nTo use less memory\r\n\r\n**Code to reproduce the issue**\r\nThe model detail is [here](https://drive.google.com/open?id=170KpjIGFmNNt-SS-6oXgVbRnZR4-7ah_) (the same of this issue https://github.com/tensorflow/tensorflow/issues/35553)", "comments": ["@arthurflor23 \r\n\r\ncould you please check on stable version 2.4.1 and let us know if this is still an issue.", "Hi\r\nI tested here (TF 2.4.1), and the bahdanau version still getting high gpu memory.\r\n\r\nI dont know if this is really an error, or not. So feel free to close."]}, {"number": 35590, "title": "experimental_new_converter / error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab, python 3 runtime\r\n- TensorFlow installed from (source or binary): pip3 install tf-nightly\r\n- TensorFlow version (or github SHA if from source): '2.1.0-dev20200104'\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nconverter = tensorflow.lite.TFLiteConverter.from_keras_model(loaded_model)\r\nconverter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter=True\r\ntflite_quant_model = converter.convert()\r\n\r\n**The output from the converter invocation**\r\n\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-27-934d73e7faa7> in <module>()\r\n      3 converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\n      4 converter.experimental_new_converter=True\r\n----> 5 tflite_quant_model = converter.convert()\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    213       stdout = _try_convert_to_unicode(stdout)\r\n    214       stderr = _try_convert_to_unicode(stderr)\r\n--> 215       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    216   finally:\r\n    217     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-01-05 08:26:27.512446: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.\r\n2020-01-05 08:26:27.512515: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.\r\n2020-01-05 08:26:27.711202: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\nloc(callsite(\"model_1/up_sampling2d_1/resize/ResizeNearestNeighbor\"(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\":853:0) at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\":947:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\":409:0 at callsite(\"<ipython-input-27-934d73e7faa7>\":2:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2882:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2822:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\":2718:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\":533:0 at callsite(\"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\":196:0 at \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\":399:0)))))))))): error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main'\r\nOps that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py:853:9: error: 'tf.ResizeNearestNeighbor' op is neither a custom op nor a flex op\r\n        self._initialize(args, kwargs, add_initializers_to=initializers)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py:947:5: note: called from\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n    ^\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py:409:5: note: called from\r\n    concrete_func = func.get_concrete_function()\r\n    ^\r\n<ipython-input-27-934d73e7faa7>: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:17: note: called from\r\n                exec(code_obj, self.user_global_ns, self.user_ns)\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:17: note: called from\r\n                if self.run_code(code, result):\r\n                ^\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:20: note: called from\r\n                   interactivity=interactivity, compiler=compiler, result=result)\r\n                   ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:533:9: note: called from\r\n        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n        ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:196:13: note: called from\r\n            res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n            ^\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:41: note: called from\r\n                                        user_expressions, allow_stdin)\r\n                                        ^\r\n<unknown>:0: error: failed while converting: 'main'\r\nOps that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.\r\n\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\nthe old toco based converter works as expected,\r\nwhen testing: converter.experimental_new_converter=True\r\nthe above error appears\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@gogela,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "The full code is below and  attached zip with model json file.\r\nI'm unfortunately unable to share the weights h5 file.\r\n\r\n%tensorflow_version 2.x\r\nfrom tensorflow.keras.models import model_from_json\r\nfrom tensorflow.keras.models import load_model\r\njson_file = open('model.json', 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\nloaded_model = model_from_json(loaded_model_json)\r\nloaded_model.load_weights('model.h5')\r\n\r\nconverter = tensorflow.lite.TFLiteConverter.from_keras_model(loaded_model)\r\nconverter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter=True\r\ntflite_quant_model = converter.convert()\r\n\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4026478/model.zip)\r\n", "@gogela Can you please try adding the following line. When I added, I don't see any error. Please check and let us know. Thanks.\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\n[Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/767a9270d4aaea440f715bffcca730e0/untitled736.ipynb) is the gist for your reference. Thanks!\r\n\r\nPlease close the issue if it was resolved for you. thanks!", "I am closing this issue as it was resolved. Please let me know if I am mistaken. thanks!", "I'm currently faced with this issue `Exception: <unknown>:0: error: loc(\"Dilation2D\"): 'tf.Dilation2D' op is neither a custom op nor a flex op`\r\nWhen will tflite support tf.nn.dilation2d??", "Also if I add `converter.allow_custom_ops = True` it works and exports a tflite file.\r\nHowever, it crashes in the phone with error: \r\n`Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 0 (FlexIdentity) failed to prepare.`", "@gigadeplex Can you please open a new issue with a description (as above) and a standalone code to reproduce the issue? Thanks!", "> \r\n> \r\n> @gigadeplex Can you please open a new issue with a description (as above) and a standalone code to reproduce the issue? Thanks!\r\n\r\nHi @jvishnuvardhan , thank you for your reply! I took your advice and just opened an issue regarding tf.Dilation2D and included a link to a saved model which is derived from pytorch through onnx #40420 ", "> @gogela Can you please try adding the following line. When I added, I don't see any error. Please check and let us know. Thanks.\r\n> \r\n> ```\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n> \r\n> [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/767a9270d4aaea440f715bffcca730e0/untitled736.ipynb) is the gist for your reference. Thanks!\r\n> \r\n> Please close the issue if it was resolved for you. thanks!\r\n\r\nThank you My issue was solved because of you . I was trying for last 10 days", "> @gogela Can you please try adding the following line. When I added, I don't see any error. Please check and let us know. Thanks.\r\n> \r\n> ```\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                                        tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n> \r\n> [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/767a9270d4aaea440f715bffcca730e0/untitled736.ipynb) is the gist for your reference. Thanks!\r\n> \r\n> Please close the issue if it was resolved for you. thanks!\r\n\r\nThank you so much. You have probably saved my several working hours.", "> I am closing this issue as it was resolved. Please let me know if I am mistaken. thanks!\r\n\r\nThank you so much. But I don't know why to use that line. And, do you have a tutorial about TF Lite, please tell me. ", "@tucachmo2202 Please follow the guides [here](https://www.tensorflow.org/lite/performance/model_optimization). Thanks!\r\nIn the above example, there are two kinds of `ops` supported (`TFLITE_BUILTINS` and `SELECT_TF_OPS`].\r\n\r\nTFLITE_BUILTINS : These are the ops builtin specifically for tflite. They are optimized for tflite. But, currently there are limited ops are supported by `TFLITE_BUILTINS`.\r\n\r\nSELECT_TF_OPS : If you are using some `op` that is not part of  `TFLITE_BUILTINS` then the conversion code automatically selects Tensorflow Ops. These are not really optimized for TFLite, so it might need consume more memory.\r\n\r\nHope this helps. Thanks!\r\n", "> @tucachmo2202 Please follow the guides [here](https://www.tensorflow.org/lite/performance/model_optimization). Thanks!\r\n> In the above example, there are two kinds of `ops` supported (`TFLITE_BUILTINS` and `SELECT_TF_OPS`].\r\n> \r\n> TFLITE_BUILTINS : These are the ops builtin specifically for tflite. They are optimized for tflite. But, currently there are limited ops are supported by `TFLITE_BUILTINS`.\r\n> \r\n> SELECT_TF_OPS : If you are using some `op` that is not part of `TFLITE_BUILTINS` then the conversion code automatically selects Tensorflow Ops. These are not really optimized for TFLite, so it might need consume more memory.\r\n> \r\n> Hope this helps. Thanks!\r\n\r\nThanks. I did make sense of TF Lite, but I am getting stuck on Non maximum suppression operation, it is not support for TF Lite and I don't want to use SELECT_TF_OPS option. Do you know how to convert or re-implement the that NMS op, please tell me. Thanks very much!", "Hi @tucachmo2202 have you done for implementing NMS without SELECT_TF_OPS? I'm facing the issue too. Let me know if you have the solution. Thanks in advance!"]}, {"number": 35589, "title": "Config value monolithic is not defined ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0.326\r\n- GPU model and memory: 1080ti 11MB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nTrying to create a standalone C++ installation.\r\n\r\nSuccessfully configured and built the Python package.\r\n\r\nSince I didn't find any help in the official tensorflow documentation, tried the solution here:\r\n\r\n[https://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api](url)\r\n\r\nSince I'm adding TF to an existing OpenCV application, the recommended solution from [14267](https://github.com/tensorflow/tensorflow/issues/14267) would be \r\n\r\n`bazel build //tensorflow:libtensorflow_cc.so --config=monolithic`\r\n\r\nBut that results in error:\r\n \r\n`Config value monolithic is not defined in any .rc file`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThe original build command:\r\n\r\n`bazel build -c opt --jobs 32 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --define=grpc_no_ares=true\r\n`\r\n\r\nHere is the configure session:\r\n\r\n\r\n`./configure\r\n\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is /home/david/TF/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/david/TF/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/david/TF/lib/python3.6/site-packages]`\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    /usr/local/cuda/lib64\r\n    /usr/local/cuda/include\r\nFound cuDNN 7 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 6 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include/x86_64-linux-gnu`\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.`\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=gdr         \t# Build with GDR support.\r\n\t--config=verbs       \t# Build with libverbs support.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=noignite    \t# Disable Apache Ignite support.\r\n\t--config=nokafka     \t# Disable Apache Kafka support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\na`\r\n", "comments": ["Perhaps  this [issue thread](https://github.com/tensorflow/tensorflow/issues/23401) may help to solve the problem.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35589\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35589\">No</a>\n"]}, {"number": 35588, "title": "Tried installing tensorflow packages in Conda for Ubuntu but kept facing import errors", "body": "\r\n### Describe the problem\r\nIssues while running Handwritten Digit classifier\r\n\r\n### Source code / logs\r\nImportError                               Traceback (most recent call last)\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: /home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-b3fe95775323> in <module>\r\n      1 import cv2\r\n      2 import numpy as np\r\n----> 3 from keras.datasets import mnist\r\n      4 from keras.models import load_model\r\n      5 \r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/utils/conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/__init__.py in <module>\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py in <module>\r\n      3 from __future__ import print_function\r\n      4 \r\n----> 5 import tensorflow as tf\r\n      6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/ponaravind/yes/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so: undefined symbol: cuDevicePrimaryCtxGetState\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@pon-aravind \r\nPlease let us know which TensorFlow version you are trying to install?\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35588\">No</a>\n"]}, {"number": 35587, "title": "Looking to install 1.15 C++ ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0.326\r\n- GPU model and memory: 1080ti 11MB\r\n\r\n**Describe the problem**\r\n\r\nBuild command:\r\n\r\n`bazel build -c opt --jobs 32 --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 -k //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --define=grpc_no_ares=true\r\n`\r\n\r\nCompleted the build with bazel, and want to produce the libraries and includes to install in my system, but I'm unable to find anywhere ()many years experience with CMAKE, 0 with bazel).\r\n\r\nI found [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md](url) for C (NOT what I'm looking for but wanted to get started with something..), and ran the command instructed there\r\n\r\n`bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test`\r\n\r\nBut returns \r\n\r\n`ERROR: Config value opt is not defined in any .rc file`\r\n\r\nBut the real question is how do I install tensorflow 1.15 C++ \r\n\r\nMany thanks!!\r\n", "comments": ["Although I didn't find any documetation for building a standalone c++ install, I found what seems as a solution here:\r\n\r\n[https://stackoverflow.com/questions/33620794/how-to-build-and-use-google-tensorflow-c-api](url)\r\n\r\n`bazel build //tensorflow:libtensorflow_cc.so --config=monolithic`\r\n\r\nBut that results in error:\r\n \r\n`Config value monolithic is not defined in any .rc file`\r\n\r\nI'm using OpenCV hence the need for `config=monolithic`.\r\n\r\nI opened a separate issue for this other problem, but since I don't have an official source on how to produce the c++ standalone, I am leaving this one here also, since they seem to be different issues.\r\n\r\nMany thanks for any help!\r\n\r\nDave", "Apologies for the delay in response. Is this still an issue?\r\nDid you configure the build with `./configure` option before building?\r\nhttps://www.tensorflow.org/install/source#configure_the_build", "Hi Yasir thanks for your reply!\r\n\r\nThis is four projects ago....   Looking at my project log, what I did then after being stuck with this TF problem was to create the model  on Pytrorch docker exported to ONNX  and converted that to TensorRT on the edge computer. Inference is running on TensorRT quite well...\r\n\r\nHave used Pytorch since.\r\n\r\nThanks"]}, {"number": 35586, "title": "Tensorflow-GPU processing fit on CPU", "body": "**System information**\r\n- Haven't written custom code.\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: Anaconda\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA: 10.0.130, cuDNN: 7.6.5\r\n- GPU model and memory: GTX 1060 3Gb\r\n\r\n**Describe the current behavior**\r\nTensorflow GPU is using the CPU instead of the GPU to fit. If I use tf.compat.v1.debugging.set_log_device_placement(True) when trying to fit the model I get:\r\n\r\n2020-01-04 21:00:20.616161: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-01-04 21:00:20.616622: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\r\n2020-01-04 21:00:20.637944: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0 \r\n\r\nBut for example on other ocations (not fitting):\r\n\r\n2020-01-04 21:00:19.201662: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\r\n\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport gym\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom collections import deque\r\nimport random\r\nimport tensorflow as tf\r\nimport gc\r\nimport time\r\nprint(tf.test.is_gpu_available())\r\ntf.compat.v1.debugging.set_log_device_placement(True)\r\n\r\n\r\nclass Agent:\r\n    def __init__(self, state_size, action_size):\r\n        self.LEARNING_RATE = 0.001\r\n        self.EPSILON = 1.0\r\n        self.EPSILON_DECAY = 0.97\r\n        self.MIN_EPSILON = 0.01\r\n        self.DISCOUNT = 0.95\r\n\r\n        self.model = keras.Sequential()\r\n        self.model.add(keras.layers.Dense(state_size, activation=\"relu\"))\r\n        self.model.add(keras.layers.Dense(24, activation=\"relu\"))\r\n        self.model.add(keras.layers.Dense(48, activation=\"relu\"))\r\n        self.model.add(keras.layers.Dense(action_size, activation=\"linear\"))\r\n        self.model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.LEARNING_RATE),\r\n                                                           loss=\"mse\")\r\n\r\n        self.memory = deque(maxlen=2000)\r\n\r\n    def act(self, state):\r\n        action = 0\r\n\r\n        if state is None:\r\n            action = 1\r\n        else:\r\n            if np.random.rand() <= self.EPSILON:\r\n                action = random.randint(0, 2)\r\n            else:\r\n                action = self.model.predict(state)\r\n                keras.backend.clear_session()\r\n                gc.collect()\r\n                action = np.argmax(action)\r\n\r\n        return action\r\n\r\n    def remember(self, state, reward, action, next_state, done):\r\n        self.memory.append((state, reward, action, next_state, done))\r\n\r\n    def retrain(self, minibatch_size):\r\n        if minibatch_size < len(self.memory):\r\n            minibatch = random.sample(self.memory, minibatch_size)\r\n            now = time.time()\r\n            for i in range(100):\r\n                print(\"Start time:\", str(now))\r\n            for state, reward, action, next_state, done in minibatch:\r\n                target = reward\r\n\r\n                if not done:\r\n                    target = reward + self.DISCOUNT*np.amax(self.model.predict(next_state)[0])\r\n                    keras.backend.clear_session()\r\n                    gc.collect()\r\n\r\n                if not (state is None):\r\n                    target_f = self.model.predict(state)\r\n                    keras.backend.clear_session()\r\n                    gc.collect()\r\n                    target_f[0][action] = target\r\n                    self.model.fit(state, target_f, epochs=1, verbose=0)\r\n\r\n            print(\"Elapsed:\", str(time.time() - now))\r\n\r\n        if self.EPSILON > self.MIN_EPSILON:\r\n            self.EPSILON *= self.EPSILON_DECAY\r\n\r\n    def load(self, name):\r\n        self.model.build(input_shape=(1, 2))\r\n        self.model.load_weights(name)\r\n\r\n    def save(self):\r\n        self.model.save_weights(str(episode + 6700) + \".h5\")\r\n\r\n\r\nclass Environment:\r\n    def __init__(self):\r\n        self.env = gym.make(\"MountainCar-v0\")\r\n        self.reward = 0\r\n        self.max_height = -999\r\n        self.record_pos = -9\r\n\r\n    def make_move(self, action):\r\n        state, reward, done, info = self.env.step(action)\r\n\r\n        if state[0] > self.max_height:\r\n            if state[0] > self.record_pos:\r\n                self.record_pos = state[0]\r\n                self.max_height = self.record_pos\r\n                self.reward = 50\r\n            else:\r\n                self.max_height = state[0]\r\n                self.reward = 10\r\n\r\n        if state[0] == 0.5:\r\n            self.reward = 100\r\n\r\n        return state, reward, done\r\n\r\n    def get_state_size(self):\r\n        return self.env.observation_space.shape[0]\r\n\r\n    def get_action_size(self):\r\n        return self.env.action_space.n\r\n\r\n    def reset(self):\r\n        self.env.reset()\r\n        self.reward = 0\r\n        self.max_height = -999\r\n\r\n    def close(self):\r\n        self.env.close()\r\n\r\n    def render(self):\r\n        self.env.render()\r\n\r\n    def get_max_position(self):\r\n        return self.record_pos\r\n\r\n\r\nenvironment = Environment()\r\nagent = Agent(environment.get_state_size(), environment.get_action_size())\r\nagent.load(\"6700.h5\")\r\nfor episode in range(1000):\r\n    i = 0\r\n    state = None\r\n    environment.reset()\r\n    done = False\r\n    previous_height = 0\r\n    while not done:\r\n        if episode % 100 == 0:\r\n            environment.render()\r\n\r\n        action = agent.act(state)\r\n        next_state, reward, done = environment.make_move(action)\r\n\r\n        next_state = np.array(next_state)\r\n        next_state = np.reshape(next_state, [1, environment.get_state_size()])\r\n\r\n        if state is not None:\r\n            agent.remember(state, reward, action, next_state, done)\r\n\r\n        state = next_state\r\n\r\n    print(\"Max height:\", environment.get_max_position(), \"episode:\", episode)\r\n    if episode % 100 == 0 and episode != 0:\r\n        agent.save()\r\n    agent.retrain(32)\r\n\r\nenvironment.close()\r\n```\r\n\r\n**Other log that might be useful is:**\r\nprint(tf.test.is_gpu_available()) ----> True\r\n\r\nAlso, the GPU load is 0%.\r\n", "comments": ["@PBB-as \r\n\r\nRequest you to share colab link or minimal standalone code with proper indentation to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @PBB-as\r\n> \r\n> Request you to share colab link or minimal standalone code with proper indentation to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nDone!", "@PBB-as \r\nI tried reproducing the issue with TF 2.0. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d5da2812927731bd336770e9a03fb1fc/untitled536.ipynb). Is this the expected behavior?. Thanks!", "@ravikyram \r\nThat was logging me at first too, but if you keep running the code (train a few samples) you'll note that Tensorflow will use the CPU instead of GPU as I mentioned before.\r\n\r\n> 2020-01-04 21:00:20.637944: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\r\n\r\nI think that the problem is solved using:\r\n\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.95 #Or any other fraction != 1\r\nsession = tf.compat.v1.Session(config=config)\r\n```\r\n\r\nAnd then running Tensorflow as follows:\r\n`with session:`", "@PBB-as \r\n\r\nPlease, let me know if we can close this issue since it looks to be fixed. Thanks!", "@ravikyram \r\nYes, I am closing it. Thank you!", "@santiagopardal  I am having the same issue. Could you elaborate a bit more on how you use the session to solve it? (Attaching the modified code would help). Thanks. "]}, {"number": 35585, "title": "Difference in training accuracy and loss using gradientTape vs model.fit with binary_accuracy: A bug?", "body": "Hi all, \r\n\r\nI am running a training loop using gradientTape which works well, however I am getting different training accuracy metrics when training using the gradientTape loop vs a straight model.fit method. I apologise if this should be a question for stack overflow, however, to the best of my knowledge the parameters are the same and therefore should be producing exactly the same results (or very close at least).. I therefore think there may be a bug and if any one can help me elucidate this i would really appreciate it!\r\n\r\nI have prepared a sequential model as follows:\r\n\r\n```\r\nmodel=tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(units=64, input_dim=5078, activation=\"relu\"))\r\nmodel.add(tf.keras.layers.Dense(units=32, activation=\"relu\"))\r\nmodel.add(tf.keras.layers.Dense(units=100, activation=\"relu\"))\r\nmodel.add(tf.keras.layers.Dense(units=24, activation=\"sigmoid\"))\r\n```\r\nand for the ` model.fit` method, fit as follows:\r\n\r\n```\r\nmodel.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\r\n\r\nmodel.fit(X_train, y_train,\r\n batch_size=32,\r\n epochs=100, verbose=1,\r\n validation_split=0.15,\r\n shuffle=True)\r\n```\r\nThis works well and produces the following results (please note 100 epochs is overkill and the model overfits, however this is just to keep the same epochs as the as the gradientTape loop, otherwise there would be an early-stopping callback normally...\r\n\r\nThe model metrics are as follows:\r\n\r\n```\r\n 32/119 [=======>......................] - ETA: 0s - loss: 0.0699 - acc: 0.9753\r\n119/119 [==============================] - 0s 168us/sample - **loss: 0.0668** - acc: **0.9779** - val_loss: **0.2350** - val_acc: **0.9048**\r\n```\r\n\r\nThis is the expected behaviour (minus the overfitting)... Now when I create the gradientTape loop as follows, the accuracy metrics are of by about ~4-5% during the same 100 epochs, and the reason i suspect a bug is because i believe i am using the appropriate metrics:\r\n\r\n```\r\ndef random_batch(X,y, batch_size=32):\r\n    idx= np.random.randint(len(X), size=batch_size)\r\n    return X[idx], y[idx]\r\n\r\n##Further split train data to training set and validation set\r\n\r\nX_train, X_val, y_train, y_val = train_test_split(\r\n    X_train, y_train, test_size=0.15, random_state=1)\r\n\r\n```\r\n\r\n```\r\n##Run autodiff on model\r\n\r\nn_epochs=100\r\nbatch_size=32\r\nn_steps=len(X_train)//batch_size\r\n\r\noptimizer=tf.keras.optimizers.Adam()\r\nloss=tf.keras.losses.BinaryCrossentropy()\r\n\r\nmetricLoss=tf.keras.metrics.BinaryCrossentropy()\r\nmetricsAcc=tf.keras.metrics.BinaryAccuracy()\r\n\r\nval_acc_metric=tf.keras.metrics.BinaryAccuracy()\r\nval_acc_loss=tf.keras.metrics.BinaryCrossentropy()\r\n\r\n\r\ntrain_loss_results = []\r\ntrain_accuracy_results = []\r\n\r\nvalidation_loss_results = []\r\nvalidation_accuracy_results = []\r\n\r\n# for loop iterate over epochs\r\nfor epoch in range(n_epochs):\r\n\r\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\r\n\r\n    # for loop iterate over batches\r\n    for step in range(1, n_steps + 1):\r\n        X_batch, y_batch=random_batch(X_train.values, y_train)\r\n\r\n        # gradientTape autodiff\r\n        with tf.GradientTape() as tape:\r\n            y_pred=model(X_batch, training=True)\r\n            loss_values=loss(y_batch, y_pred)\r\n        gradients=tape.gradient(loss_values, model.trainable_weights)\r\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\r\n\r\n        metricLoss(y_batch, y_pred)\r\n        metricsAcc.update_state(y_batch, y_pred)\r\n\r\n        # Loss and accuracy\r\n        train_loss_results.append(loss_values)\r\n        train_accuracy_results.append(metricsAcc.result())\r\n\r\n        # Read out training results\r\n        readout = 'Epoch {}, Training loss: {}, Training accuracy: {}'\r\n        print(readout.format(epoch + 1, loss_values,\r\n                              metricsAcc.result() * 100))\r\n\r\n        metricsAcc.reset_states\r\n\r\n        # Run a validation loop at the end of each epoch\r\n\r\n    for valbatch in range(1+ n_steps +1):\r\n        X_batchVal, y_batchVal = random_batch(X_val.values, y_val)\r\n\r\n        val_logits = model(X_batchVal)\r\n        # Update val metrics\r\n        val_acc_metric(y_batchVal, val_logits)\r\n        val_acc = val_acc_metric.result()\r\n\r\n        val_acc_metric.update_state(y_batchVal, val_logits)\r\n\r\n        val_loss=val_acc_loss(y_batchVal, val_logits)\r\n\r\n        validation_loss_results.append(val_loss)\r\n        validation_accuracy_results.append(val_acc_metric.result())\r\n\r\n        # Read out validation results\r\n        print( 'Validation loss: ' , float(val_loss),'Validation acc: %s' % (float(val_acc * 100),) )\r\n\r\n        val_acc_metric.reset_states()\r\n```\r\n\r\nWhen i run this code, it works fine, and the iterations update the states of the accuracy and loss: however, the training accuracy is much lower than the model.fit method, after running also for 100 epochs: showing final epoch result that is printed (each same epoch is iterating over each batch):\r\n\r\nEpoch 100, Training loss: 0.027735430747270584, Training accuracy: 93.6534423828125\r\nEpoch 100, Training loss: 0.03832387551665306, Training accuracy: 93.67249298095703\r\n**Epoch 100, Training loss: 0.035500235855579376, Training accuracy: 93.69097900390625**\r\nValidation loss:  0.3204055726528168 Validation acc: 90.36458587646484\r\nValidation loss:  0.32066160440444946 Validation acc: 89.71354675292969\r\nValidation loss:  0.32083287835121155 Validation acc: 90.49479675292969\r\nValidation loss:  0.3209479749202728 Validation acc: 90.10416412353516\r\n**Validation loss:  0.32088229060173035 Validation acc: 90.625**\r\n\r\nAs you can see,  the training accuracy is ~4-5% lower compared to the model.fit method. The loss records fine, and also, the validation data looks pretty much just like the validation data in the model.fit method. \r\n\r\nAdditionally, when i plot accuracy and loss in both model.fit and geadientTape methods, the shape of the curves look pretty much the same, and they both begin to overfit at similar points! but again, there is a huge discrepancy in the training accuracy. \r\n\r\nI have specified the adam optimizer as well binary_crossentropy loss in model.fit and gradientTape. For model.fit, when I specific 'accuracy' or 'acc' for metrics, my understanding is that it will call on the binary_accuracy for calculating the accuracy. So as far as I am aware the parameters are similar that results should be fairly similar. \r\n\r\nAdditionally, when i call` model.compile` after training the model with `gradientTape` just to confirm evaluation, the results are slightly different again and look more like the model.fit method:\r\n\r\n```\r\n**Training**\r\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\nprint('\\n', model.evaluate(X_train, y_train, verbose=1)[1])\r\n\r\n32/101 [========>.....................] - ETA: 0s - loss: 0.0336 - acc: 0.9948\r\n101/101 [==============================] - 0s 307us/sample - **loss: 0.0330 - acc: 0.9942**\r\n\r\n**Validation**\r\nmodel.compile(optimizer=optimizer, loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\nprint('\\n', model.evaluate(X_val, y_val, verbose=1)[1])\r\n\r\n18/18 [==============================] - 0s 111us/sample - **loss: 0.3879 - acc: 0.9028**\r\n```\r\n\r\nNow model.evaluate shows a loss and accuracy that are very similar to the model.fit method when i call evaluate on X_train and y_train. This is why i am suspect of a bug? Interestingly, the model.evaluate on validation data look similar to the gradientTape loop which leaves me really confused as i am therefore unsure of the true training accuracy and loss!\r\n\r\nIf anyone can help i would really appreciate this... I am happy to provide further code upstream of the model etc.. Again, apologies if this is not a bug but this seems really confusing to me like an incorrect behaviour...\r\n\r\n\r\n\r\n\r\n", "comments": ["Same Problem [#35533 ](https://github.com/tensorflow/tensorflow/issues/35533)", "@NLP-ZY is this a known problem? I didn't understand your comment about it being caused by the relu activation? Is this a known bug? If so is there a solution? ", "@amjass12 There are two problem. First caused by relu activation, when I use model.fit, the result is perfect in acc & loss & val_acc & val_loss, but when I use GradientTape, the acc & loss is unusual, abount 8 epoch, every batch predictions is same. Seceond, when I remove relu activation, every batch predictions become  usual, the acc & loss as excepted, but the val_acc & val_loss also bad. Anyway, under the same network, the result of model.fit is much better than GradientTape on val_acc & val_loss.", "Network of 13 classification, my code is [#35533](#35533)\r\nModel.fit result op 30 epochs:\r\nloss: 0.0050 - acc: 0.9987 - val_loss: 0.0716 - val_acc: 0.9908\r\nGradient result on 30 epochs:\r\nloss: 0.0160 - acc: 0.9987 - val_loss: 4.0659 - val_acc: 0.8882", "Hi @NLP-ZY ,\n\nThank you for the reply and detailed information. This is very interesting, is this likely a bug? \n\nWith regards to removing the relu, can you expand on this please .. \n\nDo you mean you use a different activation function?  I would really like to fix this as soon as possible.  ", "sorry to respond slowly, I am busy recently. remove relu means use activation=None. I have try many times, I think gradienttape maybe have some bugs, the result of gradienttape is worse than model.fit", "```\r\n# -*- coding: utf8 -*-\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow import keras as tfk\r\n\r\n\r\nclass Test(tfk.Model):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.embedding_layer = tfk.layers.Embedding(50000, 300)\r\n        self.conv1d_layer = tfk.layers.Conv1D(256, 5)\r\n        self.pool_layer = tfk.layers.MaxPool1D(pool_size=5, strides=2)\r\n        self.dense1_layer = tfk.layers.Dense(128, activation=tfk.activations.relu)\r\n        self.dense2_layer = tfk.layers.Dense(10, activation=tfk.activations.softmax)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        hidden = self.embedding_layer(inputs)\r\n        hidden = self.conv1d_layer(hidden)\r\n        hidden = self.pool_layer(hidden)\r\n        hidden = tfk.layers.Flatten()(hidden)\r\n        hidden = self.dense1_layer(hidden)\r\n        y_pred = self.dense2_layer(hidden)\r\n        return y_pred\r\n\r\n\r\nclass Test2(tfk.Model):\r\n    def __init__(self):\r\n        super(Test2, self).__init__()\r\n        self.embedding_layer = tfk.layers.Embedding(50000, 300)\r\n        self.rnn_layer = tfk.layers.LSTM(200)\r\n        self.dense1_layer = tfk.layers.Dense(128, activation=tfk.activations.relu)\r\n        self.dense2_layer = tfk.layers.Dense(10, activation=tfk.activations.softmax)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        hidden = self.embedding_layer(inputs)\r\n        hidden = self.rnn_layer(hidden)\r\n        hidden = self.dense1_layer(hidden)\r\n        y_pred = self.dense2_layer(hidden)\r\n        return y_pred\r\n\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\nepochs = 30\r\nx = np.random.randint(low=0, high=50000, size=(10000, 128))\r\ny = np.random.randint(low=0, high=10, size=(10000,))\r\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\r\ntrainset = tf.data.Dataset.zip(\r\n    (tf.data.Dataset.from_tensor_slices(x_train), tf.data.Dataset.from_tensor_slices(y_train))).batch(300)\r\nvalset = tf.data.Dataset.zip(\r\n    (tf.data.Dataset.from_tensor_slices(x_val), tf.data.Dataset.from_tensor_slices(y_val))).batch(300)\r\n# model = Test()\r\nmodel = Test2()\r\ntrain_acc = tf.metrics.SparseCategoricalAccuracy()\r\nval_acc = tf.metrics.SparseCategoricalAccuracy()\r\ntrain_loss = tf.metrics.Mean()\r\nval_loss = tf.metrics.Mean()\r\nloss_object = tf.losses.SparseCategoricalCrossentropy()\r\noptimizer = tf.optimizers.Adam()\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss_object, metrics=[train_acc])\r\nmodel.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=300, epochs=epochs)\r\n\r\n# model = Test()\r\nmodel = Test2()\r\n\r\n\r\n@tf.function\r\ndef train_op(x, y):\r\n    with tf.GradientTape() as tape:\r\n        y_pred = model(x)\r\n        loss = loss_object(y, y_pred)\r\n        train_loss.update_state(loss)\r\n        train_acc.update_state(y, y_pred)\r\n        tf.print(y_pred)\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\n@tf.function\r\ndef val_op(x, y):\r\n    y_pred = model(x)\r\n    loss = loss_object(y, y_pred)\r\n    val_loss.update_state(loss)\r\n    val_acc.update_state(y, y_pred)\r\n\r\n\r\nfor epoch in range(epochs):\r\n    print(\"Epoch {}/{}\".format(epoch + 1, epochs))\r\n    bar = tfk.utils.Progbar(len(y_train), unit_name=\"sample\", stateful_metrics={\"loss\", \"acc\"})\r\n    log_values = []\r\n    for batch_x, batch_y in trainset:\r\n        train_op(batch_x, batch_y)\r\n        log_values.append((\"loss\", train_loss.result().numpy()))\r\n        log_values.append((\"acc\", train_acc.result().numpy()))\r\n        bar.add(len(batch_y), log_values)\r\n    for batch_x, batch_y in valset:\r\n        val_op(batch_x, batch_y)\r\n    print(\"val_loss -\", val_loss.result().numpy(), \"val_acc -\", val_acc.result().numpy())\r\n```\r\n@amjass12 this is my test code, under the same network, the result of model.fit is better than gradienttape", "@gowthamkpr can you help us to figure out ? Is there some implicit operation in model.fit ? why there is much diff in model.fit vs gradienttape", "Hi @NLP-ZY ,\n\nNo worries! Yes this is interesting behaviour indeed!  I assume specifying activation =None means that it uses no activation function and is therefore linear model?\n\nI would really like to understand this behaviour and get to the bottom of this as I need gradient tape for a shared dense layer!  ", "I also run into the same problem. The loss is far larger when I updated the weights using tf.GradientTape than calling model.fit. I created a [reproducible example](https://colab.research.google.com/drive/1aIRqi_x-YGAFtZCWL4gkOPkW_8sSlZ_N) in Colab. Could anyone have a look? It would take 2 minutes to reproduce the problem.", "Do you have any updates on this issue? Like what's causing it? Maybe a quick fix?", "I have the exact same problem - consistently much worse results using gradient tape as opposed to model fit, for the exact same network and training.  The network I'm using has batch norm, relu, and dropout.", "I am glad this problem is reproducible, maybe someone can look into this?", "yes right now because of this I have to give up on tensorflow 2 and revert to 1.x - may consider pytorch as well", "Any update on this? it would be nice to make some progress with this as i will need to use this soon for some unequal length input and merged layers :D \r\n\r\nthanks!", "Experiencing the same here. Any updates on when to expect a fix??? ", "Please take a look at issue #38596, linked above. I believe this is the underlying issue for why the Keras `model.fit()` method computes the wrong epoch loss.", "@NLP-ZY I can reproduce your error. Gist is [here](https://colab.research.google.com/gist/jvishnuvardhan/b14f74161a6d3236b90e092114709543/untitled99.ipynb). The reasons behind the discrepancy is mentioned [here](https://github.com/tensorflow/tensorflow/issues/32895#issuecomment-614813600).\r\n\r\nIn short, \r\n1) due to numerical instability issues, we need to remove `softmax` from the final layer of the model.\r\n2) When you specify `loss_object` as `tf.keras.losses.SparseCategoricalCrossentropy()`, Under the hood, model.fit uses `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`. However, in the custom training with tf.function decoration, we need to explicitly specify it as `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\r\n\r\nI made those changes and ran your code for 25 epochs. I get 93.24% training accuracy. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/7b0781f7f7201eea170d7270c6de4b0d/untitled100.ipynb). Thanks.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n", "@amjass12 Can you please follow the steps mentioned above and verify whether it resolved the issue for you. If your issue was not resolved, can you please post a complete standalone code to reproduce your issue. \r\n\r\nIf the issue was resolved, then please close the issue. Thanks!\r\n", "@jvishnuvardhan Thanks for your reply. I have try to use `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ` replace `tf.keras.losses.SparseCategoricalCrossentropy()`;\r\nthe loss of GradientTape is decrease slower than Model.fit too, this can't solve my problem.\r\nI want to figure out how can I use GradientTape get the same loss & acc with model.fit on the same epochs, because there are some complex model can't use model.fit. I cat get loss: 0.0087 - acc: 0.9998 on epoch 5(model.fit); but loss: 0.9447 - acc: 0.6677 on epoch 5(GradientTape).  Maybe when I use GradientTape, I should train more epochs ?", "@jvishnuvardhan I try to set epoch=100 for GradientTape and set epoch=5 for mode.fit. I get loss: 0.0030 - acc;: 1.0000 on epoch 5(model.fit), get loss: 1.4832 - acc: 0.9774 on epoch 100 (GradientTape). you can see my result on the [gist](https://colab.research.google.com/gist/NLP-ZY/dba4ddafeef5c08f245a062bb6758602/untitled99.ipynb). Please help us figure out why GradientTape can't work as Model.fit.", "Does using tf.gradients instead GradientTape make a difference in your cases?? ", "@amalroy2016 Thanks, I have a try, but it doesn't work.", "@jvishnuvardhan\n\nMany thanks for the message I will do this and report back asap!", "@amjass12 Did you had a time to look into my gist. Please also check the [solution](https://github.com/tensorflow/tensorflow/issues/35533#issuecomment-626128762) (using `reset_states`) that worked for @NLP-ZY. \r\n\r\nPlease verify once and close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35585\">No</a>\n"]}, {"number": 35584, "title": "TF 1.14 build failed on Ubuntu 18.04: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version:1.14\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?: Virtualenv\r\n- Bazel version (if compiling from source):1.1.0\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: Cuda=10.2/cuDNN=7.6.5\r\n- GPU model and memory:Geforce GTX 1070/8G/Driver Version: 440.33.01    \r\n\r\n\r\n**Describe the problem**\r\nUndefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n\r\nBuild fails with error: ERROR: /data/vkurien/tensorflow-build/tensorflow/tensorflow/python/keras/api/BUILD:115:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 776, in <module>\r\n    main()\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 772, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 629, in create_api_files\r\n    compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 503, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"/home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/vkurien/.virtualenvs/tensorflow-build-1.14/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 684, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/vkurien/.cache/bazel/_bazel_vkurien/229479d4bc4ba13038e7d45343e2dcc2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /data/vkurien/tensorflow-build/tensorflow/tensorflow/python/tools/BUILD:98:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nINFO: Elapsed time: 6732.030s, Critical Path: 456.19s\r\nINFO: 4238 processes: 4238 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRan ./configure and accepted CUDA and TensorRT option. \r\nThen ran: bazel-1.1.0 build --config=opt --config=v1 --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is probably a repeat of issue [build failure for TF2](https://github.com/tensorflow/tensorflow/issues/35115). If you'd rather add it there, I can\r\n", "@1zoom yes, I believe it's the same problem. Simple solution is to use bazel 0.2x instead of 1.1.0.", "When I try downgrading by the following command\r\n\r\n`sudo apt install bazel-0.29.0`\r\n\r\nI get the following error\r\n\r\n> Reading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nE: Unable to locate package bazel-0.29.0\r\nE: Couldn't find any package by glob 'bazel-0.29.0'\r\nE: Couldn't find any package by regex 'bazel-0.29.0'\r\n\r\n\r\nHow do I install bazel-0.2x", "I installed from source and got the error that I do need bazel 1.0.0 or higher.\r\n\r\n> (base) antpc@ant-pc: /home/antpc/tensorflow$ which bazel\r\n/home/antpc/bin/bazel\r\n(base) antpc@ant-pc:/home/antpc/tensorflow$ bazel --version\r\nbazel 0.29.1\r\n(base) antpc@ant-pc:/home/antpc/tensorflow$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease upgrade your bazel installation to version 1.0.0 or higher to build TensorFlow!", "@sujoyrc, @freedomtan ,\r\n\r\n1. I confirmed that using bazel 0.26.1  does work successfully.\r\n2. In terms of the specific above, you have to change the following files to make the build work with bazel-0.26.1:\r\n3. configure.py: \r\n-_TF_MIN_BAZEL_VERSION = '1.0.0'\r\n-_TF_MAX_BAZEL_VERSION = '1.1.0'\r\n+_TF_MIN_BAZEL_VERSION = '0.26.0'\r\n+_TF_MAX_BAZEL_VERSION = '0.26.1'\r\n4) WORKSPACE:\r\n-check_bazel_version_at_least(\"1.0.0\")\r\n+check_bazel_version_at_least(\"0.26.0\")\r\n\r\n5) .bazelrc:  # Enable using platform specific build settings\r\n-build --enable_platform_specific_config\r\n+build --config=linux <-- If you are using windows then change this. \r\n\r\nThat made it all work\r\n\r\n", "Thanks it worked", "@1zoom ,\r\nPlease close the issue if the issue is resolved, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35584\">No</a>\n", "Workaround found, but no solution - closing for now", "This bazel 0.26.1 solution worked for me also - Ubuntu16, python 3.5.2, TF2.1 cuda gpu, gcc 5.4.0, cuda 10.1, cudnn7, GeForce GTX 1060 6GB", "Also works for me. In my case, since I use bazelisk, I also had to change the .bazelversion file to '0.26.1' to make this work.\r\n \r\n Any chance of backporting this fix to the r2.1 branch?\r\n", "> @sujoyrc, @freedomtan ,\r\n> \r\n> 1. I confirmed that using bazel 0.26.1  does work successfully.\r\n> 2. In terms of the specific above, you have to change the following files to make the build work with bazel-0.26.1:\r\n> 3. configure.py:\r\n>    -_TF_MIN_BAZEL_VERSION = '1.0.0'\r\n>    -_TF_MAX_BAZEL_VERSION = '1.1.0'\r\n>    +_TF_MIN_BAZEL_VERSION = '0.26.0'\r\n>    +_TF_MAX_BAZEL_VERSION = '0.26.1'\r\n> \r\n> 1. WORKSPACE:\r\n>    -check_bazel_version_at_least(\"1.0.0\")\r\n>    +check_bazel_version_at_least(\"0.26.0\")\r\n> 2. .bazelrc:  # Enable using platform specific build settings\r\n>    -build --enable_platform_specific_config\r\n>    +build --config=linux <-- If you are using windows then change this.\r\n> \r\n> That made it all work\r\n\r\n\r\nStill buggy from my side...\r\n\r\n```console\r\nINFO: From ProtoCompile tensorflow/python/framework/cpp_shape_inference.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nERROR: ....../tensorflow/core/util/BUILD:338:1: Executing genrule //tensorflow/core/util:version_info_gen failed (Profiling timer expired): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\n/bin/bash: line 1: 11745 Profiling timer expired bazel-out/host/bin/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref \"bazel-out/k8-opt/bin/tensorflow/core/util/version_info.cc\" --git_tag_override=${GIT_TAG_OVERRIDE:-}\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 78.994s, Critical Path: 13.68s\r\nINFO: 231 processes: 231 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@jiapei100 What were your build flags? Mine were the standard cuda and opt flags, nothing else. I'm wondering why protobuf directories don't exist. To make sure that everything starts from a clean state, without pollution from prior builds, I tend to rm -fr .cache/bazel first.  ", "In case it helps, adding the bazel flag `--noincompatible_do_not_split_linking_cmdline` when using bazel 0.27.1 and python3 allowed my build to complete successfully."]}, {"number": 35583, "title": "TensorRT 6.0.1 performs worse than TensorRT 5.1.6 on Jetson AGX Xavier", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack 4.3 (L4T 32.3.1 Ubuntu 18.04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Nvidia Official Wheel\r\n- TensorFlow version (use command below): 1.13.1 & 1.15.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0.326 / 7.5.0 & 7.6.3\r\n- GPU model and memory: Tegra Xavier 16 GB\r\n\r\n**Describe the current behavior**\r\nI am currently trying some examples within from docker containers on new release of Jetpack (4.3). I wanted to do some benchmarkings and compare TensorRT 6.0.1 and cuDNN 7.6.3 versions with the old ones (5.1.6 and 7.5.0 respectively). I have some issues regarding to this as follows;\r\n\r\n1. **24 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.13.1+nv19.3, tensorrt=5.1.6, cudnn=7.5.0 opencv=3.3.1\r\n2. **9.6 FPS** with --> protobuf=3.8.0, tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, opencv=4.1.1\r\n3. **8.5 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, opencv=4.1.1\r\n4. **6.5 FPS** with --> protobuf=3.6.1, tensorflow-gpu=1.15.0+nv19.11, tensorrt=5.1.6, cudnn=7.5.0, opencv=3.4.6\r\n\r\n**Describe the expected behavior**\r\nImproved inference performance (above 24 FPS) with the new versions of Tensorflow, cuDNN and TensorRT\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://github.com/jkjung-avt/tf_trt_models --> `python3 camera_tf_trt.py --usb --model ssd_mobilenet_v1_coco --build`\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI've created a [topic](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/) on Nvidia DevTalk forum and here is the answer I got from Nvidia people: [DevTalk forum comment](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/post/5416335/#5416335)", "comments": ["Another benchmark;\r\n\r\n5. 21 FPS with --> protobuf=3.6.1, tensorflow-gpu=1.13.1+nv19.03, tensorrt=5.1.6, cudnn=7.6.3, opencv=4.1.1", "CC @DEKHTIARJonathan ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 35582, "title": "Tensorflow 1.14.0 can not be built from source on Nvidia Jetson", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: L4T 32.3.1 Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.14.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.24.1\r\n- **GCC/Compiler version (if compiling from source)**: GCC 7.4\r\n- **CUDA/cuDNN version**: 10.0.326 / 7.6.3\r\n- **GPU model and memory**: Tegra Xavier / 16 GB\r\n- **Exact command to reproduce**: \r\n```\r\nexport TMP=/tmp\r\nPYTHON_BIN_PATH=$(which python3) \\\r\nPYTHON_LIB_PATH=$(python3 -c 'import site; print(site.getsitepackages()[0])') \\\r\nTF_CUDA_COMPUTE_CAPABILITIES=${cuda_compute} \\\r\nTF_CUDA_VERSION=10.0 \\\r\nTF_CUDA_CLANG=0 \\\r\nTF_CUDNN_VERSION=7 \\\r\nTF_TENSORRT_VERSION=${trt_version} \\\r\nCUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\nCUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \\\r\nTENSORRT_INSTALL_PATH=/usr/lib/aarch64-linux-gnu \\\r\nTF_NEED_IGNITE=0 \\\r\nTF_ENABLE_XLA=0 \\\r\nTF_NEED_OPENCL_SYCL=0 \\\r\nTF_NEED_COMPUTECPP=0 \\\r\nTF_NEED_ROCM=0 \\\r\nTF_NEED_CUDA=1 \\\r\nTF_NEED_TENSORRT=1 \\\r\nTF_NEED_OPENCL=0 \\\r\nTF_NEED_MPI=0 \\\r\nGCC_HOST_COMPILER_PATH=$(which gcc) \\\r\nCC_OPT_FLAGS=\"-march=native\" \\\r\nTF_SET_ANDROID_WORKSPACE=0 \\\r\n    ./configure\r\n```\r\n```\r\nbazel build --config=opt \\\r\n\t    --config=cuda \\\r\n\t    --local_resources=${local_resources} \\\r\n            //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nHi, I am currently trying to build tensorflow 1.14.0 on Nvidia Jetson platform for TensorRT 6.0.1. But it is too painful to do so because it keep throwing errors and fails. I have applied some patches to the source code as described in below links:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/19956#issuecomment-397212193\r\n- https://github.com/tensorflow/tensorflow/issues/28277#issuecomment-517469318\r\n- https://github.com/tensorflow/tensorflow/issues/32925#issuecomment-537593098\r\n\r\nHowever, after all of the above patches, I still get the following errors shown on the logs section below. It is not the full log but if it is required, I can provide the full log too. Thanks in advance.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nIn file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,\r\n                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,\r\n                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:\r\nbazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5611:33: note: declared here\r\n     TRT_DEPRECATED virtual void setInt8Calibrator(IInt8Calibrator* calibrator) TRTNOEXCEPT = 0;\r\n                                 ^~~~~~~~~~~~~~~~~\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:4939:77: warning: 'virtual nvinfer1::INetworkDefinition* nvinfer1::IBuilder::createNetwork()' is deprecated [-Wdeprecated-declarations]\r\n       TrtUniquePtrType<nvinfer1::INetworkDefinition>(builder->createNetwork());\r\n                                                                             ^\r\nIn file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,\r\n                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,\r\n                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:\r\nbazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5431:58: note: declared here\r\n     TRT_DEPRECATED virtual nvinfer1::INetworkDefinition* createNetwork() TRTNOEXCEPT = 0;\r\n                                                          ^~~~~~~~~~~~~\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:5011:62: warning: 'virtual nvinfer1::ICudaEngine* nvinfer1::IBuilder::buildCudaEngine(nvinfer1::INetworkDefinition&)' is deprecated [-Wdeprecated-declarations]\r\n   engine->reset(builder->buildCudaEngine(*converter.network()));\r\n                                                              ^\r\nIn file included from ./tensorflow/compiler/tf2tensorrt/utils/trt_allocator.h:25:0,\r\n                 from ./tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26,\r\n                 from tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:16:\r\nbazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:5566:51: note: declared here\r\n     TRT_DEPRECATED virtual nvinfer1::ICudaEngine* buildCudaEngine(\r\n                                                   ^~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6315.216s, Critical Path: 450.27s\r\nINFO: 5761 processes: 5761 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "comments": ["Confirming that. Cannot build TF 1.14.0 on aarch64 when using nvcr.io/nvidia/l4t-base:r32.3.1 as base container and adding TRT 6.0.1", "Does switching to TF 1.15 help to solve this problem?", "@ymodak I didn't try to build 1.15 from source but I have tried to build TF 2.0 on Jetson and it was successful. Nvidia provides official TF 1.15 wheel for Jetson so I didn't need to build it myself.", "Yes, 1.15 builds fine with Bazel 0.24.1 and JetPack 4.3 ( I am building it in a container )", "1.14 fails (after patching) with :\r\n```\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc: At global scope:\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:797:8: error: 'bool tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor::isShape() const' marked 'override', but does not override\r\n   bool isShape() const override { return false; }\r\n        ^~~~~~~\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc: In constructor 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::TRT_TensorOrWeights(nvinfer1::DataType, const nvinfer1::Dims&, int)':\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:815:60: error: invalid new-expression of abstract class type 'tensorflow::tensorrt::convert::TRT_TensorOrWeights::SimpleITensor'\r\n     : simple_itensor_(new SimpleITensor(trt_dtype, trt_dims)),\r\n```\r\nHere  is the patch:\r\n```\r\n--- tensorflow/BUILD       2019-02-25 20:37:04.000000000 +0000                                                                                                                                                                                \r\n+++ tensorflow/BUILD       2019-04-04 16:45:53.441354395 +0000                                                                                                                                                                                \r\n@@ -511,6 +511,9 @@                                                                                                                                                                                                                           \r\n         \"//tensorflow/cc:scope\",\r\n         \"//tensorflow/cc/profiler\",\r\n         \"//tensorflow/core:tensorflow\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_conversion\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_op_kernels\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_engine_op_op_lib\",\r\n     ] + if_ngraph([\"@ngraph_tf//:ngraph_tf\"]),\r\n )\r\n \r\n--- third_party/toolchains/cpus/arm/BUILD~      2019-06-19 00:48:23.000000000 +0200\r\n+++ third_party/toolchains/cpus/arm/BUILD       2019-09-30 14:28:29.124554000 +0200\r\n@@ -10,6 +10,7 @@\r\n         \"armeabi|compiler\": \":cc-compiler-armeabi\",\r\n         \"local|compiler\": \":cc-compiler-local\",\r\n         \"armeabi\": \":cc-compiler-armeabi\",\r\n+        \"aarch64\": \":cc-compiler-local\",\r\n         \"k8\": \":cc-compiler-local\",\r\n         \"piii\": \":cc-compiler-local\",\r\n         \"arm\": \":cc-compiler-local\",\r\n--- third_party/gpus/crosstool/BUILD.tpl~       2019-06-19 00:48:23.000000000 +0200\r\n+++ third_party/gpus/crosstool/BUILD.tpl        2019-09-30 14:28:59.516554000 +0200\r\n@@ -29,6 +29,7 @@\r\n         \"x64_windows|msvc-cl\": \":cc-compiler-windows\",\r\n         \"x64_windows\": \":cc-compiler-windows\",\r\n         \"arm\": \":cc-compiler-local\",\r\n+        \"aarch64\": \":cc-compiler-local\",\r\n         \"k8\": \":cc-compiler-local\",\r\n         \"piii\": \":cc-compiler-local\",\r\n         \"ppc\": \":cc-compiler-local\",\r\ncommit 851b3f5a467bdd1f41c32a6b346940980530898d\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Wed Jun 26 07:11:29 2019 -0700\r\n\r\n    Disable dot-product depthwise conv when not compiling for Android with Clang.\r\n    \r\n    PiperOrigin-RevId: 255181633\r\n\r\ndiff --git a/tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc b/tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\nindex 8baf2c7253..fd5b89eaf7 100644\r\n--- tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\n+++ tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\n@@ -170,7 +170,8 @@ inline void DispatchDepthwiseConv(\r\n       // This is compiled-in even if dot-product instructions are unavailable.\r\n       // However, tests should skip dot-product testing in that case and not\r\n       // call this code.\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n       DotProduct3x3KernelType kernel_type =\r\n           optimized_ops::depthwise_conv::CategorizeDotProductKernel(\r\n               input_shape, filter_shape, params);\r\n@@ -683,7 +684,8 @@ void TestOneDepthwiseConv3x3Filter(\r\n }\r\n \r\n void TestOneNeonDot3x3(const TestParam& test_param) {\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n   CpuBackendContext backend_context;\r\n   ruy::Context* ruy_context = backend_context.ruy_context();\r\n   const auto ruy_paths = ruy_context != nullptr\r\n@@ -854,7 +856,8 @@ INSTANTIATE_TEST_SUITE_P(\r\n     TestParam::TestNameSuffix);\r\n #endif\r\n \r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n INSTANTIATE_TEST_SUITE_P(\r\n     NeonAsm, DepthwiseConvTest,\r\n     testing::Combine(\r\ndiff --git a/tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h b/tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\nindex 23940e3c33..8b57c3ed65 100644\r\n--- tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\n+++ tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\n@@ -2006,7 +2006,8 @@ inline void DepthwiseConvWithRounding(\r\n \r\n // Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\r\n // Jetson TX-2. This compiler does not support the offsetof() macro.\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n   // Dispatch to dot-product 3x3 kernels when supported.\r\n   if (cpu_flags.neon_dotprod) {\r\n     using optimized_ops::depthwise_conv::DotProduct3x3KernelType;\r\n@@ -2025,6 +2026,8 @@ inline void DepthwiseConvWithRounding(\r\n     }\r\n   }\r\n \r\n+#elif defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+\r\n   // Dispatch to non-dot-product 3x3 kernels when supported.\r\n \r\n   const int stride_width = params.stride_width;\r\ndiff --git a/tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h b/tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\nindex cf2bcb2279..9f827e988a 100644\r\n--- tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n+++ tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n@@ -5786,7 +5786,8 @@ struct WorkspacePrefetchWrite<\r\n \r\n #endif  // __aarch64__\r\n \r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n // Dot product ops hard-coded\r\n \r\n template <>\r\n\r\n--- third_party/tensorrt/tensorrt_configure.bzl~        2019-06-18 22:48:23.000000000 +0000\r\n+++ third_party/tensorrt/tensorrt_configure.bzl 2020-01-05 16:32:05.452000000 +0000\r\n@@ -27,8 +27,8 @@\r\n     \"NvUtils.h\",\r\n     \"NvInferPlugin.h\",\r\n     \"NvInferVersion.h\",\r\n-    \"NvInferRTSafe.h\",\r\n-    \"NvInferRTExt.h\",\r\n+    \"NvInferRuntime.h\",\r\n+    \"NvInferRuntimeCommon.h\",\r\n     \"NvInferPluginUtils.h\",\r\n ]\r\n```", "Thank you for this, it was very helpful.\r\n\r\nI'm running TX2 with JetPack 4.3 (comes with TensorRT 6.0.1). I tried to compile Tensorflow 1.14.1. This is the patch I had to apply (one difference with the previous one):\r\n\r\n```\r\ndiff --git tensorflow/BUILD tensorflow/BUILD\r\nindex 1e0cc9207b..9a90069ddf 100644\r\n--- tensorflow/BUILD\r\n+++ tensorflow/BUILD\r\n@@ -594,6 +594,9 @@ tf_cc_shared_object(\r\n         \"//tensorflow/cc:scope\",\r\n         \"//tensorflow/cc/profiler\",\r\n         \"//tensorflow/core:tensorflow\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_conversion\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_op_kernels\",\r\n+        \"//tensorflow/compiler/tf2tensorrt:trt_engine_op_op_lib\",\r\n     ] + if_ngraph([\"@ngraph_tf//:ngraph_tf\"]),\r\n )\r\n \r\ndiff --git tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc\r\nindex 306341301b..dfd65f6a3c 100644\r\n--- tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc\r\n+++ tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc\r\n@@ -794,7 +794,9 @@ class TRT_TensorOrWeights::SimpleITensor : public nvinfer1::ITensor {\r\n \r\n   nvinfer1::TensorFormats getAllowedFormats() const override { return 1; }\r\n \r\n-  bool isShape() const override { return false; }\r\n+  bool isShapeTensor() const override { return false; }\r\n+\r\n+  bool isExecutionTensor() const override { return true; }\r\n #endif\r\n \r\n  private:\r\ndiff --git tensorflow/lite/kernels/internal/BUILD tensorflow/lite/kernels/internal/BUILD\r\nindex 5bafcdc00c..a749a5316b 100644\r\n--- tensorflow/lite/kernels/internal/BUILD\r\n+++ tensorflow/lite/kernels/internal/BUILD\r\n@@ -22,15 +22,12 @@ HARD_FP_FLAGS_IF_APPLICABLE = select({\r\n NEON_FLAGS_IF_APPLICABLE = select({\r\n     \":arm\": [\r\n         \"-O3\",\r\n-        \"-mfpu=neon\",\r\n     ],\r\n     \":armeabi-v7a\": [\r\n         \"-O3\",\r\n-        \"-mfpu=neon\",\r\n     ],\r\n     \":armv7a\": [\r\n         \"-O3\",\r\n-        \"-mfpu=neon\",\r\n     ],\r\n     \"//conditions:default\": [\r\n         \"-O3\",\r\ndiff --git tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\nindex 287f0a46be..c9bcb35c94 100644\r\n--- tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\n+++ tensorflow/lite/kernels/internal/depthwiseconv_quantized_test.cc\r\n@@ -167,7 +167,8 @@ inline void DispatchDepthwiseConv(\r\n       // This is compiled-in even if dot-product instructions are unavailable.\r\n       // However, tests should skip dot-product testing in that case and not\r\n       // call this code.\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n       DotProduct3x3KernelType kernel_type =\r\n           optimized_ops::depthwise_conv::CategorizeDotProductKernel(\r\n               input_shape, filter_shape, params);\r\n@@ -691,7 +692,8 @@ void TestOneDepthwiseConv3x3Filter(\r\n }\r\n \r\n void TestOneNeonDot3x3(const TestParam& test_param) {\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n   CpuBackendContext backend_context;\r\n   ruy::Context* ruy_context = backend_context.ruy_context();\r\n   const auto ruy_paths = ruy_context != nullptr\r\n@@ -856,7 +858,8 @@ INSTANTIATE_TEST_SUITE_P(\r\n     TestParam::TestNameSuffix);\r\n #endif\r\n \r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n INSTANTIATE_TEST_SUITE_P(\r\n     NeonAsm, DepthwiseConvTest,\r\n     testing::Combine(\r\ndiff --git tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\nindex edf59248e1..8957425d34 100644\r\n--- tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\n+++ tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8.h\r\n@@ -2007,7 +2007,8 @@ inline void DepthwiseConvWithRounding(\r\n \r\n // Enable for arm64 except for the Nvidia Linux 4 Tegra (L4T) running on\r\n // Jetson TX-2. This compiler does not support the offsetof() macro.\r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n   // Dispatch to dot-product 3x3 kernels when supported.\r\n \r\n   ruy::Context* ruy_context = cpu_backend_context->ruy_context();\r\n@@ -2030,6 +2031,8 @@ inline void DepthwiseConvWithRounding(\r\n     }\r\n   }\r\n \r\n+#elif defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+\r\n   // Dispatch to non-dot-product 3x3 kernels when supported.\r\n \r\n   const int stride_width = params.stride_width;\r\ndiff --git tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\nindex 5b570eed18..eef46efe96 100644\r\n--- tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n+++ tensorflow/lite/kernels/internal/optimized/depthwiseconv_uint8_3x3_filter.h\r\n@@ -5790,7 +5790,8 @@ struct WorkspacePrefetchWrite<\r\n \r\n #endif  // USE_NEON &&__aarch64__\r\n \r\n-#if defined(__aarch64__) && !defined(GOOGLE_L4T)\r\n+#if defined(__aarch64__) && !defined(GOOGLE_L4T) && defined(__ANDROID__) && \\\r\n+    defined(__clang__)\r\n // Dot product ops hard-coded\r\n \r\n template <>\r\ndiff --git third_party/tensorrt/tensorrt_configure.bzl third_party/tensorrt/tensorrt_configure.bzl\r\nindex 81612e5212..959391997b 100644\r\n--- third_party/tensorrt/tensorrt_configure.bzl\r\n+++ third_party/tensorrt/tensorrt_configure.bzl\r\n@@ -27,8 +27,8 @@ _TF_TENSORRT_HEADERS_V6 = [\r\n     \"NvUtils.h\",\r\n     \"NvInferPlugin.h\",\r\n     \"NvInferVersion.h\",\r\n-    \"NvInferRTSafe.h\",\r\n-    \"NvInferRTExt.h\",\r\n+    \"NvInferRuntime.h\",\r\n+    \"NvInferRuntimeCommon.h\",\r\n     \"NvInferPluginUtils.h\",\r\n ]\r\n \r\ndiff --git third_party/toolchains/cpus/arm/BUILD third_party/toolchains/cpus/arm/BUILD\r\nindex 237c2357d8..62c1e7b3f4 100644\r\n--- third_party/toolchains/cpus/arm/BUILD\r\n+++ third_party/toolchains/cpus/arm/BUILD\r\n@@ -13,6 +13,7 @@ cc_toolchain_suite(\r\n         \"k8\": \":cc-compiler-local\",\r\n         \"piii\": \":cc-compiler-local\",\r\n         \"arm\": \":cc-compiler-local\",\r\n+        \"aarch64\": \":cc-compiler-local\",\r\n         \"s390x\": \":cc-compiler-local\",\r\n     },\r\n )\r\n```", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35582\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35582\">No</a>\n"]}, {"number": 35581, "title": "Make debug of if else statement in class Interpreter", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35581) for more info**.\n\n<!-- need_sender_cla -->", "> ed any issues), please reply here with\r\n\r\n\r\n\r\n> @googlebot I signed it!\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35581) for more info**.\n\n<!-- ok -->"]}, {"number": 35580, "title": "Make the notebook use tf 1.15.0, add tf 2.x incompatible warning", "body": "The notebook `TensorFlowLite_LSTM_Keras_Tutorial.ipynb` is not compatible with tf 2.x \r\n\r\n`pip install tf-nightly` was installing the latest nightly build and hence the code was throwing error. I have made changes so that this notebook uses tensorflow 1.15.0, so that anyone using this notebook won't get error and cells will execute smoothly. \r\n\r\nI will try to migrate this notebook to tf 2.x in some time.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/35580\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "Thanks for your contribution , can you please limit the line length to 80 characters otherwise it will fail sanity checks.\r\nHere is the link for [contribution guidelines.](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#google-python-style-guide)", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35579, "title": "tf.linalg.expm is incompatible with vectorized_map", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0-dev20191221\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nVectorizing `tf.linalg.expm` throws the error: `UnrecognizedFlagError: Unknown command line flag 'f'`\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Code to reproduce the issue**\r\n`tf.vectorized_map(lambda x: tf.linalg.expm(x), tf.reshape(tf.range(8.0),[2,2,2]))`\r\n\r\n**Other info / logs**\r\nThis may not be related specifically to expm, as it occurs with [other functions](https://github.com/tensorflow/tensorflow/issues/34734) as well\r\n", "comments": ["Was able to reproduce the issue using tf-nightly 2.1.0.dev20200106. Please find the [Gist](https://colab.sandbox.google.com/gist/amahendrakar/e695c0438fd9f99f07e13cbedd9a4cfc/35579.ipynb\r\n) here. Thanks!", "I tried the latest nightly and it looks like the issue has been resolved. Will close this issue for now but feel free to reopen if the issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35579\">No</a>\n"]}, {"number": 35578, "title": "added a usage example to average pooling 2d layer", "body": "I added a usage example to average pooling layer 2d.", "comments": ["Done!", "Thanks!", "Great job so far, please do make sure to use the function correctly, as of now your `average_pooling_layer` is just the `tf.keras.layers.AveragePooling2D` object.", "@generationzcode Can you please check rthadur's comments and keep us posted? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]