[{"number": 47662, "title": "Update aliasing in tf.linalg.band_part docs", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/band_part\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/56903ed0419882e004d4467dec452684b106e46f/tensorflow/core/api_def/base_api/api_def_MatrixBandPart.pbtxt#L50-L66\r\n\r\n`tf.matrix_band_part` in above lines should be `tf.linalg.band_part`. (`tf.matrix_band_part` is moved to `tf.compat.v1.matrix_band_part`)\r\n\r\nThe below links should also be fixed\r\n\r\n* https://github.com/tensorflow/tensorflow/blob/011228639301a8ed60d39c94ab096b074091bb4a/tensorflow/go/op/wrappers.go\r\n* https://github.com/tensorflow/tensorflow/blob/0748d9a3ab284f7c9461985db478d09e2873cf1a/tensorflow/compiler/mlir/tensorflow/ir/tf_generated_ops.td\r\n", "comments": ["@jeongukjae Are you still interested to raise a PR for this?\r\nNote that `wrappers.go` and `tf_generated_ops.td` are machine generated files therefore we should not edit manually.\r\n", "@ymodak I tried to fix this issue, but I don't use Golang, so I don't know how exactly auto-generate that files. I think it would be better that other people fix this.", "This is fixed with tf-nightly docs. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/band_part?version=nightly"]}, {"number": 47660, "title": "tensorflow-gpu==1.14.0 does not support python 3.6.12-debug", "body": "\r\n**System information**\r\n- I haven't written custom code.\r\n- OS Platform and Distribution :Linux Ubuntu 18.04.\r\n- Python version: python 3.6.12-debug\r\n\r\n**Describe the current behavior**\r\nI used python 3.6.12-debug built by pyenv:\r\n$ pip -V\r\npip 18.1 from /home/yawen/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)\r\n\r\nI wanted to install tensorflow 1.14.0, but I got the following error:\r\n$python -m pip install tensorflow-gpu==1.14.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.14.0\r\nERROR: No matching distribution found for tensorflow-gpu==1.14.0\r\n\r\nI checked my whether my python was 32bits or 64bits:\r\n>>> import platform\r\n>>> platform.architecture()\r\n('64bit', '')\r\n\r\nIt is 64bits, which satisfies the requirement of tensorflow. I don't know why tensorflow does not support python 3.6.12-debug.\r\n\r\n**Describe the expected behavior**\r\nI hope I can successfully install tensorflow on python 3.6.12-debug, or any other version of python-debug.\r\n\r\n\r\n", "comments": ["@tywofxd \r\nThere is no support for tf 1.x , so please update  tf 2.x and let us know.", "Following your instruction, I tried to install tensorflow 2.0.0, but it still did not work. Here is what I have done:\r\n$ pyver=3.6.12\r\n$ eval \"$(pyenv init -)\"\r\n$ pyenv shell \"$pyver\"-debug\r\n$ python -V\r\nPython 3.6.12\r\n$ pip -V\r\npip 21.0.1 from /home/yawen/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)\r\n$ python -m pip install tensorflow==2.0.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0\r\nERROR: No matching distribution found for tensorflow==2.0.0\r\n$ python -m pip install tensorflow==2.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0\r\nERROR: No matching distribution found for tensorflow==2.0.0\r\n\r\nI met the same error as installing tensorflow 1.14.0. Can you give me more suggestions? I appreciate it very much if you can help me.\r\n", "Hello!\r\nFollowing your instruction, I tried to install tensorflow 2.0.0, but it still did not work. Here is what I have done:\r\n$ pyver=3.6.12\r\n$ eval \"$(pyenv init -)\"\r\n$ pyenv shell \"$pyver\"-debug\r\n$ python -V\r\nPython 3.6.12\r\n$ pip -V\r\npip 21.0.1 from /home/yawen/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)\r\n$ python -m pip install tensorflow==2.0.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0\r\nERROR: No matching distribution found for tensorflow==2.0.0\r\n$ python -m pip install tensorflow==2.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\r\nLooking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.0.0\r\nERROR: No matching distribution found for tensorflow==2.0.0\r\nI met the same error as installing tensorflow 1.14.0. Can you give me more suggestions? I appreciate it very much if you can help me.\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    <notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2021\u5e743\u67089\u65e5(\u661f\u671f\u4e8c) \u4e2d\u53481:07\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"ywtan\"<yawentan_xd@foxmail.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] tensorflow-gpu==1.14.0 does not support python 3.6.12-debug (#47660)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@tywofxd\r\n There is no support for tf 1.x , so please update  tf 2.x and let us know.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@tywofxd \r\nTensorFlow v2.0 is compatible with Python 3.5 - 3.7. For more information, please take a look at the [tested build configurations.\r\n](https://www.tensorflow.org/install/source_windows#cpu).\r\n\r\nPlease verify with this [comment](Please post output of python --version, python -m pip --version and python -m pip install -vvv tensorflow) for compatibility, is your python version 64 bit.\r\n\r\n\r\nAlso please refer to similar issues: #47517, #39130, #31939 , [link](https://github.com/tensorflow/tensorflow/issues/35646#issuecomment-571876409)\r\n\r\n\r\n", "I can confirm this bug. `pip install tensorflow` works with 3.8.5-debug and 3.6.12 but not with 3.6.12-debug. pip version is 21.0.1.\r\n\r\n[Here](https://www.tensorflow.org/install/pip), you just say that `Python 3.5\u20133.8` is supported, which is not true because 3.6.12-debug is a Python version 3.5-3.8 .\r\n\r\nAll python versions were installed using pyenv.\r\n\r\n$ python --version\r\nPython 3.6.12\r\n$ pip --version\r\npip 21.0.1 from /home/volker/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)\r\n$ python -m pip --version \r\npip 21.0.1 from /home/volker/.pyenv/versions/3.6.12-debug/lib/python3.6/site-packages/pip (python 3.6)\r\n\r\nThe output of python -m pip install -vvv tensorflow is:\r\n[out.txt](https://github.com/tensorflow/tensorflow/files/6116566/out.txt)\r\n\r\n", "@@tywofxd\r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the Microsoft Visual C++ Redistributable package\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the system requirements and check if you have the correct dependencies installed.\r\n\r\nAlso, check this similar duplicate issue: #46124, #46738, #46788, #46490, #45398\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The point is that (on multiple Arch Linux machines)\r\n`pyenv install 3.6.12 `          and\r\n`pyenv install 3.8.5 --debug`\r\ngives you a python version that supports tensorflow, but\r\n`pyenv install 3.6.12 --debug`\r\ngives you a python version that does not support tensorflow.\r\n\r\n 3.8.5-debug and 3.6.12 but not with 3.6.12-debug\r\n\r\n> @@tywofxd\r\n> \r\n> You might be facing this issue because of the following reasons\r\n> \r\n> You are running 32-bit Python or 32-bit OS\r\n\r\nNope, everything is 64-bit\r\n> You have not installed the Microsoft Visual C++ Redistributable package\r\n\r\nN/A, I'm on Arch Linux.\r\n\r\n> Your CPU does not support AVX instructions.\r\n\r\n/proc/cpuinfo includes the line \"flags : ... avx ... avx2 ...\"\r\n\r\n> Please take a look at the system requirements and check if you have the correct dependencies installed.\r\n> \r\n> Also, check this similar duplicate issue: \r\n> #46124\r\n\r\nN/A, I'm on Arch Linux\r\n> #46738\r\n\r\nIncludes no new info\r\n\r\n> #46788\r\n\r\n1. different error message\r\n2. The cpu supports avx\r\n3. If the cpu would not support avx, the other python version would probably also not work.\r\n4. It is reproducible on different devices\r\n \r\n>#46490\r\n\r\n\r\n1. different error message\r\n2. The gpu supports cuda\r\n3. If the gpu would not support cudo, the other python version would probably also not work.\r\n4. It is reproducible on different devices\r\n\r\n> #45398\r\n\r\nDifferent error message\r\n> \r\n> Thanks!\r\n\r\n", "TF 1.x is no longer supported.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47660\">No</a>\n", "> TF 1.x is no longer supported.\r\n\r\nYes, but this Problem affects also newer versions.\r\n\r\nWith python 3.6.12-debug, pip install tensorflow does not work.", "@Volker-Weissmann \r\nAs this issue was opened with reference to 1.x, can you please create anew issue with 2.x and the code/steps and error faced [errorlogs]", "> @Volker-Weissmann\r\n> As this issue was opened with reference to 1.x, can you please create anew issue with 2.x and the code/steps and error faced [errorlogs]\r\n\r\nDone: https://github.com/tensorflow/tensorflow/issues/47991"]}, {"number": 47658, "title": "Change Xtensa softmax kernel to only support int8 inputs and int16 outputs", "body": "Size reduction of ~4KB:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWithout this change:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  88712\t    384\t  22704\t 111800\t  1b4b8   tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWith this change:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  84496\t    384\t  22704\t 107584\t  1a440\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nFor all of the Xtensa kernels, we are currently limiting support to only what we need for the keyword_benchmark. While this is not a scalable solution (and the reason why we have the selective registration of kernel variants), we are currently keeping the Fusion F1 and Hifimini\r\nimplementations consistently narrow in scope.\r\n\r\nThis ensures:\r\n * we have a smaller memory footprint (overfitting to a particular use-case)\r\n * all the code has correspinding unit tests (any float test cases are currently disabled for Xtensa with #if !defined(XTENSA))\r\n * the current size of the keyword_benchmark binary serves as a baseline to make sure that the selective registration API works as expected.\r\n\r\nProgress towards http://b/182209217\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47657, "title": "Converted TF2 model zoo MaskRCNN model to .tflite, however input tensor seems to have incorrect shape", "body": "I follow the gist above and converted the `model mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8` to a .tflite model.\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly: '2.5.0-dev20210308'\r\n\r\n### 2. Code\r\n\r\nConversion code:\r\n```\r\nimport tensorflow as tf\r\nfrom tflite_support import metadata as _metadata\r\n\r\nmodel_dir = r'C:\\Users\\yuh5\\PycharmProjects\\Convert1\\saved_model'\r\nsaved_model_dir = 'updated/saved_model'\r\nmodel = tf.saved_model.load(model_dir)\r\nconcrete_func = model.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([1, 1024, 1024, 3])\r\ntf.saved_model.save(model, saved_model_dir)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, signature_keys=['serving_default'])\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith tf.io.gfile.GFile('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\n# Used for adding metadata\r\npopulator = _metadata.MetadataPopulator.with_model_file(\"model.tflite\")\r\npopulator.load_associated_files([\"labels.txt\"])\r\npopulator.populate()\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nThe code run though successfully, however, the input tensor of the .tflite model has the shape shown in the image below:\r\n\r\n![Capture](https://user-images.githubusercontent.com/5137261/110395357-fd896d80-803b-11eb-9b73-738ce184b080.PNG)\r\n\r\nIs this correct? If so, when I use this model to run inference on Android with the demo code for object detection I got the follow error:\r\n\r\n`Caused by: java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (serving_default_input_tensor:0) with 3 bytes from a Java Buffer with 12582912 bytes.`\r\n\r\nHow should I modify the code to use maskRCNN model?\r\n\r\nThe model file is [here](https://nih.box.com/s/rzbde15y3jjf3pznwgptomacf79cgh7f)\r\n", "comments": ["@abattery ", "Could you also share the java code snippet for inferencing?", "Here are the relevant parts:\r\n\r\n```\r\nprivate static final int TF_OD_API_INPUT_SIZE = 1024;\r\nprivate static final boolean TF_OD_API_IS_QUANTIZED = false;\r\nprivate static final String TF_OD_API_MODEL_FILE = \"model.tflite\";\r\nprivate static final String TF_OD_API_LABELS_FILE = \"labels.txt\";\r\nprivate static final boolean MAINTAIN_ASPECT = false;\r\nprivate static final float MINIMUM_CONFIDENCE_TF_OD_API = 0.5f; \r\n```\r\n\r\n```\r\nprivate void processImage(Bitmap bitmap) {\r\n\r\n        rgbFrameBitmap = bitmap;\r\n\r\n        final Canvas canvas = new Canvas(croppedBitmap);\r\n        canvas.drawBitmap(rgbFrameBitmap, frameToCropTransform, null);\r\n\r\n        long tStart = System.currentTimeMillis();\r\n\r\n        final List<Detector.Recognition> results = detector.recognizeImage(croppedBitmap);\r\n\r\n...\r\n```\r\n\r\n ```\r\n @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSize; ++i) {\r\n      for (int j = 0; j < inputSize; ++j) {\r\n        int pixelValue = intValues[i * inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n        }\r\n      }\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    outputClasses = new float[1][NUM_DETECTIONS];\r\n    outputScores = new float[1][NUM_DETECTIONS];\r\n    numDetections = new float[1];\r\n\r\n    Object[] inputArray = {imgData};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, outputLocations);\r\n    outputMap.put(1, outputClasses);\r\n    outputMap.put(2, outputScores);\r\n    outputMap.put(3, numDetections);\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    Trace.endSection();\r\n```\r\n\r\nAnd this is the [link](https://drive.google.com/drive/folders/1mvUKZiJaM47qUPbOg-QZUzQ1zyjeIvYR?usp=sharing) to the whole project", "There are three TFLite models under https://drive.google.com/corp/drive/folders/1mS00HV8tONDNjyiX7USlyuhN2S9B6Vqw. Which one is the problematic one?", "@abattery It's \"model.tflite\"", "You need to resize the input tensor before the execution.\r\n\r\n```\r\ntfLite.resizeInput(0, new int[] {1, 1024, 1024, 3});\r\ntfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n```", "@abattery I first get an error complaining the byte array size. I figured I should probably use `tfLite.resizeInput(0, new int[] {4, 1024, 1024, 3});` since is a floating point model. Then, I got another error:\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 11 (FlexStridedSlice) failed to prepare.\r\n    \r\n    Node number 0 (WHILE) failed to invoke.\r\n```", "Since your model has enabled the Select TF option, you need to link the additional AAR to enable the Select TF option. See https://www.tensorflow.org/lite/guide/ops_select", "@abattery It worked! Only problem is it exceeds memory limit during inference... But thanks for the help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47657\">No</a>\n"]}, {"number": 47656, "title": "Fix the TFLM github CI bazel build.", "body": "We are now using a pared-down version of .bazelrc as well.\r\n\r\nThe reason for the most recent TFLM CI bazel build breakage wasi:\r\n * https://github.com/tensorflow/tensorflow/commit/6236b83d80e4907cec016fd3702d7bbf64beabf5 resulted in parsing of the .bazelrc also resulting in the BUILD and build_defs.bzl files in  //tensorflow/code/kernels/mlir_generated to be read.\r\n * Which in turn meant that CUDA related third party downloads had to be available.\r\n * And CUDA downloads are not needed for TFLM and had previously been removed from the github TFLM bazel build to speed things up.\r\n\r\nWith this change, tensorflow/lite/micro/tools/ci_build/test_bazel.sh does not error out.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47654, "title": " import tensorflow as tf gives an error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version:\r\n- Python version:Python 3.5.6\r\n- Installed using virtualenv? pip? conda?:pip \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIn [1]: import tensorflow as tf\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nC:\\Users\\Saagarika\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22\r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26\r\n\r\nC:\\Users\\Saagarika\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     52\r\n     53 # Protocol buffers\r\n---> 54 from tensorflow.core.framework.graph_pb2 import *\r\n     55 from tensorflow.core.framework.node_def_pb2 import *\r\n     56 from tensorflow.core.framework.summary_pb2 import *\r\n\r\nC:\\Users\\Saagarika\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py in <module>()\r\n      8 from google.protobuf import reflection as _reflection\r\n      9 from google.protobuf import symbol_database as _symbol_database\r\n---> 10 from google.protobuf import descriptor_pb2\r\n     11 # @@protoc_insertion_point(imports)\r\n     12\r\n\r\nC:\\Users\\Saagarika\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\descriptor_pb2.py in <module>()\r\n   1837 FileDescriptorSet = _reflection.GeneratedProtocolMessageType('FileDescriptorSet', (_message.Message,), dict(\r\n   1838   DESCRIPTOR = _FILEDESCRIPTORSET,\r\n-> 1839   __module__ = 'google.protobuf.descriptor_pb2'\r\n   1840   # @@protoc_insertion_point(class_scope:google.protobuf.FileDescriptorSet)\r\n   1841   ))\r\n\r\nTypeError: Expected a message Descriptor, got Descriptor\r\n", "comments": ["@Saagarika99 \r\nPlease refer to this issue and verify the requirements : #42367, #47517", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47654\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47654\">No</a>\n"]}, {"number": 47653, "title": "Use -stdlib=libc++ and -fno-rtti with Xtensa.", "body": "Manually confirmed with the steps outlined in #47575 that exception related symbols are no longer part of the keyword_benchmark binary when build with the Xtensa toolchain.\r\n\r\nManually tested the size with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade keyword_benchmark -j8 BUILD_TYPE=release\r\nxt-size tensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWithout this change:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  70912\t  40212\t  24856\t 135980\t  2132c\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWith this change:\r\n```\r\n   text\t   data\t    bss\t    dec\t    hex\tfilename\r\n  88712\t    384\t  22704\t 111800\t  1b4b8\ttensorflow/lite/micro/tools/make/gen/xtensa_fusion_f1_release/bin/keyword_benchmark\r\n```\r\n\r\nWhile what goes in the text and data sections has changed, the overall binary size is reduced by ~24KB.\r\n\r\nAlso confirmed that the cycles for the keyword benchmark are unaffected:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n```\r\n\r\ngives:\r\n```\r\nInitializeKeywordRunner took 159001 ticks (159 ms).\r\n\r\nKeywordRunNIerations(1) took 34253 ticks (34 ms)\r\nQUANTIZE took 800 ticks (0 ms).\r\nSVDF took 4753 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 4211 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 3145 ticks (3 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 4211 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 2890 ticks (2 ms).\r\nSVDF took 3583 ticks (3 ms).\r\nSVDF took 3054 ticks (3 ms).\r\nFULLY_CONNECTED took 1091 ticks (1 ms).\r\nSOFTMAX took 749 ticks (0 ms).\r\nQUANTIZE took 354 ticks (0 ms).\r\n\r\nKeywordRunNIerations(10) took 342530 ticks (342 ms)\r\n```\r\n\r\nAnd all the unit tests pass:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test -j8\r\n```\r\n\r\nFixes #47575\r\n\r\nWith this change, we no longer need to remove `-fno-rtti` for Xtensa and http://b/150240249 is also fixed.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47652, "title": "ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(\"dense/truediv:0\", shape=(?, 2, 209), dtype=float32). It ", "body": "Runs without error locally but it comes to this error in aws SageMaker.\r\nHier is the model\r\n\r\ninput_train = np.column_stack((input_cat1, input_cat2, input_num, input_cat3))\r\n\r\nos.makedirs(\"./data\", exist_ok = True)\r\nnp.savez('./data/training', train_input = input_train, train_output=target_cat)\r\n\r\nsage_maker_session = sagemaker.Session()\r\ntraining_input_path = sage_maker_session.upload_data('data/training.npz', key_prefix=prefix + training_folder)\r\nprint(training_input_path)\r\nprint(training_input_path)\r\n\r\ns3://sagemaker-eu-central-1-xxxxxxxxxxx/user_tracking/training/training.npz\r\n\r\n\r\n**************************************************************************************************************************************\r\n%%writefile train.py\r\n\r\n#Nachdem das Programm definiert hat, welche Argumente es ben\u00f6tigt, argparse findet heraus, wie es diese aus sys.argv auslesen kann\r\nimport argparse\r\nfrom tensorflow.keras.preprocessing.text import Tokenizer\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Activation, Dropout, TimeDistributed, RepeatVector\r\nfrom tensorflow.keras.layers import  Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\nimport tensorflow as tf\r\nimport json\r\nimport os\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n\r\nif __name__ == \"__main__\":\r\n        \r\n    parser = argparse.ArgumentParser()\r\n\r\n    # hyperparameters, die sp\u00e4ter eingestellt werden m\u00fcssen, werden hier als command-line arguments addiert\r\n    parser.add_argument('--epochs', type=int, default=60)\r\n    parser.add_argument('--batch-size', type=int, default=50)\r\n    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\r\n    parser.add_argument('--training', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\r\n    \r\n    args, _ = parser.parse_known_args()\r\n    \r\n    epochs     = args.epochs\r\n    batch_size = args.batch_size\r\n    model_dir  = args.model_dir\r\n    training_dir   = args.training\r\n\r\n    \r\n    input_train =np.load(os.path.join(training_dir, 'training.npz'))['train_input']\r\n    target =np.load(os.path.join(training_dir, 'training.npz'))['train_output']\r\n    \r\n    print (\"input_train shape:\", input_train.shape)\r\n    print (\"target shape:\", target.shape)\r\n    \r\n    input_cat1 = input_train[:,0].astype(np.int32)\r\n    input_cat2 = input_train[:,1].astype(np.int32)\r\n    input_cat3 = input_train[:,3:].astype(np.int32)\r\n    input_num = input_train[:,2].astype(np.float32)\r\n    \r\n    print (\"shape input_cat1:\", input_cat1.shape)\r\n    print (\"shape input_cat2:\", input_cat2.shape)\r\n    print (\"shape input_cat3:\", input_cat3.shape)\r\n    print (\"shape input_num:\", input_num.shape)\r\n    print (\"shape target:\", target.shape)\r\n    \r\n    \r\n    n_steps = 2     # number of timesteps in each sample\r\n    num_unique_os = 5                  #len(le_betriebsystem.classes_)+1\r\n    num_unique_browser = 10            #len(le_browser.classes_)+1\r\n    num_unique_actions = 210           #len(le_actionen.classes_)+1\r\n    \r\n    os_emb_size = 32\r\n    browser_emb_size = 32\r\n    actions_emb_size = 64\r\n    \r\n    max_seq_len = 55\r\n    \r\n    #numeric Input \r\n    numerical_input = tf.keras.Input(shape=(1,), name='numeric_input')\r\n    \r\n    #categorical Input\r\n    os_input = tf.keras.Input(shape=(1,), name='os_input')\r\n    browser_input = tf.keras.Input(shape=(1,), name='browser_input')\r\n    action_input= tf.keras.Input(shape=(max_seq_len,), name='action_input')\r\n    \r\n    emb_os = tf.keras.layers.Embedding(num_unique_os, os_emb_size)(os_input) \r\n    emb_browser = tf.keras.layers.Embedding(num_unique_browser, browser_emb_size)(browser_input)\r\n    emb_actions = tf.keras.layers.Embedding(num_unique_actions, actions_emb_size)(action_input)\r\n    \r\n    actions_repr = tf.keras.layers.LSTM(300, return_sequences=True)(emb_actions)\r\n    actions_repr = tf.keras.layers.LSTM(200)(emb_actions)\r\n    \r\n    emb_os = tf.squeeze(emb_os, axis=1)\r\n    emb_browser = tf.squeeze(emb_browser, axis=1)\r\n    \r\n    activity_repr = tf.keras.layers.Concatenate()([emb_os, emb_browser, actions_repr, numerical_input])\r\n\r\n    x = tf.keras.layers.RepeatVector(n_steps)(activity_repr)\r\n    x = tf.keras.layers.LSTM(288, return_sequences=True)(x) \r\n    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\r\n\r\n    def last_layer(x):\r\n        x = tf.keras.layers.Dense(num_unique_actions-1, activation='softmax')\r\n        return x\r\n    \r\n    next_n_actions = (tf.keras.layers.Lambda(last_layer))(x)\r\n    \r\n    #next_n_actions = tf.keras.layers.Dense(num_unique_actions-1, activation='softmax')(x)\r\n    \r\n\r\n    model = tf.keras.Model(inputs=[numerical_input, os_input, browser_input, action_input], outputs = next_n_actions)\r\n    model.summary()\r\n    \r\n  \r\n    model.compile('adam', 'categorical_crossentropy', metrics=['accuracy']) \r\n    \r\n    \r\n    \r\n    history = model.fit({'numeric_input': input_num,\r\n            'os_input': input_cat1,\r\n            'browser_input': input_cat2,\r\n            'action_input': input_cat3}, target_cat, batch_size=50, epochs=150)\r\n    \r\n    tf.saved_model.simple_save(\r\n        tf.keras.backend.get_session(),\r\n        os.path.join(model_dir, '1'),\r\n        inputs={'inputs': model.input},\r\n        outputs={t.name: t for t in model.outputs})\r\n   \r\n************************************************************************************************************************************\r\n`train_instance_type='ml.m5.xlarge'\r\n#train_instance_type='local'\r\ntf_version = tf.__version__\r\n\r\ntf_estimator = TensorFlow(entry_point='train.py', \r\n                          role=role,\r\n                          instance_count=1, \r\n                          instance_type=train_instance_type,\r\n                          framework_version='1.12', \r\n                          py_version='py3',\r\n                          script_mode=True,\r\n                          hyperparameters={\r\n                              'epochs': 150,\r\n                              'batch-size': 50\r\n                          }\r\n                         )\r\n\r\ntf_estimator.fit({'training': training_input_path})`\r\n\r\n\r\n***********************************************************************************************************************************\r\n2021-03-08 21:49:43 Starting - Starting the training job...\r\n2021-03-08 21:50:07 Starting - Launching requested ML instancesProfilerReport-yyyyyyyyyyy: InProgress\r\n......\r\n2021-03-08 21:51:08 Starting - Preparing the instances for training...\r\n2021-03-08 21:51:38 Downloading - Downloading input data...\r\n2021-03-08 21:52:11 Training - Training image download completed. Training in progress.\r\n2021-03-08 21:52:11 Uploading - Uploading generated training model\r\n2021-03-08 21:52:11 Failed - Training job failed\r\n2021-03-08 21:52:02,113 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\r\n2021-03-08 21:52:02,118 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\n2021-03-08 21:52:02,559 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\n2021-03-08 21:52:02,574 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\r\n2021-03-08 21:52:02,585 sagemaker-containers INFO     Invoking user script\r\n\r\nTraining Env:\r\n\r\n{\r\n    \"additional_framework_parameters\": {},\r\n    \"channel_input_dirs\": {\r\n        \"training\": \"/opt/ml/input/data/training\"\r\n    },\r\n    \"current_host\": \"algo-1\",\r\n    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\r\n    \"hosts\": [\r\n        \"algo-1\"\r\n    ],\r\n    \"hyperparameters\": {\r\n        \"batch-size\": 50,\r\n        \"model_dir\": \"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\",\r\n        \"epochs\": 150\r\n    },\r\n    \"input_config_dir\": \"/opt/ml/input/config\",\r\n    \"input_data_config\": {\r\n        \"training\": {\r\n            \"TrainingInputMode\": \"File\",\r\n            \"S3DistributionType\": \"FullyReplicated\",\r\n            \"RecordWrapperType\": \"None\"\r\n        }\r\n    },\r\n    \"input_dir\": \"/opt/ml/input\",\r\n    \"is_master\": true,\r\n    \"job_name\": \"sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210\",\r\n    \"log_level\": 20,\r\n    \"master_hostname\": \"algo-1\",\r\n    \"model_dir\": \"/opt/ml/model\",\r\n    \"module_dir\": \"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz\",\r\n    \"module_name\": \"train\",\r\n    \"network_interface_name\": \"eth0\",\r\n    \"num_cpus\": 4,\r\n    \"num_gpus\": 0,\r\n    \"output_data_dir\": \"/opt/ml/output/data\",\r\n    \"output_dir\": \"/opt/ml/output\",\r\n    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\r\n    \"resource_config\": {\r\n        \"current_host\": \"algo-1\",\r\n        \"hosts\": [\r\n            \"algo-1\"\r\n        ],\r\n        \"network_interface_name\": \"eth0\"\r\n    },\r\n    \"user_entry_point\": \"train.py\"\r\n}\r\n\r\nEnvironment variables:\r\n\r\nSM_HOSTS=[\"algo-1\"]\r\nSM_NETWORK_INTERFACE_NAME=eth0\r\nSM_HPS={\"batch-size\":50,\"epochs\":150,\"model_dir\":\"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\"}\r\nSM_USER_ENTRY_POINT=train.py\r\nSM_FRAMEWORK_PARAMS={}\r\nSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\r\nSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\r\nSM_OUTPUT_DATA_DIR=/opt/ml/output/data\r\nSM_CHANNELS=[\"training\"]\r\nSM_CURRENT_HOST=algo-1\r\nSM_MODULE_NAME=train\r\nSM_LOG_LEVEL=20\r\nSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\r\nSM_INPUT_DIR=/opt/ml/input\r\nSM_INPUT_CONFIG_DIR=/opt/ml/input/config\r\nSM_OUTPUT_DIR=/opt/ml/output\r\nSM_NUM_CPUS=4\r\nSM_NUM_GPUS=0\r\nSM_MODEL_DIR=/opt/ml/model\r\nSM_MODULE_DIR=s3://sagemaker-eu-central-1-xxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz\r\nSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":50,\"epochs\":150,\"model_dir\":\"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\r\nSM_USER_ARGS=[\"--batch-size\",\"50\",\"--epochs\",\"150\",\"--model_dir\",\"s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\"]\r\nSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\r\nSM_CHANNEL_TRAINING=/opt/ml/input/data/training\r\nSM_HP_BATCH-SIZE=50\r\nSM_HP_MODEL_DIR=s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\r\nSM_HP_EPOCHS=150\r\nPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\r\n\r\nInvoking script with the following command:\r\n\r\n/usr/bin/python train.py --batch-size 50 --epochs 150 --model_dir s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\r\n\r\n\r\ninput_train shape: (66, 58)\r\ntarget shape: (66, 2, 209)\r\nshape input_cat1: (66,)\r\nshape input_cat2: (66,)\r\nshape input_cat3: (66, 55)\r\nshape input_num: (66,)\r\nshape target: (66, 2, 209)\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 105, in <module>\r\n    model = tf.keras.Model(inputs=[numerical_input, os_input, browser_input, action_input], outputs = next_n_actions)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 121, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\", line 80, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 474, in _method_wrapper\r\n    method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\", line 224, in _init_graph_network\r\n    '(thus holding past layer metadata). Found: ' + str(x))\r\n**ValueError: Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata). Found: Tensor(\"dense/truediv:0\", shape=(?, 2, 209), dtype=float32)**\r\n2021-03-08 21:52:04,761 sagemaker-containers ERROR    ExecuteUserScriptError:\r\nCommand \"/usr/bin/python train.py --batch-size 50 --epochs 150--model_dir s3://sagemaker-eu-central-1-xxxxxxxxxxxxxxxxx/sagemaker-tensorflow-scriptmode-2021-03-08-21-49-43-210/model\"\r\n\r\n****************************************************************************************************************************************\r\nLOCALY I GET THIS\r\n\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\nos_input (InputLayer)           [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nbrowser_input (InputLayer)      [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\naction_input (InputLayer)       [(None, 55)]         0                                            \r\n__________________________________________________________________________________________________\r\nembedding_66 (Embedding)        (None, 1, 32)        160         os_input[0][0]                   \r\n__________________________________________________________________________________________________\r\nembedding_67 (Embedding)        (None, 1, 32)        352         browser_input[0][0]              \r\n__________________________________________________________________________________________________\r\nembedding_68 (Embedding)        (None, 55, 64)       13440       action_input[0][0]               \r\n__________________________________________________________________________________________________\r\ntf.compat.v1.squeeze_16 (TFOpLa (None, 32)           0           embedding_66[0][0]               \r\n__________________________________________________________________________________________________\r\ntf.compat.v1.squeeze_17 (TFOpLa (None, 32)           0           embedding_67[0][0]               \r\n__________________________________________________________________________________________________\r\nlstm_61 (LSTM)                  (None, 200)          212000      embedding_68[0][0]               \r\n__________________________________________________________________________________________________\r\nnumeric_input (InputLayer)      [(None, 1)]          0                                            \r\n__________________________________________________________________________________________________\r\nconcatenate_36 (Concatenate)    (None, 265)          0           tf.compat.v1.squeeze_16[0][0]    \r\n                                                                 tf.compat.v1.squeeze_17[0][0]    \r\n                                                                 lstm_61[0][0]                    \r\n                                                                 numeric_input[0][0]              \r\n__________________________________________________________________________________________________\r\nrepeat_vector_19 (RepeatVector) (None, 2, 265)       0           concatenate_36[0][0]             \r\n__________________________________________________________________________________________________\r\nlstm_62 (LSTM)                  (None, 2, 288)       638208      repeat_vector_19[0][0]           \r\n__________________________________________________________________________________________________\r\ndense_31 (Dense)                (None, 2, 209)       60401       lstm_62[0][0]    \r\n\r\n                \r\n************************************************************************************************************************************\r\nTotal params: 924,561\r\nTrainable params: 924,561\r\nNon-trainable params: 0", "comments": ["@Patrick-devX,\r\nIn order to expedite the trouble-shooting process, could you please provide \r\n- the TensorFlow version you are using\r\n- a minimal code snippet to reproduce the issue\r\n- and the exact sequence of commands / steps that you executed before running into the error.\r\n\r\nThanks!\r\n", "@amahendrakar \r\n\r\n-  I used the Tensorflow versions '2.0.4' and '1.15.4'. It came out the same error. I also thought it would work with version 2 as it did with other issues. But it did not work.\r\n\r\n- I didn't quite understand what you were being asked with this point. I try to train and deploy a multi-input Keras model with AWS Sagemaker, but there seem to be some showstopper issues with the needed libraries that expect single input for Keras models.\r\n\r\n    I have **3 categorical inpu**t variables and **one numeric** variable. The target variable is also of type categorical.I have no \r\n    test or validation data. I am only interested in the training without errors.\r\n\r\n   I merged the arrays after data preparation as follows and then stored them in s3\r\n   `input_train = np.column_stack((input_cat1, input_cat2, input_num, input_cat3))\r\n\r\n   training_input_path = sage_maker_session.upload_data('data/training.npz', key_prefix=prefix + training_folder)\r\n   print(training_input_path)\r\n   s3://sagemaker-eu-central-1-xxxxxxxxxxxxx/user_tracking/training/training.npz`\r\n\r\n   After that it went on in the script **train.py** above. **Everything worked fine, when i just run the train.py file** .  It may well \r\n   be that the  problem with the multi inputs thing, so the channel problem.\r\n\r\n\r\n- Traceback (most recent call last): File \"train.py\", line 105, in **model = tf.keras.Model(inputs=[numerical_input, os_input, browser_input, action_input], outputs = next_n_actions)** \r\n\r\nhttps://gitlab.com/patricksardin08/data-science/-/tree/master/\r\n\r\nThank you !\r\n", "> * I used the Tensorflow versions '2.0.4' and '1.15.4'. It came out the same error. I also thought it would work with version 2 as it did with other issues. But it did not work.\r\n\r\n@Patrick-devX,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47652\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47652\">No</a>\n"]}, {"number": 47651, "title": "'tf.RGBToHSV' op is neither a custom op nor a flex op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow\r\n- TensorFlow version (or github SHA if from source):  2.4.1\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n2021-03-08 08:34:48.080129: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-08 08:34:48.080319: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-08 08:34:48.324839: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-03-08 08:34:48.733404: I tensorflow/core/grappler/devices.cc:78] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\r\n2021-03-08 08:34:48.733653: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-03-08 08:34:48.734625: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2808000000 Hz\r\n2021-03-08 08:34:48.735566: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: function_optimizer did nothing. time = 0.006ms.\r\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n\r\n2021-03-08 08:34:48.946989: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n2021-03-08 08:34:48.947044: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\nloc(callsite(\"model/lambda/RGBToHSV\"(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\":748:0) at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py\":3270:0 at callsite(\"../models.py\":8:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\":917:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\":1012:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\":560:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\":424:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\":1012:0 at callsite(\"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py\":135:0 at \"/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\":634:0)))))))))): error: 'tf.RGBToHSV' op is neither a custom op nor a flex op\r\nerror: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n        tf.RGBToHSV {device = \"\"}\r\nTensorflow Lite does not support the operation\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:748:0: error: 'tf.RGBToHSV' op is neither a custom op nor a flex op\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_image_ops.py:3270:0: note: called from\r\n../models.py:8:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py:917:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:560:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:424:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1012:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saving_utils.py:135:0: note: called from\r\n/home/yilin/Camera/camera-venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:634:0: note: called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n        tf.RGBToHSV {device = \"\"}\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nKeras model:\r\n```\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\ndef hsv_conversion(x):\r\n    return tf.image.rgb_to_hsv(x)\r\n\r\ndef create_model():\r\n    layer1 = keras.Input((128,64,3))\r\n    x = layers.Lambda(hsv_conversion)(layer1)\r\n    x = layers.Conv2D(16, (5, 5), activation='relu', padding='same')(x)\r\n    x = layers.MaxPooling2D((2,2),  padding='same')(x)\r\n    x = layers.Flatten()(x)\r\n    x = layers.Dense(32, activation='relu')(x)\r\n    x = layers.Dropout(0.4)(x)\r\n    output = layers.Dense(2, activation='softmax')(x)\r\n    model = keras.Model(layer1, output)\r\n    return model\r\n```", "comments": ["Will support the above op, RGBToHSV through the Select TF option. https://www.tensorflow.org/lite/guide/ops_select Thanks.", "Thanks for your fast response @abattery! Do you have an estimate when it is going to be supported through Select TF option?"]}, {"number": 47650, "title": "[ROCm] AMP selection process for AMD GPUs", "body": "Chooses to enable based on AMD GPU architecture, with current supported arches being gfx906 and gfx908. \r\nUtilizes the same Auto Mixed Precision tests as CUDA implementation. \r\n\r\n@cheshire @chsigg for review. ", "comments": ["Also adding Reed for sanity check on auto-mixed-precision stuff.", "@cheshire I had a small mistake, which I fixed. Can we rerun the checks? ", "@cheshire I diagnosed and fixed the Linux GPU failures given on commit b44d56b, tested locally. Everything should be ready. ", "@cheshire @reedwm I accidentally pushed the wrong commit. The fix for the LinuxGPU failure was to move `IsOnSupportedGPU` after `GetCudaVersion` in `auto_mixed_precision_test.cc`. That is why `auto_mixed_precision_test.cc` couldn't compile the in the previous test. This should be the last review. "]}, {"number": 47648, "title": "[tf.data] move checkpoint tests to kernel tests part 3", "body": "This PR is a continuation of #47592  and moves the checkpoint tests of:\r\n 1. parallel_interleave\r\n 2. group_by_reducer\r\n 3. group_by_window\r\n 4. ignore_errors\r\n 5. matching_files\r\n 6. optimize (no-oss) and\r\n 7. parse_example\r\n\r\ndatasets to kernel tests.\r\n\r\nTEST LOG:\r\n```\r\n//tensorflow/python/data/experimental/kernel_tests:parallel_interleave_test PASSED in 18.4s\r\n//tensorflow/python/data/experimental/kernel_tests:group_by_window_test  PASSED in 5.5s\r\n//tensorflow/python/data/experimental/kernel_tests:group_by_reducer_test PASSED in 9.2s\r\n//tensorflow/python/data/experimental/kernel_tests:ignore_errors_test    PASSED in 4.2s\r\n//tensorflow/python/data/experimental/kernel_tests:matching_files_test   PASSED in 2.9s\r\n//tensorflow/python/data/experimental/kernel_tests:parse_example_dataset_test PASSED in 4.7s\r\n```\r\nAlso, this PR is a part of the larger cleanup as discussed in point 4 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963\r\n\r\ncc: @jsimsa I am doing it in bulk as this is a migration activity.", "comments": []}, {"number": 47647, "title": "Plotting a TensorFlow Graph with dot/graphviz", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it: Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere is [`tf.keras.utils.plot_model`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model), which draws a Keras model using dot/graphviz. It's used in many tutorials/notebooks, and I've found it very useful. However, it only works for Keras models. I could not find a way to plot arbitrary TensorFlow graphs.\r\n\r\nI wanted to understand my TensorFlow graph, so [I ended up making `plot_graph`](https://gist.github.com/jameshfisher/f99ad86fc23d2ae7c856ee2f2ec89cd8). It lets you do:\r\n\r\n```python\r\ndef py_func(x):\r\n  if tf.random.uniform(()) < 0.5:\r\n    x = x*x\r\n  x = tf.cast(x, 'float32')\r\n  return 2*x + 5\r\n\r\ntf_func = tf.function(py_func)\r\ntf_concrete_func = tf_func.get_concrete_function(tf.constant(3))\r\nmy_graph = tf_concrete_func.graph\r\nplot_graph(my_graph)\r\n```\r\n\r\nThis generates this image:\r\n\r\n![graph](https://user-images.githubusercontent.com/166966/110341720-901d1300-8022-11eb-9397-c583a60cca70.png)\r\n\r\nIf there's interest, I could polish this and contribute it as a TensorFlow feature.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo, it would be a standalone additional function, e.g. `tf.graph_util.plot_graph`.\r\n\r\n**Who will benefit with this feature?**\r\n\r\n* Those wanting to understand their TensorFlow graph visually\r\n* Those wanting to understand the TensorFlow graph language through experimentation\r\n* Tutorial/notebook writers\r\n* Those not using TensorBoard\r\n\r\n**Any Other info.**\r\n\r\n* I'm aware of TensorBoard, but I haven't really used it. Is everyone using that to visualize their graphs?", "comments": ["Using TensorBoard\u2019s Graphs [dashboard](https://www.tensorflow.org/tensorboard) is recommended for examining your TensorFlow model.\r\nPerhaps we may want to identify if there are any limitations of using TensorBoard's Graphs for your use case which can make a point to add/update the feature?", "@ymodak okay, thanks. My main issue with TensorBoard is fairly fundamental - it only works interactively as a separate black box. Saving an image programmatically is more flexible - I can put images in a blog post, I can keep a history of images, I can generate images in CI, etc. That said, the TensorBoard graph dashboard seems to work for understanding my graph structure, which was my immediate use case, so I'll close this issue."]}, {"number": 47646, "title": "[ROCm] Workaround for Eigen::half bug related bug on ROCm platform", "body": "copy-pasting the PR description from @ekuznetsov139 description for the same in ROCm fork\r\n\r\n--------------\r\n\r\nThere appears to be an ABI mismatch between gcc7 and clang that, in case of functor::Pad<Eigen::half, 3>, results in the pad value being passed incorrectly (we see the correct value of 0 in pad_op.cc, which is compiled with gcc, and a random incorrect value in pad_op.h, which is compiled with clang).\r\n\r\nThis PR switches functor::Pad to taking all arguments by reference, which works around the problem.\r\n\r\nIt also adds float16 to tested data types for pad_op_test (the problem was not noticed for a long time because pad_op_test was not testing float16).\r\n\r\n---------------\r\n\r\n/cc @rmlarsen @cantonios ... this is the TF side workaround for the same issue/bug for which we filed this Eigen PR - https://gitlab.com/libeigen/eigen/-/merge_requests/422\r\n\r\n----------------\r\n\r\n/cc @cheshire @chsigg ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47646) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47646) for more info**.\n\n<!-- need_author_consent -->", "@deven-amd Can you please sign CLA. Thanks!", "Is this still needed with the recent Eigen change?", "@cantonios not technically needed if the TF Eigen pointer has been updated to pick the Eigen side fix.\r\n\r\nBut since this commit also includes a unit-test update that will fail on ROCm without the fix, I would prefer to get this PR merged.", "> @cantonios not technically needed if the TF Eigen pointer has been updated to pick the Eigen side fix.\r\n> \r\n> But since this commit also includes a unit-test update that will fail on ROCm without the fix, I would prefer to get this PR merged.\r\n\r\nThe Eigen pointer was updated last week - so I think we should be good there.  Can you remove the unnecessary changes and double-check it passes as expected?  We should merge the added test at the least.", ">  Can you remove the unnecessary changes and double-check it passes as expected? We should merge the added test at the least.\r\n\r\ndone.\r\n\r\n@cantonios please re-approve"]}, {"number": 47645, "title": "fix **highlighting** of remarks in docstrings", "body": "This was reported in issue #47642. Two closing \"**\" added.\r\n\r\nCONTRIBUTING.md says \"As every PR requires several CPU/GPU hours of CI testing, we discourage submitting PRs to fix one typo, one warning,etc. We recommend fixing the same issue at the file level at least (e.g.: fix all typos in a file, fix all compiler warning in a file, etc.)\", so I checked the rest of the file for typos and did not find any. Added two missing periods in docstrings, though.", "comments": []}, {"number": 47643, "title": "Portable optimized versions of key tflite_u op kernels", "body": " Adds portable optimized versions of key layer ops to improve usefulness of tflite(u) on\r\nplatforms where dedicated kernel libraries are not available / applicable.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/47644\r\n\r\n\r\nThe kernels are  standard C++11 (no intrinsics, inline asm etc) but take advantage\r\nof prepare-phase precomputations, and coding tweaks reduce runtime and memory overhead.\r\n\r\nSome useful additional kernel unit tests are included.\r\n\r\nKernels contributed by Infineon Technologies.  Kudos to our Intern Jakob Kruse who did the real coding with just a code-review and tips on my part.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Thanks for the PR.\r\n\r\nSome high-level discussion topics:\r\n\r\n * We have currently made an explicit decision that any optimizations (above the reference kernels) should go in target specific kernel directories (instead of having what we previously called `portable_optimized` and what this PR is calling `generic_optimized`):\r\n https://github.com/tensorflow/tensorflow/blob/aa3bd9f6de5a76c4c226548a48e448d211978e92/tensorflow/lite/micro/docs/optimized_kernel_implementations.md#L72-L89\r\n\r\n * If this is still something that you would like to contribute for your specific target, then we would encourage making a target-specific folder and following the steps outlined in our [Optimized Kernel Implementation guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/docs/optimized_kernel_implementations.md).\r\n    * Note that we recommend having the majority of the optimized implementation available as a downloadable NN library and checking in only the kernel wrappers into the TFLM codebase.\r\n    * And the changes would need to be reviewed as one kernel per PR and (as much as possible) have documented improvements on benchmarks of interest.\r\n\r\nHappy to chat more at the next SIG-micro meeting if you would like to discuss this topic in more detail.\r\n\r\nTagging @tensorflow/micro \r\n", "Hmmm.. so  if we'd called it (say) \"stock_rv32i\"  (primary in-house use) and sort-of pretended it would not be useful on (say) an ARC core  (\"if the need arises\" in-house use-case) it would be more welcome.\r\n\r\nOf course renaming the directory would be no problem at all.  Its just a bit odd to pretend the content non-portable when that is not actually the case.    \r\n\r\nI suspect we are not alone in finding a \"base\" of not-painfully-inefficient implementations of kernels helpful when bringing up support for in-house platforms or supporting platforms with limited/application-specific acceleration capabilities.\r\n\r\nLet's chat.\r\n", "@andrewstevens-infineon Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47643) for more info**.\n\n<!-- need_author_cla -->", "Closing the current PR. Based on the discussion at the SIG-micro meeting this work may be merged in the form of a optimized kernels for risc-v which then call into the portable library that is hosted separate from the TFLM codebase."]}, {"number": 47642, "title": "Typo issue in tf.keras.losses docs", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L587\r\n(BinaryCrossentropy)\r\nhttps://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L748\r\n(SparseCategoricalCrossentropy)\r\n\r\nAbove lines should be\r\n\r\n```\r\n          **Note - Using from_logits=True may be more numerically stable.**\r\n```\r\n\r\nlike\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5b5527d81085db5dadf3886a37fef7c4943ad704/tensorflow/python/keras/losses.py#L667\r\n\r\n(CategoricalCrossentropy)\r\n", "comments": ["Thank you for reporting this. I am currently preparing a PR for this, but I will go through that file in detail to spot more typos, because the [contributing guidelines](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md) say we should avoid fixing single typos because every PR causes several hours of GPU/CPU times for testing.", "@zenogantner \r\nThank you for submitting the pr,the issue will closed once the pr is merged.", "Thank you for the pr, this issue will be moved to closed status once the pr is merged.", "@Saduf2019 \r\n\r\nI checked the changed lines in the linked PR (#47645). I think this issue can be closed.\r\n\r\nBut is it right to remove those lines instead of fixing highlightings? In my experiences, those notes were useful in numerically unstable situations like the mixed-precision training.", "Not sure. @fchollet, the main author of Keras, decided to remove the lines, I assume he knows what he is doing there.", "@jeongukjae,\r\nClosing the issue as [the PR](https://github.com/tensorflow/tensorflow/pull/47645) has been merged. Thanks! "]}, {"number": 47641, "title": "Generic fast kernels for tflite(u)", "body": "Add portable optimized versions of key layer ops to improve usefulness of tflite(u) on\r\nplatforms where dedicated kernel libraries are not available / applicable.\r\n\r\nThe kernels are portable (no intrinsics etccustom instructions are used) but take advantage\r\nof prepare-phase precomputations,  andcoding tweaks reduce runtime and memory overhead.\r\n\r\nSome useful additional kernel unit tests are included.    \r\n\r\nKernels contributed by Infineon Technologies.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47641) for more info**.\n\n<!-- need_author_cla -->", "Will close by killing upstream branch due to commits from non CLA Infineon Employee."]}, {"number": 47639, "title": "[tflite] use newer xnnpack related source for M1 bazel build", "body": "with newer related code, we can build benchmark_model with xnnpack, gpu, and coreml delegates.\r\n\r\nOn M1 machines,\r\n\r\nEither\r\n```\r\nbazel-3.7.2-arm64 tensorflow/lite/tools/benchmark:benchmark_model --config macos_arm64  --macos_cpus arm64\r\n```\r\nor\r\n```\r\nbazel-4.0-arm64 tensorflow/lite/tools/benchmark:benchmark_model --macos_cpus arm64\r\n```\r\nworks.", "comments": ["# MobileNet V1 1.0 224 inference latency\r\nunit: ms\r\nhow | average latency\r\n-- | --:\r\nCPU 1xthread | 18.847\r\nCPU XNNPACK 1xthread | 29.219\r\nCPU 4xthreads | 9.087\r\nCPU XNNPACK 4xthreads| 9.079\r\nCPU + Accelerate | 5.397\r\nGPU delegate | 2.699\r\nCoreML delegate | 1.051\r\n\r\n\r\nc.f. https://github.com/tensorflow/tensorflow/pull/47605\r\n\r\nXNNPACK numbers are not better than non-XNNPACK ones? @Maratyszcza ", "@terryheo With this patch,\r\nI can build arm64 binary on a mac x86_64 machine.\r\n```\r\nbazel-3.7.2-x86_64 tensorflow/lite/tools/benchmark:benchmark_model --config macos_arm64  --macos_cpus arm64\r\n```", "@terryheo @yyoon @teijeong could you review this PR?", "@freedomtan I suspect TFLite might be calling into Accelerate on Mac. Accelerate uses AMX accelerator, which is not documented, and thus not used in XNNPACK.", "@Maratyszcza NO, those numbers are not AMX/Accelerate numbers. Accelerate is not enabled (yet) when building bazel.\r\n\r\nInception V3 numbers look more reasonable. I also updated the MobileNet V1 table with Accelerate number.\r\n\r\n## Inception V3 float from tflite hosted model\r\n\r\nhow | average latency\r\n-- | --: \r\nCPU 1xthread | 183.136 \r\nCPU XNNPACK 1xthread | 159.613\r\nCPU 2xthreads | 103.406\r\nCPU XNNPACK 2xthreads | 85.646\r\nCPU 4xthreads | 64.729\r\nCPU XNNPACK 4xthreads | 46.066\r\nCPU + Accelerate | 31.743\r\nGPU delegate | 13.449\r\nCoreML delegate | 2.834\r\n\r\n\r\nCPU + Accelerate:\r\n\r\n```\r\nbazel build //tensorflow/lite/tools/benchmark:benchmark_model --macos_cpus=arm64 --copt=-DTF_LITE_USE_CBLAS\r\n```\r\n\r\nand `-framework Accelerate` is added to benchmark_model's BUILD file.\r\n", "Using cblas from Accelerate for convolution could be enabled on M1 machines with something like the following\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/kernels/internal/BUILD b/tensorflow/lite/kernels/internal/BUILD\r\nindex d1b0505de90..7bac11d8fb6 100644\r\n--- a/tensorflow/lite/kernels/internal/BUILD\r\n+++ b/tensorflow/lite/kernels/internal/BUILD\r\n@@ -286,7 +286,10 @@ cc_library(\r\n         \"optimized/sparse_ops/fully_connected.h\",\r\n     ],\r\n     compatible_with = get_compatible_with_portable(),\r\n-    copts = tflite_copts(),\r\n+    copts = tflite_copts() + select({\r\n+        \"//tensorflow:macos_arm64\": [\"-DTF_LITE_USE_CBLAS\"],\r\n+        \"//conditions:default\": [],\r\n+    }),\r\n     deps = [\r\n         \":common\",\r\n         \":compatibility\",\r\n@@ -307,6 +310,13 @@ cc_library(\r\n         \"@gemmlowp//:fixedpoint\",\r\n         \"@ruy//ruy/profiler:instrumentation\",\r\n     ],\r\n+    linkopts = select({\r\n+        \"//tensorflow:macos_arm64\": [\r\n+            \"-framework Accelerate\",\r\n+        ],\r\n+        \"//conditions:default\": [],\r\n+    }),\r\n+\r\n )\r\n```"]}, {"number": 47638, "title": "Cannot compile tensorflow lite application when -Werror=undef is actived", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Na\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): Tensorflow lite r2.4\r\n- Python version: Na\r\n- Bazel version (if compiling from source): Na\r\n- GCC/Compiler version (if compiling from source): gcc version 9.2.0 \r\n- CUDA/cuDNN version: Na\r\n- GPU model and memory: Na\r\n\r\n**Describe the current behavior**\r\nWhen I try to compile the minimal.cc code example with the compilation flag -Werror=undef activated I have the following error :\r\n\r\ng++ -std=c++17 -pthread -Werror=undef  minimal.cc -o minimal -I./tensorflow/include -L./tensorflow/lib -ltensorflow-lite -ldl\r\nIn file included from ./tensorflow/include/tensorflow/lite/allocation.h:27,\r\n                 from ./tensorflow/include/tensorflow/lite/interpreter.h:34,\r\n                 from minimal.cc:16:\r\n./tensorflow/include/tensorflow/lite/c/common.h:87:26: error: \"\\__clang_major__\" is not defined, evaluates to 0 [-Werror=undef]\r\n  87 |     defined(HEXAGON) || (\\__clang_major__ == 7 && \\__clang_minor__ == 1) \r\n       |                                         ^~~~~~~~~~~~~~~\r\ncc1plus: some warnings being treated as errors\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nShould compile without error\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nTo reproduce just compile the code example minimal.cc with the follow command :\r\n`g++ -std=c++17 -pthread -Werror=undef  minimal.cc -o minimal -I<Path to tf-lite include> -L<Path to tf-lite lib> -ltensorflow-lite -ldl`\r\n\r\n**Other info / logs** \r\n\r\nIt looks that the variable \\__clang_major__ was not yet defined, so we found a way to solve this issue by adding (defined(\\_clang_)   in the file [common.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/common.h) :\r\n\r\nHere is the diff\r\n\r\ndiff \\--git a/tensorflow/include/tensorflow/lite/c/common.h b/tensorflow/include/tensorflow/lite/c/common.h\r\nindex 5c2f408..bdcd636 100644\r\n--- a/tensorflow/include/tensorflow/lite/c/common.h\r\n+++ b/tensorflow/include/tensorflow/lite/c/common.h\r\n@@ -84,7 +84,7 @@ typedef struct TfLiteIntArray {\r\n // https://github.com/google/re2/commit/b94b7cd42e9f02673cd748c1ac1d16db4052514c\r\n #if (!defined(\\__clang__) && defined(\\__GNUC__) && \\__GNUC__ == 6 && \\\r\n      \\__GNUC_MINOR__ >= 1) ||                                      \\\r\n\\-    defined(HEXAGON) || (\\__clang_major__ == 7 && \\__clang_minor__ == 1)\r\n\\+    defined(HEXAGON) || (defined(\\_clang_) && \\__clang_major__ == 7 && \\__clang_minor__ == 1) \r\n   int data[0];\r\n #else\r\n\r\n\r\nWith this modification the compilation finish without error\r\n\r\n", "comments": ["@terryheo could you review this suggestion?", "The change makes sense to me. Will you send a PR?", "Nevermind, I'm making a PR.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47638\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47638\">No</a>\n"]}, {"number": 47637, "title": "Why not use multiple streams for model calculations?", "body": "I see that Tensorflow uses one stream for model calculation by default, why not use multiple streams? Will there be any impact on using multiple streams?\r\nI think that for some OPs that are not dependent on models, using multiple streams will increase the parallelism of calculations, then increasing the end-to-end training speed of the model.\r\nAny advices can help me.", "comments": ["@Richie-yan \r\nCan you please share some stand alone code in support of this request.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47636, "title": "TFLite: Not able to build target riscv", "body": "\r\nHi,\r\nI am trying to build target riscv, in riscv_makefile.inc at tensorflow/tensorflow/lite/tools/make/targets using toolchain riscv32-unknown-elf- .\r\nI have built the toolchain using following sources at https://github.com/riscv/riscv-gnu-toolchain.git.\r\nI am getting following error: ./tensorflow/lite/shared_library.h:22:10: fatal error: dlfcn.h: No such file or directory.\r\nHow can I resolve this?", "comments": ["@terryheo could you take a look at this?", "I'm also getting this error (Ubuntu 20.04, recent TensorFlow master source), but I used the riscv64-unknown-elf toolchain which shouldn't make a difference. The linux_riscv64 target seems to build normally on the other hand.", "Could you try CMake instead of Makefile?\r\n\r\nI tested it with CMake it works well.\r\nI have a Riscv toolchain under $HOME/toolchains/riscv.\r\nAnd I used the following commands.\r\n```sh\r\nRISCVCC_PREFIX=$HOME/toolchains/riscv/bin/riscv64-unknown-linux-gnu-\r\ncmake -DCMAKE_C_COMPILER=${RISCVCC_PREFIX}gcc -DCMAKE_CXX_COMPILER=${RISCVCC_PREFIX}g++ -DTFLITE_ENABLE_XNNPACK=OFF ../tensorflow/lite/\r\n../tensorflow/lite/\r\nmake -j\r\n```\r\nCheck [this](https://www.tensorflow.org/lite/guide/build_cmake) for using CMake.", "> Could you try CMake instead of Makefile?\r\n> \r\n> I tested it with CMake it works well.\r\n> I have a Riscv toolchain under $HOME/toolchains/riscv.\r\n> And I used the following commands.\r\n> \r\n> ```shell\r\n> RISCVCC_PREFIX=$HOME/toolchains/riscv/bin/riscv64-unknown-linux-gnu-\r\n> cmake -DCMAKE_C_COMPILER=${RISCVCC_PREFIX}gcc -DCMAKE_CXX_COMPILER=${RISCVCC_PREFIX}g++ -DTFLITE_ENABLE_XNNPACK=OFF ../tensorflow/lite/\r\n> ../tensorflow/lite/\r\n> make -j\r\n> ```\r\n> \r\n> Check [this](https://www.tensorflow.org/lite/guide/build_cmake) for using CMake.\r\n\r\nYou have used the RISC-V Linux toolchain (`unknown-linux-gnu-`) which doesn't have the aforementioned problem even with the usual `make`. The problem in this issue relates to the RISC-V newlib compiler (`unknown-elf-`). I tried with `cmake` too and again it fails:\r\n```-- Check if compiler accepts -pthread - no\r\nCMake Error at /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:146 (message):\r\n  Could NOT find Threads (missing: Threads_FOUND)\r\nCall Stack (most recent call first):\r\n  /usr/share/cmake-3.16/Modules/FindPackageHandleStandardArgs.cmake:393 (_FPHSA_FAILURE_MESSAGE)\r\n  /usr/share/cmake-3.16/Modules/FindThreads.cmake:220 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)\r\n  ../abseil-cpp/CMakeLists.txt:82 (find_package)\r\n```\r\nSince newlib doesn't implement threads. It also doesn't implement `dlfcn.h` (no dynamic loading in bare metal), referring to the original problem, which means this header should somehow be not included in TFLite RISC-V unknown-elf build.", "I see. We haven't tested bare metal targets.\r\nSo there might be other incompatibilities.\r\n\r\nCould you test the following code for shared_library.h file?\r\n\r\n```\r\n/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n#ifndef TENSORFLOW_LITE_SHARED_LIBRARY_H_\r\n#define TENSORFLOW_LITE_SHARED_LIBRARY_H_\r\n\r\n#if defined(_WIN32)\r\n// Windows does not have dlfcn.h/dlsym, use GetProcAddress() instead.\r\n#include <windows.h>\r\n#elif defined(__APPLE__) || defined(__linux__)\r\n#include <dlfcn.h>\r\n#endif  // defined(_WIN32)\r\n\r\nnamespace tflite {\r\n\r\n// SharedLibrary provides a uniform set of APIs across different platforms to\r\n// handle dynamic library operations\r\nclass SharedLibrary {\r\n public:\r\n#if defined(_WIN32)\r\n  static inline void* LoadLibrary(const char* lib) {\r\n    return ::LoadLibrary(lib);\r\n  }\r\n  static inline void* GetLibrarySymbol(void* handle, const char* symbol) {\r\n    return reinterpret_cast<void*>(\r\n        GetProcAddress(static_cast<HMODULE>(handle), symbol));\r\n  }\r\n  // Warning: Unlike dlsym(RTLD_DEFAULT), it doesn't search the symbol from\r\n  // dependent DLLs.\r\n  static inline void* GetSymbol(const char* symbol) {\r\n    return reinterpret_cast<void*>(GetProcAddress(nullptr, symbol));\r\n  }\r\n  static inline int UnLoadLibrary(void* handle) {\r\n    return FreeLibrary(static_cast<HMODULE>(handle));\r\n  }\r\n  static inline const char* GetError() { return \"Unknown\"; }\r\n#elif defined(__APPLE__) || defined(__linux__)\r\n  static inline void* LoadLibrary(const char* lib) {\r\n    return dlopen(lib, RTLD_LAZY | RTLD_LOCAL);\r\n  }\r\n  static inline void* GetLibrarySymbol(void* handle, const char* symbol) {\r\n    return dlsym(handle, symbol);\r\n  }\r\n  static inline void* GetSymbol(const char* symbol) {\r\n    return dlsym(RTLD_DEFAULT, symbol);\r\n  }\r\n  static inline int UnLoadLibrary(void* handle) { return dlclose(handle); }\r\n  static inline const char* GetError() { return dlerror(); }\r\n#else\r\n  static inline void* LoadLibrary(const char* lib) {\r\n    return nullptr;\r\n  }\r\n  static inline void* GetLibrarySymbol(void* handle, const char* symbol) {\r\n    return nullptr;\r\n  }\r\n  static inline void* GetSymbol(const char* symbol) {\r\n    return nullptr;\r\n  }\r\n  static inline int UnLoadLibrary(void* handle) { return 0; }\r\n  static inline const char* GetError() { return nullptr; }\r\n#endif  // defined(_WIN32)\r\n};\r\n\r\n}  // namespace tflite\r\n\r\n#endif  // TENSORFLOW_LITE_SHARED_LIBRARY_H_\r\n\r\n```\r\n", "The error mentioned in OP seems to be fixed but compilation is stuck in another stage.\r\n```\r\ntensorflow_upstream/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/../internal/../profiling/instrumentation.h:77:3: error: 'pthread_mutex_t' does not name a type\r\n   77 |   pthread_mutex_t m;\r\n      |   ^~~~~~~~~~~~~~~\r\n```\r\nA dozen more similar errors related to pthread that I won't paste here.\r\nAlso some errors related to some math functions:\r\n```\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:27,\r\n                 from tensorflow/lite/kernels/activations.cc:26:\r\n./tensorflow/lite/kernels/internal/cppmath.h: In function 'T tflite::TfLiteRound(T)':\r\n./tensorflow/lite/kernels/internal/cppmath.h:36:41: error: 'round' is not a member of 'std'; did you mean 'round'?\r\n   36 | DECLARE_STD_GLOBAL_SWITCH1(TfLiteRound, round);\r\n      |                                         ^~~~~\r\n```\r\n```\r\n./tensorflow/lite/kernels/internal/cppmath.h: In function 'T tflite::TfLiteExpm1(T)':\r\n./tensorflow/lite/kernels/internal/cppmath.h:37:41: error: 'expm1' is not a member of 'std'; did you mean 'exp'?\r\n   37 | DECLARE_STD_GLOBAL_SWITCH1(TfLiteExpm1, expm1);\r\n      |                                         ^~~~~\r\n```\r\n```\r\ntensorflow_upstream/tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/MathFunctions.h: In function 'T Eigen::numext::absdiff(const T&, const T&) [with T = long double]':\r\ntensorflow_upstream/tensorflow/lite/tools/make/downloads/eigen/Eigen/src/Core/MathFunctions.h:1277:10: error: 'fabsl' was not declared in this scope; did you mean 'fabsf'?\r\n 1277 |   return fabsl(x - y);\r\n      |          ^~~~~\r\n```\r\n```\r\n/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:62:3: error: 'clock_gettime' was not declared in this scope\r\n```\r\n```\r\ndownloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:63:20: error: '::random' has not been declared; did you mean 'Eigen::internal::random'?\r\n```\r\nAnd some others.\r\nFull build log here https://pastebin.com/FLuRY38g\r\nexpm1/round are the same thing I was getting at https://github.com/tensorflow/tensorflow/issues/47622 and someone fixed them for TFLite Micro.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47636\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47636\">No</a>\n"]}, {"number": 47635, "title": "[Graph C API] Enable plugin optimizers and configs.", "body": "This is the 3rd PR(and the last one) of Graph C API, following RFC [Modular TensorFlow Graph C API](https://github.com/tensorflow/community/blob/master/rfcs/20201027-modular-tensorflow-graph-c-api.md).\r\n\r\nThis PR is mainly to add plugin optimizer and config in meta_optimizer, which includes:\r\n1. Run plugin optimizer in meta_optimizer.\r\n2. Change config if plugin optimizer is enabled, otherwise leave it as default. Print it when plugin config is conflicit with user config.\r\n3. Call `InitGraphPlugin` in `RegisterPluggableDevicePlugin`.\r\n", "comments": ["Hi @penpornk @ezhulenev, please help to have a review. Thanks very much!", "@penpornk  This PR is the 3rd Graph C API PR. Without this PR, users won't be able to use graph plugin. ", "@ezhulenev Thanks for your review! I have addressed all your comments.", "@ShengYang1 Can you please resolve conflicts? Thanks!", "> @ShengYang1 Can you please resolve conflicts? Thanks!\r\n\r\nI have already rebased, please review. Thanks!", "#45784 just got [merged](https://github.com/tensorflow/tensorflow/commit/3a3878ff2dba3169c49991552bc1981f53f10099). Could you please resolve conflicts again? Thank you!", "@penpornk Already rebased. \r\n\r\nMeanwhile I have slightly changed the logic in `RegisterPluggableDevicePlugin`. In the previous design, it will directly return error if `SE_InitPlugin` is not found. However, plugin with graph only should also be allowed, so I changed it to \"return error if neither device nor graph is found\". Suggestions are welcome. Thanks.", "So I have good news and bad news.\r\n \r\n**Good news:** This PR is merged, and the [main PluggableDevice PR](https://github.com/tensorflow/tensorflow/pull/45784) is now confirmed safe (based on last night's tests)! \ud83c\udf89 \r\n\r\n**Bad news:** There has been some issues adding the new `RewriterConfig` option (`use_plugin_optimizers`). I added it separately [here](https://github.com/tensorflow/tensorflow/commit/9b6540a862d36acd0b8f294cb2745493acdc13dc) but the change got reverted. So I made this PR use `kUsePluginOptimizers = RewriterConfig::ON;` in `metaoptimizer.cc` and removed the changes in `eager/context.py` for now. I will change them back once I can get the `RewriterConfig` change back in.", "@penpornk Thanks for your effort to get this PR merged. It's really good news\ud83c\udf89."]}, {"number": 47634, "title": "How to reproduce Bert with XLA?", "body": "Hi,  how to reproduce Bert with XLA\uff1fIs there any relevant code provided\uff1f\r\n![image](https://user-images.githubusercontent.com/33742067/110291744-33702700-8027-11eb-80ac-5ee0f7c96184.png)\r\nhttps://www.tensorflow.org/xla\r\n\r\nI run the code from https://github.com/google-research/bert and enable xla, but it seems that it can't use XLA successfully.", "comments": ["@liym27 \r\nCan you try with the hub modules and let us know, they come with respective pre-processor.\r\nplease refer to this [link](https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html?m=1) and let us know.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47633, "title": "why my 'workers=8,use_multiprocessing=True' do not work while training?", "body": "**Here is my code\uff1a**\r\ntrain_model_input=generate_arrays_from_dataframe( data.sample(frac=1) )\r\nhistory = model.fit(train_model_input,  epochs=1, verbose=1, validation_split=0.0,steps_per_epoch= math.ceil( train_row_len/256),  workers=8,use_multiprocessing=True)  \r\n\r\nI find it only 100% CPU-Util not the expect 800%.\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/49393828/110285880-e4be8f00-801e-11eb-8e83-b26b4297729e.png)\r\n\r\nAnd my GPU-Util is 0% although GPU memory is fully used\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/49393828/110285828-ce183800-801e-11eb-9605-1e4289ae39b1.png)\r\n\r\nHow could I Increase the utilization of my CPU/GPU?\r\nthank you!\r\n", "comments": ["should i read my data(pd.read_csv) inside generate_arrays_from_file(path) ?  But i find my generate_arrays_from_dataframe entered 8 times.", "I tried keras.utils.Sequence too. The GPU-Util is between 0%-22%, and CPU-Util is about 100%. **How Can I Increase the utilization of my CPU to 800%?**\r\n\r\nmy DataGenerator code is:\r\n```\r\nclass DataGenerator(keras.utils.Sequence):\r\n    def __init__(self, df, batch_size=256,):\r\n        self.batch_size = batch_size\r\n        self.dataframe = df\r\n        self.indexes = np.arange(len(self.df))\r\n......\r\n```\r\n\r\nmy training code is:\r\n```\r\ntrain_model_input = DataGenerator.DataGenerator(df.sample(frac=1)) \r\n\r\n # strategy = tf.distribute.MirroredStrategy()\r\n # train_dist_dataset = strategy.experimental_distribute_dataset(train_model_input) #I don't know how to use Generator with tf.distribute.MirroredStrategy\r\n # with strategy.scope():  \r\nmodel = MIND(user_feature_columns,item_feature_columns,dynamic_k=False,p=1,k_max=1,num_sampled=5,user_dnn_hidden_units=(64, embedding_dim))\r\nmodel.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\r\nhistory = model.fit(train_model_input, epochs=1, verbose=1, validation_split=0.0,steps_per_epoch=math.ceil( train_row_len/256), workers=8,use_multiprocessing=True) \r\n.......\r\n\r\n```\r\nAny tips will be great, `thanks!`\r\n\r\n", "I find **different pid** while **current threads are the same** :\r\n```\r\n    def __getitem__(self, idx):\r\n        batch_datas= self.datas.iloc[idx * self.batch_size:(idx + 1) * self.batch_size, ]\r\n        print('__getitem__>>>>>>>>>>>generator yielded a batch %d  PID:%d  ident:%d' % (idx, os.getpid(), threading.currentThread().ident))\r\n        X,Y = self.data_generation(batch_datas, (idx + 1) * self.batch_size - idx * self.batch_size)\r\n        return (X,Y)\r\n```\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/49393828/110453122-d34db380-8100-11eb-957c-0258b384587c.png)\r\n![\u56fe\u7247](https://user-images.githubusercontent.com/49393828/110470981-807ef680-8116-11eb-9d80-b4b568b59fcb.png)\r\n\r\nIt looks like \u2018cpu/pid **works one by one** \u2019, not works at the same time, so cpu uses only 100%-300%. \r\n", "Hi @JWenBin, to help with the debugging process please provide a colab notebook with reproducible code. Thanks!", "> \r\n> \r\n> Hi @JWenBin, to help with the debugging process please provide a colab notebook with reproducible code. Thanks!\r\n\r\nI will continue it one month later. Thanks!", "Closing this issue due to lack of activity. Please comment or reopen if you have reproducible code. Thanks!"]}, {"number": 47632, "title": "tensorflow numpy add __setitem__ hook", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.4.1\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n``` python\r\n    a = np_array_ops.ones((2,2))\r\n    a[0, 0] = 0.\r\n    self.assertAllEqual([[0.0, 1.0], [1.0, 1.0]], a)\r\n    a = np_array_ops.ones((2,2))\r\n    a[0:2, 0] = [0., 0.]\r\n    self.assertAllEqual([[0.0, 1.0], [0.0, 1.0]], a)\r\n```\r\n**Will this change the current api? How?**\r\nYes, support item assign likes numpy\r\n**Who will benefit with this feature?**\r\nDevelopers\r\n**Any Other info.**\r\nI found ```_with_index_update_helper``` in ```tensorflow/python/ops/numpy_ops/np_array_ops.py```, why not enable it?", "comments": ["@fsx950223,\r\n[tf.scatter_nd](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) works in a similar way. Please find the [working example](https://www.tensorflow.org/guide/tensor_slicing#insert_data_into_tensors). Thanks!", "Yeah, but item assign is more general and convenient.", "@fsx950223,\r\nThe topic of `Slice Assignment` has been discussed at length in #206. \r\n\r\nAlso, please refer to this [Stack Overflow Answer](https://stackoverflow.com/questions/39157723/how-to-do-slice-assignment-in-tensorflow). \r\n\r\nWith reference to the above Github and Stack Overflow Issues, can you please confirm if we can close this issue?\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47632\">No</a>\n", "> @fsx950223,\r\n> The topic of `Slice Assignment` has been discussed at length in #206.\r\n> \r\n> Also, please refer to this [Stack Overflow Answer](https://stackoverflow.com/questions/39157723/how-to-do-slice-assignment-in-tensorflow).\r\n> \r\n> With reference to the above Github and Stack Overflow Issues, can you please confirm if we can close this issue?\r\n> \r\n> Thanks!\r\n\r\nI know how to use TensorFlow API to update tensors.\r\n"]}, {"number": 47630, "title": "arm_nn_mat_mult_nt_t_s8.c:111: undefined reference to `__SXTB16_RORn", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) NA\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nCould you please help with the following?\r\nThanks a lot!\r\n\r\nCompiling the hello_world example code on the Arduino Nano BLE 33 gives the following error: \r\nError compiling for Arduino Nano BLE 33. \r\nI think it is related to the error below. Please see the partial dump pasted below. \r\narm_nn_mat_mult_nt_t_s8.c:137: more undefined references to `__SXTB16_RORn' \r\n\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\kissfft\\kiss_fft.c:378:9: warning: incompatible implicit declaration of built-in function 'memcpy'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\kissfft\\kiss_fft.c:378:9: note: include '<string.h>' or provide a declaration of 'memcpy'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\kissfft\\kiss_fft.c:378:9: warning: argument 2 null where non-null expected [-Wnonnull]\r\n         memcpy(fout,tmpbuf,sizeof(kiss_fft_cpx)*st->nfft);\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\kissfft\\kiss_fft.c:378:9: note: in a call to built-in function 'memcpy'\r\nlibraries\\Arduino_TensorFlowLite\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mat_mult_nt_t_s8.c.o: In function `arm_nn_mat_mult_nt_t_s8':\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:111: undefined reference to `__SXTB16_RORn'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:112: undefined reference to `__SXTB16_RORn'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:118: undefined reference to `__SXTB16_RORn'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:125: undefined reference to `__SXTB16_RORn'\r\nC:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:136: undefined reference to `__SXTB16_RORn'\r\nlibraries\\Arduino_TensorFlowLite\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions\\arm_nn_mat_mult_nt_t_s8.c.o:C:\\Users\\mushtaqsyed\\Documents\\Arduino\\libraries\\Arduino_TensorFlowLite\\src\\tensorflow\\lite\\micro\\tools\\make\\downloads\\cmsis\\CMSIS\\NN\\Source\\NNSupportFunctions/arm_nn_mat_mult_nt_t_s8.c:137: more undefined references to `__SXTB16_RORn' follow\r\ncollect2.exe: error: ld returned 1 exit status\r\nexit status 1\r\nError compiling for board Arduino Nano 33 BLE.\r\n\r\n", "comments": ["@mushtaqsyed \r\nThis is not a tensorflow related issue, please refer to this [link](https://forum.arduino.cc/index.php?topic=676185.0), and open this issue in relevant repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47630\">No</a>\n"]}, {"number": 47629, "title": "AttributeError when concatenating window datasets", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow installed from pip\r\n- TensorFlow version 2.4.1 (git v2.4.0-49-g85c8b2a817f)\r\n\r\n**Current behavior**\r\nConcatenating two window datasets throws the following error: `AttributeError: 'DatasetSpec' object has no attribute 'most_specific_compatible_shape'`\r\n\r\n**Expected behavior**\r\nThe code below returns a new dataset that is the concatenation of the two\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.data.Dataset.range(3).window(1).concatenate(tf.data.Dataset.range(3, 6).window(1))\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b591a02e5118c4ec4a0a35266a16c43d/47629.ipynb). Thanks!", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ef4acf846badc579a02f60defcd0806d/untitled151.ipynb)..Thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47629\">No</a>\n"]}, {"number": 47627, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError error message details are wrong.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS.\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a.\r\n- TensorFlow installed from (source or binary): Binary, using `pip install tensorflow`.\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0.\r\n- Python version: Python 3.6.9.\r\n- Bazel version (if compiling from source): N/a.\r\n- GCC/Compiler version (if compiling from source): N/a.\r\n- CUDA/cuDNN version: N/a.\r\n- GPU model and memory: N/a.\r\n\r\n**Describe the current behavior**\r\nException:\r\n```\r\n  File \"./tune.py\", line 304, in train2\r\n    keras.callbacks.EarlyStopping(monitor='Alpha', mode=\"max\", patience=1, min_delta=0.0001)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/home/max/tensorflow/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  TypeError: `generator` yielded an element that did not match the expected structure. The expected structure was (tf.float32, tf.float32), but the yielded element was (array([...], dtype=float32), array([...], dtype=float32)).\r\n```\r\n\r\n**Describe the expected behavior**\r\nException message saying `The expected structure was tf.float32[A][B][C], but the yielded element was tf.float32[D][E][F]`.\r\n\r\n**Standalone code to reproduce the issue**\r\nNo code example. Came up during hyperparameter search, the exception prevented hyperparameters from saving.\r\n\r\n", "comments": ["@max0x7ba \r\nPlease provide simple standalone code to reproduce the error reported or a colab gist.\r\nYou my refer to similar issue and let us know: [link](https://stackoverflow.com/questions/46511328/tensorflow-dataset-from-generator-fails-with-pyfunc-exception)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47627\">No</a>\n"]}, {"number": 47626, "title": "refactoring", "body": "", "comments": ["@CyangXu  This PR is in draft, any update on this? Please. Thanks!", "Sorry for the confusion. I just want to cache some changes I have made. I don't have a precise schedule to finish this PR. How should I make this change only visible to myself?", "Make the changes on your fork, don't create a pull request."]}, {"number": 47625, "title": "refactoring", "body": "", "comments": []}]