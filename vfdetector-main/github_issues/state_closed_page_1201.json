[{"number": 17149, "title": "Cannot assign a device for operation 'dense_0/dense/Tensordot/ListDiff'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n16.04.3 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\n('v1.5.0-0-g37aa430d84', '1.5.0')\r\n- **Python version**:\r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\nnot from source\r\n- **GCC/Compiler version (if compiling from source)**:\r\nnot from source\r\n- **CUDA/cuDNN version**:\r\n9.0 / 7\r\n- **GPU model and memory**:\r\nGTX 1070 8GB\r\n- **Exact command to reproduce**:\r\n  - loading model through tensorflow.contrib.predictor.from_saved_model\r\n  - loading model through tensorflow serving (built from latest source, with GPU support)\r\n\r\n### Describe the problem\r\nI'm trying to load Estimator saved using export_savedmodel.\r\nthe model is a speech to text model with CNN, ctc_loss and ctc_greedy_decoder, trained on single GTX 1070. Training run successfully and the model is also saved. but when i load it with tensorflow serving or from_saved_model i got this error:\r\n```\r\nfailed: Invalid argument: Cannot assign a device for operation 'dense_0/dense/Tensordot/ListDiff': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nRegistered kernels:\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; out_idx in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; out_idx in [DT_INT32]\r\n```\r\n\r\n### Source code / logs\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/1739607/error.txt)\r\n[source code](https://github.com/tensorflow/tensorflow/files/1739608/bug.zip) (run estimator_CNN.py)\r\n\r\n\r\n", "comments": ["what if you use tf.einsum? \r\nSimilar to issue #11619\r\n", "@bignamehyp: \r\nI'm not calling tf.tensordot explicitly in my code\r\nmaybe because of this?\r\n```python\r\n    with tf.variable_scope('dense_0', reuse=None):\r\n        flatten_layer = tf.reshape(prev_layer, tf.stack([tf.shape(prev_layer)[0], tf.shape(prev_layer)[1], -1]))\r\n        flatten_layer.set_shape([None, None, kernel_sizes[-1] * 64])\r\n        logits = tf.layers.dense(flatten_layer, n_class, activation=None)\r\n```\r\ni want to flatten last CNN layer so i can put it into Dense\r\n\r\nalso it's still weird, because it's running in training but error when loaded from saved model.", "I think the session created by SavedModelPredictor needs to have allow_soft_placement=True.  I'll reach out to the author internally, they don't seem to be on github.", "@dieka13 tf.layers.dense produces a tensordot op if its input is of rank higher than 2. see [here](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/layers/core.py#L156).", "I reached out to the author, but I received no response.", "I removed myself as I am not expert on this. ", "Same problem with `tf.contrib.slim.fully_connected`\r\n\r\nEDIT: I forgot to flatten the CNN output before feeding it to the fully connected layer, got the hint from @david-haber above.", "@dieka13 Can you try\r\n\r\n`tf.contrib.predictor.from_saved_model(saved_model_dir, config=tf.ConfigProto(allow_soft_placement=True))`", "@jamieas sorry, it's been too long. I do not longer have access to the source code.", "@dieka13 thanks for your response. Sorry that it took so long for this bug to be addressed.\r\n\r\n@rohan100jain should we close this bug as obsolete?", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17148, "title": "fix: Error on we feed float16 values into BeamSearchDecoder", "body": "I tried to feed float16 values into BeamSearchDecoder. Then `Type Error` occurred.\r\n\r\nThe cause is that:\r\n\r\n1. `_mask_probs` in beam_search_decoder calls `array_ops.one_hot()` and the `on_value` arg is `0.`.\r\n2. `one_hot` in array_ops infers the `dtype` of `on_value` from the value `0.`\r\n3.  `dtype` of `on_value` becomes `tf.float32`\r\n4. then raises `TypeError: dtype <dtype: 'float32'> of on_value does not match dtype parameter <dtype: 'float16'>`\r\n\r\nThe main cause is that `array_ops.one_hot` gets value `0.` but the `dtype` is unknown, so I convert the value `0.` into tensor that has right `dtype` before it is fed to `one_hot`.\r\n\r\n# Code that raises error\r\n\r\n```py3\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers import core as layers_core\r\n\r\nFL_TYPE = tf.float16  # error does not occur if FL_TYPE is tf.float32\r\nhidden_dim = 16\r\nvocab_size = 100\r\nbeam_width = 2\r\n\r\nwith tf.Graph().as_default():\r\n    cell = tf.nn.rnn_cell.GRUCell(hidden_dim)\r\n    embeddings = tf.Variable(tf.random_uniform(shape=[vocab_size, hidden_dim], dtype=FL_TYPE))\r\n    output_layer = layers_core.Dense(vocab_size, use_bias=False)\r\n\r\n    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n        cell=cell,\r\n        embedding=embeddings,\r\n        start_tokens=tf.ones([1], dtype=tf.int32),\r\n        end_token=99,\r\n        initial_state=tf.random_uniform(shape=[beam_width, hidden_dim], dtype=FL_TYPE),\r\n        beam_width=beam_width,\r\n        output_layer=output_layer,\r\n    )\r\n    decoder_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder=decoder,\r\n        maximum_iterations=20,\r\n         scope='generator_decode'\r\n    )\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(decoder_outputs)\r\n```\r\n\r\noutput is bellow\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-321fa872acec> in <module>()\r\n     24         decoder=decoder,\r\n     25         maximum_iterations=20,\r\n---> 26          scope='generator_decode'\r\n     27     )\r\n     28 \r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\r\n    307         ],\r\n    308         parallel_iterations=parallel_iterations,\r\n--> 309         swap_memory=swap_memory)\r\n    310 \r\n    311     final_outputs_ta = res[1]\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\r\n   2932         swap_memory=swap_memory)\r\n   2933     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2934     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2935     if maximum_iterations is not None:\r\n   2936       return result[1]\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2718       self.Enter()\r\n   2719       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2720           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2721     finally:\r\n   2722       self.Exit()\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2660         flat_sequence=vars_for_body_with_tensor_arrays)\r\n   2661     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n-> 2662     body_result = body(*packed_vars_for_body)\r\n   2663     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access\r\n   2664     if not nest.is_sequence(body_result):\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\r\n    252       \"\"\"\r\n    253       (next_outputs, decoder_state, next_inputs,\r\n--> 254        decoder_finished) = decoder.step(time, inputs, state)\r\n    255       if decoder.tracks_own_finished:\r\n    256         next_finished = decoder_finished\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in step(self, time, inputs, state, name)\r\n    497           beam_width=beam_width,\r\n    498           end_token=end_token,\r\n--> 499           length_penalty_weight=length_penalty_weight)\r\n    500 \r\n    501       finished = beam_search_state.finished\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in _beam_search_step(time, logits, next_cell_state, beam_state, batch_size, beam_width, end_token, length_penalty_weight)\r\n    539   # Final Shape: [batch_size, beam_width, vocab_size]\r\n    540   step_log_probs = nn_ops.log_softmax(logits)\r\n--> 541   step_log_probs = _mask_probs(step_log_probs, end_token, previously_finished)\r\n    542   total_probs = array_ops.expand_dims(beam_state.log_probs, 2) + step_log_probs\r\n    543 \r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in _mask_probs(probs, eos_token, finished)\r\n    723       dtype=probs.dtype,\r\n    724       on_value=0.,\r\n--> 725       off_value=probs.dtype.min)\r\n    726   finished_probs = array_ops.tile(\r\n    727       array_ops.reshape(finished_row, [1, 1, -1]),\r\n\r\n~/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py in one_hot(indices, depth, on_value, off_value, axis, dtype, name)\r\n   2337         if on_exists and on_dtype != dtype:\r\n   2338           raise TypeError(\"dtype {0} of on_value does not match \"\r\n-> 2339                           \"dtype parameter {1}\".format(on_dtype, dtype))\r\n   2340         if off_exists and off_dtype != dtype:\r\n   2341           raise TypeError(\"dtype {0} of off_value does not match \"\r\n\r\nTypeError: dtype <dtype: 'float32'> of on_value does not match dtype parameter <dtype: 'float16'>\r\n```", "comments": []}, {"number": 17147, "title": "ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory", "body": " I installed cuda 9.1 and set the path as suggested.\r\nShould i install cuda 8.0 for resolving this problem?\r\nError:\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/opt/conda/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/opt/conda/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory ", "comments": ["from my experience, yes you'll need cuda 8.0 hence the libcublas.so.8.0 error", "Issued is fixed by installing tensorflow 1.5 with Cuda 9.0 version."]}, {"number": 17146, "title": "tensorflow/contrib/lite/examples/label_image", "body": "Build it for desktop machines (tested on Ubuntu and OS X)\r\n\r\nbazel build --config opt --cxxopt=-std=c++11 //tensorflow/contrib/lite/examples/label_image:label_image\r\n\r\nI am not able to build the this example on my Ubuntu 16.04 Intel Desktop using the command above.\r\nUsing TF 1.5 and the error is related to NEON. Can I run .tflite models on Desktop?\r\n\r\nERROR: /home/ashish/tensorflow_1.5/tensorflow/contrib/lite/examples/label_image/BUILD:15:1: Linking of rule '//tensorflow/contrib/lite/examples/label_image:label_image' failed (Exit 1)\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::MatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonMatrixBatchVectorMultiplyAccumulate(float const*, int, int, float const*, int, float*, int)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProduct(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProduct(float const*, float const*, int, float*)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorVectorCwiseProductAccumulate(float const*, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorVectorCwiseProductAccumulate(float const*, float const*, int, float*)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonVectorBatchVectorCwiseProductAccumulate(float const*, int, float const*, int, float*)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::BatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int): error: undefined reference to 'tflite::tensor_utils::NeonBatchVectorBatchVectorDotProduct(float const*, float const*, int, int, float*, int)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::Sub1Vector(float const*, int, float*): error: undefined reference to 'tflite::tensor_utils::NeonSub1Vector(float const*, int, float*)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ClipVector(float const*, int, float, float*): error: undefined reference to 'tflite::tensor_utils::NeonClipVector(float const*, int, float, float*)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::VectorShiftLeft(float*, int, float): error: undefined reference to 'tflite::tensor_utils::NeonVectorShiftLeft(float*, int, float)'\r\nbazel-out/k8-opt/bin/tensorflow/contrib/lite/kernels/internal/_objs/tensor_utils/tensorflow/contrib/lite/kernels/internal/tensor_utils.o:tensor_utils.cc:function tflite::tensor_utils::ReductionSumVector(float const*, float*, int, int): error: undefined reference to 'tflite::tensor_utils::NeonReductionSumVector(float const*, float*, int, int)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/contrib/lite/examples/label_image:label_image failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 194.676s, Critical Path: 14.43s\r\nFAILED: Build did NOT complete successfully", "comments": ["My guess is that your platform does support NEON but is falling in the default category: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/kernels/internal/BUILD#L363\r\n\r\nTo test that hypothesis, could you force it to use \":neon_tensor_utils\" there?", "Thanks.... I am able to build with that trick.", "Is it possible to add this fix as a gist or something? Might be helpful for a lot of people.", "> Thanks.... I am able to build with that trick.\r\n\r\nHey How did we solve this issue?"]}, {"number": 17145, "title": "Feature Request: Add eval metrics to GANEstimator.", "body": "At the moment I can't see a way to pass in evaluation metrics (from tfgan) to the GANEstimator when I am using the tf.contrib.Learn.Experiment class. I want to be able to use the tf.contrib.gan.eval functions but can't see a way to do this. Am I missing  something or has this not been implemented?\r\nHave I written custom code - N/A\r\nOS Platform and Distribution - N/A\r\nTensorFlow installed from - N/A\r\nTensorFlow version - 1.6\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I also really like to see this feature. I'm happy to implement this after #19117 is reviewed and merged."]}, {"number": 17144, "title": "Update find_cuda_define to handle tabs.", "body": "The 4.0.4 release of NvInfer.h  uses tabs between the version number and the comments, this causes find_cuda_define to fail.", "comments": ["wrong branch."]}, {"number": 17143, "title": "error tf1.5 : tf.contrib.ffmpeg.decode_video", "body": " I use tensorflow 1.5 on ubuntu and decodevideo.\r\n\r\n`with tf.Session() as sess:\r\nsummary_writer = tf.summary.FileWriter('/home/xucl/app/tensorboard_log/keras_training')\r\nmovie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')\r\nmovie = tf.contrib.ffmpeg.decode_video(movie_bin)\r\nmovie_ev = movie.eval()`\r\n\r\nthe link of ffmpeg tensorflow\r\n[https://github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/contrib/ffmpeg](url)\r\n\r\nbut get an error\r\n\r\n`F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:401] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [0, 0, 0, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_Bi0OjG.err \u5df2\u653e\u5f03 (\u6838\u5fc3\u5df2\u8f6c\u50a8)`", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "more information about ffmpeg:\r\n\r\n$ ffmpeg\r\nffmpeg version 2.4.3-1ubuntu1~trusty6 Copyright (c) 2000-2014 the FFmpeg developers\r\n  built on Nov 22 2014 17:07:19 with gcc 4.8 (Ubuntu 4.8.2-19ubuntu1)\r\n  configuration: --prefix=/usr --extra-version='1ubuntu1~trusty6' --build-suffix=-ffmpeg --toolchain=hardened --extra-cflags= --extra-cxxflags= --libdir=/usr/lib/x86_64-linux-gnu --shlibdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --enable-shared --disable-stripping --enable-avresample --enable-avisynth --enable-fontconfig --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmodplug --enable-libmp3lame --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-librtmp --enable-libschroedinger --enable-libshine --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-opengl --enable-x11grab --enable-libxvid --enable-libx265 --enable-libdc1394 --enable-libiec61883 --enable-libzvbi --enable-libzmq --enable-frei0r --enable-libx264 --enable-libsoxr --enable-openal --enable-libopencv\r\n  libavutil      54.  7.100 / 54.  7.100\r\n  libavcodec     56.  1.100 / 56.  1.100\r\n  libavformat    56.  4.101 / 56.  4.101\r\n  libavdevice    56.  0.100 / 56.  0.100\r\n  libavfilter     5.  1.100 /  5.  1.100\r\n  libavresample   2.  1.  0 /  2.  1.  0\r\n  libswscale      3.  0.100 /  3.  0.100\r\n  libswresample   1.  1.100 /  1.  1.100\r\n  libpostproc    53.  0.100 / 53.  0.100\r\nHyper fast Audio and Video encoder\r\nusage: ffmpeg [options] [[infile options] -i infile]... {[outfile options] outfile}...\r\n\r\n", "I have get answer at issue pull request #13242\r\nit is a bug in tf1.5, and the code was modified after tf1.5 release.\r\nso, I will try ffmpeg.decode_video when tensorflow1.6 release.", "> I have get answer at issue pull request #13242\r\n> it is a bug in tf1.5, and the code was modified after tf1.5 release.\r\n> so, I will try ffmpeg.decode_video when tensorflow1.6 release.\r\n\r\nAny updates? I still run into this error with tf.1.12.0"]}, {"number": 17142, "title": "fix typo", "body": "", "comments": []}, {"number": 17141, "title": "Branch 186214551", "body": "Manually merged, mostly formatting changes.\r\n\r\nConflicts:\r\n\tRELEASE.md\r\n\tconfigure.py\r\n\ttensorflow/contrib/cmake/external/zlib.cmake\r\n\ttensorflow/contrib/cmake/python_modules.txt\r\n\ttensorflow/contrib/cmake/tests/cuda/compatibility_test.c\r\n\ttensorflow/contrib/cmake/tests/cuda/compatibility_test.cc\r\n\ttensorflow/contrib/data/python/ops/dataset_ops.py\r\n\ttensorflow/contrib/gan/python/eval/python/summaries_test.py\r\n\ttensorflow/contrib/layers/python/layers/layers.py\r\n\ttensorflow/contrib/layers/python/layers/layers_test.py\r\n\ttensorflow/contrib/tpu/profiler/pip_package/setup.py\r\n\ttensorflow/core/public/version.h\r\n\ttensorflow/docs_src/install/install_c.md\r\n\ttensorflow/docs_src/install/install_go.md\r\n\ttensorflow/docs_src/install/install_java.md\r\n\ttensorflow/docs_src/install/install_linux.md\r\n\ttensorflow/docs_src/install/install_mac.md\r\n\ttensorflow/docs_src/install/install_sources.md\r\n\ttensorflow/examples/image_retraining/retrain.py\r\n\ttensorflow/python/framework/test_util.py\r\n\ttensorflow/python/keras/_impl/keras/layers/lstm_test.py\r\n\ttensorflow/python/layers/utils.py\r\n\ttensorflow/python/ops/bitwise_ops_test.py\r\n\ttensorflow/python/ops/distributions/beta.py\r\n\ttensorflow/python/ops/image_ops_test.py\r\n\ttensorflow/python/ops/losses/losses_impl.py\r\n\ttensorflow/tools/pip_package/setup.py", "comments": []}, {"number": 17140, "title": "Update to mobile intro documentation", "body": "I found a grammatical/spelling error while reading this documentation.  ", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17139, "title": "Fix typos in Operation Semantics docs", "body": "Fix MathJax beta display.\r\nUpdate '( assuming' to '(assuming'.\r\nUpdate 'in a the first' to 'in the first'.\r\nChange 'nop' to 'no-op' to match other occurrences.\r\nMisc other minor updates (periods, hyphen, etc).", "comments": ["Thank you for reviewing, @MarkDaoust "]}, {"number": 17138, "title": "Windows installation page lists wrong cudnn version", "body": "The windows installation pages specifically asks to use cuDNN 6 - [link](https://www.tensorflow.org/install/install_windows). Then, when running tensorflow it looks specifically for cuDNN7.\r\n\r\n    % (build_info.cudnn_dll_name, build_info.cudnn_version_number))\r\nImportError: Could not find 'cudnn64_7.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Note that installing cuDNN is a separate step from installing CUDA, and this DLL is often found in a different directory from the CUDA DLLs. You may install the necessary DLL by downloading cuDNN 7 from this URL: https://developer.nvidia.com/cudnn\r\n\r\nWould be great if the page modified this to specifically ask for cuDNN 7.\r\n\r\n", "comments": ["Thanks for the note.\r\n\r\n@gunan @av8ramit - does the Windows page need to be updated?", "FYI @markdaoust\r\nI thought we updated the website with the correct information, could you confirm?", "Thanks for the reminder!\r\n\r\nThis is fixed now: https://www.tensorflow.org/install/install_windows"]}, {"number": 17137, "title": "Wrong Bazel version check when building from source", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.10.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 8.0 / 7.0\r\n- **GPU model and memory**: GeForce GTX 970, 4GB VRAM\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nI am trying to build TensorFLow 1.5 with Bazel 0.10.1 from the Google apt repo ([`http://storage.googleapis.com/bazel-apt`](http://storage.googleapis.com/bazel-apt)).\r\nThe build immediately fails with a misleading error message:\r\n`Current Bazel version is 0.10.1, expected at least 0.5.4`\r\nbecause the build script is comparing strings instead of integers of the version numbers.\r\n\r\nBacktrace:\r\n```\r\n$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /home/christian/Development/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):\r\n\tFile \"/home/christian/Development/tensorflow/WORKSPACE\", line 15\r\n\t\tclosure_repositories()\r\n\tFile \"/home/christian/.cache/bazel/_bazel_christian/d6e111ab803bbbeb04cac5fc3b321976/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n\t\t_check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n\tFile \"/home/christian/.cache/bazel/_bazel_christian/d6e111ab803bbbeb04cac5fc3b321976/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n\t\tfail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.1\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: /home/christian/Development/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\n\tFile \"/home/christian/Development/tensorflow/WORKSPACE\", line 41\r\n\t\ttf_workspace()\r\n\tFile \"/home/christian/Development/tensorflow/tensorflow/workspace.bzl\", line 48, in tf_workspace\r\n\t\tcheck_version(\"0.5.4\")\r\n\tFile \"/home/christian/Development/tensorflow/tensorflow/workspace.bzl\", line 38, in check_version\r\n\t\tfail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.10.1, expected at least 0.5.4\r\nERROR: Error evaluating WORKSPACE file\r\n\r\n```", "comments": ["Yes, version check uses alphabetical order wrongly before 1.6, and I believe the bug has been fixed in the master branch. ", "Had the same problem, this fixed it. Thanks!", "Can this fix be backported to 1.5 so that people can build a stable version from source?", "@christian-rauch I rechecked the r1.5 branch, and found that 8ac09f8c patch, fortunately, has been merged since 13 days ago, see:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5b10b3474bea72e29875264bb34be476e187039c/tensorflow/workspace.bzl#L45\r\n\r\nSo, could you pull the latest codes on r1.5 branch, and test it again? Thank you.", "The `r1.5` branch works. I previously checked out the `v1.5.0` tag, in the assumptions that both point to the same commit.", "Good news! Could you close the issue since it has been fixed ? ", "Yes. But maybe the current `r1.5` branch could be tagged, such that the next person that tries to build `v1.5.0` doesn't need to dig into the same issue."]}, {"number": 17135, "title": "Inconsistency on the Dataset document", "body": "### System information\r\nN/A\r\n\r\n### Describe the problem\r\nThere are some inconsistencies in the document about Dataset.\r\n\r\n#### Problem 1\r\nIn https://www.tensorflow.org/get_started/premade_estimators, i.e. the link to the most recent stable release (v1.5 for now), in \"Getting the sample code\" section, there is something that reads:\r\n\r\n> The program described in this document is premade_estimator.py. This program uses iris_data.py To fetch its training data.\r\n\r\nThe links of `premade_estimator.py` and `iris_data.py` in the above sentence are to the master branch, instead of the branch corresponding to the correct version. It is not a good idea to point to the master branch because:\r\n\r\nIf you follow the link of  `iris_data.py`, you can see https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py#L39\r\n`return dataset` which is not the same as the `return dataset.make_one_shot_iterator().get_next()` in the document. This confuses the users.\r\n\r\nTo fix this, I would suggest to change the script that generates the web page to automatically generate links pointing to the correct github tag.\r\n\r\n#### Problem 2\r\nIn https://www.tensorflow.org/versions/master/get_started/premade_estimators\r\ni.e. the \"Getting Started with TensorFlow\" for the master branch,  the example code has already been changed to return a `Dataset`, which is consistent with https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py#L39\r\nThis is good here.\r\n\r\nBut in https://www.tensorflow.org/versions/master/get_started/datasets_quickstart\r\nthe sample code still returns `dataset.make_one_shot_iterator().get_next()`\r\n\r\nI would suggest making it consistent by returning a `Dataset` in the above code.  And if needed, consider adding something like:\r\n> Since version 1.5, `Estimator` now supports `Dataset: input_fn` to return a `Dataset` instead of `Tensors`. So `return dataset.make_one_shot_iterator().get_next()` can be simplified to `return dataset`\r\n\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "............\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "iris_data.py needs an update. @nealwu please take a look. Thanks", "Re-assigning to @MarkDaoust for the samples/ folder.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 77 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 93 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 124 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @MarkDaoust: It has been 139 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A chunk of this is fixed.\r\n\r\nThe \"pointing to the correctly versioned branch\" will be fixed as part of the migration of tensorflow docs to their own repo (coming soon)."]}, {"number": 17134, "title": "TFTS: Enhancements to timeseries head", "body": "* Moved loss creation to `create_loss` that satisfies `_Head`\r\n* Added namescope using `self._name`\r\n* Added loss summary identifiable reusing `head_lib._summary_key`\r\n* Made `logits_dimension` always be 1 (not used anyways) just to satisfy `_Head`", "comments": ["@allenlavoie Thanks! I just fixed them as well as the BUILD file. ", "Test failure is due to one flaky keras test (unrelated to this PR):\r\n```\r\nFAIL: test_stateful_metrics (__main__.KerasMetricsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/bazel_pip/tensorflow/python/keras/metrics_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/keras/_impl/keras/metrics_test.py\", line 144, in test_stateful_metrics\r\n    self.assertAllClose(outs[2], ref_true_pos(y, preds), atol=1e-5)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 1190, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 1162, in _assertAllCloseRecursive\r\n    path_str))\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/tensorflow/python/framework/test_util.py\", line 1102, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/numpy/testing/nose_tools/utils.py\", line 1396, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/tmpfs/src/github/tensorflow/tf_build_env/lib/python3.5/site-packages/numpy/testing/nose_tools/utils.py\", line 779, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-05\r\nMismatched value: a is different from b.\r\n(mismatch 100.0%)\r\n x: array(416, dtype=int32)\r\n y: array(414)\r\n```", "@allenlavoie CI passed now!"]}, {"number": 17133, "title": "Documentation for LSTMStateTuple is misleading", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **TensorFlow installed from (source or binary)**: no\r\n- **TensorFlow version (use command below)**: 1.5\r\n\r\nThe [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMStateTuple) for `LSTMStateTuple` states that\r\n\r\n> Stores two elements: `(c, h)`, in that order. Where `c` is the hidden state and `h` is the output.\r\n\r\nThe property naming is very confusing. It suggests that `h` is the `h`idden state, but the documentation contradicts this intuition.", "comments": ["The naming is a little confusing, however it keeps consistent with its original paper: http://arxiv.org/abs/1409.2329\r\nAnd c is short for \"memory cell\". ", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17132, "title": "Flag names no longer compatible with argparse", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0\r\n- **Python version**: 3.6.3\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.app.flags.DEFINE_string(\r\n    name='flag-name',\r\n    default=None,\r\n    help='Flag description.'\r\n)\r\nprint(tf.app.flags.FLAGS.flag_name)\r\n```\r\n\r\n### Describe the problem\r\nUp to version 1.4.1 you would be able to access the flag defined as \"flag-name\" via `FLAGS.flag_name` just like in [argparse](https://docs.python.org/3/library/argparse.html). Since 1.5.0 this functionality is no longer available.\r\n\r\n### Source code / logs\r\nHere's the traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"flagtest.py\", line 15, in <module>\r\n    print(tf.app.flags.FLAGS.flag_name)\r\n  File \"/code/venv3/lib/python3.6/site-packages/tensorflow/python/platform/flags.py\", line 85, in __getattr__\r\n    return wrapped.__getattr__(name)\r\n  File \"/code/venv3/lib/python3.6/site-packages/absl/flags/_flagvalues.py\", line 470, in __getattr__\r\n    raise AttributeError(name)\r\nAttributeError: flag_name\r\n```\r\n\r\nSince up to version 1.4.1 this was the default functionality, bringing this back should not affect many projects.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I am getting the same error on Windows10 when running model_test.py in deeplab model\r\nTensorflow 1.6\r\nCUDA version 9.0      cudnn64_7.dll\r\nGPU GeForce GTX 1070 ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 57 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17131, "title": "Update TF Lite android demo to Android Studio 3", "body": "Upgrade gradle settings to Android Studio 3.\r\nAlso fix githignore, otherwise included files from the build folder.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17130, "title": "Java: SIGSEGV when `Tensors.create`-ing from an uninitialized array", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\n  macOS 10.13.3\r\n  JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)\r\n- **TensorFlow installed from (source or binary)**: maven\r\n```xml\r\n    <dependency>\r\n      <groupId>org.tensorflow</groupId>\r\n      <artifactId>tensorflow</artifactId>\r\n      <version>1.4.0</version>\r\n    </dependency>\r\n```\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```java\r\n    // JUnit test case\r\n    public void testSigSegv() {\r\n        byte[][] bb = new byte[3][];  // note: new byte[3][1] doesn't crash, presumably it is initialized by the compile\r\n\r\n        bb[0] = new byte[] { 0 };\r\n        bb[1] = new byte[] { 1 };\r\n        // no bb[2]\r\n\r\n        // next line sigsegv's\r\n        Tensors.create(bb);\r\n    }\r\n```\r\n\r\n### Describe the problem\r\n\r\nUsing `Tensors.create` to get a `Tensor<String>` from an uninitialized 2D `byte` array (`byte[][]`) results in a `SIGSEGV` from JNI. See full log below.\r\n\r\n### Source code / logs\r\n\r\nWhen running the above test case, I get the following output.\r\n```\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x000000010571f0b3, pid=30179, tid=0x0000000000001a03\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode bsd-amd64 compressed oops)\r\n# Problematic frame:\r\n# V  [libjvm.dylib+0x31f0b3]\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /Users/ben/AGLabs/nfl/JavaProjects/nlp/hs_err_pid30179.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n#\r\n\r\nProcess finished with exit code 134 (interrupted by signal 6: SIGABRT)\r\n```\r\n\r\nI'm not familiar enough with JNI in general to know if I'm expecting too much. I won't be surprised if you mark this `wontfix`, I was just surprised to be able to crash the process with a segfault given some bad data. Easy enough to work around (I will size/init my arrays more carefully) but thought you might want to know.\r\n\r\nCheers! Thanks so much for this library!\r\n", "comments": ["Thanks very much for the report! Yes, this is indeed a bug (it should **not** be possible to cause the JVM to segfault when using the Java API :).\r\n\r\nWill send a fix shortly."]}, {"number": 17129, "title": "Error when training custom object with tensorflow.", "body": "###System Information\r\nHave I written custom code(No)\r\nOS Platform and Distribution(Linux 16.04 LTS)\r\nTensorFlow installed from(python3 package)\r\nTensorFlow version(1.5.0)\r\nCPU(Intel Core i5-3320M)\r\n\r\n\r\n\r\nI am trying to train my custom object by using tensorflow, however, when I run command it shows an error with protobuf:\r\n\r\n**File \"/usr/local/lib/python3.5/dist-packages/google/protobuf/text_format.py\", line 1152, in ConsumeIdentifierOrNumber raise self.ParseError('Expected identifier or number, got %s.' % result) \r\n google.protobuf.text_format.ParseError: 235:1 : Expected identifier or number, got <.**\r\n\r\nI am using following command to run train.py:\r\n\r\n**python3 train.py --logtostderr --train_dir=training/--pipeline_config_path=training/ssd_mobilenet_v1_pets.config**\r\n\r\nAlso I am attaching screenshot with the error.\r\n\r\n![51eur](https://user-images.githubusercontent.com/22894915/36381522-be5f4336-157d-11e8-957b-002758acca77.jpg)\r\n\r\nI am beginner in this topic, so I would be grateful for any help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I met the same problem,have you solved it?", "I faced the same issue and was able to resolve it by correcting my **labelmap.pbtxt**. \r\nFor custom model, you will need to write your own **labelmap.pbtxt**. I am pasting my example below for 1 class - _reading_\r\n\r\n`item {\r\n  id: 1\r\n  name: 'reading'\r\n}\r\n`", "I face with the same issue. Any idea how to solve it?", "I faced the same problem, to solve it I had to take the format of mscoco_label_map.pbtxt which is different\r\nitem {\r\n  name: \"/m/01g317\"\r\n  id: 1\r\n  display_name: \"person\"\r\n}", "you can file the above mentioned file \"mscoco_lab_map.txt in models/research/object_detection/data"]}, {"number": 17128, "title": "How can I get kernel wait time for GPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr1.5\r\n- **Python version**: \r\n2\r\n- **Bazel version (if compiling from source)**:\r\n0.5.4\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0\r\n- **GPU model and memory**:\r\nTitan XP, 12GB\r\n- **Exact command to reproduce**:\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI've tested the code to get kernel queue wait time for GPU, but GPU kernel time is constants even though the number of operations is increased. Instead, the CPU kernel time is changed like below.\r\nCan I get the information to analyze the results? Moreover, profiling of kernel queue wait time would be useful. Do you have any plan to add that kind of features?\r\n\r\n#ops| GPU kernel time(us)| CPU kernel time (us)\r\n50 | 1862 | 184\r\n100 | 1746 | 181\r\n150 |  1750 | 167\r\n200 | 1750 | 173\r\n250 | 1751 | 307\r\n300 | 1756 | 551\r\n350 | 1757 | 721\r\n400 | 1767 | 859\r\n450 | 1767 | 959\r\n\r\n \r\n\r\n### Source code / logs\r\n    ```\r\n    opts = tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY\r\n    opts['min_accelerator_micros'] = 1\r\n    run_metadata = tf.RunMetadata()\r\n\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    #config.allow_soft_placement = True\r\n    config.inter_op_parallelism_threads=0\r\n    config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n    with tf.Session(config=config) as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      #sess.run(conv_ops)\r\n      #print('first itereation ends')\r\n      #sess.run(conv_ops)\r\n      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n\r\n      sess.run(conv_ops, options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata)\r\n\r\n    root_node = tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n                  tf.get_default_graph(),\r\n                  run_meta=run_metadata,\r\n                  tfprof_options=opts)\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17127, "title": "Write to tensor array fails within while loop", "body": "\r\n\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n Yes I am writing a while loop that reads and also updates the tensor array that's being passed to it.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nWhen I attempt to write to a tensor array within a while loop, it leads me to an error \r\n\"Could not write to TensorArray index 0 because it has already been read.\"\r\n\r\n### Source code \r\n\r\n\r\n```\r\ndef tf_soft_nms(boxes,scores,nms_t,dt_t):\r\n\r\n  sigma = tf.constant(0.3)\r\n  y1 = tf.gather(boxes,0,axis=1)\r\n  x1 = tf.gather(boxes,1,axis=1)\r\n  y2 = tf.gather(boxes,2,axis=1)\r\n  x2 = tf.gather(boxes,3,axis=1)\r\n  area = tf.multiply((y2-y1) , (x2-x1))\r\n\r\n\r\n  picked_ixs = tf.TensorArray(tf.int32,clear_after_read=False,size=0,dynamic_size=True)\r\n  scores_ta = tf.TensorArray(scores.dtype,clear_after_read=False,size=scores.shape[0],colocate_with_first_write_call=False)\r\n  scores_ta = scores_ta.unstack(scores)\r\n  loop_vars = (picked_ixs,scores_ta,nms_t,dt_t)\r\n  score_p = tf.Print(scores,[scores],\"all_Scores\")\r\n\r\n  def cond(picked_ixs,scores,nms_t,dt_t):\r\n    scores_tf = scores.stack()\r\n    valid_indexes = tf.greater(scores_tf,dt_t)\r\n    valid = tf.Print(valid_indexes,[valid_indexes],\"Conditional check\")\r\n    #scores = scores.write(0,tf.constant(0.0))\r\n    return tf.reduce_any(valid_indexes)\r\n\r\n  def nms_iter(picked_ixs,scores_ta,nms_t,dt_t):\r\n    global area\r\n    global boxes\r\n    #sort scores pick top score\r\n    scores_tf = scores_ip.stack()\r\n    scores_ixs = tf.nn.top_k(scores_tf,tf.shape(scores_tf)[0]).indices\r\n    pick = scores_ixs[0]\r\n\r\n\r\n    #write top score to picked\r\n    write_index = picked_ixs.size()\r\n    picked_ixs = picked_ixs.write(write_index,pick)\r\n    scores_ta = scores_ta.write(pick,tf.constant(0.2))\r\n    picked_tf = picked_ixs.stack()\r\n     return picked_ixs,scores_ta,nms_t,dt_t\r\n\r\n```\r\n\r\n", "comments": ["Adding the stack trace,\r\n\r\n> Caused by op 'while/TensorArrayWrite_1/TensorArrayWriteV3', defined at:\r\n>   File \"nms.py\", line 175, in <module>\r\n>     test_pynms()\r\n>   File \"nms.py\", line 171, in test_pynms\r\n>     picked_tf = tf_soft_nms(boxes_tf,scores_tf,nms_t_tf,dt_tf)\r\n>   File \"nms.py\", line 107, in tf_soft_nms\r\n>     picked = tf.while_loop(cond,nms_iter,loop_vars,back_prop=False,parallel_iterations=1)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2816, in while_loop\r\n>     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2640, in BuildLoop\r\n>     pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2590, in _BuildLoop\r\n>     body_result = body(*packed_vars_for_body)\r\n>   File \"nms.py\", line 82, in nms_iter\r\n>     scores_ta = scores_ta.write(pick,tf.constant(0.2))\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\r\n>     return _add_should_use_warning(fn(*args, **kwargs))\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 310, in write\r\n>     name=name)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 5038, in _tensor_array_write_v3\r\n>     flow_in=flow_in, name=name)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n>     op_def=op_def)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n>     op_def=op_def)\r\n>   File \"/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n>     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n> \r\n> InvalidArgumentError (see above for traceback): TensorArray TensorArray_1_1: Could not write to TensorArray index 0 because it has already been read.\r\n> \t [[Node: while/TensorArrayWrite_1/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](while/TensorArrayStack/TensorArraySizeV3/Enter, while/strided_slice_1, while/Const_1, while/Switch_1:1)]]\r\n> ", "Please clarify where while_loop is used.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Yes. I am experiencing exactly the same problem. I can write into the TensorArray while I am trying to write values into it in the tf.while_loop could you check this bug again? Maybe it's realted to CUDA 10.0. It would be grateful if op posts his environments.", "> Yes. I am experiencing exactly the same problem. I can write into the TensorArray while I am trying to write values into it in the tf.while_loop could you check this bug again? Maybe it's realted to CUDA 10.0. It would be grateful if op posts his environments.\r\n\r\nSame problem happens under CUDA 9 too with TF 1.12"]}, {"number": 17126, "title": "Cleaner documentation for tf.confusion_matrix", "body": "Also use `tf.stack` with `axis=1` instead of a stack + transpose.", "comments": []}, {"number": 17125, "title": "Remove extraneous check for Eager mode", "body": "The check is already made once at the start of the method", "comments": []}, {"number": 17124, "title": "tf serving - dependency_optimizer - Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree", "body": "I am trying to serve a tensorflow/nmt model with tf serving. I copied my SavedModel into the docker image and I'm serving it using:\r\n\r\n`bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=model_test --model_base_path=/serving/SavedModel &> model_test.log &`\r\n\r\nHere the logs: \r\n\r\n```\r\nroot@8de7a26d2e65:/serving# cat model_test.log\r\n2018-02-19 10:07:01.227129: I tensorflow_serving/model_servers/main.cc:153] Building single TensorFlow model file config:  model_name: rxn_test model_base_path: /serving/SavedModel\r\n2018-02-19 10:07:01.228912: I tensorflow_serving/model_servers/server_core.cc:444] Adding/updating models.\r\n2018-02-19 10:07:01.228996: I tensorflow_serving/model_servers/server_core.cc:499]  (Re-)adding model: rxn_test\r\n2018-02-19 10:07:01.330004: I tensorflow_serving/core/basic_manager.cc:716] Successfully reserved resources to load servable {name: rxn_test version: 1}\r\n2018-02-19 10:07:01.330050: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: rxn_test version: 1}\r\n2018-02-19 10:07:01.330072: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: rxn_test version: 1}\r\n2018-02-19 10:07:01.330097: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:360] Attempting to load native SavedModelBundle in bundle-shim from: /serving/SavedModel/1\r\n2018-02-19 10:07:01.330124: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:240] Loading SavedModel with tags: { serve }; from: /serving/SavedModel/1\r\n2018-02-19 10:07:01.338657: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-02-19 10:07:01.371196: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:159] Restoring SavedModel bundle.\r\n2018-02-19 10:07:01.641102: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:194] Running LegacyInitOp on SavedModel bundle.\r\n2018-02-19 10:07:01.659991: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: success. Took 329854 microseconds.\r\n2018-02-19 10:07:01.660289: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: rxn_test version: 1}\r\n2018-02-19 10:07:01.664151: I tensorflow_serving/model_servers/main.cc:315] **Running ModelServer at 0.0.0.0:9000 ...** <--- it runs \r\n```\r\n\r\nWhen I try to predict something, using predict.py:\r\n\r\n`python predict.py --server=localhost:9000 --inputs='input string' `\r\n\r\n it fails with following error: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 193, in _blocking_unary_unary\r\n    credentials=_credentials(protocol_options))\r\n  File \"/envs/tf35/lib/python3.5/site-packages/grpc/_channel.py\", line 487, in __call__\r\n    return _end_unary_response_blocking(state, call, False, deadline)\r\n  File \"/envs/tf35/lib/python3.5/site-packages/grpc/_channel.py\", line 437, in _end_unary_response_blocking\r\n    raise _Rendezvous(state, None, None, deadline)\r\ngrpc._channel._Rendezvous: <_Rendezvous of RPC that terminated with (StatusCode.INVALID_ARGUMENT, Incomplete graph, missing 1 inputs for dynamic_seq2seq/decoder/decoder/GatherTree)>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 50, in <module>\r\n    tf.app.run()\r\n  File \"/envs/tf35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"predict.py\", line 45, in main\r\n    result = stub.Predict(request, 60.0)  # 60 secs timeout\r\n  File \"/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 309, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/envs/tf35/lib/python3.5/site-packages/grpc/beta/_client_adaptations.py\", line 195, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"Incomplete graph, missing 1 inputs for dynamic_seq2seq/decoder/decoder/GatherTree\")\r\n```\r\n\r\nAnd in the model_test.log\r\n```\r\n2018-02-19 10:07:16.628894: E external/org_tensorflow/tensorflow/core/grappler/optimizers/dependency_optimizer.cc:584] Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree\r\n2018-02-19 10:07:16.638890: E external/org_tensorflow/tensorflow/core/grappler/optimizers/dependency_optimizer.cc:584] Non-existent input  for node dynamic_seq2seq/decoder/decoder/GatherTree\r\n```\r\n\r\nDid anyone experience this?\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I am using the following docker file to build the container:\r\nhttps://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile.devel\r\n\r\nThe error I described above was probably because I had exported the SavedModel with tensorflow 1.4.1 and it has some incompatibilities with tf1.5, which I assume is used by the most recent version of tensorflow/serving. \r\n\r\nIf I now export the model with tf1.5, I get another error, but this time an expected one (because so far I didn't find a way to initialise a dataset iterator with a feed_dict in tf/serving):\r\n\r\n`2018-02-20 07:48:40.409613: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1202] OP_REQUIRES failed at iterator_ops.cc:844 : Failed precondition: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element.`\r\n\r\nI will close this issue. Thanks!"]}, {"number": 17123, "title": "Add broadcasting functionality for Div and Sub ops.", "body": "Hi,\r\nAdded broadcasting functionality for Div and Sub ops following the examples of Add and Mul.\r\nRegards,\r\nHovhannes", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I fixed conflicts and committed them, so please remove \"stalled\" label", "Hi,\r\nIs there anything that I should do to make this pull request be merged.\r\nRegards,\r\nHovhannes", "Can you look at the test failures?\r\n```\r\nIn file included from tensorflow/contrib/lite/kernels/pooling.cc:26:\r\n./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:1415:6: error: redefinition of 'BroadcastDiv'\r\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\r\n     ^\r\n./tensorflow/contrib/lite/kernels/internal/reference/reference_ops.h:1347:6: note: previous definition is here\r\nvoid BroadcastDiv(const T* input1_data, const Dims<4>& input1_dims,\r\n```", "Thanks jhseu for help."]}, {"number": 17122, "title": "Update convolutional.py", "body": "Adding tf_export decorators/calls to TensorFlow functions and constants.", "comments": ["What is this used for?", "Is it useful to publish this as a hidden class?", "yup", "I'm not sure we would want to allow this. We make the API specifically to call out what you can rely on and what is stable. Anything that it hidden is by definition not part of the API. You can of course always access it directly, but I don't think it's a good idea to expose it into the TensorFlow module. ", "Hmm but it was not hidden right and why do you think it shouldn't be exposed to API ?\r\n", "It is a private class, if you use it you should be very aware that you are relying on an internal implementation detail. \r\n\r\nYou can always use this (this is Python, after all), you just have to explicitly import it from the file it's defined in.", "hmmm ok"]}, {"number": 17121, "title": "Feature Request for the back-propagated errors in intermediate layers", "body": "After the forward procedure, one loss and one error were generated for the batch data. Then according to the chain rule ,the error was back-propagated to the previous layers to update the parameters in each layer. Suppose I have the following network architecture:\r\n\r\n     I->(W1)->C1->(W2)->C2->(W3)->O\r\n\r\n`I` is the input, `O` is the output, `W1,W2,W3` is the weights for 3 layers. `C1` and `C2` are the outputs for the first two layers. With `O` and the ground truth, we obtain the loss and the error which will be back-propagated. My question is: In TensorFlow, are there any methods to get the errors back-propagated to `C1` and `C2`?  \r\n\r\nI know we could get the parameter operators as follows:\r\n\r\n    W1_op = tf.get_default_graph().get_tensor_by_name('W1')\r\n    W1_op = ...\r\n\r\nMy final purpose is to check if the errors are right in my network because I cannot check if the gradient in some certain layer (a new user-defined op) of this network is computed correctly. I want to check its gradient by checking the errors before and after this layer (by viewing the errors and comparing the errors).\r\n\r\nI know that we could use the `tf.test.check_gradient` to do gradient check, but it seems the output for gradient check of this new operator depends on the inputs. In some cases, the  gradients check can be accepted (i.e., the theoretical gradient and the numerical gradient are very close evaluated by a threshold value, say, 1e-3), but in some other cases, the gradients check can fail, which depends on the parameters of that op.  Thus, I'm not sure if this is good or valid operator that is suitable for learning.\r\n\r\nIn the Caffe framework, it seems those `errors` were saved in `diff` memory for each layer. I want to get these back-propagated `errors` in each layer. Does anybody know how to get that?", "comments": []}, {"number": 17120, "title": "Imagenet classification with VGG16 pretrained weights (Keras interface) doesnt seem to work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Described below in detail\r\n\r\n### Describe the problem\r\nI tried to use VGG16 image net classifier which is given through keras interface in tensorflow (tf.keras.applications.VGG16) by grabbing the graph given by Keras and then using it. But it doesn't seem to work.   \r\n\r\nI thought it might be issue with how I am using it ,but after thinking over it a lot I have concluded that this might an issue with Tensorflow. I had posed about it on SO at https://stackoverflow.com/questions/48850537/issue-with-imagenet-classification-with-vgg16-pretrained-weights\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom tensorflow.python.keras._impl.keras.applications import imagenet_utils\r\n\r\n\r\nmodel = tf.keras.applications.VGG16()\r\nVGG = model.graph\r\n\r\nVGG.get_operations()\r\ninput = VGG.get_tensor_by_name(\"input_1:0\")\r\noutput = VGG.get_tensor_by_name(\"predictions/Softmax:0\")\r\nprint(input)\r\nprint(output)\r\n\r\nI = Image.open(\"Elephant.jpg\")\r\nnew_img = I.resize((224,224))\r\nimage_array = np.array(new_img)[:, :, 0:3]\r\nimage_array = np.expand_dims(image_array, axis=0)\r\nimage_array = image_array.astype(np.float32)\r\nimage_array = image_array/255\r\n\r\n\r\nwith tf.Session(graph=VGG) as sess:\r\n    init_op = tf.global_variables_initializer()\r\n    sess.run(init_op)\r\n    pred = (sess.run(output,{input:image_array}))\r\n    print(imagenet_utils.decode_predictions(pred))\r\n```\r\nAnd below is the result I get when run:  \r\n\r\n**Tensor(\"input_1:0\", shape=(?, 224, 224, 3), dtype=float32)\r\nTensor(\"predictions/Softmax:0\", shape=(?, 1000), dtype=float32)**\r\n\r\n**[[('n02281406', 'sulphur_butterfly', 0.0022673723), ('n01882714', 'koala', 0.0021256246), ('n04325704', 'stole', 0.0020583202), ('n01496331', 'electric_ray', 0.0020416214), ('n01797886', 'ruffed_grouse', 0.0020229272)]]**\r\n\r\nClearly the classfication results are not the one expected.  \r\n\r\nAlso If I dont run init_op for global variable initializing I get an error **Attempting to use uninitialized value block1_conv1/bias**", "comments": ["Closing this out as it seems you've figured out the problem and [updated the answer on StackOverflow](https://stackoverflow.com/a/48868312).\r\n\r\nAs you gathered, in the code snippet above you were creating a new session and then calling `sess.run(tf.global_variables_initializer())` which would re-initialize all variables to random values instead of using the values saved in the checkpoint."]}, {"number": 17119, "title": "Apparent thread-safety issue in tensorflow/core/kernels/queue_op.h", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.6.0-rc1-277-g993006fa76', '1.6.0-rc1')\r\n- **Python version**: Python 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.10.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n- **CUDA/cuDNN version**: 9.1 / 7.0.5\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory (CUDA compute capability 3.0)\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nClang warns about a thread-safety issue in `tensorflow/core/kernels/queue_op.h` at lines 46 and 47 which warnings appear to be valid.\r\n\r\nHere is the code around that line:\r\n\r\n```c++\r\n  void Compute(OpKernelContext* context) override {\r\n    ResourceOpKernel<QueueInterface>::Compute(context);\r\n    if (resource_ && context->track_allocations()) {                          // Line 46\r\n      context->record_persistent_memory_allocation(resource_->MemoryUsed());  // Line 47\r\n    }\r\n  }\r\n```\r\n\r\nNo lock is held on `mu_`.\r\n\r\nIf there is no thread safety issue, I think that a comment should be added to explain why, as it's not clear.\r\n\r\n### Source code / logs\r\n<pre>\r\n./tensorflow/core/kernels/queue_op.h:46:9: warning: reading variable 'resource_' requires holding mutex 'mu_' [-Wthread-safety-analysis]\r\n    if (resource_ && context->track_allocations()) {\r\n        ^\r\n./tensorflow/core/kernels/queue_op.h:47:52: warning: reading variable 'resource_' requires holding mutex 'mu_' [-Wthread-safety-analysis]\r\n      context->record_persistent_memory_allocation(resource_->MemoryUsed());\r\n                                                   ^\r\n</pre>", "comments": ["@yuefengz can you please comment it?", "I have a pending change internally that would fix this issue.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think that this was fixed by 3d9c27742693f9859e2fb75de57fe108520de712"]}]