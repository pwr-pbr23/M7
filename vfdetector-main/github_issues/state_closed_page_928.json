[{"number": 25609, "title": "tf.nn.conv2d_transpose different behaviour if output_shape is a list", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\ntf version 1.12. It probably relates to other versions too. System information doesn't matter.\r\n\r\n**Describe the current behavior**\r\nlines 1235, 1326, 1459, 2535 of nn_ops.py includes the following error check:\r\nif isinstance(output_shape, (list, np.ndarray)):\r\n\r\nHowever, if output_shape is a tuple instead of a list, then the error check is skipped.\r\n\r\n**Describe the expected behavior**\r\nBehaviour should be the same for lists or tuples. Fix is easy:\r\n\r\nif isinstance(output_shape, (list, tuple, np.ndarray)):\r\n\r\n", "comments": ["@mccane would you be willing to submit a PR? (No worries if not)", "Nah - more trouble than it\u2019s worth for me at the moment. Someone else can claim all the glory (or blame):-)\n\n> On 9/02/2019, at 08:33, Skye Wanderman-Milne <notifications@github.com> wrote:\n> \n> @mccane would you be willing to submit a PR? (No worries if not)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\nCheers,\n\nBrendan\n\n\n\n", "I'm afraid that only TensorShape and list are valid for `output_shape`.", "Well that is a bizarre requirement unless there is some chance the output shape contents will be changed in the function. But OK, if that\u2019s what you\u2019re going to stick with then please have an error check that fails when a tuple is passed in. At the moment the function carries on without doing the error checking and this causes bizarre errors later on that are difficult to find. In any case, a TensorShape can be initialised by a tuple, so I\u2019m baffled as to why a tuple is not valid input. And it is not true that only a list or Tensor are valid inputs since a numpy ndarray is also in the error test.\n\n> On 10/02/2019, at 01:33, Yan Facai (\u989c\u53d1\u624d) <notifications@github.com> wrote:\n> \n> I'm afraid that only TensorShape and list are valid for output_shape.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\nCheers,\n\nBrendan\n\n\n\n", "I understand, and for convenience, I'm fine with your proposal as well . As python is a dynamical language, it's not so easy to check type strictly without cost (type hints only supports for py3). As @skye said, welcome to submit a PR :-)", "hi, couldn't find behavior for current nn.conv2d_transpose but it was in nn.atrous_conv2d_transpose. Just added an error for tuple output-shape. \r\nhttps://github.com/tensorflow/tensorflow/pull/25803\r\n", "Thanks.\n\n> On 17/02/2019, at 17:16, ryan jiang <notifications@github.com> wrote:\n> \n> hi, couldn't find behavior for current nn.conv2d_transpose but it was in nn.atrous_conv2d_transpose. Just added an error for tuple output-shape.\n> #25803\n> #25803\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n\nCheers,\n\nBrendan\n\n\n\n", "Bumping this issue. I'm wondering how the type check in line 2370 of nn_ops.py plays into this. From what I see, a tuple is casted to a list. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8de272b3f3b73bea8d947c5f15143a9f1cfcfc6f/tensorflow/python/ops/nn_ops.py#L2370", "@mccane,\r\nCan you please confirm if we can Close this issue as the [associated PR](https://github.com/tensorflow/tensorflow/pull/25803) has been Merged? Thanks! ", "Yep.\n\nOn Thu, 22 Apr 2021 at 18:56, rmothukuru ***@***.***> wrote:\n\n> @mccane <https://github.com/mccane>,\n> Can you please confirm if we can Close this issue as the associated PR\n> <https://github.com/tensorflow/tensorflow/pull/25803> has been Merged?\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25609#issuecomment-824590263>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAMIIHKVA2J6CYLMSUPO7GDTJ7CBBANCNFSM4GVTTXTQ>\n> .\n>\n\n\n-- \nCheers,\n\nBrendan\n"]}, {"number": 25608, "title": "Building tensorflow from Source for arm64", "body": "Hi All,\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Bazel installed version : 0.21.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.3 \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/a\r\n\r\n\r\n\r\n\r\nI'm trying to build tensorflow from source on arm64 (aarch64) but got stucked at the following:\r\n\r\nINFO: From Compiling tensorflow/core/kernels/diag_op.cc [for host]:\r\ntensorflow/core/kernels/diag_op.cc:118:1: warning: multi-line comment [-Wcomment]\r\n // `new_index = i1*(s2*...sk*s1*...*sk) + i2*(s3*...*sk*s1*...*sk) +... + \\\r\n ^\r\n**ERROR: /home/rock64/tensorflow/tensorflow/core/kernels/BUILD:2187:1: C++ compilation of rule '//tensorflow/core/kernels:resource_variable_ops' failed (Exit 4)**\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3734.152s, Critical Path: 279.33s\r\nINFO: 4043 processes: 4043 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nBazel installed version : 0.21.0\r\nTensorflow : 1.12.0\r\ninstalled gcc-aarch64-linux-gnu & g++-aarch64-linux-gnu\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@TropSpirit I am closing this issue as it was a duplicate issue #25607 . Please fill the template in that open issue then we will try to resolve as soon as possible. In future, please don't create duplicate issues. Thanks!", "@jvishnuvardhan  this is not duplicate issue #25607 is error while using cross compile and this issue #25608 is directly compiling/building from source on arm64 bit processor and i'm getting this error. Two different issues. ", "@TropSpirit Sorry for the mistake. Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Thanks!", "@TropSpirit If the issue was resolved, please close it. Thanks!", "HEY\uff0ci got the same issue. changing the bazel version do not work. Is it the reason of gcc version?how about you to fix this issue?"]}, {"number": 25607, "title": "Error while Building tensorflow using cross-tool for aarch64 ", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source trying to build and install for aarch64 (arm64)\r\n- TensorFlow version:  1.12.0 (trying to install)\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NO - virtualenv or pip\r\n- Bazel version (if compiling from source): 0.19.0 (tried with new version same issue)\r\n- GCC/Compiler version (if compiling from source): - gcc-aarch64-linux-gnu & g++-arch64-linux-gnu\r\n- CUDA/cuDNN version: No CUDA or XcuDNN\r\n- GPU model and memory:  NO GPU used \r\n\r\nInstalled bazel - 0.19.0 version\r\nTensorflow source latest from git\r\nBuilding in Ubuntu for arm64 bit(aarch64) cpu using cross-tool. \r\n\r\nCommand for building:\r\nbazel build -c opt //tensorflow/examples/label_image --cpu=aarch64 -- crosstool_top=//tools/aarch64_compiler:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures\r\n\r\nAnd here is the error logs:\r\n****ERROR**: /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/external/mkl_dnn/BUILD.bazel:71:1: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command** \r\n  (cd /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3.6 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3.6 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /usr/bin/aarch64-linux-gnu-gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o' -iquote external/mkl_dnn -iquote bazel-out/aarch64-opt/genfiles/external/mkl_dnn -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/genfiles/external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -isystem external/mkl_dnn/include -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/xbyak -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o)\r\nIn file included from external/mkl_dnn/src/cpu/cpu_isa_traits.hpp:35:0,\r\n                 from external/mkl_dnn/src/cpu/jit_generator.hpp:21,\r\n                 from external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.hpp:23,\r\n                 from external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp:23:\r\nexternal/mkl_dnn/src/cpu/xbyak/xbyak_util.h:84:21: **fatal error: cpuid.h: No such file or directory**\r\ncompilation terminated.\r\nTarget //tensorflow/examples/label_image:label_image failed to build\r\nINFO: Elapsed time: 49.893s, Critical Path: 10.85s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 194 processes: 194 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Ubuntu 16.04\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n* TensorFlow installed from (source or binary): source trying to build and install for aarch64 (arm64)\r\n* TensorFlow version:  1.12.0 (trying to install)\r\n* Python version: 3.6\r\n* Installed using virtualenv? pip? conda?: NO - virtualenv or pip\r\n* Bazel version (if compiling from source): 0.19.0 (tried with new version same issue)\r\n* GCC/Compiler version (if compiling from source): - gcc-aarch64-linux-gnu & g++-arch64-linux-gnu\r\n* CUDA/cuDNN version: No CUDA or XcuDNN\r\n* GPU model and memory:  NO GPU used\r\n\r\nInstalled bazel - 0.19.0 version\r\nTensorflow source latest from git\r\nBuilding in Ubuntu for arm64 bit(aarch64) cpu using cross-tool.\r\n\r\nCommand for building:\r\nbazel build -c opt //tensorflow/examples/label_image --cpu=aarch64 -- crosstool_top=//tools/aarch64_compiler:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures\r\n\r\nAnd here is the error logs:\r\n****ERROR**: /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/external/mkl_dnn/BUILD.bazel:71:1: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command**\r\n(cd /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/execroot/org_tensorflow && exec env - PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin PWD=/proc/self/cwd PYTHON_BIN_PATH=/usr/bin/python3.6 PYTHON_LIB_PATH=/usr/lib/python3.6 TF_DOWNLOAD_CLANG=0 TF_NEED_CUDA=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_ROCM=0 /usr/bin/aarch64-linux-gnu-gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o' -iquote external/mkl_dnn -iquote bazel-out/aarch64-opt/genfiles/external/mkl_dnn -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/genfiles/external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -isystem external/mkl_dnn/include -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/xbyak -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o)\r\nIn file included from external/mkl_dnn/src/cpu/cpu_isa_traits.hpp:35:0,\r\nfrom external/mkl_dnn/src/cpu/jit_generator.hpp:21,\r\nfrom external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.hpp:23,\r\nfrom external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp:23:\r\nexternal/mkl_dnn/src/cpu/xbyak/xbyak_util.h:84:21: **fatal error: cpuid.h: No such file or directory**\r\ncompilation terminated.\r\nTarget //tensorflow/examples/label_image:label_image failed to build\r\nINFO: Elapsed time: 49.893s, Critical Path: 10.85s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 194 processes: 194 local.\r\nFAILED: Build did NOT complete successfully", "@TropSpirit One thing I quickly noticed is version of Bazel. Please check the test build configuration [here](https://www.tensorflow.org/install/source#linux). DO you mind downgrading Bazel to 0.15.0 and test whether the error is same or not. Meanwhile I will also take a deeper look at the issue. Thanks!", "I tried using bazel 0.15.0 for crosscompiling still same error. i even tried tensorflow 1.8 and bazel 0.10.0\r\n\r\n$ bazel build -c opt //tensorflow/examples/label_image --cpu=aarch64 --crosstool_top=//tools/aarch64_compiler:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures\r\n\r\n\r\n**_ERROR: /home/cmp/.cache/bazel/_bazel_nshivash/a20291383e693f326d5a9bf14cdd7b80/external/nsync/BUILD:466:13: Configurable attribute \"copts\" doesn't match this configuration (would a default condition help?).\r\nConditions checked:_**\r\n @nsync//:android_arm\r\n @nsync//:android_arm64\r\n @nsync//:android_armeabi\r\n @nsync//:android_x86_32\r\n @nsync//:android_x86_64\r\n @nsync//:clang_macos_x86_64\r\n @nsync//:freebsd\r\n @nsync//:gcc_linux_aarch64\r\n @nsync//:gcc_linux_ppc64\r\n @nsync//:gcc_linux_s390x\r\n @nsync//:gcc_linux_x86_32_1\r\n @nsync//:gcc_linux_x86_64_1\r\n @nsync//:gcc_linux_x86_64_2\r\n @nsync//:ios_x86_64\r\n @nsync//:msvc_windows_x86_64\r\nERROR: Analysis of target '//tensorflow/examples/label_image:label_image' failed; build aborted: \r\n", "Any update on this ? ", "i have the same issue.. is there a way around?", "Hi, I had the same issue. It seems to be solved with this flag in the `baze build` command:\r\n\r\n`bazel build --define tensorflow_mkldnn_contraction_kernel=0 ...`", "@AlbertoLanaro Thanks for the solution. @TropSpirit and @NKucza Could you both try the solution? Thanks!", "Sorry for the slow response! We don't officially support aarch64 linux builds (we don't have continuous integration, etc set up to catch these sort of issues, so it's hard to offer a great experience) but the suggestions for disabling MKL in this case make sense. Closing since that seems like a fix."]}, {"number": 25606, "title": "~40% slow down since 1.13.0.dev20190202", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: public Colab instance and Debian\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: >= 1.13.0.dev20190202\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: K80 (originally occurred on T4 and V100)\r\n\r\n**Describe the current behavior**\r\nPerformance on some simple looped graphs has worsen by ~40% in between 1.13.0.dev20190201 and 1.13.0.dev20190202 and still exists in the current nightly build (see provided colab). This slow down was consistent across VMs and GPUs when running my research code. The provided code is my best attempt at boiling it down.\r\n\r\nHere are the results from the colab benchmark copied for convenience. The `num_batched_iter` partially unrolls the `tf.while_loop` body for that many iterations to reduce its overhead and leverage other graph optimizations. Before the first timed `sess.run(...)` call, 5 warm-up call are made. Results reported aggregate 40 different runs. The colab runtimes were completely reset before benchmarking each version. \r\n\r\nOn the newest tf-nightly-gpu build, I got:\r\n```\r\n1.13.0-dev20190207\r\nBenchmarking simple_loop_case...\r\nname                                                                         mean        min        max     q=0.05     q=0.95\r\n----------------------------------------------------------------------  ---------  ---------  ---------  ---------  ---------\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=True    0.526553   0.512475   0.551661   0.51285    0.543843\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=False   1.18145    1.15179    1.2054     1.16411    1.1976\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=True   0.0664629  0.0633538  0.0774522  0.0636203  0.0729571\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=False  0.249716   0.238131   0.261069   0.241182   0.259239\r\n```\r\nHere are the results on the version just before the issue appears:\r\n```\r\n1.13.0-dev20190201\r\nBenchmarking simple_loop_case...\r\nname                                                                         mean        min        max     q=0.05     q=0.95\r\n----------------------------------------------------------------------  ---------  ---------  ---------  ---------  ---------\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=True    0.519208   0.483462   0.570542   0.487508   0.569501\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=False   1.21199    1.18047    1.24812    1.18563    1.23686\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=True   0.0483549  0.0455515  0.0589228  0.0463824  0.0505414\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=False  0.175403   0.168438   0.192948   0.169144   0.18713\r\n```\r\n\r\n**Code to reproduce the issue**\r\nhttps://gist.github.com/gehring/4e7bd9b6f0c5d73c545777236fc507d0\r\n\r\n", "comments": ["The issue still remains in the most recent nightly build:\r\n```\r\n1.13.0-dev20190212\r\nBenchmarking simple_loop_case...\r\nname                                                                        mean        min        max    q=0.05     q=0.95\r\n----------------------------------------------------------------------  --------  ---------  ---------  --------  ---------\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=True    0.569304  0.551403   0.60731    0.556146  0.585934\r\nsimple loop, num_batched_iter=1, back_prop=False, test_converge=False   1.24792   1.21103    1.37319    1.21408   1.31663\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=True   0.071491  0.0682344  0.0827348  0.068548  0.0796988\r\nsimple loop, num_batched_iter=20, back_prop=False, test_converge=False  0.244619  0.236556   0.25513    0.237421  0.250926\r\n```", "@gehring Is this still an issue for you. As `TF1.x` is out of support, can you try `TF2.x` [Here](https://colab.research.google.com/gist/jvishnuvardhan/8a7574cc3c93cd008607e6d87b853788/tf_perf_issues.ipynb) is a gist comparing `TF1.13` and `TF1.15`. Thanks!\r\n\r\nI am closing this issue. Please open a new issue if the issue persists performance issue with `TF2.x` version. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25606\">No</a>\n"]}, {"number": 25605, "title": "[DONOTMERGE] Test fix pip test failure", "body": "", "comments": []}, {"number": 25604, "title": "[DONOTMERGE] Test fix pip test failure", "body": "", "comments": []}, {"number": 25603, "title": "[DONOTMERGE] Test fix pip test failure", "body": "", "comments": []}, {"number": 25602, "title": "[DONOTMERGE] Test fix pip test failure", "body": "", "comments": []}, {"number": 25601, "title": "Batchnorm does not work in Eager mode in TF 1.12: InternalError: Could not find valid device for node", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux based on Dockerfile tensorflow/tensorflow:1.12.0-devel-gpu-py3, that is Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nDockerfile tensorflow/tensorflow:1.12.0-devel-gpu-py3\r\n- TensorFlow version (use command below):\r\n1.12.0\r\n- Python version:\r\nPython 3.5.2\r\n- Bazel version (if compiling from source):\r\nNo\r\n- GCC/Compiler version (if compiling from source):\r\nNo\r\n- CUDA/cuDNN version:\r\n== cuda libs  ===================================================\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart-f7fdd8d7.so.9.0\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n- GPU model and memory:\r\nGeForce GTX TITAN X 11.93GiB\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3496, in _fused_batch_norm\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.\r\nNode: {{node FusedBatchNorm}} = FusedBatchNorm[T=DT_DOUBLE, data_format=\"NHWC\", epsilon=0.001, is_training=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\r\nAll kernels registered for op FusedBatchNorm :\r\n  device='XLA_GPU'; T in [DT_FLOAT]\r\n  device='XLA_CPU'; T in [DT_FLOAT]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_FLOAT]\r\n [Op:FusedBatchNorm]\r\n$ \r\n\r\n**Describe the expected behavior**\r\nI expect Tensorflow to find that my GPU (or at least my CPU) is a valid device to execute a Batchnorm operation on. It is not.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\ntf.enable_eager_execution()\r\nx = np.random.rand(2,416,416,3) * 255\r\nx = tf.convert_to_tensor(x)\r\nbn = BatchNormalization()\r\nx = bn(x)\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTensorflow version 1.12.0\r\nKeras version 2.1.6-tf\r\n2019-02-07 22:59:14.551505: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-02-07 22:59:14.678286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 11.92GiB freeMemory: 7.08GiB\r\n2019-02-07 22:59:14.781607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 11.93GiB freeMemory: 11.81GiB\r\n2019-02-07 22:59:14.782011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-02-07 22:59:15.310401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-07 22:59:15.310438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-02-07 22:59:15.310450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y \r\n2019-02-07 22:59:15.310457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N \r\n2019-02-07 22:59:15.311331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6821 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0, compute capability: 5.2)\r\n2019-02-07 22:59:15.311691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11427 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0, compute capability: 5.2)\r\nTraceback (most recent call last):\r\n  File \"/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/ptvsd_launcher.py\", line 45, in <module>\r\n    main(ptvsdArgs)\r\n  File \"/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/lib/python/ptvsd/__main__.py\", line 348, in main\r\n    run()\r\n  File \"/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/lib/python/ptvsd/__main__.py\", line 253, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/usr/lib/python3.5/runpy.py\", line 254, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib/python3.5/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/niclasdn/development/yolov3_pytorch/run_yolov3_minitest.py\", line 70, in <module>\r\n    x = bn(x)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 757, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 514, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 401, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/tf_utils.py\", line 52, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py\", line 398, in _fused_batch_norm_inference\r\n    data_format=self._data_format)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py\", line 909, in fused_batch_norm\r\n    name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 3496, in _fused_batch_norm\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.\r\nNode: {{node FusedBatchNorm}} = FusedBatchNorm[T=DT_DOUBLE, data_format=\"NHWC\", epsilon=0.001, is_training=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)\r\nAll kernels registered for op FusedBatchNorm :\r\n  device='XLA_GPU'; T in [DT_FLOAT]\r\n  device='XLA_CPU'; T in [DT_FLOAT]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_FLOAT]\r\n [Op:FusedBatchNorm]\r\n$ ", "comments": ["@nizlas I am not sure why you are not able to run. I was able to run the your code as follows. I had to change float type from float64 to float32 and removed brackets around enabling eager. Please check whether this will work on your computer.\r\n \r\nimport tensorflow as tf\r\ntf.enable_eager_execution\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nbn = BatchNormalization()\r\nx = np.random.rand(2,416,416,3) * 255\r\nx=x.astype(np.float32)\r\nx = tf.convert_to_tensor(x)\r\n\r\nx = bn(x)\r\n\r\nThanks!", "Removing the parantheses seems to put everything on the CPU (at least my computer became extremely slow and the CPU was working 100%) so I would not recommend that. Not sure what it even means...replacing a function call with a variable?\r\n\r\nMy mistake was that numpy by default assigned float64 to the input tensor. However the error message is still quite confusing... I would have expected some warning that the layer has no implementation for float64. It becomes even more confusing since the convlayer DOES execute in float64 (but very slowly) so one does naturally interprets the differing behaviour between convolution2d and Batchnorm to be an error in the framework.\r\n\r\nAlso the documentation is very behind with respect to this. One would appreciate any kind of comment on datatype support in the documentation...\r\n\r\nAnyway, with float32 it works. Thanks for the feedback!\r\n ", "@nizlas Thanks! I had to remove parentheses because Spyder was throwing error. But, it is just Spyder issue. In other places like Google colab, I need to use parentheses. \r\nClosing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25601)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25601)\r\n"]}, {"number": 25600, "title": "Add TF_PACKAGE_VERSION to Dockerfiles that install via pip", "body": "This change adds `--build-arg TF_PACKAGE_VERSION=...` to the standard\nDockerfiles. It also picks up a tiny --build-arg documentation typo.\n\nI also merged the dockerfile specs into one spec. Since the Dockerfiles\nare usually going to be built all at once (and the default behavior of\nthe assembler is to wipe the build directory), it seems more convenient\nto put them all in one place.", "comments": []}, {"number": 25599, "title": "Tests for TensorSliceDataset and SparseTensorSliceDataset", "body": "This PR adds the tests for TensorSliceDataset and SparseTensorSliceDataset. As #25496 is not merged yet, there are two duplicated util functions (`CreateTensor()` and `ExpectEqual()`) in this PR. Once either of them is merged, will update the other one. \r\n\r\ncc @jsimsa ", "comments": ["@jsimsa Thanks for your quick review! [This commit](https://github.com/tensorflow/tensorflow/pull/25599/commits/1f711cc6f11fdf0d528f4d20c70fd7b9713c72bf) replaces `TF_CHECK_OK` with `TF_RETURN_IF_ERROR` and adds the missing new line. Could you have a look at the changes?", "The test failure is caused by `//bazel_pip/tensorflow/contrib/gan:classifier_metrics_test`, so it is unrelated."]}, {"number": 25598, "title": "tf_upgrade_v2 upgrade script should do absl-py conversions", "body": "Currently the tf_upgrade_v2 upgrade script changes uses of tf.app, tf.flags & tf.logging to tf.compat.v1.*. It seems that is would be more elegant for us to simply add the necessary absl-py imports and modify the code to use those instead.", "comments": ["@jaingaurav Could you please let us know if you still need help on this issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25597, "title": "Dll exception", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nXPS 15 9570 specs --\r\nProcessor : Intel(r) Core(Tm) i7-8750H CPU @2.20GHz 2.21GHz\r\nRam : 16 Gb\r\nSystem type : 64 bit, x64 based processor\r\nWindows 10 Home single Language  --\r\nversion = 1803\r\nOS Build 17134.523\r\n\r\n- TensorFlow installed from (source or binary): \r\nUsed pip install\r\n- TensorFlow version:  \r\ntensorflow-gpu = 1.12.0\r\n- Python version: python 64bit 3.6.8\r\n- Installed using virtualenv? pip? conda?: \r\nusing pip\r\n- CUDA/cuDNN version: \r\nTried CUDA toolkit 9.2 and 10.0 with similar error. \r\ncudnn - 7.4.2 (Dec  14)\r\n\r\n- GPU model and memory:\r\nNvidia : NVIDIA GeForce GTX 1050 Ti with Max-Q Design\r\nMem : 4GB GDDR5\r\nDriver Version - 24.21.13.9793\r\n\r\n\r\n**Describe the problem**\r\nWhen I import tensorflow it gives me error:\r\nImportError: DLL load failed: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed the path as shown [here](https://www.youtube.com/watch?v=HExRhnO5Mqs).\r\n1. Uninstalled python\r\n2. First I downloaded visual studio community version 17.\r\n3. Then I downloaded and installed Cuda toolkit 9.2\r\n4. Then downloaded cudnn 7.4.2. Added the path of folders bin,include,lib and all internal folder of path variable.\r\n5. Installed python\r\n6: Run command 'pip install --ignore-installed --upgrade tensorflow-gpu'\r\n7. Ran python on command prompt and Imported tensorflow\r\n\r\n\r\n\r\n**Any other info / logs**\r\nComplete tracebook:\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n\r\n\r\n\r\nTried solution:\r\n[This solution](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso)", "comments": ["@aniketbote Please check [Tested build supports](https://www.tensorflow.org/install/source_windows#gpu).    What OS you have in your system? What is your Bazel version? I have noticed that MSVC is higher verion. You might need to downgrade to 2015 version. There is another [issue](https://github.com/tensorflow/tensorflow/issues/24886) with WINDOWS10 where it limits the path length. Also, check whether CUDA location is referencing correctly.  Please let me know how it progresses. Thanks!", "My OS as stated in issue is windows 10.\r\nI don't have bazel since I am building it from source.\r\nI have uninstalled Microsoft visual studio 2017 community edition, cuda and delete cudnn from system\r\nI started fresh as per the tested build supports. \r\n1) Uninstalled python and deleted the site - packages folder from the system\r\n2) Downloaded Microsoft visual studio 2015 community edition from [here](https://stackoverflow.com/questions/44290672/how-to-download-visual-studio-community-edition-2015-not-2017l)\r\n3) Downloaded cuda 9.2 for windows from [here](https://developer.nvidia.com/cuda-toolkit-archive)\r\n4) Downloaded cudnn 7.4.2 from [here](https://developer.nvidia.com/cuda-toolkit-archive)\r\n5) set path in environment variable for user.\r\n6) In cmd pip install tensorflw-gpu\r\n7) import tensorflow\r\n\r\nResult : Result was similar as previous.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Aniket\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "@aniketbote Did you remove the restriction on path length as mentioned in my earlier response. Thanks!", "Yes, i followed the link provided by you.\r\nMy long path was already enabled (set to 1).", "@aniketbote Thanks for the confirmation. I will try to install on my windows10 system and get back to you. Meanwhile, try to install CUDA 9.0 because some of the modules looking for a cuda9.0 path. This is one of the common error. So it is better to load 9.0 version or check whether all the paths (CUDA-->cuDNN, Python, tensorflow, other modules) are referenced correctly. Thanks!", "> Yes, i followed the link provided by you.\r\n> My long path was already enabled (set to 1).\r\n\r\nhave you solved this problem ?  I meet the same one ", "> @aniketbote Thanks for the confirmation. I will try to install on my windows10 system and get back to you. Meanwhile, try to install CUDA 9.0 because some of the modules looking for a cuda9.0 path. This is one of the common error. So it is better to load 9.0 version or check whether all the paths (CUDA-->cuDNN, Python, tensorflow, other modules) are referenced correctly. Thanks!\r\n\r\nWhen I try to install cuda 9.0 version in my system. \r\nI get this.\r\n![capture](https://user-images.githubusercontent.com/38111546/52531047-3c517a00-2d35-11e9-824e-c1a95525287f.PNG)\r\n\r\nHowever when I try to install 9.2 and 10 it works fine.\r\nThe above mentioned are the only two that works fine.\r\n", "@aniketbote Thanks. Did you check whether all the paths to CUDA/cuDNN are set right? Thanks!", "> @aniketbote Thanks. Did you check whether all the paths to CUDA/cuDNN are set right? Thanks!\r\n\r\nYes, the path is set.", "I tried it on \r\n**cuda 9.0 \r\ncudnn 7.05**\r\n\r\nThe path variables for this installation:\r\n![capture](https://user-images.githubusercontent.com/38111546/52789680-88c8ec80-308a-11e9-990e-73cfc98cb8fa.PNG)\r\n", "@aniketbote Could you uninstall python and tensorflow, and try to reinstall everything using my instruction [here](https://github.com/jvishnuvardhan/Installing-TensorFlow-GPU-on-Windows-10-TF1.12). If you want to use CUDA10, then you can install TF1.13 following the same procedure listed there but use newer version of drivers. Please let me know how it progresses. Thanks!", "This could be the same problem as this other unresolved issue, see my comment there: https://github.com/tensorflow/tensorflow/issues/25552#issuecomment-470159497", "Yes I think it is the same problem. Any particular way it could be resolved? @alvaroslm ", "*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25597\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25597\">No</a>\n"]}, {"number": 25596, "title": "[ROCm][GPU] Finalize ROCm GPU kernels early with -fno-gpu-rdc", "body": "- finalize GPU kernels at compile-time with -fno-gpu-rdc\r\n- abolish using hipcc as linker\r\n- use gcc as linker, link with HIP/HCC runtime\r\n- do NOT use --gc-sections at link time to prevese .kernel section for GPU kernels\r\n\r\nThis is the beginning of a series of PR to migrate ROCm execution path from dynamically linked with libraries on ROCm platform, to dynamically load libraries on ROCm platform.", "comments": ["+ @parallelo / @deven-amd \r\n@timshen91 , your earlier PR #25354 would actually break our work-in-progress community build setup, and this PR aims to address that.\r\n\r\nAlso it paves the way for StreamExecutor on ROCm path to dynamically load ROCm libraries. The first step is to use gcc as linker and refrain from using hipcc as the linker. Subsequent PRs would gradually tune bazel rules for ROCm StreamExecutor to enable dynamically loaded ROCm libraries.", "@tatianashp . Sorry for pulling you to this PR out of blue. I was informed by @dagamayank that you'll help facilitate ROCm-related PRs be reviewed. I'm wondering would it be possible to help expedite this PR, along with other ones for StreamExecutor? We are currently setting up community builds and this PR would unblock us from getting it done.", "The single failed case in \"Ubuntu Python3 PIP\" target should have no relationship with this PR.", "I am not very experienced with crosstool.\r\n@r4nt would be a much better reviewer for this than me.", "@gunan , thanks for your comment. it seems @r4nt can't be assigned?", "@pragyaak wondering would it be possible to help expedite the process for this PR. to my understanding there is no ROCm machines in TensorFlow build / test infrastructure and AMD is setting a community build now. This PR is now the only missing piece for the community build to be able to track TensorFlow mainline with `--build=rocm` enabled.\r\n\r\n", "@jlebar / @timshen91 Just want to check if it's possible to expedite the process to get this PR landed sooner. Thanks.", "> it seems @r4nt can't be assigned?\r\n\r\n@r4nt you probably need to add yourself to the TF Github organization.\r\n\r\n@whchung I'm fine with approving this because it only affects rocm builds.  We're still working out our policies for this, but for now it would be helpful if you could confirm that someone at AMD did a careful review of this?", "@jlebar Thanks. Logic here does pass tests in our downstream fork:\r\nhttps://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/310", "> Logic here does pass tests in our downstream fork:\r\n[ROCmSoftwarePlatform#310](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/310)\r\n\r\nShould I take it that nobody on your side reviewed this code?", "@jlebar nope. @scchan has reviewed and approved it. He is the main developer in HCC to implement the flag I introduced in this PR.", "OK, sgtm!", "Most of the failures are related to `hwloc` which should has nothing to do with this PR."]}, {"number": 25595, "title": "Fix test sharding", "body": "Test sharding appears to have been broken by commit 87cc788.  The result\r\nbeing that many tests are simply not run when sharding is enabled.\r\nThe issue seems to be that the filtering for sharding is happening\r\ntwice, once in tensorflow/python/platform/googletest.py and then once\r\nagain in external/absl_py/absl/testing/absltest.py.  This commit fixes\r\nthe issue by removing the shard filtering code in googletest.py.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/25594", "comments": ["@hgadig what is blocking this PR?", "> @hgadig what is blocking this PR?\r\n\r\nUnable to create internal CL. I see there are multiple patches in gerrit.", "> > @hgadig what is blocking this PR?\r\n> \r\n> Unable to create internal CL. I see there are multiple patches in gerrit.\r\n\r\nAh, is that because I added a commit to the PR from the web UI? Is there a different way to do that?", "> > > @hgadig what is blocking this PR?\r\n> > \r\n> > \r\n> > Unable to create internal CL. I see there are multiple patches in gerrit.\r\n> \r\n> Ah, is that because I added a commit to the PR from the web UI? Is there a different way to do that?\r\n\r\nI suspect that could be the reason. I think that commit can be pushed by the author or you have to create a new PR to add your commits.", "@hgadig, @revan I can update this PR but will wait for your guidance to do so.  What would be the plan, to cherrypick the second commit onto my fork and push?", "@hgadig,@revan The new PR is  here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/25893\r\n\r\nIt still contains the g_main function as I believe it is still necessary.", "Closing this PR as a new PR(https://github.com/tensorflow/tensorflow/pull/25893) is created."]}, {"number": 25594, "title": "Test sharding appears to be broken", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.8.0-17025-g3e713f9 1.12.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\n\r\nI'm currently testing the AVX512 builds.  There are some tests that are known to fail on these builds.  Patches are pending for these issues (#21676) but they are not yet merged.  The weird thing is that the tests have been passing on my machine for the last two weeks even though they should fail.  I did some digging and it turned out that the tests only pass if sharding is enabled.  They fail as expected if sharding is disabled.  So if, on an AVX512 build,  I do \r\n\r\nbazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test\r\n\r\nthe test passes\r\n\r\nand if I do\r\n\r\n bazel test --test_sharding_strategy=disabled --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test\r\n\r\nthe test fails, as it should.  It turns out that the test is passing when sharding is enabled as not all of the sub-tests are being run.  This issue seems to have been caused by commit #87cc788 which changed the unit test framework.  As far as I can tell, filtering for sharding is now happening twice, once in googletest.py and once in absltest.\r\n\r\nThe issue can be reproduced on my machine, a 10 core SKX core i9 as follows\r\n\r\nHere's how to reproduce the issue\r\n\r\n```\r\n$ bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test\r\n```\r\n\r\nThen check shard 18.  I see\r\n\r\n```\r\n$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/python/kernel_tests:embedding_ops_test\r\n-----------------------------------------------------------------------------\r\nRunning tests under Python 3.5.2: /usr/bin/python3\r\n----------------------------------------------------------------------\r\nRan 0 tests in 0.000s\r\n\r\nOK\r\n```\r\nNote 0 tests run.\r\n\r\n**Describe the expected behavior**\r\nAnd what I'm expecting to see is \r\n\r\n```\r\n$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/python/kernel_tests:embedding_ops_test\r\n-----------------------------------------------------------------------------\r\nRunning tests under Python 3.5.2: /usr/bin/python3\r\n2019-02-06 08:57:56.395259: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz\r\n2019-02-06 08:57:56.396001: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x29fe960 executing computations on platform Host. Devices:\r\n2019-02-06 08:57:56.396024: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n[ RUN      ] EmbeddingLookupTest.testMaxNorm\r\n[       OK ] EmbeddingLookupTest.testMaxNorm\r\n[ RUN      ] SafeEmbeddingLookupSparseTest.test_safe_embedding_lookup_sparse_3d_return_special_vector\r\nW0206 08:57:56.425108 140697567004416 deprecation.py:506] From /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py:787: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0206 08:57:56.500279 140697567004416 deprecation.py:323] From /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/embedding_ops.py:527: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\n[       OK ] SafeEmbeddingLookupSparseTest.test_safe_embedding_lookup_sparse_3d_return_special_vector\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.151s\r\n\r\nOK\r\n```\r\n\r\nHere I've chosen a shard with a test that passes on my machine but you can hopefully see the difference.\r\n\r\n**Code to reproduce the issue**\r\n\r\nOn a 10 core machine\r\n\r\n$ bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test\r\n$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log\r\n\r\n**Other info / logs**\r\n\r\nI have a patch that I will post shortly.  It may not be the correct fix, but it should point to the part of the code where I believe the problem to be.", "comments": []}, {"number": 25593, "title": "Python 3.7 (nightly) TF 1.12, GLIBC 2.23 import failure on Centos 7.5", "body": "Using Centos 7.5 which has glibc2.17, tensorflow nightly build fails on import\r\nSince updating glibc on Centos appears is not advised, is their a hard requirement for GLIBC 2.23?\r\nThank you.\r\n\r\nImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /python3.7/site                                                                          -packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n", "comments": ["Python 3.7 will be supported in the TF 1.13 official release. Please switch to python 3.6 in the meantime. Thanks!", "Thanks, we are trying to get all platforms to 3.7, (from 3.5) will wait for 1.13, thank you!\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25593)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25593)\r\n", "Using Tensorflow 1.14, installed via pip in Centos with python 3.7.4, the issue persists, and sme error is thrown", "This issue has not been resolved. Same issue `ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`\r\n\r\nUsing\r\npython 3.7.4\r\nTensorflow 1.14.0\r\nCentOS7"]}, {"number": 25592, "title": "tf.contrib.summary.image() fails silently and inconsistently", "body": "**System information**\r\n\r\n* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  No\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Ubuntu 18.04\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  No\r\n* TensorFlow installed from (source or binary):\r\n  Source\r\n* TensorFlow version (use command below):\r\n  b'v1.12.0-5845-g764109a352' 1.12.0\r\n* Python version:\r\n  3.6.7\r\n* Bazel version (if compiling from source):\r\n  Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0\r\n  Build label: 0.21.0\r\n  Build time: Wed Dec 19 12:58:44 2018 (1545224324)\r\n* GCC/Compiler version (if compiling from source):\r\n  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n* CUDA/cuDNN version:\r\n  Cuda compilation tools, release 10.0, V10.0.130\r\n* GPU model and memory:\r\n  GTX 1060 Max Q, 6gb VRAM\r\n\r\n**Describe the current behavior**\r\nI am running into a very weird problem where sometimes images will be logged in tensorboard / the event file, but most of the time they are not. \r\n\r\nThe issue with this is that I can't get consistent behaviour. One time I will run it and it will log normally, and then without changing any code, the next time it won't. And it is really hard to find out anything when `tensorboard --inspect` shows nothing under -image either.\r\n\r\nI should mention that the input data is not always the same, although it is always taken from the same state space (2D game environment with finite number of possible states).\r\n\r\n**Describe the expected behavior**\r\nIf it was not able to log my images, the function should at the very least provide some information to the user as to why it was not able to do so. \r\n\r\n**Code to reproduce the issue**\r\nThis is not possible, as I can not consistently reproduce it myself. It is seemingly completely random, and provides no log, error or warning information.\r\n\r\n**Other info / logs**\r\nAs stated above, there are no logs of any sort.", "comments": ["Update: it managed to log the images again, here is what `tensorboard --inspect` shows (its normal).\r\nImmediately following this, it was unable to log images again after rerunning the program with no changes.\r\n\r\n```\r\nI0207 03:26:05.014535 MainThread program.py:189] Not bringing up TensorBoard, but inspecting event files.\r\nI0207 03:26:05.014535 139688753342272 program.py:189] Not bringing up TensorBoard, but inspecting event files.\r\n======================================================================\r\nProcessing event files... (this can take a few minutes)\r\n======================================================================\r\n\r\nFound event files in:\r\n./logs/2019-02-07_03:25:19\r\n\r\nThese tags are in ./logs/2019-02-07_03:25:19:\r\naudio -\r\nhistograms\r\n   auto_encoder/sequential/conv2d/bias_0\r\n   auto_encoder/sequential/conv2d/bias_0/m\r\n   auto_encoder/sequential/conv2d/bias_0/v\r\n   auto_encoder/sequential/conv2d/kernel_0\r\n   auto_encoder/sequential/conv2d/kernel_0/m\r\n   auto_encoder/sequential/conv2d/kernel_0/v\r\n   auto_encoder/sequential/conv2d_12/bias_0\r\n   auto_encoder/sequential/conv2d_12/bias_0/m\r\n   auto_encoder/sequential/conv2d_12/bias_0/v\r\n   auto_encoder/sequential/conv2d_12/kernel_0\r\n   auto_encoder/sequential/conv2d_12/kernel_0/m\r\n   auto_encoder/sequential/conv2d_12/kernel_0/v\r\n   auto_encoder/sequential/conv2d_15/bias_0\r\n   auto_encoder/sequential/conv2d_15/bias_0/m\r\n   auto_encoder/sequential/conv2d_15/bias_0/v\r\n   auto_encoder/sequential/conv2d_15/kernel_0\r\n   auto_encoder/sequential/conv2d_15/kernel_0/m\r\n   auto_encoder/sequential/conv2d_15/kernel_0/v\r\n   auto_encoder/sequential/conv2d_3/bias_0\r\n   auto_encoder/sequential/conv2d_3/bias_0/m\r\n   auto_encoder/sequential/conv2d_3/bias_0/v\r\n   auto_encoder/sequential/conv2d_3/kernel_0\r\n   auto_encoder/sequential/conv2d_3/kernel_0/m\r\n   auto_encoder/sequential/conv2d_3/kernel_0/v\r\n   auto_encoder/sequential/conv2d_6/bias_0\r\n   auto_encoder/sequential/conv2d_6/bias_0/m\r\n   auto_encoder/sequential/conv2d_6/bias_0/v\r\n   auto_encoder/sequential/conv2d_6/kernel_0\r\n   auto_encoder/sequential/conv2d_6/kernel_0/m\r\n   auto_encoder/sequential/conv2d_6/kernel_0/v\r\n   auto_encoder/sequential/conv2d_9/bias_0\r\n   auto_encoder/sequential/conv2d_9/bias_0/m\r\n   auto_encoder/sequential/conv2d_9/bias_0/v\r\n   auto_encoder/sequential/conv2d_9/kernel_0\r\n   auto_encoder/sequential/conv2d_9/kernel_0/m\r\n   auto_encoder/sequential/conv2d_9/kernel_0/v\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/beta_0\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/beta_0/m\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/beta_0/v\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/gamma_0\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/gamma_0/m\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1/gamma_0/v\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/beta_0\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/beta_0/m\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/beta_0/v\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/gamma_0\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/gamma_0/m\r\n   auto_encoder/sequential/residual/sequential_1/batch_normalization_v1_1/gamma_0/v\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/bias_0\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/bias_0/m\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/bias_0/v\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/kernel_0\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/kernel_0/m\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_1/kernel_0/v\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/bias_0\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/bias_0/m\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/bias_0/v\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/kernel_0\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/kernel_0/m\r\n   auto_encoder/sequential/residual/sequential_1/conv2d_2/kernel_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/beta_0\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/beta_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/beta_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/gamma_0\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/gamma_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_2/gamma_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/beta_0\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/beta_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/beta_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/gamma_0\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/gamma_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/batch_normalization_v1_3/gamma_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/bias_0\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/bias_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/bias_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/kernel_0\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/kernel_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_4/kernel_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/bias_0\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/bias_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/bias_0/v\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/kernel_0\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/kernel_0/m\r\n   auto_encoder/sequential/residual_1/sequential_2/conv2d_5/kernel_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/beta_0\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/beta_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/beta_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/gamma_0\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/gamma_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_4/gamma_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/beta_0\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/beta_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/beta_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/gamma_0\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/gamma_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/batch_normalization_v1_5/gamma_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/bias_0\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/bias_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/bias_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/kernel_0\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/kernel_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_7/kernel_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/bias_0\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/bias_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/bias_0/v\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/kernel_0\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/kernel_0/m\r\n   auto_encoder/sequential/residual_2/sequential_3/conv2d_8/kernel_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/beta_0\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/beta_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/beta_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/gamma_0\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/gamma_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_6/gamma_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/beta_0\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/beta_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/beta_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/gamma_0\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/gamma_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/batch_normalization_v1_7/gamma_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/bias_0\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/bias_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/bias_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/kernel_0\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/kernel_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_10/kernel_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/bias_0\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/bias_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/bias_0/v\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/kernel_0\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/kernel_0/m\r\n   auto_encoder/sequential/residual_3/sequential_4/conv2d_11/kernel_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/beta_0\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/beta_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/beta_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/gamma_0\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/gamma_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_8/gamma_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/beta_0\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/beta_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/beta_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/gamma_0\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/gamma_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/batch_normalization_v1_9/gamma_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/bias_0\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/bias_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/bias_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/kernel_0\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/kernel_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_13/kernel_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/bias_0\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/bias_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/bias_0/v\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/kernel_0\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/kernel_0/m\r\n   auto_encoder/sequential/residual_4/sequential_5/conv2d_14/kernel_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/beta_0\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/beta_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/beta_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/gamma_0\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/gamma_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_10/gamma_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/beta_0\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/beta_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/beta_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/gamma_0\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/gamma_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/batch_normalization_v1_11/gamma_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/bias_0\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/bias_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/bias_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/kernel_0\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/kernel_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_16/kernel_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/bias_0\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/bias_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/bias_0/v\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/kernel_0\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/kernel_0/m\r\n   auto_encoder/sequential/residual_5/sequential_6/conv2d_17/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_18/bias_0\r\n   auto_encoder/sequential_7/conv2d_18/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_18/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_18/kernel_0\r\n   auto_encoder/sequential_7/conv2d_18/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_18/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_31/bias_0\r\n   auto_encoder/sequential_7/conv2d_31/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_31/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_31/kernel_0\r\n   auto_encoder/sequential_7/conv2d_31/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_31/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose/bias_0\r\n   auto_encoder/sequential_7/conv2d_transpose/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose/kernel_0\r\n   auto_encoder/sequential_7/conv2d_transpose/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_1/bias_0\r\n   auto_encoder/sequential_7/conv2d_transpose_1/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_1/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_1/kernel_0\r\n   auto_encoder/sequential_7/conv2d_transpose_1/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_1/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_2/bias_0\r\n   auto_encoder/sequential_7/conv2d_transpose_2/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_2/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_2/kernel_0\r\n   auto_encoder/sequential_7/conv2d_transpose_2/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_2/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_3/bias_0\r\n   auto_encoder/sequential_7/conv2d_transpose_3/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_3/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_3/kernel_0\r\n   auto_encoder/sequential_7/conv2d_transpose_3/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_3/kernel_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_4/bias_0\r\n   auto_encoder/sequential_7/conv2d_transpose_4/bias_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_4/bias_0/v\r\n   auto_encoder/sequential_7/conv2d_transpose_4/kernel_0\r\n   auto_encoder/sequential_7/conv2d_transpose_4/kernel_0/m\r\n   auto_encoder/sequential_7/conv2d_transpose_4/kernel_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/beta_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/beta_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/beta_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/gamma_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/gamma_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_20/gamma_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/beta_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/beta_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/beta_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/gamma_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/gamma_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/batch_normalization_v1_21/gamma_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/bias_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/bias_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/bias_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/kernel_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/kernel_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_27/kernel_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/bias_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/bias_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/bias_0/v\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/kernel_0\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/kernel_0/m\r\n   auto_encoder/sequential_7/residual_10/sequential_12/conv2d_28/kernel_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/beta_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/beta_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/beta_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/gamma_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/gamma_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_22/gamma_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/beta_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/beta_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/beta_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/gamma_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/gamma_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/batch_normalization_v1_23/gamma_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/bias_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/bias_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/bias_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/kernel_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/kernel_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_29/kernel_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/bias_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/bias_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/bias_0/v\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/kernel_0\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/kernel_0/m\r\n   auto_encoder/sequential_7/residual_11/sequential_13/conv2d_30/kernel_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/beta_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/beta_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/beta_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/gamma_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/gamma_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_12/gamma_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/beta_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/beta_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/beta_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/gamma_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/gamma_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/batch_normalization_v1_13/gamma_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/bias_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/bias_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/bias_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/kernel_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/kernel_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_19/kernel_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/bias_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/bias_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/bias_0/v\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/kernel_0\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/kernel_0/m\r\n   auto_encoder/sequential_7/residual_6/sequential_8/conv2d_20/kernel_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/beta_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/beta_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/beta_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/gamma_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/gamma_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_14/gamma_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/beta_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/beta_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/beta_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/gamma_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/gamma_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/batch_normalization_v1_15/gamma_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/bias_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/bias_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/bias_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/kernel_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/kernel_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_21/kernel_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/bias_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/bias_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/bias_0/v\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/kernel_0\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/kernel_0/m\r\n   auto_encoder/sequential_7/residual_7/sequential_9/conv2d_22/kernel_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/beta_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/beta_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/beta_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/gamma_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/gamma_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_16/gamma_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/beta_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/beta_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/beta_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/gamma_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/gamma_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/batch_normalization_v1_17/gamma_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/bias_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/bias_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/bias_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/kernel_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/kernel_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_23/kernel_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/bias_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/bias_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/bias_0/v\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/kernel_0\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/kernel_0/m\r\n   auto_encoder/sequential_7/residual_8/sequential_10/conv2d_24/kernel_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/beta_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/beta_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/beta_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/gamma_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/gamma_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_18/gamma_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/beta_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/beta_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/beta_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/gamma_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/gamma_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/batch_normalization_v1_19/gamma_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/bias_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/bias_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/bias_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/kernel_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/kernel_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_25/kernel_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/bias_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/bias_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/bias_0/v\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/kernel_0\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/kernel_0/m\r\n   auto_encoder/sequential_7/residual_9/sequential_11/conv2d_26/kernel_0/v\r\n   logits\r\nimages\r\n   logits/image/0\r\n   logits/image/1\r\n   logits/image/2\r\nscalars\r\n   loss\r\ntensor -\r\n======================================================================\r\n\r\nEvent statistics for ./logs/2019-02-07_03:25:19:\r\naudio -\r\ngraph -\r\nhistograms\r\n   first_step           0\r\n   last_step            160\r\n   max_step             160\r\n   min_step             0\r\n   num_steps            17\r\n   outoforder_steps     []\r\nimages\r\n   first_step           160\r\n   last_step            160\r\n   max_step             160\r\n   min_step             160\r\n   num_steps            1\r\n   outoforder_steps     []\r\nscalars\r\n   first_step           0\r\n   last_step            150\r\n   max_step             150\r\n   min_step             0\r\n   num_steps            16\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\ntensor -\r\n======================================================================\r\n```", "@jpatts Please check tested [build configurations](https://www.tensorflow.org/install/source#linux). Please check Bazel version and CUDA. I think this error may be due to bazel version 0.21.0. For the system configuration you mentioned, Bazel 0.15.0 is supported. Need to downgrade CUDA/cuDNN. Could you try to install 0.15.0 version and check whether the issue persists? Thanks!", "@jpatts Please check a [solution](https://github.com/tensorflow/tensorflow/issues/25490#issuecomment-461410778) to similar issue like you have. The user downgraded and posted solution that worked for him. Please let me know how it progresses. Thanks!", "This is definitely not the reason, as like I said, there is no error message. Its a silent failure. I found that tf.contrib.summary.image() was failing depending on what the values of the input were. This is a serious problem because if it fails, I need to know why. Thus I think this is a design problem with tf.contrib.summary.image().", "Furthermore, I have noticed that it only fails (randomly) in my test function, not my train function. The main difference between the two is that train always has the same batch size, whereas test has the same batch size until it decreases for the last batch, as I run through all my test data exactly once. What I think is happening is that when the batch sizes differ in size, tf.contrib.summary.image() is unable to handle that, and instead overwrites or deletes all log information concerning images.", "@jpatts I understand your feelings. It is really difficult to find root-cause of the error when the installed version of some of the applications were not tested. If you can downgrade those applications or upgrade TF version, then we can try to find root-cause.  Meanwhile, we will see what we can do. Thanks and Have a good day!", "@jpatts Could you check whether you are able to run a simple deep learning code (mnist or any) without any errors? I want to make sure the error is not related to build/Installation and also not related to Tensorboard. Are you using any random seed? Is it possible to upload code to GitHub so that we can debug to see whether the issue is due to some randomness in the code. Thanks!", "@jvishnuvardhan all of my models work fine, and so does the rest of tensorboard. There is no chance that this is an issue with my specific system. Unfortunately I cannot show much code, asides from this:\r\n\r\n```\r\n# Training loop:\r\nbatch = self.data_tr.shuffle(self.size).batch(cfg.batch_size)\r\n# ..... do stuff ......\r\nwith tf.GradientTape() as tape:\r\n           # Approximate next frame\r\n           logits = self.model(x)\r\n           # ..... irrelevant code to this problem ......\r\n\r\nwith tf.contrib.summary.always_record_summaries():\r\n           tf.contrib.summary.image('logits', logits)\r\n\r\n# Testing loop:\r\nbatch = self.data_ts.shuffle(self.size).batch(cfg.batch_size)\r\n# ..... do stuff ......\r\nlogits = self.model(x)\r\nwith tf.contrib.summary.always_record_summaries():\r\n            tf.contrib.summary.image('logits', logits)\r\n```\r\n\r\nI only included the code that could affect the outcome of .image(). Note that logits is the exact same for both training and testing.\r\n\r\nAs you can see, there are two primary differences between the training and testing loops.\r\n1. The training loop has `with tf.GradientTape() as tape:`\r\n2. The testing loop is batched with a static batch size. This means unless data_ts % batch_size == 0, there will be a remainder, and thus the last batch will be of a size < batch_size\r\n\r\nThe problem is almost certainly being caused by 2., as the training images are always logged, whereas the testing images are only seldomly logged. Given the fact that my data_ts size is constantly fluctuating between runs, the most likely answer is that `tf.contrib.summary.image()` has an issue when logging data (N, H, W, C), where N is not constant for all data. The issue is likely that it simply overwrites or loses track of what data goes where in the timeline. This explains why the issue is not present during training, as N is constant.", "@jpatts Hello, \r\nI set the batch size to 1. But, I still cannot see any image in Tensorboard and the tab is not activated. But, I am able to use tf.contrib.summary.scalar with no difficulties.", "Hi @shahabty, so you are encountering the same problem then? If its not batch size I don't know what the problem would be.", "Actually changing batch size to 1 fixed the problem for me completely; I tried this over multiple runs. So this supports the argument I made above that a non-constant batch size will break this function, or at least cause it to delete all log information for images.", "@jpatts I have added tf.contrib.summary.image to the code. And the code works properly. The only problem is that the image tab is not activated in Tensorboard. I also tried to only visualize the input image but the same hidden bug happened again. \r\nDo you have any solution for the problem you have mentioned?", "From what I understand, if you take a look at tf.summary.image, it details that it does preprocessing to the image before logging it. I assume the tf.contrib.summary.image does the same. It\u2019s obvious that there are problems with this function, so I would try to log some other image data to make sure your input is correct, and make sure the batch sizes are the same for all data.", "Hi, I unassigned since I'm not the owner of this code. Perhaps someone from the TensorBoard can help?", "@jpatts it would be very helpful to have code that at least occasionally reproduces this, even if you can't reproduce it reliably.  It sounds like it was reliable enough that you could be confident a batch size of 1 fixed the problem which suggests that you do have code that reproduces the problem often enough to notice it?\r\n\r\nSome questions:\r\n- How are you calling `create_file_writer()`?\r\n- Does the problem still happen if you manually call `flush()` after every call to `image()`?\r\n- Have you noticed this with any other types of summaries or only `image()`?\r\n- Is there any pattern in the batch sizes that you see not being logged properly?  E.g. are they all larger batch sizes, for example?", "Hey, quick question answers:\r\n1. Check code below\r\n2. I have yet to try that\r\n3. Only image\r\n4. Issues occur at every batch size, including 1. However 1 seems to work the most often.\r\n\r\nHere is a code example that illustrates the exact problem occurring. Changing batch size of 1 to 3 will break the logging, as 100 % 3 != 0. However strangely enough changing size of the images dataset to something like 10 will also break logging for batches of 1....\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef log_state(images, name):\r\n    with tf.contrib.summary.record_summaries_every_n_global_steps(1, 0):\r\n        tf.contrib.summary.image(name, images)\r\n\r\n# Build Writers\r\nlog_path = \"./logs\"\r\nwriter = tf.contrib.summary.create_file_writer(log_path)\r\nwriter.set_as_default()\r\n\r\n# Preprocessing\r\nimages = []\r\nfor i in range(100):\r\n    images.append(np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32))\r\n\r\nimages = np.array(images)\r\ndata_tr = tf.data.Dataset.from_tensor_slices((images))\r\n\r\nfor epoch in range(1):\r\n    batch = data_tr.batch(1)\r\n    for image in batch:\r\n        log_state(image, \"image\")\r\n```", "@jpatts,\r\nSorry for the delayed response. Your code seems to work properly with `batch_size > 1` as well. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/d1058e4c18c323d346cd77ab093dbc51/gh_25592.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25592\">No</a>\n"]}, {"number": 25591, "title": "Bug in tf.image.resize_images", "body": "### System information\r\n\r\nI am running Ubuntu 18.0.4 on my PC with CUDA-10 and Python 3.6 and Pillow 5.4.1\r\n```\r\nlibcudnn7:\r\n  Installed: 7.4.1.5-1+cuda10.0\r\n  Candidate: 7.4.2.24-1+cuda10.0\r\n```\r\nwith Tensorflow version `b'v1.12.0-0-ga6d8ffae09' 1.12.0`\r\n\r\n\r\n### Describe the problem\r\nI am trying to resize an image with `tf.image.resize_images` using all available methods but I am getting completely crappy results for anything except NEAREST_NEIGHBOR\r\n\r\nFor example simply loading this image from the test set of Imagenet\r\n![ilsvrc2012_test_00000002](https://user-images.githubusercontent.com/4192637/52434595-fff10480-2b0f-11e9-8e2c-6358b81760fe.JPEG)\r\n\r\nI get colorful pixel salad if I put it through `resize_bilinear`\r\nsee code and output below.\r\n\r\n### Source code / logs\r\nTo reproduce place the code and the image file above in the same folder or adjust the file-path accordingly:\r\n```\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nimport time\r\nimport numpy as np\r\n\r\nfile_read = tf.read_file('ILSVRC2012_test_00000002.JPEG')\r\nloaded_image = tf.image.decode_jpeg(file_read, channels=3)\r\nreshape_image = tf.expand_dims(loaded_image, 0)\r\nresized_image = tf.image.resize_bilinear(reshape_image, size=[510, 520], align_corners=False)\r\nresized_image_v2 = tf.image.resize_images(reshape_image, size=[300, 320],\r\n                                          method=tf.image.ResizeMethod.BILINEAR, align_corners=False)\r\n# resized_image_v2 = tf.image.resize_images(reshape_image, size=[300, 320],\r\n                                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, align_corners=True)\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    orig_image = sess.run(loaded_image)\r\n    resized_v1 = sess.run(resized_image)\r\n    resized_v2 = sess.run(resized_image_v2)\r\n    orig_img = Image.fromarray(orig_image, 'RGB')\r\n    resized_v1_img = Image.fromarray(np.squeeze(resized_v1), 'RGB')\r\n    resized_v2_img = Image.fromarray(np.squeeze(resized_v2), 'RGB')\r\n    orig_img.show()\r\n    resized_v1_img.show()\r\n    resized_v2_img.show()\r\n    time.sleep(10)\r\n    orig_img.close()\r\n    resized_v1_img.close()\r\n    resized_v2_img.close()\r\n```\r\nThis produces for the original image:\r\n![original_image](https://user-images.githubusercontent.com/4192637/52435280-ba353b80-2b11-11e9-8a38-494195b5f3a4.jpg)\r\n\r\nfor the larger resize: \r\n![resize_large](https://user-images.githubusercontent.com/4192637/52435296-c4573a00-2b11-11e9-9c4b-cad0f1a4ea2d.jpg)\r\n\r\nfor the smaller resize:\r\n![resize_small](https://user-images.githubusercontent.com/4192637/52435319-d1742900-2b11-11e9-80b2-9e4261e5ab26.jpg)\r\n\r\nI get similar crappy results if I set `align_corners=True`\r\n\r\nHowever if I use `method=tf.image.ResizeMethod.NEAREST_NEIGHBOR` (i.e. the commented out line), I get a descent result\r\n![resize_small2](https://user-images.githubusercontent.com/4192637/52435620-9a524780-2b12-11e9-9140-4ff7a168a595.jpg)\r\n\r\nWith `BICUBIC` and `AREA` the results are similar pixel garbage as with `BILINEAR`\r\n\r\nAm I missing something here? This seems to be seriously broken.\r\n\r\n\r\n", "comments": ["Trying the same with OpenCV gives me perfectly reasonable results for both Bilinear and Nearest Neighbor\r\n```\r\nimport cv2\r\n\r\n\r\nfile_read = cv2.imread('ILSVRC2012_test_00000002.JPEG')\r\n\r\nresized_image_bl = cv2.resize(file_read, dsize=(300, 320), interpolation=cv2.INTER_LINEAR)\r\nresized_image_nn = cv2.resize(file_read, dsize=(300, 320), interpolation=cv2.INTER_NEAREST)\r\n\r\ncv2.imshow('original_image', file_read)\r\ncv2.imshow('bilinear image', resized_image_bl)\r\ncv2.imshow('NN image', resized_image_nn)\r\n\r\ncv2.waitKey(0)  # waits until a key is pressed\r\ncv2.destroyAllWindows()  # destroys the window showing image\r\n```", "Never mind. I simply did not catch that BILINEAR converts the Tensor automatically to tf.float32 ...\r\na simple numpy `astype(np.uint8)` on the output Tensor before creating the PIL image solves the mytery ...\r\nGrrrr\r\n\r\n", "@maxfiedler Thanks for resolving and posting the solution here. Thanks again!", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25591)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25591)\r\n"]}, {"number": 25589, "title": "Support for public Ambiq SDK", "body": "", "comments": []}, {"number": 25588, "title": "Object Detection: Build for TF Lite Demo completes successfully, but custom model fails with operations error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 29 x86_64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto Z3 Play\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: v1.12.0 or v1.13.0-rc0\r\n- Python version: 3.6 latest revision\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 8.2.1 20181215 (Red Hat 8.2.1-6)\r\n- CUDA/cuDNN version: none, TPU used for training\r\n- GPU model and memory: none, TPU used for training\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Model built using TPU training command listed in documentation for running on the cloud\r\n\r\n2. Model downloaded from Google Cloud storage bucket\r\n\r\n3. Exported model using:\r\n```bash\r\npython -m object_detection/export_tflite_ssd_graph \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$OUTPUT_DIR \\\r\n--add_postprocessing_op=true\r\n```\r\n\r\n4. Converted model using TOCO:\r\n```bash\r\nbazel run -c opt //tensorflow/contrib/lite/toco:toco \\\r\n--incompatible_package_name_is_a_function=false \\\r\n-- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,640,640,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=FLOAT \\\r\n--allow_custom_ops\r\n```\r\n\r\n5. Edited the Bazel BUILD file to include exported model, and edited DetectionActivity to include custom model and change to non-quantized ops.\r\n\r\n6. Built application using:\r\n```bash\r\nbazel build -c opt --config=android_arm64 --cxxopt='--std=c++11' \"//tensorflow/contrib/lite/examples/android:tflite_demo\"\r\n```\r\n\r\n7. Ran demo with Android Studio APK profiler, and observed the following error message using Logcat: (snipped for clarity)\r\n```\r\nCannot create interpreter: Didn't find custom op for name 'ResizeNearestNeighbor' with version 1\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere are all of the files that I have present:\r\n\r\n* Training Configuration: https://storage.cloud.google.com/robocubs-ml/debug/config/tpu.config\r\n* Checkpoint Files\r\n    * Checkpoint Descriptor: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/checkpoint\r\n    * Graph: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/graph.pbtxt\r\n    * Checkpoint Data: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.data-00000-of-00001\r\n    * Checkpoint Index: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.index\r\n    * Checkpoint Meta: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.meta\r\n     * Pipeline Config: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/pipeline.config\r\n* Saved Model\r\n     * Checkpoint Descriptor: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/checkpoint\r\n     * Frozen Inference Graph: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/checkpoint\r\n     * Checkpoint Data: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.data-00000-of-00001\r\n     * Checkpoint Index: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.index\r\n     * Checkpoint Meta: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.meta\r\n     * Pipeline Config: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/pipeline.config\r\n     * Saved Model: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/saved_model/saved_model.pb\r\n* TensorFlow Lite Files\r\n     * TFLite Graph: https://storage.cloud.google.com/robocubs-ml/debug/tflite/tflite_graph.pb\r\n     * TFLite Graph (PBTXT): https://storage.cloud.google.com/robocubs-ml/debug/tflite/tflite_graph.pbtxt\r\n     * Final TFLite Model: https://storage.cloud.google.com/robocubs-ml/debug/tflite/detect.tflite", "comments": ["ResizeNearestNeighbor should be supported now. Can you check out the source from head and try again? Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25588\">No</a>\n"]}, {"number": 25587, "title": "Could you please include small examples with the functions embedded in real code with the documentation?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0 preview\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCould you please include small examples with the functions embedded in real code with the documentation?\r\n\r\n**Will this change the current api? How?**\r\nMake the usage more elucidate.\r\n\r\n**Who will benefit with this feature?**\r\nAll the new and intermediate users.\r\n\r\n**Any Other info.**\r\nFor example, you can see this documentation API doc: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/audio/decode_wav\r\n\r\nThis is the function call, but there isn't any boilerplate code to show the sequence of its usage.  This is just one example.  All the function call should be similar.", "comments": ["Hi @shyamalschandra, \r\n\r\nThanks for your interest. This is a known issue. There are thousands of functions, it's just a question of prioritization.\r\n\r\nIf you have any examples like this that you'd like to contribute, these `api_docs` files are generated from the python docstrings. We should welcome submissions.\r\n\r\nBut I don't see added value in keeping this bug open.", "@MarkDaoust: Shouldn't you wait for more people to respond before negating the suggestion or feature request?", "@shyamalschandra I don't mean to give the impression that I'm negating your request!\r\n\r\nThe request is 100% valid. I agree that it's something we should work on.\r\n\r\nIssues in the tracker here however are more helpful if they're precise and achievable. Like \"better docs for tf/audio/decode_wav\"\r\n\r\nThis overall task would make sense to track with a spread-sheet and issues for the 100 most popular apis. This is just such a big task, that it doesn't fit in 1 of these bugs.  Who would we assign it to? How would we know if we're done?", "Would love to help with the documentation by writing some functional examples. Is there already a plan to work with the 100 most popular apis? @MarkDaoust ", "I believe @dynamicwebpaige is working on setting up a package to make doc sprints simpler for community groups.\r\n\r\n> Would love to help with the documentation by writing some functional examples.\r\n\r\nThe python api_docs are (mostly) generated from the from python doc-strings, and the pages on tensorflow.org (usually) link back to the right code page here.\r\n\r\nIf you see something wrong, and know how to fix it please send us a PR. It's usually pretty simple to get docs-fixes merged.", "What happened?  It has been more 235 days.  What happened?", "@Sri-vatsa : I don't know if there's a public list, but whatever's important to you may be important to someone else. If you send improvements to the python docstrings we'll accept them.\r\n\r\n@shyamalschandra : One actionable development is that  TensorFlow now encourages the [\"doctest\" format](https://docs.python.org/3/library/doctest.html) for code examples.  \r\n\r\nIf you use carets `>>>` instead of code fences \"```\" the example in the doc will be run as a unit test. Any contribution converting an example to a real runnable doctest example is encouraged."]}, {"number": 25586, "title": "Simplify python loops", "body": "This PR replaces loops over `range(len(...))` with `enumerate()` or `zip()`. This makes the code (arguably) easier to read and in some cases even slightly faster.", "comments": ["I resolved the merge conflicts and slimmed down the PR a bit, to make review easier. @rthadur Would you mind taking a look at it again?", "This PR currently just accumulates merge conflicts and doesn't add much value. I'm closing it for now and may extract some bits into smaller PRs."]}, {"number": 25585, "title": "Inconsistent results between tf1 and tf2-preview with Keras", "body": "I'm just trying to migrate a simple code from `keras 2.2.4` towards `tensorflow 2.0.0-preview`.\r\n\r\nLet's first consider the following input :\r\n\r\n```python\r\nimport numpy as np\r\n\r\nembedding_size = 4\r\nvocab_size = 10\r\n\r\nembedding_matrix = np.arange(embedding_size * vocab_size, dtype='float32')\r\nembedding_matrix = embedding_matrix.reshape(vocab_size, embedding_size)\r\nprint(embedding_matrix)\r\n# [[ 0.  1.  2.  3.]\r\n#  [ 4.  5.  6.  7.]\r\n#  [ 8.  9. 10. 11.]\r\n#  [12. 13. 14. 15.]\r\n#  [16. 17. 18. 19.]\r\n#  [20. 21. 22. 23.]\r\n#  [24. 25. 26. 27.]\r\n#  [28. 29. 30. 31.]\r\n#  [32. 33. 34. 35.]\r\n#  [36. 37. 38. 39.]]\r\n````\r\n\r\nAnd the network with just an input layer and an embedding output layer:\r\n\r\n```python\r\nx = Input(shape=[1], name='input')\r\nembedding = embedding_layer(x)\r\nmodel = Model(inputs=x, outputs=embedding)\r\n```\r\n\r\nSo mainly the only differences between both codes should be with the imports:\r\n\r\n```python\r\nfrom keras.layers import Embedding, Input\r\nfrom keras.models import Model\r\n````\r\n\r\nis changed to:\r\n\r\n```python\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Embedding, Input\r\nfrom tensorflow.keras.models import Model\r\n```\r\n\r\nNothing special about that.\r\n\r\nNow I'm trying to project a single integer label into the matching embedding vector:\r\n\r\n```python\r\nlabels_to_encode = np.array([[3]])\r\nmodel.predict(labels_to_encode)\r\n```\r\n\r\nIn the first case, with `tensorflow 1.9.0`, I get the desired and *correct* output : \r\n\r\n```python\r\narray([[[12., 13., 14., 15.]]], dtype=float32)\r\n```\r\n\r\nBut with `tensorflow 2.0.0-preview`, the weights are completely different and the output isn't correct:\r\n\r\n```python\r\narray([[[ 0.00646725,  0.00275606, -0.0351016 ,  0.01088192]]],\r\n      dtype=float32)\r\n```\r\n\r\nAny explanation for this ? Is this an expect behaviour ? Am I missing something or it's a possible bug ? ", "comments": ["Hi @eliasah, thanks for the issue! Could you share how you are creating `embedding_layer`?", "Oh sure @omalleyt12 ! Sorry, I thought it was there.\r\n\r\nSo here it is : \r\n\r\n```python\r\nimport numpy as np\r\n\r\nembedding_size = 4\r\nvocab_size = 10\r\n\r\nembedding_matrix = np.arange(embedding_size * vocab_size, dtype='float32')\r\nembedding_matrix = embedding_matrix.reshape(vocab_size, embedding_size)\r\n\r\nembedding_layer = Embedding(\r\n    output_dim=embedding_size, input_dim=vocab_size,\r\n    weights=[embedding_matrix], input_length=1)\r\n```", "Hmm, it looks like in both latest Keras and latest TF Keras, support for the `weights` argument no longer exists:\r\n\r\nhttps://github.com/keras-team/keras/blob/master/keras/layers/embeddings.py#L80\r\n\r\nI'm seeing the right results with this approach:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Embedding, Input\r\nfrom tensorflow.keras.models import Model\r\n\r\ntf.enable_eager_execution()\r\nimport numpy as np\r\n\r\nembedding_size = 4\r\nvocab_size = 10\r\n\r\ndef embeddings_initializer(*args, **kwargs):\r\n  embedding_matrix = np.arange(embedding_size * vocab_size, dtype='float32')\r\n  embedding_matrix = embedding_matrix.reshape(vocab_size, embedding_size)\r\n  return tf.Variable(embedding_matrix)\r\n\r\nembedding_layer = Embedding(\r\n    output_dim=embedding_size, input_dim=vocab_size,\r\n    embeddings_initializer=embeddings_initializer, input_length=1)\r\n\r\nx = Input(shape=[1], name='input')\r\nembedding = embedding_layer(x)\r\nmodel = Model(inputs=x, outputs=embedding)\r\n\r\nlabels_to_encode = np.array([[3]])\r\nmodel.predict(labels_to_encode)\r\n```\r\n\r\nWe should probably throw an error or at least a warning when unsupported args are passed\r\n", "This sounds good to me @omalleyt12 ! Thanks for your help. "]}, {"number": 25584, "title": "Reopen: Generalize MinMax monotonic optimizer", "body": "This PR generalizes the MinMax monotonic optimizer to support `SegmentMax`, `UnsortedSegmentMax` and `ArgMax` operations.\r\n\r\nThis is a rebased version of #25330 which was reverted due to https://github.com/tensorflow/tensorflow/pull/25330#issuecomment-460858086.\r\n\r\nThis PR surfaces a bug in the existing algorithmic optimizer which is fixed in #25535, thus #25535 should be merged prior to this.\r\n\r\n/cc @ezhulenev", "comments": ["#25535 has been merged and CI errors seem to be unrelated. This should be good to go now.", "Thanks for merging. You might want to also checkout PR #25455 and PR #25300 which add further optimizations."]}, {"number": 25583, "title": "TF Keras local_test dataformat compatible test case updated", "body": "TF Keras local_test dataformat compatible test case updated", "comments": ["@fchollet \r\n\r\nThanks for quick review, please review the review comment fix changes.", "Conflict resolve changes checkin done under PR #27670"]}, {"number": 25582, "title": "Load multiple TensorRT graphs. Cannot add function 'TRTEngineOp_0_native_segment' error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.2.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1080Ti\r\n\r\n\r\n**Describe the current behavior**\r\nI have optimized frozen graphs with tensorrt using example code from here: https://github.com/NVIDIA-AI-IOT/tf_trt_models\r\nEverything works fine. Inference works faster. \r\n\r\nIn my project, I have multiple models. And when I try to load both optimized graphs, following error occurs: **ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.** Separately models work fine though. Even if pipeline includes only one optimized and others are non-optimized - it also works fine.\r\n\r\nBoth models are ssd_inception_v2.\r\n\r\n**Other info / logs**\r\n```\r\n2019-02-07 10:56:31.606178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz\r\n2019-02-07 10:56:31.607938: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55ed1968ea70 executing computations on platform Host. Devices:\r\n2019-02-07 10:56:31.607999: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-02-07 10:56:31.612806: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcuda.so.1 locally\r\n2019-02-07 10:56:32.144695: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55ed1973dda0 executing computations on platform CUDA. Devices:\r\n2019-02-07 10:56:32.144755: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-02-07 10:56:32.145360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-02-07 10:56:32.145396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2019-02-07 10:56:32.257785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-07 10:56:32.257824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 \r\n2019-02-07 10:56:32.257833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N \r\n2019-02-07 10:56:32.258003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10445 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n/root/Recognizer2/weights/ssd_inception_v2_transport_feimani/inference/frozen_inference_graph.pb\r\n2019-02-07 10:56:32.870341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2019-02-07 10:56:32.870395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-02-07 10:56:32.870402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 \r\n2019-02-07 10:56:32.870409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N \r\n2019-02-07 10:56:32.870562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10445 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 427, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.\r\n```\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25582)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25582)\r\n"]}, {"number": 25581, "title": "Issue regarding import library for tensorflow-gpu 1.0.1 version", "body": "System information\r\n- OS Platform: Linux Ubuntu 15.10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: TensorFlow-GPU 1.0.1\r\n- Python version: 3.7\r\n- Installed using pip command: pip install tensorflow-gpu==1.0.1\r\n- Using anaconda jupyter for programming\r\n- CUDA/cuDNN version: 8\r\n- GPU model and memory:NVIDIA Corporation GM204 [GeForce GTX 970]\r\n\r\nI'm using tensorflow-gpu package to train seq2seq model on a dataset. But it gives error durring importing libraries.\r\n----------------------------------------------------------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/comsats/Desktop/saher_tariq/Panday_group/reaction_prediction_seq2seq-master/bin/train.py\", line 29, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n----------------------------------------------------------------------------------------------------------------------------------\r\nI have tried after re-installation of tensorflow but it gives same error. Model works fine when i had used only tensorflow 1.0.1 only for cpu March issue, but was very slow in working. So then i tried for tensorflow-gpu which gives me this error. And also this model scripts are compatible to tensorflow-gpu 1.0.1 version. As i got help to install the package from the discussion for the model working. https://github.com/pandegroup/reaction_prediction_seq2seq/issues/1 \r\nAny help relating to issue is appreciated. \r\nThank you.", "comments": ["@sahertariq07 Please check tested [build configurations](https://www.tensorflow.org/install/source#linux). Python version you are using is not supported for the TF version you are using. Please check Bazel version also. Another thing I noticed was libcudart.so.8.0 error. This was one of the common error and there are [solutions](https://github.com/tensorflow/tensorflow/issues/5343) in the GitHub and Stackoverflow etc. Please search \"ImportError: libcudart.so.8.0:\" in GitHub or elsewhere, you can find the solution. I think the CUDA path has some issues. Please let me know how it progresses. Thanks! ", "Thank you very much @jvishnuvardhan for your response. My issue is resolved. It was due to compatibility with my system's driver. I'm going to close this issue now.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25581)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25581)\r\n"]}, {"number": 25580, "title": "Edit Distance Matrix for CTC", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe input will be labels and the predicted output(from ctc greedy/beam decoder, SparseTensor). It will calculate the following 4 things:\r\n1. The edit distance between the output and the ground truth\r\n2. Substitution Matrix: It will store all the entries where a value was substituted in the output to reach the ground truth. Shape = [n_symbols, n_symbols]. Like the confusion matrix\r\n3. Insertion Matrix: It will store all the insertions to be made in the output to reach the grond truth. Shape = [n_symbols]\r\n4. Deletion Matrix: It will store all the deletions to be made in the output to reach the grond truth. Shape = [n_symbols]\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nThe target audience is everyone who uses edit distance(word error rate, letter error rate) as a metric and wants to know where the model is failing. The 3 matrices will be quite helpful to debug errors in the output. \r\n\r\n**Any Other info.**\r\nI'm willing to contribute. I can write it well in Python, C/C++. I want ideas from people and a little help in writing the tensorflow op", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 25579, "title": "Linear alpha shape does not match the number of input channels.Node number 9 (GpuDelegate) failed to prepare.", "body": "Dear TensorFlow developer,\r\n\r\nI implemented my custom model in my app. And I tried to set GPU delegate.  \r\nBut if I instantiate `Interpreter` with GPU delegate option, the following error occurs.  \r\n```\r\nLinear alpha shape does not match the number of input channels.Node number 9 (GpuDelegate) failed to prepare.\r\n```\r\nWhat does this error message suppose to mean?  \r\nOr is there anything I need to be careful on TFLite converting?  \r\nAnything would be helpful.  \r\nThanks!  \r\n\r\nThe model I used is here.\r\n[classify.tflite.zip](https://github.com/tensorflow/tensorflow/files/2839976/classify.tflite.zip)", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks!", "Thank you for your reply.  \r\nIssue template is shown below.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: on Simulator and on Samsung Galaxy S8\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 0.0.0-gpu-experimental\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Android Studio version: 3.3\r\n\r\n**Describe the problem**\r\nI implemented my custom model in my app following the [TensorFlow Lite GPU Delegate Tutorial](https://www.tensorflow.org/lite/performance/gpu).  \r\nThe code of essensial part is below. The model I used in this code is already attached on above comment.  \r\n```\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.experimental.GpuDelegate;\r\n\r\nGpuDelegate delegate = new GpuDelegate();\r\nInterpreter.Options options = (new Interpreter.Options()).addDelegate(delegate);\r\nInterpreter interpreter = new Interpreter(model, options);\r\n```\r\n\r\nWhen `interpreter` is instantiated, the following error occurs.  \r\n```\r\nLinear alpha shape does not match the number of input channels.Node number 9 (GpuDelegate) failed to prepare. \r\n```\r\n\r\nWhat does this error message suppose to mean?  \r\nNeed any special process when converting to tflite model in order to use GPU delegate?  \r\nThanks!", "@impjdi have you observed this before? ", "@jdduke No, I haven't seen this, but then again, I haven't seen models with PReLU flying by (that's where you get this error message).\r\n\r\n@ksekine Sorry for quite a belated response on this; I just got looped in.  The error message means that the tensor shapes and network don't match.  It was checking whether the tensor's channel size matches the linear alpha vector size in PReLU, but looks like there was a mismatch?", "Thanks for the information that PReLU is the cause.  \r\nI can work around this issue by removing PReLU from the model.  \r\nSo I'll close this issue if no one else is in trouble.  \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25579\">No</a>\n"]}]