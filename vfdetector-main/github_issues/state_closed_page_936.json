[{"number": 25362, "title": "Feature: TF Hub compatibility with TF 2.0.", "body": "**[TensorFlow Hub](https://www.tensorflow.org/hub/)** is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. \r\n\r\nTransfer learning can:\r\n- Train a model with a smaller dataset,\r\n- Improve generalization, and\r\n- Speed up training.\r\n\r\nThe purpose of this issue is to migrate modules in TF Hub into the TF 2.0 Keras API. Each migrated module must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["is there a way to import existing 1.x tf-hub models to tf 2.0 ?\r\n", "Status update: when building tensorflow and tensorflow_hub from HEAD, most hub 1.x modules can be loaded via `hub.load()` in eager mode. In which case one can use: \"m.signatures[\"default\"]\" to access the methods.", "Status update: tensorflow-hub 0.5 was released and together with tensorflow release 2.0.0b1 many old modules are now loadable with `hub.load()` and usable in eager mode.", "Awesome! Thanks for updating, @andresusanopinto. \ud83d\ude04 \r\n\r\nClosing out this ticket, and moving status to **Done** on the project tracker.", "Would be great to have a clean tutorial somewhere on how to use TensorFlow Hub with TF 2.0.\r\n\r\nFor now, this script works for me on older modules but it's not perfect:\r\n\r\n```python\r\nmodule_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/3\"\r\n\r\nmodule = hub.load(module_url, tags=[])\r\n# This is not very clean\r\nmodel = module.signatures[\"default\"]\r\n\r\nimages = tf.random.uniform((10, 224, 224, 3))\r\nfeatures = model(images)\r\n\r\n# `model` returns a dict so I need to get the default key\r\nfeatures = features[\"default\"]\r\n```", "This should not be closed until fully documented."]}, {"number": 25361, "title": "Feature: TensorFlow Lite compatibility with TF 2.0.", "body": "**Problem**\r\nTensorFlow 2.0 introduces features that are not supported by the conversion process used to convert TensorFlow models to TensorFlow Lite models. This issue will track the changes that must be made in order to support the conversion process in TensorFlow 2.0.\r\n\r\n**Goals:**\r\n\r\n- Add support for converting TensorFlow models generated in TensorFlow 2.0.\r\n- Continue to support conversion for SavedModels generated in TensorFlow 1.X.\r\n- Support conversion for Keras model files generated in TensorFlow 1.X and 2.0.\r\n\r\n**Non-goals:**\r\n\r\n- Continue to support conversion for frozen GraphDefs generated in TensorFlow 1.X. The related API will be deprecated in 2.0.\r\n", "comments": ["#25575 ", "What is the status of TFLite at this point? Are we able to use it with TF2 yet? I see this is marked as \"Done\" in the TF2 project, but I'd appreciate an explicit indicator on this. Thanks!", "Documentation on converting a 2.0 model to TFLite is available here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r2/convert/index.md", "The final documentation is available here: https://www.tensorflow.org/lite/r2/convert/.", "I see a commit on this in https://github.com/tensorflow/tensorflow/commit/d6a7c6a4a6894cfc2de38bab192d6f615071e419\r\nDoes it mean nightly has it working already?", "@reactivetype Correct. The converter is working in the nightly (`tf-nightly-2.0-preview`)."]}, {"number": 25360, "title": "Feature: TensorFlow.js compatibility with TF 2.0.", "body": "- Function inlining issue.\r\n- [SavedModel breaks in TensorFlow.js](https://github.com/tensorflow/tfjs/issues/1123).", "comments": ["@dynamicwebpaige, @allenlavoie\r\nCan we please close this issue since it is marked as done in TensorFlow 2.0? Thanks!"]}, {"number": 25359, "title": "Feature: Support for FP16 training in Keras.", "body": "", "comments": ["I am closing this issue as the `mixed precision` training is already part of recent `tf-nightly`. Please feel free to open any new issues if you find any error with `mixed precision` training. Thanks!"]}, {"number": 25358, "title": "Error occurs when training Mask R-CNN on dataset coco 2017, failed to run optimizer, stage RemoveStackStridedSliceSameAxis", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 64 bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow_gpu 1.13.0-rc0\r\n- Python version:  Python 3.6.7, pip 18.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0.130, libcudnn7-dev_7.4.2.24-1, libcudnn7_7.4.2.24-1\r\n- GPU model and memory: Nvidia TITAN Xp 12GB\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nb'v1.13.0-rc0-0-g6ce86799c8' 1.13.0-rc0\r\n\r\n**Describe the current behavior**\r\nHi, I'm trying to train Mask R-CNN model (https://github.com/matterport/Mask_RCNN) with dataset coco 2017, but it reports error as following, saying failed to run optimizer ArithmeticOptimizer, how to fix it, please help, thanks!\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nCode is available at https://github.com/matterport/Mask_RCNN \r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2019-01-31 10:11:33.060384: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n2019-01-31 10:11:33.060528: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n2019-01-31 10:11:43.572615: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n2019-01-31 10:11:43.572742: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n999/1000 [============================>.] - ETA: 0s - loss: 0.3451 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0632 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.16012019-01-31 10:24:40.241827: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n2019-01-31 10:24:40.241952: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice_37. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n2019-01-31 10:24:41.861193: W ./tensorflow/core/grappler/optimizers/graph_optimizer_stage.h:241] Failed to run optimizer ArithmeticOptimizer, stage RemoveStackStridedSliceSameAxis node proposal_targets/strided_slice. Error: ValidateStridedSliceOp returned partial shapes [1,?,?] and [?,?]\r\n1000/1000 [==============================] - 880s 880ms/step - loss: 0.3449 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.0729 - mrcnn_class_loss: 0.0631 - mrcnn_bbox_loss: 0.0454 - mrcnn_mask_loss: 0.1600 - val_loss: 2.0419 - val_rpn_class_loss: 0.1070 - val_rpn_bbox_loss: 0.9081 - val_mrcnn_class_loss: 0.4638 - val_mrcnn_bbox_loss: 0.2041 - val_mrcnn_mask_loss: 0.3590\r\nEpoch 6/12\r\n1000/1000 [==============================] - 799s 799ms/step - loss: 0.3522 - rpn_class_loss: 0.0037 - rpn_bbox_loss: 0.0779 - mrcnn_class_loss: 0.0551 - mrcnn_bbox_loss: 0.0508 - mrcnn_mask_loss: 0.1647 - val_loss: 1.5071 - val_rpn_class_loss: 0.0384 - val_rpn_bbox_loss: 0.8576 - val_mrcnn_class_loss: 0.1833 - val_mrcnn_bbox_loss: 0.1751 - val_mrcnn_mask_loss: 0.2529", "comments": ["This issue is more suitable for [Mask_RCNN repo](https://github.com/matterport/Mask_RCNN/issues). Please post it on Mask_RCNN repo since its not a bug or a feature request on TF side. If you think we have mistaken, then please provide a minimal reproducible test case to validate the issue reported here. Thanks!"]}, {"number": 25357, "title": "Feature: Unify argument names (~70).", "body": "", "comments": ["@dynamicwebpaige Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25356, "title": "Feature: TensorBoard compatibility with TF 2.0.", "body": "**Overview**\r\nThe `tf.summary` module from TF 1.x will be replaced by a new API that differs as follows:\r\n\r\n1. The **data-format-specific parts** will be defined in `tensorboard.summary`.\r\n2. The **generated summary events** will use a more extensible \"wire format\".\r\n3. The **write-side code** will use the V2 summary-writing API (today's `tf.contrib.summary`).\r\n\r\n**Background**\r\nToday three sets of summary ops exist: `tf.summary`, `tf.contrib.summary`, and `tb.summary` (aka `tensorboard.summary`).  They differ across two main dimensions: the \"summary-writing API\" which determines how user code calls the ops, and the \"wire format\" of how the summary data gets encoded in event files.  Both dimensions have legacy (V1) and new (V2) styles.  The proposed new summary ops fill in the (V2, V2) quadrant of this grid:\r\n\r\n|                                                | V1 (FileWriter, merge_summaries) | V2 (create_file_writer, eager-compatible) |\r\n|------------------------------------------------|----------------------------------|-------------------------------------------|\r\n| V1 (dedicated fields per type in tf.Summary)   | `tf.summary`    | `tf.contrib.summary`                        |\r\n| V2 (extensible tensor format, plugin metadata) | `tb.summary`  | [proposed] `tf.summary`  in TF 2.0, exporting symbols from `tb.summary`  |\r\n", "comments": ["#25524, #24632 ", "Are there any examples or docs in development about this? I am not at all clear on what the intended tensorboard usage is in (the evolving) tensorflow 2.0 api.", "@cottrell Yes, there are some new TensorBoard tutorials designed with the TF 2.0 alpha in mind on our new TensorBoard documentation site: https://www.tensorflow.org/tensorboard/r2/get_started", "Is there any example using the high-level Estimator's API? I can only find tutorials with keras API. If not, can anyone give an example with `LinearClassifier` for example?", "@HugoDLopes I believe the default Estimator usage should still work with TensorBoard as they did in 1.x - if you point TensorBoard at the model directory used by your Estimator, the default metrics should still appear.\r\n\r\nIt doesn't yet work out of the box to use the TF 2.0 tf.summary API in custom Estimator code unless you set up the writing logic yourself; the plan is to either make that work as well as 1.x or we can add tutorials.  But that's only for custom Estimator model functions, not canned estimators.", " @dynamicwebpaige Could you please have a look on this document [link](https://www.tensorflow.org/tensorboard/migrate) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25355, "title": "Library Conversion: Sonnet", "body": "**[Sonnet](https://github.com/deepmind/sonnet/tree/master/docs)** is an open-source library built on top of TensorFlow, created by researchers at **[DeepMind](https://deepmind.com/)**. The main purpose of Sonnet is to first construct Python objects which represent some part of a neural network, and then separately connect these objects into the TensorFlow computation graph.\r\n\r\nFor more information about Sonnet, check out [DeepMind's announcement](https://deepmind.com/blog/open-sourcing-sonnet/).\r\n\r\nThe purpose of this ticket will be to migrate the Sonnet library to the TF 2.0 API.\r\n\r\n- Convert all existing Sonnet code.\r\n- Run unit tests.\r\n- Ensure processes are well understood.", "comments": ["@tomhennigan is working on this AFAIK", "WIP!", "What about deepmind graph_nets? https://github.com/deepmind/graph_nets/", "@bhack - It looks like [**Graph Nets**](https://github.com/deepmind/graph_nets/) has dependencies on both **Sonnet** and **TensorFlow Probability** - which means that those libraries would need to be converted to TF 2.0 before Graph Nets could start migrating. \ud83d\ude0a\r\n\r\nIf you'd like to help test the upgrade process for any of the above, though, or to take a stab at porting TensorFlow 1.x code to 2.0, we would appreciate it!", "You can use now use Sonnet with TensorFlow 2.0 (code available on [this branch](https://github.com/deepmind/sonnet/tree/v2)). A package will be placed on PyPI within the next few months, and this comment will be updated with a link.\r\n\r\nTry it out and let us know what you think! \ud83d\ude04 "]}, {"number": 25354, "title": "Do not invoke ROCM toolchain unless -x rocm is specified.", "body": "Hi @whchung ,\r\n\r\nDo you think this is a reasonable change? I was trying to build //tensorflow/stream_executor/rocm:all, with all rocm libraries installed, but the machine doesn't have a GPU, and it can't run hcc due to libstdc++ version mismatch (it's not Ubuntu 16.04).\r\n\r\nSuch change will cause bazel to always invoke GCC, and it seems to be consistent with the intention of \"-x rocm\".", "comments": []}, {"number": 25353, "title": "Library Conversion: OpenSeq2Seq", "body": "**[OpenSeq2Seq](https://github.com/NVIDIA/OpenSeq2Seq)** is a toolkit for distributed and mixed-precision training of sequence-to-sequence (seq2seq) models, created by the researchers at NVIDIA.\r\n\r\nOpenSeq2Seq is built using TensorFlow and provides all the necessary building blocks for training encoder-decoder models for neural machine translation, automatic speech recognition, speech synthesis, and language modeling. For documentation and TensorFlow 1.x installation instructions, check [here](https://nvidia.github.io/OpenSeq2Seq/).\r\n\r\nThe purpose of this ticket will be to migrate OpenSeq2Seq to the TF 2.0 API.\r\n\r\n- Convert all existing OpenSeq2Seq code.\r\n- Run unit tests.\r\n- Ensure processes are well understood.\r\n\r\n", "comments": ["@dynamicwebpaige, could I have a see on this for my first contribution?", "Thanks @dynamicwebpaige for filing this issue. Since the conversion work will be a collaboration between me and Boris on Nvidia side, I will cc him once I got his github user name. Also the code will be committed to Nvidia's repo instead of tensorflow.", "https://github.com/tensorflow/addons/pull/72", "Hi @dynamicwebpaige @qlzh727 @borisgin\r\nIs Nvidia's OpenSeq2Seq still being migrated to TF 2.0 ? \r\nI have re-implemented our AV speech toolkit for this latest version of tensorflow using tensorflow/addons/seq2seq (thanks @qlzh727 for all your help so far), yet I am facing a memory leak issue. Was wondering if I could have a look at some code samples in OpenSeq2Seq.\r\n\r\nThanks,\r\nGeorge"]}, {"number": 25351, "title": "Unification with return indentation of other functions.", "body": "Listing the return values like the _ConcatGrad function or the _StrideSliceGrad function in the same file seems to be readable.", "comments": ["Thanks @jayhpark530. I think it's inconsistent because the signature fits in one line. I hope our linter doesn't revert this during import.", "Looks like our linter did revert this change. I will close this PR. Thanks @jayhpark530 "]}, {"number": 25350, "title": "Library Conversion: TF Probability", "body": "**[TensorFlow Probability](https://www.tensorflow.org/probability/)** (TFP) is a Python library built on TensorFlow that makes it easy to combine probabilistic models and deep learning on modern hardware (TPU, GPU). It's for data scientists, statisticians, ML researchers, and practitioners who want to encode domain knowledge to understand data and make predictions. TFP includes:\r\n\r\n- A wide selection of probability distributions and bijectors.\r\n- Tools to build deep probabilistic models, including probabilistic layers and the Edward2 language.\r\n- Variational inference and Markov chain Monte Carlo.\r\n- Optimizers such as Nelder-Mead, BFGS, and SGLD.\r\n\r\nThe purpose of this ticket will be to migrate TF Probability to the TF 2.0 API.\r\n\r\n- Convert TF Probability library.\r\n- Run unit tests.\r\n- Ensure processes are well understood.\r\n", "comments": ["This has been fixed, thanks to @jvdillon @jaingaurav and others!", "Is this currently in the nightly build?", "@zhangandyx: Yes please use the tfp-nightly pip package. @jvdillon might be able to comment more on the specific release plans.", "We can cut a stable release after tf 1.14\n\nOn Wed, Mar 13, 2019, 5:22 PM Gaurav Jain <notifications@github.com> wrote:\n\n> @zhangandyx <https://github.com/zhangandyx>: Yes please use the\n> tfp-nightly pip package. @jvdillon <https://github.com/jvdillon> might be\n> able to comment more on the specific release plans.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25350#issuecomment-472656593>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABFZtkWQTwnREO4ZIPYAJMv5RfSSwElnks5vWZY2gaJpZM4abcgL>\n> .\n>\n"]}, {"number": 25349, "title": "Library Conversion: Open AI Baselines", "body": "**[OpenAI Baselines](https://github.com/openai/baselines)** is a set of high-quality implementations of reinforcement learning algorithms. These algorithms make it easier for the research community to replicate, refine, and identify new ideas, and creates solid baselines to build research on top of.\r\n\r\nThe purpose of this ticket will be to migrate the OpenAI Baselines library to the TF 2.0 API.\r\n\r\n- Convert OpenAI Baselines library.\r\n- Run unit tests.\r\n- Ensure processes are well understood.\r\n\r\nFor more information on OpenAI baselines, check out their blog post [here](https://blog.openai.com/openai-baselines-dqn/) - and be sure to amble around their [website](https://openai.com/), as well as the OpenAI [gym](http://gym.openai.com/)!", "comments": ["I believe even google [dopamine](https://github.com/google/dopamine) needs an update to tf 2.0 ", "Hi I have previously worked on some projects with OpenAI Baselines, can I have a try on this?", "Adding @tanzhenyu as an assignee here because he's been taking a lead on this & on coordinating with the OpenAI team. Zhenyu, do you have any part of the migration in mind that you feel open source contributors would be able to assist with right now?", "Sorry for the delay. Yes it'd be great to get some help from external contributors. @ffahleraz would you be able to help on one or two algos?", "@tanzhenyu I'd be happy to assist as well! I could help with an algorithm or two. Would that be useful?", "@josiahbjorgaard Yep that'd be very helpful. The discussion with OpenAI is to first port things to my own repo. I have already done so with PPO/A2C/DDPG, and working on DQN right now. We can use help on 1) GAIL, 2) ACER, 3) ACKTR, 4) HER, 5) TRPO.\r\nHere's my repo: https://github.com/tanzhenyu/baselines-tf2. You can branch and make PR directly, and once everything is finalized we can port to OpenAI master branch.\r\nI have written most util methods that are required, so hopefully the work required for each algo is stand-alone. @josiahbjorgaard @ffahleraz Let me know what algos you want to grab. ", "@tanzhenyu I'll start on ACER\r\n", "> I believe even google [dopamine](https://github.com/google/dopamine) needs an update to tf 2.0\r\n\r\n[TF-Agents](https://github.com/tensorflow/agents) is prioritized over Dopamine - @dynamicwebpaige. \r\n\r\nIf anyone is interested, there's also Stable Baselines - a TensorFlow-based fork of OpenAI's Baselines: https://github.com/Stable-Baselines-Team/stable-baselines with ACER, A2C, ACKTR, HER, PPO2, SAC etc. Full blog post [here](https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82).", "@tanzhenyu I will start on ACKTR, if no one works already on this.\r\n\r\n", "> @tanzhenyu I will start on ACKTR, if no one works already on this.\r\n\r\nGo for it @MoritzTaylor - handing the ownership over to you - you may be a lot quicker \ud83d\udc4d", "@MoritzTaylor Sounds good. Have fun and let me know how I could help!", "I will start working on GAIL! :rocket: ", "@MoritzTaylor @josiahbjorgaard @seungjaeryanlee \r\nGently ping to check the status on your end :-)\r\n\r\nSome update from my side: I have merged the current local repo into baselines [tf2 branch](https://github.com/openai/baselines/pull/978), you could either sync from there or sync from my repo (since I will probably continue working on hindsight experience replay).\r\n\r\nAnd let me know how I can help!", "@tanzhenyu \r\nI have converted most of the files. The only thing I still need to convert is the Kronecker Factor Optimizer, which is a little bit harder. After I finished to convert the optimizer I want to do some testing. I'll try to complete the conversion in the next few days.\r\n\r\nI sync already from your repo to be up to date with your commits.", "@MoritzTaylor Ah yes, KFAC is a great optimizer and unfortunately we don't include them in TF core.", "@josiahbjorgaard @seungjaeryanlee Gently ping and let me know if you need additional help.", "@tanzhenyu Somebody else should take over mine. I haven't had a chance to get anywhere with it.", "@josiahbjorgaard Alright.", "> > I believe even google [dopamine](https://github.com/google/dopamine) needs an update to tf 2.0\r\n> \r\n> [TF-Agents](https://github.com/tensorflow/agents) is prioritized over Dopamine - @dynamicwebpaige.\r\n> \r\n> If anyone is interested, there's also Stable Baselines - a TensorFlow-based fork of OpenAI's Baselines: https://github.com/Stable-Baselines-Team/stable-baselines with ACER, A2C, ACKTR, HER, PPO2, SAC etc. Full blog post [here](https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82).\r\n\r\nYes, we would like also to support tf 2.0 in the future for stable baselines.\r\nIn short, it is OpenAI baselines with a nice API (scikit learn like) + documentation + new algorithms (SAC, TD3)\r\nThe main repo url: https://github.com/hill-a/stable-baselines\r\nDocumentation: https://stable-baselines.readthedocs.io/en/master/\r\n\r\nCorresponding issue: https://github.com/hill-a/stable-baselines/issues/366\r\n\r\n@dynamicwebpaige Should I open a new issue for that?", "@tanzhenyu if there are any open issues that a community contributor can help with, I'd be more than happy to help!", "@avnishn can you help with converting [ACER](https://github.com/openai/baselines/tree/master/baselines/acer) to TF2?", "Sure thing! Is there a date that you need it by? @tanzhenyu ", "@avnishn Probably in a month or two. I don't have much bandwidth to make this at the moment, but ACER should be pretty straightforward compared to other algs. Let me know if you run into any issues.", "@tanzhenyu \r\nSorry that I did not finished already, even I said last time, that I am finish soon. Fortunately I have more time now.\r\nI reach out because I want to ask if it is possible to rewrite KFAC from ground up, since it is not as obvious to transform the TF1 code of it to TF2 as I thought? I think rewriting it, could be easier here, since I've tried to preserve most of the main code so far.", "@MoritzTaylor Yes that makes sense to rewrite while maintaining most of the main logic.", "@tanzhenyu It seems nobody has taken TRPO, I'd be happy to help with that or any other alogs.", "> @tanzhenyu It seems nobody has taken TRPO, I'd be happy to help with that or any other alogs.\r\n\r\n[Here](https://github.com/openai/baselines/tree/tf2/baselines/trpo_mpi). ", "@tanzhenyu it seems TRPO is rewritten. If there are any open issues that I can help with, I'd be very happy to help!", "> @tanzhenyu\r\n> \u5bf9\u4e0d\u8d77\uff0c\u6211\u8fd8\u6ca1\u5b8c\u6210\uff0c\u5373\u4f7f\u6211\u4e0a\u6b21\u8bf4\u8fc7\uff0c\u6211\u4e5f\u5feb\u5b8c\u6210\u4e86\u3002\u5e78\u8fd0\u7684\u662f\u6211\u73b0\u5728\u6709\u66f4\u591a\u65f6\u95f4\u3002\r\n> \u6211\u4f38\u51fa\u63f4\u624b\u662f\u56e0\u4e3a\u6211\u60f3\u95ee\u662f\u5426\u6709\u53ef\u80fd\u4ece\u5934\u5f00\u59cb\u91cd\u5199KFAC\uff0c\u56e0\u4e3a\u5c06\u5176TF1\u4ee3\u7801\u8f6c\u6362\u4e3aTF2\u5e76\u4e0d\u50cf\u6211\u60f3\u7684\u90a3\u4e48\u660e\u663e\uff1f\u6211\u8ba4\u4e3a\u5728\u8fd9\u91cc\u91cd\u5199\u5b83\u53ef\u80fd\u4f1a\u66f4\u5bb9\u6613\uff0c\u56e0\u4e3a\u5230\u76ee\u524d\u4e3a\u6b62\u6211\u4e00\u76f4\u5c1d\u8bd5\u4fdd\u7559\u5927\u591a\u6570\u4e3b\u8981\u4ee3\u7801\u3002\r\n\r\nHave you finished KFAC yet? I cloned your KFAC.py and got an error \r\nAttributeError: 'KfacOptimizer' object has no attribute '_update_stats_op'", "@araffin It seems that you don't have big plans to upgrade stable-baselines to TF 2, although you already started another Github project, but you are apparently investing your time more on stable-baselines-3 with PyTorch. TF 2.0 is more widely used than PyTorch, so I feel this is somehow a mistake.", "> @araffin It seems that you don't have big plans to upgrade stable-baselines to TF 2, although you already started another Github project, but you are apparently investing your time more on stable-baselines-3 with PyTorch. TF 2.0 is more widely used than PyTorch, so I feel this is somehow a mistake.\r\n\r\ni already replied to you in the SB issue, but for others the answers about our choice to use PyTorch are in https://github.com/hill-a/stable-baselines/issues/366 and https://github.com/hill-a/stable-baselines/issues/733", "I'm gonna close this issue, given the OpenAI Baseline conversions is complete under branch tf-2. We don't have intermediate plans to support StableBaselines."]}, {"number": 25348, "title": "Library Conversion: TensorRT", "body": "`tf.contrib` will be deprecated in TF 2.0, which means that TensorRT must migrate to a new home! The purpose of this ticket will be to migrate NVIDIA's TF-TRT library to the TF 2.0 API.\r\n\r\n- Convert TF-TRT library.\r\n- Run unit tests.\r\n- Ensure processes are well understood.\r\n\r\nTensorFlow integration with TensorRT (TF-TRT) optimizes and executes compatible subgraphs, allowing TensorFlow to execute the remaining graph. While you can still use TensorFlow's wide and flexible feature set, TensorRT will parse the model and apply optimizations to the portions of the graph wherever possible. Therefore, the workflow includes importing a trained TensorFlow model (graph and weights), freezing the graph, creating an optimized graph with TensorRT, importing it back as the default graph, and running inference. [[0](https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html)]\r\n\r\nFor the TensorFlow 1.x implementation of TF-TRT, check [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tensorrt).\r\n\r\n[0] [NVIDIA Deep Learning Frameworks Documentation](https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html) (2019)\r\n", "comments": ["TF-TRT is supported in TF 2.0 alpha, and here is a [simple example](https://github.com/aaroey/tensorflow/blob/tftrt20/tftrt20/test.py) for that.", "I tried to run the simple example from @aaroey with tensorflow 2.0.0-beta0 and this raises an error: \r\n\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/envs/Deep2/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 427, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input 1 of node StatefulPartitionedCall was passed float from conv1/kernel:0 incompatible with expected resource.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorrt_test.py\", line 50, in <module>\r\n    converter.save(saved_model_dir_trt)\r\n  File \"/anaconda3/envs/Deep2/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 822, in save\r\n    super(TrtGraphConverter, self).save(output_saved_model_dir)\r\n  File \"/anaconda3/envs/Deep2/lib/python3.6/site-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 432, in save\r\n    importer.import_graph_def(self._converted_graph_def, name=\"\")\r\n  File \"/anaconda3/envs/Deep2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/anaconda3/envs/Deep2/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 431, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Input 1 of node StatefulPartitionedCall was passed float from conv1/kernel:0 incompatible with expected resource.\r\n\r\ndid I miss something?\r\n", "I have the same issue as @Pidem have.", "My guess is that this is related to the TensorRT version that Tensorflow was build with ?@nocotan @aaroey \r\n", "@Pidem @nocotan we made some changes since  2.0-beta0, so I just updated the test script. Would you please retry with tf-nightly-gpu-2.0-preview? Thanks.", "@aaroey It does run ! thanks. ", "on a similar note, @aaroey , I think the following lines of code should return the value \"True\". \r\n\r\nfrom tensorflow.compiler.tf2tensorrt.wrap_py_utils import is_tensorrt_enabled\r\nprint(is_tensorrt_enabled())\r\n", "@aaroey I tried running your example on [this Colab Notebook](https://colab.research.google.com/drive/14iTHCILtUDf3TxrfV6NfHkthoQYmKrgK) but surprisingly, Colab crashes. Your inputs here would be very helpful. However, when I ran it on my GCP CE instance (that contains a V100) it was smooth. \r\n\r\n![image](https://user-images.githubusercontent.com/22957388/69046977-2be6d200-0a20-11ea-9d0d-e8642b17f444.png)\r\n", "@sayakpaul could you share the crash log?", "I tried using the sample from this[Colab](https://colab.research.google.com/github/vinhngx/tensorrt/blob/vinhn-tf20-notebook/tftrt/examples/image-classification/TFv2-TF-TRT-inference-from-Keras-saved-model.ipynb?hl=en#scrollTo=lFKQPoLO_ikd)\r\n\r\nStructure of my inference graph is:\r\n\r\n` print(infer.structured_outputs)\r\n{'conv2d_101': TensorSpec(shape=<unknown>, dtype=tf.float32, name='conv2d_101'), 'conv2d_109': TensorSpec(shape=<unknown>, dtype=tf.float32, name='conv2d_109'), 'conv2d_93': TensorSpec(shape=<unknown>, dtype=tf.float32, name='conv2d_93')}`\r\n\r\nI am trying to convert YoloV4 based on keras from this [repo](https://github.com/taipingeric/yolo-v4-tf.keras)\r\n\r\nBut as per the colab when I used infex(input_1= x)  i get the following error:\r\n\r\n`  [[{{node StatefulPartitionedCall/model_1/conv2d/Conv2D}}]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1669, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1679, in _call_impl\r\n    cancellation_manager)\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1762, in _call_with_structured_signature\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/saved_model/load.py\", line 116, in _call_flat\r\n    cancellation_manager)\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/mnt/d/Testing/research/yolo-v4/yolo-v4/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n         [[{{node StatefulPartitionedCall/model_1/conv2d/Conv2D}}]]\r\n         [[PartitionedCall/TRTEngineOp_0_1]] [Op:__inference_signature_wrapper_130807]\r\n\r\nFunction call stack:\r\nsignature_wrapper`"]}, {"number": 25347, "title": "Fix variables that seem to not be modified to consts.", "body": "Based on this when I pointed out this to be const but it was too late, it was merging soon: https://github.com/tensorflow/tensorflow/pull/25269#discussion_r252492749\r\n\r\nPlease guide me incase I can make this even better.\r\n\r\n", "comments": ["> Yay for const!\r\n\r\nThanks =P, Glad someone else is also as happy as me to see ```const```."]}, {"number": 25345, "title": "Fix wheel building.", "body": "This pr fixes an issue with the toco package that was preventing build. It is likely that during migration $(location ) call accidentally dropped and genrule command-line was created using bazel target label instead of the binary. This was causing build to fail.", "comments": ["I saw same error. This PR fixed it.\r\nERROR: /home/jenkins/workspace/TensorFlow_PPC64LE_CPU_Build/tensorflow/lite/python/testdata/BUILD:20:1: Executing genrule //tensorflow/lite/python/testdata:permute_uint8 failed (Exit 1)\r\n/bin/bash: //tensorflow/lite/toco:toco: No such file or directory", "This looks correct to me so I'll approve it. @angersson @aselle FYI.", "Yeah, looks good. Thanks for this.", "@angersson do you need to do something else to approve? Github seems to require approval.", "Sorry about that. Done.", "@samikama \"import/copybara\" says import didn't affect any internal file , could you please rebase your branch once ", "I think the fix already has been done internally is what that means.", "Yes, it seems another pr was made 6 hours ago.", "> I think the fix already has been done internally is what that means.\r\n\r\n@aselle ,Thank you, so we close this PR or merge manually ?", "It would be nice if you can streamline fixes such as this from the internal repos as these are blockers for many people working on master and we wouldn't have to waste time identifying and providing a fix for them and wait for PRs to be approved.", "closing this PR as changes were done internally"]}, {"number": 25344, "title": "2.0 Reference Models: NCF Model (1 GPU, 8 GPU with dist strat and Keras)", "body": "This is an implementation of the **Neural Collaborative Filtering** (NCF) framework with Neural Matrix Factorization (NeuMF) model as described in the [Neural Collaborative Filtering](https://arxiv.org/abs/1708.05031) paper. Current implementation is based on the code from the authors' [NCF code](https://github.com/hexiangnan/neural_collaborative_filtering) and the Stanford implementation in the [MLPerf Repo](https://github.com/mlperf/reference/tree/master/recommendation/pytorch).\r\n\r\n**Neural Collaborative Filtering** is a general framework for collaborative filtering of recommendations in which a neural network architecture is used to model user-item interactions. Unlike traditional models, NCF does _not_ resort to Matrix Factorization (MF) with an inner product on latent features of users and items. It replaces the inner product with a multi-layer perceptron that can learn an arbitrary function from data.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/blob/6518c1c7711ef1fdbe925b3c5c71e62910374e3e/official/recommendation/README.md).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["Closing this out - losses are comparable, as is time to train for the 2 GPU multi-worker example."]}, {"number": 25343, "title": "2.0 Reference Models: NMT Model (1 GPU, 8 GPU with dist strat and Keras)", "body": "**Sequence-to-sequence** (seq2seq) models ([Sutskever et al., 2014](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf), [Cho et al., 2014](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)) have enjoyed great success in a variety of tasks such as machine translation, speech recognition, and text summarization. \r\n\r\nThis model will give TF 2.0 end users a full understanding of seq2seq models and show them how to build a competitive seq2seq model from scratch. We focus on the task of **Neural Machine Translation** (NMT), which was the very first [testbed for seq2seq models](https://research.googleblog.com/2016/09/a-neural-network-for-machine.html).\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/nmt).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": []}, {"number": 25342, "title": "2.0 Reference Models: Transformer (1 GPU, 8 GPU with dist strat and Keras)", "body": "**Transformer** is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.\r\n\r\nSee the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762) for more background.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/transformer).\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.", "comments": ["The code is done, we need better documentation for usage before we close this bug. ", "Transformer V2 initial documentation is checked in with usage guide.\r\nWe can close this bug. Tody and the team will work on adding more stats there.", "Thanks @saberkun ! "]}, {"number": 25341, "title": "2.0 Reference Models: Keras Application Set (1 GPU)", "body": "Keras Applications is the `applications` module of the Keras deep learning library. It provides model definitions and pre-trained weights for a number of popular architectures, such as VGG16, ResNet50, Xception, MobileNet, and more.\r\n\r\nThe purpose of this issue is to migrate models into the TF 2.0 Keras API. Each migrated model must be eager and distribution compatible, with tests, and all associated engineering artifacts.\r\n\r\nFor more information on tf.keras.applications, check [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications).", "comments": ["Does this mean that tf-nightly-gpu-2.0 does not have keras yet? \r\n![image](https://user-images.githubusercontent.com/4762399/52161347-3214f780-2707-11e9-9710-28666a8d52fd.png)\r\n", "@Worthy7 You should still be able to use Keras in the nightly GPU preview of TF 2.0; full API reference docs are listed [here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf).\r\n\r\nWhich version is listed when you try `tf.__version__`?", "\r\n![image](https://user-images.githubusercontent.com/4762399/52162808-85457500-271c-11e9-822b-500df7b9c4a8.png)\r\n\r\nDo I need to ALSO have tf-nightly normal installed too? I thought CPU and GPU were standalone packages?\r\n\r\n![image](https://user-images.githubusercontent.com/4762399/52162817-a5753400-271c-11e9-90a0-1fa17997ba71.png)\r\n\r\nI just have tf-nightly-2-gpu installed (which installed the estimator pack too)\r\n![image](https://user-images.githubusercontent.com/4762399/52162934-05200f00-271e-11e9-864d-f642ccb1028d.png)\r\n\r\nFYI, if I install tf-nightly-2-cpu, everything works, but it doesn't use my gpu. So I am not sure what I'm doing wrong here. Btw the error also happens if you clone this notebook and just change it to the GPU verison in the imports section: \r\n\r\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/dcgan.ipynb\r\n\r\nAll I am trying to do is run this example on GPU.", "It seems my notebook was in some kind of corrupt state with me installing/removing packages.\r\nIn the end I think this issue was ultimately caused by a combination of me not having Cuda 10 properly set up, and other versions of tf being held in python memory/not uninstalled properly.", "@Worthy7 - Glad to hear it, and sorry for the frustration! Is everything working as expected now?", "Actually no, I tried to install this on a different machine, and I can't\r\neven get the package because it seems to perhaps be lacking some\r\ndependencies? AFAIK, tf-nightly-2-gpu-preview only needs the estimator 2.0\r\npackage right?\r\nBecause I get this error:\r\n\r\n`Could not install packages due to an EnvironmentError: [Errno 2] No such\r\nfile or directory:\r\n'C:\\\\Users\\\\Ian\\\\AppData\\\\Local\\\\Temp\\\\pip-install-5nwjiit3\\\\tf-nightly-gpu-2.0-preview\\\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190205.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/Eigen/src/IterativeLinearSolvers/LeastSquareConjugateGradient.h'`\r\n", "@Worthy7 - Could you try this [solution](https://github.com/tensorflow/tensorflow/issues/24886) that worked for another user and let us know? Sometimes there are path length limits on Windows.", "That fixed it! Thanks @dynamicwebpaige. \r\nNow.. I did actually get the same issue again as I had originally. It seems that just the Tensorflow namespace exists (so it imports) but there are no functions (so I couldn't even run __version__).\r\n\r\nI reinstalled the packages after removing anything else related to tf, and it seems to be working again.\r\n", "@Worthy7 Excellent! Feel free to reach out by email if you run into any other snags. \ud83d\ude04 ", "I'm encountering the same error mentioned above by Worthy7. I've applied the path length solution suggested in the link provided by dynamicwebpaige, but this didn't solve the issue. If you read the path generating the error, the base of the path is using windows double back slashes, while the appended part of the path uses posix forward slashes to separate directories. Is it possible that this is the source of the issue? If so, how can I fix it?\r\n\r\n```\r\nC:\\Users\\asus>pip install tf-nightly-gpu-2.0-preview\r\nCollecting tf-nightly-gpu-2.0-preview\r\n  Using cached https://files.pythonhosted.org/packages/39/a9/5adba3fe991a703600f50e1edd186953e08efae9d517cae8a81c5b15d2eb/tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64.whl\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\asus\\\\AppData\\\\Local\\\\Temp\\\\pip-install-f6tbv4z9\\\\tf-nightly-gpu-2.0-preview\\\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190327.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/Eigen/src/Core/products/GeneralMatrixMatrixTriang\r\nular_BLAS.h'\r\n\r\n```\r\n", "After reinstalling things the right way, you might find it simply doesn't\nwork because of some kind of cache somewhere (it seems).\n\nI had to restart jupyter to see the changes to my packages sometimes.\n\nOn Tue, 2 Apr 2019, 01:14 dorian821, <notifications@github.com> wrote:\n\n> I'm encountering the same error mentioned above by Worthy7. I've applied\n> the path length solution suggested in the link provided by dynamicwebpaige,\n> but this didn't solve the issue. If you read the path generating the error,\n> the base of the path is using windows double back slashes, while the\n> appended part of the path uses posix forward slashes to separate\n> directories. Is it possible that this is the source of the issue? If so,\n> how can I fix it?\n>\n> C:\\Users\\asus>pip install tf-nightly-gpu-2.0-preview\n> Collecting tf-nightly-gpu-2.0-preview\n>   Using cached https://files.pythonhosted.org/packages/39/a9/5adba3fe991a703600f50e1edd186953e08efae9d517cae8a81c5b15d2eb/tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64.whl\n> Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\asus\\\\AppData\\\\Local\\\\Temp\\\\pip-install-f6tbv4z9\\\\tf-nightly-gpu-2.0-preview\\\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190327.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/Eigen/src/Core/products/GeneralMatrixMatrixTriang\n> ular_BLAS.h'\n>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25341#issuecomment-478642977>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEirH0xlJXIwpEHXfF0mn3sEhNKDPVEWks5vcjBegaJpZM4abZJp>\n> .\n>\n", "Thanks, turns out the workaround suggested here doesn't work on Windows 7.  I attempted the solution suggested in the link below, but encountered another path error. Looks like I'll have to switch to Windows 10.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/24834", "@dynamicwebpaige We are checking to see if you still need help on this issue. There is a high possibility that this was fixed with later TF versions. We recommend that you upgrade to 2.7 which is latest stable version of TF and let us know if the issue still persists in newer versions. have a look at [link](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/applications) Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 25340, "title": "2.0 Reference Models: ResNet V1.5 (1 GPU and 8 GPU with dist-strat and Keras)", "body": "Deep residual networks, or ResNets for short, proposed the breakthrough idea of identity mappings in order to enable training of very deep convolutional neural networks. This issue will track the creation of 1 GPU and 8 GPU TF 2.0-compatible implementations of ResNet for the ImageNet dataset.\r\n\r\nSee the following papers for more background:\r\n\r\n[1] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\r\n\r\n[2] [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.\r\n\r\nFor the TensorFlow 1.x equivalent, please refer [here](https://github.com/tensorflow/models/tree/master/official/resnet).", "comments": ["Official ResNet50 v1.5 with Keras here: https://github.com/tensorflow/models/tree/master/official/resnet/keras\r\n\r\nWe will be adding README to it as well. "]}, {"number": 25339, "title": "[BUG] gradients of `tf.cond` with float64 breaks with ResourceVariable", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 3 tried: binary and head (w & w/o xla)\r\n- TensorFlow version (use command below): 1.12 resp. head\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):irrelevant\r\n- GCC/Compiler version (if compiling from source):irrelevant\r\n- CUDA/cuDNN version: irrelevant\r\n- GPU model and memory: irrelevant\r\n\r\n**Describe the current behavior**\r\nUsing a (resource!) Variable in tf.cond (and everything in tf.float64) breaks the gradient computation with an error like:\r\n```python\r\nTypeError: Tensors in list passed to 'inputs' of 'Merge' Op have types [float64, float32] that don't all match.\r\n```\r\n\r\nIn gradients_impl.py, _GradientsHelper (line 805) are ZerosLikeOutsideLoop produced with op.This produces in control_flow_ops.py, line 1469, (iff the variable dtype is resource) zeros with \r\n```python\r\narray_ops.zeros(gen_resource_variable_ops.variable_shape(switch_val))\r\n```\r\nThis uses the dtype of the op.outputs. But for a resource dtype, this creates float32 zeros (which then collides with all the other float64 around). Any other type therefore works.\r\n\r\nTried (next to binary 1.12) also against Master/HEAD with and without XLA (on TF compilation). Same error\r\n\r\nSolution: no idea how to get the datatype out there or something. Sorry.\r\n\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nvar1 = tf.get_variable('var1', initializer=tf.constant(42., dtype=tf.float64), use_resource=True)\r\nwith tf.Session() as sess:\r\n    sess.run(var1.initializer)\r\n    res = tf.constant(0., dtype=tf.float64)\r\n    cond1 = tf.cond(tf.constant(True), lambda: res, lambda: res + var1)\r\n    grad_cond = tf.gradients(cond1, var1)\r\n```\r\n\r\n\r\n", "comments": ["Can you try setting the environment variable TF_ENABLE_CONTROL_FLOW_V2=1 and seeing if you still get an error? (If setting the env var in code, make sure to do it before importing tf).", "@skye I still get the error, yes (both with 1.12 and head)", "@mayou36,\r\nSorry for the delayed response. Your code could be run without any error in the **`Tensorflow Version 1.15.2`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/477abf7964ce7c71691ea425107c0052/gh_25339.ipynb) of the working code. Thanks!", "Hi no worries, many thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25339\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25339\">No</a>\n"]}, {"number": 25338, "title": "ecm3531 specific changes for DebugLog", "body": "", "comments": ["The Kokoro failures seem unrelated, we should try to get this in I think."]}, {"number": 25337, "title": "CUDA_ERROR_OUT_OF_MEMORY: out of memory with RTX 2070", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 1809\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 9.0.176 / 7.4.2.24\r\n- GPU model and memory: RTX 2070 8G\r\n\r\n\r\n\r\n**Describe the problem**\r\nOut of memory. The same code run perfectly with the same environment with a GTX 1070 so memory should be no factor. I just swapped a graphics card and the error arose.\r\nThe error output is initially CUBLAS_STATUS_ALLOC_FAILED so I looked around a bit and used this chunk of code in #7072:\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\nset_session(sess)\r\n```\r\nAfter applying this code the error output is attached below in \"Any other info / logs\" section.\r\nFrom \"failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\" I think \"freeMemory: 6.57GiB\" is allocated as usual but somehow not available to cuda.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nTry to train a CNN model with\r\n``` \r\nmodel.fit_generator(train_datagen, steps_per_epoch=train_datagen.n // batch_size, epochs=epochs, verbose=2, validation_data=dev_datagen, validation_steps=dev_datagen.n // batch_size)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nUsing TensorFlow backend.\r\n2019-01-30 22:54:18.923758: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-01-30 22:54:19.139480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 8.00GiB freeMemory: 6.57GiB\r\n2019-01-30 22:54:19.139744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-30 22:54:20.029695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-30 22:54:20.029852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-30 22:54:20.029938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-30 22:54:20.030160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6309 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nFound 209023 images belonging to 2 classes.\r\nFound 11002 images belonging to 2 classes.\r\nEpoch 1/200\r\n2019-01-30 22:54:52.312147: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.312662: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.80G (1932735232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.312846: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.62G (1739461632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.313021: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.46G (1565515520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.480583: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.480762: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.481276: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.481446: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.581230: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.581404: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.581690: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.581856: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.603866: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.604042: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.604320: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.604489: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.609692: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.609966: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.610424: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.610587: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.626099: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.626276: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.626553: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-01-30 22:54:52.626718: W tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2019-01-30 22:54:52.665175: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.85G (3060860928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory", "comments": ["Has this problem been solved  ? \r\n\r\n I am facing exactly same problem with \r\npython 3.6.7 \r\ncuda 9.0\r\ncudnn 7.3.1\r\ngpu model : 2 gpus:  each GTX1080ti  identical \r\ntensorflow-gpu 1.11.0 \r\n\r\nrunning on python virtual environment \r\n\r\n\r\nI am not running heavy model , I am running simple code to see if gpu is being loaded and used , but I got many lines of error all looks like the following line : \r\n2019-04-18 21:33:39.704417: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 8.62G (9253279744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n\r\nI restart pycharm and my system but there was no change. Any help please ? \r\n", "@bmiftah I did a clean install of everything with ddu and it somehow works, that's why I closed the issue.", "If I may , what do you mean by clean install ? ", "Same problem, what is a clean install ?", "I uninstalled all related packages and libraries, uninstalled graphics driver with DDU and reinstalled them again. I don\u2019t know why it works but my best guess is it has something to do with the graphics driver. When I upgrade to 2070 super I also have to reinstall the driver to make it work.", "I think that it happens because of properties of rtx graphic card. a certain portion of rtx 20xx graphic memory (2.9Gb of 7994Mb in rtx 2070s) is only available when using float16 data type in tensorflow. if you allocate whole graphic card memory, you must use two data types float32, float16.\r\n\r\nopt = tf.keras.optimizers.Adam(1e-4)\r\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\r\nmodel.compile(loss=custom_loss, optimizer=opt)\r\n\r\n", "if you use opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt), automatically convert nodes to float16 and your graphic card memory can be all allocated."]}, {"number": 25336, "title": "Add the tests for some tf.data kernels", "body": "This PR develops the tests for tf.data kernel implementations in C++. It designs a test base class and adds the tests for RangeDataset and MapDataset.\r\n\r\ncc @jsimsa ", "comments": ["```\r\n\r\nRunning do_buildifier on 412 files\r\ntensorflow/core/kernels/data/BUILD # reformat listsort unsafesort sort:tf_cc_test.deps\r\nbuildifier took 0 s\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n212,213d211\r\n<         \":map_dataset_op\",\r\n<         \":range_dataset_op\",\r\n216,217c214,215\r\n<         \"//tensorflow/core:testlib\",\r\n<         \"//tensorflow/core:test_main\",\r\n---\r\n>         \":map_dataset_op\",\r\n>         \":range_dataset_op\",\r\n222c220,221\r\n<         \"//tensorflow/core/kernels:function_ops\",\r\n---\r\n>         \"//tensorflow/core:test_main\",\r\n>         \"//tensorflow/core:testlib\",\r\n223a223\r\n>         \"//tensorflow/core/kernels:function_ops\",\r\n399d398\r\n<         \":range_dataset_op\",\r\n401c400\r\n<         \"//tensorflow/core:testlib\",\r\n---\r\n>         \":range_dataset_op\",\r\n403c402\r\n<\r\n---\r\n>         \"//tensorflow/core:testlib\",\r\nPlease fix manually or run buildifier <file> to auto-fix.\r\n```", "The other test failuers seems to be unreleated AFAICT.", "@jsimsa Thanks for running the tests. I fix the style issue in BUILD file (using this command `buildifier tensorflow/tensorflow/core/kernels/data/BUILD`). Could you run the test again?", "@jsimsa The test failures seem to be unrelated. After checking the logs, the failures are caused by TensorFlow lite, such as `ERROR: /tmpfs/src/github/tensorflow/tensorflow/lite/python/testdata/BUILD:10:1: Executing genrule //tensorflow/lite/python/testdata:permute_float failed (Exit 1)`", "@feihugis can you please check the failed tests.", "@rthadur I can only access the failure logs of `MacOS Contrib` test. It is unrelated and caused by TensorFlow lite toco. This [PR](https://github.com/tensorflow/tensorflow/pull/25345) is pending for fixing this issue.\r\n\r\nI don't know why I could not access other logs. Could you re-trigger the tests again?\r\n\r\n", "@rthadur Thanks for running the tests. These test failures are unrelated to this PR: \r\n\r\n- the test failures in`MacOS Contrib` and `MacOS Python2 and CC` are caused by TensorFlow lite; \r\n\r\n- the test failures in `Windows Bazel` are caused by `tensorflow/python/kernel_tests:lookup_ops_test`", "Internally, the newly added tests are failing to build:\r\n\r\n```\r\nBroken by //third_party/tensorflow/core/kernels/data:dataset_test_base\r\nCppCompile action:\r\n./third_party/tensorflow/core/kernels/data/dataset_test_base.h:29:10: error: module //third_party/tensorflow/core/kernels/data:dataset_test_base does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/dataset_utils.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:dataset_test_base\r\n#include \"third_party/tensorflow/core/kernels/data/dataset_utils.h\"\r\n         ^\r\n1 error generated.\r\n\r\nBroken by //third_party/tensorflow/core/kernels/data:map_dataset_op_test\r\nCppCompile action:\r\nthird_party/tensorflow/core/kernels/data/map_dataset_op_test.cc:26:10: error: module //third_party/tensorflow/core/kernels/data:map_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/dataset_utils.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:map_dataset_op_test\r\n#include \"third_party/tensorflow/core/kernels/data/dataset_utils.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/map_dataset_op_test.cc:29:10: error: module //third_party/tensorflow/core/kernels/data:map_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/platform/test.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:map_dataset_op_test\r\n#include \"third_party/tensorflow/core/platform/test.h\"\r\n         ^\r\n2 errors generated.\r\n\r\nBroken by //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\nCppCompile action:\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:16:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/framework/dataset.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/framework/dataset.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:19:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/framework/node_def_builder.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/framework/node_def_builder.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:20:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/framework/partial_tensor_shape.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/framework/partial_tensor_shape.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:21:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/framework/variant.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/framework/variant.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:22:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/framework/variant_tensor_data.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/framework/variant_tensor_data.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:24:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/dataset_utils.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/kernels/data/dataset_utils.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:27:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/platform/test.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/platform/test.h\"\r\n         ^\r\nthird_party/tensorflow/core/kernels/data/range_dataset_op_test.cc:28:10: error: module //third_party/tensorflow/core/kernels/data:range_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/util/ptr_util.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test\r\n#include \"third_party/tensorflow/core/util/ptr_util.h\"\r\n         ^\r\n8 errors generated.\r\n```", "@jsimsa I could not reproduce this error when running `bazel build -c opt //tensorflow/core/kernels/data`. Do you know how to reproduce it?\r\n\r\nDoes the log mean that some dependencies are missing in the BUILD file for the newly added tests? To fix the issues, the log suggests to run `build_cleaner //third_party/tensorflow/core/kernels/data:range_dataset_op_test`. But I did not find a way to install `build_cleaner`. Do you know how to install `build_cleaner`?", "Yes, that's what it means. Build cleaner is an internal tool. You will need to make sure that for the listed header files, you add dependencies that provide them.", "@jsimsa [cae2b48](https://github.com/tensorflow/tensorflow/pull/25336/commits/cae2b48ea50cc768c38f5037c99bd09ebf5539cd) adds the missing dependencies. Could you help review it and re-trigger the build test?"]}, {"number": 25335, "title": "Bazel: Remove deprecated experimental_shortened_obj_file_path option", "body": "`experimental_shortened_obj_file_path` is depreacted since [version `0.17.0`](https://blog.bazel.build/2018/09/14/bazel-0.17.html) and activated by default. Since Tensorflow requires [at least bazel `0.19.0`](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1559), this flag can be savely removed.", "comments": ["Those test failures do not seem relevant."]}, {"number": 25334, "title": "Fix a minor typo", "body": "", "comments": []}, {"number": 25333, "title": "Eager execution - GradientTape.gradient() only returns gradient for last variable w.r.t. y?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nGradientTape.gradient() returns gradient for only the last variable in var_list, which is a strange behavior different from tf.gradient()\r\n\r\n**Describe the expected behavior**\r\nGradientTape.gradient() returns gradients for every variable in the var_list, same as tf.gradient()\r\n\r\n**Code to reproduce the issue**\r\nWith eager execution (wrong output with only gradient for last variable - x2 calculated):\r\nx1 = tf.constant(3.0)\r\nx2 = x1 * x1\r\nwith tf.GradientTape() as g:\r\n  g.watch([x1, x2])\r\n  y = x2 + 1\r\ng.gradient([x1, x2]).numpy()\r\n[OUTPUT] 1.0\r\n\r\nWithout eager execution (the expected output):\r\nx1 = tf.constant(3.0)\r\nx2 = x1 * x1\r\ny = x2 + 1\r\nop = tf.gradient(y, [x1, x2])\r\nwith tf.Session() as sess:\r\n  print(sess.run(op))\r\n[OUTPUT] [6.0, 1.0]\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I'm confused. Did you mean to write `g.gradient([x1, x2]).numpy()`? I'd have assumed you meant `g.gradient(y, [x1, x2]).numpy()`?", "Your are right. That was a typo.", "Then I cannot reproduce. When I run the code as you wrote it I get the error about the missing sources argument. When I run with the version I wrote it complains (as it should) that you're calling .numpy() on a list. If I don't do that I get the result which is working as intended, [None, 1.0]. The gradient wrt x1 is None because you defined x1 outside the tape; if you want a non-None gradient do something like:\r\n\r\n```\r\nx1 = tf.constant(3.0)\r\nwith tf.GradientTape() as g:\r\n  g.watch(x1)\r\n  x2 = x1 * x1\r\n  y = x2 + 1\r\ng.gradient(y, [x1, x2])\r\n```\r\n\r\nwhich returns the right values (6 and 1).", "I guess by \"defined x1 outside the tape\" you mean the operation w.r.t. x1 rather than the x1 variable itself?\r\n\r\nI double checked the document and indeed it says:\r\n\r\n> \"Operations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".\"", "I meant that x2 was defined before you created the tape, so the tape did\nnot know how x1 and x2 were related.\n\nOn Fri, Feb 1, 2019 at 7:45 AM peidaqi <notifications@github.com> wrote:\n\n> I guess by \"defined x1 outside the tape\" you mean the operation w.r.t. x1\n> rather than the x1 variable itself?\n>\n> I double checked the document and indeed it says:\n>\n> \"Operations are recorded if they are executed within this context manager\n> and at least one of their inputs is being \"watched\".\"\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25333#issuecomment-459764839>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVuUlhsnJzgQeX0S9AKi1bLJnQMYks5vJGEogaJpZM4aa6Lr>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 25332, "title": "Add missing elementwise monotonic functions", "body": "This PR adds `Acos`, `Acosh`, `Asin`, `Atan`, `Softsign` and `Softplus` to the collection of elementwise monotonic operations. This allows the arithmetic optimizer to optimize more cases.\r\n\r\nContinuation of #25330", "comments": []}, {"number": 25331, "title": "Unsupported Ops: RandomUniform, SoftmaxCrossEntropyWithLogits.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu \r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (or github SHA if from source): 1.13.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, FLOOR, FULLY_CONNECTED, MAX_POOL_2D, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: RandomUniform, SoftmaxCrossEntropyWithLogits.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/smohan/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/smohan/.local/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/smohan/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/smohan/.local/lib/python3.5/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\n\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["As the output suggests you need to add custom operators since the operations are not supported by TF Lite. Please refer this document to know more about [adding custom operators](https://www.tensorflow.org/lite/custom_operators). Thanks!", "Automatically closing this issue due to lack of activity. Feel free to reopen when additional information is available. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}]