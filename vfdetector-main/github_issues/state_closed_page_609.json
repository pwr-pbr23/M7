[{"number": 35394, "title": "[ROCm] Support of GRU and LSTM", "body": "This PR fixes the tests //tensorflow/python/keras/layers:gru_v2_test, //tensorflow/python/keras/layers:lstm_v2_test, and //tensorflow/python/keras/distribute:keras_rnn_model_correctness_test on the ROCm platform.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35394) for more info**.\n\n<!-- need_author_consent -->", "@ekuznetsov139  \r\n\r\nCould you please sign cla ? Thank you.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35394) for more info**.\n\n<!-- ok -->", "/cc @jerryyin \r\n\r\n@ekuznetsov139, please add a commit to this PR to remove the `no_rocm` tag from all the tests (if any)) that will start passing as a result of this PR.", "@ekuznetsov139 \r\n\r\nCan you please check @ deven-amd 's comments and keep us posted? Thanks!", "Done.", "@qlzh727 to answer all your comments in one place:\r\n* ROCm provides a backend implementation of LSTM and GRU for AMD GPUs that is separate from the CuDNN LSTM implementation (which is for NVIDIA only). \r\n* All the existing kernels (see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cudnn_rnn_ops.cc) have \"CuDNN\" in their names for historical reasons, but, internally, they already route either to CuDNN or to ROCm, depending on the hardware. \r\n* I've renamed cudnn_lstm into gpu_lstm and cudnn_gru into gpu_gru in recurrent_v2.py to avoid confusion. Renaming the rest of the existing kernels (if you think that is a good idea) would change the API and that is outside the scope of this PR. ", "@ekuznetsov139 \r\n\r\nplease fix the python linter errors. thanks.\r\n\r\n```\r\nFAIL: Found 38 non-whitelisted pylint errors:                                                                                             \r\ntensorflow/python/keras/layers/recurrent_v2.py:571: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).            \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:706: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).            \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:707: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).            \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1290: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).           \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required around assignment               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1346: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required around assignment               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1347: [C0326(bad-whitespace), ] Exactly one space required after comma                     \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1461: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).           \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/recurrent_v2.py:1462: [C0330(bad-continuation), ] Wrong continued indentation (remove 2 spaces).           \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:146: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:318: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:334: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:379: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:463: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:565: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/gru_v2_test.py:693: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:251: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:322: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:369: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:414: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:589: [W0311(bad-indentation), ] Bad indentation. Found 8 spaces, expected 6                \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:590: [C0330(bad-continuation), ] Wrong continued indentation (add 10 spaces).              \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:629: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:767: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n                                                                                                                                          \r\ntensorflow/python/keras/layers/lstm_v2_test.py:869: [C0330(bad-continuation), ] Wrong continued indentation (add 8 spaces).               \r\n```", "@ekuznetsov139 Can you please check @deven-amd's comments and keep us posted? Thanks!", "Done. (This linter is picky ... can this kind of formatting be done with a script?)", "@ekuznetsov139 need to merge conflicts again :(", "@ekuznetsov139 Can you please resolve conflicts? Thanks!", "@googlebot I consent", "@qlzh727,  please re-approve. previous CI run ran in to pylint errors. I have pushed out a commit to fix them.\r\nthanks"]}, {"number": 35393, "title": "TF 2.0 Custom Metric 'Tensor' object has no attribute 'numpy' ", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: tensorflow-gpu 2.0.0\r\n- Python version: 3.7.3\r\n\r\n**Code**\r\n\r\nmodel.py\r\n```\r\nfrom metrics import r2_score\r\n\r\n\r\ndef create_model(input_shape, output_shape):\r\n    model = models.Sequential([\r\n        layers.LSTM(64, return_sequences=True, input_shape=input_shape),\r\n        layers.LSTM(16, activation='relu'),\r\n        layers.Dense(output_shape)\r\n    ])\r\n\r\n    model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\r\n\r\n    return model\r\n```\r\n\r\nmetrics.py\r\n```\r\nfrom sklearn.metrics import r2_score as skl_r2_score\r\n\r\n\r\ndef r2_score(y_true, y_pred):\r\n    return skl_r2_score(y_true.numpy(), y_pred.numpy())\r\n\r\n```\r\n**Current behavior**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 38, in <module>\r\n    model = load_model(args.model_path, (history_length, len(series_features)), forecast_length)\r\n  File \"C:\\Users\\haaka\\ML\\goog\\model.py\", line 24, in load_model\r\n    model = create_model(input_shape, output_shape)\r\n  File \"C:\\Users\\haaka\\ML\\goog\\model.py\", line 15, in create_model\r\n    model.compile(optimizer='adam', loss='mse', metrics=[r2_score])\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 366, in compile\r\n    masks=self._prepare_output_masks())\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2063, in _handle_metrics\r\n    target, output, output_mask))\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2014, in _handle_per_output_metrics\r\n    metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\", line 1067, in call_metric_function\r\n    return metric_fn(y_true, y_pred, sample_weight=weights)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 193, in __call__\r\n    replica_local_fn, *args, **kwargs)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\distribute\\distributed_training_utils.py\", line 1135, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 176, in replica_local_fn\r\n    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\metrics_utils.py\", line 75, in decorated\r\n    update_op = update_state_fn(*args, **kwargs)\r\n  File \"C:\\Users\\haaka\\ML\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\metrics.py\", line 581, in update_state\r\n    matches = self._fn(y_true, y_pred, **self._fn_kwargs)\r\n  File \"C:\\Users\\haaka\\ML\\goog\\metrics.py\", line 7, in r2_score\r\n    return skl_r2_score(y_true.numpy(), y_pred.numpy())\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\n\r\nIn the issue [#27519](https://github.com/tensorflow/tensorflow/issues/27519) (which has been closed and does not offer a solution to my problem) others described a problem similar to mine where they were trying to implement a custom metric and encountered the error `'Tensor' object has no attribute 'numpy'`. I believe this is a bug because in TF 2.0 eager execution is enabled by default so I should not be receiving this error.", "comments": ["keras compile is run with graph mode in default", "Try the custom train loop (confirmed work) or compile with eagerly = True (I don't know if it work), or, use py_function at graph mode.", "Thank you for your response. I have solved the problem using a different method:\r\n\r\nReplace\r\n```\r\nfrom sklearn.metrics import r2_score as skl_r2_score\r\n\r\n\r\ndef r2_score(y_true, y_pred):\r\n    return skl_r2_score(y_true.numpy(), y_pred.numpy())\r\n```\r\nwith\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef r2(y_true, y_pred):\r\n    total_error = tf.reduce_sum(tf.square(tf.subtract(y_true, tf.reduce_mean(y_true))))\r\n    unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred)))\r\n    r_squared = tf.subtract(tf.cast(1, tf.float32), tf.divide(unexplained_error, total_error))\r\n\r\n    return r_squared\r\n```", "model.compile(..., run_eagerly=True) worked for me when creating a custom metric", "> model.compile(..., run_eagerly=True) worked for me when creating a custom metric\r\n\r\nIn my multi_lable_classification task, I try to use numpy operation in the custom metric and I have added \"run_eagerly = True\", but still failed"]}, {"number": 35392, "title": "[Intel MKL] Avoid unnecessary data reorders", "body": "This PR improves performance by avoiding some unnecessary input reorders since MKL-DNN now performs well on plain format (like NHWC). This improves performance for pooling and batch norm ops.", "comments": []}, {"number": 35391, "title": "[r2.1:Cherrypick][Intel MKL] Klocwork fix", "body": "", "comments": ["Closing this PR as this not against `master`, please open a new PR against `master` \r\n", "@rthadur There is already one against master branch #35390, but I created this because eventually we need to cherry-pick it to r2.1.", "No worries this will be cherry picked , we only accept security fixes. ", "Reopening as the 2.1 final release is not done yet, so we can get cherry-picks until then.", "Actually closing for now, let's wait for #35390 to land on master. Then make a PR against r2.1 with `git cherry-pick ${hash_of_commit_on_master}`. This way we ensure that both branches are in sync w.r.t. this patch.\r\n\r\nnote: You can add me as a reviewer for any Klocwork PRs, on any branch", "Thank you @mihaimaruseac "]}, {"number": 35390, "title": "[Intel MKL] Klocwork fix", "body": "", "comments": ["Can we cherry-pick this into r2.1 as this is required to release TF 2.1? Here is the PR against r2.1 #35391 ", "@alextp No unit test is failing. Klocwork is a tool that scans code for security and reliability issues. Intel cannot release code if it does not pass this scan. The change in this PR has no functional impact. It is needed just to make the tool scan pass which requires every member variable to be initialized in the constructor. "]}, {"number": 35389, "title": "ERROR: Next operations are not supported by GPU delegate.", "body": "INFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Next operations are not supported by GPU delegate:\r\nCONCATENATION: \r\nCONV_2D: \r\nCUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\nDEPTHWISE_CONV_2D: \r\nLOGISTIC: \r\nRESHAPE: \r\nFirst 0 operations will run on the GPU, and the remaining 64 on the CPU.\r\n\r\n\r\nI've converted model to tflite and i'm getting this error. Can anybody help me why i'm getting this and/or what could be the possible reason?\r\n\r\nHere's the code:\r\n\r\nbool ObjectDetector::init(const std::string &model_file, bool is_quantized,\r\n                          const std::string &labels_file)\r\n{\r\n    // Load model.\r\n    model_ = tflite::FlatBufferModel::BuildFromFile(model_file.c_str());\r\n    if (!model_)\r\n    {\r\n        LOG(ERROR) << \"Failed to load model: \" << model_file;\r\n        return false;\r\n    }\r\n\r\n    // Create interpreter.\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder(*model_, resolver)(&interpreter_);\r\n    if (!interpreter_)\r\n    {\r\n        LOG(ERROR) << \"Failed to create interpreter!\";\r\n        return false;\r\n    }\r\n    /*if (interpreter_->AllocateTensors() != kTfLiteOk)\r\n    {\r\n        LOG(ERROR) << \"Failed to allocate tensors!\";\r\n        return false;\r\n    }\r\n    const TfLiteGpuDelegateOptions options = {\r\n    .metadata = NULL,\r\n    .compile_options = {\r\n        .precision_loss_allowed = 1,  // FP16\r\n        .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\r\n        .dynamic_batch_enabled = 0,   // Not fully functional yet\r\n    },\r\n    };*/\r\n    auto* delegate = TfLiteGpuDelegateCreate(/*default options=*/nullptr);\r\n    if (interpreter_->ModifyGraphWithDelegate(delegate) != kTfLiteOk) return false;\r\n    //LOG(ERROR) << \"DetectEmptyParkingSlots:: TfLiteGpuDelegateCreate Passed!!! Hurray!.....\" << std::endl;\r\n    \r\n    //interpreter_->SetNumThreads(1);\r\n    interpreter_->UseNNAPI(1);\r\n\r\n\r\n\r\n    // Find input tensors.\r\n    if (interpreter_->inputs().size() != 1)\r\n    {\r\n        LOG(ERROR) << \"Graph needs to have 1 and only 1 input!\";\r\n        return false;\r\n    }\r\n    input_tensor_ = interpreter_->tensor(interpreter_->inputs()[0]);\r\n    if (is_quantized)\r\n    {\r\n        if (input_tensor_->type != kTfLiteUInt8)\r\n        {\r\n            LOG(ERROR) << \"Quantized graph's input should be kTfLiteUInt8!\";\r\n            return false;\r\n        }\r\n    }\r\n    else\r\n    {\r\n        if (input_tensor_->type != kTfLiteFloat32)\r\n            LOG(ERROR) << \"Quantized graph's input should be kTfLiteFloat32!\";\r\n        {\r\n            return false;\r\n        }\r\n    }\r\n\r\n    // Find output tensors.\r\n    if (interpreter_->outputs().size() != 4)\r\n    {\r\n        LOG(ERROR) << \"Graph needs to have 4 and only 4 outputs!\";\r\n        return false;\r\n    }\r\n    output_locations_ = interpreter_->tensor(interpreter_->outputs()[0]);\r\n    output_classes_ = interpreter_->tensor(interpreter_->outputs()[1]);\r\n    output_scores_ = interpreter_->tensor(interpreter_->outputs()[2]);\r\n    num_detections_ = interpreter_->tensor(interpreter_->outputs()[3]);\r\n\r\n    std::vector<std::string> labels;\r\n    ReadLines(labels_file, &labels);\r\n    labels_ = labels;\r\n    return true;\r\n}\r\n", "comments": ["I have the impression you messed up with the TF source code based on:\r\n\r\n> ERROR: Next operations are not supported by GPU delegate:\r\n> CONCATENATION:\r\n> CONV_2D:\r\n> DEPTHWISE_CONV_2D:\r\n> LOGISTIC:\r\n> RESHAPE:\r\n\r\nAs these are properly supported on the GPU delegate.  It is natural for \r\n\r\n> CUSTOM TFLite_Detection_PostProcess: Operation is not supported.\r\n\r\nas the GPU delegate doesn't implement that.", "Any pointers on how to solve it? Moreover, i haven't touch anything on the TF code part.\r\n@impjdi ", "any update on the same? can i have a proper doc or reference on how to use gpu delecate?", "Your code snippet is correct in terms of how to initialize things.  What I'm not clear is why you're getting \r\n\r\n> ERROR: Next operations are not supported by GPU delegate:\r\n> CONCATENATION:\r\n> CONV_2D:\r\n> DEPTHWISE_CONV_2D:\r\n> LOGISTIC:\r\n> RESHAPE:\r\n\r\nas we support those.  Can you attach the model?", "[detect.zip](https://github.com/tensorflow/tensorflow/files/4095732/detect.zip)\r\nthis is the model. also i would like to know that is there any specific way to convert the model to run in gpu? i've followed this link for conversion https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 .", "I see you're using a quantized network.  For GPU, you want float models :)", "sorry, I posted a different model. \r\n[detect.zip](https://github.com/tensorflow/tensorflow/files/4096225/detect.zip)\r\n\r\nthis is the correct model.\r\n", "ok, i'll explain what I did, may be from there you can guide me better. I took ssd_mobilenet_v1 model, did transfer learning on my data and saved the model using https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 this link. After training, I exported the model using python object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$OUTPUT_DIR \\\r\n--add_postprocessing_op=true . Now, from here, can you please guide on what to do next to run it on gpu. I'm using snapdragon 8155 soc. It'll be very helpful. @impjdi ", "@impjdi any update?\r\n", "I'm not familiar with the object detector, but the post clearly states that it's related to quantization.  If you inspect your network, you can see it has FakeQuant ops everywhere.  Those are not compatible with GPUs as mentioned before.  Try to get a hold of the original float model.", "@impjdi can you please add someone who can help me in converting ssd_mobilenet_v1 model into tflite float model for running in snapdragon board using gpu delegate. I can't find a proper **documentation** anywhere. When i'm using this script :\r\n\r\ntflite_convert --output_file ./model.tflite --saved_model_dir /home/shaurya/Downloads/ssd_mobilenet_v1_coco_2018_01_28/saved_model --output_format TFLITE --inference_type FLOAT --input_arrays image_tensor --input_shapes 1,300,300,3 --output_arrays detection_boxes,detection_classes,detection_scores,num_detections\r\n\r\nI'm getting this error: ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n\r\nIt's been more than 2 month and i'm struck.\r\n\r\n@achowdhery ", "any help sir?", "The command line that you use looks correct to me, but note that I'm not super familiar with TOCO myself; I usually only work with the final Flatbuffer model, and use flatc to convert back and forth.\r\n\r\nI would try to grep for the error message in the code base and find out what condition is failing.", "@impjdi do you have any follow steps on how to convert an object detection model into it's compatible tflite format so that we can use gpu delegate to run it?\r\n\r\nI tried converting it the model into it's tflite format but i'm getting model initialisation error. no documentation is available in internet. Do you any?\r\n", "[detect.zip](https://github.com/tensorflow/tensorflow/files/4132456/detect.zip)\r\n@impjdi can you check this model? Is this fine?\r\n\r\nI tried to remove fake quant ops.", "Yeah, I think this looks like a float model.  Of course, GPU delegate won't be able to run the last op (TFLite_Detection_PostProcess) but the rest should be runnable on the GPU.", "is there any reason why it's not supported??\r\n\r\nAnyways, I'm able to use GPU now.", " We have limited resource and cannot even implement every builtin op.  A very specialized post processing op is surely very far away from being prioritized.\r\n\r\nGlad that GPU works for you now."]}, {"number": 35388, "title": "Added usage examples to some APIs", "body": "Added usage examples to these APIs\r\n- image.random_flip_up_down\r\n- image.flip_up_down\r\n- image.random_flip_left_right\r\n- image.flip_left_right\r\n- image.transpose\r\n- image.random_brightness\r\n- image.random_contrast\r\n- image.random_hue\r\n- image.random_jpeg_quality\r\n- image.random_saturation\r\n\r\n-- Re-Opened the PR since the last one wasn't based off the master --", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35388) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@googlebot I consent", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35388) for more info**.\n\n<!-- ok -->", "Yeah actually that idea feels good, I will do it ASAP. Thanks for the recommendation! ", "Thanks for the update. However, it still fails to be testable.\r\n\r\nI was thinking of something more on the lines of https://github.com/tensorflow/tensorflow/blob/5078cab51cea45364602f9d2a1d30057799af4b0/tensorflow/python/ops/array_ops.py#L100-L109", "Hmm yeah, ok. I will be changing them in a few mins.", "Ok, made the changes. Can you check again?", "Looks better but still not there:\r\n\r\n* Don't use `tf.random.normal` as then testing would depend on the seed that is being used. Instead, explicitly use some values (like in the example above, `t = [[1, 2, 3],...`)\r\n* Use `>>>` instead of `>>` as `>>>` signal that this is documentation test which will then be tested.\r\n* There is no need for the triple backquote markers and the `python` tagging. The `>>>` makes this look like properly-formatted code in the documentation.\r\n* Don't insert these examples after the typing/argument documentation, but before that. `Args:`/`Returns:`/etc. are expected to always be the last blocks in a docstring.", "Sanity test fails with:\r\n\r\n```\r\nFAIL: Found 2 non-whitelisted pylint errors:\r\ntensorflow/python/ops/image_ops_impl.py:3993: [C0301(line-too-long), ] Line too long (100/80)\r\n\r\ntensorflow/python/ops/image_ops_impl.py:4076: [C0301(line-too-long), ] Line too long (100/80)\r\n```", "Should be fixed now with https://github.com/tensorflow/tensorflow/pull/35388/commits/f972477ecacdcbfc20f3feb21086778a718a160c.", "Is Ubuntu CPU build failing a thing that I should change or should I just ignore that? Because you just gave me Sanity error last time and now it's fixed but Ubuntu CPU still seems to give an error.", "The [log](https://source.cloud.google.com/results/invocations/772364a7-49bb-4d99-8028-8977bd3e2044/log) suggests is a doctest error:\r\n\r\n```\r\nFAIL: //tensorflow/tools/docs:tf_doctest (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/docs/tf_doctest/test_attempts/attempt_2.log)\r\nFAIL: //tensorflow/tools/docs:tf_doctest (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/docs/tf_doctest/test.log)\r\n\r\nFAILED: //tensorflow/tools/docs:tf_doctest (Summary)\r\n      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/docs/tf_doctest/test.log\r\n      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/docs/tf_doctest/test_attempts/attempt_1.log\r\n      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/tools/docs/tf_doctest/test_attempts/attempt_2.log\r\n```\r\n\r\nand then the failures are down in the log, looking like\r\n\r\n```\r\nFailed example:\r\n    tf.image.adjust_brightness(x, delta=0.1)\r\nExpected:\r\n    array([[[ 1.1,  2.1,  3.1],\r\n        [ 4.1,  5.1,  6.1]],\r\nGot:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[ 1.1,  2.1,  3.1],\r\n            [ 4.1,  5.1,  6.1]],\r\n\r\n           [[ 7.1,  8.1,  9.1],\r\n            [10.1, 11.1, 12.1]]], dtype=float32)>\r\n```", "Pretty interesting because it looks like I have written the thing that should be there. Also, expected value doesn't start with `tf.Tensor` and neither my example output in the file starts like that but it says it got something that started with it. I guess expected & got is just mixed labels? So should I just add the `tf.Tensor` line as well or do you have any other idea what this error is about?\r\n\r\n![image](https://user-images.githubusercontent.com/40995274/71532155-1d36e900-2903-11ea-880f-930516ce8ab6.png)\r\n", "You should include the line(s) that start with `tf.Tensor` You basically need to get the entire output.\r\n\r\nSee also https://www.tensorflow.org/community/contribute/docs_ref", "I hope it is fixed this time because I have changed it for all of them.", "Oof what again? Can you send me the errors once again?", "You should be able to see the log too (click on \"Details\" on the right)\r\n\r\nThey look like\r\n\r\n```\r\nFailed example:\r\n    tf.image.adjust_contrast(x, 2)\r\nExpected:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[-3.5, -2.5, -1.5],\r\n        [ 2.5,  3.5,  4.5]],\r\nGot:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[-3.5, -2.5, -1.5],\r\n            [ 2.5,  3.5,  4.5]],\r\n\r\n           [[ 8.5,  9.5, 10.5],\r\n            [14.5, 15.5, 16.5]]], dtype=float32)>\r\n```\r\n\r\nWhich probably means spacing is an issue. Make sure you don't use tab characters", "Oh yeah, thanks, I don't know why but it was asking me login credentials before so I thought it would still ask but now looks like I can see the logs. I will be fixing those in a few moments.", "Also, I just noticed that there is a problem with naming as I said before, I'm 100% sure of it now. Last time, when there was error, it said \r\n```\r\nExpected:\r\n    array([[[ 1.1,  2.1,  3.1],\r\n        [ 4.1,  5.1,  6.1]],\r\nGot:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[ 1.1,  2.1,  3.1],\r\n            [ 4.1,  5.1,  6.1]],\r\n\r\n           [[ 7.1,  8.1,  9.1],\r\n            [10.1, 11.1, 12.1]]], dtype=float32)>\r\n```\r\n\r\nbut actually I was confused that expected & got are on the wrong places. This time, when I check it back it's the same thing, my code doesn't have tabs but it expects me to put tabs as I can understand now since the \"Expected\" is same as the current status of my code\r\n\r\n```\r\nExpected:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[-3.5, -2.5, -1.5],\r\n        [ 2.5,  3.5,  4.5]],\r\nGot:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[-3.5, -2.5, -1.5],\r\n            [ 2.5,  3.5,  4.5]],\r\n\r\n           [[ 8.5,  9.5, 10.5],\r\n            [14.5, 15.5, 16.5]]], dtype=float32)>\r\n```\r\n\r\nWhere should I put this issue? I guess I can't since Ubuntu CPU CI is not an open source thing I believe ?", "Another thing is I guess the doctest doesn't accept multiline commands but if I don't make the examples of `extract_glimpse_v2` and `extract_glimpse` in 2 seperate lines, then Sanity gives error because line is too long. Is it a problem if I turn this back to non-doctest mode by adding \" \\`\\`\\`python\" ? Here is the error which I'm talking about:\r\n\r\n```\r\nFailed example:\r\n    tf.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],\r\nException raised:\r\n    Traceback (most recent call last):\r\n      File \"/usr/lib/python3.6/doctest.py\", line 1330, in __run\r\n        compileflags, 1), test.globs)\r\n      File \"<doctest tensorflow.python.ops.image_ops_impl.extract_glimpse_v2[1]>\", line 1\r\n        tf.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],\r\n                                                                 ^\r\n    SyntaxError: unexpected EOF while parsing\r\n```", "Ok actually I thought last one is my mistake, I forgot to add `...` to start of the next line to show the line continues.", "I didn't want to do this but I tried a lot to run `tf_doctests.py` on my local which I just discovered but I had no luck getting it working. It always gave me `cannot import name 'tf_doctest_lib' from 'tensorflow.tools.docs'` error which I couldn't solve by trying the solutions on https://github.com/tensorflow/tensorflow/issues/32111 . I hope it won't fail this time tho. ", "Adding @yashk2810 for the `cannot import name 'tf_doctest_lib' from 'tensorflow.tools.docs'` error when running `tf_doctests.py`.\r\n\r\nRegarding seeing the logs: you can only see them once they complete (once you see the error/success badge).", "It says that `CAUTION: tf_doctest doesn't work if *some* of the *float output* is hidden with a \"...\".\r\n` I guess I need to change them back.", "Also as I said, \"Expected\" and \"Got\" are in the wrong places. Where can I report it? Or did you already report it? Because I believe it will lead to lots of confusions.", "\"Expected\" is what you typed there to be. \"Got\" is what the test obtained. They look right to me:\r\n\r\n```\r\nExpected:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[1.        , 1.        , 1.        ],\r\n            [0.99... , 0.99... , 0.99... ]],\r\n           [[0.99... , 0.99... , 0.99... ],\r\n            [0.98..., 0.98..., 0.98...]]], dtype=float32)>\r\nGot:\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[1.        , 1.        , 1.        ],\r\n            [0.9960785 , 0.9960785 , 0.9960785 ]],\r\n\r\n           [[0.9921569 , 0.9921569 , 0.9921569 ],\r\n            [0.98823535, 0.98823535, 0.98823535]]], dtype=float32)>\r\n```\r\n\r\nwhen you typed (https://github.com/tensorflow/tensorflow/pull/35388/files#diff-70c103f055e332fe570ec9cfd87120a4R2147-R2157)\r\n\r\n```\r\n    <tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\r\n    array([[[1.        , 1.        , 1.        ],\r\n            [0.99... , 0.99... , 0.99... ]],\r\n           [[0.99... , 0.99... , 0.99... ],\r\n            [0.98..., 0.98..., 0.98...]]], dtype=float32)>\r\n```\r\n\r\nwhich is the same as the string under \"Expected\".\r\n\r\nRegarding `...`, it seems we have some additional changes to doctest. In general, you would use `...`, but for TF we have extra checks so it is not needed.", "For random output, please replace the float output with `...`. For consistency, we do not allow `...` in float output.", "> \"Expected\" is what you typed there to be. \"Got\" is what the test obtained. They look right to me:\r\n\r\nWell that makes sense too, I always thought it like \"Expected\" is the expected, correct value but the \"Got\" is what the doctest file got when it checked my output.", "> For random output, please replace the float output with `...`. For consistency, we do not allow `...` in float output.\r\n\r\nWell, yeah we've been trying to do that for a long time but looks like doctests won't work with it because when I checked the error logs of Ubuntu CPU CI, it says like `CAUTION: tf_doctest doesn't work if *some* of the *float output* is hidden with a \"...\".` . And also, when I checked the conribution steps, it says that `...` should be used whenever the dtype is not exact so it should be like `tf.Tensor(blahblah, dtype=..., blahblah)`", "> yeah we've been trying to do that for a long time but looks like doctests won't work \r\n\r\nIt will work if the entire float output is hidden and not parts of it.\r\n\r\nSo something like this will work:\r\n```\r\n<tf.Tensor: dtype=float32, numpy=...>\r\n```\r\n\r\nBut this won't work:\r\n```\r\n<tf.Tensor: dtype=float32, numpy=[1.9..., 3.6...]>\r\n```", "If you put output which is random, it will fail whenever the doctest will run, because the output will change everytime.", "Also, don't worry about precision. We will round off the values, so you can just copy paste **non-random** values.", "Ok, I will be doing the changes in a few mins.", "Any ideas why copybara gave an error? I don't see a details button next to it.", "That's an internal error. We will retrigger it in a while once it should pass. Sorry about that", "Ah ok no problem, I was worried that something has gone wrong again because this took longer than it should have. I thank you a lot for helping me out and I wish you have a merry christmas! :) ", "Might it be caused because the master is currently ahead of my branch?", "Let's try rebasing on master", "Pretty interesting, why would it give error when I just deleted whitespaces :D Tho, can you access the logs? It gives me 404 error when I try to check the logs.", "I get 404 on that log at the moment, but no worries. It will be run again prior to merging", "I think you might need to rebase against master again", "Sure thing, done!", "By the way, I guess last error was a false-triggered error since it was giving 404 when reaching to logs and I have seen that when you re-ran it, it didn't give an error.", "@msteknoadam can you please resolve conflicts ", "Sorry, I was sleeping at the moment you requestef for changes but thanks for doing the changes for me. I'm very happy that this PR finally got merged :D Have a great day everyone! "]}, {"number": 35387, "title": "TF1.14.0 C++  CPU single thread ", "body": "I compiling tensorflow library (libtensorflow_cc.so libtensorflow_framework.so) by bazel\uff0cand used it in CPU mode. I want to run the program linked by the built tf library in one thread. The configuration bellow is set.\r\n      \r\n     \r\n\r\n- SessionOptions opts;\r\n\r\n- opts.config.set_inter_op_parallelism_threads(1)\r\n\r\n- opts.config.set_intra_op_parallelism_threads(1)\r\n\r\n\r\nBut the number of the threads  is 4. except the main thread, there are 3 threads, which be created by the bellow function:\r\n\r\n- tensorflow::thread::EigenEnvironment::CreateThread()\r\n\r\n- Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n\r\nhow to close these three threads?\r\n", "comments": ["@crystalwan Please post this question in stackoverflow as this is not related to bug/performance, feature request, build/install related issue. Thanks!", "@crystalwan did you solve the problem? I'm facing the same problem. I couldn't find answer to this question on the following github and stackoverflow issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/42510\r\nhttps://github.com/tensorflow/tensorflow/issues/33627\r\nhttps://github.com/usnistgov/frvt/issues/12\r\nhttps://github.com/tensorflow/models/issues/3176\r\nhttps://github.com/tensorflow/tensorflow/issues/13853\r\nhttps://stackoverflow.com/questions/47548145/understanding-tensorflow-inter-intra-parallelism-threads\r\nhttps://stackoverflow.com/questions/60206113/how-to-stop-tensorflow-from-multi-threading\r\nhttps://stackoverflow.com/questions/34389945/changing-the-number-of-threads-in-tensorflow-on-cifar10\r\nhttps://stackoverflow.com/questions/39035639/importing-tensorflow-spawns-threads"]}, {"number": 35386, "title": "Use tf-lite C++ API for both Android and iOS", "body": "**System information**\r\n- OS Platform and Distribution: macOS 10.14.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v.2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: source\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): clang version 7.0.1\r\n\r\nI am a mobile developer, using C++ to build portable parts of my apps that work on both Android and iOS.\r\nHere is my workflow:\r\n- For portable parts, I code a cross-platform library on macOS, build/unit-test it on macOS\r\n- then, build/unit-test it on Linux\r\n- create android and ios builds of the library using the respective toolchains, and integrate it into the mobile apps\r\n\r\nWhile exploring tflite, I can see multiple APIs: C, C++, Obj-C, Swift, Java.\r\nTo write portable code, Obj-C/Swift/Java APIs are out of picture.\r\nIs there any common API (C or C++) which can be used to evaluate tf-lite models on macOS, Linux, Android and iOS? If yes, what are the bazel targets to build?", "comments": ["I have been using C API (`tensorflow/lite/c`) to successfully run inference on TFLite models on macOS, Windows and Android ARM/ARM64 from Unity. Right now I am trying to see whether I can use the same API for iOS.\r\n\r\nBazel commands for building shared libraries are, in order for macOS/Windows, ARM64 Android and ARM Android:\r\n```\r\nbazel build -c opt --cxxopt=--std=c++11 //tensorflow/lite/c:tensorflowlite_c\r\nbazel build -c opt --cxxopt=--std=c++11 --config=android_arm64 //tensorflow/lite/c:tensorflowlite_c\r\nbazel build -c opt --cxxopt=--std=c++11 --config=android_arm //tensorflow/lite/c:tensorflowlite_c\r\n```\r\nYou need to run `./configure` prior to this to set up Bazel to use Android toolchain.", "@dimitrijer short answer for building C API library for iOS arm64:\r\n```\r\n bazel build -c opt --config ios_arm64 //tensorflow/lite/experimental/ios:TensorFlowLiteC_framework  \r\n```\r\n\r\nAnd you may want to check `tensorflow/lite/experimental/ios/BUILD`", "@freedomtan thanks, will check that out!\r\n\r\nI also found out a set of scripts that build C++ TFLite static library for multiple Android/iOS architectures at `tensorflow/lite/tools/make` - and the library also includes the C API! I get universal `libtensorflowlite.a` library after running `build_ios_universal_lib.sh -a \"armv7 armv7s arm64\"`. If I copy over C API headers, I should be able to link against this static library during Unity XCode build.", "I was able to build C++ static library for iOS, following instructions provided by @dimitrijer. Is there a similar way to build it for Android for following architectures: `'x86', 'x86_64', 'armeabi-v7a', 'arm64-v8a'`?", "There are `android, android_arm (armeabi-v7a), android_arm64 (arm64-v8a), android_x86, and android_x86_64`. Check your `$ROOT_OF_TENSORFLOW_SORUCE/.bazelrc` or https://github.com/tensorflow/tensorflow/blob/master/.bazelrc for how they are defined.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35385, "title": "Interesting bug, in keras, tf.shape not compatible with tensor.get_shape()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- TensorFlow installed from (source or binary):conda binary\r\n- TensorFlow version (use command below):2.0\r\n- Python version:3.7\r\n- CUDA/cuDNN version:430\r\n- GPU model and memory:2 * 24GB Titan RTX\r\n\r\nI found this bug is very interesting.\r\n\r\nFor example:\r\n\r\n> a = tf.ones([1, 512, 512, 3])\r\n> b = tf.ones([1, 256, 256, 3])\r\n> c = tf.ones([1, 1, 1, 3])\r\n> \r\n> a_size = tf.shape(a)[1:3]\r\n> b = tf.resize(b, size = a_size)\r\n> \r\n> b_size = b.get_shape()[1:3]\r\n> c = tf.resize(c, size = b_size) # Bug line\r\n\r\nIn the last line (bug line). if the model is run eagerly, it is okay. However, if run with keras.fit (Graph mode I guess), it will give the error \"**raise ValueError('\\'size\\' must be a 1-D int32 Tensor')**\"\r\n\r\n\r\n\r\n", "comments": ["@edwardyehuang, Thanks for reporting the issue.\r\nCould you post the complete code snippet to reproduce the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35384, "title": "TF1.15 Distribute Mode Error ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos7.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF1.15\r\n- Python version: python3.6\r\n- CUDA/cuDNN version: 10.0/7.6.2\r\n\r\n**Code**\r\n\r\n mirror_strategy = tf.distribute.MirroredStrategy()\r\n with mirror_strategy.scope():\r\n    model = test_net()\r\n    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\r\n    model.compile(\r\n        optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\r\n    )\r\n\r\n    model.fit(\r\n        x_train, \r\n        y_train,\r\n        batch_size=256,\r\n        epochs=10,\r\n        validation_data=(x_test, y_test),\r\n        verbose=2,\r\n    )\r\n\r\n**The Error Information**\r\n\r\nNo registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node {{node MultiDeviceIteratorGetNextFromShard}}\r\n        .  Registered:  device='CPU'\r\n", "comments": ["You have fixed this bug in TF2.0. Is there any update of TF1.15", "@KANGRuipeng \r\nLooks like code is incomplete.Can you please provide simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!", "> @KANGRuipeng\r\n> Looks like code is incomplete.Can you please provide simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!\r\n\r\nThe test_net is a simple conv-net. You can define it by yourself.", "def block(x):\r\n\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n\r\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n\r\n    x = tf.keras.layers.Dense(10)(x)\r\n    x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)\r\n\r\n    return x\r\n\r\n\r\ndef FCN():  # pylint: disable = invalid-name\r\n    in_tensor = tf.keras.Input(shape=(32, 32, 3))\r\n    out_tensors = block(in_tensor)\r\n    model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name=\"FCN\")\r\n    return model", "> def block(x):\r\n> \r\n> ```\r\n> x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n> x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n> x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n> x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n> \r\n> x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n> x = tf.keras.layers.Flatten()(x)\r\n> \r\n> x = tf.keras.layers.Dense(10)(x)\r\n> x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)\r\n> \r\n> return x\r\n> ```\r\n> \r\n> def FCN(): # pylint: disable = invalid-name\r\n> in_tensor = tf.keras.Input(shape=(32, 32, 3))\r\n> out_tensors = block(in_tensor)\r\n> model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name=\"FCN\")\r\n> return model\r\n\r\nThis is the test_net", "The data we use is CIFAR10", "KANGRuipeng @ can you point me to the specific PR/bug that was fixed for TF 2?\r\nCan you add the end to end example(input data included) which reproduces this issue? I am unable to repro it with the example I have. Thanks!", "cifar10 = tf.keras.datasets.cifar10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nx_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0\r\n", "> cifar10 = tf.keras.datasets.cifar10\r\n> (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n> x_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0\r\n\r\nthis is the input data that I use", "> KANGRuipeng @ can you point me to the specific PR/bug that was fixed for TF 2?\r\n> Can you add the end to end example(input data included) which reproduces this issue? I am unable to repro it with the example I have. Thanks!\r\n\r\nany feedback?\r\n", "```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ndef block(x):\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n    x =tf.keras.layers.Conv2D(filters=96, kernel_size=3, strides=2, padding=\"same\")(x)\r\n\r\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n\r\n    x = tf.keras.layers.Dense(10)(x)\r\n    x = tf.keras.layers.Activation(activation=tf.nn.softmax)(x)\r\n\r\n    return x\r\ndef FCN():\r\n    in_tensor = tf.keras.Input(shape=(32, 32, 3))\r\n    out_tensors = block(in_tensor)\r\n    model = tf.keras.Model(inputs=in_tensor, outputs=out_tensors, name=\"FCN\")\r\n    return model\r\n```", "```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom test_net import FCN\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\r\n\r\n# load data and norm to [-1,1]\r\ncifar10 = tf.keras.datasets.cifar10\r\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\nx_train, x_test = (x_train - 128.0) / 128.0, (x_test - 128.0) / 128.0\r\n\r\nx_test= x_test[0:9984]\r\ny_test = y_test[0:9984]\r\n\r\n# config the used GPU memory size\r\nconfig = tf.ConfigProto(allow_soft_placement=True)\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.3\r\ntf.keras.backend.set_session(tf.Session(config=config))\r\n\r\n\r\nmirror_strategy = tf.distribute.MirroredStrategy()\r\nwith mirror_strategy.scope():\r\n    model = FCN()\r\n    sgd = tf.keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\r\n    model.compile(\r\n    optimizer=sgd, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\r\n    )\r\n\r\n    model.fit(\r\n        x_train, \r\n        y_train,\r\n        batch_size=256,\r\n        epochs=10,\r\n        validation_data=(x_test, y_test),\r\n        verbose=2,\r\n    )\r\n```", "@ravikyram @anj-s @tensorflowbutler @ymodak  The up two replies are the net-defination code and the training script. ", "Is there any one following this issue?", "@KANGRuipeng I am unable to repro this error. If you can share a colab link to the code and error message we can take a look. Also I have seen this message as a warning but not an error so I am surprised that this was a hard error. You can also try moving to TF 2 is that is possible. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35384\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35384\">No</a>\n", "> @KANGRuipeng I am unable to repro this error. If you can share a colab link to the code and error message we can take a look. Also I have seen this message as a warning but not an error so I am surprised that this was a hard error. You can also try moving to TF 2 is that is possible.\r\n\r\nIs there any report performance issue of TF1.15 under distributed mode?"]}, {"number": 35382, "title": "Save model to pb and pbtxt", "body": "I would like to use a model trained on python on C++\r\n\r\ntried by tflite but cmake needed don't work\r\nby the past, I had to use pb file and pbtxt file that works.\r\n\r\nIn TF 2.0 how could I save a Model to pb and pbtxt file to use it on C++\r\nOr is there a better way that doesn't involve Cmake ?\r\n\r\nThanks a lot,", "comments": ["@GaranceRichard, Please follow [this](https://www.tensorflow.org/guide/saved_model#load_a_savedmodel_in_c) guide to load a savedmodel in C++. Let us know if it helps. Thanks!", "with my model (as model) I did :\r\ntf.saved_model.save(model,\"\") which gives me a saved_model.pb\r\nsent to C++ dev, I'll tell you if it's ok...", "@GaranceRichard, Any update!", "Closing this issue now, Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 35381, "title": "pip install tensorflow-gpu==2.1.0rc2 failed!", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.1.0rc2\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia MX150\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe tensorflow-gpu-estimator error message occurs regardless of whether I install tensorflow-estimator first before tensorflow-gpu-2.1.0rc2 or with no installation of tensorflow-estimator.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nC:\\>pip install tensorflow-estimator\r\nCollecting tensorflow-estimator\r\n  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\r\nInstalling collected packages: tensorflow-estimator\r\nSuccessfully installed tensorflow-estimator-2.1.0\r\n\r\nC:\\>pip install tensorflow-gpu==2.1.0rc2\r\nCollecting tensorflow-gpu==2.1.0rc2\r\n  Using cached https://files.pythonhosted.org/packages/c5/5d/4d1cb80c38b987a8d0cace1d22a366c843af8966352bcc178450530e0a4f/tensorflow_gpu-2.1.0rc2-cp37-cp37m-win_amd64.whl\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in c:\\python37-64\\lib\\site-packages (from tensorflow-gpu==2.1.0rc2) (1.1.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in c:\\python37-64\\lib\\site-packages (from tensorflow-gpu==2.1.0rc2) (1.17.2+mkl)\r\nRequirement already satisfied: protobuf>=3.8.0 in c:\\python37-64\\lib\\site-packages (from tensorflow-gpu==2.1.0rc2) (3.10.0)\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0rc2) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0rc2)\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFollowing is my pip list output before I start installing tensorflow-estimator and tensorflow-gpu-2.1.0rc2.\r\n```\r\nC:\\>pip list\r\nPackage                 Version\r\n-----------              -----------\r\nabsl-py                 0.8.1\r\nappdirs                 1.4.3\r\nastor                   0.8.0\r\nautopep8                1.4.4\r\ncachetools              3.1.1\r\ncairocffi               1.1.0\r\ncertifi                 2019.9.11\r\ncffi                    1.13.1\r\nchardet                 3.0.4\r\nClick                   7.0\r\ncycler                  0.10.0\r\ndecorator               4.4.1\r\ndlib                    19.17.0\r\neditdistance            0.5.3\r\nentrypoints             0.3\r\nessential-generators    0.9.2\r\nface-recognition        1.2.3\r\nface-recognition-models 0.3.0\r\nflake8                  3.7.9\r\ngast                    0.2.2\r\ngoogle-auth             1.7.0\r\ngoogle-auth-oauthlib    0.4.1\r\ngoogle-pasta            0.1.8\r\ngraphviz                0.13\r\ngrpcio                  1.24.3\r\nh5py                    2.10.0\r\nidna                    2.8\r\nimageio                 2.6.1\r\nimgaug                  0.3.0\r\nimutils                 0.5.3\r\nkaldiio                 2.14.1\r\nKeras                   2.3.1\r\nKeras-Applications      1.0.8\r\nkeras-ocr               0.2\r\nKeras-Preprocessing     1.1.0\r\nkiwisolver              1.1.0\r\nllvmlite                0.30.0\r\nMako                    1.1.0\r\nMarkdown                3.1.1\r\nMarkupSafe              1.1.1\r\nmatplotlib              3.1.1\r\nmccabe                  0.6.1\r\nmnist                   0.2.2\r\nmock                    3.0.5\r\nnetworkx                2.4\r\nnumba                   0.46.0\r\nnumpy                   1.17.2+mkl\r\noauthlib                3.1.0\r\nopencv-contrib-python   4.1.1.26\r\nopencv-python-headless  4.1.2.30\r\nopt-einsum              3.1.0\r\nPillow                  6.1.0\r\npip                     19.3.1\r\nprotobuf                3.10.0\r\npyasn1                  0.4.7\r\npyasn1-modules          0.2.7\r\npycodestyle             2.5.0\r\npycparser               2.19\r\npycuda                  2019.1.2\r\npydot                   1.4.1\r\npyflakes                2.1.1\r\npyparsing               2.4.2\r\npypiwin32               223\r\npytesseract             0.3.0\r\npython-dateutil         2.8.0\r\npytools                 2019.1.1\r\npyttsx3                 2.71\r\nPyWavelets              1.1.1\r\npywin32                 224\r\nPyYAML                  5.1.2\r\nrequests                2.22.0\r\nrequests-oauthlib       1.3.0\r\nrsa                     4.0\r\nscikit-cuda             0.5.3\r\nscikit-image            0.16.2\r\nscipy                   1.4.1\r\nsetuptools              41.4.0\r\nShapely                 1.6.4.post2\r\nsix                     1.12.0\r\ntermcolor               1.1.0\r\nurllib3                 1.25.6\r\nWerkzeug                0.16.0\r\nwheel                   0.33.6\r\nwrapt                   1.11.2\r\n```", "comments": ["@OoiSC, Not necessary to install tensorflow-estimator first. Install tensorflow-gpu, which internally install all the dependencies like tensorflow-estimator, tensorboard. Please check the attached [gist](https://colab.sandbox.google.com/gist/gadagashwini/63a3ed96f09f56bf7f665565f239e082/untitled323.ipynb) for more. Thanks!", "I also tried installing tensorflow-gpu 2.1.0rc2 without first installing\ntensorflow-estimator.\n\nFailed with exactly same error message.\n\nI ran my installation at the Windows 10 command line using pip install.\n\n\nOn Tue, 24 Dec 2019, 16:32 gadagashwini, <notifications@github.com> wrote:\n\n> @OoiSC <https://github.com/OoiSC>, Not necessary to install\n> tensorflow-estimator first. Install tensorflow-gpu, which internally\n> install all the dependencies like tensorflow-estimator, tensorboard. Please\n> check the attached gist\n> <https://colab.sandbox.google.com/gist/gadagashwini/63a3ed96f09f56bf7f665565f239e082/untitled323.ipynb>\n> for more. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35381?email_source=notifications&email_token=AN2LX2CWTBXQXOXFFWJGMJDQ2HCIZA5CNFSM4J63OP4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHSZDJA#issuecomment-568693156>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN2LX2HYUXHMX7ZJ4MBUD5TQ2HCIZANCNFSM4J63OP4A>\n> .\n>\n", "The error message from my installation is\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement\r\ntensorflow-gpu-estimator<2.2.0,>=2.1.0rc0 (from tensorflow-gpu==2.1.0rc2)\r\n(from versions: none)\r\n```\r\n\r\nIt is looking for tensorflow-gpu-estimator which does not exist.\r\n\r\nOn Tue, 24 Dec 2019, 16:32 gadagashwini, <notifications@github.com> wrote:\r\n\r\n> @OoiSC <https://github.com/OoiSC>, Not necessary to install\r\n> tensorflow-estimator first. Install tensorflow-gpu, which internally\r\n> install all the dependencies like tensorflow-estimator, tensorboard. Please\r\n> check the attached gist\r\n> <https://colab.sandbox.google.com/gist/gadagashwini/63a3ed96f09f56bf7f665565f239e082/untitled323.ipynb>\r\n> for more. Thanks!\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/35381?email_source=notifications&email_token=AN2LX2CWTBXQXOXFFWJGMJDQ2HCIZA5CNFSM4J63OP4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHSZDJA#issuecomment-568693156>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AN2LX2HYUXHMX7ZJ4MBUD5TQ2HCIZANCNFSM4J63OP4A>\r\n> .\r\n>\r\n", "@OoiSC, Could you uninstall existing tensorflow-gpu and create virtual environment and install it again.\r\nThanks!\r\n", "I have shown the list of python modules installed by pip list. You will\nnotice that there are no tensorflow-gpu or any tensorflow modules\ninstalled. This means that I install tensorflow-gpu in an environment where\nI had uninstalled all tensorflow modules.\n\n\nI do not use virtual environments.\n\n\nOn Tue, 24 Dec 2019, 17:20 gadagashwini, <notifications@github.com> wrote:\n\n> @OoiSC <https://github.com/OoiSC>, Could you uninstall existing\n> tensorflow-gpu and create virtual environment and install it again.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35381?email_source=notifications&email_token=AN2LX2HQO5K7VVJQLQ4JHRLQ2HH4BA5CNFSM4J63OP4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHS3UYY#issuecomment-568703587>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN2LX2DYOK2MBSBNGLNXXYLQ2HH4BANCNFSM4J63OP4A>\n> .\n>\n", "Couldn't reproduce on Linux but was able to reproduce in Windows.\r\n\r\nIt seems this comes from the way we generate the new pip packages (@yifeif, can you take a look?)\r\n\r\nI'll create a `tensorflow-gpu-estimator` pip package to temporarily fix this.", "I created the `tensorflow-gpu-estimator` pip package and was able to `pip install tensorflow-gpu==2.1.0rc2` on Windows. @OoiSC, can you please try this too?", "Thanks. I was sick during the last 2\ndays and I did not read my email until today.\n\nWill try tomorrow.\n\n\nOn Fri, 27 Dec 2019, 01:26 Mihai Maruseac, <notifications@github.com> wrote:\n\n> I created the tensorflow-gpu-estimator pip package and was able to pip\n> install tensorflow-gpu==2.1.0rc2 on Windows. @OoiSC\n> <https://github.com/OoiSC>, can you please try this too?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35381?email_source=notifications&email_token=AN2LX2FAFNCUZQ6ZYNFURM3Q2TSLFA5CNFSM4J63OP4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHV3THY#issuecomment-569096607>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN2LX2H4NZMR4V5LJ7MU2Y3Q2TSLFANCNFSM4J63OP4A>\n> .\n>\n", "My windows 10 installation of tensorflow-gpu==2.1.0.rc2 is now successful\nas it can now collect tensorflow-gpu=estimator.\n\nThanks!\n\nOn Fri, Dec 27, 2019 at 1:26 AM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> I created the tensorflow-gpu-estimator pip package and was able to pip\n> install tensorflow-gpu==2.1.0rc2 on Windows. @OoiSC\n> <https://github.com/OoiSC>, can you please try this too?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35381?email_source=notifications&email_token=AN2LX2FAFNCUZQ6ZYNFURM3Q2TSLFA5CNFSM4J63OP4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHV3THY#issuecomment-569096607>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN2LX2H4NZMR4V5LJ7MU2Y3Q2TSLFANCNFSM4J63OP4A>\n> .\n>\n", "Thank you for confirming. I'm going to close the issue then, as it seems solved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35381\">No</a>\n", "the exact same issue arise for tensorflow-gpu==2.2.0-rc2", "Thanks for the ping. We will release estimator 2.2.0 final this week and we should fix this. Reopening this issue until then.", "Estimator 2.2.0 has been released and it also has the gpu counterpart. When TF 2.2.0 gets released (very soon now, probably by Monday), this should be fixed.", "Closing as all components have been released", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35381\">No</a>\n"]}, {"number": 35380, "title": "tensorflow lite performance slower than regular tensorflow model", "body": "i converted object detection model into tensorflow lite and i am trying to run it in raspberry pi but the performance of tensorflow lite is slower in desktop. How can i increase the performance speed of the tensorflow lite? ", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "- OS: Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 0.15.2\r\n- CUDA/cuDNN version: 9.0\r\n\r\n\r\n    def __init__(self, model_path='TFLite_model/detect.tflite'):\r\n        self.car_boxes = []\r\n        # define lite graph and load tensorflow\r\n        self.interpreter = Interpreter(model_path=model_path)\r\n        self.interpreter.allocate_tensors()\r\n        self.input_details = self.interpreter.get_input_details()\r\n        self.output_details = self.interpreter.get_output_details()\r\n\r\n    # Helper function to convert image into numpy array\r\n    \"\"\"def load_image_into_numpy_array(self, image):\r\n        (im_width, im_height) = image.size\r\n        return np.array(image.getdata()).reshape(\r\n            (im_height, im_width, 3)).astype(np.uint8)\r\n        # Helper function to convert normalized box coordinates to pixels\"\"\"\r\n\r\n    def box_normal_to_pixel(self, box, dim):\r\n        height, width = dim[0], dim[1]\r\n        box_pixel = [int(box[0] * height), int(box[1] * width), int(box[2] * height), int(box[3] * width)]\r\n        return np.array(box_pixel)\r\n\r\n    def get_localization(self, image):\r\n        # Resize and normalize image for network input\r\n        frame = cv2.resize(image, (300, 300))\r\n        frame = np.expand_dims(frame, axis=0)\r\n        frame = (2.0 / 255.0) * frame - 1.0\r\n        frame = frame.astype('float32')\r\n\r\n        category_index = {1: {'id': 1, 'name': u'pothole'}}\r\n\r\n        # run model\r\n        self.interpreter.set_tensor(self.input_details[0]['index'], frame)\r\n        self.interpreter.invoke()\r\n\r\n        # get results\r\n        boxes = self.interpreter.get_tensor(self.output_details[0]['index'])\r\n        classes = self.interpreter.get_tensor(self.output_details[1]['index'])\r\n        scores = self.interpreter.get_tensor(self.output_details[2]['index'])\r\n        num = self.interpreter.get_tensor(self.output_details[3]['index'])\r\n\r\n        visualization_utils.visualize_boxes_and_labels_on_image_array(image,\r\n                                                                      np.squeeze(boxes[0]),\r\n                                                                      np.squeeze(classes[0]+1).astype(np.int32),\r\n                                                                      np.squeeze(scores[0]),\r\n                                                                      category_index,\r\n                                                                      use_normalized_coordinates=True,\r\n                                                                      min_score_thresh=.4,\r\n                                                                      line_thickness=3)\r\n\r\n        # plt.figure(figsize=(9, 6))\r\n        # plt.imshow(image)\r\n        # plt.show()\r\n\r\n        boxes = np.squeeze(boxes[0])\r\n        classes = np.squeeze(classes[0]+1)\r\n        scores = np.squeeze(scores[0])\r\n\r\nThis is the code I have been running but it is quite slow. Can you please tell me why its performance speed is so slow?", "Hey, @Red025  I will suggest you for optimizing your tf.lite model. You can look here for Model optimization.\r\nhttps://www.tensorflow.org/lite/performance/model_optimization\r\nI hope this is what you're looking for.\r\n\r\n", "Hi,\r\n\r\nNot all kernels are optimized to run on desktop. What is the model you're trying to run ? can you share it ?\r\n\r\nThanks", "i have used ssd_mobilenet to train the model ", "Assigning to T.J who looks in work related to desktop.\r\n\r\nT.J can you please have a look.", "Hi,\r\n\r\nMay I ask, what model of Raspberry Pi are you using and may I see the bazel command you used to build? Thanks!\r\n", "https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop\r\n\r\nTensorflow Lite is optimized for ARM neon, and is slower running on your x86_64 desktop. ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "I am running it on TF2.0/Windows Desktop with GPU. ITs extremely slow\r\nCan someone help me?\r\n", "> I am running it on TF2.0/Windows Desktop with GPU. ITs extremely slow Can someone help me?\r\n\r\nme too..\r\n"]}, {"number": 35379, "title": "Cannot export keras model to SavedModel if mixed-precision policy is enabled", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): starting from 2.0, nightly version tested\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen keras mixed-precision policy \"mixed_float16\" is in use, we can't save the keras model in SavedModel format with method `keras.models.Model.save` without a specific `signatures`. It seems like a mismatch between input signature inferred by the model itself and the auto-casted inputs:\r\n\r\n```\r\nValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'conv1_pad/Cast:0' shape=(None, 224, 224, 3) dtype=float16>,)), input_signature ((TensorSpec(shape=(None, None, None, None), dtype=tf.float32, name=None),))\r\n```\r\n\r\nAlthough we can use `graph_rewrite` as mixed-precision training method to bypass this autocasting issue, but `graph_rewrite` is not working in some cases (e.g. train a subclassed model with `tf.GradientTape`) thus it is not recommended by tensorflow official guide. For flexibility we do hope to use mixed-precision policy in mixed-precision training, and directly exporting mixed-precision trained model to SavedModel for deployment is straightforward in production pipeline.\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\nWe can reproduce this bug by using the official image classification training example from https://github.com/tensorflow/models/tree/master/official/vision/image_classification\r\n\r\n```\r\n\"\"\"\r\nTest mixed-precision policy model saving\r\n\"\"\"\r\nimport logging\r\nimport os\r\n\r\nfrom absl import app as absl_app\r\nimport tensorflow as tf\r\n\r\nfrom official.vision.image_classification.resnet_model import resnet50\r\n\r\n\r\ndef main(argv):\r\n  tf.compat.v1.enable_eager_execution()\r\n  \r\n  # setup mixed-precision policy\r\n  # the policy enables the autocasting behavior in keras layers\r\n  policy = tf.keras.mixed_precision.experimental.Policy(\r\n        'mixed_float16', loss_scale=128)\r\n  tf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n  model = resnet50(1000)\r\n  model_dir = 'temp/saved_model_test'\r\n\r\n  if not os.path.isdir(model_dir):\r\n    os.makedirs(model_dir)\r\n  model.save(model_dir,\r\n             save_format='tf')\r\n  logging.info('Exported trained model to directory {}'.format(\r\n      model_dir))\r\n\r\n\r\nif __name__ == '__main__':\r\n  absl_app.run(main)\r\n```\r\n", "comments": ["Gently ping @reedwm; does this look familiar to you?", "When tried replicating the issue, I got the error as per the colab [gist](https://colab.sandbox.google.com/gist/oanush/1cc1d54c6411da3cdc2d5009b59775b1/35379.ipynb).Thanks!", "Thank you for filing this and the short example to reproduce. I will have a fix soon. In the future, I plan on testing the official models with SavedModel to ensure issues like this do not occur in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35379\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35379\">No</a>\n", "@reedwm thanks for the prompt fix!\r\n\r\n@goldiegadde shall we apply the fix above to r2.1? Mixed precision along with XLA improves training performance dramatically and we\u2019d really like to use it with our e2e training/inference pipeline.", "Is this working for anyone? I updated to 2.1 and I no longer get this same error but now I get\r\n\r\nTypeError: Input 'filter' of 'Conv2D' Op has type float16 that does not match type float32 of argument 'input'.\r\n\r\nI tried this first on my models and then ran the given snippet and got the same deal.", "Unfortunately the fix did not make it into 2.1 :(. It will be in 2.2 however, and the fix is already in `tf-nightly`\r\n\r\nI'm not sure why the error message is different. Note that `Model.save_weights` still works, just not `Model.save`. \r\n\r\nAs a workaround, you can call `Model.save_weights`, rebuild the model in fp32, call `Model.load_weights` on the fp32 model then call `Model.save`. But understandably, this is very irritating, and it doesn't save a mixed precision model. Using the old [`tf.train.experimental.enable_mixed_precision_graph_rewrite`](https://www.tensorflow.org/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite) API also works but note we plan on deprecating then removing it (it will remain in `tf.compat.v1` for a long time though)."]}, {"number": 35378, "title": "TensorflowLite gives wrong results when use GPU delegate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (Android7.1 Snapdragon 625):\r\n- Mobile device (HUAWEI Nova CAZ-AL 10) if the issue happens on mobile device:\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly):\r\n\r\n\r\n**Describe the current behavior**\r\nI use **yolov3** model on android platform to do object detection. When I did object detection on CPU the results are right but when I add GPU module the results are totally different.\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n/**\r\n * Wrapper for frozen detection models trained using the Tensorflow Object Detection API:\r\n * github.com/tensorflow/models/tree/master/research/object_detection\r\n */\r\npublic class TFLitePanoObjectDetectionAPIModel implements Classifier {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  private static final int NUM_of_classes = 19;\r\n  private static final int NUM_THREADS = 4;\r\n  private boolean isModelQuantized;\r\n  private int inputSize;\r\n  private Vector<String> labels = new Vector<String>();\r\n  private int[] intValues;\r\n  private float[][][][] output_1;\r\n  private float[][][][] output_2;\r\n  private float[][][][] output_3;\r\n  private ByteBuffer imgData;\r\n  private Interpreter tflite;\r\n  private MappedByteBuffer tfliteModel;\r\n  private final Interpreter.Options tfliteOptions = new Interpreter.Options();\r\n  private GpuDelegate gpuDelegate = null;\r\n  private NnApiDelegate nnapiDelegate = null;\r\n  private int gridNum;\r\n\r\n  private int BoxNum_each_gird=3;\r\n  private float[][][][] floatValues;\r\n\r\n\r\n  private TFLitePanoObjectDetectionAPIModel() {}\r\n  private float scoreThreshold = 0.3f;\r\n  private int blockSize=32;\r\n\r\n  public static Classifier create(\r\n      final AssetManager assetManager,\r\n      final String modelFilename,\r\n      final String labelFilename,\r\n      final int inputSize,\r\n      final boolean isQuantized)\r\n      throws IOException {\r\n    final TFLitePanoObjectDetectionAPIModel d = new TFLitePanoObjectDetectionAPIModel();\r\n    try {\r\n\r\n      d.tfliteModel=loadModelFile(assetManager, modelFilename);\r\n      if (d.gpuDelegate == null)\r\n      {\r\n        d.gpuDelegate = new GpuDelegate();\r\n        d.tfliteOptions.addDelegate(d.gpuDelegate);\r\n      }\r\n      d.tflite = new Interpreter(d.tfliteModel,d.tfliteOptions);\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 4; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * 2*d.inputSize * 3 * numBytesPerChannel);//\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * 2*d.inputSize];\r\n\r\n    d.gridNum=d.inputSize/32;\r\n    d.floatValues=new float[1][d.inputSize][2 * d.inputSize ][3];\r\n    d.output_1 = new float[1][][][];\r\n    d.output_2 = new float[1][][][];\r\n    d.output_3 = new float[1][][][];\r\n\r\n\r\n    return d;\r\n  }\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n    imgData.rewind();\r\n\r\n    for (int i = 0; i < inputSize; ++i)\r\n    {\r\n      for (int j = 0; j < 2*inputSize; ++j)\r\n      {\r\n        int pixelValue = intValues[i * 2*inputSize + j];\r\n          // Float model\r\n          floatValues[0][i][j][0]=((pixelValue >> 16) & 0xFF)/255.0f ;\r\n          floatValues[0][i][j][1]=((pixelValue >> 8) & 0xFF)/255.0f ;\r\n          floatValues[0][i][j][2]=(pixelValue& 0xFF) /255.0f;\r\n      }\r\n    }\r\n\r\n     int channelNum=BoxNum_each_gird*(NUM_of_classes+5);\r\n\r\n    output_1 = new float[1][gridNum][2*gridNum][channelNum];\r\n    output_2 = new float[1][2*gridNum][4*gridNum][channelNum];\r\n    output_3 = new float[1][4*gridNum][8*gridNum][channelNum];\r\n\r\n    Object[] inputArray = {floatValues};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, output_1);\r\n    outputMap.put(1, output_2);\r\n    outputMap.put(2, output_3);\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    long startTime = SystemClock.uptimeMillis();\r\n    tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    long lastingTime=SystemClock.uptimeMillis()-startTime;\r\n\r\n    LOGGER.i(\"runForMultipleInputsOutputs time of each image: \" + lastingTime + \"ms\");\r\n    Trace.endSection(); \r\n    }\r\n\r\n}`\r\n**Describe the expected behavior**\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["PS: I can run the demos in tensorflowlite correctly on my phone.\r\n \r\nAnd I used **modifyGraphWithDelegate()** function and got the following logs:\r\n\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nI/tflite: Created TensorFlow Lite delegate for GPU.\r\nE/libEGL: validate_display:99 error 3008 (EGL_BAD_DISPLAY)\r\nI/tflite: Initialized OpenCL-based API.\r\nI/art: Background sticky concurrent mark sweep GC freed 580(74KB) AllocSpace objects, 1(20KB) LOS objects, 0% free, 74MB/74MB, paused 287us total 174.102ms\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 8779\r\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.lite.examples.detection/org.tensorflow.lite.examples.detection.PanoDetectorActivity}: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MAXIMUM: Operation is not supported.\r\n    RESIZE_NEAREST_NEIGHBOR: Operation is not supported.\r\n    First 2 operations will run on the GPU, and the remaining 249 on the CPU.\r\n    ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2793)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2864)\r\n        at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1567)\r\n        at android.os.Handler.dispatchMessage(Handler.java:105)\r\n        at android.os.Looper.loop(Looper.java:156)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6523)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:942)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:832)\r\n     Caused by: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Next operations are not supported by GPU delegate:\r\n    MAXIMUM: Operation is not supported.\r\n    RESIZE_NEAREST_NEIGHBOR: Operation is not supported.\r\n    First 2 operations will run on the GPU, and the remaining 249 on the CPU.\r\n    ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.modifyGraphWithDelegate(NativeInterpreterWrapper.java:195)\r\n        at org.tensorflow.lite.Interpreter.modifyGraphWithDelegate(Interpreter.java:409)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLitePanoObjectDetectionAPIModel.recognizeImage(TFLitePanoObjectDetectionAPIModel.java:281)\r\n        at org.tensorflow.lite.examples.detection.PanoDetectorActivity.onCreate(PanoDetectorActivity.java:134)\r\n        at android.app.Activity.performCreate(Activity.java:6915)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1123)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2746)\r\n        \t... 9 more\r\nI/Process: Sending signal. PID: 8779 SIG: 9\r\nDisconnected from the target VM, address: 'localhost:8608', transport: 'socket'\r\n\r\nSo, I have questions:\r\n1. Did I start GPU correctly?\r\n2. I only add GPU option to my code why the results are totally different compared with results on CPU whose results are right?", "That's quite a wall of code that you just dumped on the wall =/\r\n\r\n> Did I start GPU correctly?\r\n\r\nApparently not; the real failure is:\r\n\r\n> ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n\r\nYou would have to find out what triggers that condition.  I, unfortunately, haven't experienced that condition and can't tell what you're doing wrong.\r\n\r\n> I only add GPU option to my code why the results are totally different compared with results on CPU whose results are right?\r\n\r\nBased on:\r\n\r\n> First 2 operations will run on the GPU, and the remaining 249 on the CPU.\r\n\r\nThere is some relay; first 2 ops on the GPU, and then the 249 ops on the CPU.  Maybe something is broken there?", "@impjdi Thank you very much for your help. Sorry with such wall of codes.\r\n\r\nToday I find a strange thing that the nomalization methods of input data for using CPU and GPU are different. Is that right?\r\n\r\nSeveral days ago I normalized my input like:\r\n```\r\n floatValues[0][i][j][0]=((pixelValue >> 16) & 0xFF)/255.0f ;\r\n floatValues[0][i][j][1]=((pixelValue >> 8) & 0xFF)/255.0f ;\r\n floatValues[0][i][j][2]=(pixelValue& 0xFF) /255.0f;\r\n```\r\nThis normalization was the same as using CPU and the results were totally different, but today I change my input normalization like:\r\n\r\n```\r\nfloatValues[0][i][j][0]=(((pixelValue >> 16) & 0xFF)-127.5f)/127.5f ;\r\nfloatValues[0][i][j][1]=(((pixelValue >> 8) & 0xFF)-127.5f)/127.5f ;\r\nfloatValues[0][i][j][2]=((pixelValue& 0xFF)-127.5f) /127.5f;\r\n```\r\nThe results turns to be better but not totally the same as using CPU's results.\r\n\r\nNow I have other questions:\r\n1. Are normalization methods of input for using CPU and GPU different?\r\n2. Should the detection results of CPU and GPU be totally the same?\r\n3. My input is 832x416, is that too large for GPU?\r\n4. There are some ops like LeakyRelu which is not supported by TFLite-GPU. Will this affect my precision?\r\n\r\nExpecting your answers!\r\nThank you very much.", "1. For the same model, normalization should be the same.  GPU doesn't do anything on top of it; it just processes the input as is.\r\n\r\n2. It can't be totally the same, because of floating point precision loss.  Even with CPU, two implementations will give you different numbers.  Remember, with floating point, a + b + c is not the same as c + b + a.  It should be close though.\r\n\r\n3. It depends on the intermediate tensors too, not just the input tensor.  However, as a data point, I had to reduce 513x513 in DeepLab segmentation to 257x257 so that it runs on the GPU, because 513x513 was too huge.  But in those cases, you will most likely get other types of error (out of memory or something like that), rather than a wrong answer.\r\n\r\n4. You would have to read the log; it will tell you how many ops were run on the GPU and how many on the CPU.  In normal case, you should get a similar answer because if GPU fails to replace parts of the graph, it will fall back to CPU.", "@impjdi  I totally agree with your comments. \r\nI checked supported ops again and I found that the version sometimes is V1 sometimes is V2. Do you know the exact version like 1.13. We have done a lot of tests and we found that problems occured at concatenation op. The problem might be the version of TF we used when we transformed the model. We haved tested 1.15, 1.14,1.13 the concatenation's results are all wrong.\r\n![concatenate_cpu_value](https://user-images.githubusercontent.com/19260578/72862340-c5c04900-3d07-11ea-81ce-12d21eeca633.JPG)\r\n\r\n\r\n![concatenate_no_value](https://user-images.githubusercontent.com/19260578/72862342-c6f17600-3d07-11ea-927b-ab5bdbafa6a4.JPG)\r\n", "@wangyiwei0306 We see that you are using older version of tensorflow .Many bug have been fixed in latest version. We recommend that you upgrade to latest stable version of tensorflow 2.6.0 and let us know if the issue still persists in newer versions .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 35377, "title": "Get error when build tensorflow-lite-hexagon.aar", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.0\r\n- Python version:2.7.15\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):1.1.0\r\n- GCC/Compiler version (if compiling from source):8.3.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI try to build a tensorflow-lite-hexagon.aar with the instructions as follows:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/hexagon_delegate.md\r\nIt used the command :\r\nbazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon\r\nto build.But I get the error like this:\r\nERROR: /prj/F3060803/Project/Tensorflow/tensorflow-master/tensorflow/lite/c/BUILD:130:1: C++ compilation of rule '//tensorflow/lite/c:common' failed (Exit 1)\r\nTarget //tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon failed to build\r\nI wonder how should I do?Thanks\r\n\r\n", "comments": ["Hi,\r\n  I try to build tensorflow-lite-hexagon.aar with the lasted code from github,but it always could not be successful and gave the different errors each times.When google can provide the stable release?Thanks", "Hi,\r\nSorry for late reply as i was out of office.\r\nI just tried building it and it is working. Can you try again ?\r\n\r\nWe will be providing a hosted version on JCenter to use soon.\r\n\r\nThanks", "Hi,\r\nThank you for your reply.Can you tell me which version on the git you tried?I have tried the lasted code downloaded from github,but still got the error:\r\nERROR: /home/foxconn/.cache/bazel/_bazel_foxconn/dd512d2a00295bff465168b5ce1aeaf7/external/androidndk/BUILD.bazel:41:1: in cc_toolchain_suite rule @androidndk//:toolchain-gnu-libstdcpp: cc_toolchain_suite '@androidndk//:toolchain-gnu-libstdcpp' does not contain a toolchain for cpu 'k8'\r\nThe tf version is 2.0 and python version is 2.7.15.Can you provide me the version you sucessed?Thanks", "Steps:\r\n1) git clone https://github.com/tensorflow/tensorflow.git\r\n2) cd tensorflow\r\n3) run ./configure\r\n** don't forget to configure the android workspace - Answer Y when asked \r\n\"Would you like to interactively configure ./WORKSPACE for Android builds?\"\r\n4)  bazel build -c opt --config=android_arm64 tensorflow/lite/experimental/delegates/hexagon/java:tensorflow-lite-hexagon\r\n\r\n\r\nIf you followed the above steps and still have a problem, then please send details on bazel version you're using and what is the environment setup you configured for android (point #3 above)\r\n\r\nThanks", "Hi,\r\nI have tried again and still get the error.Here is the details:\r\nERROR: /prj/F3060803/Project/Tensorflow/tensorflow-lastest/tensorflow-master/tensorflow/lite/experimental/delegates/hexagon/builders/BUILD:8:1: C++ compilation of rule '//tensorflow/lite/experimental/delegates/hexagon/builders:op_builder' failed (Exit 1)\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:31:36: error: no member named 'numeric_limits' in namespace 'std'\r\n                              std::numeric_limits<uint8_t>::min(),\r\n                              ~~~~~^\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:31:51: error: unexpected type name 'uint8_t': expected expression\r\n                              std::numeric_limits<uint8_t>::min(),\r\n                                                  ^\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:31:61: error: no member named 'min' in the global namespace\r\n                              std::numeric_limits<uint8_t>::min(),\r\n                                                          ~~^\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:32:36: error: no member named 'numeric_limits' in namespace 'std'\r\n                              std::numeric_limits<uint8_t>::max());\r\n                              ~~~~~^\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:32:51: error: unexpected type name 'uint8_t': expected expression\r\n                              std::numeric_limits<uint8_t>::max());\r\n                                                  ^\r\ntensorflow/lite/experimental/delegates/hexagon/builders/neg_op_builder.cc:32:61: error: no member named 'max' in the global namespace\r\n                              std::numeric_limits<uint8_t>::max());\r\n                                                          ~~^\r\n6 errors generated.\r\n\r\nThe bazel version is 1.1.0 and android configuration I set is:\r\nmin android-ndk api level: 21\r\nandroid api level: 28\r\nandroid build tools: 28.0.3\r\n\r\nPlease help to check.Thanks", "Which NDK are you using.\r\nCan you try with ndk r17c ?", "I use r16b now and I will try r17c later.", "Hi,\r\nI used r17c to built and it was successful.But the size of .aar is only 2.2kB and when I excute the code:hexagonDelegate = new HexagonDelegate(activity);\r\nI got the error:\r\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.lite.examples.detection-z7pYJM-s1UhmIszRu4C0Qg==/base.apk\"],nativeLibraryDirectories=[/data/app/org.tensorflow.lite.examples.detection-z7pYJM-s1UhmIszRu4C0Qg==/lib/arm64, /data/app/org.tensorflow.lite.examples.detection-z7pYJM-s1UhmIszRu4C0Qg==/base.apk!/lib/arm64-v8a, /system/lib64, /system/product/lib64]]] couldn't find \"libtensorflowlite_hexagon_jni.so\"\r\nwhere can I find the libtensorflowlite_hexagon_jni.so?Thanks", "Hi,karimnosseir\r\n  Do you have any suggestions?", "Hi @thinksmert \r\n\r\nThe AAR is now available on bintray, can you download it and try it\r\nhttps://bintray.com/google/tensorflow/tensorflow-lite-hexagon/\r\n\r\nNote: It might take up to 24 Hrs before Gradle dependency works, so if you got failure for resolving the dependency then try downloading and adding to your app", "Hi,karimnosseir\r\n Thank you for your reply and I will try the aar later.But I still want to know where can I find libtensorflowlite_hexagon_jni.so.I guess it should be built from hexagon delegate source code because I found some build rule about the .so in tensorflow\\lite\\experimental\\delegates\\hexagon\\java\\BUILD file.But I don't know why it does not be built.", "Hi @thinksmert,\r\nIt should be built by the rule defined in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/delegates/hexagon/java/BUILD\r\n\r\nBut looks like you have some configuration issue which results in something wrong.\r\n\r\nCan you please try the AAR\r\n\r\nThanks", "I mean the Jcenter hosted one\r\n\r\nThanks", "Hi,karimnosseir\r\nWhich issue should be corrected?I don't change the built rule and it is just the one downloaded from git.\r\nOtherwise, I have tried the AAR and it works.But it sames can only use quantized model?I use quantized model and the accuracy is bad than float model with gpu.Do you have any suggestion that can improve the accuracy?\r\nThanks ", "By issue i meant configuration issue for the environment (for example NDK).\r\nAnyways, you can use the one provided on JCenter - glad it is working.\r\nFor quantized, yes it works for quantized models only.\r\nRefer to https://www.tensorflow.org/lite/performance/best_practices#quantization\r\nfor some pointers if you're quantizing your model.\r\nIt is expected that you will lose some accuracy, but how much you're losing depends on what you're doing.\r\n\r\nThanks", "Closing the bug, since the issue is fixed with switching to using the AAR provided on JCenter.\r\n\r\nThanks"]}, {"number": 35376, "title": "7.4.1 but source was compiled with: 7.6.0", "body": "Successfully opened dynamic library libcudnn.so.7\r\n2019-12-24 05:34:15.862520: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.1 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\n\r\n\r\n**System information**\r\n- Ubuntu 16.04\r\n- TensorFlow version: tensorflow-gpu==1.15.0\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:-virtualenv\r\n- CUDA 10\r\ncuDNN version:\r\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 4\r\n#define CUDNN_PATCHLEVEL 1\r\n--\r\n#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n- GPU model and memory:\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@SreenijaK, Please elaborate the issue and also provide the exact sequence of commands / steps that you executed before running into the problem. Thanks! ", "@SreenijaK, Elaborate the issue with context. Thanks!", "you should download the  cuDNN v7.6.5 in the nivida website then release it. you will find a youfilefolder/cuda/lib64, then add the lib64 to your LD_LIBRARY_PATH. It works for me, my code was this:\r\n1. download cudnn-10.0-linux-x64-v7.6.5.32.tgz\r\n2.tar -zxvf cudnn-10.0-linux-x64-v7.6.5.32.tgz\r\n3. vim ~/.bashrc, add this line\"export $LD_LIBRARY_PATH=yourfolder/cuda/lib64:$LD_LIBRARY_PATH\" and save\r\n4. source ~/.bashrc", "@SreenijaK, Download the cuDNN 7.6.5 from nvidia website https://developer.nvidia.com/rdp/cudnn-archive. And install the following packages\r\n```\r\nsudo dpkg -i bcudnn7_7.6.5.32-1+cuda10.0_amd64.deb\r\nsudo dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.0_amd64.deb\r\nsudo dpkg -i libcudnn7-doc_7.6.5.32-1+cuda10.0_amd64.deb\r\n```\r\nAdd the path $LD_LIBRARY_PATH as mentioned by @Xiaohui-Z.\r\nLet us know how it progress. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@gadagashwini do I need to uninstall previous version first?", "@PalashSharma15, In case of multiple versions, Add the desired version path $LD_LIBRARY_PATH ", "> @PalashSharma15, In case of multiple versions, Add the desired version path $LD_LIBRARY_PATH\r\n\r\nOkay Thanks"]}, {"number": 35375, "title": "Add complex64 and complex128 support for tf.stack on GPU", "body": "This PR tries to address the issue raised in #34575\r\nwhere there is no GPU support for complex64 and complex128\r\nfor tf.stack\r\n\r\nThis PR adds the complex64 and complex128 support.\r\n\r\nThis PR fixes #34575\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 35374, "title": "Add usage example to tf.image.pad_to_bounding_box", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35374) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35374) for more info**.\n\n<!-- ok -->", "@RichardXiao13 thank you, it is still failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/17549807-4074-4d49-bc57-37698b3250d3/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests)\r\n\r\nPlease run the doctest locally as mentiones here in the [contributor guidelines.](https://www.tensorflow.org/community/contribute/docs_ref)", "@rthadur I am having some trouble getting the doctest to run on my machine. I have been trying for many days to no avail. I am getting an import error message stating \r\n\"\r\nTraceback (most recent call last):\r\nFile \"tf_doctest.py\", line 31, in <module>\r\nfrom tensorflow.tools.docs import tf_doctest_lib\r\nImportError: cannot import name 'tf_doctest_lib' \r\n\"\r\n I am not quite sure how to resolve this.", "Actually, I just figured it out!", "@RichardXiao13 thank you for fixing ,we see some sanity checks failing , can you please check.", "Just updated it!", "@RichardXiao13 Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 35373, "title": "Added Usage Example for Image.Transpose()", "body": "The usage example I added\r\nUsage Example: ```python >> import tensorflow as tf >> x =\r\n    tf.random.normal(shape=(100, 200, 3)) >> tf.image.transpose(x) `", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35373) for more info**.\n\n<!-- need_sender_cla -->", "\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35373) for more info**.\n\n<!-- ok -->", "Yes, please. I feel like I'm reviewing 100 versions of the same thing all\nwith the same problem.\n\nMaybe tell the following to all submitters:\n 1. Read https://www.tensorflow.org/community/contribute/docs_ref\n 2. make sure there are no floating point numbers with too much precision\n(replace the trailing digits with ...)\n 3. make sure there are no random numbers in the doctest (as otherwise it's\nflaky)\n\nOn Thu, Dec 26, 2019 at 7:54 PM Kilaru Yasaswi Sri Chandra Gandhi <\nnotifications@github.com> wrote:\n\n> *@kyscg* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/image_ops_impl.py\n> <https://github.com/tensorflow/tensorflow/pull/35373#discussion_r361575246>\n> :\n>\n> > @@ -638,10 +638,10 @@ def transpose(image, name=None):\n>      If `image` was 3-D, a 3-D float Tensor of shape\n>     `[width, height, channels]`\n>\n> -   Usage Example: ```python >>\n> -    import tensorflow as tf >>\n> -    x = tf.random.normal(shape=(100, 200, 3)) >>\n> -    tf.image.transpose(x) `\n> +   Usage Example: ```python\n> +    >>> import tensorflow as tf >>\n>\n> About the PR's, these are all student submissions from GCI-2019. Would you\n> like me to check for duplicates and inform those students to redirect\n> elsewhere?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/35373?email_source=notifications&email_token=AAABHRJGJYNAL5H2L3BKXA3Q2V355A5CNFSM4J62OCSKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCQIQ6UQ#discussion_r361575246>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRN6JHRJ3BHFSUBDHMLQ2V355ANCNFSM4J62OCSA>\n> .\n>\n\n\n-- \n - Alex\n", "I'll do that. Gracias", "@Vansh-Sethi thank you, it is still failing doctest can you please check here for [logs](https://source.cloud.google.com/results/invocations/e7b3dd0a-9cd7-4859-9baf-807cb1e0e467/targets/%2F%2Ftensorflow%2Ftools%2Fdocs:tf_doctest/tests).\r\nPlease run the doctest locally as mentioned here in the [contributor guidelines](https://www.tensorflow.org/community/contribute/docs_ref).", "@Vansh-Sethi Any update on this PR, please. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 35372, "title": "Reading a Tensorflow 2.0's tfrecords File?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n\r\n\r\nI have a .tfrecords file that stores my serialised data. How can I re-load it next time? It is impractical to generate it fro raw data each time when it is needed. Note that I tried TFRecordReader from [here](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/t-f-record-reader) but it does not work at all\r\n\r\ntf prompts\r\n\r\n`\r\n\t\tAttributeError: module 'tensorflow' has no attribute 'TFRecordReader'\r\n`", "comments": ["@yourtheron \r\nWill it be possible to share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.", "@ravikyram I found the root cause and the solution. In fact, if I want to write my serialised data into a TFRecord file, I cannot use `tensorflow::ops::TFRecordReader`. Instead, I have to use something like\r\n```\r\nwith tf.io.TFRecordWriter(RecorderFileName) as writer:\r\n```\r\nand if I want to read data from the file, I have to firstly 'record' the file into a raw dataset by\r\n```\r\ntrRawDataset = tf.data.TFRecordDataset(TrRecorderFileName)\r\n```\r\nThen read data into a training-ready dataset by `map()` function\r\n```\r\ndsTrain = trRawDataset.map(decodeMsg2Img)\r\n```\r\nwhere `decodeMsg2Img` is a function that uses `tf.io.parse_single_example` to parse the data as per features I gave when I wrote the data into the TFRecord file.\r\n\r\nThanks all the same! I am new to tf. It is quite a learning.", "@yourtheron \r\n\r\nGlad to know you found the solution.I am closing the issue since its resolved. Thanks!"]}, {"number": 35371, "title": "How to concatenate two models in tensorflow for End2End system?", "body": "Hi \r\n\r\nI have trained two models **separately**.\r\n**Model A** use `x` to predict `y`;\r\n**Model B** use `y` to predict `z`.\r\n\r\nNow I want to splice \u200b\u200bthem into one **Model C** which can use `x` to directly predict `z`.\r\n\r\nIn other words, I want to merge two graphs into one graph, or I want to freeze two checkpoints into one pb file.\r\n\r\nDo you have any good idea to achieve this ? Thanks!\r\n", "comments": []}, {"number": 35370, "title": "Added usage example for tf.math.sigmoid", "body": "Added a brief example showing how to use tf.math.sigmoid ", "comments": ["Hi @BashirSbaiti, thanks for the PR. I have noticed that we have 4 commits for a single example that you have added. We should do it in a single commit so that it becomes easier to understand what that commit does. You can squash the extra commits, feel free to ask if you face any problem.", "(Sorry if this is a dumb question because I am new to github)\r\nDoes this mean I should open a new PR or edit this one somehow?", "@BashirSbaiti It's a good question :smile:, You don't have to open a new PR. We will work on this together but before that, I would suggest going through [Squash Commit ](https://www.internalpointers.com/post/squash-commits-into-one-git)and then try it with your branch.\r\n\r\nHere are some hints for you:\r\n1. git rebase -i HEAD~4\r\n2. For last 3 commit's change \"pick\" to \"squash\"\r\n3. save changes and close editor\r\n4. force push", "You don't have to open a new one. \r\n\r\nRun `git rebase -i <your-branch>`\r\nYou'll see all your commits starting with `pick`, keep the first one unchanged and change the rest to `squash`\r\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35370) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.\r\n", "@rahul2240 I did a rebase to drop the extra commits and just keep the one that is the combination of the original four but it isn't showing up here after I ran git push origin master. Do you know why?  ", "@googlebot I consent.", "@BashirSbaiti I can see that you have added old commits also while rebasing, please get rid of it. We only need your last commit. \r\nDid you use \"git push origin master\"? aren't you working in a branch?\r\n", "Closing this PR as this not against `master`, please open a new PR against `master` "]}, {"number": 35369, "title": "Tensorflow Python modules import the same module under multiple different names", "body": "I am using tensorflow 2.1.0rc1-2 on Arch Linux.\r\n\r\nA Python project I work with contains some machinery to detect if the same .py file is imported under multiple different names, resulting in different module objects. This [can lead to subtle breakage](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-double-import-trap) as classes defined in the two copies of the module are officially different classes, so instances of one of them return False when using `isinstance()` with the class from the other copy of the module. Basically, it's a bad idea, so the project I work with raises a big error to tell us when we've accidentally done it.\r\n\r\nTensorflow has a large number of modules imported twice under different names, which causes my project to raise an error about it. Of course I can just whitelist tensorflow in my project, but I think this is a bad idea for tensorflow to do. If it is going to have the same module under multiple names, the import machinery hacks in tensorflow should ensure the multiple names at least point to the same module object, rather than there being multiple module objects in the interpreter object corresponding to the same .py file.\r\n\r\nThe issue can be demonstrated with the following:\r\n\r\n```python\r\nimport sys\r\nimport tensorflow\r\n\r\nfor name1, module1 in sys.modules.copy().items():\r\n    for name2, module2 in sys.modules.copy().items():\r\n        file1 = getattr(module1, '__file__', None)\r\n        file2 = getattr(module2, '__file__', None)\r\n        if file1 is not None and file2 is not None:\r\n            if file1 == file2 and name1 != name2 and module1 is not module2:\r\n                print(name1, name2)\r\n```\r\nThis prints\r\n```\r\ntensorflow._api tensorflow_core._api\r\ntensorflow.python tensorflow_core.python\r\ntensorflow.tools tensorflow_core.tools\r\ntensorflow.core tensorflow_core.core\r\ntensorflow.compiler tensorflow_core.compiler\r\ntensorflow.lite tensorflow_core.lite\r\ntensorflow.keras tensorflow.python.keras.api._v2.keras\r\ntensorflow.keras tensorflow_core.python.keras.api._v2.keras\r\ntensorflow.keras tensorflow_core.keras\r\ntensorflow.compat tensorflow._api.v2.compat\r\ntensorflow.compat tensorflow_core._api.v2.compat\r\n\r\n<snip>\r\n```\r\n\r\nBasically there are copies of many modules imported both as `tensorflow.<blah>` and `tensorflow_core.<blah>`.\r\n\r\nTensorflow should consider changing its import machinery to avoid this, for the reasons outlined in the above linked post on the double import gotcha in Python.", "comments": ["Thank you for this issue. It will take a while but it looks like something we should be doing.", "@chrisjbillington It was an issue with `TF2.1`. However, it was resolved with recent TF versions. I have checked with `tf-nightly` and I cannot reproduce the issue. [Here]( https://colab.research.google.com/gist/jvishnuvardhan/cfdb1a3a6c62a104083b0ab8cc7d2b91/untitled.ipynb) is a gist for our reference.\r\n\r\nFor a reference, [Here](https://colab.research.google.com/gist/jvishnuvardhan/64fa9a34b3812e04f5e2c30867aef193/untitled981.ipynb) is a gist with `TF2.1`. \r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen or create a new issue if this persists with newer versions. thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35369\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35369\">No</a>\n", "Thanks @jvishnuvardhan , I haven't verified that this is no longer an issue, but appreciate you letting me know and I will take your word for it :)"]}, {"number": 35368, "title": "Fixed spacing after periods", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35368) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35368) for more info**.\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35368) for more info**.\n\n<!-- ok -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\n"]}, {"number": 35367, "title": "Explicit horovod spec and update CUDA to 10.1", "body": "TensorFlow 2.1 was been upgraded to CUDA 10.1. This change (finally)\nupdates the non-devel GPU dockerfiles as well. Additionally, it moves\nthe horovod partial specs into their own key, because TensorFlow's CI\nuses the \"ubuntu\" keys in CI, when horovod packages are not available.", "comments": ["Err, actually resolves https://github.com/tensorflow/tensorflow/issues/35364"]}, {"number": 35366, "title": "Math.not_equal example added", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35366) for more info**.\n\n<!-- need_sender_cla -->", "> \r\n> \r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> \r\n>     * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> \r\n> ##### Corporate signers\r\n> \r\n>     * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> \r\n>     * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n>     * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35366) for more info**.\r\n\r\n@googlebot I signed it!", "> \r\n> \r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> \r\n>     * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> \r\n> ##### Corporate signers\r\n> \r\n>     * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> \r\n>     * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n>     * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35366) for more info**.\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35366) for more info**.\n\n<!-- ok -->", "Closing this PR as this not against `master`, please open a new PR against `master` \r\n"]}, {"number": 35365, "title": "Cannot save Keras model to pb using model.save()", "body": "Hello!\r\nAs been said in https://www.tensorflow.org/guide/keras/save_and_serialize#export_to_savedmodel , to save the Keras model to the format that could be used by TensorFlow, I need to use model.save() and provide save_format='tf', but what it throws me an exception.\r\n\r\n**System information**\r\ntf_env_collect.sh has been used to get:\r\n\r\n== check python ===================================================\r\npython version: 3.6.10\r\npython branch: \r\npython build version: ('default', 'Dec 23 2019 13:58:32')\r\npython compiler version: GCC 7.4.0\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019\r\nos release version: 4.15.0-72-generic\r\nos platform: Linux-4.15.0-72-generic-x86_64-with-debian-buster-sid\r\nlinux distribution: ('debian', 'buster/sid', '')\r\nlinux os distribution: ('debian', 'buster/sid', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='hellfire', release='4.15.0-72-generic', version='#81-Ubuntu SMP Tue Nov 26 12:20:02 UTC 2019', machine='x86_64', processor='x86_64')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                1.18.0 \r\nprotobuf             3.11.2 \r\ntensorflow           1.14.0 \r\ntensorflow-estimator 1.14.0 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 1.14.0\r\ntf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5\r\ntf.version.COMPILER_VERSION = 4.8.5\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n/tmp/tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 1.14.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/noxx/.pyenv/versions/3.6.10/envs/dmitriystf2/lib/python3.6/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 10, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nException:\r\n```\r\nTraceback (most recent call last):\r\n  File \"load_file2.py\", line 14, in <module>\r\n    classifier.save('/tmp/keras-model.pb', save_format='tf')\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModel is saved as described in https://www.tensorflow.org/guide/keras/save_and_serialize#export_to_savedmodel\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport pandas as pd\r\nimport tensorflow as tf;\r\nimport keras;\r\nfrom keras import Sequential\r\nfrom keras.layers import Dense\r\nimport json;\r\nimport numpy as np;\r\n\r\nclassifier = Sequential()\r\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=4))\r\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\r\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics = ['accuracy'])\r\n\r\nclassifier.save('/tmp/keras-model.pb', save_format='tf')\r\n```\r\n\r\n**Other info / logs**\r\nI have the same behavior on tensorflow2\r\n\r\nThanks.", "comments": ["@justnoxx ,\r\nHello, please try saving model in .h5 format using command `classifier.save('my_model.h5')`, model should be saved successfully.\r\nKindly find the gist of [colab](https://colab.sandbox.google.com/gist/oanush/175dc66b70dd893a710f0b09c8538a97/35365.ipynb) for reference.Thanks!", "@oanush thanks for your answer! .h5 model works fine, there are no issues with that.\r\nLooks like that my issue is resolved for now by replacing \r\n\r\n```\r\nimport keras;\r\nfrom keras import Sequential\r\nfrom keras.layers import Dense\r\n```\r\n\r\nwith\r\n\r\n```\r\nfrom tensorflow.keras import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\n```\r\n\r\nIt looks like that it is my fault, so, I will close my issue, but I am just wondering, how it could be that it does not work from direct Keras import? Do they have a different versions?\r\n\r\nThanks for your time.", "@justnoxx , the first part `import keras` uses [tensorflow as backend](https://keras.io/backend/), and the second one is [tf.keras ](https://www.tensorflow.org/guide/keras)both are not the same.Thanks!", "Thanks a lot. It is clear now."]}, {"number": 35364, "title": "no 2.1.0-rc1 or 2.1.0-rc2 in dockerhub only 2.0.0", "body": "\r\n**System information**\r\n- TensorFlow version: 2.1.0\r\n\r\n**Describe the problem**\r\nTensorflow/tensorflow does not list 2.1.0-rc1 or rc2 in the list of available tags.\r\nThese are available on colab but somehow did not make it to dockerhub.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nhttps://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1&name=2.1.0\r\nobserve no tags\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I tried to build the docker from git but get the following error\r\n```\r\ndocker build --build-arg USE_PYTHON_3_NOT_2=true --build-arg TF_PACKAGE_VERSION=2.1.0-rc2  -f ./dockerfiles/gpu.Dockerfile -t tf .\r\ndocker run --gpus all -it tf\r\npython3\r\nimport tensorflow as tf\r\n2019-12-26 14:45:30.126243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2019-12-26 14:45:30.126620: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2019-12-26 14:45:30.126633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n```\r\nTrying various things to fix the above, but no success so far.\r\n", "@johngrabner Thanks for your report. #35367 should have fixed this -- it seems like the images were never updated to support CUDA 10.1, which TensorFlow 2.1 requires. Can you try `git pull`?\r\n\r\nI'm waiting on some updates to our internal CI to be able to push the new images. Hopefully, they'll be available before the end of the day.", "Looks like image got uploaded\r\n```\r\n2.1.0rc2-gpu\r\ndocker pull tensorflow/tensorflow:2.1.0rc2-gpu\r\nLast updated10 minutes agobytensorflowpackages\r\n```\r\n\r\nTrying it out now", "I think it is working, but there is a warning on the second line\r\n\r\n2019-12-26 21:53:03.304048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2019-12-26 21:53:03.304418: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2019-12-26 21:53:03.304431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2019-12-26 21:53:08.633799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-26 21:53:08.656459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.656893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 21:53:08.656950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 21:53:08.657003: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 21:53:08.658244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 21:53:08.658569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 21:53:08.659846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 21:53:08.660518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 21:53:08.660557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 21:53:08.660628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.661080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.661436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 21:53:08.661693: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-26 21:53:08.682339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-12-26 21:53:08.682953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x87addb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 21:53:08.682968: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-26 21:53:08.760651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.761034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8159cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 21:53:08.761048: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-12-26 21:53:08.761157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.761519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 21:53:08.761548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 21:53:08.761559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 21:53:08.761569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 21:53:08.761579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 21:53:08.761593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 21:53:08.761607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 21:53:08.761642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 21:53:08.761696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.762104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.762428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 21:53:08.762457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 21:53:08.931909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-26 21:53:08.931936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-26 21:53:08.931942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-26 21:53:08.932156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.932582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 21:53:08.932950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8288 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n", "It there a standard test you normally run.\r\n\r\nThe sample code I am using is not happy with rc2\r\n\r\nsudo docker exec -it 282c2ff49852 /bin/bash\r\n```\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nYou are running this container as user with ID 1000 and group 1000,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\ntf-docker /tf > python3\r\nPython 3.6.9 (default, Nov  7 2019, 10:44:02) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2019-12-26 22:10:33.702129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2019-12-26 22:10:33.703898: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] **Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2019-12-26 22:10:33.703956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.**\r\n>>> print(tf.__version__)\r\n2.1.0-rc2\r\n>>> device_lib.list_local_devices()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nNameError: name 'device_lib' is not defined\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'Session'\r\n>>> from tensorflow.python.client import device_lib\r\n>>> print(device_lib.list_local_devices())\r\n2019-12-26 22:14:23.959816: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-26 22:14:24.005268: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-12-26 22:14:24.006412: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x596fee0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 22:14:24.006426: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-26 22:14:24.008570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-26 22:14:24.122916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.123297: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59a2520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 22:14:24.123312: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-12-26 22:14:24.123431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.123737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 22:14:24.123767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 22:14:24.123791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 22:14:24.141434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 22:14:24.144722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 22:14:24.189225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 22:14:24.196701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 22:14:24.196859: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 22:14:24.197215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.199081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.200805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 22:14:24.201544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 22:14:24.827415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-26 22:14:24.827445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-26 22:14:24.827451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-26 22:14:24.827988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.828372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:14:24.828708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 9441 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 12645776964756662951\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 8039547365867512922\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 14991733659365383547\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 9900208948\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 15540725193523866116\r\nphysical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\r\n]\r\n>>> exit\r\nUse exit() or Ctrl-D (i.e. EOF) to exit\r\n>>> exit()\r\ntf-docker /tf > \r\ntf-docker /tf > cd /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/\r\ntf-docker /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src > python main.py --source=bentham --train\r\n2019-12-26 22:15:29.610690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2019-12-26 22:15:29.610942: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvrtc.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2019-12-26 22:15:29.610972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2019-12-26 22:15:35.159777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-26 22:15:35.197746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.198250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 22:15:35.198296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 22:15:35.198384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 22:15:35.199606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 22:15:35.199860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 22:15:35.201211: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 22:15:35.201923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 22:15:35.201963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 22:15:35.202050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.202537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.203006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 22:15:35.203418: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-26 22:15:35.229352: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-12-26 22:15:35.229900: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x894c1f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 22:15:35.229913: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-26 22:15:35.305420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.305824: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x88bd8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 22:15:35.305839: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-12-26 22:15:35.306028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.306367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 22:15:35.306395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 22:15:35.306405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 22:15:35.306417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 22:15:35.306430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 22:15:35.306447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 22:15:35.306484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 22:15:35.306507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 22:15:35.306572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.306891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.307176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 22:15:35.307206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 22:15:35.477274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-26 22:15:35.477303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-26 22:15:35.477309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-26 22:15:35.477490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.477903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 22:15:35.478271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9440 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput (InputLayer)           [(None, 1024, 128, 1)]    0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 512, 64, 16)       160       \r\n_________________________________________________________________\r\np_re_lu (PReLU)              (None, 512, 64, 16)       16        \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 512, 64, 16)       112       \r\n_________________________________________________________________\r\nfull_gated_conv2d (FullGated (None, 512, 64, 16)       4640      \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 512, 64, 32)       4640      \r\n_________________________________________________________________\r\np_re_lu_1 (PReLU)            (None, 512, 64, 32)       32        \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 512, 64, 32)       224       \r\n_________________________________________________________________\r\nfull_gated_conv2d_1 (FullGat (None, 512, 64, 32)       18496     \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 256, 16, 40)       10280     \r\n_________________________________________________________________\r\np_re_lu_2 (PReLU)            (None, 256, 16, 40)       40        \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 256, 16, 40)       280       \r\n_________________________________________________________________\r\nfull_gated_conv2d_2 (FullGat (None, 256, 16, 40)       28880     \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 256, 16, 40)       0         \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 256, 16, 48)       17328     \r\n_________________________________________________________________\r\np_re_lu_3 (PReLU)            (None, 256, 16, 48)       48        \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 256, 16, 48)       336       \r\n_________________________________________________________________\r\nfull_gated_conv2d_3 (FullGat (None, 256, 16, 48)       41568     \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 256, 16, 48)       0         \r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 128, 4, 56)        21560     \r\n_________________________________________________________________\r\np_re_lu_4 (PReLU)            (None, 128, 4, 56)        56        \r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 128, 4, 56)        392       \r\n_________________________________________________________________\r\nfull_gated_conv2d_4 (FullGat (None, 128, 4, 56)        56560     \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 128, 4, 56)        0         \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 128, 4, 64)        32320     \r\n_________________________________________________________________\r\np_re_lu_5 (PReLU)            (None, 128, 4, 64)        64        \r\n_________________________________________________________________\r\nbatch_normalization_5 (Batch (None, 128, 4, 64)        448       \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 128, 2, 64)        0         \r\n_________________________________________________________________\r\nreshape (Reshape)            (None, 128, 128)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 128, 256)          198144    \r\n_________________________________________________________________\r\ntime_distributed (TimeDistri (None, 128, 128)          32896     \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 128, 256)          198144    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 128, 98)           25186     \r\n=================================================================\r\nTotal params: 692,850\r\nTrainable params: 691,570\r\nNon-trainable params: 1,280\r\n_________________________________________________________________\r\nTrain for 545 steps, validate for 85 steps\r\nEpoch 1/1000\r\n2019-12-26 22:15:43.691246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 22:15:44.122500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 22:15:45.384099: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-26 22:15:45.395668: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-26 22:15:45.396141: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n\t [[loss/time_distributed_1_loss/Log/_98]]\r\n2019-12-26 22:15:45.396203: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n  1/545 [..............................] - ETA: 1:01:25WARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\nWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\nWARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: lr\r\n  1/545 [..............................] - ETA: 1:02:00Traceback (most recent call last):\r\n  File \"main.py\", line 155, in <module>\r\n    verbose=1)\r\n  File \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py\", line 175, in fit\r\n    use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py:175) ]]\r\n\t [[loss/time_distributed_1_loss/Log/_98]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py:175) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_17754]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n2019-12-26 22:15:45.548644: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n```", "is there a way for me to pull down #35434 and see if it fixes the issue?", "I don't think that fix is sufficient.\r\n\r\nI added the same lines you added but to my dockerfile\r\n```\r\nFROM tensorflow/tensorflow:2.1.0rc2-gpu-py3-jupyter\r\n\r\n# to see if  #35434 fixes the issues\r\nARG CUDA=10.1\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        cuda-nvrtc-${CUDA/./-} \\\r\n        cuda-nvrtc-dev-${CUDA/./-} \r\n\r\n# stuff i need for the app\r\nRUN pip install autopep8==1.4.4\r\nRUN pip install editdistance==0.5.3\r\nRUN pip install flake8==3.7.9\r\nRUN pip install kaldiio==2.15.0\r\nRUN pip install numba==0.46.0\r\nRUN [\"apt-get\", \"install\", \"-y\", \"libsm6\", \"libxext6\", \"libxrender-dev\"]\r\nRUN pip install opencv-python==4.1.2.30\r\n```\r\n\r\nto make sure docker is clean, rebooted and did \r\n`sudo systemctl restart docker`\r\n\r\nThen cuda errors\r\n\r\n```\r\nsudo docker exec -it 7abde00f1842 /bin/bash\r\n\r\n________                               _______________                \r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / \r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\r\n\r\n\r\nYou are running this container as user with ID 1000 and group 1000,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\ntf-docker /tf > cd /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/\r\ntf-docker /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src > python main.py --source=bentham --train\r\n2019-12-26 23:01:13.422419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6\r\n2019-12-26 23:01:13.446401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6\r\n2019-12-26 23:01:19.131157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-12-26 23:01:19.163713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.164228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 23:01:19.164285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 23:01:19.164339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 23:01:19.181939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 23:01:19.186051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 23:01:19.219655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 23:01:19.222981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 23:01:19.223009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 23:01:19.223076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.223463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.223765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 23:01:19.224099: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-12-26 23:01:19.250610: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3696000000 Hz\r\n2019-12-26 23:01:19.251627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8035b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 23:01:19.251648: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-12-26 23:01:19.344922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.345338: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7e7bd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-12-26 23:01:19.345355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-12-26 23:01:19.345489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.345810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.755GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2019-12-26 23:01:19.345842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 23:01:19.345857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 23:01:19.345873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-12-26 23:01:19.345888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-12-26 23:01:19.345903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-12-26 23:01:19.345917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-12-26 23:01:19.345931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 23:01:19.345977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.346316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.346639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-26 23:01:19.347084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-12-26 23:01:19.924373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-26 23:01:19.924403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-26 23:01:19.924410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-26 23:01:19.924955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.925353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-12-26 23:01:19.925701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9581 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput (InputLayer)           [(None, 1024, 128, 1)]    0         \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 512, 64, 16)       160       \r\n_________________________________________________________________\r\np_re_lu (PReLU)              (None, 512, 64, 16)       16        \r\n_________________________________________________________________\r\nbatch_normalization (BatchNo (None, 512, 64, 16)       112       \r\n_________________________________________________________________\r\nfull_gated_conv2d (FullGated (None, 512, 64, 16)       4640      \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 512, 64, 32)       4640      \r\n_________________________________________________________________\r\np_re_lu_1 (PReLU)            (None, 512, 64, 32)       32        \r\n_________________________________________________________________\r\nbatch_normalization_1 (Batch (None, 512, 64, 32)       224       \r\n_________________________________________________________________\r\nfull_gated_conv2d_1 (FullGat (None, 512, 64, 32)       18496     \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 256, 16, 40)       10280     \r\n_________________________________________________________________\r\np_re_lu_2 (PReLU)            (None, 256, 16, 40)       40        \r\n_________________________________________________________________\r\nbatch_normalization_2 (Batch (None, 256, 16, 40)       280       \r\n_________________________________________________________________\r\nfull_gated_conv2d_2 (FullGat (None, 256, 16, 40)       28880     \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 256, 16, 40)       0         \r\n_________________________________________________________________\r\nconv2d_3 (Conv2D)            (None, 256, 16, 48)       17328     \r\n_________________________________________________________________\r\np_re_lu_3 (PReLU)            (None, 256, 16, 48)       48        \r\n_________________________________________________________________\r\nbatch_normalization_3 (Batch (None, 256, 16, 48)       336       \r\n_________________________________________________________________\r\nfull_gated_conv2d_3 (FullGat (None, 256, 16, 48)       41568     \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 256, 16, 48)       0         \r\n_________________________________________________________________\r\nconv2d_4 (Conv2D)            (None, 128, 4, 56)        21560     \r\n_________________________________________________________________\r\np_re_lu_4 (PReLU)            (None, 128, 4, 56)        56        \r\n_________________________________________________________________\r\nbatch_normalization_4 (Batch (None, 128, 4, 56)        392       \r\n_________________________________________________________________\r\nfull_gated_conv2d_4 (FullGat (None, 128, 4, 56)        56560     \r\n_________________________________________________________________\r\ndropout_2 (Dropout)          (None, 128, 4, 56)        0         \r\n_________________________________________________________________\r\nconv2d_5 (Conv2D)            (None, 128, 4, 64)        32320     \r\n_________________________________________________________________\r\np_re_lu_5 (PReLU)            (None, 128, 4, 64)        64        \r\n_________________________________________________________________\r\nbatch_normalization_5 (Batch (None, 128, 4, 64)        448       \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 128, 2, 64)        0         \r\n_________________________________________________________________\r\nreshape (Reshape)            (None, 128, 128)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 128, 256)          198144    \r\n_________________________________________________________________\r\ntime_distributed (TimeDistri (None, 128, 128)          32896     \r\n_________________________________________________________________\r\nbidirectional_1 (Bidirection (None, 128, 256)          198144    \r\n_________________________________________________________________\r\ntime_distributed_1 (TimeDist (None, 128, 98)           25186     \r\n=================================================================\r\nTotal params: 692,850\r\nTrainable params: 691,570\r\nNon-trainable params: 1,280\r\n_________________________________________________________________\r\nTrain for 545 steps, validate for 85 steps\r\nEpoch 1/1000\r\n2019-12-26 23:01:27.960566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-12-26 23:01:28.353452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-12-26 23:01:29.606042: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-26 23:01:29.617482: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-26 23:01:29.617918: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n\t [[loss/time_distributed_1_loss/scan/while/body/_1/Less/_64]]\r\n2019-12-26 23:01:29.618012: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n  1/545 [..............................] - ETA: 59:56WARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\nWARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\nWARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: lr\r\n  1/545 [..............................] - ETA: 1:00:30Traceback (most recent call last):\r\n  File \"main.py\", line 155, in <module>\r\n    verbose=1)\r\n  File \"/home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py\", line 175, in fit\r\n    use_multiprocessing=use_multiprocessing, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py:175) ]]\r\n\t [[loss/time_distributed_1_loss/scan/while/body/_1/Less/_64]]\r\n  (1) Unknown:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at /home/john/Documents/GitHub/Offline_Handwriting_Recognition/Solutions/arthurflor23_handwritten-text-recognition/handwritten-text-recognition-master/src/network/model.py:175) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_distributed_function_17754]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n\r\n2019-12-26 23:01:29.771757: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n\r\n```\r\n\r\n", "### Still crashes\r\n\r\n```\r\n2019-12-27 00:08:54.931864: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-12-27 00:08:54.932313: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n\t [[loss/time_distributed_1_loss/GatherNd/_102]]\r\n```\r\n\r\nafter adding the following to my dockerfile (ie to match the fixes you shared)\r\n\r\n```\r\nFROM tensorflow/tensorflow:2.1.0rc2-gpu-py3-jupyter\r\n\r\n# to see if  #35434 fixes the issues\r\nARG CUDA=10.1\r\nRUN apt-get update && apt-get install -y --no-install-recommends \\\r\n        cuda-nvrtc-${CUDA/./-} \\\r\n        cuda-nvrtc-dev-${CUDA/./-} \\\r\n        libcublas10=10.2.1.243-1 \\ \r\n        libcublas-dev=10.2.1.243-1 \r\n```\r\n\r\n### Maybe this code is stressing  cuda differently\r\n\r\nI tried a simple 10 line nmist example and it works correctly with gpu, even before the last change and with only the first change.\r\n\r\nThe code that is causing the above error is sourced from https://github.com/arthurflor23/handwritten-text-recognition.  I am not too familiar with it. Author claims its solid.\r\n\r\nArthur runs this code on colab with 2.1.0-rc1.  I am running this on docker and \r\nit only worked for me with a cpu docker upgraded to 2.1.0.rc1 and not with a gpu docker.\r\n\r\nAs you can see it fails with the GPU version even with these fixes.  \r\n\r\nI use the GPU docker all of the time, but with 1.13.1-gpu-py3-jupyter on other code.\r\nArthur's code that is dying, uses the new 2.0 api.\r\n\r\n### More Robust Test:\r\n\r\nDo you have a more robust test case I can run, my setup is not too special, but \r\nmaybe you don't have this at your location.\r\n- GeForce RTX 2080 Ti/PCIe/SSE2\r\n- Intel\u00ae Core\u2122 i7-8700K CPU @ 3.70GHz\r\n- Ubuntu 18.04.3 LTS\r\n\r\nAny how, let me know how I can help.", "still broken", "The original issue of missing docker images is now resolved. For other build problems, please file a new issue.", "A workaround for second issue was made available by https://github.com/tensorflow/tensorflow/issues/25446.\r\n\r\nLooks like tf 2.0 stresses cuda more.\r\n\r\nhttps://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\r\n\r\nSo any psudo complex tf using rtx video cards will need this as the first few lines of code.\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n```\r\n\r\n\r\n"]}]