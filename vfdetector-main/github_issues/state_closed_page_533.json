[{"number": 37740, "title": "AlreadyExistsError when using large tensors", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS and Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from: pip  \r\n- Python version: 3.6.8 and 3.7.5 \r\n- CUDA/cuDNN version: Did not use GPU\r\n\r\nTensorflow version: 2.1.0\r\n\r\n**Describe the current behavior**\r\n\r\nThe code casts \r\n\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError\r\n\r\n**Describe the expected behavior**\r\n\r\nIt does not cast that error\r\n\r\n**Standalone code to reproduce the issue** \r\nHere's a minimal example I managed to make. The error only occurs if I use gradient-tape, if i remove the gradient tape the code runs perfectly. But I would like to be able to take the gradient.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass C:\r\n    def __init__(self):\r\n        n = 2000\r\n        self.ae = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)\r\n        self.aa = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)\r\n        self.fr = tf.Variable(0.5, trainable=True, dtype=tf.float32)\r\n\r\n        self.kp = tf.Variable(np.zeros(n), trainable=False, dtype=tf.float32)\r\n\r\n    @tf.function\r\n    def loss_op(self, k: tf.Tensor, a: tf.Tensor, s: tf.Tensor):\r\n        l = tf.constant(0.0)\r\n\r\n        def loop_fn(i, k, l):\r\n            p = tf.clip_by_value(k[a[i]], 0.01, 0.99)\r\n            l = l - (s[i] * tf.math.log(p) + (1 - s[i]) * tf.math.log(1 - p))\r\n\r\n            at = self.aa[a[i]]\r\n\r\n            gk = (self.ae[a[i]] - k) * at\r\n            lk = k * at\r\n\r\n            k = tf.clip_by_value(\r\n                k + s[i] * gk - (1 - s[i]) * self.fr * lk, 0.0, 1.0)\r\n            return i + 1, k, l\r\n\r\n        def loop_cond(i: tf.Tensor, _, __):\r\n            return tf.logical_and(tf.greater_equal(s[i], 0), tf.less(i, 199))\r\n\r\n        _, _, l = tf.while_loop(loop_cond, loop_fn, (0, k, l), back_prop=True)\r\n        return l\r\n\r\n    @tf.function\r\n    def regularizer(self, tensor: tf.Tensor):\r\n        return tf.reduce_sum(tf.math.log(tf.abs(tensor) + 1))\r\n\r\n    @tf.function\r\n    def train_op(self, a, s, opt):\r\n        with tf.GradientTape() as tape:\r\n            loss = self.loss_op(self.kp, a, s)\r\n\r\n            aal = self.regularizer(self.aa)\r\n            al = self.regularizer(self.ae)\r\n\r\n            o = loss + 0.5 * (aal + al)\r\n\r\n        train_vars = [self.ae, self.aa, self.fr]\r\n\r\n        gradient = tape.gradient(o, train_vars)\r\n        opt.apply_gradients(zip(gradient, train_vars))\r\n\r\n        return loss\r\n\r\nc = C()\r\n\r\no = tf.optimizers.Adam(learning_rate=1e-3)\r\n\r\na = np.arange(200, dtype=np.int32)\r\ns = np.ones(200, dtype=np.float32)\r\n\r\nc.train_op(a, s, o)\r\n```\r\n\r\nHere's a stack-trace if that is helpful\r\n\r\n```bash\r\n2020-03-20 09:56:30.208767: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradient\r\ns/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE                                                                                            \r\n2020-03-20 09:56:30.209510: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Already exists: Resource __per_step_0/StatefulPartitionedCall_\r\n3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE                                                                                  \r\n         [[{{node StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var}}]]\r\n2020-03-20 09:56:30.209650: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2020-03-20 09:56:30.209813: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n2020-03-20 09:56:30.219319: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\nTraceback (most recent call last):\r\n  File \"error.py\", line 69, in <module>\r\n    c.train_op(a, s, o)\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"/home/jonas/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\r\n         [[{{node StatefulPartitionedCall_3/gradients/while_grad/while_grad/body/_193/gradients/AddN/tmp_var}}]] [Op:__inference_train_op_806]\r\n\r\nFunction call stack:\r\ntrain_op\r\n```\r\n", "comments": ["If i set n = 1898 inside the class it does not crash. But with n = 1899 it does crash", "@JonasRSV,\r\nI was able to reproduce the issue with [TF 2.1](https://colab.research.google.com/gist/amahendrakar/79ad775a5f4c4a2c64d59bf5b3844e30/37740.ipynb). However, the issue seems to be resolved in [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7e109fdb5282175b1c2c48c5fb8ad135/37740-nightly.ipynb). Please find the attached gist. Thanks!", "> @JonasRSV,\r\n> I was able to reproduce the issue with [TF 2.1](https://colab.research.google.com/gist/amahendrakar/79ad775a5f4c4a2c64d59bf5b3844e30/37740.ipynb). However, the issue seems to be resolved in [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7e109fdb5282175b1c2c48c5fb8ad135/37740-nightly.ipynb). Please find the attached gist. Thanks!\r\n\r\nYes, using tensorflow-nightly fixed the error. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37740\">No</a>\n", "The problem persists even with nightly. If i run the following code snippet from a jupyter notebook I get the same error for 3.6.8 and 3.7 (But only in jupyter notebooks so far)\r\n\r\nHere are the jupyter version details:\r\n```bash\r\n> jupyter --version\r\njupyter core     : 4.6.1\r\njupyter-notebook : 6.0.3\r\nqtconsole        : 4.6.0\r\nipython          : 7.11.1\r\nipykernel        : 5.1.4\r\njupyter client   : 5.3.4\r\njupyter lab      : not installed\r\nnbconvert        : 5.6.1\r\nipywidgets       : 7.5.1\r\nnbformat         : 5.0.4\r\ntraitlets        : 4.3.3\r\n```\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass C:\r\n    def __init__(self):\r\n        n = 8000\r\n        self.pe = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)\r\n        self.ne = tf.Variable(np.eye(n), trainable=True, dtype=tf.float32)\r\n        self.kp = tf.Variable(np.zeros(n), trainable=True, dtype=tf.float32)\r\n\r\n    @tf.function\r\n    def loss_op(self, k: tf.Tensor, a: tf.Tensor, s: tf.Tensor):\r\n        l = tf.constant(0.0)\r\n\r\n        def loop_fn(i, k, l):\r\n            p = tf.clip_by_value(k[a[i]], 0.01, 0.99)\r\n            l = l - (s[i] * tf.math.log(p) + (1 - s[i]) * tf.math.log(1 - p))\r\n\r\n            k = tf.clip_by_value(\r\n                k + s[i] * self.pe[a[i]] + (1 - s[i]) * self.ne[a[i]], -30.0, 30.0)\r\n            return i + 1, k, l\r\n\r\n        def loop_cond(i: tf.Tensor, _, __):\r\n            return tf.logical_and(tf.greater_equal(s[i], 0), tf.less(i, 199))\r\n\r\n        _, _, l = tf.while_loop(loop_cond, loop_fn, (0, k, l), back_prop=True)\r\n        return l\r\n\r\n    @tf.function\r\n    def regularizer(self, tensor: tf.Tensor):\r\n        return tf.reduce_sum(tf.math.log(tf.abs(tensor) + 1))\r\n\r\n    @tf.function\r\n    def train_op(self, a, s, opt):\r\n        with tf.GradientTape() as tape:\r\n            loss = self.loss_op(self.kp, a, s)\r\n\r\n            pel = self.regularizer(self.pe)\r\n            nel = self.regularizer(self.ne)\r\n\r\n            o = loss + 0.5 * (pel + nel)\r\n\r\n        train_vars = [self.pe, self.ne, self.kp]\r\n\r\n        gradient = tape.gradient(o, train_vars)\r\n        opt.apply_gradients(zip(gradient, train_vars))\r\n\r\n        self.pe.assign(tf.clip_by_value(self.pe, -30, 30))\r\n        self.ne.assign(tf.clip_by_value(self.ne, -30, 30))\r\n        self.kp.assign(tf.clip_by_value(self.kp, -30, 30))\r\n\r\n        return loss\r\n\r\n\r\nc = C()\r\n\r\no = tf.optimizers.Adam(learning_rate=1e-3)\r\n\r\na = np.arange(200, dtype=np.int32)\r\ns = np.ones(200, dtype=np.float32)\r\n\r\nc.train_op(a, s, o)\r\nc.train_op(a, s, o)\r\n```\r\n\r\n\r\n", "@JonasRSV,\r\nI was able to run the above code without any issues with TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/35581a7e1aacee4bca5070e141f653ea/37740-tf-nightly.ipynb). Thanks!", "@JonasRSV,\r\nAny updates regarding this issue? Thanks!", "Hey, I updated my version of jupyter and then it worked, \r\n\r\nthanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37740\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37740\">No</a>\n"]}, {"number": 37739, "title": "kernel_concatenation_test fails with \"Aborted (Core dumped)\"", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 765ddddb22\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Linux x86_64\r\n\r\n**Describe the problem**\r\nRunning\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test`\r\n\r\nresults in the following output\r\n`tensorflow/lite/micro/testing/test_linux_binary.sh: line 46:  4201 Aborted                 (core dumped) $1 > ${MICRO_LOG_FILENAME} 2>&1\r\ntensorflow/lite/micro/tools/make/Makefile:321: recipe for target 'test_kernel_concatenation_test' failed\r\nmake: *** [test_kernel_concatenation_test] Error 134\r\n`\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n`$ make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test`\r\n", "comments": ["Confirming that I see the test segfault, though I am able to build it. Because of the recent flatbuffer version upgrade, please try ```make clean clean_downloads``` and then try to run the test.\r\n\r\nWe will look into the test segfaulting.\r\n\r\n```\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile test_kernel_concatenation_test\r\ntensorflow/lite/micro/testing/test_linux_binary.sh tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_concatenation_test '~~~ALL TESTS PASSED~~~'\r\nSegmentation fault\r\ntensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_concatenation_test: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.\r\nTesting TwoInputsAllAxesCombinations\r\nSegmentation fault\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:321: test_kernel_concatenation_test] Error 1\r\n```", "I just tried this and it worked for me.  @jenselofsson can you confirm that this is now fixed for you as well?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37739\">No</a>\n"]}, {"number": 37738, "title": "Support SparseTensor in tf.keras.Discretization layer.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF2.1 and TF2.2-rc0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFeature: [tf.keras.preprocessing.Discretization](https://github.com/tensorflow/tensorflow/blob/765ddddb2278551441ecaf45528898cd839df5c9/tensorflow/python/keras/layers/preprocessing/discretization.py#L33) can process the SparseTensor inputs.\r\nCurrent State: This layer cannot process SparseTensor.\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nThe model developer who want to discretize the value from Sparse Input. It's a common for recommendation & ranking scenario.\r\n\r\n**Any Other info.**\r\n", "comments": ["Yep, we should probably support it. This will probably need some work and api approval. Do you have a more concrete use case?", "(Actually on second thought, we can easily achieve this by return tf.SparseTensor(indices=input.indices, values=bucketize(input.values), dense_shape=input.dense_shape, so not much work). Understanding use cases are pretty helpful though :-)", "@brightcoder01 gentle ping, do you have any concrete example? I intend to include it in the commit description.", "> @brightcoder01 gentle ping, do you have any concrete example? I intend to include it in the commit description.\r\n\r\nThe use case is that we want to discretize the numeric values with the missing value to integer ids and feed the ids into embedding.\r\n\r\nExample, there are missing values of features in the dataset like:\r\n|  age | height | income | price | label|\r\n| ---- | ------ | --------| ----- | -----|\r\n| 32   | 156     | 7800     | 100   |  0    |\r\n| 56 | 172     |      | 130   |  1    |\r\n| 47 |     |   6700   |    |  1    |\r\n| 27 |   141  |   8100   |    |  0   |\r\n\r\nWe can save the table into a text file and read the value as a string when we create `tf.data.Dataset`. The output value of the `Dataset` for missing value is an empty string. For example, the batch data  from `Dataset` with size=4 is:\r\n```python\r\nfeatures = {\r\n    \"age\":[[\"32\"], [\"56\"], [\"47\"], [\"27\"]],\r\n    \"height\":[[\"156\"], [\"172\"], [\"\"], [\"141\"]],\r\n    \"income\":[[\"7800\"], [\"\"], [\"6700\"], [\"8100\"]],\r\n    \"price\":[[\"100\"], [\"130\"], [\"\"], [\"\"]],\r\n}\r\nlabels = [[\"0\"], [\"1\"], [\"1\"], [\"0\"]]\r\n```\r\nThen we need to convert the string values to numeric values. Firstly, we must drop the empty string because we can not convert the empty to a numeric value. So we need to convert the tensor of each feature to a sparse tensor. Because some rows don't have any value after dropping empty string. Then we can use `tf.strings.to_number` to convert the string values to int.\r\nFor example, the sparse tensor of the batch for height is:\r\n```python\r\ntf.SparseTensor(\r\n    index=[[0,0], [1,0], [3,0]],\r\n    values=[156, 172, 141],\r\n   dense_shape=(4,1)\r\n)\r\n```\r\nThen, we want to discretize the height with boundaries=[140,150,160] and the result is\r\n```python\r\ntf.SparseTensor(\r\n    index=[[0,0], [1,0], [3,0]],\r\n    values=[2, 3, 1],\r\n   dense_shape=(4,1)\r\n)\r\n```\r\nAnd then, we will feed the sparse tensor into embedding using `tf.nn.safe_embedding_lookup_sparse`\r\n\r\nAlthough, we can set a default value for the missing value to create a tensor and then cast the tensor to int. But, the embedding result of the row with missing value is not zero. If we use a sparse tensor without the missing value, the embedding result of the row will be zero. If we sum the embedding  results of multiple features, it will make difference.", "Fixed here.\r\nhttps://github.com/tensorflow/tensorflow/commit/4a441b23646965e615800872bfe72aa478e7e8d3"]}, {"number": 37737, "title": "Training Keras models with moving averages on Estimator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI am using code snippets like this to define a Keras model with moving averages of its weights:\r\n\r\n``` Python\r\nmodel = ...\r\nema = tf.train.ExponentialMovingAverage(0.995)\r\nmodel.add_update(ema.apply())\r\n```\r\n\r\nHowever, the update op is lost after the model is converted into an estimator:\r\n\r\n``` Python\r\nmodel.compile(...)\r\nestimator = tf.keras.estimator.model_to_estimator(model)  # The above update op is lost here\r\n```\r\n\r\nInvestigation shows that it is lost during the cloning of the model.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe update op should be kept as is, and an estimator should be able to train such a model.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@herberteuler \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "The TensorFlow I am using is version 1.14, but the behavior should be the same in newer versions. [Here](https://gist.github.com/herberteuler/7b2c9de3e35e635811e0caacd812f893) is a toy \"model\" to demonstrate the issue.", "I have tried on colab with TF version 1.14, 1.15 and i am seeing \r\n`KeyError: \"The name 'weight/kernel/ExponentialMovingAverage:0' refers to a Tensor which does not exist. The operation, 'weight/kernel/ExponentialMovingAverage', does not exist in the graph.\"`.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/978ced381ec195b2a083c804e2023d14/untitled746.ipynb). Is this the expected behavior? Thanks!", "No, the last exception is not expected.\r\n\r\nI have added three notes in the output, shown in cyan, green, and red, respectively.\r\n\r\nThe first part trains a \"raw\" model, where the update ops are executed as the training processes, and as can be seen from there, the moving averages are updated.\r\n\r\nThe second part shows that by adding updates to a Keras model, the same set of update ops can be added to the graph.\r\n\r\nIf everything had gone well, the training of the Keras model should be the same as that of the raw model. But as the third part shows, it is not. This is because, during [the cloning of the model](https://github.com/tensorflow/tensorflow/blob/bd24a8316d62bb59b11d188110f01bfb8c9f3f55/tensorflow/python/keras/models.py#L385), only layers are recreated, the updates are not. So the update ops are not part of the execution plan, hence the absence of the moving average variables in the graph.", "@herberteuler Are you running into the same error with Tensorflow 1.15 and Tensorflow 2.x? Thanks!", "The logic in question is not changed much in these versions, so I only checked with TensorFlow 2.1.\r\n\r\nWell in TF2.1, the `tf.layers` APIs are gone, so the expected behavior cannot be demonstrated anymore. But with a little changes to the code, the same erroneous behavior can be observed from TF2.1.\r\n\r\nThe changed code can be found [here](https://gist.github.com/herberteuler/8c4350db197bd02892aaec9e5bbf3338).", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37737\">No</a>\n"]}, {"number": 37736, "title": "questions about transformer.ipynb tutorial", "body": "Hi. I am new to transformer and I'm trying to understand this transformer tutorial (https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb).\r\n\r\nIn this tutorial, a transformer has encoders, decoders, and a final linear layer.\r\nBut in the paper, a transformer has a softmax layer after the final linear layer.\r\nI think that `predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)` line correctly returns the  expected output anyway, but I just want to know why the softmax layer is not implemented in this tutorial. If I add a softmax layer after the final linear layer and train the model, will the prediction result be different?\r\n\r\nAnd why this tutorial used two separate vocabs instead of a shared subwords vocabulary? (maybe to keep the example simple?) If I want to use a shared vocabulary, should I implement the `shared_embedding_and_softmax_weights` part in tensor2tensor?\r\n\r\nThanks.", "comments": ["The TensorFlow team decided to ban softmax layers from tensorflow.org, unless there's a really good reason.\r\n\r\nhttps://github.com/tensorflow/docs/commit/0775cca21a59d7f4c8f0a61a9baca1a8f5eb38e1\r\n\r\nBasically, TensorFlow has to patch your network to prevent overflows if you try to calculate crossentropy from a softmax output. this leads to surprising behavior when the patching cannot be applied. So it's cleaner to just never do that, and use a `Crossentropy(from_logits=True)`.\r\n\r\nThe argmax still gets the correct result. \r\n\r\nOne-vocab or two? I don't know."]}, {"number": 42788, "title": "Generate unexpected notebook cell in Japanese Documentation", "body": "I found issues about after convert to docs that generated extra notebook cell.\r\n\r\n- https://www.tensorflow.org/tutorials/keras/classification in Japanesse version\r\n\r\n![image](https://user-images.githubusercontent.com/2786333/77142134-896c5780-6ac2-11ea-94f8-8e97e35c7521.png)\r\n\r\n- Original: notebook \r\n- https://github.com/masa-ita/tf-docs/blob/b8cd622e042ce322345d5fa30858087ee1abcab1/site/ja/tutorials/keras/basic_classification.ipynb\r\n\r\n", "comments": ["@lamberta The original notebook in docs-l10n repository does not include such an empty cell.\r\nIs that a problem in Google's internal system building the documents from notebooks?", "Probably related to the `%tensorflow_version 2.x` that we need to include in Colab. See https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/classification.ipynb\r\nThis block is stripped when imported into tensorflow.org.\r\nBut soon enough, Colab will default to TF 2 and we can remove these blocks throughout.", "> But soon enough, Colab will default to TF 2 and we can remove these blocks throughout.\r\n\r\nDone."]}, {"number": 37735, "title": "SavedModelEstimator not found in TF 2", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): mac-os\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7 - Bazel\r\nversion (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from\r\nsource): n/a\r\n- CUDA/cuDNN version: - GPU model and memory: n/a\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntf.estimator.experimental.SavedModelEstimator not found\r\n**Describe the expected behavior**\r\naccording to https://github.com/tensorflow/estimator/blob/r2.1/tensorflow_estimator/python/estimator/canned/saved_model_estimator.py#L112 it should be found at \"tf.estimator.experimental.SavedModelEstimator\"\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.estimator.experimental.SavedModelEstimator\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow_estimator.python.estimator.api._v2.estimator.experimental' has no attribute 'SavedModelEstimator'\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/c0ae75aa3359ec40f6ea2c1df8a925db/untitled473.ipynb). Thanks", "Update: was able to get SavedModelEstimator from below: \r\n```\r\nfrom tensorflow_estimator.python.estimator.canned import saved_model_estimator\r\nsaved_model_estimator.SavedModelEstimator\r\n```\r\nwhy not `tf.estimator.experimental.SavedModelEstimator`?", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "We are on TF2 now. No need to resolve it. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37735\">No</a>\n"]}, {"number": 37734, "title": "Static filesystem registration (modular filesystems)", "body": "When using modular filesystems, the filesystem registration in `core/platform/default/env.cc` (and similar) is no longer valid. We expect filesystems to be loaded from DSOs.\r\n\r\nHowever, we need a filesystem while building the pip package as the `genrule`s for created the `api_def` files need to write to disk. Furthermore, there are a few tests around API generation that depend on having a filesystem around.\r\n\r\nThis PR only adds a method to statically register a filesystem if a Bazel target is added as a dependency to a `cc_library`/`cc_binary`. We recommend adding this target to `cc_binary` to prevent registering the same filesystem multiple times (multiple registrations result in error but the program can still continue safely) or potential ODR violations.\r\n\r\nTested in the following ways:\r\n\r\n1. Applying this diff, we can run the modular filesystem tests while loading the filesystem plugin:\r\n\r\n    ```diff\r\n    diff --git a/tensorflow/core/platform/default/env.cc b/tensorflow/core/platform/default/env.cc\r\n    index 5f7822f658..4bf331b607 100644\r\n    --- a/tensorflow/core/platform/default/env.cc\r\n    +++ b/tensorflow/core/platform/default/env.cc\r\n    @@ -212,8 +212,8 @@ class PosixEnv : public Env {\r\n     }  // namespace\r\n \r\n     #if defined(PLATFORM_POSIX) || defined(__APPLE__) || defined(__ANDROID__)\r\n    -REGISTER_FILE_SYSTEM(\"\", PosixFileSystem);\r\n    -REGISTER_FILE_SYSTEM(\"file\", LocalPosixFileSystem);\r\n    +//REGISTER_FILE_SYSTEM(\"\", PosixFileSystem);\r\n    +//REGISTER_FILE_SYSTEM(\"file\", LocalPosixFileSystem);\r\n     Env* Env::Default() {\r\n       static Env* default_env = new PosixEnv;\r\n       return default_env;\r\n    ```\r\n\r\n    where the test is done via\r\n\r\n    ```bash\r\n    (tf) [tensorflow] \u03bb bazel build //tensorflow/c/experimental/filesystem/plugins/posix:libposix_filesystem.so\r\n    ...\r\n    (tf) [tensorflow] \u03bb bazel build //tensorflow/c/experimental/filesystem:modular_filesystem_test\r\n    ...\r\n    (tf) [tensorflow] \u03bb bazel-bin/tensorflow/c/experimental/filesystem/modular_filesystem_test --scheme=file --dso=bazel-bin/tensorflow/c/experimental/filesystem/plugins/posix/libposix_filesystem.so\r\n    ...\r\n    [----------] 104 tests from ModularFileSystem/ModularFileSystemTest (32 ms total)\r\n\r\n    [----------] Global test environment tear-down\r\n    [==========] 104 tests from 1 test suite ran. (32 ms total)\r\n    [  PASSED  ] 104 tests.\r\n    ```\r\n\r\n1. Without the above diff or with it, `bazel test //tensorflow/cc:cc_op_gen_test` fails with ` Non-OK-status: env->NewWritableFile(dot_h_fname, &h) status: Unimplemented: File system scheme '[local]' not implemented`. However, with this additional diff (to do static registration) the test passes\r\n\r\n    ```diff\r\n    diff --git a/tensorflow/cc/BUILD b/tensorflow/cc/BUILD\r\n    index 5251ccdf1c..13b60f62fa 100644\r\n    --- a/tensorflow/cc/BUILD\r\n    +++ b/tensorflow/cc/BUILD\r\n    @@ -672,6 +672,7 @@ tf_cc_test(\r\n             \"framework/cc_op_gen_test.cc\",\r\n         ],\r\n         deps = [\r\n    +        \"//tensorflow/c/experimental/filesystem/plugins/posix:posix_filesystem_static\",\r\n             \"//tensorflow/core:framework\",\r\n             \"//tensorflow/core:lib\",\r\n             \"//tensorflow/core:lib_internal\",\r\n    ```\r\n\r\n    ```bash\r\n    (tf) [tensorflow] \u03bb bazel test //tensorflow/cc:cc_op_gen_test\r\n    ...\r\n    INFO: Build completed successfully, 9 total actions\r\n    //tensorflow/cc:cc_op_gen_test                                           PASSED in 0.0s\r\n\r\n    Executed 1 out of 1 test: 1 test passes.\r\n    INFO: Build completed successfully, 9 total actions\r\n    ```\r\n\r\n1. Adding the same static target to the `cc_op_gen_main` and to the `xla_ops` targets allows us to build the pip package successfully\r\n\r\n    ```diff\r\n    diff --git a/tensorflow/cc/BUILD b/tensorflow/cc/BUILD\r\n    index 5251ccdf1c..7a7f7865d1 100644\r\n    --- a/tensorflow/cc/BUILD\r\n    +++ b/tensorflow/cc/BUILD\r\n    @@ -655,6 +655,7 @@ cc_library(\r\n             \"//tensorflow/core/api_def:base_api_def\",\r\n         ],\r\n         deps = [\r\n    +        \"//tensorflow/c/experimental/filesystem/plugins/posix:posix_filesystem_static\",\r\n             \"//tensorflow/core:framework_headers_lib\",\r\n             \"//tensorflow/core:lib\",\r\n             \"//tensorflow/core:lib_internal\",\r\n    diff --git a/tensorflow/compiler/tf2xla/ops/BUILD b/tensorflow/compiler/tf2xla/ops/BUILD\r\n    index b116a09dd0..16c43ead2d 100644\r\n    --- a/tensorflow/compiler/tf2xla/ops/BUILD\r\n    +++ b/tensorflow/compiler/tf2xla/ops/BUILD\r\n    @@ -13,6 +13,7 @@ cc_library(\r\n         name = \"xla_ops\",\r\n         srcs = [\"xla_ops.cc\"],\r\n         deps = [\r\n    +        \"//tensorflow/c/experimental/filesystem/plugins/posix:posix_filesystem_static\",\r\n             \"//tensorflow/compiler/xla:xla_data_proto_cc\",\r\n             \"//tensorflow/core:framework\",\r\n             \"//tensorflow/core:lib\",\r\n    ```\r\n\r\n    ```bash\r\n    (tf) [tensorflow] \u03bb bazel build  //tensorflow/tools/pip_package:build_pip_package\r\n    ...\r\n    Target //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n      bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n    INFO: Elapsed time: 3904.842s, Critical Path: 539.10s\r\n    INFO: 1478 processes: 1478 local.\r\n    INFO: Build completed successfully, 1494 total actions\r\n    ```\r\n\r\nNote: the diffs mentioned in the testins section will need to be applied when all filesystems are converted to modular world, as the switch commit/PR. We cannot effectively apply them before unless we want to accept some divergence (some filesystem modular, some still static as in the old world).\r\n\r\nSee tensorflow/community#101 for the general modular filesystems design RFC.", "comments": ["I'm uploading this manually, was a proof of concept for GitHub contributions"]}, {"number": 37733, "title": "TensorFlow Lite Error: tensorflow/lite/kernels/concatenation.cc:52 axis >= 0 was not true.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MACOS Catilina 10.15.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source):1.15.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\ngit clone https://github.com/yuanlida/nc.git\r\nFirstly run this file first and you can get saved_model in one minute.\r\nhttps://github.com/yuanlida/nc/blob/master/bilstm_onefile/bi_lstm_mini_net.py\r\nThen run this file and you can get a tflite file named bi-lstm.tflite in the path ../model/Bi-LSTM\r\nhttps://github.com/yuanlida/nc/blob/master/tf_converter.py\r\nThe copy this files to the Xcode project, I can't allocateTensors.\r\n```\r\n# Copy and paste here the exact command\r\nI found this problem is coused by this python method:\r\n\r\n                _output = bidirectional_dynamic_rnn(\r\n                    cell_fw, cell_bw, char_embeddings,\r\n                    # sequence_length=word_lengths,\r\n                    dtype=tf.float32,\r\n                    time_major=True\r\n                )\r\n\r\n                _, ((_, output_fw), (_, output_bw)) = _output\r\n                output = tf.concat([output_fw, output_bw], axis=-1)\r\nThe Swift code is :\r\n      guard let modelPath = Bundle.main.path(forResource: \"bi-lstm\", ofType: \"tflite\") else {\r\n            print(\"Failed to load the model file.\")\r\n            return\r\n        }\r\n\r\n        var options = Interpreter.Options()\r\n        options.threadCount = 4\r\n            let interpreter = try Interpreter(modelPath: modelPath, options: options)\r\n            try interpreter.allocateTensors()\r\n            print(interpreter.inputTensorCount)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-03-20 07:49:14.148393+0800 SignatureTextClassfication[25938:3910176] Initialized TensorFlow Lite runtime.\r\nTensorFlow Lite Error: tensorflow/lite/kernels/concatenation.cc:52 axis >= 0 was not true.\r\nTensorFlow Lite Error: Node number 1 (CONCATENATION) failed to prepare.\r\n\r\nFailed to create the interpreter with error: Failed to allocate memory for input tensors.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n[bi-lstm.tflite.zip](https://github.com/tensorflow/tensorflow/files/4357639/bi-lstm.tflite.zip)\r\n\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["When change the code to :\r\n                output = tf.concat([output_fw, output_bw], axis=1)\r\nIt will cause this problem:\r\n\"\"\"\r\n2020-03-20 10:31:13.355855+0800 SignatureTextClassfication[60881:4131912] Initialized TensorFlow Lite runtime.\r\nTensorFlow Lite Error: tensorflow/lite/kernels/concatenation.cc:53 axis < t0->dims->size was not true.\r\nTensorFlow Lite Error: Node number 1 (CONCATENATION) failed to prepare.\r\n\"\"\"", "I can test it by python script now.\r\nimport tensorflow as tf\r\n\r\ninterpreter=tf.lite.Interpreter(model_path='./model/Bi-LSTM/bi-lstm.tflite')\r\n\r\ninterpreter.allocate_tensors()", "@yuanlida \r\ncould you please let us know the python version used by you.\r\nplease refer to [this link](https://github.com/tensorflow/tensorflow/issues/34345), for reference.", "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.15.2\r\nPython version: 3.7.6\r\n\r\n", "I have tested the program in TF 2.1 TF2.2 TF1.5 and python 3.6 python 3.7, I got the same error.", "Why removed the label?", "I am waiting...", "I come across the same issue", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37733\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37733\">No</a>\n"]}, {"number": 37732, "title": "TFLite Micro Speech not detecting audio on ESP-EYE", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCross compile OSX 10.15.3 to esp xstensa\r\n- TensorFlow installed from (source or binary):\r\nUsing Tensorflow source master branch\r\n- Tensorflow version (commit SHA if source):\r\nb2ea4cc49d7ff501ec7bdbfcee7bfdaa3e3befd2 and 5c4931bbf69e0f006f210c6382a234e83dd4dc8e\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\nESP32 using esp-idf\r\n\r\n**Describe the problem**\r\nI'm trying to use the `micro_speech` demo for TFLite following the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) for the `ESP-EYE`. The exact board I have can be found at [mouser](https://www.mouser.com/ProductDetail/Espressif-Systems/ESP-EYE?qs=sGAEpiMZZMu3sxpa5v1qrjQmuz3X2%252B1RFNXM50fXoAA%3D). After following the build steps I noticed that no audio input was being detected. I updated `command_responder.cc` to use the ESP logger with each call:\r\n\r\n```cc\r\n#include \"esp_log.h\"\r\n#include \"command_responder.h\"\r\n\r\nstatic const char *TAG = \"TF_LITE_COMMAND_RESPONDER\";\r\n// The default implementation writes out the name of the recognized command\r\n// to the error console. Real applications will want to take some custom\r\n// action instead, and should implement their own versions of this function.\r\nvoid RespondToCommand(tflite::ErrorReporter* error_reporter,\r\n                      int32_t current_time, const char* found_command,\r\n                      uint8_t score, bool is_new_command) {\r\n  if (is_new_command) {\r\n    TF_LITE_REPORT_ERROR(error_reporter, \"Heard %s (%d) @%dms\", found_command,\r\n                         score, current_time);\r\n    ESP_LOGI(TAG, \"Heard %s (%d) @%dms\", found_command, score, current_time);\r\n  }\r\n  ESP_LOGI(TAG, \"Heard %s (%d) @%dms\", found_command, score, current_time);\r\n}\r\n```\r\n\r\n`build`, `flash`, `monitor` and the board doesn't appear to recognize any audio:\r\n\r\n```bash\r\n(37) tron:esp-idf n0mn0m$ idf.py --port /dev/cu.SLAB_USBtoUART flash monitor\r\nChecking Python dependencies...\r\nPython requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.\r\nAdding flash's dependency \"all\" to list of actions\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[1/3] Performing build step for 'bootloader'\r\nninja: no work to do.\r\nExecuting action: flash\r\nRunning esptool.py in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"/Users/n0mn0m/.virtualenvs/37/bin/python /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash @flash_project_args\"...\r\nesptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x8000 partition_table/partition-table.bin 0x1000 bootloader/bootloader.bin 0x10000 micro_speech.bin\r\nesptool.py v2.8\r\nSerial port /dev/cu.SLAB_USBtoUART\r\nConnecting........__\r\nDetecting chip type... ESP32\r\nChip is ESP32D0WDQ5 (revision 1)\r\nFeatures: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None\r\nCrystal is 40MHz\r\nMAC: bc:dd:c2:d0:23:4c\r\nUploading stub...\r\nRunning stub...\r\nStub running...\r\nChanging baud rate to 460800\r\nChanged.\r\nConfiguring flash size...\r\nCompressed 3072 bytes to 103...\r\nWrote 3072 bytes (103 compressed) at 0x00008000 in 0.0 seconds (effective 2555.0 kbit/s)...\r\nHash of data verified.\r\nCompressed 27296 bytes to 16010...\r\nWrote 27296 bytes (16010 compressed) at 0x00001000 in 0.4 seconds (effective 595.8 kbit/s)...\r\nHash of data verified.\r\nCompressed 252256 bytes to 149646...\r\nWrote 252256 bytes (149646 compressed) at 0x00010000 in 3.7 seconds (effective 543.5 kbit/s)...\r\nHash of data verified.\r\n\r\nLeaving...\r\nHard resetting via RTS pin...\r\nExecuting action: monitor\r\nRunning idf_monitor in directory /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\nExecuting \"/Users/n0mn0m/.virtualenvs/37/bin/python /Users/n0mn0m/projects/esp/esp-idf/tools/idf_monitor.py -p /dev/cu.SLAB_USBtoUART -b 115200 /Users/n0mn0m/projects/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.elf -m '/Users/n0mn0m/.virtualenvs/37/bin/python' '/Users/n0mn0m/projects/esp/esp-idf/tools/idf.py' '--port' '/dev/cu.SLAB_USBtoUART'\"...\r\n--- idf_monitor on /dev/cu.SLAB_USBtoUART 115200 ---\r\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\r\nets Jun  8 2016 00:22:57\r\n\r\nrst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)\r\nconfigsip: 0, SPIWP:0xee\r\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\r\nmode:DIO, clock div:1\r\nload:0x3fff0018,len:4\r\nload:0x3fff001c,len:7304\r\nload:0x40078000,len:14944\r\nho 0 tail 12 room 4\r\nload:0x40080400,len:4940\r\nentry 0x40080704\r\nI (64) boot: Chip Revision: 1\r\nI (70) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (41) boot: ESP-IDF v4.0-223-gfdbdf9a0e-dirty 2nd stage bootloader\r\nI (41) boot: compile time 15:49:55\r\nI (42) boot: Enabling RNG early entropy source...\r\nI (48) qio_mode: Enabling default flash chip QIO\r\nI (53) boot: SPI Speed      : 80MHz\r\nI (57) boot: SPI Mode       : QIO\r\nI (61) boot: SPI Flash Size : 2MB\r\nI (65) boot: Partition Table:\r\nI (69) boot: ## Label            Usage          Type ST Offset   Length\r\nI (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (91) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (98) boot: End of partition table\r\nI (103) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (110) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d5c0 ( 54720) map\r\nI (134) esp_image: segment 1: paddr=0x0001d5e8 vaddr=0x3ffb0000 size=0x020e8 (  8424) load\r\nI (137) esp_image: segment 2: paddr=0x0001f6d8 vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at /Users/n0mn0m/projects/esp/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (141) esp_image: segment 3: paddr=0x0001fae0 vaddr=0x40080400 size=0x00530 (  1328) load\r\nI (150) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x245b8 (148920) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (199) esp_image: segment 5: paddr=0x000445d8 vaddr=0x40080930 size=0x09358 ( 37720) load\r\nI (218) boot: Loaded app from partition at offset 0x10000\r\nI (218) boot: Disabling RNG early entropy source...\r\nI (218) cpu_start: Pro cpu up.\r\nI (222) cpu_start: Application information:\r\nI (227) cpu_start: Project name:     micro_speech\r\nI (232) cpu_start: App version:      v1.12.1-25881-gb2ea4cc49d\r\nI (239) cpu_start: Compile time:     Mar 18 2020 15:49:49\r\nI (245) cpu_start: ELF file SHA256:  22cc8962718b499b...\r\nI (251) cpu_start: ESP-IDF:          v4.0-223-gfdbdf9a0e-dirty\r\nI (257) cpu_start: Starting app cpu, entry point is 0x40080fe8\r\n0x40080fe8: call_start_cpu1 at /Users/n0mn0m/projects/esp/esp-idf/components/esp32/cpu_start.c:271\r\n\r\nI (0) cpu_start: App cpu up.\r\nI (268) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (274) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (281) heap_init: At 3FFB69C8 len 00029638 (165 KiB): DRAM\r\nI (287) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (293) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (300) heap_init: At 40089C88 len 00016378 (88 KiB): IRAM\r\nI (306) cpu_start: Pro cpu start user code\r\nI (323) spi_flash: detected chip: generic\r\nI (323) spi_flash: flash io: qio\r\nW (323) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.\r\nI (334) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\nI (428) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (428) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (438) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 16, CLKM: 39, BCK: 8, MCLK: 4096000.000, SCLK: 512800.000000, diva: 64, divb: 4\r\nI (558) TF_LITE_AUDIO_PROVIDER: Audio Recording started\r\nI (1578) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @0ms\r\nI (2478) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @1100ms\r\nI (3268) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2000ms\r\nI (4068) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2800ms\r\nI (4868) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @3600ms\r\nI (5568) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @4400ms\r\nI (6258) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5100ms\r\n```\r\n\r\nI will mention that the board worked with the default `Lexin` keyword demo application so the microphone is functional.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n`git clone` Tensorflow\r\nFollow the instructions [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-esp32) creating the `micro_speech` project and setting the `IDF_PATH`.\r\nUpdate `command_responder.cc` as mentioned above to see what is heard in the ESP monitor.\r\nRun `idf.py build` follow by `idf.py flash monitor`.\r\nObserve the monitor output which is showing silence on each pass while I speak loudly, softly and with environment noise.\r\n\r\n\r\n\r\nJust to make sure, I cloned master and did this clean from the top and replicated the issue.\r\n\r\n```bash\r\nLast login: Wed Mar 18 20:35:13 on ttys003\r\n(38) tron:esp-idf n0mn0m$ cd\r\n(38) tron:~ n0mn0m$ esp\r\nAdding ESP-IDF tools to PATH...\r\nChecking if Python packages are up to date...\r\nPython requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.\r\nAdded the following directories to PATH:\r\n  /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool\r\n  /Users/n0mn0m/projects/esp/esp-idf/components/espcoredump\r\n  /Users/n0mn0m/projects/esp/esp-idf/components/partition_table/\r\n  /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin\r\n  /Users/n0mn0m/.espressif/tools/esp32ulp-elf/2.28.51.20170517/esp32ulp-elf-binutils/bin\r\n  /Users/n0mn0m/.espressif/tools/openocd-esp32/v0.10.0-esp32-20190313/openocd-esp32/bin\r\n  /Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools\r\nDone! You can now compile ESP-IDF projects.\r\nGo to the project directory and run:\r\n\r\n  idf.py build\r\n\r\n(38) tron:~ n0mn0m$ git clone https://github.com/tensorflow/tensorflow\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 191, done.\r\nremote: Counting objects: 100% (191/191), done.\r\nremote: Compressing objects: 100% (109/109), done.\r\nremote: Total 855064 (delta 91), reused 127 (delta 82), pack-reused 854873\r\nReceiving objects: 100% (855064/855064), 495.33 MiB | 12.16 MiB/s, done.\r\nResolving deltas: 100% (692955/692955), done.\r\nChecking out files: 100% (19882/19882), done.\r\n(38) tron:~ n0mn0m$ gmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_micro_speech_esp_project\r\ngmake: tensorflow/lite/micro/tools/make/Makefile: No such file or directory\r\ngmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/Makefile'.  Stop.\r\n(38) tron:~ n0mn0m$ cd tensorflow/\r\n(38) tron:tensorflow n0mn0m$ gmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_micro_speech_esp_project\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp  \r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/flatbuffers/archive/v1.12.0.tar.gz\" \"c62ffefb3d4548b127cca14ce047f16c\" tensorflow/lite/micro/tools/make/downloads/flatbuffers  \r\ndownloading https://github.com/google/flatbuffers/archive/v1.12.0.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft \r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\" \"fe2934bd0788f1dcc7af3f0a954542ab\" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\" \"8a7d2c70325f53136faea6dde517b8cc\" tensorflow/lite/micro/tools/make/downloads/person_model_int8  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\r\n(38) tron:tensorflow n0mn0m$ cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\n(38) tron:esp-idf n0mn0m$ idf.py build\r\nChecking Python dependencies...\r\nPython requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.\r\nExecuting action: all (aliases: build)\r\nRunning cmake in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"cmake -G Ninja -DPYTHON_DEPS_CHECKED=1 -DESP_PLATFORM=1 --warn-uninitialized -DWARN_UNINITIALIZED=1 -DCCACHE_ENABLE=0 /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\"...\r\nWarn about uninitialized values.\r\n-- Found Git: /usr/bin/git (found version \"2.21.1 (Apple Git-122.3)\") \r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/bt/controller/lib is out of date.  Run 'git\r\n  submodule update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:2 (include)\r\n\r\n\r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/esp_wifi/lib_esp32 is out of date.  Run 'git\r\n  submodule update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:2 (include)\r\n\r\n\r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/lwip/lwip is out of date.  Run 'git submodule\r\n  update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:2 (include)\r\n\r\n\r\n-- IDF_TARGET not set, using default target: esp32\r\n-- The C compiler identification is GNU 8.2.0\r\n-- The CXX compiler identification is GNU 8.2.0\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc\r\n-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc\r\n-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++\r\n-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Project version: v1.12.1-27700-g5c4931bbf6\r\n-- Building ESP-IDF components for target esp32\r\nLoading defaults file /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults...\r\n/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:16 CONFIG_FLASHMODE_QIO was replaced with CONFIG_ESPTOOLPY_FLASHMODE_QIO\r\n/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:18 CONFIG_INT_WDT was replaced with CONFIG_ESP_INT_WDT\r\n/Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/sdkconfig.defaults:19 CONFIG_TASK_WDT was replaced with CONFIG_ESP_TASK_WDT\r\n-- Found PythonInterp: /Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python (found version \"3.8.2\") \r\n-- Found Perl: /opt/local/bin/perl (found version \"5.28.2\") \r\n-- Adding linker script /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/esp-idf/esp32/esp32_out.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.project.ld.in\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.peripherals.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.syscalls.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-data.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld\r\n-- Components: app_trace app_update asio bootloader bootloader_support bt coap console cxx driver efuse esp-tls esp32 esp_adc_cal esp_common esp_eth esp_event esp_gdbstub esp_http_client esp_http_server esp_https_ota esp_https_server esp_local_ctrl esp_ringbuf esp_rom esp_websocket_client esp_wifi espcoredump esptool_py expat fatfs freemodbus freertos heap idf_test jsmn json libsodium log lwip main mbedtls mdns mqtt newlib nghttp nvs_flash openssl partition_table protobuf-c protocomm pthread sdmmc soc spi_flash spiffs tcp_transport tcpip_adapter tfmicro ulp unity vfs wear_levelling wifi_provisioning wpa_supplicant xtensa\r\n-- Component paths: /Users/n0mn0m/projects/esp/esp-idf/components/app_trace /Users/n0mn0m/projects/esp/esp-idf/components/app_update /Users/n0mn0m/projects/esp/esp-idf/components/asio /Users/n0mn0m/projects/esp/esp-idf/components/bootloader /Users/n0mn0m/projects/esp/esp-idf/components/bootloader_support /Users/n0mn0m/projects/esp/esp-idf/components/bt /Users/n0mn0m/projects/esp/esp-idf/components/coap /Users/n0mn0m/projects/esp/esp-idf/components/console /Users/n0mn0m/projects/esp/esp-idf/components/cxx /Users/n0mn0m/projects/esp/esp-idf/components/driver /Users/n0mn0m/projects/esp/esp-idf/components/efuse /Users/n0mn0m/projects/esp/esp-idf/components/esp-tls /Users/n0mn0m/projects/esp/esp-idf/components/esp32 /Users/n0mn0m/projects/esp/esp-idf/components/esp_adc_cal /Users/n0mn0m/projects/esp/esp-idf/components/esp_common /Users/n0mn0m/projects/esp/esp-idf/components/esp_eth /Users/n0mn0m/projects/esp/esp-idf/components/esp_event /Users/n0mn0m/projects/esp/esp-idf/components/esp_gdbstub /Users/n0mn0m/projects/esp/esp-idf/components/esp_http_client /Users/n0mn0m/projects/esp/esp-idf/components/esp_http_server /Users/n0mn0m/projects/esp/esp-idf/components/esp_https_ota /Users/n0mn0m/projects/esp/esp-idf/components/esp_https_server /Users/n0mn0m/projects/esp/esp-idf/components/esp_local_ctrl /Users/n0mn0m/projects/esp/esp-idf/components/esp_ringbuf /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom /Users/n0mn0m/projects/esp/esp-idf/components/esp_websocket_client /Users/n0mn0m/projects/esp/esp-idf/components/esp_wifi /Users/n0mn0m/projects/esp/esp-idf/components/espcoredump /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py /Users/n0mn0m/projects/esp/esp-idf/components/expat /Users/n0mn0m/projects/esp/esp-idf/components/fatfs /Users/n0mn0m/projects/esp/esp-idf/components/freemodbus /Users/n0mn0m/projects/esp/esp-idf/components/freertos /Users/n0mn0m/projects/esp/esp-idf/components/heap /Users/n0mn0m/projects/esp/esp-idf/components/idf_test /Users/n0mn0m/projects/esp/esp-idf/components/jsmn /Users/n0mn0m/projects/esp/esp-idf/components/json /Users/n0mn0m/projects/esp/esp-idf/components/libsodium /Users/n0mn0m/projects/esp/esp-idf/components/log /Users/n0mn0m/projects/esp/esp-idf/components/lwip /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/main /Users/n0mn0m/projects/esp/esp-idf/components/mbedtls /Users/n0mn0m/projects/esp/esp-idf/components/mdns /Users/n0mn0m/projects/esp/esp-idf/components/mqtt /Users/n0mn0m/projects/esp/esp-idf/components/newlib /Users/n0mn0m/projects/esp/esp-idf/components/nghttp /Users/n0mn0m/projects/esp/esp-idf/components/nvs_flash /Users/n0mn0m/projects/esp/esp-idf/components/openssl /Users/n0mn0m/projects/esp/esp-idf/components/partition_table /Users/n0mn0m/projects/esp/esp-idf/components/protobuf-c /Users/n0mn0m/projects/esp/esp-idf/components/protocomm /Users/n0mn0m/projects/esp/esp-idf/components/pthread /Users/n0mn0m/projects/esp/esp-idf/components/sdmmc /Users/n0mn0m/projects/esp/esp-idf/components/soc /Users/n0mn0m/projects/esp/esp-idf/components/spi_flash /Users/n0mn0m/projects/esp/esp-idf/components/spiffs /Users/n0mn0m/projects/esp/esp-idf/components/tcp_transport /Users/n0mn0m/projects/esp/esp-idf/components/tcpip_adapter /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/components/tfmicro /Users/n0mn0m/projects/esp/esp-idf/components/ulp /Users/n0mn0m/projects/esp/esp-idf/components/unity /Users/n0mn0m/projects/esp/esp-idf/components/vfs /Users/n0mn0m/projects/esp/esp-idf/components/wear_levelling /Users/n0mn0m/projects/esp/esp-idf/components/wifi_provisioning /Users/n0mn0m/projects/esp/esp-idf/components/wpa_supplicant /Users/n0mn0m/projects/esp/esp-idf/components/xtensa\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nRunning ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[425/902] Performing configure step for 'bootloader'\r\nWarn about uninitialized values.\r\n-- Found Git: /usr/bin/git (found version \"2.21.1 (Apple Git-122.3)\") \r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/bt/controller/lib is out of date.  Run 'git\r\n  submodule update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:20 (include)\r\n\r\n\r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/esp_wifi/lib_esp32 is out of date.  Run 'git\r\n  submodule update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:20 (include)\r\n\r\n\r\nCMake Warning at /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/git_submodules.cmake:52 (message):\r\n  Git submodule components/lwip/lwip is out of date.  Run 'git submodule\r\n  update --init --recursive' to fix.\r\nCall Stack (most recent call first):\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:78 (git_submodule_check)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/build.cmake:159 (__build_get_idf_git_revision)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/idf.cmake:43 (__build_init)\r\n  /Users/n0mn0m/projects/esp/esp-idf/tools/cmake/project.cmake:7 (include)\r\n  CMakeLists.txt:20 (include)\r\n\r\n\r\n-- The C compiler identification is GNU 8.2.0\r\n-- The CXX compiler identification is GNU 8.2.0\r\n-- The ASM compiler identification is GNU\r\n-- Found assembler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc\r\n-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc\r\n-- Check for working C compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-gcc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++\r\n-- Check for working CXX compiler: /Users/n0mn0m/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32-elf/bin/xtensa-esp32-elf-g++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Project version: v4.0-223-gfdbdf9a0e-dirty\r\n-- Building ESP-IDF components for target esp32\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp32/ld/esp32.peripherals.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.newlib-funcs.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom/esp32/ld/esp32.rom.libgcc.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main/esp32.bootloader.ld\r\n-- Adding linker script /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main/esp32.bootloader.rom.ld\r\n-- Components: bootloader bootloader_support efuse esp32 esp_common esp_rom esptool_py log main micro-ecc partition_table soc spi_flash xtensa\r\n-- Component paths: /Users/n0mn0m/projects/esp/esp-idf/components/bootloader /Users/n0mn0m/projects/esp/esp-idf/components/bootloader_support /Users/n0mn0m/projects/esp/esp-idf/components/efuse /Users/n0mn0m/projects/esp/esp-idf/components/esp32 /Users/n0mn0m/projects/esp/esp-idf/components/esp_common /Users/n0mn0m/projects/esp/esp-idf/components/esp_rom /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py /Users/n0mn0m/projects/esp/esp-idf/components/log /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/main /Users/n0mn0m/projects/esp/esp-idf/components/bootloader/subproject/components/micro-ecc /Users/n0mn0m/projects/esp/esp-idf/components/partition_table /Users/n0mn0m/projects/esp/esp-idf/components/soc /Users/n0mn0m/projects/esp/esp-idf/components/spi_flash /Users/n0mn0m/projects/esp/esp-idf/components/xtensa\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/bootloader\r\n[489/902] Performing build step for 'bootloader'\r\n[1/62] Generating project_elf_src.c\r\n[2/62] Building C object CMakeFiles/bootloader.elf.dir/project_elf_src.c.obj\r\n[3/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/gpio_periph.c.obj\r\n[4/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_periph.c.obj\r\n[5/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_pm.c.obj\r\n[6/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/cpu_util.c.obj\r\n[7/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/sdmmc_periph.c.obj\r\n[8/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/soc_memory_layout.c.obj\r\n[9/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/sdio_slave_periph.c.obj\r\n[10/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/spi_periph.c.obj\r\n[11/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/lldesc.c.obj\r\n[12/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_time.c.obj\r\n[13/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_init.c.obj\r\n[14/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_clk_init.c.obj\r\n[15/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/soc_include_legacy_warn.c.obj\r\n[16/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/memory_layout_utils.c.obj\r\n[17/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_wdt.c.obj\r\n[18/62] Building ASM object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/debug_helpers_asm.S.obj\r\n[19/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_slave_hal.c.obj\r\n[20/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_sleep.c.obj\r\n[21/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/eri.c.obj\r\n[22/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_slave_hal_iram.c.obj\r\n[23/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/debug_helpers.c.obj\r\n[24/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_flash_hal.c.obj\r\n[25/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_hal.c.obj\r\n[26/62] Building C object esp-idf/xtensa/CMakeFiles/__idf_xtensa.dir/trax.c.obj\r\n[27/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/esp32/esp_efuse_table.c.obj\r\n[28/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/esp32/rtc_clk.c.obj\r\n[29/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_hal_iram.c.obj\r\n[30/62] Building C object esp-idf/soc/CMakeFiles/__idf_soc.dir/src/hal/spi_flash_hal_iram.c.obj\r\n[31/62] Building C object esp-idf/log/CMakeFiles/__idf_log.dir/log.c.obj\r\n[32/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_fields.c.obj\r\n[33/62] Linking C static library esp-idf/log/liblog.a\r\n[34/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_api.c.obj\r\n[35/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_clock.c.obj\r\n[36/62] Linking C static library esp-idf/xtensa/libxtensa.a\r\n[37/62] Linking C static library esp-idf/soc/libsoc.a\r\n[38/62] Building C object esp-idf/spi_flash/CMakeFiles/__idf_spi_flash.dir/spi_flash_rom_patch.c.obj\r\n[39/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_flash.c.obj\r\n[40/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_flash_config.c.obj\r\n[41/62] Building C object esp-idf/efuse/CMakeFiles/__idf_efuse.dir/src/esp_efuse_utility.c.obj\r\n[42/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_random.c.obj\r\n[43/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_partitions.c.obj\r\n[44/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_encrypt.c.obj\r\n[45/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_common.c.obj\r\n[46/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/secure_boot_signatures.c.obj\r\n[47/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/bootloader_sha.c.obj\r\n[48/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/flash_qio_mode.c.obj\r\n[49/62] Building C object esp-idf/main/CMakeFiles/__idf_main.dir/bootloader_start.c.obj\r\n[50/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp_image_format.c.obj\r\n[51/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/flash_encrypt.c.obj\r\n[52/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/esp32/secure_boot.c.obj\r\n[53/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_utility.c.obj\r\n[54/62] Building C object esp-idf/bootloader_support/CMakeFiles/__idf_bootloader_support.dir/src/bootloader_init.c.obj\r\n[55/62] Building C object esp-idf/micro-ecc/CMakeFiles/__idf_micro-ecc.dir/micro-ecc/uECC.c.obj\r\n[56/62] Linking C static library esp-idf/micro-ecc/libmicro-ecc.a\r\n[57/62] Linking C static library esp-idf/bootloader_support/libbootloader_support.a\r\n[58/62] Linking C static library esp-idf/efuse/libefuse.a\r\n[59/62] Linking C static library esp-idf/spi_flash/libspi_flash.a\r\n[60/62] Linking C static library esp-idf/main/libmain.a\r\n[61/62] Linking C executable bootloader.elf\r\n[62/62] Generating binary image from built executable\r\nesptool.py v2.8\r\nGenerated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/bootloader/bootloader.bin\r\n[807/902] Building CXX object esp-idf/main/CMakeFiles/__idf_main.dir/esp/main.cc.obj\r\n../main/esp/main.cc: In function 'void app_main()':\r\n../main/esp/main.cc:34:32: warning: cast between incompatible function types from 'int (*)(int, char**)' to 'TaskFunction_t' {aka 'void (*)(void*)'} [-Wcast-function-type]\r\n   xTaskCreate((TaskFunction_t)&tf_main, \"tensorflow\", 32 * 1024, NULL, 8, NULL);\r\n                                ^~~~~~~\r\n[818/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ir/tensorflow/lite/experimental/microfrontend/lib/fft.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[819/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...nsorflow/lite/experimental/microfrontend/lib/fft_util.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[834/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/debug_log.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[836/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_error_reporter.cc.ob\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[840/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_time.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[842/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_string.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[843/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_utils.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[845/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/activations.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[848/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/memory_helpers.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[850/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/simple_memory_allocator.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[851/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_interpreter.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[852/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i....dir/tensorflow/lite/micro/micro_optional_debug_tools.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[853/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_allocator.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[854/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/ceil.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[855/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/circular_buffer.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[856/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/arg_min_max.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[857/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ro.dir/tensorflow/lite/micro/kernels/all_ops_resolver.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[858/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/floor.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[859/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/add.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[860/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/dequantize.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[861/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/elementwise.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[862/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...micro.dir/tensorflow/lite/micro/kernels/concatenation.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[863/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...icro.dir/tensorflow/lite/micro/kernels/depthwise_conv.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[864/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/conv.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[865/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/neg.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[866/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/logistic.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[867/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/logical.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[868/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/fully_connected.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[869/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pack.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[870/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/test_helpers.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[871/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/quantize.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[872/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/reduce.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[873/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/prelu.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[874/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/reshape.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[875/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pooling.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[876/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/round.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[877/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/pad.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[878/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...rflow/lite/micro/memory_planner/greedy_memory_planner.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[879/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...rflow/lite/micro/memory_planner/linear_memory_planner.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[881/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/error_reporter.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[882/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/split.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[883/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/unpack.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[884/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/mul.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[885/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/tensor_utils.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[886/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/svdf.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[887/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...cro.dir/tensorflow/lite/micro/kernels/maximum_minimum.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[888/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...ir/tensorflow/lite/kernels/internal/quantization_util.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[889/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...micro.dir/tensorflow/lite/micro/kernels/strided_slice.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[890/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/kernels/kernel_util.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[891/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/softmax.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[892/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/core/api/op_resolver.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[893/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/testing/test_utils.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[894/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/sub.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[895/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/kernels/comparisons.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[896/902] Building CXX object esp-idf/tfmicro/CMakeFiles/__i...o.dir/tensorflow/lite/core/api/flatbuffer_conversions.cc.obj\r\ncc1plus: warning: command line option '-std=c11' is valid for C/ObjC but not for C++\r\n[902/902] Generating binary image from built executable\r\nesptool.py v2.8\r\nGenerated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.bin\r\n\r\nProject build complete. To flash, run this command:\r\n/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python ../../../../../../../../../../../projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p (PORT) -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_size detect --flash_freq 80m 0x1000 build/bootloader/bootloader.bin 0x8000 build/partition_table/partition-table.bin 0x10000 build/micro_speech.bin\r\nor run 'idf.py -p (PORT) flash'\r\n(38) tron:esp-idf n0mn0m$ ls\r\nCMakeLists.txt\t\tREADME_ESP.md\t\tcomponents\t\tsdkconfig\r\nLICENSE\t\t\tbuild\t\t\tmain\t\t\tsdkconfig.defaults\r\n(38) tron:esp-idf n0mn0m$ vim main/command_responder.cc \r\n(38) tron:esp-idf n0mn0m$ idf.py build\r\nChecking Python dependencies...\r\nPython requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[1/8] Performing build step for 'bootloader'\r\nninja: no work to do.\r\n[6/6] Generating binary image from built executable\r\nesptool.py v2.8\r\nGenerated /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.bin\r\n\r\nProject build complete. To flash, run this command:\r\n/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python ../../../../../../../../../../../projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p (PORT) -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_size detect --flash_freq 80m 0x1000 build/bootloader/bootloader.bin 0x8000 build/partition_table/partition-table.bin 0x10000 build/micro_speech.bin\r\nor run 'idf.py -p (PORT) flash'\r\n(38) tron:esp-idf n0mn0m$ idf.py --port /dev/cu.SLAB_USBtoUART flash monitor\r\nChecking Python dependencies...\r\nPython requirements from /Users/n0mn0m/projects/esp/esp-idf/requirements.txt are satisfied.\r\nAdding flash's dependency \"all\" to list of actions\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[1/3] Performing build step for 'bootloader'\r\nninja: no work to do.\r\nExecuting action: flash\r\nRunning esptool.py in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python /Users/n0mn0m/projects/esp/esp-idf/components/esptool_py/esptool/esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash @flash_project_args\"...\r\nesptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x8000 partition_table/partition-table.bin 0x1000 bootloader/bootloader.bin 0x10000 micro_speech.bin\r\nesptool.py v2.8\r\nSerial port /dev/cu.SLAB_USBtoUART\r\nConnecting........__\r\nDetecting chip type... ESP32\r\nChip is ESP32D0WDQ5 (revision 1)\r\nFeatures: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None\r\nCrystal is 40MHz\r\nMAC: bc:dd:c2:d0:23:4c\r\nUploading stub...\r\nRunning stub...\r\nStub running...\r\nChanging baud rate to 460800\r\nChanged.\r\nConfiguring flash size...\r\nCompressed 3072 bytes to 103...\r\nWrote 3072 bytes (103 compressed) at 0x00008000 in 0.0 seconds (effective 2196.8 kbit/s)...\r\nHash of data verified.\r\nCompressed 27296 bytes to 16009...\r\nWrote 27296 bytes (16009 compressed) at 0x00001000 in 0.4 seconds (effective 599.2 kbit/s)...\r\nHash of data verified.\r\nCompressed 256864 bytes to 153590...\r\nWrote 256864 bytes (153590 compressed) at 0x00010000 in 3.7 seconds (effective 552.3 kbit/s)...\r\nHash of data verified.\r\n\r\nLeaving...\r\nHard resetting via RTS pin...\r\nExecuting action: monitor\r\nRunning idf_monitor in directory /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\nExecuting \"/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python /Users/n0mn0m/projects/esp/esp-idf/tools/idf_monitor.py -p /dev/cu.SLAB_USBtoUART -b 115200 /Users/n0mn0m/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.elf -m '/Users/n0mn0m/.espressif/python_env/idf4.0_py3.8_env/bin/python' '/Users/n0mn0m/projects/esp/esp-idf/tools/idf.py' '--port' '/dev/cu.SLAB_USBtoUART'\"...\r\n--- idf_monitor on /dev/cu.SLAB_USBtoUART 115200 ---\r\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\r\nets Jun  8 2016 00:22:57\r\n\r\nrst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)\r\nconfigsip: 0, SPIWP:0xee\r\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\r\nmode:DIO, clock div:1\r\nload:0x3fff0018,len:4\r\nload:0x3fff001c,len:7304\r\nload:0x40078000,len:14944\r\nho 0 tail 12 room 4\r\nload:0x40080400,len:4940\r\nentry 0x40080704\r\nI (64) boot: Chip Revision: 1\r\nI (70) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (41) boot: ESP-IDF v4.0-223-gfdbdf9a0e-dirty 2nd stage bootloader\r\nI (41) boot: compile time 20:36:40\r\nI (41) boot: Enabling RNG early entropy source...\r\nI (47) qio_mode: Enabling default flash chip QIO\r\nI (53) boot: SPI Speed      : 80MHz\r\nI (57) boot: SPI Mode       : QIO\r\nI (61) boot: SPI Flash Size : 2MB\r\nI (65) boot: Partition Table:\r\nI (69) boot: ## Label            Usage          Type ST Offset   Length\r\nI (76) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (83) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (91) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (98) boot: End of partition table\r\nI (102) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (110) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d740 ( 55104) map\r\nI (134) esp_image: segment 1: paddr=0x0001d768 vaddr=0x3ffb0000 size=0x020e8 (  8424) load\r\nI (137) esp_image: segment 2: paddr=0x0001f858 vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at /Users/n0mn0m/projects/esp/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (141) esp_image: segment 3: paddr=0x0001fc60 vaddr=0x40080400 size=0x003b0 (   944) load\r\nI (150) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x25640 (153152) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (200) esp_image: segment 5: paddr=0x00045660 vaddr=0x400807b0 size=0x094d8 ( 38104) load\r\nI (219) boot: Loaded app from partition at offset 0x10000\r\nI (219) boot: Disabling RNG early entropy source...\r\nI (220) cpu_start: Pro cpu up.\r\nI (223) cpu_start: Application information:\r\nI (228) cpu_start: Project name:     micro_speech\r\nI (233) cpu_start: App version:      v1.12.1-27700-g5c4931bbf6\r\nI (240) cpu_start: Compile time:     Mar 19 2020 20:36:26\r\nI (246) cpu_start: ELF file SHA256:  de88378406963121...\r\nI (252) cpu_start: ESP-IDF:          v4.0-223-gfdbdf9a0e-dirty\r\nI (258) cpu_start: Starting app cpu, entry point is 0x40080fe8\r\n0x40080fe8: call_start_cpu1 at /Users/n0mn0m/projects/esp/esp-idf/components/esp32/cpu_start.c:271\r\n\r\nI (0) cpu_start: App cpu up.\r\nI (269) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (276) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (282) heap_init: At 3FFB69D8 len 00029628 (165 KiB): DRAM\r\nI (288) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (294) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (301) heap_init: At 40089C88 len 00016378 (88 KiB): IRAM\r\nI (307) cpu_start: Pro cpu start user code\r\nI (324) spi_flash: detected chip: generic\r\nI (324) spi_flash: flash io: qio\r\nW (324) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.\r\nI (336) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\nI (429) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (429) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (439) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 16, CLKM: 39, BCK: 8, MCLK: 4096000.000, SCLK: 512800.000000, diva: 64, divb: 4\r\nI (559) TF_LITE_AUDIO_PROVIDER: Audio Recording started\r\nI (1569) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @0ms\r\nI (2469) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @1100ms\r\nI (3259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2000ms\r\nI (4069) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @2800ms\r\nI (4869) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @3600ms\r\nI (5569) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @4400ms\r\nI (6259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5100ms\r\nI (6969) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @5800ms\r\nI (7579) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @6500ms\r\nI (8169) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @7100ms\r\nI (8759) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @7700ms\r\nI (9259) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @8300ms\r\nI (9769) TF_LITE_COMMAND_RESPONDER: Heard silence (0) @8800ms\r\nI (10269) TF_LITE_COMMAND_RESPONDER: Heard silence (64) @9300ms\r\nI (10759) TF_LITE_COMMAND_RESPONDER: Heard silence (64) @9800ms\r\n```\r\n", "comments": ["I have similar problem while using esp-eye board. I ran the command:\r\n```\r\nidf.py --port  /dev/cu.SLAB_USBtoUART flash monitor\r\n```\r\nI got this: \r\n```\r\n Checking Python dependencies...\r\nPython requirements from /Users/george/esp/esp-idf/requirements.txt are satisfied.\r\nAdding flash's dependency \"all\" to list of actions\r\nExecuting action: all (aliases: build)\r\nRunning ninja in directory /Users/george/Documents/embedded_python/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"ninja all\"...\r\n[1/4] cd /Users/george/Documents/embedded_python/tensorflow/tensorflow/lite/micro/tool...echo \"*******************************************************************************\r\nPartition table binary generated. Contents:\r\n*******************************************************************************\r\n# Espressif ESP32 Partition Table\r\n# Name, Type, SubType, Offset, Size, Flags\r\nnvs,data,nvs,0x9000,24K,\r\nphy_init,data,phy,0xf000,4K,\r\nfactory,app,factory,0x10000,1M,\r\n*******************************************************************************\r\n[2/4] Performing build step for 'bootloader'\r\nninja: no work to do.\r\nExecuting action: flash\r\nRunning esptool.py in directory /Users/george/Documents/embedded_python/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build\r\nExecuting \"/Users/george/.espressif/python_env/idf4.0_py3.7_env/bin/python /Users/george/esp/esp-idf/components/esptool_py/esptool/esptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash @flash_project_args\"...\r\nesptool.py -p /dev/cu.SLAB_USBtoUART -b 460800 --before default_reset --after hard_reset write_flash --flash_mode dio --flash_freq 80m --flash_size 2MB 0x8000 partition_table/partition-table.bin 0x1000 bootloader/bootloader.bin 0x10000 micro_speech.bin\r\nesptool.py v2.8\r\nSerial port /dev/cu.SLAB_USBtoUART\r\nConnecting....\r\nDetecting chip type... ESP32\r\nChip is ESP32D0WDQ5 (revision 1)\r\nFeatures: WiFi, BT, Dual Core, 240MHz, VRef calibration in efuse, Coding Scheme None\r\nCrystal is 40MHz\r\nMAC: ac:67:b2:6f:9f:a8\r\nUploading stub...\r\nRunning stub...\r\nStub running...\r\nChanging baud rate to 460800\r\nChanged.\r\nConfiguring flash size...\r\nCompressed 3072 bytes to 103...\r\nWrote 3072 bytes (103 compressed) at 0x00008000 in 0.0 seconds (effective 2478.3 kbit/s)...\r\nHash of data verified.\r\nCompressed 27232 bytes to 15989...\r\nWrote 27232 bytes (15989 compressed) at 0x00001000 in 0.4 seconds (effective 591.0 kbit/s)...\r\nHash of data verified.\r\nCompressed 251728 bytes to 151389...\r\nWrote 251728 bytes (151389 compressed) at 0x00010000 in 3.6 seconds (effective 566.8 kbit/s)...\r\nHash of data verified.\r\n\r\nLeaving...\r\nHard resetting via RTS pin...\r\nExecuting action: monitor\r\nRunning idf_monitor in directory /Users/george/Documents/embedded_python/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf\r\nExecuting \"/Users/george/.espressif/python_env/idf4.0_py3.7_env/bin/python /Users/george/esp/esp-idf/tools/idf_monitor.py -p /dev/cu.SLAB_USBtoUART -b 115200 /Users/george/Documents/embedded_python/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/micro_speech/esp-idf/build/micro_speech.elf -m '/Users/george/.espressif/python_env/idf4.0_py3.7_env/bin/python' '/Users/george/esp/esp-idf/tools/idf.py' '--port' '/dev/cu.SLAB_USBtoUART'\"...\r\n--- idf_monitor on /dev/cu.SLAB_USBtoUART 115200 ---\r\n--- Quit: Ctrl+] | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---\r\nets Jun  8 2016 00:22:57\r\n\r\nrst:0x1 (POWERON_RESET),boot:0x13 (SPI_FAST_FLASH_BOOT)\r\nconfigsip: 0, SPIWP:0xee\r\nclk_drv:0x00,q_drv:0x00,d_drv:0x00,cs0_drv:0x00,hd_drv:0x00,wp_drv:0x00\r\nmode:DIO, clock div:1\r\nload:0x3fff0018,len:4\r\nload:0x3fff001c,len:7248\r\nho 0 tail 12 room 4\r\nload:0x40078000,len:14944\r\nload:0x40080400,len:4940\r\nentry 0x40080704\r\nI (69) boot: Chip Revision: 1\r\nI (69) boot_comm: chip revision: 1, min. bootloader chip revision: 0\r\nI (41) boot: ESP-IDF v4.0.1-dirty 2nd stage bootloader\r\nI (41) boot: compile time 23:49:37\r\nI (41) boot: Enabling RNG early entropy source...\r\nI (46) qio_mode: Enabling default flash chip QIO\r\nI (51) boot: SPI Speed      : 80MHz\r\nI (55) boot: SPI Mode       : QIO\r\nI (59) boot: SPI Flash Size : 2MB\r\nI (63) boot: Partition Table:\r\nI (67) boot: ## Label            Usage          Type ST Offset   Length\r\nI (74) boot:  0 nvs              WiFi data        01 02 00009000 00006000\r\nI (81) boot:  1 phy_init         RF data          01 01 0000f000 00001000\r\nI (89) boot:  2 factory          factory app      00 00 00010000 00100000\r\nI (96) boot: End of partition table\r\nI (101) boot_comm: chip revision: 1, min. application chip revision: 0\r\nI (108) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0d538 ( 54584) map\r\nI (132) esp_image: segment 1: paddr=0x0001d560 vaddr=0x3ffb0000 size=0x02154 (  8532) load\r\nI (135) esp_image: segment 2: paddr=0x0001f6bc vaddr=0x40080000 size=0x00400 (  1024) load\r\n0x40080000: _WindowOverflow4 at /Users/george/esp/esp-idf/components/freertos/xtensa_vectors.S:1778\r\n\r\nI (139) esp_image: segment 3: paddr=0x0001fac4 vaddr=0x40080400 size=0x0054c (  1356) load\r\nI (148) esp_image: segment 4: paddr=0x00020018 vaddr=0x400d0018 size=0x2435c (148316) map\r\n0x400d0018: _stext at ??:?\r\n\r\nI (196) esp_image: segment 5: paddr=0x0004437c vaddr=0x4008094c size=0x093a8 ( 37800) load\r\nI (216) boot: Loaded app from partition at offset 0x10000\r\nI (216) boot: Disabling RNG early entropy source...\r\nI (216) cpu_start: Pro cpu up.\r\nI (220) cpu_start: Application information:\r\nI (225) cpu_start: Project name:     micro_speech\r\nI (230) cpu_start: App version:      v1.12.1-34231-gbde4f48eaa\r\nI (237) cpu_start: Compile time:     Nov 16 2020 23:49:20\r\nI (243) cpu_start: ELF file SHA256:  e5712b1fd5e27940...\r\nI (249) cpu_start: ESP-IDF:          v4.0.1-dirty\r\nI (254) cpu_start: Starting app cpu, entry point is 0x40081038\r\n0x40081038: call_start_cpu1 at /Users/george/esp/esp-idf/components/esp32/cpu_start.c:271\r\n\r\nI (0) cpu_start: App cpu up.\r\nI (264) heap_init: Initializing. RAM available for dynamic allocation:\r\nI (271) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM\r\nI (277) heap_init: At 3FFB6A28 len 000295D8 (165 KiB): DRAM\r\nI (284) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM\r\nI (290) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM\r\nI (296) heap_init: At 40089CF4 len 0001630C (88 KiB): IRAM\r\nI (303) cpu_start: Pro cpu start user code\r\nI (320) spi_flash: detected chip: generic\r\nI (320) spi_flash: flash io: qio\r\nW (320) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.\r\nI (331) cpu_start: Starting scheduler on PRO CPU.\r\nI (0) cpu_start: Starting scheduler on APP CPU.\r\nI (425) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (425) I2S: DMA Malloc info, datalen=blocksize=600, dma_buf_count=3\r\nI (435) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 16, CLKM: 39, BCK: 8, MCLK: 4096000.000, SCLK: 512800.000000, diva: 64, divb: 4\r\nI (555) TF_LITE_AUDIO_PROVIDER: Audio Recording started\r\n```\r\nNo progress just stuck.\r\n", "My experience was hit and miss. I had to reflash, set the log level to `DEBUG` and would get good behavior on audio recognition sometimes. The detection was fairly hit and miss, but would work from time to time.", "@AdityaHPatwardhan  who has contributed the ESP32 porting for speech recognition, in case there are some insights.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37732\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37732\">No</a>\n"]}, {"number": 37731, "title": "Single-precision `tf.math.erf` with TF2 produces (slightly) different results to TF1 and scipy", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): macOS 10.15.2d and Colab\r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0\r\n- Python version: 3.7.3\r\n\r\n**Describe the current behavior**\r\n`tf.math.erf` with `float32` can produce slightly different results between TF1 and TF2 (scipy is consistent with TF1).\r\n\r\n**Describe the expected behavior**\r\nI wouldn't expect any difference between the two (although I confess I'm not sure this is actually a bug, maybe it's an expected side-effect of some other change).\r\n\r\n**Standalone code to reproduce the issue** \r\nWith TF2 (https://colab.research.google.com/drive/1K3QB6bmu1GmAVepPMpTNgLkIKXKKB7eX):\r\n```\r\nimport tensorflow as tf\r\nimport scipy\r\nimport numpy as np\r\nprint(tf.math.erf(-1.0606601).numpy())\r\nprint(scipy.special.erf(np.array(-1.0606601, dtype=np.float32)))\r\n```\r\nproduces\r\n```\r\n-0.86638564\r\n-0.8663856\r\n```\r\n\r\nWith TF1 (https://colab.research.google.com/drive/1dQZOXSX8t4Jy1ZIk9Xzs9gxpbUn8acws):\r\n```\r\nimport tensorflow as tf\r\nimport scipy\r\nimport numpy as np\r\nprint(tf.Session().run(tf.math.erf(-1.0606601)))\r\nprint(scipy.special.erf(np.array(-1.0606601, dtype=np.float32)))\r\n```\r\nproduces\r\n```\r\n-0.8663856\r\n-0.8663856\r\n```\r\n\r\nWith double precision the result is consistently\r\n```\r\n-0.8663855711671024\r\n```", "comments": ["Works without any issues with [TF 1.15](https://colab.research.google.com/gist/amahendrakar/1668c50f153d153f463d82a7789de5b9/37731-1-15.ipynb). Was able to reproduce the issue with [TF 2.1](https://colab.research.google.com/gist/amahendrakar/03f5d435f3447b70b139111d371e9d50/37731-2-1.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b574d2f99ce8a58e26f05faa7849c597/37731-tf-nightly.ipynb). Thanks!", "The results I get on my desktop are different, reproducible with TensorFlow 2.x (TF=TensorFlow result, SP=scipy result):\r\n\r\n```\r\n# <dtype: 'float16'>\r\n# TF erf(-1.0606601) = -0.8662109375\r\n# SP erf(-1.0606601) = -0.8663440942764282\r\n#\r\n# <dtype: 'float32'>\r\n# TF erf(-1.0606601) = -0.8663856387138367\r\n# SP erf(-1.0606601) = -0.8663855791091919\r\n#\r\n# <dtype: 'float64'>\r\n# TF erf(-1.0606601) = -0.8663855711671024\r\n# SP erf(-1.0606601) = -0.8663855711671024\r\n```\r\n\r\nNote: those are all consistent with the numeric precison for the given types: float16 ~ 3 digits, float32 ~ 7 digits.", "@charmasaur please do not rely on binary equality for floating point data. The math functions in the standard library typically only gives guarantees to be with a few ulps (units in the last place). Within that bound, libraries should be free to make different implementation choices to  make the best accuracy/speed trade-off for the underlying hardware.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37731\">No</a>\n", "Makes sense, thanks!"]}, {"number": 37730, "title": "STM32F746NG Hello World example fails in /tensorflow/lite/micro/kernels/cmsis-nn/conv.cc", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\ncat /etc/lsb-release\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=19.10\r\nDISTRIB_CODENAME=eoan\r\nDISTRIB_DESCRIPTION=\"Pop!_OS 19.10\"\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): \r\ncommit bfafc1acef59ff5a7ba2bf2675350812e552d5ad\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\narm mbed\r\n\r\nmbed config --list\r\n[mbed] Global config:\r\nGCC_ARM_PATH=/opt/gcc-arm-none-eabi-9-2019-q4-major/bin/\r\nARM_PATH=/opt/gcc-arm-none-eabi-9-2019-q4-major/bin/arm-none-eabi-gcc\r\n\r\n**Describe the problem**\r\nwhen building tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed$\r\nmbed compile -m DISCO_F746NG -t GCC_ARM\r\n\r\nthe following build time error is returned : \r\nCompile [ 79.9%]: conv.cc\r\n[Error] arm_nnsupportfunctions.h@505,39: 'Q31_MIN' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@507,18: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@660,36: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@667,64: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@668,65: 'Q31_MIN' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@674,49: 'Q31_MAX' was not declared in this scope\r\n[ERROR] '_queue.SimpleQueue' object has no attribute 'queue'\r\n\r\nand with -v added \r\n\r\nCompile [ 74.9%]: conv.cc\r\nCompile: /opt/gcc-arm-none-eabi-9-2019-q4-major/bin/arm-none-eabi-g++ -std=gnu++14 -fno-rtti -Wvla -c -Wall -Wextra -Wno-unused-parameter -Wno-missing-field-initializers -fmessage-length=0 -fno-exceptions -ffunction-sections -fdata-sections -funsigned-char -MMD -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -g -DMBED_TRAP_ERRORS_ENABLED=1 -mcpu=cortex-m7 -mthumb -mfpu=fpv5-sp-d16 -mfloat-abi=softfp -DMBED_ROM1_START=0x200000 -DMBED_ROM1_SIZE=0x100000 -DMBED_ROM_START=0x8000000 -DMBED_ROM_SIZE=0x100000 -DMBED_RAM_START=0x20010000 -DMBED_RAM_SIZE=0x40000 -DMBED_RAM1_START=0x20000000 -DMBED_RAM1_SIZE=0x10000 -D__MBED__=1 -DDEVICE_ANALOGOUT=1 -DDEVICE_STDIO_MESSAGES=1 -D__CMSIS_RTOS -DTARGET_NAME=DISCO_F746NG -DTARGET_RTOS_M4_M7 -DCOMPONENT_PSA_SRV_IMPL=1 -DARM_MATH_CM7 -DTARGET_M7 -DTARGET_STM32F7 -DDEVICE_CAN=1 -DTARGET_CORTEX -DDEVICE_PWMOUT=1 -DCOMPONENT_QSPIF=1 -D__FPU_PRESENT=1 -DTARGET_STM32F746 -D__MBED_CMSIS_RTOS_CM -DDEVICE_PORTOUT=1 -DMBED_TICKLESS -DDEVICE_ANALOGIN=1 -DDEVICE_I2C_ASYNCH=1 -DTARGET_STM -DDEVICE_CRC=1 -DTARGET_CORTEX_M -DUSE_HAL_DRIVER -DDEVICE_I2CSLAVE=1 -DDEVICE_SERIAL_FC=1 -DCOMPONENT_FLASHIAP=1 -DTARGET_STM32F746NG -DTARGET_LIKE_CORTEX_M7 -D__CORTEX_M7 -DDEVICE_USBDEVICE=1 -DTARGET_LIKE_MBED -DDEVICE_PORTIN=1 -DDEVICE_WATCHDOG=1 -DDEVICE_SERIAL_ASYNCH=1 -DDEVICE_PORTINOUT=1 -DDEVICE_SPI=1 -DTOOLCHAIN_GCC_ARM -DDEVICE_SLEEP=1 -DUSBHOST_OTHER -DDEVICE_TRNG=1 -DTARGET_FAMILY_STM32 -DDEVICE_LPTICKER=1 -DTARGET_FF_ARDUINO -DDEVICE_RTC=1 -DEXTRA_IDLE_STACK_REQUIRED -DTRANSACTION_QUEUE_SIZE_SPI=2 -DDEVICE_FLASH=1 -DDEVICE_SPI_ASYNCH=1 -DCOMPONENT_PSA_SRV_EMUL=1 -DDEVICE_INTERRUPTIN=1 -DUSE_FULL_LL_DRIVER -DDEVICE_USTICKER=1 -DTOOLCHAIN_GCC -DDEVICE_MPU=1 -DTARGET_STM_EMAC -DTARGET_RELEASE -DMBED_BUILD_TIMESTAMP=1584658515.2890425 -DTARGET_STM32F746xG -DDEVICE_RESET_REASON=1 -DDEVICE_EMAC=1 -DDEVICE_SERIAL=1 -DDEVICE_QSPI=1 -DUSB_STM_HAL -DCOMPONENT_NSPE=1 -DDEVICE_I2C=1 -DDEVICE_SPISLAVE=1 -DTARGET_DISCO_F746NG @./BUILD/DISCO_F746NG/GCC_ARM/.includes_12aa10db82ce4f9a526c01c2017b7c4b.txt -include ./BUILD/DISCO_F746NG/GCC_ARM/mbed_config.h -MD -MF BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/kernels/cmsis-nn/conv.d -o BUILD/DISCO_F746NG/GCC_ARM/tensorflow/lite/micro/kernels/cmsis-nn/conv.o ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc\r\n[Error] arm_nnsupportfunctions.h@505,39: 'Q31_MIN' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@507,18: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@660,36: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@667,64: 'Q31_MAX' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@668,65: 'Q31_MIN' was not declared in this scope\r\n[Error] arm_nnsupportfunctions.h@674,49: 'Q31_MAX' was not declared in this scope\r\n[DEBUG] Return: 1\r\n[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_sat_doubling_high_mult(q31_t, q31_t)':\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:505:39: error: 'Q31_MIN' was not declared in this scope\r\n[DEBUG] Output:   505 |     if ((m1 == m2) && (m1 == (int32_t)Q31_MIN))\r\n[DEBUG] Output:       |                                       ^~~~~~~\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:507:18: error: 'Q31_MAX' was not declared in this scope\r\n[DEBUG] Output:   507 |         result = Q31_MAX;\r\n[DEBUG] Output:       |                  ^~~~~~~\r\n[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_exp_on_negative_values(int32_t)':\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:660:36: error: 'Q31_MAX' was not declared in this scope\r\n[DEBUG] Output:   660 |     return SELECT_USING_MASK(mask, Q31_MAX, result);\r\n[DEBUG] Output:       |                                    ^~~~~~~\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n[DEBUG] Output:       |                                                  ^\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_mult_by_power_of_two(int32_t, int32_t)':\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:667:64: error: 'Q31_MAX' was not declared in this scope\r\n[DEBUG] Output:   667 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val > thresh), Q31_MAX, result);\r\n[DEBUG] Output:       |                                                                ^~~~~~~\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n[DEBUG] Output:       |                                                  ^\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:668:65: error: 'Q31_MIN' was not declared in this scope\r\n[DEBUG] Output:   668 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val < -thresh), Q31_MIN, result);\r\n[DEBUG] Output:       |                                                                 ^~~~~~~\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n[DEBUG] Output:    45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n[DEBUG] Output:       |                                                  ^\r\n[DEBUG] Output: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n[DEBUG] Output:                  from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_one_over_one_plus_x_for_x_in_0_1(int32_t)':\r\n[DEBUG] Output: ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:674:49: error: 'Q31_MAX' was not declared in this scope\r\n[DEBUG] Output:   674 |     const int64_t sum = (int64_t)val + (int64_t)Q31_MAX;\r\n[DEBUG] Output:       |                                                 ^~~~~~~\r\nTraceback (most recent call last):\r\n  File \"/home/tgall/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 555, in compile_queue\r\n    res['command']\r\n  File \"/home/tgall/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/toolchains/mbed_toolchain.py\", line 682, in compile_output\r\n    raise ToolException(stderr)\r\ntools.utils.ToolException: In file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_sat_doubling_high_mult(q31_t, q31_t)':\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:505:39: error: 'Q31_MIN' was not declared in this scope\r\n  505 |     if ((m1 == m2) && (m1 == (int32_t)Q31_MIN))\r\n      |                                       ^~~~~~~\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:507:18: error: 'Q31_MAX' was not declared in this scope\r\n  507 |         result = Q31_MAX;\r\n      |                  ^~~~~~~\r\nIn file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_exp_on_negative_values(int32_t)':\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:660:36: error: 'Q31_MAX' was not declared in this scope\r\n  660 |     return SELECT_USING_MASK(mask, Q31_MAX, result);\r\n      |                                    ^~~~~~~\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n      |                                                  ^\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'q31_t arm_nn_mult_by_power_of_two(int32_t, int32_t)':\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:667:64: error: 'Q31_MAX' was not declared in this scope\r\n  667 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val > thresh), Q31_MAX, result);\r\n      |                                                                ^~~~~~~\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n      |                                                  ^\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:668:65: error: 'Q31_MIN' was not declared in this scope\r\n  668 |     result = SELECT_USING_MASK(MASK_IF_NON_ZERO(val < -thresh), Q31_MIN, result);\r\n      |                                                                 ^~~~~~~\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:45:50: note: in definition of macro 'SELECT_USING_MASK'\r\n   45 | #define SELECT_USING_MASK(mask, a, b) ((mask) & (a)) ^ (~(mask) & (b))\r\n      |                                                  ^\r\nIn file included from ./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnfunctions.h:118,\r\n                 from ./tensorflow/lite/micro/kernels/cmsis-nn/conv.cc:18:\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h: In function 'int32_t arm_nn_one_over_one_plus_x_for_x_in_0_1(int32_t)':\r\n./tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/NN/Include/arm_nnsupportfunctions.h:674:49: error: 'Q31_MAX' was not declared in this scope\r\n  674 |     const int64_t sum = (int64_t)val + (int64_t)Q31_MAX;\r\n      |                                                 ^~~~~~~\r\n\r\n\r\nLooking at it the declarations for Q31_MAX etc seem to be in arm_math.h which seems to be found in : \r\ntensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/he\r\nllo_world/mbed/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include\r\n\r\n~/tinyml/tensorflow/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/he\r\nllo_world/mbed/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include$ ls\r\narm_common_tables.h  arm_helium_utils.h  arm_mve_tables.h\r\narm_const_structs.h  arm_math.h          arm_vec_math.h\r\n\r\n \r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["A slight update: \r\n\r\nIf you add the following to micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h the build will be successful : \r\n\r\n#define Q31_MAX   ((q31_t)(0x7FFFFFFFL))\r\n#define Q15_MAX   ((q15_t)(0x7FFF))\r\n#define Q7_MAX    ((q7_t)(0x7F))\r\n#define Q31_MIN   ((q31_t)(0x80000000L))\r\n#define Q15_MIN   ((q15_t)(0x8000))\r\n#define Q7_MIN    ((q7_t)(0x80))\r\n\r\n#define Q31_ABSMAX   ((q31_t)(0x7FFFFFFFL))\r\n#define Q15_ABSMAX   ((q15_t)(0x7FFF))\r\n#define Q7_ABSMAX    ((q7_t)(0x7F))\r\n#define Q31_ABSMIN   ((q31_t)0)\r\n#define Q15_ABSMIN   ((q15_t)0)\r\n#define Q7_ABSMIN    ((q7_t)0)\r\n\r\nLooking upstream to the mbed project there is this bit where they've entirely removed arm_math.h! \r\n\r\nhttps://github.com/ARMmbed/mbed-os/commit/5a985a7da5b93e7fb4be92e015f663d5619033ee\r\n\r\n", "Hi @tom-gall , I think we discussed this briefly in the TFLu SIG gitter forum, but I'll add some info here as well for visibility. There is a copy of the CMSIS arm_math.h in the mbed folder structure (`/mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h` as you mention). It may conflict with the arm_math.h that is downloaded directly from the CMSIS repo via the TFLu CMSIS third party download mechanism. This version of arm_math.h is typically newer. Here's some more info on the topic for a sustainable solution: https://github.com/ARMmbed/mbed-os/issues/12568#issuecomment-611436173. Short term, I believe this can be fixed by replacing arm_math.h as described here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/cmsis-nn/README.md", "@tom-gall Could you please update as per the above comment and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37730\">No</a>\n"]}, {"number": 37729, "title": "import tensorflow is very slow", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: \r\nCUDA Version: 10.1\r\ncudnn-10.1\r\n- GPU model and memory:\r\nTITAN RTX\r\n24190MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm thinking about using tensorflow 2.x for research. However, every time I import tf, it takes about 5 seconds.  `import tensorflow` is 10 times slower than`import torch`. \r\n\r\n**Describe the expected behavior**\r\nI was expecting the two to be roughly the same level for a fresh import. I know these two libraries have their own design choices. But I'm just wondering is there any chance to speed up the import statement. As a researcher, I need to constantly debug and 5s is just not good.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nfrom timeit import default_timer as timer\r\nimport os\r\n\r\nprint('import pytorch')\r\nstart = timer()\r\nimport torch\r\n\r\nend = timer()\r\nprint('Elapsed time: ' + str(end - start))\r\n\r\nprint('import tensorflow')\r\nstart = timer()\r\nimport tensorflow\r\n\r\nend = timer()\r\nprint('Elapsed time: ' + str(end - start))\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nimport pytorch\r\nElapsed time: 0.46338812075555325\r\nimport tensorflow\r\nElapsed time: 4.396180961281061\r\n```\r\n", "comments": ["The log to trace import `python3 -v -c 'import tensorflow' 2>&1 | ts '%H:%M:%.S' | grep 'import ' > log.txt` is attached below. \r\n\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/4357125/log.txt)\r\n", "@hankcs \r\ni have executed your code in nightly and this issue does not exist in nightly, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/630d50d7ee9c483d10ad99f23702129d/37729.ipynb)", "> @hankcs\r\n> i have executed your code in nightly and this issue does not exist in nightly, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/630d50d7ee9c483d10ad99f23702129d/37729.ipynb)\r\n\r\nThank you for your reply. I'm still getting similar performance with tf-nightly. In colab pytorch got 0.2s but tf-nightly got 2s. I ran your gist too, the results are similar.\r\n\r\nSee https://colab.research.google.com/drive/1-NUqqa7W_6_rYP3r09uSm6pnYHQXtLoh\r\n", "@hankcs I tried several time with `TF2.1` and `tf-nightly` and I could not reproduce the issue. Some time TF is faster and sometime Pytorch is faster but the difference is not much. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/1b4db6482cc71218451f1e4b7ed120cb/untitled36.ipynb). \r\n\r\nOutput is as follows\r\n\r\n```\r\nimport pytorch\r\nElapsed time: 2.196739539999996\r\nimport tensorflow\r\nElapsed time: 1.7435289440000048\r\n```\r\n\r\nI have a question. Are you importing it in `for` loop?. You can import once and run your code multiple times to debug. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "> I have a question. Are you importing it in for loop?. You can import once and run your code multiple times to debug. Thanks!\r\n\r\nNo, I'm importing only once. Redundant import takes no time.\r\n\r\nI found some clues. If I terminate the colab session then execute your gist, pytorch takes 2 seconds and tf takes 1-4 seconds which shows they are on the same level. \r\n<img width=\"768\" alt=\"hankcs com 2020-03-20 at 5 00 25 PM\" src=\"https://user-images.githubusercontent.com/5326890/77205952-58912000-6acc-11ea-9a60-117b9cd7480b.png\">\r\n**However, if I restart the runtime, pytorch takes 0.2 second but tf still takes 2 seconds.**\r\n<img width=\"322\" alt=\"hankcs com 2020-03-20 at 5 01 01 PM\" src=\"https://user-images.githubusercontent.com/5326890/77205999-6777d280-6acc-11ea-9968-a58b70cfa1c6.png\">\r\n\r\nThese shows that pytorch can make use of cache in memory while tf somehow can not. If you restart the session (reboot the system), there is no cache then they perform the same.", "@hankcs I ran the colab you shared and I got the similar results as I showed. Check the following output. \r\n\r\n```\r\nimport pytorch\r\nElapsed time: 2.00347342200007\r\nimport tensorflow\r\nElapsed time: 1.8452622999998312\r\n```", "@hankcs Can you please check the same in your local? Thanks!", "Sure, my local machine running 2.1.0 gives similar result to what I observe in colab.\r\n\r\n```\r\nimport pytorch\r\nElapsed time: 0.45859437994658947\r\nimport tensorflow\r\n2.1.0\r\nElapsed time: 4.311788005754352\r\n```\r\n\r\nI'll report tf-nightly later as I have some jobs running.", "```\r\n(t) mihaimaruseac@ankh:/tmp/t$ pip install -q tensorflow torch\r\n(t) mihaimaruseac@ankh:/tmp/t$ time python -c \"import tensorflow\"\r\n2020-03-20 14:47:02.352581: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-03-20 14:47:02.352679: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-03-20 14:47:02.352694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n\r\nreal    0m8.390s\r\nuser    0m2.386s\r\nsys     0m9.376s\r\n(t) mihaimaruseac@ankh:/tmp/t$ time python -c \"import tensorflow\"\r\n2020-03-20 14:47:18.194391: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-03-20 14:47:18.194475: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-03-20 14:47:18.194490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n\r\nreal    0m1.616s\r\nuser    0m3.429s\r\nsys     0m3.624s\r\n(t) mihaimaruseac@ankh:/tmp/t$ time python -c \"import pytorch\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'pytorch'\r\n\r\nreal    0m0.066s\r\nuser    0m0.058s\r\nsys     0m0.014s\r\n(t) mihaimaruseac@ankh:/tmp/t$ time python -c \"import torch\"\r\n\r\nreal    0m7.498s\r\nuser    0m2.299s\r\nsys     0m10.536s\r\n(t) mihaimaruseac@ankh:/tmp/t$ time python -c \"import torch\"\r\n\r\nreal    0m0.547s\r\nuser    0m2.314s\r\nsys     0m3.578s\r\n```\r\n\r\nThere is indeed a slowdown in tf import but it looks like first import time is the same in both pytorch and TF.\r\n\r\nWe have an internal bug assigned for speeding up TF import", "Thank you. I'm grateful to know that tf teams are making efforts to address this issue. I'll promote tf in my research projects.", "getting the same issue, any update?", "Also this seems to be a thing with caching by your operating system. On Windows the first time I load it by the time it imports may be 4 seconds+, which is unacceptable on an SSD on a Ryzen 9 computer. Then I close the interpreter and do it again, it imports in less than a second. This is why you fail to reproduce it in my humble opinion. Make sure you freshly restart your computer and run the test thereafter, also analyse the output of python -X importtime\r\n\r\n", "In my case the benchmarks were immediately after installing the pips, so they were not loaded in a cache.\r\n\r\nFor Windows, there is already a known issue regarding slow down to disk access when there are multiple files around https://github.com/microsoft/WSL/issues/4197", "any update on this issue?\r\nI have similar issue one my local mac machine:\r\n\r\n```\r\nimport pytorch\r\nElapsed time: 0.302831451\r\nimport tensorflow\r\nElapsed time: 3.103030106\r\n```\r\n\r\n**System information**\r\n\r\n* Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n* OS Platform and Distribution (e.g.,Linux Ubuntu 16.04):  macOS Big Sur version 11.01 \r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version (use command below):  2.3.1\r\n* Python version: Python 3.7.1\r\n", "Hello guys,\r\n\r\nAny updates? \r\n\r\nI am using a conda virtual environment.\r\n\r\n`import tensorflow as tf` takes an absurd amount of time.\r\n\r\nNote: I am using tensorflow cpu. \r\n\r\nWhat can be done to reduce this import time?", "same issue, any updates?\r\n", "Was able to replicate the issue with TF v2.5 ,  Yes it takes more time when the program is loaded first time  , you might see the difference between first time and second time loading (less than a millisecond) ,  please find the [gist ](https://colab.research.google.com/gist/mohantym/601bd3691c224cf95894016bf3ceaefd/37729.ipynb)here ..Thanks!", "@hankcs Looks like this was resolved in recent TF versions. I checked with `TF2.7` with CPU and GPU. [Here](https://colab.research.google.com/gist/jvishnuvardhan/5b8b89f8d3874e1df20d4dccde39b235/untitled36.ipynb) is a gist for reference. Thanks!\r\n\r\nResults are very different where pytorch is 10x slower than TF. I am not sure whether this is something related to colab. Will check it on my local soon. Thanks!\r\n\r\n```\r\n# results with TF2.7 (CPU)\r\n# import pytorch\r\n# Elapsed time: 26.26447787500001\r\n# import tensorflow\r\n# Elapsed time: 2.672678635000011\r\n\r\n# results with TF2.7 (GPU)\r\n# import pytorch\r\n# Elapsed time: 24.94221987399999\r\n# import tensorflow\r\n# Elapsed time: 2.79547692299991\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jvishnuvardhan Thank for your efforts. Indeed tf2.6 has made significant improvements on its importing speed. Though it's still slower than torch1.10.\r\n\r\nTested on a MBP 2021 with M1 MAX\r\n\r\n```\r\nimport tensorflow\r\nElapsed time: 1.139700959\r\nimport torch\r\nElapsed time: 0.383514875\r\n```\r\n\r\nMaybe macOS is just a rare case. I'm good with 1s importing time so I'll close this.", "It is still very slow. This was second run. Windows 11. \r\n```\r\nPS C:\\Users\\vojta> pip show tensorflow\r\nName: tensorflow\r\nVersion: 2.8.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\python39\\lib\\site-packages\r\nRequires: gast, tensorflow-io-gcs-filesystem, keras, absl-py, grpcio, tf-estimator-nightly, wrapt, setuptools, astunparse, tensorboard, flatbuffers, opt-einsum, six, typing-extensions, keras-preprocessing, termcolor, protobuf, libclang, google-pasta, h5py, numpy\r\nRequired-by: tf-models-official, tensorflow-text\r\nPS C:\\Users\\vojta> pip show tensorflow^C\r\nPS C:\\Users\\vojta> python --version\r\nPython 3.9.10\r\nPS C:\\Users\\vojta> Measure-Command {python -c \"import tensorflow\"}\r\n2022-03-03 20:47:27.191109: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2022-03-03 20:47:27.191369: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n\r\nDays              : 0\r\nHours             : 0\r\nMinutes           : 0\r\nSeconds           : 3\r\nMilliseconds      : 326\r\n```", "For me, it takes about 25 seconds to import tensorflow. Here is the output of `conda env export --no-builds`:\r\n ```yaml\r\n name: some_name\r\nchannels:\r\n  - anaconda\r\n  - defaults\r\ndependencies:\r\n  - argon2-cffi=21.3.0\r\n  - argon2-cffi-bindings=21.2.0\r\n  - attrs=20.2.0\r\n  - backcall=0.2.0\r\n  - bleach=3.2.1\r\n  - bzip2=1.0.8\r\n  - ca-certificates=2020.10.14\r\n  - certifi=2021.5.30\r\n  - cffi=1.15.0\r\n  - colorama=0.4.4\r\n  - debugpy=1.5.1\r\n  - decorator=4.4.2\r\n  - defusedxml=0.6.0\r\n  - entrypoints=0.3\r\n  - icu=58.2\r\n  - importlib-metadata=2.0.0\r\n  - importlib_metadata=2.0.0\r\n  - ipykernel=6.4.1\r\n  - ipython=7.29.0\r\n  - ipython_genutils=0.2.0\r\n  - ipywidgets=7.5.1\r\n  - jedi=0.18.1\r\n  - jinja2=2.11.2\r\n  - jpeg=9b\r\n  - jsonschema=3.2.0\r\n  - jupyter=1.0.0\r\n  - jupyter_client=6.1.7\r\n  - jupyter_console=6.2.0\r\n  - jupyter_core=4.9.2\r\n  - libffi=3.4.2\r\n  - libpng=1.6.37\r\n  - markupsafe=2.0.1\r\n  - matplotlib-inline=0.1.2\r\n  - mistune=0.8.4\r\n  - nbconvert=5.5.0\r\n  - nbformat=5.0.8\r\n  - nest-asyncio=1.5.1\r\n  - notebook=6.4.8\r\n  - openssl=1.1.1m\r\n  - packaging=20.4\r\n  - pandoc=2.11\r\n  - pandocfilters=1.5.0\r\n  - parso=0.8.0\r\n  - pickleshare=0.7.5\r\n  - pip=21.2.4\r\n  - prometheus_client=0.8.0\r\n  - prompt-toolkit=3.0.8\r\n  - prompt_toolkit=3.0.8\r\n  - pycparser=2.20\r\n  - pygments=2.7.1\r\n  - pyparsing=2.4.7\r\n  - pyqt=5.9.2\r\n  - pyrsistent=0.18.0\r\n  - python=3.10.0\r\n  - python-dateutil=2.8.1\r\n  - pywin32=302\r\n  - pywinpty=2.0.2\r\n  - pyzmq=22.3.0\r\n  - qt=5.9.7\r\n  - qtconsole=4.7.7\r\n  - qtpy=1.9.0\r\n  - send2trash=1.8.0\r\n  - setuptools=58.0.4\r\n  - sip=4.19.13\r\n  - sqlite=3.38.0\r\n  - terminado=0.13.1\r\n  - testpath=0.4.4\r\n  - tk=8.6.11\r\n  - tornado=6.1\r\n  - traitlets=5.0.5\r\n  - typing_extensions=3.7.4.3\r\n  - tzdata=2021e\r\n  - vc=14.2\r\n  - vs2015_runtime=14.27.29016\r\n  - wcwidth=0.2.5\r\n  - webencodings=0.5.1\r\n  - wheel=0.37.1\r\n  - widgetsnbextension=3.5.2\r\n  - wincertstore=0.2\r\n  - winpty=0.4.3\r\n  - xz=5.2.5\r\n  - zipp=3.3.1\r\n  - zlib=1.2.11\r\n  - pip:\r\n    - absl-py==1.0.0\r\n    - astunparse==1.6.3\r\n    - cachetools==5.0.0\r\n    - charset-normalizer==2.0.12\r\n    - click==8.0.4\r\n    - configobj==5.0.6\r\n    - cycler==0.11.0\r\n    - flatbuffers==2.0\r\n    - fonttools==4.30.0\r\n    - gast==0.5.3\r\n    - google-auth==2.6.0\r\n    - google-auth-oauthlib==0.4.6\r\n    - google-pasta==0.2.0\r\n    - grpcio==1.44.0\r\n    - h5py==3.6.0\r\n    - idna==3.3\r\n    - joblib==1.1.0\r\n    - keras==2.8.0\r\n    - keras-preprocessing==1.1.2\r\n    - kiwisolver==1.4.0\r\n    - libclang==13.0.0\r\n    - markdown==3.3.6\r\n    - matplotlib==3.5.1\r\n    - nltk==3.7\r\n    - numpy==1.22.3\r\n    - oauthlib==3.2.0\r\n    - opt-einsum==3.3.0\r\n    - pandas==1.4.1\r\n    - pillow==9.0.1\r\n    - plotly==5.6.0\r\n    - protobuf==3.19.4\r\n    - pyasn1==0.4.8\r\n    - pyasn1-modules==0.2.8\r\n    - pytz==2021.3\r\n    - regex==2022.3.15\r\n    - requests==2.27.1\r\n    - requests-oauthlib==1.3.1\r\n    - rsa==4.8\r\n    - scikit-learn==1.0.2\r\n    - scipy==1.8.0\r\n    - six==1.16.0\r\n    - tenacity==8.0.1\r\n    - tensorboard==2.8.0\r\n    - tensorboard-data-server==0.6.1\r\n    - tensorboard-plugin-wit==1.8.1\r\n    - tensorflow==2.8.0\r\n    - tensorflow-io-gcs-filesystem==0.24.0\r\n    - termcolor==1.1.0\r\n    - tf-estimator-nightly==2.8.0.dev2021122109\r\n    - threadpoolctl==3.1.0\r\n    - tqdm==4.63.0\r\n    - typing-extensions==4.1.1\r\n    - urllib3==1.26.9\r\n    - werkzeug==2.0.3\r\n    - wrapt==1.14.0\r\n    - yellowbrick==1.4\r\nprefix: C:\\Users\\my_username\\Anaconda3\\envs\\some_name\r\n```", "i also meet the same problem.  import tensorflow needs 1second.  \r\nis there any solution to fix this  problem?\r\n\r\nbest\r\nLisa Shi"]}, {"number": 37728, "title": "I get completely Wrong Results/Prediction(No result or wrong predictions) when I use Tflite format.", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): pip3 install --user --upgrade tensorflow\r\n- TensorFlow version (or github SHA if from source): 1.14.0\r\n\r\n\r\nI get good results on frozen_inference_graph.pb but I get completely Wrong Results/Prediction(No result or wrong predictions) when I convert it to Tflite format.\r\nI trained two models with different datasets(road signs and fruits) and got wrong results in both.\r\n\r\nInitially I followed [this Blog](https://towardsdatascience.com/deeppicar-part-6-963334b2abe0) for road sign detection.\r\nWhen I use my Tflite file in android app, it always returns 0 results . So I thought,Maybe there is something wrong with my App code. But then I created an under trained model on same data set(100 steps) and then used that in my App. This time,results were returned but obviously they were completely wrong. However,It helped me understand that there was something wrong with my Tflite file.\r\nNote: I used Android app from official repo of Tensorflow object detection.\r\n\r\nThen I followed [ this Medium Blog](https://medium.com/datadriveninvestor/how-to-train-your-own-custom-model-with-tensorflow-object-detection-api-and-deploy-it-into-android-aeacab7fa76f) for fruits detection.\r\nHere also I got good results on frozen_inference_graph.pb but my tflite was giving completely wrong result by detecting any objects and background randomly. \r\nHere is the [notebook](https://colab.research.google.com/drive/1khXaKmKUb6OE7OAYgkj0KKM6lZ7DtaUG) for fruit detection.\r\n\r\nI will be explaining steps I followed to generate Tflite file for road sign detection as per https://towardsdatascience.com/deeppicar-part-6-963334b2abe0\r\n\r\nPlease refer  [my Github Repo](https://github.com/omkardhawal21/my_tflite) for some essential files like my Mobilenet -SSD mOdel ,  model.ckpt-2000 , frozen_inference_graph.pb , tflite_graph.pb and sign.tflite files \r\n\r\nI have used a non-Quantized model for training on the same dataset and trained for 2000 steps.\r\nMy tensorflow version is 1.14\r\n\r\nI follwed the below steps to obtained Tflite file\r\n\r\n**1)Train Model**\r\n\r\npython3 model_main.py \\\r\n--pipeline_config_path=/machine-learning/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config \\\r\n--model_dir=/home/omkar/machine-learning/road_sign_detection/ \\\r\n--alsologtostderr \\\r\n--num_train_steps=2000 \\\r\n--num_eval_steps=50\r\n\r\n**2) Export tflite_graph.pb using model.ckpt-2000 obtained after Training.** \r\n\r\npython3 export_tflite_ssd_graph.py \\\r\n--pipeline_config_path='/machine-learning/ssd_mobilenet_v2_coco_2018_03_29/pipeline.config' \\\r\n--trained_checkpoint_prefix='/machine-learning/road_sign_detection/model.ckpt-2000' \\\r\n--output_directory='/home/omkar/machine-learning/road_sign_detection/tflite' \\\r\n--add_postprocessing_op=true\r\n\r\n**3) Generate Tflite file from tflite_graph.pb**\r\n\r\ntflite_convert \\\r\n--output_file='/machine-learning/road_sign_detection/tflite/mysign.tflite' \\\r\n--graph_def_file='/machine-learning/road_sign_detection/tflite/tflite_graph.pb' \\ --input_arrays='normalized_input_image_tensor' \\\r\n--inference_type=FLOAT \\\r\n--output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\r\n--mean_values=128 \\\r\n--std_dev_values=128 \\\r\n--input_shapes=1,300,300,3 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_nudging_weights_to_use_fast_gemm_kernel=true \r\n--allow_custom_ops\r\n\r\nNote: I've also tried using TOCO to convert to Tflite but achieved same results.\r\n\r\n**Failure details**\r\n Tflite produces completely wrong Results .\r\nOn road sign detection model, there were no results/ zero predicted objects.\r\nOn fruit detection model there were too many wrong predictions/results.\r\n\r\nAm I doing something wrong ? \r\nIs there something wrong with the process,commands ?\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "@omkardhawal21 \r\n\r\nHi, I know it is closed but I'm having the exact same problem as you did.\r\n\r\nCould you find the solution?\r\n\r\nThanks.", "@ArefAz \r\nWhen I used Google collab to convert the same model to tflite, I got the desired result. I think it was some version issue on my local PC.\r\n\r\nPlease refer my [notebook](https://colab.research.google.com/drive/1khXaKmKUb6OE7OAYgkj0KKM6lZ7DtaUG?usp=sharing)\r\n\r\nRefer the **Exporting Tflite Graph** section and the sections ahead. \r\n\r\nAlso please check if your data is good (has good variety in background). \r\nThank you\r\n\r\n", "I was able to fix this issue by using this command:\r\n\r\n`tflite_convert --graph_def_file tflite_inference_graph/tflite_graph.pb --output_file=amin/amin_detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops`\r\n\r\nI was using toco which is deprecated. I wanted to use a quantized model so I trained **ssdlite_mobilenet_v2_coco_2018_05_09** and added graph_rewriter at the end of my config file. I explained more [here](https://stackoverflow.com/questions/61749548/how-to-convert-tflite-graph-pb-to-detect-tflite-properly/61851433#61851433)."]}, {"number": 37727, "title": "Issue with calling tf.device", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Ubuntu19.10\r\n- From pip\r\n- TensorFlow version:2.1.0\r\n- Python version:3.7.4\r\n- Installed using virtualenv? pip? conda?:pip\r\n- GCC/Compiler version (if compiling from source):7.3.0\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory: RTX2060 Super 8G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen calling with\r\n                    tf.device(tf.compat.v1.train.replica_device_setter(\r\n                    worker_device=worker, ps_device='/cpu:0', ps_tasks=1)):\r\nIt gives error of not supporting functions. \r\n\r\n\r\n**Any other info / logs**\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTraceback (most recent call last):\r\n  File \"export_detections.py\", line 38, in <module>\r\n    with experiment._init_graph(config, with_dataset=True) as (net, dataset):\r\n  File \"/home/hashswan/anaconda3/lib/python3.7/contextlib.py\", line 112, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/hashswan/Desktop/SuperPoint/superpoint/experiment.py\", line 73, in _init_graph\r\n    n_gpus=n_gpus, **config['model'])\r\n  File \"/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py\", line 122, in __init__\r\n    self._build_graph()\r\n  File \"/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py\", line 275, in _build_graph\r\n    self._pred_graph(self.pred_in)\r\n  File \"/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py\", line 223, in _pred_graph\r\n    pred_out = self._gpu_tower(data, Mode.PRED, self.config['pred_batch_size'])\r\n  File \"/home/hashswan/Desktop/SuperPoint/superpoint/models/base_model.py\", line 157, in _gpu_tower\r\n    worker_device=worker, ps_device='/cpu:0', ps_tasks=1)):\r\n  File \"/home/hashswan/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 5078, in device_v2\r\n    raise RuntimeError(\"tf.device does not support functions.\")\r\nRuntimeError: tf.device does not support functions.\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\n", "comments": ["@SwagJ,\r\nLooks like the given code is incomplete. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/6b0b4f8af34c85e9652bffaa7e8f0ac6/37727.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar \r\nHi, \r\n\r\nThat was the code in my project. I revised it into this in colab: tf.device(tf.compat.v1.train.replica_device_setter(worker_device='/gpu:0',ps_device='/cpu:0', ps_tasks=1))\r\n\r\nAnd the error message is the same. \r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: tf.device does not support functions.\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/393b1d62ef3db6dede9e491813bc6326/37727.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/d50d8540251b226a65c5c4c35462c6e6/tf-nightly.ipynb). Please find the attached gist. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37727\">No</a>\n", "What is the follow-up? Is any ticket for this bug?", "Same error on Mac when calling\r\n\r\n```\r\nwith tf.device(tf.compat.v1.train.replica_device_setter(1, worker_device=worker_device)):\r\n```\r\n\r\n\r\n\r\n`RuntimeError(\"tf.device does not support functions.\")`", "@woj-i, @cmal,\r\nCould y'all please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!"]}, {"number": 37726, "title": "Gradients do not exist for variables after tf.concat().", "body": "<em>Tensorflow is unable to compute gradients after merging two variables with `tf.concat()`.</em> The issue is demonstrated in the following colab: https://colab.research.google.com/drive/1dkCcL5jfBmo47EsvmhNumjIkCGIdeFd5\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have based my issue on codes composed from oficial tutorials.\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04): Google Colab \r\n- TensorFlow installed from (source or binary): Binary\r\n - TensorFlow version (use command below): v2.1.0-0-ge5bf8de410 2.1.0\r\nI have tested the code also on TF 2.2 rc1.\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n\r\nTensorflow is unable to compute gradients after merging two variables with `tf.concat()`. The variables are not modified during training despite setting them as trainable.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow should be able to compute gradients also for concatenated variables.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nThe full test is available in Colab: [colab](https://colab.research.google.com/drive/1dkCcL5jfBmo47EsvmhNumjIkCGIdeFd5)\r\n\r\nThe most important part is the following:\r\n\r\n\r\n```\r\nclass CustomEmbedding(tf.keras.layers.Layer):\r\n  \r\n  def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\r\n    super(CustomEmbedding, self).__init__(**kwargs)\r\n    self.input_dim = input_dim\r\n    self.output_dim = output_dim\r\n    self.mask_zero = mask_zero\r\n    self.embeddings = None\r\n   \r\n  def build(self, input_shape):\r\n    e1 = self.add_weight(\r\n      shape=(int(self.input_dim/2), self.output_dim),\r\n      dtype=\"float32\", trainable=True,\r\n      name=\"e1\")\r\n\r\n    e2 = self.add_weight(\r\n      shape=(self.input_dim-int(self.input_dim/2), self.output_dim),\r\n      dtype=\"float32\", trainable=True,\r\n      name=\"e2\")\r\n    \r\n    self.embeddings = tf.concat((e1, e2), 0)\r\n\r\n    tf.print(self.embeddings)\r\n    \r\n  def call(self, inputs):\r\n    return tf.nn.embedding_lookup(self.embeddings, inputs)\r\n  \r\n  def compute_mask(self, inputs, mask=None):\r\n    if not self.mask_zero:\r\n      return None\r\n    return tf.not_equal(inputs, 0)\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.Embedding(3,32,trainable=False))\r\nmodel.add(layers.LSTM(32))\r\nmodel.add(layers.Dense(16, \"relu\"))\r\nmodel.add(layers.Dense(2, \"softmax\"))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(data)\r\n```\r\n\r\n\r\n**Other info / logs** \r\n\r\n```\r\n[[-0.042568922 0.302248985 0.401204079 ... -0.204555377 0.235091716 0.257138401]\r\n [-0.372319102 -0.415126026 0.340110391 ... -0.386968911 -0.410127133 -0.135176718]\r\n [0.341201216 0.208624214 0.357687324 ... 0.0621320605 0.0829377472 0.119318634]\r\n [0.380090982 0.0431897044 -0.2187078 ... -0.246274695 0.0664974749 0.223051161]]\r\nTrain for 400 steps\r\nWARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.\r\n400/400 [==============================] - 4s 11ms/step - loss: 0.3301 - accuracy: 1.0000\r\n[[-0.042568922 0.302248985 0.401204079 ... -0.204555377 0.235091716 0.257138401]\r\n [-0.372319102 -0.415126026 0.340110391 ... -0.386968911 -0.410127133 -0.135176718]\r\n [0.341201216 0.208624214 0.357687324 ... 0.0621320605 0.0829377472 0.119318634]\r\n [0.380090982 0.0431897044 -0.2187078 ... -0.246274695 0.0664974749 0.223051161]]\r\n```", "comments": ["@konopik, I tried to reproduce the issue on colab but getting error `InvalidArgumentError: indices[0,2] = 7 is not in [0, 3) [Op:GatherV2] name: custom_embedding_1/embedding_lookup/`. \r\nPlease take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/fcc7476107f244bbec9d28ae4560d479/untitled472.ipynb#scrollTo=pkm-D6rzZTo9) and confirm. Thanks", "Sorry for mixing two issues together. This problem is not the core of my issue. Moreover, I see that it occurs in the **CPU mode only**. I have removed this cell. It is not related to the problem.\r\n\r\nWhen GPU acceleration is turned only. This error is silently ignored. May be another issue?", "Was able to replicate the reported issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/16e4da5328cf90b6d5a4405a466bf041/untitled475.ipynb). Thanks!", "My reply is about the warning you've mentioned :\r\nWARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['sequential_4/custom_embedding_4/e1:0', 'sequential_4/custom_embedding_4/e2:0'] when minimizing the loss.\r\n\r\nWARNING:tensorflow:Gradients do not exist for variables, this warning is often exists in self-defined leyers, because sometimes your self-defined leyers have no connections with output layers.\r\nTo ignore the warning you can add you layers to you model and make sure your layers defined by your own is directly or indirectly to the output.", "tf.concat() doesn't make sense in the warning Gradients do not exist for variables, so if this warning happens, check the layer whether it is connected to the output.", "Please, look at the code. I use a sequential model. The variable is indeed connected to the output. Exactly the same code, but without `tf.contact()`, issues no warning and works all right.", "The problem is still here using TensorFlow `2.3.0` : \r\n\r\n### Reproducible Example\r\n\r\n```python\r\nimport tensorflow as tf\r\nw1 = tf.Variable([[1.0]])\r\nw2 = tf.Variable([[3.0]])\r\nw = tf.concat([w1,w2],0)\r\nx = tf.random.normal((1,2))\r\ny = tf.reduce_sum(x,1)\r\nwith tf.GradientTape() as tape:\r\n    r = tf.matmul(w,x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(tape.gradient(loss, w))\r\n```\r\nOutputs `None`", "Is there any alternative for using concat instead? \r\n\r\nI tried it with the tf.stack command and have the same problem.", "I am also having this problem. I am using Tensorflow 2.3.1. \r\n\r\nAs a test, I also tried making a rank 3 tf.Variable tensor, forming a list by indexing the first axis of the tensor, and then either using tf.stack, tf.concat, or even tf.keras.layers.Concatenate on the list. Finally the loss is just a tf.reduce_sum on the new tensor. The gradients between the loss and the tf.Variable are 'none' in all of these simple test cases, even when the resulting tensor is identical (in shape and in its elements) to the initial tensor.\r\n\r\nIn another test, I tried as the OP mentioned. I created separate tf.Variables, stored them to a list, and then applied either tf.stack, tf.concat, or tf.keras.layers.Concatenate on the list. Forming a loss by tf.reduce_sum and computing gradients returns 'none' here as well.", "@Lescurel \r\n\r\nHaving `with tf.GradientTape() as tape:` before the `tf.concat` call works for me:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nw1 = tf.Variable([[1.0]])\r\nw2 = tf.Variable([[3.0]])\r\nwith tf.GradientTape() as tape:\r\n    w = tf.concat([w1, w2], 0)\r\n    x = tf.random.normal((1, 2))\r\n    y = tf.reduce_sum(x, 1)\r\n    r = tf.matmul(w, x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(tape.gradient(loss, w))\r\n```\r\n\r\nOutput:\r\n```bash\r\n2021-01-27 17:47:45.371562: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-27 17:47:45.371699: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-27 17:47:45.372290: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\ntf.Tensor(\r\n[[0.12809184]\r\n [1.3158427 ]], shape=(2, 1), dtype=float32)\r\n```", "I reproduced @acxz 's result. This is insightful.\r\n\r\nSo tf.concat can only be performed in the tf.GradientTape context in order to maintain gradients? Is this a bug or by design? Can someone please explain why that is the case? How is tf.concat different than other TensorFlow functions in this regard?\r\n\r\nThanks!", "@jvishnuvardhan do you think you can take a quick look at this issue? Seems to be a recurring problem with many people. Just getting some clarification would be nice as well.\r\n\r\nThx.", "I have another minimal example that is similar to this:\r\n\r\n```\r\na = tf.Variable(3.)\r\nb = tf.Variable(2.)\r\n#remove the comment sign # to make it work\r\n#a = tf.Variable([3.])\r\n#b = tf.Variable([2.])\r\nwith tf.GradientTape() as g:\r\n    g.watch(a)\r\n    g.watch(b)\r\n    c = tf.reduce_sum(tf.concat([a , b] , axis = 0))\r\nprint(g.gradient(c , a))\r\n```", "Please advise if this issue has been resolved. I am using 2.4.0 and having the issue of the tape.gradient() returns None with tf.concat().\r\n\r\nTried to workaround by using tf.Variable which has the same result of tf.concat but still tape.gradient() returns None. \r\n```\r\n        _Y = tf.Variable(\r\n            initial_value=tf.zeros(shape=tf.shape(self.Y), dtype=TYPE_NN_FLOAT),\r\n            trainable=True\r\n        )\r\n        # tf.concat([_ye, _ys], axis=1)\r\n        _Y[::, 0:1].assign(_ye)\r\n        _Y[::, 1:].assign(_ys)\r\n```", "I absolutely needed tf.concat to experiment around, so I found this easy workaround:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.random.normal((1, 2))\r\nw1 = tf.Variable([[1.0]])\r\nw2 = tf.Variable([[3.0]])\r\n\r\n# jm-willy's workaround\r\nw = tf.Variable(tf.concat([w1, w2], 0))  # This. Replacing tf.Variable with tf.constant returns None\r\ny = tf.reduce_sum(x, 1)\r\nwith tf.GradientTape() as tape:\r\n    r = tf.matmul(w, x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(w)\r\nprint(tape.gradient(loss, w))\r\nprint('*' * 100)\r\n\r\n# acxz's workaround\r\nwith tf.GradientTape() as tape:\r\n    w = tf.concat([w1, w2], 0)\r\n    y = tf.reduce_sum(x, 1)\r\n    r = tf.matmul(w, x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(w)\r\nprint(tape.gradient(loss, w))\r\nprint('*' * 100)\r\n\r\n# Lescurel's Reproducible Example which returns None\r\nw = tf.concat([w1, w2], 0)\r\ny = tf.reduce_sum(x, 1)\r\nwith tf.GradientTape() as tape:\r\n    r = tf.matmul(w, x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(w)\r\nprint(tape.gradient(loss, w))\r\nprint('*' * 100)\r\n```\r\noutput:\r\n\r\n```\r\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, numpy=\r\narray([[1.],\r\n       [3.]], dtype=float32)>\r\ntf.Tensor(\r\n[[-2.203636 ]\r\n [ 2.7455258]], shape=(2, 1), dtype=float32)\r\n****************************************************************************************************\r\ntf.Tensor(\r\n[[1.]\r\n [3.]], shape=(2, 1), dtype=float32)\r\ntf.Tensor(\r\n[[-2.203636 ]\r\n [ 2.7455258]], shape=(2, 1), dtype=float32)\r\n****************************************************************************************************\r\ntf.Tensor(\r\n[[1.]\r\n [3.]], shape=(2, 1), dtype=float32)\r\nNone\r\n****************************************************************************************************\r\n```\r\n\r\nReplacing `tf.Variable` with `tf.constant` returns None. Seems like `tf.concat` returns a non-trainable object and thus gradient can't flow.\r\n\r\nSince `tf.concat` is a commonly used function, I think the docs should be modified to include a temporal solution until the underlying bug is fixed.\r\n\r\nHow is the Keras LSTM implementation able to work?. Because it uses `tf.keras.concatenate`or `tf.concat` each iteration.\r\n\r\n@acxz @Lescurel ", "`tf.concat` on scalars is not supported, please use `tf.stack` for scalars instead. The fact that calling `tf.concat` on scalars in eager mode does not raise an error is a bug. For example:\r\n\r\n```\r\ndef f(a, b):\r\n  return tf.concat([a , b], axis = 0)\r\n```\r\n\r\n```\r\nf(1., 2.)  # Works but it should not\r\n```\r\n\r\n```\r\ntf.function(f)(1., 2.)  # Does not work\r\n```\r\n\r\nRaises:\r\n\r\n```ValueError: Can't concatenate scalars (use tf.stack instead)```\r\n\r\nI agree we should raise a better error message here instead of silently dropping gradients.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37726\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37726\">No</a>\n", "@saxenasaurabh The code above returns `None` for tensors `w1 = tf.Variable([[1.0], [1.0]])` `w2 = tf.Variable([[3.0], [3.0]])`. Same with scalars`w1 = tf.Variable([[1.0]])` `w2 = tf.Variable([[3.0]])`", "Hi @jm-willy thanks for putting together the workarounds! To make sure I understand this issue correctly, the problem is after tf.concat w1 and w2 got no gradients, right?\r\n\r\nIn \r\n```\r\n# jm-willy's workaround\r\nw = tf.Variable(tf.concat([w1, w2], 0))  # This. Replacing tf.Variable with tf.constant returns None\r\ny = tf.reduce_sum(x, 1)\r\nwith tf.GradientTape() as tape:\r\n    r = tf.matmul(w, x)\r\n    loss = tf.metrics.mse(y, r)\r\nprint(w)\r\nprint(tape.gradient(loss, [w1,w2,w]))\r\nprint('*' * 100)\r\n```\r\nIf I print out the gradients for w1, w2, and w using TF2.5, it shows w1 and w2 still get None as their gradients.\r\n```\r\n[None, None, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\r\narray([[0.6949118],\r\n       [3.9068654]], dtype=float32)>]\r\n``` \r\nwhereas in acxz's workaround w1 and w2 do have gradients:\r\n```\r\n[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6949118]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[3.9068654]], dtype=float32)>, <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\r\narray([[0.6949118],\r\n       [3.9068654]], dtype=float32)>]\r\n```\r\n\r\nMy question is, is there any other workarounds that works without explicitly calling gradient tape? I'm using Keras model API and use fit/evaluate/etc to run the model, hence do not have direct access to gradient tapes...", "Hi, if someone worked out this problem? How to get the gradients if we have lots of tf.concat in our function?\r\n\r\nI'm using an amount of tf.concat in my functions, and to get the gradients I've got InvalideArgumenError every time. It doesn't point out which line/concat is wrong...\r\n\r\n`InvalidArgumentError: Determined shape must either match input shape along split_dim exactly if fully specified, or be less than the size of the input along split_dim if not fully specified.  Got: 2 [Op:SplitV] name: split`\r\n\r\nI've tried to replace all tf.concat -> tf.stack/tf.reshape, then it tunrs out another problem:\r\n\r\n`InvalidArgumentError: Input to reshape is a tensor with 16 values, but the requested shape has 2 [Op:Reshape]`\r\n", "Hi!\r\n\r\nSame issue here... I am trying to re-write MAVNet (https://github.com/sudakshin/imitation_learning/blob/master/2.train_model/MavNet.py) into genuine Tensorflow (instead of TFLearn) and train it. Note that MAVNet consists of numerous tf.concat().\r\nFor some reason, gradient flow seems to break whenever it passes tf.concat(), and the network is not trained at all.\r\n\r\nOn the other hand, when trying to train another neural network that is similar to MAVNet but does not have such tf.concat() functions, the whole network is trained properly.\r\n\r\nThis issue is reproduced in not only TF 2.3, but also in TF 2.8.\r\n\r\nI believe this is a very serious issue since, as far as this issue persists, all neural network models that rely on tf.concat, tf.keras.layers.concatenate, or tf.keras.layers.Concatenate will not be trained properly at all...", "> Hi, if someone worked out this problem? How to get the gradients if we have lots of tf.concat in our function?\r\n> \r\n> I'm using an amount of tf.concat in my functions, and to get the gradients I've got InvalideArgumenError every time. It doesn't point out which line/concat is wrong...\r\n> \r\n> `InvalidArgumentError: Determined shape must either match input shape along split_dim exactly if fully specified, or be less than the size of the input along split_dim if not fully specified. Got: 2 [Op:SplitV] name: split`\r\n> \r\n> I've tried to replace all tf.concat -> tf.stack/tf.reshape, then it tunrs out another problem:\r\n> \r\n> `InvalidArgumentError: Input to reshape is a tensor with 16 values, but the requested shape has 2 [Op:Reshape]`\r\n\r\nI've realized that my problem is not depending on the tf.concat(). I was wrongly given the shape of the gradient when I define the customer gradient.\r\n\r\nPS: In my opinion, because of the invisible of process inside TF, we can not really trace what happened inside TensorFlow and when something was wrong, the Error can not exactly represent the real issue. Please check again your code and the gradients you are manipulating. Make sure what the issue in your case."]}, {"number": 37725, "title": "UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "**System information** \r\n- OS Platform and Distribution: Ubuntu 18.04.\r\n- LInux Kernel: \r\n> 5.3.0-40-generic #32~18.04.1-Ubuntu SMP Mon Feb 3 14:05:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n\r\n- TensorFlow installed from source as written [here](https://www.tensorflow.org/install/source#gpu_support_2) (tried from binary also): \r\n- Build was with all flags OFF except cuda-Y\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.6.9 \r\n- Bazel: 0.29.1\r\n- GCC/Compiler version: 7.5.0 \r\n- CUDA/cuDNN version: - GPU model and memory:\r\nCUDA: 10.1\r\nCuDNN: 7.6.5 ( tried 7.6.1 also)\r\nGPU model: Nvidia Geforce 1660 Ti (6 GB)\r\n------------------------------------------------------------------------------------------------------------------------------\r\n\r\nTried to execute simple MNIST CNN [example](https://keras.io/examples/mnist_cnn/) but with **tensorflow.keras** importing NOT keras!\r\n\r\n**Error**\r\n> UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n> \t [[node sequential/conv2d/Conv2D (defined at <ipython-input-4-288e0dab5d94>:70) ]] [Op:__inference_distributed_function_1004]\r\n> \r\n> Function call stack:\r\n> distributed_function\r\n\r\n------------------------------------------------------------------------------------------------------------------------------\r\nFirstly, I got this error when installed tensorflow-gpu from binary via pip.\r\n\r\nThen, I thought that against CUDA-10.1 I have to manually build tensorflow via bazel but after successful built - **I got the same error**. I have also tried change CuDNN version from 7.6.5 to 7.6.1 but again it did not help. \r\n\r\n**CUDA** downloaded **from nvidia website** and works perfectly\r\n**CuDNN** is rightly installed by downloading it **from nvidia website** and copying into `/usr/loca/cuda-*`\r\n```\r\ncp -P cuda/include/cudnn.h /usr/local/cuda-${CUDA_VERSION}/include\r\ncp -P cuda/lib64/libcudnn* /usr/local/cuda-${CUDA_VERSION}/lib64/\r\nchmod a+r /usr/local/cuda-${CUDA_VERSION}/lib64/libcudnn*\r\n```\r\n\r\nIf I try to train model without **Convolution layers** all works great with GPU calculations:\r\nfor example\r\n```\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n```\r\n\r\nIf you need some additional information, I will add it.\r\n\r\nPlease, help me solve this issue, I truly don't know what else try using. BTW. As I am concerned, I need CUDA-10.1  because only this CUDA supports my Nvidia Geforce 1660 Ti.\r\n", "comments": ["@innjoshka \r\nplease share the tensorflow version on which this issue is faced.\r\n\r\ncould you please refer to existing similar issues as per error message shared:\r\n#34355 #34888 #34518 #28326 #25160  #24828 #33666", "@Saduf2019 added tensorflow version to the top.\r\n\r\nfrom this issue #34888 I got it working\r\n\r\n```\r\n import tensorflow as tf\r\n gpus= tf.config.experimental.list_physical_devices('GPU')\r\n tf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\n\r\n\r\n------------------------------------------------------------------------------------------------------------------\r\n\r\n **set_memory_growth**, Conv2D, NN now trains - **GPU 22s/epoch** vs 30s/epoch CPU\r\nWithout  **set_memory_growth**, Conv2D, NN now trains - error\r\n\r\nBut after **set_memory_growth**, NN now trains, **GPU 22s/epoch** vs 30s/epoch CPU. ( Not the great improvement) And how is that possible that 6GB GPU errors on MNIST dataset with Conv2D layer? \r\n\r\nTested on this architecture\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n```\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------\r\n\r\n**Without** **set_memory_growth**  and without Conv2D layers on MNIST,  trains time on my machine takes **GPU 3s**.\r\n\r\n**With** **set_memory_growth**  and without Conv2D layers on MNIST,  trains time on my machine takes **GPU 5-6s**. \r\n\r\nTested on this architecture\r\n\r\n```\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n```\r\n\r\nIs that normal that difference 22s vs 5-6s? I understand that different architecture but Conv2D and 4 times time difference ", "@innjoshka Can you please check this with `tensorflow 2.2.0-rc1` and let me know if the issue still persists. Thanks!", "@gowthamkpr I will try, Should I build from source or download via pip (as I know `tensorflow 2.2.0-rc1` cannot be downloaded via pip, only build from source, am I right?)\r\nCan you give working versions of packages to successfully build `tensorflow 2.2.0-rc1`:\r\nCUDA version\r\nCuDNN version \r\nBazel version\r\nNvidia driver version (435 is the latest for 10.1)\r\n\r\n", "```\r\nimport tensorflow as tf\r\ngpus= tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n```\r\n\r\nusing this code inside my jupyter notebook worked. \r\n\r\n```\r\ntesorflow version = 2.1.0\r\nDriver Version: 440.64       \r\nCUDA Version: 10.2 \r\ncudnn version: 7.6.5 \r\n```\r\n\r\n\r\n", "Closing this issue as it has been resolved. Please add additional comments for us to open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37725\">No</a>\n", "I had this same issue and any one of this solution helped me, but I got another solution. Apparently some TF.keras libraries were giving me problems instead I imported just keras libraries. \r\nFor example, here is my importing list for one project:\r\nimport sys\r\nimport os\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\nfrom keras.models import Sequential  # Can not be moved\r\nfrom tensorflow.keras.models import load_model\r\nfrom keras.layers import Conv2D, MaxPooling2D  # Can not be moved\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense # Can not be moved\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.preprocessing import image\r\nimport numpy as np\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\nfrom keras import optimizers\r\nimport matplotlib.pyplot as plt\r\n\r\nI am using:\r\nPython : 3.6.8\r\nCuda: 10\r\nCunn: 7.6.0\r\ntensorflow-gpu       2.0.0\r\nKeras                2.3.1\r\n\r\nI spent more than 8 hours trying all the solutions of this chat and just after doing this importing and configuration everything goes well. Actually, really well.\r\n\r\nI believe that I can upgrade to tensorflow 2.1 and the corresponding Cuda and Cunn without any problem.\r\n"]}, {"number": 37724, "title": "[TF 2.2] TFLite Android ARM64 benchmark tool fails when compiling XNNPACK", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.2.0-rc1 and master\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0\r\n\r\n**Describe the problem**\r\n\r\nCompiling the TFLite benchmarking tool for Android ARM64 following the steps described [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#to-buildinstallrun) fails due to NEON float16 errors in XNNPACK with the error message posted below. This happens for both v2.2.0-rc1 and current master, but worked correctly on 2.1.0.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\nHowever `--config=android_arm` compiles without issues.\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: /private/var/tmp/_bazel_lukasgeiger/b38acd93c344ce8dbb5b0de713e0e337/external/XNNPACK/BUILD.bazel:1687:1: C++ compilation of rule '@XNNPACK//:neonfp16arith_ukernels' failed (Exit 1)\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:76:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c0, va0, 0);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:77:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c0, va1, 0);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:78:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c0, va2, 0);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:79:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c0, va3, 0);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:94:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c1, va0, 1);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:95:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c1, va1, 1);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:96:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c1, va2, 1);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:97:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c1, va3, 1);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:112:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c2, va0, 2);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:113:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c2, va1, 2);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:114:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c2, va2, 2);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:115:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c2, va3, 2);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:130:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc0x01234567 = vfmaq_lane_f16(vacc0x01234567, vb01234567c3, va0, 3);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:131:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc1x01234567 = vfmaq_lane_f16(vacc1x01234567, vb01234567c3, va1, 3);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:132:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc2x01234567 = vfmaq_lane_f16(vacc2x01234567, vb01234567c3, va2, 3);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:133:26: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n          vacc3x01234567 = vfmaq_lane_f16(vacc3x01234567, vb01234567c3, va3, 3);\r\n                         ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:157:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n        vacc0x01234567 = vfmaq_f16(vacc0x01234567, va0, vb01234567);\r\n                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:158:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n        vacc1x01234567 = vfmaq_f16(vacc1x01234567, va1, vb01234567);\r\n                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/XNNPACK/src/f16-gemm/gen/4x8-neonfp16arith-ld64.c:159:24: error: assigning to 'float16x8_t' (vector of 8 'float16_t' values) from incompatible type 'int'\r\n        vacc2x01234567 = vfmaq_f16(vacc2x01234567, va2, vb01234567);\r\n                       ^ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nTarget //tensorflow/lite/tools/benchmark:benchmark_model failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 473.607s, Critical Path: 33.08s\r\nINFO: 1229 processes: 1229 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["I was able to compile the model benchmark by upgrading Android NDK to `18.1.5063045`. It would be good if the minimal supported version would be documented somewhere.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37724\">No</a>\n", "This worked thanks.", "Thanks!", "I had this exact same problem, and upgrading to NDK v18 also fixed it for me, thanks Igeiger!"]}, {"number": 37723, "title": "Keras Unable to clone_model a load_model result saved in TF format", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes, see attached example\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): Binary\r\n- TensorFlow version (use command below): 2.0.1\r\n- Python version: 3.6.8\r\n- Bazel\r\nversion (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to clone a loaded model that had been saved in TF format (i.e. a directory rather than `.h5`) creates an error.  This appears to be because the loaded model is of a slightly malformed type.\r\n\r\n**Describe the expected behavior**\r\n\r\n`clone_model` should be able to clone derived types.  `load_model` should return a true model instance even when loading a TF format model.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# make dummy functional model\r\nx = tf.keras.layers.Input(10)\r\ny = tf.keras.layers.Dense(10)(x)\r\n\r\na = tf.keras.models.Model(x,[y])\r\n\r\n# save model with tf format\r\na.save('foo_model')\r\n\r\nprint('Type of a is {0}'.format(type(a)))\r\nprint('a._is_graph_network = {0}'.format(a._is_graph_network))\r\n\r\n\r\n# read in the model\r\nb = tf.keras.models.load_model('foo_model')\r\n\r\nprint('Type of b is {0}'.format(type(b)))\r\nprint('b._is_graph_network = {0}'.format(b._is_graph_network))\r\n\r\n# Try to clone b, errors because b is not of correct type.\r\nc = tf.keras.models.clone_model(b)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nOutput\r\n```\r\nType of a is <class 'tensorflow.python.keras.engine.training.Model'>\r\na._is_graph_network = True\r\nType of b is <class 'tensorflow.python.keras.saving.saved_model.load.Model'>\r\nb._is_graph_network = False\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/proj/keras_load_clone.py in <module>()\r\n     21 \r\n     22 # Try to clone b, errors because b is not of correct type.\r\n---> 23 c = tf.keras.models.clone_model(b)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in clone_model(model, input_tensors, clone_function)\r\n    420   else:\r\n    421     return _clone_functional_model(\r\n--> 422         model, input_tensors=input_tensors, layer_fn=clone_function)\r\n    423 \r\n    424 \r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in _clone_functional_model(model, input_tensors, layer_fn)\r\n    163                      'got a `Sequential` instance instead:', model)\r\n    164   if not model._is_graph_network:\r\n--> 165     raise ValueError('Expected `model` argument '\r\n    166                      'to be a functional `Model` instance, '\r\n    167                      'but got a subclass model instead.')\r\n\r\nValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.\r\n```\r\n\r\nI've tried overwriting the `_is_graph_network` attribute, but that just pushes off the problem to another line.\r\n\r\nThe reason I have to load and then clone is so I can recreate old models with specific datatypes (because TF 2.0.1 can't save mixed_float16 models, I had to save as float32 and fix after loading).  Normally, one would just rebuild the model from the actual commands, but I've made backward-compatibility breaking changes since then.  I'd like to still be able to use my old models.  I'm looking at just updating to TF2.2, but I don't control my environment so that may not be an option.", "comments": ["@dvbuntu,\r\nI was able to reproduce the issue with [TF v2.0.1](https://colab.research.google.com/gist/amahendrakar/f91d8c5a8e92207e3bc4fc44d69d321c/37723.ipynb) and [TF v2.1.0](https://colab.research.google.com/gist/amahendrakar/aded3fa39bcd45c04682257f92fe0023/37723-2-1.ipynb). However, I was able to run the code without any issues with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b25afc6fff1c83162b03bfcb2bada4fb/37723-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Good to hear it's fixed in 2.2.  I'll use this example to push for the upgrade.", "@dvbuntu,\r\nPlease feel free to close the issue if resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37723\">No</a>\n"]}, {"number": 37722, "title": "Fix: Sparce Tensor wrong exeption Message when passing argument with wrong Type", "body": "resolves #37640 ", "comments": ["@rushabh-v Can you please check build failures. Thanks!", "Can you take a look again, please? @gbaned ", "@rushabh-v Can you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@rushabh-v Any update on this PR, please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37721, "title": "Partial shapes for keras.InputLayer with sparse=True ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIt is currently not possible to use input `SparseTensor` into e.g. `keras.Sequential` with partially defined shapes. This is because `SparseTensors` need to be declared specifically, thus necessitating the use of `keras.InputLayer` as the fiirst layer in the model. \r\n\r\nIf the shape of `keras.InputLayer` for sparse inputs is not fully defined (including the batch size), the\r\nproduced `SparseTensor` will have `dense_shape` where only the rank is defined.\r\nThis happens because unless the `dense_shape`is fully defined, a `placeholder` will be used for feeding in  the `dense_shape`, removing the partial shape information.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nThis will make models of the form\r\n```\r\nSequenctial([\r\n   InputLayer(shape=(num_features,), sparse=True), \r\n  Dense(num_units)\r\n])\r\n```\r\nwork, as they require partial shape for the input of the `Dense` layer.\r\n", "comments": ["@ngc92,\r\nSorry for the delayed response. Since it is more than an year since this issue has been raised, can you please refer the [latest documentation of Sparse Tensor](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) and let us know if this **`Feature`** is implemented? Thanks!", "It appears that this works now. It is also shown in the documentation here \r\nhttps://www.tensorflow.org/guide/sparse_tensor#tfkeras", "This seems to be working only for 2d sparse inputs. I have a SparseTensor with partial shape `(batch_size, None, n_features)`, where `batch_size` and `n_features` are fixed. And I create a model as shown here https://www.tensorflow.org/guide/sparse_tensor#tfkeras\r\n\r\nMy code\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.keras.Input(batch_shape=(6,None,4), sparse=True)\r\ny = tf.keras.layers.Dense(4)(x)\r\nmodel = tf.keras.Model(x, y)\r\n\r\na = np.random.random((6,10,4))\r\nsparse_data = tf.sparse.from_dense(a)\r\nmodel(sparse_data)\r\nmodel.summary()\r\nmodel.predict(sparse_data)\r\n```\r\n\r\nand I receive the following error \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\099391\\OneDrive - American Airlines, Inc\\Documents\\Projects\\NP_E-QSI\\srcc\\test.py\", line 5, in <module>\r\n    y = tf.keras.layers.Dense(4)(x)\r\n  File \"C:\\Users\\099391\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"C:\\Users\\099391\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 545, in make_tensor_proto\r\n    raise TypeError(f\"Failed to convert elements of {values} to Tensor. \"\r\nTypeError: Exception encountered when calling layer \"dense\" (type Dense).\r\n\r\nFailed to convert elements of SparseTensor(indices=Tensor(\"Placeholder_1:0\", shape=(None, 3), dtype=int64), values=Tensor(\"Placeholder:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"PlaceholderWithDefault:0\", shape=(3,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\r\n\r\nCall arguments received:\r\n  \u2022 inputs=<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000001F193E519A0>\r\n```"]}, {"number": 37720, "title": "tf.contrib Module not found in tensorflow. Training and graph export not possible.", "body": "Since tuesday  around 9 AM CEST I was not able to proceed training from checkpoint or even export current checkpoint via export_inference_graph.py. Tested on both **TF 1.15 and 2.1.0** and it worked flawlessly up until yesterday. Is there any workaround currently?\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ( Google **Colab**)\r\nMobile device (e.g., Pixel 4, Samsung Galaxy 10) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version (use command below): **1.15** and **2.1.0**\r\nPython version: 3.6.9\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory: depends on which one Colab assigns to me\r\nPlease provide the entire URL of the model you are using?\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config\r\n\r\nDescribe the current behavior\r\nUsing /usr/local/lib/python3.6/dist-packages\r\nFinished processing dependencies for object-detection==0.1\r\nenv: PYTHONPATH=/content/models/research:/content/models/research/slim\r\nTraceback (most recent call last):\r\nFile \"object_detection/builders/model_builder_test.py\", line 23, in\r\nfrom object_detection.builders import model_builder\r\nFile \"/content/models/research/object_detection/builders/model_builder.py\", line 22, in\r\nfrom object_detection.builders import box_predictor_builder\r\nFile \"/content/models/research/object_detection/builders/box_predictor_builder.py\", line 20, in\r\nfrom object_detection.predictors import convolutional_box_predictor\r\nFile \"/content/models/research/object_detection/predictors/convolutional_box_predictor.py\", line 23, in\r\nslim = tf.contrib.slim\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'\r\n\r\nsame with train.py:\r\n\r\nTraceback (most recent call last):\r\nFile \"object_detection/legacy/train.py\", line 48, in\r\nfrom tensorflow.contrib import framework as contrib_framework\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nand model_main.py:\r\nTraceback (most recent call last):\r\nFile \"object_detection/model_main.py\", line 26, in\r\nfrom object_detection import model_lib\r\nFile \"/content/models/research/object_detection/model_lib.py\", line 27, in\r\nfrom object_detection import eval_util\r\nFile \"/content/models/research/object_detection/eval_util.py\", line 40, in\r\nslim = tf.contrib.slim\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'\r\n\r\nand even export_inference_graph.py:\r\n\r\nTraceback (most recent call last):\r\nFile \"object_detection/export_inference_graph.py\", line 108, in\r\nfrom object_detection import exporter\r\nFile \"/content/models/research/object_detection/exporter.py\", line 20, in\r\nfrom tensorflow.contrib.quantize.python import graph_matcher\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\nDescribe the expected behavior\r\n\r\ntest the model builder:\r\n\r\nobject_detection/builders/model_builder_test.py outputs 17 in TF1(10 in TF2) successful tests\r\n\r\nCode to reproduce the issue\r\n\r\n!python object_detection/builders/model_builder_test.py\r\n\r\nor just run **this notebook** in **Colab**:\r\n\r\nhttps://colab.research.google.com/drive/1F4ggfg9_xnCcGSBKb5-ovYi_DQFZp-uV\r\n\r\n** UPD: 1.14 version works partly. Training fails after the first evaluation due to typeerror in numpy with float64 to int conversion.", "comments": ["every time you open a new terminal on a new start you need to add `export PYTHONPATH=$PYTHONPATH:pwd:pwd/slim` to your terminal . If you need more help refer to\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md", "@maxkaustav \r\nI don't run TF locally. It's in Google Colab, similar to Jupyter Notebook.\r\nI did the following in Colab:\r\n\r\n%set_env PYTHONPATH=/content/models/research:/content/models/research/slim\r\n\r\nbut it doesn't solve the issue. The module tf.contrib is still missing.", "@synergy178 \r\nplease refer to[ this issue](https://github.com/tensorflow/tensorflow/issues/36419#issuecomment-581182883) and [this comment](https://github.com/tensorflow/tensorflow/issues/36269#issuecomment-579625089) as per error faced by you and let us know.\r\n\r\n#30794  #35197 #36878 #31350 #", "@Saduf2019 thanks for your reply. As far as I understood TF 2.0+ doesn't have tf.contrib and  TF 1.15 will be supported for the next 3 years. But afterwards there won't be any updates. \r\nBut like I already wrote in the description: the module is **missing** in **TF 1.15**. That's the main problem. Otherwise this issue wouldn't even bother me for the next 3 years.\r\n\r\n``\r\nFinished processing dependencies for object-detection==0.1\r\nenv: PYTHONPATH=/content/models/research:/content/models/research/slim\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/content/models/research/object_detection/builders/model_builder.py\", line 22, in <module>\r\n    from object_detection.builders import box_predictor_builder\r\n  File \"/content/models/research/object_detection/builders/box_predictor_builder.py\", line 20, in <module>\r\n    from object_detection.predictors import convolutional_box_predictor\r\n  File \"/content/models/research/object_detection/predictors/convolutional_box_predictor.py\", line 23, in <module>\r\n    slim = tf.contrib.slim\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'   ``", "@synergy178\r\nplease refer to these [comment](https://github.com/tensorflow/tensorflow/issues/30794#issuecomment-600378740)  as per which [1.14](https://github.com/tensorflow/tensorflow/issues/30794#issuecomment-551022084) works fine", "@synergy178\r\ncan you please use 1.14 or below versions and update us", "With TF 1.14.0 there come a lot of deprecation warnings, but somehow it starts to train.\r\nModel_main.py trains until saving the checkpoint and during the evaluation causes a type error\r\n _object of type <class 'numpy.float64'> cannot be safely interpreted as an integer_.  \r\nI guess it's referring to this [issue] (https://github.com/numpy/numpy/issues/15345).\r\n\r\nconsole output:\r\n`\r\nI0323 13:08:24.638170 139901407041408 session_manager.py:502] Done running local_init_op.\r\nINFO:tensorflow:Performing evaluation on 10 images.\r\nI0323 13:08:40.283396 139898070918912 coco_evaluation.py:205] Performing evaluation on 10 images.\r\ncreating index...\r\nindex created!\r\nINFO:tensorflow:Loading and preparing annotation results...\r\nI0323 13:08:40.284336 139898070918912 coco_tools.py:115] Loading and preparing annotation results...\r\nINFO:tensorflow:DONE (t=0.00s)\r\nI0323 13:08:40.286311 139898070918912 coco_tools.py:137] DONE (t=0.00s)\r\ncreating index...\r\nindex created!\r\n2020-03-23 13:08:40.302812: W tensorflow/core/framework/op_kernel.cc:1490] Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\r\n    num = operator.index(num)\r\n\r\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n    self._metrics = self.evaluate()\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n    iouType=iou_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n    self.params = Params(iouType=iouType) # parameters\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n    self.setDetParams()\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n\r\n  File \"<__array_function__ internals>\", line 6, in linspace\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\r\n    .format(type(num)))\r\n\r\nTypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\r\n  (0) Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n  (1) Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n\t [[IteratorGetNext/_2033]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1411, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1169, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: 2 root error(s) found.\r\n  (0) Out of range: End of sequence\r\n\t [[node IteratorGetNext (defined at object_detection/model_main.py:105) ]]\r\n  (1) Out of range: End of sequence\r\n\t [[node IteratorGetNext (defined at object_detection/model_main.py:105) ]]\r\n\t [[IteratorGetNext/_2033]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'IteratorGetNext':\r\n  File \"object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n    run_metadata=run_metadata))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n    if self._save(run_context.session, global_step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n    if l.after_save(session, step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n    return _evaluate()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\r\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\r\n    self._call_model_fn_eval(input_fn, self.config))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1534, in _call_model_fn_eval\r\n    input_fn, ModeKeys.EVAL)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py\", line 65, in parse_input_fn_result\r\n    result = iterator.get_next()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 426, in get_next\r\n    output_shapes=self._structure._flat_shapes, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1947, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\r\n    num = operator.index(num)\r\n\r\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n    self._metrics = self.evaluate()\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n    iouType=iou_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n    self.params = Params(iouType=iouType) # parameters\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n    self.setDetParams()\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n\r\n  File \"<__array_function__ internals>\", line 6, in linspace\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\r\n    .format(type(num)))\r\n\r\nTypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n\r\n\r\n\t [[{{node PyFunc_3}}]]\r\n\t [[cond_5/Detections_Left_Groundtruth_Right/5/_2891]]\r\n  (1) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\r\n    num = operator.index(num)\r\n\r\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n    self._metrics = self.evaluate()\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n    iouType=iou_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n    self.params = Params(iouType=iouType) # parameters\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n    self.setDetParams()\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n\r\n  File \"<__array_function__ internals>\", line 6, in linspace\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\r\n    .format(type(num)))\r\n\r\nTypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n\r\n\r\n\t [[{{node PyFunc_3}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n    run_metadata=run_metadata))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n    if self._save(run_context.session, global_step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n    if l.after_save(session, step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n    return _evaluate()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 508, in _evaluate\r\n    output_dir=self.eval_dir(name))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1609, in _evaluate_run\r\n    config=self._session_config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py\", line 272, in _evaluate_once\r\n    session.run(eval_ops, feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 854, in __exit__\r\n    self._close_internal(exception_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 887, in _close_internal\r\n    h.end(self._coordinated_creator.tf_sess)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 951, in end\r\n    self._final_ops, feed_dict=self._final_ops_feed_dict)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\r\n    num = operator.index(num)\r\n\r\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n    self._metrics = self.evaluate()\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n    iouType=iou_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n    self.params = Params(iouType=iouType) # parameters\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n    self.setDetParams()\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n\r\n  File \"<__array_function__ internals>\", line 6, in linspace\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\r\n    .format(type(num)))\r\n\r\nTypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n\r\n\r\n\t [[node PyFunc_3 (defined at /content/models/research/object_detection/metrics/coco_evaluation.py:394) ]]\r\n\t [[cond_5/Detections_Left_Groundtruth_Right/5/_2891]]\r\n  (1) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\r\n    num = operator.index(num)\r\n\r\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 209, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\r\n    self._metrics = self.evaluate()\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\r\n    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\r\n\r\n  File \"/content/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\r\n    iouType=iou_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\r\n    self.params = Params(iouType=iouType) # parameters\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\r\n    self.setDetParams()\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\r\n    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\r\n\r\n  File \"<__array_function__ internals>\", line 6, in linspace\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\r\n    .format(type(num)))\r\n\r\nTypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\r\n\r\n\r\n\t [[node PyFunc_3 (defined at /content/models/research/object_detection/metrics/coco_evaluation.py:394) ]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nOriginal stack trace for 'PyFunc_3':\r\n  File \"object_detection/model_main.py\", line 109, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"object_detection/model_main.py\", line 105, in main\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1192, in _train_model_default\r\n    saving_listeners)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1419, in run\r\n    run_metadata=run_metadata))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 594, in after_run\r\n    if self._save(run_context.session, global_step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 619, in _save\r\n    if l.after_save(session, step):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n    return _evaluate()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\r\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\r\n    self._call_model_fn_eval(input_fn, self.config))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1537, in _call_model_fn_eval\r\n    features, labels, ModeKeys.EVAL, config)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1146, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/content/models/research/object_detection/model_lib.py\", line 482, in model_fn\r\n    eval_config, list(category_index.values()), eval_dict)\r\n  File \"/content/models/research/object_detection/eval_util.py\", line 947, in get_eval_metric_ops_for_evaluators\r\n    eval_dict))\r\n  File \"/content/models/research/object_detection/metrics/coco_evaluation.py\", line 394, in get_estimator_eval_metric_ops\r\n    first_value_op = tf.py_func(first_value_func, [], tf.float32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 480, in py_func\r\n    return py_func_common(func, inp, Tout, stateful, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 462, in py_func_common\r\n    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 285, in _internal_py_func\r\n    input=inp, token=token, Tout=Tout, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_script_ops.py\", line 159, in py_func\r\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()`", "@synergy178\r\nplease provide with simple standalone code for us to replicate the issue faced on our local", "https://colab.research.google.com/drive/1F4ggfg9_xnCcGSBKb5-ovYi_DQFZp-uV\r\n\r\nI hope this notebook can help", "![image](https://user-images.githubusercontent.com/61747889/77506072-d32cb780-6e9f-11ea-9dbd-cfd102c6f466.png)\r\nI'm facing the same issue with TF 2.1.0", "> https://colab.research.google.com/drive/1F4ggfg9_xnCcGSBKb5-ovYi_DQFZp-uV\r\n> \r\n> I hope this notebook can help\r\n\r\nTried executing the steps given in this comment. But, facing the same issue. \r\n\r\n------------------------------------------------------\r\nUsing /usr/local/lib/python3.6/dist-packages\r\nFinished processing dependencies for object-detection==0.1\r\nenv: PYTHONPATH=/content/models/research:/content/models/research/slim\r\nTraceback (most recent call last):\r\n  File \"object_detection/builders/model_builder_test.py\", line 23, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"/content/models/research/object_detection/builders/model_builder.py\", line 22, in <module>\r\n    from object_detection.builders import box_predictor_builder\r\n  File \"/content/models/research/object_detection/builders/box_predictor_builder.py\", line 20, in <module>\r\n    from object_detection.predictors import convolutional_box_predictor\r\n  File \"/content/models/research/object_detection/predictors/convolutional_box_predictor.py\", line 23, in <module>\r\n    slim = tf.contrib.slim\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'", "@synergy178\r\nthe error faced is not with tf.contrib please create this issue on stackoverflow.\r\nplease confirm if we may move this to closed status\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37720\">No</a>\n", "I have also faced similar errors with colab and tensorflow 1.15.2\r\nI have solved the problem by manually installing tensorflow 1.15.3 using\r\n`!pip install tensorflow-gpu==1.15.3`"]}, {"number": 37719, "title": "Add -lrt to linkflags", "body": "Fixes compilation on e.g. older CentOS, see for details #15129\r\n\r\n@gunan You said you'd like to review and merge a fix for #15129, so I created one based on a patch used within EasyBuild introduced in https://github.com/easybuilders/easybuild-easyconfigs/pull/6089 but updated to latest TF master and guarded for windows.", "comments": ["Hi.\r\n\r\nThis breaks OSX\r\n\r\n```\r\nERROR: /Volumes/BuildData/tmpfs/tensor_flow/tensorflow/cc/BUILD:507:1: Couldn't build file tensorflow/cc/ops/array_ops_gen_cc: Linking of rule '//tensorflow/cc:ops/array_ops_gen_cc' failed (Exit 1)\r\nld: library not found for -lrt\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nCan you take a look please? We will roll it back for now", "Sorry, I expected something like this to be caught by CI. We sucessfully use the patch in EasyBuild for a while now so I wasn't expecting any issues on Linux-ish systems.\r\n\r\nIs there some way to add this flag only when it is supported? I don't know Bazel besides the obvious changes done here.", "Proposal: Add and use:\r\n\r\n```\r\ndef if_linux(a):\r\n    return select({\r\n        clean_dep(\"//tensorflow:linux\"): a,\r\n        \"//conditions:default\": [],\r\n    })\r\n```\r\n\r\nWould that work?"]}, {"number": 37718, "title": "Draw boundary box after classification", "body": "Hi, \r\nMy problem is: I would to draw a boundary around the object was predicted by a neural network. Here I use a vgg16 model (you can see the code below, it's very simple). But after the last line of code, where I predict the classes of the object. I would to draw a box but I don't how I can do that.\r\nSomeone have a tutorial for me ?\r\nThanks\r\n[pre_entrained_model_vgg_16.py.tar.gz](https://github.com/tensorflow/tensorflow/files/4355415/pre_entrained_model_vgg_16.py.tar.gz)\r\n", "comments": ["try this one\r\n`import tensorflow as tf`\r\n`import matplotlib.pyplot as plt`\r\n`import numpy as np`\r\n`img = tf.zeros([1, 255, 255, 3])`\r\n`img`\r\n`plt.imshow(img[0])`\r\n`box = np.array([0.1, 0.1, 0.78, 0.78\r\n               ])`\r\n`boxes`\r\n`colors = np.array([[0.0, 1.0, 1.0]])`\r\n`img2=tf.image.draw_bounding_boxes(img, boxes, colors)`\r\n`plt.imshow(img2[0])`", "@AlturRang \r\n\r\nThanks for reporting the issue. As per @maxkaustav code please define boxes as well in the code\r\n```\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimg = tf.zeros([1, 255, 255, 3])\r\nimg\r\n#boxes = tf.constant([[[0.05, 0.05, 0.9, 0.7], [0.3, 0.4, 0.5, 0.6]]])\r\nboxes = tf.constant([[[0.05, 0.05, 0.9, 0.7]]])\r\n\r\nplt.imshow(img[0])\r\nbox = np.array([0.1, 0.1, 0.78, 0.78 ])\r\nboxes\r\ncolors = np.array([[0.0, 1.0, 1.0]])\r\nimg2=tf.image.draw_bounding_boxes(img, boxes, colors)\r\nplt.imshow(img2[0])\r\n```\r\nPlease, refer the[ link](https://www.tensorflow.org/api_docs/python/tf/image/draw_bounding_boxes) and see if it helps you. Thanks!", "thank for your quick answer and for the help"]}, {"number": 37717, "title": "Segmentation fault with nightly-gpu docker", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): Nightly-gpu (2.2)\r\n- Python version: - Bazel\r\nversion (if compiling from source): Python 3.6.9\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: - GPU model and memory: RTX 2080 Ti 11GB CUDA 10.1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nSegmentation fault (core dumped) mid-train after calling model.fit()\r\n\r\nI use a callback to check memory and it seems like may be there is a memory leak.\r\n\r\n**Describe the expected behavior**\r\n\r\nNot have this happen\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCannot provide, i have a large dataset (10gb for training inputs).\r\n\r\nI can train fine on my local 1060 6gb card with same amount of CPU memory (32GB). But it seems like this docker image has a problem with a memory leak.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nNo logs, just this error.\r\n", "comments": ["@ben-arnao,\r\nIn order to expedite the trouble-shooting process, we need the reproducible code. \r\nIs it possible to provide the minimal code snippet with a sample dataset. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37717\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37717\">No</a>\n"]}, {"number": 37716, "title": "get_static_value does not support tf.identity", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`get_static_value` currently cannot see through a `tf.identity`op, see the following minimal example\r\n```\r\n>>> @tf.function\r\n... def test():\r\n...     a = tf.constant([5, 4])\r\n...     print(tf.get_static_value(a))\r\n...     b = tf.identity(a)\r\n...     print(tf.get_static_value(b))\r\n...     \r\n>>> test()\r\n[5 4]\r\nNone\r\n```\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nI noticed this problem while investigating the reason for `tf.sparse.retain` losing shape information, even though by definition it should return the same shape as its input. Turns out it constructs a new `SparseTensor` with `dense_shape` given by `tf.identity(input.dense_shape)`. There might be other sparse functions with similar problems, but I have not investigated.\r\n\r\nI think improving `get_static_shape` is preferable to providing better shape info for `sparse.retain`, so for now I am not going to submit a feature request for that.\r\n\r\n**Any Other info.**\r\n", "comments": ["this should be fixed with the merging of #38006"]}, {"number": 37715, "title": "ImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 \r\n- TensorFlow installed from (source or binary):  ?\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source): ?\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: NVIDA Geforce GTX 950, 2Go\r\n\r\n\r\n\r\n**  the problem**\r\nHello I installed tensorflow this morning, but the import does not work. Can anyone help me here? Thx\r\n\r\n```\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\tgalt\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\tgalt\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\r\n    from . _api.v2 import audio\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\tgalt\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\tgalt\\.p2\\pool\\plugins\\org.python.pydev.core_7.5.0.202001101138\\pysrc\\_pydev_bundle\\pydev_import_hook.py\", line 24, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\tgalt\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\tgalt\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\tgalt\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Une routine d\u2019initialisation d\u2019une biblioth\u00e8que de liens dynamiques (DLL) a \u00e9chou\u00e9.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\n\r\nPrevious steps :\r\nI had a problem of dll before this bug so I followed the log instrusction and I installed some stuff to have msvcp140_1.dll \r\nthen i notice I did not have CUDA installed so I installed cuda, then i got this bug.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@Thomas-Galtier \r\nCould please downgrade the cuda version and try, please refer to [this link](https://github.com/tensorflow/tensorflow/issues/36167) to help you resolve the issue, please confirm if it helps.\r\n\r\nbelow are list of similar issues\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204#22823", "I tried cuda 10.0 and 10.1 it did not help.\r\n", "@Thomas-Galtier \r\nDid you install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019?\r\n\r\nAccording to the docs at https://www.tensorflow.org/install/pip,\r\n\r\nInstall the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019. Starting with the TensorFlow 2.1.0 version, the msvcp140_1.dll file is required from this package (which may not be provided from older redistributable packages). The redistributable comes with Visual Studio 2019 but can be installed separately:\r\n\r\nGo to the [Microsoft Visual C++ downloads](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads),\r\nScroll down the page to the Visual Studio 2015, 2017 and 2019 section.\r\nDownload and install the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 for your platform.\r\nMake sure long paths are [enabled on Windows.\r\n](https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing)\r\nInstall the [64-bit Python 3 release](https://www.python.org/downloads/windows/) for Windows (select pip as an optional feature).\r\n\r\nTensorflow 2.1.0 is compiled using MSVC 2019, which appears to require an additional DLL.", "Yes i did. The msvcp140_1.dll is on my computer...", "Is your python 64 bits? Does your CPU support AVX?\r\n\r\nClosing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37715\">No</a>\n", "Sorry, closed by mistake. I see you already installed the latest MSVC.\r\n\r\nSo questions left to triage are: are you using a 64 bits Python? Does your CPU support AVX?", "Yes its a python 64 bits. I guess my CPU support AVX because I ended up creating a linux partition to run tensorflow on a different OS and it worked.\r\n", "@Thomas-Galtier \r\nplease let us know if this is still an issue", "@Thomas-Galtier\r\nplease update as per above comment", "I did not manage to make it work on windows so I switched to linux and it worked", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37715\">No</a>\n", "Is there any update about this issue. I still have it and no able to use tensorflow. Thanks "]}, {"number": 37714, "title": "Tensorflow 2.2.0 does not save metrics using model.compile(metrics=['accuracy'])", "body": "Minimal example for bug:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = tf.keras.Sequential([\r\nlayers.Dense(64, activation='relu', input_shape=(32,)),\r\nlayers.Dense(10)])\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),\r\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nprint(model.metrics)\r\nprint(model.metrics_names)\r\n```\r\n\r\noutput with tensorflow 2.1.0:\r\n[<tensorflow.python.keras.metrics.MeanMetricWrapper object at 0x1497f1400>]\r\n['loss', 'accuracy']\r\n\r\noutput with tensorflow 2.2.0rc0:\r\n[]\r\n[]\r\n", "comments": ["Thank you for opening the issue. I was able to reproduce the error. Take a look at this [gist](https://colab.research.google.com/gist/jaketae/b02028462cf17b8782bb826ae30c6f5d).", "@MalteEbner TF 2.2 changes the code, you need call `fit` or `train_on_batch` first then you can get `model.metrics` or `model.metrics_names` correctly, see [this colab](https://colab.research.google.com/drive/1AunEdHAAUZPbH0q4aGsZQ44hL1WKoiOr)", "@MalteEbner \r\n\r\nIs this still an issue?. Please close this thread if it solves your question. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37714\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37714\">No</a>\n"]}, {"number": 37713, "title": "Problem with documentation tutorial", "body": "Hello tf team. I was trying the embedding tutorial as mentioned on the tf docs webpage.\r\n[https://www.tensorflow.org/tutorials/text/word_embeddings](https://www.tensorflow.org/tutorials/text/word_embeddings)\r\n\r\nThe below two lines do give me error while I am following the same documentation as mentioned in the webpage.\r\n\r\ntrain_batches = train_data.shuffle(1000).padded_batch(10)\r\ntest_batches = test_data.shuffle(1000).padded_batch(10)\r\n\r\n![image](https://user-images.githubusercontent.com/47158509/77058288-3e3b4180-69fb-11ea-932a-df913d43c385.png)\r\n\r\nI have manually tried to fix the error by putting padded_shapes as [None] or [None, None] but both of them have thrown error.\r\n\r\n", "comments": ["- The same does not work in RNN tutorial as well. The API change of padded shape seems to have broken both the tutorials.", "@oke-aditya, try with \r\n`train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None], []))\r\ntest_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None], []))`\r\nI have successfully compiled above mentioned tutorial. For your reference link of gist is [here](https://colab.research.google.com/gist/khimraj/5dce9e670e6502294ab52a911c3bf370/untitled8.ipynb).\r\nHope it helps you.", "Thank you @khimraj  \r\n- Seems to be working fine. \r\n- I think they need this as edit in documentation\r\n", "Maybe we can issue a pull request to fix this up. This  exists in two of the tutorials :smile: ", "- Any explanation why the ` padded shape = ([None], [])` is the parameter.", "@oke-aditya, please refer [this](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/data/ops/dataset_ops.py#L1383-L1481)", "I'll add a note to all of these to clarify.", "- Please fix in [https://www.tensorflow.org/tutorials/text/text_classification_rnn](https://www.tensorflow.org/tutorials/text/text_classification_rnn) as well. Just the simple change should. @khimraj ", "- I get the problem. In tf v2.1 API the argument was mandatory [https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) \r\n\r\n- Whereas in tf v 2.2rc API it is optional. [https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/data/Dataset#padded_batch](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/data/Dataset#padded_batch) This seems to confuse on the documentation page. The tutorial should mention this point. Then it would be clear.\r\n\r\n- Displaying the tensorflow version using `print(tensorflow.__version__)` in tutorial and above comment can make this stuff clear.", "@oke-aditya   that commit of mine (e449e964532654e11dc25ebe75d3e89c05266d51) clarifies this in all the tutorials that use `padded_batch`. This should be published to tensorflow.org within 24h.\r\n\r\nThanks for reporting.", "- Thank you for that fix. Really great work. Crystal clear in docs now :+1: "]}, {"number": 37712, "title": "make command fails due to outdated flatbuffers", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2decf5694a\r\n\r\n**Describe the problem**\r\nWhen running\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile test\r\nthe following error occurs:\r\n\r\n`In file included from ./tensorflow/lite/core/api/op_resolver.h:20,                                                                                                                                                                                                                         \r\n                 from ./tensorflow/lite/micro/micro_interpreter.h:20,                                                                                                                                                                                                                      \r\n                 from tensorflow/lite/micro/examples/person_detection_experimental/person_detection_test.cc:23:                                                                                                                                                                            \r\n./tensorflow/lite/schema/schema_generated.h: In function \u2018const char* tflite::EnumNameTensorType(tflite::TensorType)\u2019:                                                                                                                                                                     \r\n./tensorflow/lite/schema/schema_generated.h:416:20: error: \u2018IsOutRange\u2019 is not a member of \u2018flatbuffers\u2019                                                                                                                                                                                           `\r\n\r\nThis is because IsOutRange is not available in flatbuffer v1.11.0, which is currently downloaded by the make system. If I replace the flatbuffers directory in **tensorflow/lite/micro/tools/make/downloads** with version 1.12.0 of flatbuffer source from [here](https://github.com/google/flatbuffers/releases/tag/v1.12.0), it compiles fine.\r\n\r\nExcept for the test_micro_features_generator_test which fails due to not being able to find kiss_fft.h.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile test\r\n", "comments": ["Solved by 2dc1efeb1ba4b911c053768fa25bdb56932656d2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37712\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37712\">No</a>\n"]}]