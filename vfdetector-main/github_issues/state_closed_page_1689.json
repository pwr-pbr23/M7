[{"number": 2232, "title": "Links for API broken", "body": "On tensorflow website the links for API are broken on [this](https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html) page. \n", "comments": ["@Arafatk I think that is fixed in 0.8. [https://www.tensorflow.org/versions/r0.8/api_docs/cc/index.html](https://www.tensorflow.org/versions/r0.8/api_docs/cc/index.html)\n", "@hunkim  Thanks. \n"]}, {"number": 2231, "title": "add R-CNN support with ROI pooling ?", "body": "Hi, \nI'm interested in implementing a R-CNN or similar object detection/recognition model. there is a theano [port](https://github.com/ddtm/theano-roi-pooling) of Ross. Girshick's implementation in Caffe. \nplease let me know if there is a plan to implement R-CNN / release the ROI pooling layer in Tensorflow?\n\nplease see.\n[Faster RCNN on caffe.](https://github.com/rbgirshick/py-faster-rcnn)\n", "comments": ["Please see #739 and https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/hnbRY-L4RBg\n", "Closing since the conversation is happening over in #739.\n"]}, {"number": 2230, "title": "Merge changes from the last several days", "body": "This was a pretty hard conflict to resolve from internal to external, so we'll see if I did it right.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "Okay all tests passed, so merging (looks like my conflict resolution worked, as far as tests could determine).\n"]}, {"number": 2229, "title": "Check failed", "body": "F tensorflow/core/framework/tensor_shape.cc:211] Check failed: num_elements_ <= kMaxElements (9291474468864 vs. 1099511627776)\nI can't figure out why the above error happens. What's wrong ?\n", "comments": ["There's a hard bound on the number of elements in a Tensor (about a trillion), do you really need to use a 9 trillion element tensor?\n", "I found I have created a big SparseTensor\n"]}, {"number": 2228, "title": "Inconsistence output with resize_images() using different method ", "body": "Extended from [Stackoverflow question](http://stackoverflow.com/questions/37032251/tensorflow-image-resize-mess-up-image-on-unknown-image-size)\n### Environment info\n\nOperating System: Ubuntu 14.04 / OS X Yosemite\n\nInstalled version of CUDA and cuDNN: \nCUDA Version : 7.5\ncuDnn: 4\n\nTensorflow version: 0.8.0\n\nWhen resizing image with variable dimension (output from `decode_jpeg`), image tend to get messed up. However, if `reshape()` is applied prior to `resize()`, image will get resized properly. This issue does not affect `ResizeMethod.Nearest_Neighbour`. \n### Steps to reproduce\n\n```\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfile_contents = tf.read_file('./2008_000002.jpg')\nim = tf.image.decode_jpeg(file_contents)\nim_bi = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BILINEAR)\nim_nn = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\nim_bic = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BICUBIC)\nim_ar = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.AREA)\n\n# im = tf.reshape(im, shape=[256, 256, 3])\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nimg_bi, img_nn, img_bic, img_ar = sess.run([im_bi, im_nn, im_bic, im_ar])\n\nplt.imshow(img_bi)\nplt.title(\"BILINEAR\")\nplt.figure()\n\nplt.imshow(img_nn)\nplt.title(\"NEAREST_NEIGHBOR\")\nplt.figure()\n\nplt.imshow(img_bic)\nplt.title(\"BICUBIC\")\nplt.figure()\n\nplt.imshow(img_ar)\nplt.title('AREA')\nplt.show()\n```\n\n[Original image](http://imgur.com/MskVWIV)\n\nResult\n![nearest Neighbour](https://cloud.githubusercontent.com/assets/650407/15034074/7b973208-12a4-11e6-943e-ea04e1389c01.png)\n![bilinear](https://cloud.githubusercontent.com/assets/650407/15034080/88bfbc84-12a4-11e6-81be-b0fa632d7997.png)\n![bicubic](https://cloud.githubusercontent.com/assets/650407/15034081/88c2a41c-12a4-11e6-9681-f008ec36ff0f.png)\n![area](https://cloud.githubusercontent.com/assets/650407/15034079/88bb4604-12a4-11e6-9ff9-1a53c49a5ab3.png)\n", "comments": ["Can reproduce at head with\n(remove \"from tensorflow.python.ops import image_ops\")\n\n```\nim_bi = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BILINEAR)\nim_nn = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\nim_bic = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BICUBIC)\nim_ar = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.AREA)\n\n```\n", "![bad-sun](https://cloud.githubusercontent.com/assets/23068/15034991/3019a0e0-1231-11e6-9697-7f26bfd28253.jpg)\n", "Closing as duplicate of #1029.  Though this version does have prettier pictures.\n", "Any conclusion on how to fix this problem? I meet the same issue with Tensorflow 1.0.", "@yuezhizizhang \r\nchange your image type, and it will be showed properly. \r\nresize image by  bilinear, bicubic and area will change image data type(from uint8 to float32)\r\nim_bi = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BILINEAR)\r\nim_nn = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\nim_bic = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.BICUBIC)\r\nim_ar = tf.image.resize_images(im, 256, 256, method=tf.image.ResizeMethod.AREA)\r\n\r\nim_bi = tf.cast(im_bi, tf.uint8)\r\nim_bic = tf.cast(im_bic, tf.uint8)\r\nim_ar = tf.cast(im_ar, tf.uint8)\r\n", "@peigolee \r\nExactly.\r\n''\r\nimport numpy as np\r\nplt.imshow(np.uint8(img_bi))\r\n''\r\nResized images are showed properly.\r\nBecause:\r\nthe nn method will not create any new pixel value, type of image/data is unchanged;\r\nthe other three methods will create new values.\r\n"]}, {"number": 2227, "title": "Fix Issue  #2225 InvalidArgumentError: No OpKernel T=DT_INT32 for log, exp, etc.", "body": "This PR fixes  #2225,  InvalidArgumentError: No OpKernel T=DT_INT32 for log, exp, etc.\n## Issue\n\nhttps://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html says, \"A Tensor. Must be one of the following types: float32, float64, int32, complex64, int64\", but code does not:\n\n`REGISTER4(UnaryOp, CPU, \"Cos\", functor::cos, float, Eigen::half, double, complex64);`\n\nThey don't support int32 and int64. So this simple code fails:\n\n```\nx = np.array([1], dtype=np.int32)\nsess = tf.Session()\nprint(sess.run(tf.log(x)))\n```\n## Solution\n\nThanks to a hint from @yaroslavvb, the patch is rather simple:\n\n```\n-REGISTER4(UnaryOp, CPU, \"Log\", functor::log, float, Eigen::half, double,        \n-          complex64);       \n+REGISTER6(UnaryOp, CPU, \"Log\", functor::log, float, Eigen::half, double, int32,\n+         int64, complex64);\n```\n## Tests\n\nI also added several tests to verify the patch. They all pass in my cpu-only machine:\n\n```\n...\n//tensorflow/python:math_grad_test                                       PASSED in 2.8s\n//tensorflow/python:math_ops_test                                        PASSED in 1.5s\n//tensorflow/python:matmul_op_test                                       PASSED in 3.1s\n...\n//tensorflow/python:xent_op_test                                         PASSED in 1.4s\n\nExecuted 186 out of 187 tests: 187 tests pass.\n```\n\nIf I just run math_ops_test without this patch, it fails with InvalidArgumentError: \n\n```\n//tensorflow/python:math_ops_test                                        FAILED in 1.4s\n  /home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/testlogs/tensorflow/python/math_ops_test/test.log\n\nExecuted 186 out of 187 tests: 186 tests pass and 1 fails locally.\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Thanks! Looks good generally, waiting for tests to pass....could you squash the commits?\n", "OK, so apparently this was enabled before and it generated nvcc warnings, because the macro creates a kernel that can automatically downgrade floating point inputs to integers. Also it's a weird kernel to enable because trig functions use floating point math internally anyway, so you don't gain anything (as opposed to, say, defining integer valued matrix multiplication). To deal with user-side crash, the more principled way might be automatic promotion of input arguments (int->float). Sorry this was a bad suggestion on my part :/\n", "@yaroslavvb Thanks for the comments. I'll squash the commits.\n\nI am not sure I understand this correctly:\n\n> To deal with user-side crash, the more principled way might be automatic promotion of input arguments (int->float).\n\nCan you elaborate more? Do you have another fix suggestion?  I am not sure promoting int->float is a good idea. Their max values and precisions are different. \n\nBTW, this fix was inspired by other ops like pow:\n\n```\nREGISTER6(BinaryOp, CPU, \"Pow\", functor::pow, float, Eigen::half, double, int32,\n          int64, complex64);\n```\n\nPlease let me know.\n", "I think promoting int32->float or double will have the same effect as in the current implementation. IE, int32 may have different precision than float, but it doesn't matter if internal implementation converts it to float anyway since output is floating point even if input is integer. The difference with `Pow` function is in that case integer inputs also produce integer outputs\n", "@yaroslavvb I see the point now. Thanks for the explanation. @zheng-xq suggested API doc update. I guess it would be a simple solution and let users promote their int values to float. \n", "Interestingly, I've got another question in my (Korean) community:\n\n```\nInvalidArgumentError: No OpKernel was registered to support Op 'Conv2D' with these attrs\n[[Node: Conv2D_19 = Conv2D[T=DT_DOUBLE, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Conv2D_19/input, Conv2D_19/filter)]]\nCaused by op u'Conv2D_19', defined at: \n```\n\nI guess it's not in the basic math ops, but somewhat related: \"Conv2D[T=DT_DOUBLE...\" I think we need to address this issue, since many espacially, beginners are confused by this error. The error message is also hard to understand. :-)\n", "I think that's a slightly different issue -- someone has data of type `DT_DOUBLE`, but only convolution with `DT_FLOAT` implementation is available, so it fails. The idea is that it's better to fail and request explicit conversion, than silently lose precision.\n\nAnother example of original problem is that the following fails. Automatic type promotion would help with the problem but it needs someone familiar with the internals to implement and I don't think anyone is working/planning to work on it at the moment.\n\n`sess.run(tf.constant(1)+tf.constant(np.array(1)))`\n", "@yaroslavvb I see. Then, just API doc update at this point?\n\n\"A Tensor. Must be one of the following types: float32, float64, ~~int32~~, complex64, ~~int64~~\"\n", "Yes\nOn May 5, 2016 4:42 PM, \"Sung Kim\" notifications@github.com wrote:\n\n> @yaroslavvb https://github.com/yaroslavvb I see. Then, just API doc\n> update at this point?\n> \n> \"A Tensor. Must be one of the following types: float32, float64, int32,\n> complex64, int64\"\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2227#issuecomment-217311256\n", "@yaroslavvb Thanks. Working on it.\n"]}, {"number": 2226, "title": "Evaluated expressions of variables differ sometimes when using the GPU", "body": "The evaluated value of the l1 or l2 loss of variables differ sometimes when using the GPU. This seems to happen when the variables are \"large\".\n\nEven though the difference is not that much in the example below, these differences have resulted in a 98% test accuracy on a network trained on the CPU, but a 10% test accuracy on the same network trained on the GPU.\n### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5 and 7.0 [cuda_info.txt](https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt)\nGPU: Titan X\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\npython -c \"import tensorflow; print(tensorflow.**version**)\"I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n### Steps to reproduce\n\n```\nimport tensorflow as tf\nW1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nW2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\nl2 = tf.reduce_sum(W1 * W1) + tf.reduce_sum(W2 * W2)\nfor i in range(100):\n    print('%.10f' % sess.run(l2))\n```\n", "comments": ["The GPU kernel for reduce_sum is known to be non-deterministic since it\nuses the atomic operations. When the number of elements is large, the\ndifference could be quite large.\n\nIf your model is trained with dropout in it, it tends to be less likely to\nbe affected by the noise.\n\nOtherwise, it is a good idea to have a small example where we can observe\nthe 98% -> 10% difference.\n\nOn Wed, May 4, 2016 at 5:03 PM, alexlee-gk notifications@github.com wrote:\n\n> The evaluated value of the l1 or l2 loss of variables differ sometimes\n> when using the GPU. This seems to happen when the variables are \"large\".\n> \n> Even though the difference is not that much in the example below, these\n> differences have resulted in a 98% test accuracy on a network trained on\n> the CPU, but a 10% test accuracy on the same network trained on the GPU.\n> Environment info\n> \n> Operating System: Ubuntu 14.04\n> \n> Installed version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt\n> https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt\n> GPU: Titan X\n> \n> If installed from binary pip package, provide:\n> 1. Which pip package you installed.\n> pip install --upgrade\n> https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n> 2. The output from python -c \"import tensorflow; print(tensorflow.\n> _version_)\".\n> python -c \"import tensorflow; print(tensorflow._version_)\"I\n> tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcurand.so locally\n> 0.8.0\n> Steps to reproduce\n> \n> import tensorflow as tf\n> W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n> W2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n> sess = tf.InteractiveSession()\n> sess.run(tf.initialize_all_variables())\n> l2 = tf.reduce_sum(W1 \\* W1) + tf.reduce_sum(W2 \\* W2)\n> for i in range(100):\n>     print('%.10f' % sess.run(l2))\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2226\n", "I have condensed my code into a single script that builds a small convnet and trains it for 1000 iterations. The network achieves a test accuracy of 97.7% and 9.8% when I use the CPU and GPU respectively.\n\nI have also attached weights for initializing the network; this particular initialization seems to be particularly bad even though the non-bias weights were initialized using a truncated normal distribution.\n\nAnother thing to note is that this behavior happens with a particular bad choice of hyperparameters (learning rate 0.1 and momentum 0.1).\n\nI'm aware that a lot of things could be better in the script, such as using a lower learning rate, higher momentum, dropout, batch norm, etc. However, I'm interested in this particular setting since this is what we developed for a class that I'm teaching (CS188 at UC Berkeley), and the behavior of our online autograder differs from that of the student's local autograder. Another puzzling thing is that the online autograder is the one getting ~10% accuracy even though it is not using the GPU. This autograder is running Ubuntu 14.04 on aws.\n\n[convnet_determinism_check.zip](https://github.com/tensorflow/tensorflow/files/249904/convnet_determinism_check.zip)\n", "Update: If the network is not initialized with the provided weights, but instead initialized using the truncated normal distribution (for the non-bias parameters), the network achieves around 95% test accuracy, which still seems significant.\n", "@zheng-xq: Any thoughts?  @alexlee-gk: If not, we may have to close this as expected behavior, unfortunately.  It sounds like your 97.7% vs. 9.8% difference occurs for a model where different CPU systems can also produce 10%, and thus that you may have found an impressively unreliable set of hyperparameters.  If using a smaller learning rate helps, it may just mean that the GPU version blows up slightly earlier than the CPU version.\n", "@alexlee-gk, the problem could be attributed to small differences introduced in the non-deterministic behavior on GPU. Or some numerical instability due to the precision difference between CPU and GPU. \n\nThe best way to tell whether this is actually a bug is to narrow them into a small script that shows a big difference in one or two steps. If that is not possible, then the model might be unstable, and this might be an expected behavior. \n", "I think it is reasonable to attribute the problem as an expected behavior, which is caused due to numerical instability.\n\nThat being said, I just want to point out that this behavior is not limited to GPU builds. There were sets of hyperparameters in which I would get high accuracy in Mac OSX (CPU) and Ubuntu 14.04 (CPU), but a lower accuracy in Ubuntu 14.04 running on AWS (CPU).\n", "@alexlee-gk, thanks for bringing this issue to our attention. \n\nClosing as expected behavior. Feel free to reopen if you can reproduce with large difference within one or two steps. \n", "At some point I was debugging tests failing when running with -copt=avx2 instead of avx. We couldn't get rid of the numeric difference and ended up just raising tolerances on tests. What happens is that there's some small operation that produces small difference in the range of ulp or two and then it blows up. In particular, I noticed forward pass during MNIST training to be relatively stable, but during backward pass, tiny difference blow up to billions of ulps\n"]}, {"number": 2225, "title": "InvalidArgumentError: No OpKernel T=DT_INT32 for log, exp, etc.", "body": "### Environment info\n\nOperating System: Mac/Ubuntu\nInstalled version of CUDA and cuDNN: None\ncommit b8883a237b71e877759327fb4b9077847d4cb16c\n### Steps to reproduce\n\nThanks to http://stackoverflow.com/questions/37027762:\n\n```\nimport tensorflow as tf\nimport numpy as np\nx = np.array([1], dtype=np.int32)\nsess = tf.Session()\nprint(sess.run(tf.log(x)))\n```\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nhttps://www.tensorflow.org/versions/r0.8/api_docs/python/math_ops.html#log says \"x: A Tensor. Must be one of the following types: float32, float64, int32, complex64, int64\", but int32 and in64 throws InvalidArgumentError:\n\nInvalidArgumentError: No OpKernel was registered to support Op 'Log' with these attrs\n     [[Node: Log_4 = Log[T=DT_INT32](Log_4/x)]].\n\nI haven't checked all, but `exp`, `cos` and `sin` have the same issue.\n\nI added a simple test:\n\n```\n--- a/tensorflow/python/ops/math_ops_test.py\n+++ b/tensorflow/python/ops/math_ops_test.py\n@@ -28,6 +28,21 @@ from tensorflow.python.platform import googletest\n exp = np.exp\n log = np.log\n\n+class LogTest(test_util.TensorFlowTestCase):\n+\n+  def testLog(self):\n+    x = np.array([1], dtype=np.int32)\n+    with self.test_session():\n+      y_tf = math_ops.log(x).eval()\n+      self.assertEqual(y_tf, 0)\n\n```\n\nwhich clearly reveals this issue:\n\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n-----------------------------------------------------------------------------\n..E..........\n======================================================================\nERROR: testLog (__main__.LogTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/ops/math_ops_test.py\", line 44, in testLog\n    y_tf = math_ops.log(x).eval()\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/framework/ops.py\", line 502, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/framework/ops.py\", line 3352, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/client/session.py\", line 343, in run\n    run_metadata_ptr)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/client/session.py\", line 578, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/client/session.py\", line 651, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/client/session.py\", line 673, in _do_call\n    e.code)\nInvalidArgumentError: No OpKernel was registered to support Op 'Log' with these attrs\n     [[Node: Log = Log[T=DT_INT32, _device=\"/device:CPU:0\"](Log/x)]]\nCaused by op u'Log', defined at:\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/ops/math_ops_test.py\", line 110, in <module>\n    googletest.main()\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/platform/googletest.py\", line 84, in main\n    benchmark.benchmarks_main(true_main=g_main)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/platform/benchmark.py\", line 301, in benchmarks_main\n    true_main()\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/platform/googletest.py\", line 58, in g_main\n    return unittest_main(*args, **kwargs)\n  File \"/usr/lib/python2.7/unittest/main.py\", line 95, in __init__\n    self.runTests()\n  File \"/usr/lib/python2.7/unittest/main.py\", line 232, in runTests\n    self.result = testRunner.run(self.test)\n  File \"/usr/lib/python2.7/unittest/runner.py\", line 151, in run\n    test(result)\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\n    return self.run(*args, **kwds)\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\n    test(result)\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 70, in __call__\n    return self.run(*args, **kwds)\n  File \"/usr/lib/python2.7/unittest/suite.py\", line 108, in run\n    test(result)\n  File \"/usr/lib/python2.7/unittest/case.py\", line 395, in __call__\n    return self.run(*args, **kwds)\n  File \"/usr/lib/python2.7/unittest/case.py\", line 331, in run\n    testMethod()\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/ops/math_ops_test.py\", line 44, in testLog\n    y_tf = math_ops.log(x).eval()\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/ops/gen_math_ops.py\", line 843, in log\n    return _op_def_lib.apply_op(\"Log\", x=x, name=name)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/ops/op_def_library.py\", line 694, in apply_op\n    op_def=op_def)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/framework/ops.py\", line 2153, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/hunkim/.cache/bazel/_bazel_hunkim/6785122ce03f8c29e79889062219c7d3/tensorflow/bazel-out/local-fastbuild/bin/tensorflow/python/math_ops_test.runfiles/tensorflow/python/framework/ops.py\", line 1153, in __init__\n    self._traceback = _extract_stack()\n\n\n----------------------------------------------------------------------\nRan 13 tests in 0.144s\n\nFAILED (errors=1)\n```\n", "comments": ["The kernels were removed as part of the internal clean-up, since there is no good reason to have integer version of those functions. Apparently the doc was not updated.\n\nYou are welcome to make a contribution to update the doc before the TensorFlow team can get to it. \n"]}, {"number": 2224, "title": "Fix links inside distributed/index.md", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "PR merged. Thanks!\n"]}, {"number": 2223, "title": "feed_previous argument for basic_rnn_seq2seq", "body": "Feature request: can the `basic_rnn_seq2seq` function in the seq2seq module implement a `feed_previous` argument, just like the `embedding_rnn_seq2seq` function?\n", "comments": ["@lukaszkaiser: Can you comment?  Should we mark this contributions welcome? \n", "Yes!\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Can we reopen the issue for implementing the 'feed_previous' feature in basic_rnn_seq2seq? This is useful for time series predictions. Otherwise we will need to implement our own seq2seq using lstm from scratch which may not be as optimised I think"]}, {"number": 2222, "title": "Add port mapping in the setup documentation to make sure Jupyter notebook server port is exposed.", "body": "Need to add `-p 8888:8888` to expose the port to host in the Docker documentation.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n\nNeed to pass checks even for documentation change.\n", "Can you also add the flags to the docker_run script? And maybe add a sentence about what it's for?\n", "This PR is for updating documentation.\nWill send another PR to update the script. \n\nWould that be ok?\n", "That would be ok. Can you add a sentence explaining why the -p argument is\nnecessary?\n\nOn Sun, May 8, 2016 at 11:01 AM Henry Saputra notifications@github.com\nwrote:\n\n> This PR is for updating documentation.\n> Will send another PR to update the script.\n> \n> Would that be ok?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2222#issuecomment-217736643\n", "Update PR with description about the Docker -p option.\n", "Thx for merging, @martinwicke \n"]}, {"number": 2221, "title": "API Doc: Moved the usual termination code into a finally block", "body": "Moved the usual termination code into a finally block so that the code can be executed even when SystemExit, KeyboardInterrupt, GeneratorExit raised (These exceptions derive not Exception but BaseException.)\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Test this please, Jenkins.\n"]}, {"number": 2220, "title": "Correct RunConfig example link", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2219, "title": "Can't allocate memory from GPU error in digits.py", "body": "### Environment info\n\nOperating System: 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r--   1 root root    322936 Eyl 19  2015 libcudadevrt.a\nlrwxrwxrwx   1 root root        16 Mar 30 15:25 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx   1 root root        19 Mar 30 15:25 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rw-r--r--   1 root root    383336 Eyl 19  2015 libcudart.so.7.5.18\n-rw-r--r--   1 root root    720192 Eyl 19  2015 libcudart_static.a\nlrwxrwxrwx   1 root root        12 Nis 14 18:53 libcuda.so -> libcuda.so.1\nlrwxrwxrwx   1 root root        17 Nis 14 18:53 libcuda.so.1 -> libcuda.so.361.42\n-rw-r--r--   1 root root  16881416 Mar 23 02:42 libcuda.so.361.42\n-rwxr-xr-x   1 root root  61453024 Nis 30 11:36 libcudnn.so\n-rwxr-xr-x   1 root root  61453024 Nis 30 11:36 libcudnn.so.4\n-rwxr-xr-x   1 root root  61453024 Nis 30 11:36 libcudnn.so.4.0.7\n-rwxr-xr-x   1 root root  59823168 Nis 30 11:12 libcudnn.so.5\n-rwxr-xr-x   1 root root  59823168 Nis 30 11:12 libcudnn.so.5.0.4\n-rw-r--r--   1 root root  62025862 Nis 30 11:36 libcudnn_static.a\n```\n\nInstalled using:\n\n``````\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl```\n``````\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n### Steps to reproduce\n\n```\n(tensorflow)username@pcname:~/Research/ai/tf_examples$ python digits.py \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.291\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 5.53G (5935898624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nStep #100, epoch #8, avg. train loss: 2.79814\nStep #200, epoch #16, avg. train loss: 1.27029\nStep #300, epoch #25, avg. train loss: 0.98960\nStep #400, epoch #33, avg. train loss: 0.84844\nStep #500, epoch #41, avg. train loss: 0.75324\nAccuracy: 0.744444\n```\n\nRunning digits.py throws the \"failed to allocate 5.53G\" (the available memory on GPU is 6GB).  \n### Possible solution\n\nI can restrict the allocated memory using \n\n```\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n```\n\nbut I am wondering if there is any other way to handle this error. \n", "comments": ["Is this error fatal in your log? Depends on which part of TF triggers this error, it might or might not be a problem. \n", "@neurotenguin Are you running skflow digits.py example? Try change the `RunConfig` and pass it to your estimator before training. See example [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/iris_run_config.py).\n", "@zheng-xq Not fatal, but I don't get CUDA_ERROR_OUT_OF_MEMORY error in other tutorials. Need to trace it in the cuda_driver.cc to see the how gpu memory requests are handled.\n\n@terrytangyuan Yep. Thanks for pointing out that example. I thought maybe there was some autoscaling magic going on.\n"]}, {"number": 2218, "title": "Add HDFS support", "body": "Becase of massive data,data is stored in hdfs\nCan tensorflow support to read from hdfs directly?\n", "comments": ["use pyspark and run tensorflow in a reduce operation, or better yet, iterate over the resulting query and pass it into the local model as a placeholder\n", "I'd like to use the second way.\niterate over the resulting query ,\nCan it possbile read minibatch from data files,like caffe,What API should I use?\n", "I believe that spark has a to_iterable operation: \nhttp://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.toLocalIterator\n\npyspark has the tools available to preprocess the data in an RDD before returning it to the current python process, use this command to return a random subset of the current RDD:\nhttp://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sample\n", "Can it possbile to use other ways?No using the spark.\nBecasue i am not familar with spark\n", "I think we can write a custom data reader, that reads data from HDFS, which seems doable: https://www.tensorflow.org/versions/r0.8/how_tos/new_data_formats/index.html\n\nAny plan for this? I might be able to help with implementation.\n", "I'm working on this.  The testing is a little tricky to integrate but it should be done soon :)\n", "Are there any updates on this? Thanks!\n", "@itsmeolivia @phvu Are you still working on this?  We're looking forward to this feature as well.\n", "@alexatknit When you say to use Spark, do you mean to input batches of rdd's as a placeholder? \n", "@tobegit3hub No I am not working on this, as @itsmeolivia said he is almost done.\n", "@itsmeolivia Will Kerberos be supported for the HDFS integration?\n", "See also https://github.com/tensorflow/tensorflow/issues/2655#issuecomment-238602661\n", "HDFS support is in master and will be in the 0.11 release. There'll be a how to on the website that's live with 0.11, but I've copied and pasted it below. If you run into any bugs, file an issue and assign it to me.\n\nI haven't tested Kerberos support. Patches are welcome if any code is needed for that:\n\n> To use HDFS with TensorFlow, change the file paths you use to read and write\n> data to an HDFS path. For example:\n> \n> ``` python\n> filename_queue = tf.train.string_input_producer([\n>     \"hdfs://namenode:8020/path/to/file1.csv\",\n>     \"hdfs://namonode:8020/path/to/file2.csv\",\n> ])\n> ```\n> \n> If you want to use the namenode specified in your HDFS configuration files, then\n> change the file prefix to `hdfs://default/`.\n> \n> When launching your TensorFlow program, the following environment variables must\n> be set:\n> -   **JAVA_HOME**: The location of your Java installation.\n> -   **HADOOP_HDFS_HOME**: The location of your HDFS installation. You can also\n>   set this environment variable by running:\n> \n> ``` shell\n> source $HADOOP_HOME/libexec/hadoop-config.sh\n> ```\n> -   **LD_LIBRARY_PATH**: To include the path to libjvm.so. On Linux:\n> \n> ``` shell\n> export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$JAVA_HOME/jre/lib/amd64/server\n> ```\n> -   **CLASSPATH**: The Hadoop jars must be added prior to running your\n>   TensorFlow program. The CLASSPATH set by\n>   `$HADOOP_HOME/libexec/hadoop-config.sh` is insufficient. Globs must be\n>   expanded as described in the libhdfs documentation:\n> \n> ``` shell\n> CLASSPATH=$($HADOOP_HDFS_HOME/bin/hdfs classpath --glob) python your_script.py\n> ```\n", "If I want to access data in HDFS (remote cluster) that's outside my GPU machine (local), do I still need to install and configure HDFS locally before I can point the file path like:\n\n```\nfilename_queue = tf.train.string_input_producer([\n    \"hdfs://namenode:8020/path/to/file1.csv\",\n    \"hdfs://namonode:8020/path/to/file2.csv\",\n])\n```\n", "You need to install Hadoop locally, but you don't need to configure it. You can just download a binary Hadoop release or use a standard distribution (e.g., Cloudera).\n", "Got you. Thanks a lot! I will try it out.\n", "How to use hive in distributed tensorflow?Please post any example if available.", "@jhseu Hi, I use tensorflow 1.0, and I don't know how to write checkpoint files and event logs files directly to hdfs directory. My current code is like this:\r\n`hdfs_path=\"hdfs://*\" \r\nlocal_path = \"./\" \r\nwith tf.Session(graph=tf.get_default_graph()) as sess: \r\n    W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights') \r\n    b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases') \r\n    init = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()) \r\n    saver = tf.train.Saver() \r\n    sess.run(init) \r\n    summary_writer = tf.summary.FileWriter(hdfs_path,graph_def=sess.graph_def) \r\n    saver.save(sess,save_path=hdfs_path+\"save_net.ckpt\") `\r\nWhat's more, I have got the environment of hdfs already, and I run my python code to write hdfs through subpreprocess.Popen successfully."]}, {"number": 2217, "title": "Wrap cuSOLVER in stream", "body": "I have started to wrap NVIDIA's cusolver library (LAPACK-like) for my work on #367. Is this something the TF team would consider supporting/helping with? I have so far only wrapped potrf, but adding more functions should be fairly easy now that I have done the long-winded bit.\n\n[Its in my fork](https://github.com/c0g/tensorflow).\n", "comments": ["A good way to start with tf.contrib. Once it matures enough the TF team would move it to the core. \n", "Okay, sounds good. Is there an easy way to tell Bazel to build with a contrib/ masking files in Tensorflow proper? Apologies if this is documented somewhere obvious!\n\n> On 4 May 2016, at 18:27, zheng-xq notifications@github.com wrote:\n> \n> A good way to start with tf.contrib. Once it matures enough the TF team would move it to the core.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n", "Here are a few examples in tf.contrib.\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/README.md\n\nOnce you created your own directory, you need to create your own BUILD file to set up your target and dependencies. Then bazel can build it like any other target. \n", "@c0g: Thanks, we look forward to PRs!  @rmlarsen: Do you have any comments?\n", "Hi, sorry my silence in this - I've been ill. I have not been able to puzzle out how to integrate my changes inside of tf.contrib - they're changes to fairly key things like stream.h, the cuda stuff in third party, etc. It seems like most of the other projects inside of contrib are more modular and seem mainly to be discrete ops.\n\nWhat I have been trying to do is make a BUILD file such that I could write:\n\n`bazel build -c opt --config=cuda //tensorflow/contrib/solver_stream:build_pip_package` \n\nand have that build the pip package in exactly the same way as a normal build, but with the files I have in the directory structure masking the original tensorflow files. Unfortunately I have got nowhere. \n\nI can't think of a way to make my changes part of contrib without some mechanism like this - for example, how can I test and install my work without building a pip package?\n", "@c0g, all the stream_executor has to directly go to the stream_executor source. And we will ask the owners from stream_executor to review them. \n\nIn general, once you set up your ops and BUILD file, any project that wants to use your op can depends on it directly, in addition to normal TensorFlow dependency. \n\nIf you want really your op to be available in the default TensorFlow pip_package, once it is mature enough, you can do it to here:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L24\n", "@girving @rmlarsen do the developers at Google have any plans in this specific direction or for GPU linear algebra solvers in general? Having spoken to @c0g  it looks like he will be unable to spend more time on this thread. \n", "I think this would be a great addition to TF if somebody would have the cycles to work on it. I'm not sure the TF team has cycles to work on this in the short term, but would be happy to advice and review PRs.\n\nAs @zheng-xq suggests, perhaps this should live in contrib & stream executor to begin with.\n", "Adding stream-executor owner henline@ here. \n\nOne more thing, we are in the process of getting rid of our forked version of stream-executor. Stream-executor will be officially released as LLVM subproject: parallel-libs. And TensorFlow will eventually switch to that. We don't have a specific timeline yet. But if this development takes a while, it is helpful to pay attention to that transition. \n\nSee this announcement for more details: \n\nhttp://lists.llvm.org/pipermail/llvm-dev/2016-June/101028.html\n", "@henline @zheng-xq .Great. Keen to here of any developments. \n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 2216, "title": "DirectSessionRegistrar not found in libraries", "body": "tensorflow branch r0.7    android-ndk-r10e    bazel-0.2.1\n\nI build the android:tensorflow_demo with the command below:\n\n=> bazel build //tensorflow/examples/android:tensorflow_demo --verbose_failures\n\nThe build is successful, and it generate some files:\n\n=> ??????????????.apk\n=> tensorflow_demo.so\n=> libandroid_tensorflow_lib.a\n=> libre2.a and so on\n\nI want to use Windows Android SDK/NDK to develop Android APP with these static libraries.\n\nWhen I initialize the tensorflow session, it prompt that direct session not found.\n\nThen I browse the code tensorflow\\core\\common_runtime\\direct_session.cc.\nAt the tail, I find:\n\n\"\"\"\nclass DirectSessionFactory : public SessionFactory {\n public:\n  DirectSessionFactory() {}\n\n  Session\\* NewSession(const SessionOptions& options) override {\n    std::vector<Device*> devices;\n    DeviceFactory::AddDevices(options, \"/job:localhost/replica:0/task:0\",\n                              &devices);\n    return new DirectSession(options, new DeviceMgr(devices));\n  }\n};\n\nclass DirectSessionRegistrar {\n public:\n  DirectSessionRegistrar() {\n    SessionFactory::Register(\"DIRECT_SESSION\", new DirectSessionFactory());\n  }\n};\nstatic DirectSessionRegistrar registrar;\n\"\"\"\n\nnm libandroid_tensorflow_lib.a | grep \"???\"\nnm direct_session.o | grep \"???\"\n\nI can grep DirectSessionFactory from libandroid_tensorflow_lib.a/direct_session.o, but not DirectSessionRegistrar.\n\nIt seems that libandroid_tensorflow_lib.a/direct_session.o does not contain DirectSessionRegistrar.\n\nWhat can i do?\n", "comments": ["@mrry, any idea here? \n", "Hi, did you try the alwayslinks=1 and linkopt workarounds suggested in https://github.com/tensorflow/tensorflow/issues/2047?\n\nAnother potential solution is to add an explicit reference to DirectSession in the code to make sure it's not getting stripped out.\n\nIt'd be nice to know the root cause of why this is happening to people occasionally, of course, but I've never been able to reproduce.\n", "Another thing worth checking: how much free disk space and memory does your system have? I've seen cases where Bazel \"forgets\" that it has successfully built entire .so libraries when running low on resources -- I wouldn't be terribly surprised if the same thing happened for individual .o files.\n\nCan you do a bazel clean, then try building with `--jobs=1 -s` and attach the entire build log here please?\n", "I'm not a linker expert, but it looks like the problem might be discussed in this SO question: [Static initialization and destruction of a static library's globals not happening with g++](http://stackoverflow.com/q/1804606/3574081). Perhaps there are some linker options that you need (e.g. `-Wl,--whole-archive` as suggested in [this comment](http://stackoverflow.com/questions/1804606/static-initialization-and-destruction-of-a-static-librarys-globals-not-happenin#comment1692937_1804618))?\n", "A colleague suggests adding the following option to your build command for the code that uses `libandroid_tensorflow_lib.a`: `linkopts=[\"-Wl,-no-as-needed\"]`.\n", "@zheng-xq @andrewharp @mrry \n\nIt my fault, now it's ok.\n\nI develop under Windows, and I use Android.mk to build.\n\nI must use LOCAL_WHOLE_STATIC_LIBRARIES instead of LOCAL_STATIC_LIBRARIES to link these libraries. And it also depend on android-ndk-r10e, just like bazel build.\n\nLOCAL_WHOLE_STATIC_LIBRARIES := \\\n                                android_tensorflow_lib \\\n                                protobuf re2 \\\n                                protobuf_lite \\\n                                protos_all_cc\n\nNow I can develop APP under Windows with Eclipse and Android.mk.\n\nthanks.\n"]}, {"number": 2215, "title": "How to classify an image with the trained cifar10 model?", "body": "After trained a cifar10 model, I want to test the model with a image. There are some other people trying to make it, but it's still a confusing question. My test code is presented, as follows:\n\n```\nfrom datetime import datetime\nimport math\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport cifar10\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string('checkpoint_dir', 'cifar10_train/',\n                           \"\"\"Directory where to read model checkpoints.\"\"\")\n\n\ndef  evaluate_images(images):\n  logit = cifar10.inference(images)\n  load_trained_model(logit)\n\ndef load_trained_model(logits):\n  with tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      # Restores from checkpoint\n      saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n      print('No checkpoint file found')\n      return\n\n    predict = tf.argmax(logits,1)\n    print(predict.eval(), '\\n')\n\ndef img_read(filename):\n  if not tf.gfile.Exists(filename):\n    tf.logging.fatal('File does not exists %s', filename)\n  image_data = tf.gfile.FastGFile(filename, 'rb').read()\n  image = tf.cast(image_data, tf.float32)\n  return image\n\nfilename = '1.png'\nimages = img_read(filename)\nevaluate_images(images)\n```\n\nBut it occurs the bug: \n\n```\nFile \"/home/swoda/tensorflow/tensorflow/models/image/cifar10/cifar10.py\", line 192, in inference\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2156, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1612, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/common_shapes.py\", line 202, in conv2d_shape\n    input_shape = op.inputs[0].get_shape().with_rank(4)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 625, in with_rank\n    raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\nValueError: Shape () must have rank 4\n```\n\nIf you know how to fix this problem, please tell me. Thank you very much!\n", "comments": ["The network expects a batch of images, so add `tf.expand_dims(image, -1)`.\n", "Did the `tf.expand_dims(image, -1)` ever solve the problem? I tried that workaround writing `image = tf.expand_dims(tf.cast(image_data, tf.float32), -1)`, but now the error is:\n\n> File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 625, in with_rank\n>     raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\n> ValueError: Shape (1,) must have rank 4\n", "This may work.\r\nimg = tf.reshape(img, (1,32,32,3))\r\n", "use tf.train.shuffle_batch() to change image into [1,size,size,3] and change batch_size to 1 can solve this bug", "@yhlleo  Have fixed the bug? How to run this case, as if it has not a main function. By the way, do you have the complete code?"]}, {"number": 2214, "title": "Fix init value in inner_product", "body": "inner_product accumulates into the type of the initial value\nmeaning that the result actually has type int instead of int64.\nThis will lead to truncation errors resulting in invalid writes\non buffers larger than 4GB.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins : test this please.\n", "Test this please, Jenkins.\n", "@tensorflow-jenkins, test this please.\n"]}, {"number": 2213, "title": "Add support for 3d convolutions", "body": "This patch is a WIP, but adds functionality for 3d convolutions as requested in #150.\n### Approach\n\nThis patch adds 3d kernels and glue code, largely based off the 2d kernels, as separate modules. We add `conv_3d`, `conv3d_ops`, `conv3d_grad_ops`, and others to the original `conv_2d`, `conv_ops`, and `conv_grad_ops`, which remain unchanged.\n\nAs there is logic in the 2d kernels which the 3d kernel simply replicates, this patch is quite large. Generalizing the conv operator to n dimensions might solve 1d and 4d use cases (#1661 and #1136) as well as reducing the overall quantity of code in the codebase.  As this patch includes a comprehensive test suite, this patch should help if or when that happens. \n### Progress\n- [x] Test suite for GPU/CPU\n- [x] CPU kernel implementation\n- [ ] GPU kernel implementation \n\nCurrently, there are ~40 tests in `tensorflow/python/kernel_tests/conv3d_ops_test.py` which test shapes, outputs, strides, and gradients for a variety of input. They pass on my CPU instance: \n\n![image](https://cloud.githubusercontent.com/assets/3162580/15002320/d0bdaec0-1154-11e6-8e44-f9082e3108c6.png)\n\nI currently lack access to a GPU so I haven't worked on the GPU kernel yet, but hopefully that should happen soon.\n### Todo\n\nThis is a learning project for me, so I'd love to get feedback on how this might become more helpful, or how it might fit in better with the TF team's architectural design.  Thanks!\n", "comments": ["Can one of the admins verify this patch?\n", "A 3D convolution is almost done internally. I'll add the internal author to comment on the progress. \n", "Yeah it's unfortunate that you went through all this effort since it's probably arriving with the next push (including GPU support).  I would have suggested mentioning you were working on it earlier! :(\n", "Also, our fault for not indicating on that bug that it was being worked on internally. It came together pretty quickly in the last week or so and we forgot to update this\n", "Oh bummer! Well was still valuable to learn about it -- wasn't so much work. What about 1D and 4D?\n", "Sure!  conv1d and conv4d would be nice!  Once the conv3d implementation is available you can generally see how to do it for both CPU and GPU -- there's opportunities for lots of code sharing across all of the ops then.\n", "3d convolutions added in https://github.com/tensorflow/tensorflow/commit/6a187ccddaebb741ea77fc3201c6e36625f0aadb, you can take a look to see how it was done and maybe look for opportunities for code sharing with something like conv1d if you think it's worth doing!\n", "1d convolution would be good to have. Also I wonder if anyone would be interested in having different boundary conditions in all types of convolutions. for instance I needed to have periodic boundary conditions where the pixels on the edges are connected to pixels on opposite edges, and so on. I had to modify my training/test sets in order to account for that.\n", "Does GPU kernel of 3d convolutions have been implemented?\n", "@zhongying811, yes. It is tf.nn.conv3d\n", "Who has some 3d convolution(3DCNN) example codes on GPU.\nI'm a beginner. Thanks.\n", "@zhongying811 The code on GPU is the same as on CPU, where it runs only depends on the TensorFlow version installed.\n\nI don't have a simple example, but basically, you can follow the [tutorial for deep MNIST](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts) for 2D data and go to 3D data, and replace\n\n```\ntf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n```\n\nwith\n\n```\ntf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n```\n\nand\n\n```\ntf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n```\n\nwith\n\n```\ntf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n```\n\nAnd all shapes by addinge one dimension. Otherwise, it is really quite similar.\n\nIf you are a beginner, here are two good online courses about machine learning in general:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://www.udacity.com/course/machine-learning-engineer-nanodegree-by-google--nd009\n\nI suggest that you ask any other questions on [StackOverflow](http://stackoverflow.com/) (using the tensorflow tag).\n", "@akors Thanks a lot. And the online courses are just what I need.\n", "Hi,I'd like to get the demo of conv3d of tensorflow. But I can't find it  and I hope I can get one. Is anyone  familiar with it.  \n", "@wolfbill \n\nHere's an example of conv3d usage:\n\n<script src=\"https://gist.github.com/akors/52bba57f724e38229da9d85fbc85b673.js\"></script>\n\nhttps://gist.github.com/akors/52bba57f724e38229da9d85fbc85b673\n", "Who has some 3d convolution or 2d convolution example codes with C++ interface.\n", "@akors Hi man. How are you doing ? That's really happy to get your message. We can't  access gist.github.com in mainland.We can only access github.com\u3002\uff23ould you place your example on github\uff1f\n", "\uff20akors I get the file from url. Thank you.\n", "Hi all,\r\n\r\nRequest for a feature: Is there a function called as separable_conv3d? \r\n\r\nI think it is more important in the case of 3D convolutions to be separable. (because it takes lot of memory otherwise) \r\n\r\nCould anyone please let me know if it exists?", "Hi all\r\nI am trying to produce a batch for 3DCNN , then i try this code:\r\n\r\nimage_batch, label_batch = tf.train.batch([image, label],\r\n                                          batch_size = 50,\r\n                                          shapes = [[12,250,115,3], label.get_shape()])\r\n\r\nraise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\nValueError: Shapes (?, ?, 3) and (12, 250, 115, 3) are incompatible\r\n\r\nThen i got this error, have you seen such error?\r\nSo is there sample for image data?"]}, {"number": 2212, "title": "installation issue on Mac OS", "body": "Hello,\n\nI am following the installation guide here (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md), and here is the message I met with. I want to install on Mac OSX. Thanks.\n\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\nThe directory '/Users/foo/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nThe directory '/Users/foo/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\nCollecting tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl (19.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19.3MB 65kB/s\nRequirement already up-to-date: six>=1.10.0 in /Library/Python/2.7/site-packages/six-1.10.0-py2.7.egg (from tensorflow==0.8.0)\nCollecting protobuf==3.0.0b2 (from tensorflow==0.8.0)\n  Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 327kB 2.4MB/s\nCollecting wheel (from tensorflow==0.8.0)\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71kB 9.5MB/s\nCollecting numpy>=1.10.1 (from tensorflow==0.8.0)\n  Downloading numpy-1.11.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.9MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.9MB 322kB/s\nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.8.0)\n  Downloading setuptools-21.0.0-py2.py3-none-any.whl (509kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 512kB 1.9MB/s\nInstalling collected packages: setuptools, protobuf, wheel, numpy, tensorflow\n  Found existing installation: setuptools 1.1.6\n    Uninstalling setuptools-1.1.6:\nException:\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/basecommand.py\", line 209, in main\n    status = self.run(options, args)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/commands/install.py\", line 317, in run\n    prefix=options.prefix_path,\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/req/req_set.py\", line 726, in install\n    requirement.uninstall(auto_confirm=True)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/req/req_install.py\", line 746, in uninstall\n    paths_to_remove.remove(auto_confirm)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/req/req_uninstall.py\", line 115, in remove\n    renames(path, new_path)\n  File \"/Library/Python/2.7/site-packages/pip-8.1.1-py2.7.egg/pip/utils/**init**.py\", line 267, in renames\n    shutil.move(old, new)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 299, in move\n    copytree(src, real_dst, symlinks=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/shutil.py\", line 208, in copytree\n    raise Error, errors\nError: [('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py', \"[Errno 1] Operation not permitted: '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/__init__.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py', \"[Errno 1] Operation not permitted: '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.py'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc', \"[Errno 1] Operation not permitted: '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib/markers.pyc'\"), ('/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib', \"[Errno 1] Operation not permitted: '/tmp/pip-a1DXRT-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/_markerlib'\")]\n", "comments": ["This is a pip problem.  Running \n`sudo pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl` works for me.  You can read more [here](https://github.com/pypa/pip/issues/3165).\n", "Hi Olivia, still has issue, here are more detailed information, appreciate if you could advise. Thanks.\n\n$ python\nPython 2.7.11 |Continuum Analytics, Inc.| (default, Dec  6 2015, 18:57:58)\n[GCC 4.2.1 (Apple Inc. build 5577)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"tensorflow/python/**init**.py\", line 49, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow\n> > > ImportError: cannot import name pywrap_tensorflow\n"]}, {"number": 2211, "title": "tf.add_check_numerics_ops() adds ops to while_loop frame", "body": "When I construct a graph that contains a tf.while_loop, and then try to call tf.add_check_numerics_ops(), I get an InvalidArgumentError.\n\nGiven the code\n\n``` python\nimport tensorflow as tf\n\ndef test():\n    i = tf.constant(0, tf.float32)\n    c = lambda i: tf.less(i, 10)\n    b = lambda i: tf.add(i, 1)\n    r = tf.while_loop(c, b, [i])\n\n    check = tf.add_check_numerics_ops()\n\n    with tf.Session() as sess:\n        print sess.run([r,check])\n```\n\nI get the error\n\n``` python\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"minimal_bug_example.py\", line 12, in test\n    print sess.run([r,check])\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: All inputs to node CheckNumerics_1 must be from the same frame.\n```\n\nI'm running Tensorflow installed from pip, version 0.8.0. As far as I can tell, this is caused by the while_loop creating a new control flow context, but the check numerics op living outside that context.\n", "comments": [" One potential dirty fix I figured out is to change [this line](https://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/python/ops/numerics.py#L65) to\n\n``` python\nif output.dtype in [dtypes.float32, dtypes.float64] and output.op._get_control_flow_context() == ops.get_default_graph()._get_control_flow_context():\n```\n\nwhich makes sure to only add check_numerics ops on ops in the outermost context. There are probably better ways to do it, and I haven't looked to closely at the code, but this worked as a temporary workaround for me.\n", "I would not use tf.add_check_numerics_ops in this case, since it doesn't know what are the hidden nodes that shouldn't be checked. \n\nAdd tf.check_numerics to the node that you want to check it yourself. \n", "I was attempting to use add_check_numerics_ops for debugging purposes. One of the models I was building was spitting out NaNs and I wanted to figure out where they were coming from. Ideally I'd like it to check within the while loop too, but that doesn't seem to work.\n", "You should be able to use tf.check_numerics within the loop. It is like an\nidentity op.\n\nOn Tue, May 3, 2016 at 4:13 PM, Daniel Johnson notifications@github.com\nwrote:\n\n> I was attempting to use add_check_numerics_ops for debugging purposes. One\n> of the models I was building was spitting out NaNs and I wanted to figure\n> out where they were coming from. Ideally I'd like it to check within the\n> while loop too, but that doesn't seem to work.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2211#issuecomment-216692313\n", "I am also getting an error message similar to this. But I am not able to understand exactly what does it mean\n", "@yuanbyu: Should I mark this contributions welcome?  It seems like it would be nice if `tf.add_check_numerics_ops` just did the right thing in this case. \n", "Yes, sounds like a good idea.\n", "I'm getting this error as well, and I'm not explicitly using a while_loop, although my model does have an RNN in it which may explain this.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I think this should be reopened. `tf.add_check_numerics_ops()` currently is useless when there is a loop or other custom context in the computation graph."]}, {"number": 2210, "title": "Sharing a GPU between tensorflow and another cuda code.", "body": "Hello,\n\nI am trying to run tensorflow and another cuda code in the same python script. My script is coded such that it iteratively uses the other cuda code to generate bunch of samples and then it uses tensorflow to process them. The tensoflow session is always open while doing these iterations.\n\nThe problem is that in tensorflow 0.7, I get the following error when the session is closing (exiting python):\n\nF tensorflow/stream_executor/cuda/cuda_driver.cc:302] current context was not created by the StreamExecutor cuda_driver API: 0x2eb5c50; a CUDA runtime call was likely performed without using a StreamExecutor context\n\nAnd, in tensorflow 0.8, I get another error in middle of run:\n\n**\\* Error in `python': munmap_chunk(): invalid pointer: 0x00007ffea63b7380 ***\nAborted (core dumped)\n\nI have modified my other cuda code to use cuda's Stream Executor context but this did not solve any of these issues. I'm also pretty sure my cuda code does not have any memory leak. If I replace my cuda code with its cpu version or the tensorflow operations with numpy counterparts every thing works perfectly. I have also limited gpu memory for TF using: \n\ntf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n\nI am wondering if we actually can share a single GPU between tenorflow and another cuda code in the same script? If so, is there any particular protocol that I should follow.\n", "comments": ["Off the top of my head, you may need to explicitly create, push and pop a CUDA context for your own code. Otherwise, you may end up using Tensorflow's context with it's own streams etc.\n", "Adding @zheng-xq.  As noted in https://github.com/tensorflow/tensorflow/issues/916, sharing a GPU between TensorFlow and other code is not generally supported.  However, you mentioned that you modified your other code to use stream executor, which seems like it has a chance.  Are you saying you got the same error after switching to stream executor, or did the error change?\n", "@sjperkins is correct. If you want to share the same GPU between TensorFlow and other Cuda library, you have to be very careful. No Cuda Runtime API is safe. Either you call stream-executor, or call Cuda Driver API, and manage the context yourself. Every time when you are done with your Cuda work, restore the context back to the previous state, because that is what stream-executor implicitly expects. \n", "Closing this issue as it is as supporting shared use of GPU is not currently or planned to be supported.\n", "I know this is closed, but just in case anybody needs a tip:\r\n\r\nOne possible workaround is to launch the Tensorflow code in a separate worker thread.  This isolates Tensorflow's CUDA context and allows it to coexist peacefully with Caffe (and other CUDA-enabled libraries).  \r\n\r\nI'm using python's `threading` and `Queue` libraries for this, and no complaints (although they're still battling for memory, but that's a different story)."]}, {"number": 2209, "title": "Word2vec overflows int32 corpus_size_", "body": "Hello, I'm trying to build a big word model, and I'm getting overflows in the corpus_size_ declared here\nhttps://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/models/embedding/word2vec_kernels.cc#L119\n\nThe error is being thrown here:\n\nhttps://github.com/tensorflow/tensorflow/blob/e39d8feebb9666a331345cd8d960f5ade4652bba/tensorflow/models/embedding/word2vec_kernels.cc#L191\n\nBecause negative numbers are less than positive numbers. I'm going to change them to uint64 in my own build, if it doesn't cause problems should I make a PR?\n", "comments": ["@snakecharmer1024, int64 is better than uint64 for size. \n\nAdding @zffchen78 to comment whether it is okay to int64. It is possible that you need to change several places to make this work. \n", "@zheng-xq I don't understand these things that well, why would you prefer signed to unsigned?\n", "In general, TensorFlow prefers int64 when large indices or size are needed. One reason is that the following code would work naturally. \n\nfor (auto i = v_size - 1; i>=0; i--) {...}\n", "Thank you for offering to create a pr. Unfortunately, changing from signed to unsigned often creates lots of subtle bugs (especially in the case @zheng-xq suggest). So for now I am closing this bug. Hopefully using int64 when you have more than 2^31 positive is a workable solution for you.\n"]}, {"number": 2207, "title": "Feature request: Implement SVD", "body": "Feature request: Implement SVD\n\nLike for example in Torch:\nhttps://github.com/torch/torch7/blob/master/doc/maths.md#torchsvdresu-ress-resv-a--s-or-a\n", "comments": ["@rmlarsen, I'm assigning this to you because you mentioned there was a plan to add SVD this quarter. Feel free to reassign if someone else is doing the work....\n", "Hi, Just saw this after returning from vacation: Yes, I do intend to add this.\n", "I am working on this, too. What algorithm are you planning to use?\n", "I plan to add an op for dense, since-machine SVD using the divide-and-conquer in Eigen. For sparse it would be nice to implement something like the Lanczos algorithm in PROPACK (which I'm the author of), but I have no immediate plans to work on that.\n", "This is a feature I would like to see, which would be necessary for implementing tensor networks as in http://arxiv.org/abs/1605.05775 in Tensorflow. \n", "Ops for computing the SVD of dense matrices or batches of matrices have been added: \n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L479\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_ops.py#L522\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/svd_op.cc\n", "I am comparing this with PCA implementation from scikit learn and the results don't seem to be consistent. Here is the results and code for a toy example. Can you please comment why the results might be different?\n\n`# USING TENSORFLOW\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nx_tf = tf.Variable([[0.0, 1.0, 0.0, 7.0, 0.0], [2.0, 0.0, 3.0, 4.0, 5.0], [4.0, 0.0, 0.0, 6.0, 7.0]])\nx_tf.initializer.run()\ns, u, v = tf.svd(x_tf, full_matrices = False)\nu.eval()\n\n# output: array([[-0.38829127, -0.91980982, -0.05638742],\n\n# [-0.53017205,  0.27301854, -0.80273181],\n\n# [-0.75375545,  0.28179872,  0.5936681 ]], dtype=float32)\n\n# USING SCIKIT LEARN\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3, whiten = False)\nx = np.array([[0.0, 1.0, 0.0, 7.0, 0.0], \n              [2.0, 0.0, 3.0, 4.0, 5.0],\n              [4.0, 0.0, 0.0, 6.0, 7.0]])\npca.fit(x)\nprint pca.components_.transpose()\n\n# output: [[ 0.44859172  0.28423808 -0.706726  ]\n\n# [-0.13301986  0.05621156 -0.34735227]\n\n# [ 0.12523156 -0.76362648 -0.44948593]\n\n# [-0.21650757  0.56529588 -0.281509  ]\n\n# [ 0.84765129  0.11560341  0.31400611]]`\n\nEDIT ##########################################################\nAlright, I fixed this by subtracting mean:\n\n`x = tf.Variable([[0.0, 1.0, 0.0, 7.0, 0.0], [2.0, 0.0, 3.0, 4.0, 5.0], [4.0, 0.0, 0.0, 6.0, 7.0]]);\n x_tf = tf.sub(x, tf.reduce_mean(x, 0))`\n\nI hope this will be helpful for those who want to apply PCA.\n"]}, {"number": 2206, "title": "[WIP] Initial proof-of-concept NodeJS binding.", "body": "Retrieves the tensorflow versions string.\n\nTest in the contrib/nodejs directory with:\n\n```\nnpm run generate && npm install && npm test\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@ry would you be open to reviewing this? \n", "@danmane yea sure. I'll check it out tomorrow\n", "Thanks!\n\nOn Tue, May 3, 2016 at 4:10 PM ry notifications@github.com wrote:\n\n> @danmane https://github.com/danmane yea sure. I'll check it out tomorrow\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-216691907\n", "True progress! thanks @peterbraden\n", "I'm working on a further test, which will actually call some tensorflow code, but I have a bunch of outstanding problems, which I'm documenting in this gist: https://gist.github.com/peterbraden/1f6e52045c27cfcb8a5c13b1ef82c94f - not sure the best way to collaborate on this, but if anyone has any suggestions or workarounds, please let me know.\n", "@danmane @peterbraden it looks fine to me as a proof-of-concept... I was able to build an run the test on ubuntu. This might be better as a separate project outside the TF tree? Also I'd have some style nitpicks like in `test/suite.js`.\n", "@ry Thanks for the review, I did this as part of contrib as a response to this comment: https://github.com/tensorflow/tensorflow/issues/37#issuecomment-216272659\n\nWould love to hear your style nitpicks too.\n\nI'm continuing work in this branch: https://github.com/tensorflow/tensorflow/compare/master...peterbraden:nodejs-wip - I think that until I have a useful subset of functionality, this isn't ready for merging.\n\nWould welcome all feedback or collaboration though. I think we need to wait for this too: https://github.com/nodejs/node-gyp/pull/919\n", "@martinwicke https://www.npmjs.com/package/standard this has evolved as a standard in the node community, powerful linter and node js focused static analysis\n", "@danmane how different is this from google style? I'm happy to use a standard if there is one.\n", "I'm not so interested in specific code lint styles, I can defer to people there. \n\nI've been working in this branch https://github.com/tensorflow/tensorflow/compare/master...peterbraden:nodejs-wip - I'm not sure the best way to have transparency here, should I add to this pull-request, even though it's not ready to be merged, and have a long-running PR, or should I wait until I have something more complete? I don't have a huge amount of time to work on this, and would love to attract some collaborators.\n", "You can make a PR, add [WIP] to the title and reference this thread in the\ndescription.\nOn Thu, May 19, 2016 at 00:00 Peter Braden notifications@github.com wrote:\n\n> I'm not so interested in specific code lint styles, I can defer to people\n> there.\n> \n> I've been working in this branch master...peterbraden:nodejs-wip\n> https://github.com/tensorflow/tensorflow/compare/master...peterbraden:nodejs-wip\n> - I'm not sure the best way to have transparency here, should I add to this\n>   pull-request, even though it's not ready to be merged, and have a\n>   long-running PR, or should I wait until I have something more complete? I\n>   don't have a huge amount of time to work on this, and would love to attract\n>   some collaborators.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-220243286\n", "@martinwicke @billautomata\nThere are a few notable diffs, e.g. [googlejs](https://google.github.io/styleguide/javascriptguide.xml) calls for semicolons always, standard calls for no semicolons at all. But given that node is basically unused within Google, I see no benefit in tying the node frontend to Google's style guide. And it looks like standard has pretty good adoption based on the download stats.  So let's go with package/standard.\n", "Wow. No semicolons ever? That's insane. I would have expected semicolons to\nbe part of any style guide for JS given how brittle JS is around line\nbreaks.\n\nAnyway. package/standard it is.\nOn Thu, May 19, 2016 at 10:56 Daniel W Mane notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke @billautomata\n> https://github.com/billautomata\n> There are a few notable diffs, e.g. googlejs\n> https://google.github.io/styleguide/javascriptguide.xml calls for\n> semicolons always, standard calls for no semicolons at all. But given that\n> node is basically unused within Google, I see no benefit in tying the node\n> frontend to Google's style guide. And it looks like standard has pretty\n> good adoption based on the download stats. So let's go with\n> package/standard.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-220402876\n", "Node itself uses a style similar (same?) to Google's JS style... package/standard seems rather non-standard despite its name.\nhttps://github.com/nodejs/node/blob/395cc885f4eb21c2002dbfe405d31da275699aa8/lib/path.js\n", "I like the approach of http://standardjs.com/ + eslint to pass for development handy rules as only warnings ... e.g. debug. maybe is handy to webpack for different stages?\n", "Any progress on this?\n", "Hey\nUsing google's eslint config is pretty easy and provides good defaults.\nIt's also easy to override rules from the base.\nQuick start - ready for copy-paste in terminal:\n\n```\nnpm install --save eslint eslint-config-google\ncat << EOF >.eslintrc.js\nmodule.exports = {\n\n    // extending google's eslint config as base\n    extends: 'google',\n\n    // overriding rules from base\n    rules: {\n    }\n};\nEOF\n```\n", "@peterbraden what should the next steps be with this PR? Is this in a state where it should be merged into the TensorFlow repo? You mentioned wanting to attract collaborators; that doesn't seem to have happened so far. \n\nWe're trying to control the number of open pull requests so things are easier to manage, so if we don't have a story on merging this code we may close it due to inactivity. \n", "@peterbraden @danmane \n\nI am not sure how this project is being managed, but in my opinion it would be best to merge this as a beta feature rather than closing, otherwise how would you ever get enough collaborators to try it out and contribute issues/fixes?\n\nI would love to help and contribute, but not sure what are the gaps, so if you have some breakdown it would be helpful.\n\nPerhaps contributing tests would be a good starting point?\n\nBTW regarding https://github.com/feross/standard, while it is popular it's also very opinionated, so unless you feel very opinionated about specific coding style, you are probably better of using eslint with https://github.com/google/eslint-config-google as base which provides great starting point, and can be adapted to specific needs. But that's not something to stop from merging, and can be applied later as well.\n", "My guess is that  tensorflow is currently developing a c-api which then can be used with swig to build wrappers for multiple languages.\nTake a look at: https://github.com/tensorflow/tensorflow/commits/master/tensorflow/c/c_api.h\n", "That is correct -- and while the C API in 0.10.0RC0 is almost there, it's\nnot quite done.\n\nOn Tue, Aug 9, 2016 at 7:28 AM chrisber notifications@github.com wrote:\n\n> My guess is that tensorflow is currently developing a c-api which then can\n> be used with swig to build wrappers for multiple languages.\n> Take a look at:\n> https://github.com/tensorflow/tensorflow/commits/master/tensorflow/c/c_api.h\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2206#issuecomment-238570461,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_aXFQqbXGTYcu6mM78z1sa9fXgaUks5qeI59gaJpZM4IWjaw\n> .\n", "@danmane The next steps would be to start using SWIG to wrap the appropriate features. I was working in this branch: https://github.com/tensorflow/tensorflow/compare/master...peterbraden:nodejs-wip\n\nI don't really have much time to work on this at present. If you need to close the PR that's fine, but if somebody continues the attempt in the future, then this will be a good starting point, so some way of pointing them to this would be good. \n", "@peterbraden @danmane Should we close the PR or perhaps leave it open with a contributions welcome tag? Or would you prefer to open an issue with a reference to the closed PR?\n", "Either work for me. Thanks!\n", "Hey, just note that SWIG might be good for setting up TF sessions from node.js code, but I think that running a session doesn't make sense by SWIG function call, since it will block the node.js event loop from processing events for the entire session run time.\n\nThe run call can use async workers:\nhttps://github.com/nodejs/nan/blob/master/doc/asyncworker.md\n\nOr a similar model for offloading the work with threads away from the main event loop:\nhttp://docs.libuv.org/en/v1.x/threading.html\n", "As mentioned earlier in the thread, we're discouraging new uses of the C++ API through SWIG. Any language bindings should now go through the C API, which is pretty close to feature-complete with the C++ API:\nhttps://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/c/c_api.h\n\nYou can see an in-progress binding that uses the C API here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/go\n", "I'm closing this. We'd like to not use SWIG going forward (and move to the C API instead). \n\nRe #37: We're realistically not able to support node.js bindings at the moment and therefore currently see no path for this to get merged out of contrib. This would be more suitable for a separate repo. \n"]}, {"number": 2205, "title": "Wheel not supported Python3.5 ", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: ArchLinux x64 Anaconda Environment\n\nInstalled version of CUDA and cuDNN: CPU Only Version\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   pip 8.1.1 from /opt/conda/envs/AI_Playground/lib/python3.5/site-packages (python 3.5)\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   Tensorflow is not yet Installed\n   If installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Install anaconda\n2. Create Python3.5 environment\n3. Follow tensorflow install guide for Anaconda\n### What have you tried?\n1. The command the tutorial provides to create the env : \n\n$ conda create -n tensorflow python=3.5\n\nand then the command:\n\n pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n\nreference different c python versions 3.5 and c python 3.4 respectively.\n\nI tried tweaking the version in the link to cp35 in both instances which resulted in a 404 error from the pip script. Which I kind of expected. \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I suspect that this has to do with the fact that the wheel you are specifying is for python3.4. However, it's fairly easy to build from source, if no wheel for python3.5 exists on the storage server yet.\n", "Yeah that's exactly what I figured, I was more concerned that the tutorial specified anaconda with python3.5 and the wheel they tell you to use is their 3.4 build. Since it doesn't seem they have a 3.5 version for the wheel file I figured I'd bring it to their attention. A small oversight, but I imagine it'll cause issues for a number of people who use the tutorial.\n", "This just bit me today. I would definitely appreciate a python 3.5 wheel, as I'm working on a system with only python 3.5.\n", "You can install provided wheel for python 3.5.\nFirst, you download locally whl for python 3.4.\nAfter that, you rename tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl to smth like tensorflow-0.8.0-cp35-none-linux_x86_64.whl\n\nI hope this helps!\n", "Part of the problem is the Conda installation instructions are incorrect. It says to create a new virtual environment with Python 3.5 but the wheel is for 3.4. \n", "Manually downloading and renaming works for me, so thanks @aam-at !\nHowever, could the the TensorFlow team create wheels for Python3.5 as well? That would be very nice :)\n", "Manually renaming it did not work for me.\n\nsudo pip3 install ./tensorflow-0.8.0-cp35-none-linux_x86_64.whl\n\nGave me the same error:\ntensorflow-0.8.0-cp35-none-linux_x86_64.whl is not a supported wheel on this platform.\n\nOn Ubuntu 16.04\n\nAnything else that you did?\n", "@xksteven \nYou can try renaming it to: `tensorflow-0.8.0-py3-none-any.whl`. This worked for me.\n", "@akors \nThank you! That did the trick.\n", "With PR https://github.com/tensorflow/tensorflow/pull/2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: \nhttps://github.com/tensorflow/tensorflow/\n", "Merci @akors il m'a fallu 2 jours pour trouver cette solution j'ai juste renomm\u00e9 la version 0.8.0 en 1.2.0 donc \u00e7a m'a fait\r\n`tensorflow-1.2.0-py3-none-any.whl`"]}, {"number": 2204, "title": "Added a function to report download progress of the MNIST data on the\u2026", "body": "\u2026 first Udacity example. This commit is primarily intended for users with slow internet connections.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2203, "title": "Added a function to report download progress of the MNIST data on Uda\u2026", "body": "\u2026city example.  This commit is primarily intended for users with slow internet connections.\n\n[See screenshot of new functionality](https://www.dropbox.com/s/1ts0qsxjb1yaocy/Screenshot%202016-05-03%2018.45.46.png?dl=0)\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2202, "title": "add check for read only filesystem", "body": "", "comments": ["@jendap Do you plan to submit the same changes internally as well? This PR will not affect CL tests until that is done or someone pulls latest changes to internal.\n", "I forgot about this one. Sorry. It is updated now.\n\n@caisq it is not time sensitive. It will get synced one day ;-)\n", "ok to merge?\n"]}]