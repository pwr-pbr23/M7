[{"number": 18830, "title": "How to change the attributes of a variable that can be trained or not \uff1f", "body": "First, a model was obtained from the trainable variables. Then I needed to change the trainable variable to an untrainable variable and continue training.\r\nHave any idea may be help? please help.Thanks", "comments": ["@zxqchat: Check [here](https://stackoverflow.com/questions/37326002/is-it-possible-to-make-a-trainable-variable-not-trainable)", "yeah .I have view this page before ,but I need to change some trainable variables ,not all and not little. Have any efficient way to implement?", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18829, "title": "How to quantify ssd_mobilenet_v1_coco model and toco to .tflite ?", "body": "## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.8.0\r\n- Python version:2.7.12\r\n- Bazel version (if compiling from source): 0.12.0\r\n- CUDA/cuDNN version: cuda-9.0/7.0\r\n- GPU model and memory: GeForce GTX 1080/8105MiB\r\n- Exact command to reproduce: N/A\r\n- Phone: Moto G4 Play (need to use Camera, not Camera2)\r\n\r\nHi,  guys,\r\nI am trying to quantify ssd_mobilenet_v1 using tensorflow object detection api, according to @coutner [post ](https://github.com/tensorflow/tensorflow/issues/18342).\r\nFirst, I replace all the graph_hook_fn in\r\n [trainer.py](https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/trainer.py) tf.contrib.quantize.create_training_graph and enable fused batch norm in [ssd_mobilenet_v1_feature_extractor.py](https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114)\r\nAfter training, I used the following script to transform the model.\r\n\r\n1. when running optimize_for_inference, some warning had arisen.\r\n```\r\n$ bazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n>     --input=$DETECT_PB --output=$STRIPPED_PB --frozen_graph=True \\\r\n>     --input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n>     --toco_compatible=True \\\r\n>     --alsologtostderr\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/BatchNorm/FusedBatchNorm'\r\nWARNING:tensorflow:Didn't find expected Conv2D input to 'FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/BatchNorm/FusedBatchNorm'\r\n```\r\nWhen Running toco,\r\n```\r\n$ bazel run tensorflow/contrib/lite/toco:toco -- \r\n>   --input_file=$STRIPPED_PB \\\r\n>   --output_file=$DETECT_FB \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_shapes=1,300,300,3 \\\r\n>   --input_arrays=Preprocessor/sub \\\r\n>   --output_arrays=concat,concat_1 \\\r\n>   --std_values=128 \\\r\n>   --mean_values=127 \\\r\n>   --dump_graphviz=/tmp\r\n2018-04-24 18:51:51.645315: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 253 operators, 450 arrays (0 quantized)\r\n2018-04-24 18:51:51.686011: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 253 operators, 450 arrays (0 quantized)\r\n2018-04-24 18:51:51.774563: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 62 operators, 169 arrays (1 quantized)\r\n2018-04-24 18:51:51.775525: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 62 operators, 169 arrays (1 quantized)\r\n2018-04-24 18:51:51.776094: F tensorflow/contrib/lite/toco/tooling_util.cc:1464] Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, \r\nwhich is necessary for quantization. Either target a non-quantized output format, or change the input \r\ngraph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you \r\ndo not care about the accuracy of results.\r\nAborted (core dumped)\r\n```\r\nIs there anyone who can help ? @andrewharp ,\r\nthanks.\r\n\r\n\r\n", "comments": ["Can you try --inference_type=FLOAT?", "@andrehentz, Thanks\uff01\r\n", "@andrehentz doesn't that disable quantization? I thought the whole point of @WenguoLi 's question was to _quantize_ in tflite...", "@WenguoLi have you tested your model on android device? I had the same problem and solved by changing to FLOAT but when I run it on android I get the following error:\r\n`E/AndroidRuntime: FATAL EXCEPTION: inference\r\n                  Process: org.tensorflow.lite.demo, PID: 28270\r\n                  java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\r\n                      at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:175)\r\n                      at org.tensorflow.lite.Tensor.setTo(Tensor.java:65)\r\n                      at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:126)\r\n                      at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:168)\r\n                      at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:194)\r\n                      at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)\r\n                      at android.os.Handler.handleCallback(Handler.java:751)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                      at android.os.Looper.loop(Looper.java:154)\r\n                      at android.os.HandlerThread.run(HandlerThread.java:61)\r\n`", "@MohammadMoradi   hello, I face the same pronlem, \"Cannot convert between a TensorFlowLite buffer with 270000 bytes and a ByteBuffer with 1080000 bytes\" .  have you solved it ?", "@zhyj3038 hello, the problem is that when using float model, you must allocate 4 * 270000 = 1080000 because the float is 32bit and the quantized version of model is 8bit. You must change the input allocated buffer to match your model type.", "@MohammadMoradi    thanks very much .   I am not good at java and android , you help me a lot !", "@MohammadMoradi @WenguoLi @zhyj3038 \r\nHello all,\r\n\r\nI am facing the same problem when I am using `inference_type=FLOAT`. Do you guys solved the problem? \r\nWhere should I change the value for bytebuffer? I am not getting how should one allocate 4*270000 when using float buffer. \r\n\r\nPlease share your suggestion.\r\n\r\nThank you", "@MohammadMoradi @WenguoLi  @zhyj3038 \r\nHello , \r\nany update on @alokranjan007 problem ?\r\nI am also having the same issue.\r\nThanks!\r\n", "@jass-singh    you can not use float, but you must quantize the model.", "@zhyj3038 i tried that too using \r\n--inference_type=QUANTIZED_UINT8\r\nbut then i am not able to create .tflite  : \r\n\r\n\r\n2018-08-24 13:16:55.166755: F tensorflow/contrib/lite/toco/tooling_util.cc:1619] Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n\r\n", "I use mobilenet v2", "@jass-singh @zhyj3038 Hello,\r\n\r\nA fix: I would recommend that you use`inference_type=FLOAT` for the conversion. However, do change DetectorActivity.java file in the tensorflow/contrib/lite/examples/android/app/src/main/java as below:\r\nprivate static final boolean TF_OD_API_IS_QUANTIZED = false; (It should be true by default).\r\n\r\nAfter this save your file and try to build the apk. Let me know if it works :)\r\n\r\n\r\nCheers!!\r\n", "@alokranjan007 \r\nI did but got error : \r\nCaused by: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\r\n\r\nWhere should i make changes to handle bytes problem?", "@jass-singh Could you share the detailed steps of converting frozen graph into .tflite format. Just want to check if your are following in right direction. Which model you are using?\r\n\r\nThank you", "@alokranjan007 \r\n\r\n1. I am using Mobilenet model ssd_mobilenet_v1_coco_2017_11_17 , retraining on faces.\r\n\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\r\nexport CONFIG_FILE=/home/jaskaran/Git_Download_Libraries/face_detection_inference_graph_2018_08_23/pipeline.config\r\nexport CHECKPOINT_PATH=/home/jaskaran/Git_Download_Libraries/face_detection_inference_graph_2018_08_23/model.ckpt\r\nexport OUTPUT_DIR=/home/jaskaran/Git_Download_Libraries/face_detection_inference_graph_2018_08_23/\r\n\r\n2. Further, creating .pb using pipeline.config,\r\n\r\npython object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$OUTPUT_DIR \\\r\n--add_postprocessing_op=true\r\n\r\n\r\n3. Then, made changes in DetectorActivity.java file in the tensorflow/contrib/lite/examples/android/app/src/main/java as below:\r\nprivate static final boolean TF_OD_API_IS_QUANTIZED = false;\r\n\r\n\r\n4.  At last ,\r\n\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/custom_mob_2018_08_23_v2.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=FLOAT \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n", "@jass-singh Assuming that you have already trained you model and have set the python path. I would suggest to follow below steps for inferncin.\r\n\r\nStep 1.\r\n`python export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=/home/username/tensorflow-master/models/research/object_detection/path to .config file \\\r\n--trained_checkpoint_prefix=/home/username/tensorflow-master/models/research/object_detection/path to the trained model ckpt point(your total no of steps after the training) \\\r\n--output_directory=/home/username/tensorflow-master/models/research/object_detection/any folder name \\\r\n--add_postprocessing_op=true`\r\nThis will generate tflite_graph.pb file and tflite_graph.pbtxt\r\n\r\nStep 2:\r\n`bazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=/home/username/tensorflow-master/models/research/object_detection/customized_model/path to tflite_graph.pb \\\r\n--output_file=/home/username/tensorflow-master/models/research/object_detection/your any folder name/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=FLOAT \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops`\r\n\r\nAfter this detect.tflite file will be generated then you can use this to port on device.", "Hello guys,\r\n@alokranjan007 @MohammadMoradi @WenguoLi @zhyj3038\r\ni found one solution ,\r\n\r\nAfter creating .pb file ,\r\nadd \r\n--default_ranges_min=0 \\\r\n--default_ranges_max=255 \\\r\nalong with \r\n--inference_type=QUANTIZED_UINT8 \\\r\nIt will create .tflite successfully and will work on mobile without causing any bytes issue.\r\n\r\n\r\nEg:-\r\n\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/custom_mobilenetv3.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--default_ranges_min=0 \\\r\n--default_ranges_max=255 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n\r\nThanks,\r\n\r\n", "@jass-singh That's great that it is working now. I have not tried the above mentioned step. I will check.\r\n\r\nDo you have any details how this \"default_range_min and max\" is solving the problem? Is there any link for detailed insights?\r\n\r\nCheers,\r\n\r\n", "@jass-singh  Have you add other options when you run `bazel` to convert .pb to .tflite? \r\nBecause I have  also retrained my graph from the same checkpoint **ssd_mobilenet_v1_coco_2017_11_17** as yours.\r\nSo the model is supposed to be a **float model**. \r\nIf I convert to .pb to .tflite without create the QUANTIZED_MODEL, I can get my .tflite successfully , but then I'll have the issue for Android studio (even I set **TF_OD_API_IS_QUANTIZED=false**), the app is crashed each time..\r\nBut When I tried your method to convert , I had the following error:\r\n`2018-10-03 18:49:12.586196: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:560] Check failed: is_rnn_state_array \r\n[1]    28561 abort (core dumped)  bazel run -c opt //tensorflow/contrib/lite/toco:toco --   `\r\nAlso could you tell me which branch you used when you did the covertion? ( I tried master and r1.9, the master could not even convert .pb to .tflite without option QUANTIZED_MODEL..) ", "@WenguoLi I see you closed on the recommendation to try \"try --inference_type=FLOAT\".  \r\n\r\nI have the exact same issue: it works for a float model, but not for a quantized model (--inference_type=QUANTIZED_UINT8).\r\n\r\nThe float model is too slow for me, and adding --default_ranges_min=0 and --default_ranges_max=255 makes the model unusable because of the low accuracy.\r\n\r\nI'm retraining a custom image set from a quantized model [ssd_mobilenet_v1_quantized_coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) from the model zoo.\r\n\r\n@andrehentz any ideas how to go forward?", "@switchedfabric  did you solve it? How convert pb to quant tflite?", ">  Array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n\r\nIt says that it is better to do \"_run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information_\"\r\n\r\nBut all I have is retrained_graph.pb file generated by retrain.py (for classification weights), there is no any checkpoints", "Array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n\r\nI am also getting same problem.\r\nWhen i run with default_ranges_min=0 and default_ranges_max=255, the results are way off.\r\nAny idea how we can solve this ??", "@ignvinay , from the [here](https://www.tensorflow.org/lite/convert/cmdline_examples), it's recommended that the default_ranges_max=6 due to the Relu6 function.", "> Can you try --inference_type=FLOAT?\r\n\r\nthank you so much! you saved me", "ihave faced a lot problems to get my model working using tensorflow lite. My thanks to this perfect forum. After read some books and undestand the all the kind process with tensorflow, i finally get my model working in a android envoriment", "@duvictor Hi there, am glad that you got your model working, do you mind sharing your process, and difficulties, what i can see now, we are all facing these issues right now. the issues of converting tf1 model to **QUANTIZED_UINT8**, and with **inference_type=FLOAT** it convert successfully, but when you run on the model, it's give error even though **TF_OD_API_IS_QUANTIZED = false;** Thanks. ", "> @WenguoLi I see you closed on the recommendation to try \"try --inference_type=FLOAT\".\r\n> \r\n> I have the exact same issue: it works for a float model, but not for a quantized model (--inference_type=QUANTIZED_UINT8).\r\n> \r\n> The float model is too slow for me, and adding --default_ranges_min=0 and --default_ranges_max=255 makes the model unusable because of the low accuracy.\r\n> \r\n> I'm retraining a custom image set from a quantized model [ssd_mobilenet_v1_quantized_coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) from the model zoo.\r\n> \r\n> @andrehentz any ideas how to go forward?\r\n\r\nHi, how did it go after you retrained the custom quantized model [ssd_mobilenet_v1_quantized_coco]", "Anybody figure this one out? I'm facing the \"Relu6 is lacking min/max data\" error when converting my model to tflite. I need it to be quantized to UINT8, so the inference_type=FLOAT is not going to work for me. Specifying default ranges makes the accuracy terrible, so I'm not sure where to go from here."]}, {"number": 18828, "title": "Check failed: input_shape.dim_size() <= 4 (5 vs. 4)", "body": "####System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): -\r\nTensorFlow installed from (source or binary): -\r\nTensorFlow version (use command below): 1.8.0 rc1\r\nPython version: -\r\nBazel version (if compiling from source):-\r\nGCC/Compiler version (if compiling from source): -\r\nCUDA/cuDNN version: -\r\nGPU model and memory: -\r\nExact command to reproduce: -\r\n####Describe the problem\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=one_graphRes.pb\r\nFound 1 possible inputs: (name=input_1, type=float(1), shape=[1,300,480,3]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=predictions/concat, op=ConcatV2) \r\nFound 335304 (335.30k) const parameters, 0 (0) variable parameters, and 7 control_edges\r\nOp types used: 192 Const, 73 Identity, 49 Switch, 21 Mul, 16 StridedSlice, 16 Pack, 15 BiasAdd, 15 Conv2D, 14 Add, 12 Reshape, 9 Sub, 7 Merge, 7 Elu, 7 FusedBatchNorm, 7 Rsqrt, 6 MaxPool, 4 Tile, 4 ConcatV2, 4 Shape, 2 RealDiv, 1 PlaceholderWithDefault, 1 Placeholder, 1 Sum, 1 Max, 1 Exp\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=one_graphRes.pb --show_flops --input_layer=input_1 --input_layer_type=float --input_layer_shape=1,300,480,3 --output_layer=predictions/concat\r\n\r\n bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=one_graphRes.pb --output_file=one_graphRes.tflite --inference_type=FLOAT --input_shape=1,300,480,3 --input_array=input_1 --output_array=predictions/concat\r\n\r\nCheck failed: input_shape.dim_size() <= 4 (5 vs. 4)\r\n", "comments": ["Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I also encountered this issue, but i use the python api tf.contrib.lite.toco_convert() to convert, any solution yet? Is it like it will work only with placeholder ?", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@iChiaGuo @PariksheetPinjari909 : Can you attach examples of models that I can use to reproduce this error.", "Looks like this issue has been open for a while, @iChiaGuo , @PariksheetPinjari909 please reopen if you are still having issues."]}, {"number": 18827, "title": "Feature Request: Allow for sloppy=True in TFRecordDataset's init", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: -\r\n- **TensorFlow installed from (source or binary)**: -\r\n- **TensorFlow version (use command below)**: 1.8.0 rc1\r\n- **Python version**: -\r\n- **Bazel version (if compiling from source)**:-\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\n### Describe the problem\r\nFrom TF 1.8 `TFRecordDataset` has a new init parameter `num_parallel_reads` which essentially wraps a `ParallelInterleaveDataset` around the `_TFRecordDataset` instance we're reading from.\r\nIn the current implementation, the parallel interleave is hardcoded with `sloppy=False`.\r\nWould it be possible to add `sloppy` as an additional parameter to TFRecordDataset? That would allow to simplify this:\r\n```python\r\nfiles = tf.data.list_files('somewhere/*')\r\ndataset = files.apply(tf.data.parallel_interleave(TFRecordDataset, cycle_length=num_parallel_calls, sloppy=True))\r\n```\r\nto\r\n```python\r\nfiles = tf.data.list_files('somewhere/*')\r\ndataset = TFRecordDataset(files, num_parallel_calls=num_parallel_calls, sloppy=True)\r\n```\r\n(Note that the underlying implementations are identical)", "comments": ["Nagging Assignee @mrry: It has been 151 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mrry: It has been 166 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "After giving this request some thought, we have decided *not* to add an extra argument to the `tf.data.TFRecordDataset` initializer. However, we recently added the `tf.data.Dataset.with_options()` method, and it seems appropriate to add an option to `tf.data.Options` that will make it possible to turn on \"sloppiness\" for an entire pipeline.\r\n\r\nAssigning this to @jsimsa, who already has the corresponding internal bug assigned to him!"]}, {"number": 18826, "title": "fix incorrect example code", "body": "based on https://stackoverflow.com/questions/40904979/the-print-of-string-constant-is-always-attached-with-b-intensorflow", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", " I signed it!\n\nOn Tue, Apr 24, 2018 at 4:47 PM, googlebot <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project (if not, look below for help).\n> Before we can look at your pull request, you'll need to sign a Contributor\n> License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed (or fixed any issues), please reply here (e.g. I\n> signed it!) and we'll verify it.\n> ------------------------------\n> What to do if you already signed the CLA Individual signers\n>\n>    - It's possible we don't have your GitHub username or you're using a\n>    different email address on your commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>\n> Corporate signers\n>\n>    - Your company has a Point of Contact who decides which employees are\n>    authorized to participate. Ask your POC to be added to the group of\n>    authorized contributors. If you don't know who your Point of Contact is,\n>    direct the Google project maintainer to go/cla#troubleshoot (Public\n>    version <https://opensource.google.com/docs/cla/#troubleshoot>).\n>    - The email used to register you as an authorized contributor must be\n>    the email used for the Git commit. Check your existing CLA data\n>    <https://cla.developers.google.com/clas> and verify that your email is\n>    set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - The email used to register you as an authorized contributor must\n>    also be attached to your GitHub account\n>    <https://github.com/settings/emails>.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/18826#issuecomment-383870876>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AL8-Ay-2iqSkvt_k4X1EGbxcziw_xoHaks5trvSbgaJpZM4ThV8c>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @ekelsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 47 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 62 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 77 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18825, "title": "Update README.md", "body": "Awesome and details doc!\r\n\r\nBut I woulda avoid calling it an \"awkward\" package path :)", "comments": []}, {"number": 18824, "title": "Tensorflow failed due to error LNK2019 when build with MSVC", "body": "System information\r\n\u2022\tHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nN/A\r\n\u2022\tOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows server 2016\r\n\u2022\tTensorFlow installed from (source or binary):\r\nSource\r\n\u2022\tTensorFlow version (use command below):\r\nMaster branch latest revison\r\n\u2022\tPython version:\r\nAnaconda 4.1.1 (Python 3.5 64-bit)\r\n\u2022\tBazel version (if compiling from source):\r\nN/A\r\n\u2022\tGCC/Compiler version (if compiling from source):\r\nVS2017 15.5.7\r\n\u2022\tCUDA/cuDNN version:\r\nNVidia CUDA Toolkit 8.0\r\nNVidia CUDNN 5.1\r\n\u2022\tGPU model and memory:\r\nN/A\r\n\u2022\tExact command to reproduce:\r\nN/A\r\n\r\n**Describe the problem:**\r\nTensorflow failed to build due to the error LNK2019 and error LNK1120. This should be tensorflow source issue, could you please help take a look at this? Thanks!\r\n\r\n**The failures like:**\r\nThe whole log file please see attachment.\r\n```\r\n[log_x64_build.log](https://github.com/tensorflow/tensorflow/files/1942013/log_x64_build.log)\r\n(Link target) -> \r\n         c_api.cc.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::CppShapeInferenceResult_HandleShapeAndType(void)\" (??0CppShapeInferenceResult_HandleShapeAndType@tensorflow@@QEAA@XZ) referenced in function \"protected: class tensorflow::CppShapeInferenceResult_HandleShapeAndType * __cdecl google::protobuf::internal::RepeatedPtrFieldBase::Add<class google::protobuf::RepeatedPtrField<class tensorflow::CppShapeInferenceResult_HandleShapeAndType>::TypeHandler>(class tensorflow::CppShapeInferenceResult_HandleShapeAndType *)\" (??$Add@VTypeHandler@?$RepeatedPtrField@VCppShapeInferenceResult_HandleShapeAndType@tensorflow@@@protobuf@google@@@RepeatedPtrFieldBase@internal@protobuf@google@@IEAAPEAVCppShapeInferenceResult_HandleShapeAndType@tensorflow@@PEAV45@@Z) [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n         c_api.cc.obj : error LNK2019: unresolved external symbol \"protected: __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::CppShapeInferenceResult_HandleShapeAndType(class google::protobuf::Arena *)\" (??0CppShapeInferenceResult_HandleShapeAndType@tensorflow@@IEAA@PEAVArena@protobuf@google@@@Z) referenced in function \"protected: class tensorflow::CppShapeInferenceResult_HandleShapeAndType * __cdecl google::protobuf::internal::RepeatedPtrFieldBase::Add<class google::protobuf::RepeatedPtrField<class tensorflow::CppShapeInferenceResult_HandleShapeAndType>::TypeHandler>(class tensorflow::CppShapeInferenceResult_HandleShapeAndType *)\" (??$Add@VTypeHandler@?$RepeatedPtrField@VCppShapeInferenceResult_HandleShapeAndType@tensorflow@@@protobuf@google@@@RepeatedPtrFieldBase@internal@protobuf@google@@IEAAPEAVCppShapeInferenceResult_HandleShapeAndType@tensorflow@@PEAV45@@Z) [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n         c_api.cc.obj : error LNK2019: unresolved external symbol \"private: void __cdecl tensorflow::CppShapeInferenceResult_HandleShapeAndType::_slow_mutable_shape(void)\" (?_slow_mutable_shape@CppShapeInferenceResult_HandleShapeAndType@tensorflow@@AEAAXXZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n         c_api.cc.obj : error LNK2019: unresolved external symbol \"public: __cdecl tensorflow::CppShapeInferenceResult_HandleData::CppShapeInferenceResult_HandleData(void)\" (??0CppShapeInferenceResult_HandleData@tensorflow@@QEAA@XZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n         c_api.cc.obj : error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::CppShapeInferenceResult_HandleData::~CppShapeInferenceResult_HandleData(void)\" (??1CppShapeInferenceResult_HandleData@tensorflow@@UEAA@XZ) referenced in function TFE_GetResourceHandleShapeAndType [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n         D:\\Tensorflow\\build_x64\\Release\\tensorflow.dll : fatal error LNK1120: 5 unresolved externals [D:\\Tensorflow\\build_x64\\tensorflow.vcxproj]\r\n```\r\n\r\n**Repro steps:**\r\n1. git clone https://github.com/tensorflow/tensorflow D:\\Tensorflow\\src\r\n2. pushd D:\\Tensorflow\r\n3. set PreferredToolArchitecture=x64\r\n4. set rel=Release\r\n5. set CUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\cuda\"\r\n6. set PY=C:\\ProgramData\\Anaconda3\r\n7. set CL=/FS /permissive-\r\n8. cmake D:\\Tensorflow\\src\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=C:\\ProgramData\\Anaconda3\\python.exe -DPYTHON_LIBRARIES=C:\\ProgramData\\Anaconda3\\libs\\python36.lib -DSWIG_EXECUTABLE=D:\\Tensorflow\\swigwin-3.0.12\\swig.exe -Dtensorflow_BUILD_PYTHON_TESTS=ON -Dtensorflow_BUILD_SHARED_LIB=ON\r\n9. MSBuild /m /p:Configuration=Release;Platform=x64 /p:WindowsTargetPlatformVersion=10.0.16299.0 tensorflow.sln /t:Rebuild", "comments": ["@gunan Can you take a look at this? I don't have a Windows device for reproductions.", "There are some known build issues with visual studio 2017. It may just be those.\r\nI dont have a better answer at the moment as we currently use VS 2015 update 3 to build TF on windows.", "Hi @gunan. Thanks. I found that this issue can be reproduced from master revision f196351c. Hope it will help solve this issue.", "Nagging Assignees @gunan, @angersson: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @gunan, @angersson: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I sent a few fixes and was able to successfully build with vs 2017, but I forgot to close the issues.\r\nPlease reopen if you still run into the same problem.", "Thank you for fixing this issue, @gunan. I tried your fix and Tensorflow works well."]}, {"number": 18823, "title": "Variables in autograph", "body": "This question is regarding creating tf.Variables inside an autograph function.\r\n\r\nThe semantics that we have been trying are:\r\n\r\n```\r\ndef func():\r\n   v = []\r\n   autograph.utils.set_element_type(v, tf.int32)\r\n   v.extend([0.5, 0.1])\r\n   return v.stack()\r\n\r\n# And then:\r\nt = autograph.to_graph(func)\r\n```\r\n\r\nHowever, now the return value of the function is an op, not a variable. The question is, how do I get the variable `v` as a tf.Variable \"out of\" my autograph function?\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "CC @alexbw @mdanatg ", "The answer largely depends on how you need to use the variable - after the call to to_graph, do you just want a variable initialized with what's inside v, or do you have another purpose in mind? In general, I tend to stay away from variables because they require separate initialization and I have to watch for concurrency. But if you need a trainable variable there is obviously no other choice.\r\n\r\nIn our case, calling `t()` will indeed return a Tensor, which can be used in an initializer or assignment. You could also create the variable inside the compiled code, as it's possible to mix TF calls in there (the rule of thumb is that Python variables are Tensors, and Python lists are TensorArrays). See the snippet below for a handful of examples.\r\n\r\nSide note: we haven't yet added support for extend(), so that won't work, and I changed it to appends.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.contrib import autograph\r\n\r\ndef func():\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  return v.stack()\r\n\r\ndef func_with_var():\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  return tf.get_variable('foo', initializer=v.stack())\r\n\r\ndef func_with_var_assign(my_var):\r\n  v = []\r\n  autograph.utils.set_element_type(v, tf.float32)\r\n  v.append(0.5)\r\n  v.append(0.1)\r\n  # This is trickier, assign returns an op, but we can use it.\r\n  return my_var.assign(v.stack())\r\n\r\n# Original code:\r\nt = autograph.to_graph(func, verbose=True)\r\n\r\n# Mix tf.get_variable in:\r\nt_var = autograph.to_graph(func, verbose=True)\r\n\r\n# Pass a variable separately:\r\nt_assign = autograph.to_graph(func_with_var_assign, verbose=True)\r\n\r\nwith tf.Graph().as_default():\r\n  with tf.Session() as sess:\r\n    \r\n    # Output of t:\r\n    print(sess.run(t()))\r\n    \r\n    # Output of t_var:\r\n    print(sess.run(t_var()))\r\n\r\n  # Setup to use t_assign:\r\n  my_var = tf.get_variable('foo', shape=(2,))\r\n  assign_op = t_assign(my_var)\r\n  \r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(assign_op)\r\n    print(sess.run(my_var))\r\n```\r\n", "Hi mdanatg,\r\nthank you so much for your quick response. This helped us a lot further!", "Thank you very much for clarifying this. I was wondering what the idea was on doing the following steps within a function:\r\n\r\n1. Read value from variable\r\n2. Perform some logic deciding on how to insert value into variable\r\n3. Write back to variable\r\n\r\ne.g. for a buffer insert:\r\n\r\n```\r\n    # In constructor\r\n    self.buffer = tf.get_variable('buffer', shape=(self.capacity,), dtype=tf.float32)\r\n    self.insert_fn = autograph.to_graph(self.insert, verbose=True)\r\n\r\n    def insert(self, element):\r\n        values = []\r\n        autograph.utils.set_element_type(values, tf.float32)\r\n        values.append(element)\r\n        return self.buffer.assign(values.stack())\r\n```\r\n\r\nThis works now but I would actually not want to assign the full list but just a slice (or potentially a scatter update), is this supported/ is there an intended way to do this? E.g., \r\n\r\n```\r\n        return self.buffer[start:end].assign(values.stack())\r\n```\r\nanalogue to ```tf.assign(ref=self.buffer[start:end], value=values.stack())```?", "I see. Do you mean something in these lines perhaps?\r\n\r\n(currently we've been focusing on Tensor lists, e.g. TensorArray, but we definitely intend to support Variables, so this discussion is useful for us to make sure we understand the use case well)\r\n\r\n```\r\n# Note: the prints below use py_func. Remove them if py_func is not available.\r\ndef insert(buffer, buffer_head, element):\r\n  print('Buffer before:', buffer)\r\n  tf.scatter_update(buffer, [buffer_head], [element])\r\n  tf.assign_add(buffer_head, 1)\r\n  # Need tf.identity because Variables are not subject to control dependencies.\r\n  print('Buffer after:', tf.identity(buffer))\r\n  # We just need an op to group together the assigns done in this body.\r\n  return tf.no_op()\r\n\r\ninsert_fn = autograph.to_graph(insert, verbose=True)\r\n\r\nwith tf.Graph().as_default():\r\n  capacity = 5\r\n  buffer = tf.get_variable('buffer', shape=(capacity,), initializer=tf.zeros_initializer(), dtype=tf.float32)\r\n  buffer_head = tf.get_variable('buffer_head', shape=(), dtype=tf.int32)\r\n  \r\n  make_an_insert = insert_fn(buffer, buffer_head, 3.0)\r\n\r\n  with tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    sess.run(make_an_insert)\r\n    sess.run(make_an_insert)\r\n```\r\n\r\nAssuming my understanding is correct, we could envision letting you do something in these lines:\r\n\r\n```\r\ndef insert(buffer, buffer_head, element):\r\n  magic_list = autograph.util.variable_backed_list(buffer, buffer_head)\r\n  # AutoGraph would generate all the necessary variable assigns and scatter updates.\r\n  magic_list.append(element)\r\n```", "Yes, this is exactly what I am referring to, thank you very much!\r\n\r\nJust to help you understand the types of use cases I am thinking about, this is a (rather messy) pure TensorFlow prioritized replay implementation with two buffers and sampling logic: \r\n\r\nhttps://github.com/reinforceio/tensorforce/blob/master/tensorforce/core/memories/prioritized_replay.py\r\n\r\nSo there is  fair number of variables, elements arriving into an initial buffer, being moved into a memory once their losses are available, sampling from them, and resorting according to priorities. \r\nYou can see that the implementation is rather bloated by the assigns and scatter updates which makes it difficult to read.\r\n\r\nUltimately it would be great if features like these could be done with autograph, so the main issue is reading and writing back to various variables with features such as the variable-to-list read you suggested.\r\n\r\n", "Great, I agree that facilitating complex variable manipulation in this way would be a useful feature. We can keep this issue open to track the feature.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mdanatg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mdanatg: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mdanatg: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @mdanatg: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@mdanatg, @alextp - is this still something that would be useful to add to AutoGraph?", "@dynamicwebpaige I think improvements to `TensorArray` will eventually allow expressing this use case efficiently, but it's early to tell.", "Hi @sven1977 ! Contrib has removed in 2.x versions. Please visit [migration page](https://www.tensorflow.org/guide/migrate) to upgrade your codebase to 2.x versions. Thank you.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)", "Note that autograph was moved from contrib into core, so the issue remains valid, although it is a bit stalled at the moment, as we work to resolve the actual level of op support at TF level."]}, {"number": 18822, "title": "does tensorflow take all resource from GPU, making other CUDA code slow ?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have a custom CUDA code but not register into Tensorflow OP\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary: by \"pip3 install --upgrade tensorflow-gpu\"\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu (1.7.0)\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: CUDA 9.0 cuDNN 7\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a cuda lib build from C++ for post-processing after predict result by tensorflow model.\r\nI use following way to make python able to use cuda code from C++\r\n`lib = ctypes.cdll.LoadLibrary(my.so)`\r\n\r\nIf I test the cuda code alone without tensorflow. it work fine.\r\nBut when tensorflow is used in my project, my cuda code become 10 times slower....\r\n\r\nMy  time log is in cuda .so lib, so it's no way that the gap come from python to .so  wrap.\r\n\r\nI have try to set the fraction of GPU memory to be allocated in tensorflow by:\r\n`# Assume that you have 12GB of GPU memory and want to allocate ~4GB:`\r\n`gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)`\r\n`sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))`\r\n\r\nbut useless....\r\nso I wonder does tensorflow take all resource from GPU, making other CUDA code slow ?\r\nthe only solution is make my cuda code as a tensorflow OP by register?\r\n\r\nAny suggestion? Thanks~~~\r\n\r\n----------------------Update----------------------\r\nI have tested following way to set gpu_options, but still useless....\r\n`config = tf.ConfigProto()`\r\n`config.gpu_options.allow_growth = True`\r\n`sess = tf.Session(config=config)`", "comments": ["Maybe your GPU is just busy doing tensorflow calculations.", "Hi @sleighsoft\r\nmy program flow is:\r\ntensorflow calculations >> post-processing cuda code \r\nIt won't calculate at same time.\r\nI just wonder if tensorflow force every code in same project or scope share same resource from GPU.\r\nI have 2 GeForce GTX 1080, I have tested that make tensorflow only see the first one.\r\nBut it make my C++ cuda code only see the same one in the available GPU device list....\r\n\r\nAny idea to workaround? Thanks.....", "You could try allowing your process to see both GPUs then move your tensorflow calculations to e.g. GPU 1 with `tf.device` and your post-processing to GPU 0.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "hi @jart \r\nI think it might be a bug.  Because once tensorflow be active. it will make every cuda code crash even I use \"sess.close()\"...\r\nThe case mentioned above is run on specific NVIDIA driver to make it don't crash.\r\n\r\nThe following code it a simple example cuda code run by [pycuda](https://documen.tician.de/pycuda/).\r\nonce I add `sess = tf.Session()` .  My cuda code crash. It work fine without `sess = tf.Session()`.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pycuda.autoinit\r\nimport pycuda.driver as drv\r\nimport numpy\r\nfrom pycuda.compiler import SourceModule\r\nmod = SourceModule(\"\"\"\r\n__global__ void multiply_them(float *dest, float *a, float *b)\r\n{\r\n  const int i = threadIdx.x;\r\n  dest[i] = a[i] * b[i];\r\n}\r\n\"\"\")\r\n## tensorflow will make any other cuda code crash................................\r\nsess = tf.Session()\r\nsess.close()\r\nmultiply_them = mod.get_function(\"multiply_them\")\r\na = numpy.random.randn(400).astype(numpy.float32)\r\nb = numpy.random.randn(400).astype(numpy.float32)\r\ndest = numpy.zeros_like(a)\r\nmultiply_them(drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1), grid=(1,1))\r\nprint (dest-a*b)\r\nprint(\"finish\")\r\n```\r\n\r\nIt would be wonderful if tensorflow could allow other cuda code "]}, {"number": 18821, "title": "Error:(152, 16) The method setNumThreads(int) is undefined for the type Interpreter", "body": "github\\tensorflow\\tensorflow\\contrib\\lite\\java\\demo\\app\\src\\main\\java\\com\\example\\android\\tflitecamerademo\\ImageClassifier.java\r\nError:(152, 16) The method setNumThreads(int) is undefined for the type Interpreter", "comments": ["I face the same problem in tensorflow-1.8.0rc1, although it has the method in TFLite!", "I faced the same issue, solved it by changing my app/build.gradle file.\r\n\r\nThe change is simple,\r\nReplace\r\n`compile 'org.tensorflow:tensorflow-lite:+'`\r\nWith \r\n`compile 'org.tensorflow:tensorflow-lite:0.1.7'`\r\n\r\nI hope this helps.", "i try \uff0cbut fail\uff01", " I just commented out the code below in ImageClassifier.java.\r\n\r\n```\r\n  public void setNumThreads(int num_threads) {\r\n    if (tflite != null)\r\n        tflite.setNumThreads(num_threads);\r\n  }\r\n```\r\n\r\n And I also commented out\r\n\r\n`classifier.setNumThreads(newVal);`\r\n\r\nin Camera2BasicFragment.java.\r\n\r\nAnd it built successfully.", "Nagging Assignee @tatatodd: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Checkout source to Tag tflite-v0.1.7 ,And you should build successfully.", "i try \uff0cbut fail", "commented out tflite.setNumThreads(num_threads) and classifier.setNumThreads(newVal)\r\nHow can we do other threads testing", "See https://github.com/tensorflow/tensorflow/issues/18658 for resolution of duplicate issue."]}, {"number": 18820, "title": "op not register", "body": "1. add op to `tensorflow/contrib/makefile/tf_op_files.txt` to fix problem of `op not registered`\r\n2. fix header and source not found problems when using `build_all_linux.sh`", "comments": ["@petewarden Please add a review or reassign. Thanks.", "Nagging Reviewer @petewarden: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 18819, "title": "[Error] Failed precondition: Table not initialized.", "body": "Env:\r\n  tf version 1.7.0\r\n  macOS CPU\r\n\r\nI uses `lookup_table` in my model. I freeze the model in `python` after training and then load it and do prediction in `c++`. When predicting, an error occurs. \r\n```\r\nFailed precondition: Table not initialized.\r\n\t [[Node: forward/string_to_index_4_Lookup/hash_table_Lookup = LookupTableFindV2[Tin=DT_INT64, Tout=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](forward/string_to_index_4/hash_table, _arg_input/sparse_179/id/Placeholder_0_6, forward/string_to_index/hash_table/Const)]]\r\n```\r\nHowever, I have found `table_init` op in the frozen model protobuf. The error indicates those op not executed in `c++` code. How to execute `sess.run(tf.tables_initializer())` in `c++`?\r\n```\r\n  name: \"init_all_tables\"\r\n  name: \"forward/string_to_index/hash_table/table_init\"\r\n  name: \"forward/string_to_index_1/hash_table/table_init\"\r\n  name: \"forward/string_to_index_2/hash_table/table_init\"\r\n  name: \"forward/string_to_index_3/hash_table/table_init\"\r\n  name: \"forward/string_to_index_4/hash_table/table_init\"\r\n```\r\n\r\nMain `c++` code:\r\n```\r\n  Session* session;\r\n  Status status = NewSession(SessionOptions(), &session);\r\n  if (!status.ok()) {\r\n    std::cerr << status.ToString() << \"\\n\";\r\n    return 1;\r\n  } else {\r\n    std::cout << \"Session created successfully\" << std::endl;\r\n  }\r\n\r\n  // Load graph protobuf\r\n  GraphDef graph_def;\r\n  std::string graph_path = argv[2]; // the path to frozen model protobuf\r\n  status = ReadBinaryProto(Env::Default(), graph_path, &graph_def);\r\n  if (!status.ok()) {\r\n    std::cerr << status.ToString() << std::endl;\r\n  } else {\r\n    std::cout << \"Load graph protobuf successfully\" << std::endl;\r\n  }\r\n\r\n  // Add the graph to the session\r\n  status = session->Create(graph_def);\r\n  if (!status.ok()) {\r\n    std::cerr << status.ToString() << std::endl;\r\n    return 1;\r\n  } else {\r\n    std::cout << \"Add graph to session successfully\" << std::endl;\r\n  }\r\n\r\n  // prepare inputs\r\n  std::vector<std::pair<std::string, Tensor> > inputs;\r\n  .....\r\n\r\n  // Run the session, evaluating our \"forward/predict/add\" operation from the graph\r\n  std::vector<tensorflow::Tensor> outputs;\r\n  status = session->Run(inputs, {\"forward/logit/add\"}, {}, &outputs);\r\n  if (!status.ok()) {\r\n    std::cerr << status.ToString() << std::endl;\r\n    return 1;\r\n  } else {\r\n    std::cout << \"Run session successfully\" << std::endl;\r\n  }\r\n```\r\n\r\n", "comments": ["Hi, is this problem solved? Could you please show some details? Thx.", "I have a similar issue but when using Tensorflow C library. Specifically, I don't know how to initialize the hash table the error message says.\r\n", "I have a similar issue, any solutions?", "Any update? @littleDing @usptact @elvys-zhang ?"]}, {"number": 18818, "title": "Using out-of-graph Reward in Reinforcement Learning with Iterator in tf.data", "body": "When the reward value is generated from a script that is not computed in Tensorflow graph, like ROUGE score in text summarization or BLEU score in machine translation.\r\nWe first use the graph to generate a prediction result, then calculate the reward value out of the graph, and then feed the value into the graph to update the parameters of our model by policy gradients.\r\nIf we use session.run, this will cause recomputation that is mentioned in #672.  \r\nAlthough recomputation does not cost very much, when we use the `tf.dataset` with `Iterator`, the dataset pointer will move to the next sample when calling `session.run` function.\r\nIn #672, they suggest to use `partitial_run` instead of `session.run`. But `partitial_run` can only fetch the tensor, but it can not execute the operation. \r\n\r\n\r\nSo can you add a feature that can move the pointer of `Iterator` backward? or fix the pointer when calling the `session.run` function?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code : N/A\r\nOS Platform and Distribution : all platform\r\nTensorFlow installed from : pip\r\nTensorFlow version : all version\r\nBazel version : N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory : N/A\r\nExact command to reproduce : N/A", "Feature request relating to `tf.data` backwards iterators for @mrry.", "Thanks for the suggestion. We do not plan to add this feature to `tf.data`, because it would complicate\u2014and completely change\u2014the implementation and make it more difficult to add optimizations in the future. For this kind of workflow, I'd suggest using the recently added Eager Execution mode, and storing any iterator values that you need to reuse in a local variable in your Python code."]}, {"number": 18817, "title": "docs: Clean up install_linux with pip", "body": "Cleans up the 'How to Install TensorFlow' section using pip in virtualenv and system. Uses virtualenv in a typical workflow.\r\nFixes #18767\r\n", "comments": ["Staged: go/tfo-stage/install/install_linux", "Updated your suggestion to include python3 -V\r\n\r\nBefore I leave this doc, I went ahead and moved the GPU section that started the doc below the installation procedures. It seemed a little off putting right at the start, especially since we recommend installing the CPU version to start.\r\n\r\nStaged."]}, {"number": 18816, "title": "Which version of Cudnn is compatible with cuda 8.0?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["https://developer.nvidia.com/rdp/cudnn-archive"]}, {"number": 18815, "title": "tf.image.extract_glimpse not work as expected", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 2.7.14\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 8.0/7.0\r\n- **GPU model and memory**: GTX1080, 8G\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**: python test_script.py\r\n\r\n### Describe the problem\r\nThe output for the code below, is not correct. To me, the correct output should be: [0.   1.    2.    3.   4.  ]. The offset is [0, 0], and the centered is false, so the starting point is at the most upper left pixel (with the value 0). And the glimpse height is 1 and width is 5, so the glimpse should be the first row. \r\n\r\n### Source code / logs\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput_img = np.float32(np.arange(25).reshape((5, 5)))\r\nprint input_img\r\n\r\ninput_img = tf.expand_dims(input_img, 0)\r\ninput_img = tf.expand_dims(input_img, -1)\r\n\r\noffset_ = tf.expand_dims([0.0, 0.0], 0)\r\nfirst_glimpse = tf.image.extract_glimpse(input_img, [1, 5], offset_,\r\n                                    centered=False, normalized=False, uniform_noise=False)\r\nfirst_glimpse = tf.squeeze(first_glimpse)\r\nsess = tf.Session()\r\nprint first_glimpse.eval(session=sess)\r\n\r\nOutput:\r\n[[ 0.  1.  2.  3.  4.]\r\n [ 5.  6.  7.  8.  9.]\r\n [10. 11. 12. 13. 14.]\r\n [15. 16. 17. 18. 19.]\r\n [20. 21. 22. 23. 24.]]\r\n[16.995655 12.920303  0.        1.        2.      ]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "Updated. Thanks.", "@YantianZha Hi, perhaps you mistake the `offset`, where is the center of each window. See its [API](https://www.tensorflow.org/versions/master/api_docs/python/tf/image/extract_glimpse):\r\n\r\n> A 2-D integer tensor of shape [batch_size, 2] containing the y, x locations of the **center** of each window.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 18814, "title": "Raise error on separable convolution and depthwise convolution with strides > 1 and rate > 1", "body": "In documents, `separable_con2d` and `depthwise_conv2d` should not accept `strides > 1` and `rate > 1` at the same time. But they don't have any code to check these parameters, so it worked in a wrong way.\r\n", "comments": ["Also, a test would be nice!", "Nagging Assignee @ekelsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 74 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 89 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ekelsen: It has been 104 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of activity. @Corea feel free to resend this as a different PR when you have time to address @drpngx comments."]}, {"number": 18812, "title": "Patches for Release 1.7", "body": "", "comments": ["Android test seems to be a flake, but rerunning to confirm.", "Ignoring Windows Bazel failure for 1.7 merges."]}, {"number": 18811, "title": "Patches for Release 1.6", "body": "", "comments": ["@caisq we notice that gradients_test.py is failing for testWarnings on multiple PRs. Mind if I cherrypick a fix to the r1.6 branch? I disabled it for 1.6 for python 3.5 but it might be affecting other versions as well.", "No objections :) ", "https://github.com/tensorflow/tensorflow/pull/19189 I think this might do the trick.", "GPU Python 3 fixed in https://github.com/tensorflow/tensorflow/pull/19365"]}, {"number": 18810, "title": "Patches for Release 1.5", "body": "", "comments": ["Nagging Assignee @ekelsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18809, "title": "Update install_linux.md", "body": "", "comments": []}, {"number": 18808, "title": "Place data format op on CPU:0.", "body": "Cherrypicking cl/192302833", "comments": ["Agreed that this matches the already reviewed cl/192302833."]}, {"number": 18807, "title": "OpKernel not registered, despite being listed in OpRegistry", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:macOS High Sierra 10.13.3\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7.14\r\n- **Bazel version (if compiling from source)**: 0.12.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Xcode 9.3: Apple LLVM version 9.1.0 (clang-902.0.39.1)\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `tensorflow/contrib/makefile/build_all_ios.sh -g /path/to/model.p`\r\n\r\n### Describe the problem\r\nI have a ListDiff op in my model; as such, the generated `ops_to_register` file includes the following bits (edited way down to fit nicely):\r\n\r\n```c\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"ListDiff\")\r\n  ;\r\n}\r\n```\r\n```c\r\nconstexpr const char* kNecessaryOpKernelClasses[] = {\r\n  \"ListDiffOp<::tensorflow::int32, int32>\",\r\n};\r\n```\r\n\r\nHowever, when I try to load the model, it gives:\r\n```\r\nError adding graph to session: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: rgb_to_grayscale/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](rgb_to_grayscale/Tensordot/range, rgb_to_grayscale/Tensordot/add_1)]]\r\n```\r\n\r\nIf I print the full list of registered ops via `tensorflow::OpRegistry::Global()->DebugString(false)`, the line for ListDiff is (line breaks added for readability):\r\n\r\n```\r\nOp<\r\n    name=ListDiff; \r\n    signature=x:T, y:T -> out:T, idx:out_idx; \r\n    attr=T:type; \r\n    attr=out_idx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]\r\n>\r\n```\r\n\r\nThe only thing I can think of here is that `T:type` has no allowable values (I would expect to see `DT_INT32` given my `ops_to_register.h` file). However in a working build before I added `ListDiff`,  I had a few ops that has no listed allowable values (`Reshape` and `Squeeze`, in this case). I've read through the code that registers the `ListDiff` op and couldn't find anything out of the ordinary, but I'm a fairly novice Tensorflower.\r\n\r\nPossibly related to #15732?\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["This appears to be a bug in Tensorflow 1.7; when I compiled with TF 1.6, everything worked as expected. The output of `tensorflow::OpRegistry::Global()->DebugString(false)` is identical, but no error is thrown when loading the model.", "> This appears to be a bug in Tensorflow 1.7; when I compiled with TF 1.6, everything worked as expected. The output of tensorflow::OpRegistry::Global()->DebugString(false) is identical, but no error is thrown when loading the model.\r\n\r\nTF 1.6 worked on a cut-down version of my model that excluded `ListDiff` but included `Conv2D` where TF 1.7 has the same issue as described above. But TF 1.6 still can't find an op kernel for `ListDiff` even though it's listed in the op registry.", "@satok16 @aselle any idea?", "This issue also seems to exist using TF 1.8 (with the same setup as @sadlerjw ).", "Can you verify that listdiff_op.cc is being compiled and included in the binary? It looks like the op is being included but not the kernels to execute the op. ", "@aselle I'm not sure how to verify that - I'm not much of a compiler expert. Could you please provide instructions? Thanks!", "So, you can do it a number of ways. Perhaps the easiest is to find the registration functions list_diff.cc and insert a print\r\n```cpp\r\n#include <iostream>\r\nstatic int forced_build = []() {\r\n  std::cout<<(\"I have this module\\n\")<<std::endl;\r\n  return 5;\r\n}\r\n```\r\nMake sure you see the printf when you load the tensorflow library you built.\r\n", "For anybody landing here via search -- some characters got lost in the debug print code snippet:\r\n\r\n> ```c++\r\n> #include <iostream>\r\n> static int forced_build = []() {\r\n>   std::cout<<(\"I have this module\\n\")<<std::endl;\r\n>   return 5;\r\n> }();\r\n> ```\r\n\r\n", "As contrib is deprecated, Please google the name of the module without the tf.contrib part to know it's new location and thus migrating your code accordingly by correcting the import statement.\r\n\r\n"]}, {"number": 18806, "title": "Fixed Typos", "body": "- Used https://github.com/client9/misspell to identify and fix typos throughout the repo\r\n- Thanks for all the hard work!\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/tensorflow/tensorflow/18806)\n<!-- Reviewable:end -->\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "LGTM, except for the one case where I think the suggestion should be reverted.", "@ekelsen https://github.com/tensorflow/tensorflow/pull/18806/commits/c86e47c402e7bb347389d4690618ee5f8e11dab7 reverts the overzealous change \ud83d\ude1b", "Ping @nnadeau ", "@av8ramit @caisq extra 's' removed in 08a3319"]}, {"number": 18805, "title": "Add uint32 and uint64 support with tf.train.batch", "body": "This fix tries to address the issue raised in #18586 to have uint32 and uint64 support with tf.train.batch.\r\n\r\nThis fix add uint32 and uint64 to `CopyElementToSlice` for the support. This fix also adds some test cases.\r\n\r\nThis fix fixes #18586.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18804, "title": "R1.8 merge back to master after 1.8.0-rc1", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 18803, "title": "Let cosine_distance work across multiple axes", "body": "This change lets users use `cosine_distance` to directly compute the cosine distance between n-dimensional tensors. Currently this requires reshaping the input tensors to 1D before calling `cosine_distance`. Other loss functions don't require reshaping, so it would be nice to remove this little bit of cognitive overhead.\r\n\r\nI also added tests for `cosine_distance`, for both 1- and 2-dimensional cases.", "comments": ["It has been 20 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 49 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 18802, "title": "Training iteration stops silently if not Debugging", "body": "Have I written custom code: Yes\r\nOS Platform and Distribution: Windows server 2012 R2 Standard\r\nTensorFlow installed from: Pip repositories\r\nTensorFlow version: 1.7.0\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce NA\r\n\r\n\r\nI'm attempting to train a tensorflow.contrib.keras.model.Sequencial on an image dataset. I have noticed however that when running the script normally, everything appears to work fine until the process does not exit. When I ran the scrip in debug mode, I received the message \"StopIteration: Could not import PIL.Image. The use of 'array_to_img' requires PIL\". The error is pretty self-explanatory, however is it intended behavior to fail silently like this?  I did not realize at first that the execution halted without exiting, until sometime passed.\r\n\r\nThis is my model initialization and training block:\r\n \r\n        model = tf.contrib.keras.models.Sequential()\r\n        model.add(tf.contrib.keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                         activation='relu',\r\n                         input_shape=data_train.image_shape))\r\n        model.add(tf.contrib.keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\n        model.add(tf.contrib.keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\n        model.add(tf.contrib.keras.layers.Dropout(0.25))\r\n        model.add(tf.contrib.keras.layers.Flatten())\r\n        model.add(tf.contrib.keras.layers.Dense(128, activation='relu'))\r\n        model.add(tf.contrib.keras.layers.Dropout(0.5))\r\n        model.add(tf.contrib.keras.layers.Dense(4, activation='softmax'))\r\n        \r\n        model.compile(loss=tf.contrib.keras.losses.categorical_crossentropy,\r\n                      optimizer=tf.contrib.keras.optimizers.Adadelta(),\r\n                      metrics=['accuracy'])\r\n\r\n        model.fit_generator(data_train,\r\n                            steps_per_epoch=100,\r\n                            epochs=5,\r\n                            validation_data=data_test,\r\n                            validation_steps= data_test.batch_size)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@fchollet Any comments on the intended behavior here?", "Nagging Assignees @karmel, @fchollet: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @karmel, @fchollet: It has been 33 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @karmel, @fchollet: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @karmel, @fchollet: It has been 63 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @karmel, @fchollet: It has been 78 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@omalleyt12 -- is this something you can look into?", "@karmel yep I'll take a look", "@iustin94 is ```data_train``` a subclass of keras.Sequence? I'm guessing this has to do with multiprocessing", "@omalleyt12  I can't answer this question as I don't have access to that repository currently. You might be right, I recall we were attempting to use multiprocessing however we couldn't see it in the process monitor so we suspected we were doing something wrong.", "I tried to recreate the lack of error situation with the code below but this seems to be (correctly) throwing an error for me:\r\n\r\n```python \r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nkeras = tf.keras\r\n\r\nclass MySequence(keras.utils.Sequence):\r\n  def __getitem__(self, x):\r\n    if x == 1:\r\n      raise ValueError('Should raise an error.')\r\n    return np.ones([10,100,100,3]), np.ones([10,4])\r\n\r\n  def __len__(self):\r\n    return 3\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                              activation='relu',\r\n                              input_shape=(100,100,3)))\r\nmodel.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(keras.layers.Dropout(0.25))\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(128, activation='relu'))\r\nmodel.add(keras.layers.Dropout(0.5))\r\nmodel.add(keras.layers.Dense(4, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit_generator(MySequence(),\r\n                    steps_per_epoch=3,\r\n                    epochs=1)\r\n```\r\n\r\nOn the ```multiprocessing``` bit, I looked into it more and the code you had will use ```multithreading``` and not ```multiprocessing``` to grab the input data, which may explain why you weren't seeing anything in the process monitor. If you want Keras to use ```multiprocessing``` with a generator/Sequence, you can do:\r\n\r\n```python\r\nmodel.fit_generator(MySequence(),\r\n                    steps_per_epoch=3,\r\n                    epochs=1,\r\n                    use_multiprocessing=True)\r\n```\r\n\r\nLet me know if there's any more context you can provide, otherwise I'm not sure if I'll be able to recreate this issue\r\n", "Closing for now, this issue may have been fixed since this bug was reported"]}, {"number": 18801, "title": "R1.8 merge back to master after 1.8.0-rc1 ", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 18800, "title": "tenserflow from source with cpu", "body": "HI, \r\nI ask if I can install tensorflow from source on cpu machine for converting tf model to tflite model for mobile \r\nthank you very much in advance. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 32 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}]