[{"number": 2744, "title": "Fix for build issue #2742;", "body": "(1) grabs build info from eigen.BUILD (rather than using a specified hash);\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Awesome, thanks for putting this together! One minor request - could you add a one-line comment in both places explaining why we're grabbing the hash from the file, something like \"Pull the current Eigen version name from the Bazel build file\"? Thanks again for doing this!\n", "@petewarden Done;\n", "unfortunately there are some conflicts :(\n", "I'm going to create a new pull request; Created a new pull request (2749).\n", "Thanks Gregory, and apologies, I'm guessing my manual PR updating the Eigen hash caused the conflicts.\n"]}, {"number": 2743, "title": "Updated Pi documentation and Eigen version in makefile", "body": "", "comments": []}, {"number": 2742, "title": "Ubuntu 16.04 Makefile build", "body": "I was attempting to use the Makefile to test build the library on my Ubuntu system (bazel builds to completion).\n\n```\nuname -a -m\nLinux gking-ml-vm 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n### Environment info\n\nOperating System:\n\n```\nlsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04 LTS\nRelease:    16.04\nCodename:   xenial\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls -l /path/to/cuda/lib/libcud*\nls: cannot access '/path/to/cuda/lib/libcud*': No such file or directory\n```\n\nIf installed from sources, provide the commit hash:\n\n```\ngit show\ncommit 6431560b7ec3565154cb9cdc9c827db78ccfebe7\nMerge: a0085c8 b7c4169\nAuthor: Vijay Vasudevan <vrv@google.com>\nDate:   Tue Jun 7 11:32:10 2016 -0700\n\n    Merge pull request #2710 from vrv/branch_124251558\n\n    Branch 124251558\n```\n### Steps to reproduce\n\n1.)  `bash tensorflow/contrib/makefile/download_dependencies.sh`\n2.) `make -f tensorflow/contrib/makefile/Makefile all`\n**Fails here**: because protoc installed for Ubuntu 16.04 is `2.6.1`;\n\n```\nmake -f tensorflow/contrib/makefile/Makefile all\nprotoc  tensorflow/core/util/test_log.proto --cpp_out /home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/proto/\ntensorflow/core/util/test_log.proto:2:10: Unrecognized syntax identifier \"proto3\".  This parser only recognizes \"proto2\".\ntensorflow/contrib/makefile/Makefile:325: recipe for target '/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc' failed\nmake: *** [/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/proto/tensorflow/core/util/test_log.pb.cc] Error 1\n```\n\n3.) Even with an updated `protoc` build fails again:\n\n```\ngcc --std=c++11 -I/usr/local/include -I. -I/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-d02e6a705c30 -I/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -c tensorflow/core/lib/strings/strcat.cc -o /home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/strings/strcat.o\nIn file included from tensorflow/core/lib/strings/strcat.cc:23:0:\n./third_party/eigen3/Eigen/Core:1:47: fatal error: eigen-eigen-0c0b79ecd74c/Eigen/Core: No such file or directory\ncompilation terminated.\ntensorflow/contrib/makefile/Makefile:350: recipe for target '/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/strings/strcat.o' failed\nmake: *** [/home/gking/Programming/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/strings/strcat.o] Error 1\n```\n### What have you tried?\n1. Built a new version of `protoc`:\n\n**What I did (note the dependencies given to fpm aren't correct);**:\n\n```\nwget https://github.com/google/protobuf/archive/v3.0.0-beta-3.tar.gz\ntar xvzf v3.0.0-beta-3.tar.gz\ncd protobuf-3.0.0-beta-3\n./autogen.sh\n./configure --prefix=/usr\nmake all\nmake ctags\nmake check\nmake install DESTDIR=/tmp/installdir\nfpm -s dir -t deb -p protobufc_3.0.0-beta3_amd64.deb -n protobufc -v 3.0.0-beta3 -d \"libgcc1 >= 1:4.1.1\" -d \"libstdc++6 >= 5.2\" -C /tmp/installdir usr/bin usr/lib usr/include\n```\n\nAfter purging `protobuf-compiler` and other related packages I installed the new `fpm` built package (`sudo dpkg --install protobufc_3.0.0-beta3_amd64.deb`)\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n**EDIT:** Added some additional details.\n", "comments": ["It looks like Eigen doesn't exist.  Might that be the problem?\n", "I have `libeigen3-dev` installed through `apt`:\n\n```\ndpkg -s libeigen3-dev\nPackage: libeigen3-dev\nStatus: install ok installed\nPriority: extra\nSection: libdevel\nInstalled-Size: 5130\nMaintainer: Ubuntu Developers <ubuntu-devel-discuss@lists.ubuntu.com>\nArchitecture: all\nSource: eigen3\nVersion: 3.3~beta1-2\nDepends: pkg-config\nSuggests: libeigen3-doc, libmrpt-dev\nDescription: lightweight C++ template library for linear algebra\n Eigen 3 is a lightweight C++ template library for vector and matrix math,\n a.k.a. linear algebra.\n .\n Unlike most other linear algebra libraries, Eigen 3 focuses on the simple\n mathematical needs of applications: games and other OpenGL apps, spreadsheets\n and other office apps, etc. Eigen 3 is dedicated to providing optimal speed\n with GCC. A lot of improvements since 2-nd version of Eigen.\nOriginal-Maintainer: Debian Science Maintainers <debian-science-maintainers@lists.alioth.debian.org>\nHomepage: http://eigen.tuxfamily.org\n```\n\nHowever it appears that the download dependencies uses a different `eigen` hash (than the `bazel` build). Below is from, `tensorflow/contrib/makefile/download_dependencies.sh` (truncated output):\n\n```\nEIGEN_HASH=d02e6a705c30\ncurl \"https://bitbucket.org/eigen/eigen/get/${EIGEN_HASH}.tar.gz\" \\\n-o /tmp/eigen-${EIGEN_HASH}.tar.gz\ntar xzf /tmp/eigen-${EIGEN_HASH}.tar.gz -C ${DOWNLOADS_DIR}\n```\n", "That hash looks different than from your message above, which says 0c0b79ecd74c.\n", "Yup. So it looks like the `Makefile` build rules are finding the wrong header file to include while building.\n\n```\ncat tensorflow/contrib/makefile/Makefile | grep HASH\nEIGEN_HASH := d02e6a705c30\n-I$(MAKEFILE_DIR)/downloads/eigen-eigen-$(EIGEN_HASH) \\\n-I$(MAKEFILE_DIR)/downloads/eigen-eigen-$(EIGEN_HASH) \\\n```\n\n```\ncat tensorflow/contrib/makefile/download_dependencies.sh | grep HASH\nEIGEN_HASH=d02e6a705c30\ncurl \"https://bitbucket.org/eigen/eigen/get/${EIGEN_HASH}.tar.gz\" \\\n-o /tmp/eigen-${EIGEN_HASH}.tar.gz\ntar xzf /tmp/eigen-${EIGEN_HASH}.tar.gz -C ${DOWNLOADS_DIR}\n\n```\n\nBut:\n\n```\ncat third_party/eigen3/Eigen/Core \n#include \"eigen-eigen-0c0b79ecd74c/Eigen/Core\"\n```\n\n**EDIT:** Added details;\n", "@martinwicke Do we support the Makefile build actively, or is this contributions welcome?\n", "FYI @petewarden\n\nIt's contributions welcome. Definitely since we cannot guarantee that a\ndifferent version of eigen will work at all. In fact it's quite likely it\nwill not (or work slowly).\nOn Wed, Jun 8, 2016 at 16:36 Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke Do we support the Makefile\n> build actively, or is this contributions welcome?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2742#issuecomment-224762880,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_bBDHVepl5QBpiSamg8hinQtjQnuks5qJ1HxgaJpZM4IxdOc\n> .\n", "Sorry about the problem! I have a pending PR at https://github.com/tensorflow/tensorflow/pull/2743 that fixes this particular issue. Longer-term, I want to automatically grab the checked-in Eigen version, maybe just by grepping the Bazel workspace file that holds the hash.\n", "@petewarden I created a quick pull request that should fix this issue by grabbing the information from eigen.BUILD; \n", "Closing the issue; should work properly now (provided you have a more recent version of `protoc`.\n"]}, {"number": 2741, "title": "Does max_pooling3d support strides that are not same along height, width and depth?", "body": "I am using tensorflow to train a network that has 3D convolutions and 3D max-pooling layers. Some of the 3D max-pooling layers use a stride of 1 along the depth and strides of 2 along height and depth. This seems to be causing issues.\n\nFor example: The following script should print a tensor with values of '82' but somehow some of the values turn out to be -3.40282347e+38 \n\n**import tensorflow as tf\n\nsess = tf.InteractiveSession()\nX = tf.Variable(tf.ones([8,16,112,112,3]))\nW = tf.Variable(tf.ones([3,3,3,3,64]))\nb = tf.Variable(tf.ones([64]))\nout = tf.nn.relu(tf.nn.bias_add(tf.nn.conv3d(X, W, strides=[1,1,2,2,1], padding='VALID'),b))\nout1 = tf.nn.max_pool3d(out, [1,2,3,3,1], strides=[1,1,2,2,1], padding='VALID')\nsess.run(tf.initialize_all_variables())\n\nprint out1.eval()**\n\nAdditional notes:\n- I looked into the tensorflow/python/kernel_tests/pooling_ops_3d_test.py file and all the test cases seem to have the same stride across all dimensions. \n", "comments": ["It seems to be fixed in the latest release 0.9.0\n"]}, {"number": 2740, "title": "ExponentialMovingAverage.average duplicates the current scope name", "body": "Using tensorflow nightly.\n\n``` python\nimport tensorflow as tf\nwith tf.name_scope('scope'):\n    x = tf.Variable(42, dtype=tf.float32)\n    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n    apply_op = ema.apply([x])\n    average = ema.average(x)\n    print average.name   # 'scope/scope/Variable/ExponentialMovingAverage:0'\n    print ema.average_name(x)  # 'scope/Variable/ExponentialMovingAverage'\n```\n", "comments": ["The problem seems to come from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L83, where the name of an op is used to create a variable. But the name of the op already contain the current scope, therefore the variable to create will contain the scope twice.\n", "It's been a while. I suppose this is a bug right?..\n", "Yes that looks like a bug. I'm asking the author of the code to do the quick fix.\n", "Unfortunately, after checking internally, fixing this bug would result in losing backward compatibility, which we would like to avoid both for internal and external users. So, right now, I am closing the issue.\n@girving, @mrry\n", "Isn't it true that ExponentialMovingAverage is used currently almost always outside any scopes? It shouldn't break (many) existing use cases. If I remember correctly, the class simply doesn't work well if the scope name is duplicated and users have to do manual hacking to fix the variable names.\n\nThe workaround that I'm using currently is to wrap the class with\n\n``` python\nwith tf.name_scope(None):\n   ema = tf.train.ExponentialMovingAverage(...)\n   # [rest of the code]\n```\n", "It's not just ExponentialMovingAverage, but also the accumulators used by optimizers that have this behavior. Anyone who's using either of these inside a scope of any kind would suffer backwards-incompatible checkpoints due to the naming difference. Especially given there is an easy workaround (i.e. do not create / use ExponentialMovingAverage or Optimizers inside a name scope), the pain that fixing this would cause outweighs the benefits.\n", "We don't need to share the `create_slot` function between EMA and optimizers. It's essentially just a variable creation.\n", "Two notes: \n1. `create_slots` is more than just a variable creation -- there's some subtle logic in there to deal with properly supporting slots for partitioned variables. So I'd think reusing the logic makes sense.\n2. Even if we were to only fix this for `ExponentialMovingAverage`, it would still be a backwards-incompatible graph change for anyone who was using EMA inside a name scope.\n", "Is it possible to keep the compatibility for now but print a warning about\r\na future change that will break compatibility (similar to numpy) ?\r\n", "@ppwwyyxx I'm not sure I follow.  How would we be able to safely break backwards compatibility in the future?  An advance warning that models will break is not a sufficient fix.\n", "I mean it will provide a time window for people to make necessary changes to prepare for a future break. But right it won't safely break compatibility.\n", "@ppwwyyxx I don't think it is worth making this change.  The name fix would be nice, and I agree that the current code is a mistake, but there is a high bar for breaking existing models.\n", "On latest tensorflow, the `with tf.name_scope(None)` hack still introduces variables with duplicated scope name:\r\n```python\r\ndef f():\r\n    v = tf.get_variable('W', [1])\r\n    v = v + 1\r\n    with tf.name_scope(None):\r\n        ema = tf.train.ExponentialMovingAverage(decay=0.9, name='EMA')\r\n        emaop = ema.apply([v])\r\n        average_v = ema.average(v)\r\n\r\nwith tf.variable_scope('scope'):\r\n    f()\r\nprint([k.name for k in tf.global_variables()])\r\n```\r\nwill print:\r\n```\r\n[u'scope/W:0', u'scope/add/EMA:0', u'scope/scope/add/EMA/biased:0', \r\nu'scope/scope/add/EMA/local_step:0']\r\n```", "The recently-introduced new variables in EMA also brings error when using with reuse=True.\r\nThe example below seems like a common pattern in batch normalization. It works before, but now:\r\n```python\r\ndef f(v):\r\n    ema = tf.train.ExponentialMovingAverage(0.9)\r\n    vema = ema.apply([v])\r\n    return vema\r\n\r\nwith tf.variable_scope('s'):\r\n    v1 = tf.get_variable('W', shape=[])\r\n    v1 = v1 + 1\r\n    f(v1)\r\nwith tf.variable_scope('s', reuse=True):\r\n    v2 = tf.get_variable('W', shape=[])\r\n    v2 = v2 + 1\r\n    f(v2)\r\n```\r\n```\r\nValueError: Variable s/s_1/s_1/add/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```", "There are a number of TF paradigms that previously worked because ExponentialMovingAverage didn't respect variable scopes (see https://github.com/tensorflow/tensorflow/issues/5652, for example). In the case you have here, I'm not sure that I understand the semantics of what you are trying to do: regardless of debiasing, it is an error to call apply on the same variable multiple times: \"ValueError: If the moving average of one of the variables is already being computed.\" (from `ExponentialMovingAverage` description of `apply`). Are you arguing that the wrong error message is thrown in this case?", "@joel-shor Oh. You can change 'W' to 'W2' and move it outside of the scope, it still throws error. I'm only trying to point out that EMA cannot work inside reuse=True. \r\nBy the way I don't think I'm calling EMA on the same tensors, because calling `add` twice creates two `tf.Operation`.", "1. The issue should be fixed currently; zero_debias is no longer the default in ExponentialMovingAverage.\r\n2. There is a strong possibility that is will be the default in the future.\r\n3. ExponentialMovingAverage create variables, so for future safety you might consider using it as if it respected variable scopes (ie don't put it in resuse=True variable scopes that it doesn't need to be in)", "@joel-shor Thanks. Since there are dedicated issues for the bug we can use them to track. This issue is more about the naming -- and I see there are still very long variable names with duplicated scopes for the newly-introduced biased/local_step variables. Should there be a fix for this?\r\n```python\r\ndef f(v):\r\n    ema = tf.train.ExponentialMovingAverage(0.9)\r\n    vema = ema.apply([v])\r\n    return vema\r\nwith tf.variable_scope('s'):\r\n    v1 = tf.get_variable('W', shape=[10,10,10,10])\r\n    v1 = v1 + 1\r\n    f(v1)\r\nprint [k.name for k in tf.all_variables()]\r\n# [u's/W:0', u's/s/add/ExponentialMovingAverage:0', u's/s/s/add/ExponentialMovingAverage/biased:0', u's/s/s/add/ExponentialMovingAverage/local_step:0']\r\n```", "We are not going to fix this behavior. The reason is that there are two relevant scopes: the scope that the variable was created in, and the scope that the EMA was created in. There will be potential ambiguity if one is removed. Since removing the duplication isn't clearly better, we're going to keep this behavior.", "Hi @joel-shor,  I am facing the same problem while trying to reuse batch-normalization in a network which is used twice. I get error \"/convnet_branch/conv1_1_bn/moments/Squeeze/ExponentialMovingAverage/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\"\r\n\r\nCan you suggest how to deal with this problem, I read all the above comments but couldn't figure out how it could be solved.\r\n\r\nThanks a lot!", "Hi @nainadhingra2012. On tensorflow 1.12.0, I had the same problem and fixed it by adding the line: \r\n\r\n            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n\r\nbefore ema.apply"]}, {"number": 2739, "title": "Updating the setup web page for GPU to use primary with nvidia-docker then docker_run_gpu as alternative", "body": "Updating the website setup page to align with `tools/docker/README.md`` by adding description to use nvidia-docker first before trying docker_run_gpu script for GPU scenario.\n\nAlso fix minor grammar by add `is` when explaining the port mapping. \n", "comments": ["Can one of the admins verify this patch?\n", "Sorry, there are conflicts in this file now, can you rebase?  This looks good, though it would be a good idea one day to avoid having duplicate copies of this text -- maybe one should reference the other.\n", "@hsaputra I'm sorry for not looking at this sooner. My bad!\n\nWe will merge #2795 to make it into r0.9. Please take a look at it and rebase your PR on it. There is still the typo and the os_setup.md may deserve more words about gpu docker than #2795 contains.\n", "No worries. Unfortunately I am out in the woods with limited access to Internet, so I will resolve the conflict and rebase by late Monday\n", "I updated the PR by rebasing from master, please take a look. Thanks guys!\n", "@tensorflow-jenkins test this please\n", "Sorry, one more conflict has to be resolved.  Any reason to use latest-gpu instead of just -gpu? \n\n(Does anybody know the difference?)\n", "Sure, will rebase to resolve conflict. \n\nAs for latest-gpu, I believe it was the label used for Tensorflow GPU support (see https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html)\n\nI could use gpu instead of latest-gpu if that is the right label. \n", "Ah, that's latest-gpu is fine then.\n", "Thanks for merging, @vrv \n"]}, {"number": 2738, "title": "Branch 124381148", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2737, "title": "Dynamically shaped image support for `tf.image.resize_image_with_crop_or_pad`", "body": "This patch is intended for issue https://github.com/tensorflow/tensorflow/issues/521\n\nA new keyword argument `dynamic_shape=False` is added to these functions in `tensorflow.python.image_ops`:\n`crop_to_bounding_box` `pad_to_bounding_box` `resize_image_with_crop_or_pad` `_ImageDimensions`\n\nThe change is fully backward compatible. The new functionality of `resize_image_with_crop_or_pad` is tested and worked well. However, the dynamic version of `crop_to_bounding_box` `pad_to_bounding_box` is missing many argument checks compared to the static version.\n\nKeeping both a dynamic and a static version of the same function is quite ugly. Maybe we should keep only the dynamic version?\n\n**TODO**\nTest cases\n", "comments": ["Can one of the admins verify this patch?\n", "This is great. Can you add tests for this? I think having the ability to use static checks is useful (sometimes), and backwards compatibility is nice, so I don't think you should remove he static checks.\n", "@martinwicke Should these functions simply skip checks that cannot be performed at graph construction time? That would eliminate the need  of a new argument.\n", "That would mean that tensors of unknown size will pass all the static tests? That's a good idea. I think the danger is minimal here. The errors in the resize kernels are not too bad anyway.\n", "@gaohuazuo you'll have to ping us when this is ready for review\n", "@martinwicke Ready for review.\n", "Thanks! Added bunch of comments. Mainly: Some doc nits, some nit nits, and, could you add another test that assertRaises appropriately if you pass in illegal values, and with also asserts that if you pass in tensors which have illegal values it still fails (although not with ValueError, but probably with OpError).\n", "@martinwicke Test cases updated. Also added corresponding run time check for every static check, though I'm not sure if these checks worth the increased complexity in the graph.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "There are some failing tests in this, can you check?\n", "@martinwicke `//tensorflow/python:image_ops_test` and `//tensorflow/contrib/framework:tensor_util_test` fixed. There are other failing tests, such as `//tensorflow/core/platform/cloud:oauth_client_test`, but I think they are irrelevant to the PR.\n", "Sorry, apparently we accumulated conflicts. Can you rebase?\n", "@martinwicke The conflict was caused by `import` statements inserted into the same location. Fixed.\n", "It looks like there are still conflicts. Can you check?\n", "Conflicts resolved.\n", "Jenkins, test this please\n", "Excellent! Thank you!\n"]}, {"number": 2736, "title": "Installation error on Ubuntu 16.04 64bit machine - $(PYTHON_BIN_PATH) not defined.", "body": "For bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\n```\nuname -a\nLinux ubuntu 4.4.0-21-generic #37-Ubuntu SMP Mon Apr 18 18:33:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\nCUDA not installed\n\nIf installed from sources, provide the commit hash:\nhttps://github.com/tensorflow/tensorflow/commit/592675b2b8d1cabbf923638942ea6f200abe353a\n### Steps to reproduce\n1. Install the source using `bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\n2. The necessary packages are installed but the error `in cmd attribute of genrule rule //tensorflow/contrib/session_bundle/example:half_plus_two: $(PYTHON_BIN_PATH) not defined.` is raised.\n3. The fork is on par with the upstream master.\n### What have you tried?\n1. Have tried to search for similar issues, but couldn't find anything. Please point to the correct issue in case I am missing some existing issue.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\nExtracting Bazel installation...\n.........\nINFO: Waiting for response from Bazel server (pid 8938)...\nWARNING: /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/protobuf/WORKSPACE:1: Workspace name in /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\nWARNING: /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/highwayhash/WORKSPACE:1: Workspace name in /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/highwayhash/WORKSPACE (@__main__) does not match the name given in the repository's definition (@highwayhash); this will cause a build error in future versions.\nWARNING: /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/re2/WORKSPACE:1: Workspace name in /home/maniteja/.cache/bazel/_bazel_maniteja/98850693c9ebf4963f7f6ed5da9dc97a/external/re2/WORKSPACE (@__main__) does not match the name given in the repository's definition (@re2); this will cause a build error in future versions.\nWARNING: /home/maniteja/FOSS/tensorflow/util/python/BUILD:11:16: in includes attribute of cc_library rule //util/python:python_headers: 'python_include' resolves to 'util/python/python_include' not in 'third_party'. This will be an error in the future.\nERROR: /home/maniteja/FOSS/tensorflow/tensorflow/contrib/session_bundle/example/BUILD:45:9: in cmd attribute of genrule rule //tensorflow/contrib/session_bundle/example:half_plus_two: $(PYTHON_BIN_PATH) not defined.\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\nINFO: Elapsed time: 1356.436s\n```\n\nThanks for your time. Please let me know if any additional information is needed.\n", "comments": ["Did you run `configure`?\n", "Sorry my mistake. Thanks a lot. I didn't run `configure` this time though have been doing that when trying to install on 32 bit machine. Now it is building. Apologies for the noise.\n"]}, {"number": 2735, "title": "Branch 124370170", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2734, "title": "RNN: tensorflow.python.framework.errors.InvalidArgumentError: indices[15,0] = 10535 is not in [0, 10000)", "body": "Hey. I adapted the example file (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) to work on my training data. Unfortunately as soon as the training data exceeds a certain size I'm getting an error: http://pastebin.com/JkV7MCQk What could be the issue? How can I debug this?\n\nMight it be an issue with the training data? (http://s000.tinyupload.com/index.php?file_id=02347215266721316111)\n", "comments": ["Hey. Just saw that the vocabulary size is a parameter you have to define manually? Seems not very intuitive for me as it can be computed automatically.\n", "Hello! How do you get rid of these error? It seems I run into the same problem. Maybe you can share your experience ?", "@hauluk  from the above comment we can see that we must calculate size of vocabulary explicitly and then supply it as parameter, then we will not get this error. :). ", "The problem was the trained model can't find the word in the embedding matrix. That means I used a different vocabulary for training and prediction. Because of the fixed vocabulary I needed the same vocabulary for train and new data. So it is working for me now. Thanks.", "+1 for anyone using labels originated in another language (like Lua, R) that count or index starting with 1\r\n\r\n", "I got the same problem,and I have get the right answer.Thank you!", "@yufengzhixing \r\nI got the same problem, but I haven't solved the problem. Could you share your solution? Thank you!", "Hi!\n\nI didn't used the same data set for predicting and training. That was my\nsolution.\n\nHope i could help you.\n\nAm 25.07.2017 3:24 vorm. schrieb \"shijial\" <notifications@github.com>:\n\n> @yufengzhixing <https://github.com/yufengzhixing>\n> I got the same problem, but I haven't solved the problem. Could you share\n> your solution? Thank you!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2734#issuecomment-317600816>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AJQ96YSnN7aEHoS9YIMBmStav9UfFPesks5sRUPAgaJpZM4IxLIN>\n> .\n>\n", "It happens when a new label is present when predicting.", "I also came across the same error. Check [this](https://stackoverflow.com/questions/42567398/invalidargumenterror-see-above-for-traceback-indices1-10-is-not-in-0-10/52310338#52310338). ", "Why not calculate `vocab_size` automatically? Are there any reasons? \r\n\r\n`vocab_size` is actually calculated in https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py#L457\r\n```python3\r\ntrain_data, valid_data, test_data, _ = raw_data\r\n```\r\nbut it was ignored.\r\n\r\n", "It's also worth noting that I get a similar error if the vocab.tags.txt doesn't contain \"O\", even if the training doesn't use \"O\".", "for ngram model this will yield tremendous headaches exception for ngram-map was store in the model parameter, you cannot even delete the related tuples."]}, {"number": 2733, "title": "MNIST downloader doesn't work as expected", "body": "See: https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html\n\n> For your convenience, we've included a script which automatically downloads and imports the MNIST dataset. It will create a directory 'MNIST_data' in which to store the data files.\n\nLinked to this file: https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/mnist/input_data.py\n\nWhich currently doesn't create any folder.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Assigning to tf learn people, since this seems like a bug they should fix instead?\n", "@ilblackdragon this will likely break some tests. At the very least this change would need some changes to the examples? Maybe it's better to adjust the mnist tutorial text to change the reference to the input.\n", "@tensorflow-jenkins test this please!\n\n@martinwicke not sure if it will break tests, let's see. Indeed changing the tutorial text makes sense.\n", "Jenkins, test this please? For real?\n", "Indeed, some failures unfortunately.  For example:\n\nFAIL: //tensorflow/examples/tutorials/mnist:fully_connected_feed_test (see /private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/testlogs/tensorflow/examples/tutorials/mnist/fully_connected_feed_test/test.log).\nINFO: From Testing //tensorflow/examples/tutorials/mnist:fully_connected_feed_test:\n==================== Test output for //tensorflow/examples/tutorials/mnist:fully_connected_feed_test:\nTraceback (most recent call last):\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/examples/tutorials/mnist/fully_connected_feed_test.runfiles/tensorflow/examples/tutorials/mnist/fully_connected_feed.py\", line 27, in <module>\n    from tensorflow.examples.tutorials.mnist import input_data\n  File \"/private/var/tmp/_bazel_jenkins/13e370a18c169b19baeafefb05212b85/tensorflow-pull-requests-mac/bazel-out/local_darwin-opt/bin/tensorflow/examples/tutorials/mnist/fully_connected_feed_test.runfiles/tensorflow/examples/tutorials/mnist/input_data.py\", line 30, in <module>\n    from tensorflow.examples.tutorials.mnist import input_data\nImportError: cannot import name input_data\n", "The initial pull request was plain bad code (can't really describe it better -- shame on me). I fixed the problem with another commit, and I believe it should work now without crashing stuff.\n\nHowever, I also believe this pull request should be refused, and the tutorial should be corrected instead, just to make clear that the users must run the code to download the dataset themself -- the same code that is already there. The difference in the text is that the said script won't download anything, but will help with it.\n\nHere is why I think it should be refused: This script is used in 3 more files (fully_connected_feed.py, mnist_softmax.py and mnist_with_summaries.py). These files use flags to specify the directory in which the dataset should be saved. Saving the dataset in the current folder would be too strict, and the said scripts would still download it later on their own folders.\n\nTL;DL: It will work for the tutorial, but it will download the dataset without using it. Two command lines are enough, and they are already in the tutorial. Just change the text so it won't say that the script downloads things. Reject this pull request, please. :-)\n", "Sounds good -- we'll close this request.  Feel free to send us a PR to correct our tutorials as you best see fit!\n"]}, {"number": 2732, "title": "Mention that GPU reductions are nondeterministic in docs", "body": "# The problem\n\nI am trying out the [MNIST for experts tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts) and I have inconsistent results on the GPU.\n### What do I mean by inconsistent?\n\nWith the exactly same network parameters (and randomness removed: read below in the post) every time I run the complete train-then-test process the accuracy is slightly different.\n### What have I done to visualize this problem?\n\nFor each iteration, I have calculated the differences between the variables (weights, biases) from two _independent but identical_ runs and computed the L1 norm of those differences - <br/>\n- [plot](http://i.stack.imgur.com/W5PqZ.png) of L1 norm for the first 1000 iterations in steps of 20.\n\nIn a consistent world, these differences should be always zero! \n### How did I remove randomness in the code?\n- Removed dropout entirely\n- added a graph level seed (`tf.set_random_seed(1234)`). With this the variable initialization is deterministic and also any other randomization in the code. \n- The [MNIST for experts tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/pros/index.html#deep-mnist-for-experts) uses [this script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/mnist.py) to download/load the MNIST data. I have added `numpy.random.seed(3)` in `DataSet.__init__(self, images, labels, fake_data=False, one_hot=False, dtype=dtypes.float32)` in this script to remove randomness during the shuffling process (line 154 in `DataSet.next_batch(self, batch_size, fake_data=False)`)\n- `config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)` which goes into the creation of session as `sess = tf.Session(config=config)`\n### What system am I using?\n- tensorflow 0.8 gpu version (installed via pip)\n- OpenSUSE LEAP 42.1 (x86_64)\n- Cuda Toolkit 7.5\n- CuDNN 4.0\n- Tesla K20c card with Nvidia driver 352.79\n", "comments": ["Do you get deterministic results if you run this on CPU? Results of optimized GPU computations for NN ops are usually a little bit non-deterministic, I think it's a nature of how modern GPUs work, @zheng-xq may have more understanding of this\n", "I have the same observation on both GPU and CPU. I think whenever parallel computing is use (either multiple core CPU or GPU), the results will not be deterministic. This is due to the randomness of the order of collecting the partial results from all threads, and this can lead to very small difference within machine accuracy at the beginning and then this tiny tiny difference gets amplified during iterations. \n", "@yaroslavvb Yes I do get deterministic results on the CPU.\n\nThe problem is, in a bigger network that I have designed for another dataset the inconsistencies are quite large (up to +/-25% around the mean).\n", "@jiaboli007 I have seen that it is possible to have parallel computations and yet have deterministic behaviour (e.g.: Matlab simulink can optimize models for parallel computations yet assuring deterministic behaviour).\n", "On GPU, small amount of non-deterministic results is expected. TensorFlow uses the Eigen library, which uses Cuda atomics to implement reduction operations, such as tf.reduce_sum etc. Those operations are non-determnistical. Each operation can introduce a small difference. If your model is not stable, it could accumulate into large errors, after many steps. \n\nIf you see a large difference, after one or two operations, it would be problematic. Otherwise, it is somewhat expected. Regularizers such as dropout helps the model tolerate that. \n", "This is expected behavior: see #2652 for more discussion.\n", "@zheng-xq I have already tried dropout, it didn't help. After a bit of research over the internet, I have come to know that torch had a similar [non-deterministic behaviour](https://github.com/torch/cunn/issues/84) with their SpatialMaxPooling operation but it has been fixed now. Perhaps something along the lines?\n\n@girving Also on the [same post](https://github.com/torch/cunn/issues/84) one can read that Caffe has already acomplished deterministic backward passes. Having said these, and that this \"non-deterministic behaviour\" is neither fixed nor mentioned in the documentation yet, I would like to re-open the issue.\n", "Reopened: We'd be happy to accept a PR adding a note to this effect.\n", "@girving Thank you for re-opening the issue but I believe tensorflow could do better than just documenting this, what seems to be an unintended non-deterministic behaviour. Also, I would have loved to work on the PR but will be too much digression from my work at the moment.\n\nP.S.: Don't know if you saw my last edit - on the [same post](https://github.com/torch/cunn/issues/84) one can read that Caffe has already accomplished deterministic backward passes. Perhaps this is interesting to you!\n", "@girving Also we are not yet sure if it is actually the GPU reductions and not something else which is causing the non-deterministic behaviour - this need verification before allowing entry into the docs.\n", "@shiviser Let us know what you find out!\n", "@shiviser, that particular kernel was fixed to be determnistics. I believe TensorFlow picked up the same fix. However, other Cudnn conv algorithms are still non-deterministic, since they use atomics inherently. Most frameworks may encounter them depending on the input and kernel shapes. \n\nThat being said, if your investigation reveals something else that is causing the problem, a PR to address the doc and the code is welcome. \n", "BUILD\r\nTensorFlow GPU 1.1.0\r\nUbuntu 16.04\r\nCUDA 8.0.61\r\nCUDNN 5.1\r\nNVIDIA Drivers 375\r\n2 GTX 1080Ti, with 1 1080Ti used for 2 monitors. I set CUDA_VISIBLE_DEVICES=0 for all experiments. This uses the GPU with nothing attached.\r\n\r\n@zheng-xq why is the forward pass deterministic but the very first backward pass not deterministic?\r\n\r\nOn a simple MNIST example, the logits are 100% deterministic over 100 runs (I did more runs than that but plotted 100).\r\n\r\n![logits](https://cloud.githubusercontent.com/assets/6997460/25981433/a9473ee0-3707-11e7-8a60-240963c6c471.png)\r\n\r\nHowever, when we look at the gradients computed on the last set of variables, we see small errors of scale 1e-8.\r\n\r\n![3d](https://cloud.githubusercontent.com/assets/6997460/25981444/c9561e0e-3707-11e7-8ce9-20ced60186e2.png)\r\n\r\nI proceeded to do a simpler example of simply trying to train a neural network to add.\r\n\r\nInputs: 10x1 (numpy generated with same seed)\r\nWeights: 10x1 (random normal initialized with same seed)\r\nLabels: Sum of the 10x1 input.\r\n\r\nEffectively, the neural network is trying to tune the weights such that they all become 1.0.\r\n\r\nIn fact, it's strange because the randomness has some form of determinism, as seen in the graph below.\r\n\r\n![run1](https://cloud.githubusercontent.com/assets/6997460/25981528/4ea32fca-3708-11e7-98b0-a6abcfd58347.png)\r\n![run5](https://cloud.githubusercontent.com/assets/6997460/25981532/516018a4-3708-11e7-8712-6b3824a664cf.png)\r\n![run7](https://cloud.githubusercontent.com/assets/6997460/25981535/54733dc8-3708-11e7-9fe8-9911f64942c5.png)\r\n![run11](https://cloud.githubusercontent.com/assets/6997460/25981537/596a5de8-3708-11e7-83fe-b10af531501b.png)\r\n\r\nFurthermore, the anomalous gradients are consistently the same throughout the runs!\r\n\r\n![screenshot from 2017-05-12 11-45-15](https://cloud.githubusercontent.com/assets/6997460/25981559/7d6c0926-3708-11e7-849d-bc30beb2591f.png)\r\n\r\nAnd they have the exact same error.\r\n\r\n![screenshot from 2017-05-12 11-54-42](https://cloud.githubusercontent.com/assets/6997460/25981796/e71ee284-3709-11e7-91f2-e0777bd6f48b.png)\r\n\r\n@asimshankar mentioned [here](https://github.com/tensorflow/tensorflow/issues/5527) that mismatches between CUDA and Drivers could be the problem. So I upgraded my driver to 381, and then I got these results. 1 - 7 gradients have errors, but previously, only 5 had errors and when errors happened, these 5 gradients had exactly the same values.\r\n\r\n![11](https://cloud.githubusercontent.com/assets/6997460/25981589/b574d00a-3708-11e7-8fc9-22e0575ec94b.png)\r\n\r\nI haven't looked deeply into the exact 7 gradients that have errors but a brief glance showed that they are the same as the 5 before and more.\r\n\r\nCould it be the GPU reduce order? Or could there be errors in the computation of gradients themselves?\r\n If anyone has any insight into what experiments to try next, I'll be glad to do them and post the results here. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "You could try a simple feed-forward network with squared loss. Training can be implemented with only `matmul` and `reduce_sum`, if there's non-determinism there, then pretty much everything is potentially non-deterministic. Addition of floating point numbers is not-associative, so if the other of summing things together in matmul/reduce_sum changes, that can affect results. Typically this is not considered a bug as long as relative error of results stays within machine epsilon (1e-7 for float32). Note that multiplication changes the scale of relative error -- ie, if you have an initial error in result of 1e-7, and multiply result by 10^6, the absolute error blows up, but relative error stays the same", "@yaroslavvb I'll try that out and post some results here.\r\n@girving any other things to try so we can include in the docs? I'd be happy to do a PR for this if it's within my capabilities.", "@jkschin Not sure what you mean by other things.  If the PR is just adding a note that reductions are nondeterministic, keeping is small seems good.\r\n\r\nMore broadly, I'd love a determinism push to make everything we can actually deterministic, but that will take more work.", "@yaroslavvb I realized I'm already doing a simple feed-forward network above.\r\n\r\n```python\r\ndef add_model(v):\r\n    w = tf.get_variable(name='w', shape=[vector_size],\r\n            dtype=tf.float32,\r\n            initializer=tf.random_normal_initializer)\r\n    output = tf.reduce_sum(tf.multiply(w, v), axis=1)\r\n    tf.add_to_collection('weights', w)\r\n    return output\r\n```\r\n@girving yeah you mentioned adding a note to this effect. Where should it be added? Agree on a deterministic push!\r\n\r\n", "@jkschin as an end-user who got tripped up by non-determinism, you might have an idea where this note should go, so that other people like you would see it", "Would a post [here](https://www.tensorflow.org/performance/) titled \"Non-determinism in TensorFlow\" with some example code help?", "Yes!  @martinwicke: That a good place for an overview of TensorFlow nondeterminism?", "I think it should go into the programmer's guide section, but @wolffg has a better overview of where to put this. \r\n\r\nEither way, the content would be the same, and we'd be enthusiastic about it.", "Check out a [workaround](https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow) for training a simple fully-connected net with repeatable results on the GPU. It's no panacea, but it seems relevant to the discussion! One caveat is that the reduce_sum replacement in the workaround does not handle partial reduction (i.e. it does not accept an axis argument). This is more a proof of concept, but shows it can be done. Perhaps a slower but deterministic reduce_sum backend for the GPU can be added to TensorFlow that does not rely on CUDA atomics, and an option to Session can determine whether reduce_sum uses the deterministic or fast backend (but haven't studied the TF architecture enough to know if this is feasible).", "@benjrossi nice post. I'm working on a short guide and hope to push a first draft within the next 2 weeks.", "I think this is a note for the Programmer's Guide.  We're working on a\nredesign of the table of contents for the Programmer's Guide, but I would\nsuggest you create a fresh doc entitled \"Non-determinism in TensorFlow\".\nIf you submit it as a separate doc in\ntensorflow/docs_src/programmers_guide, we may glom it into another document\nwhen we reorganize.\n\nThanks for writing it up!\n\nOn Wed, Jun 7, 2017 at 9:06 PM, Samuel Chin <notifications@github.com>\nwrote:\n\n> @benjrossi <https://github.com/benjrossi> nice post. I'm working on a\n> short guide and hope to push a first draft within the next 2 weeks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2732#issuecomment-306992651>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1YDiODpkq5uspn2YqsMItTBs0z1DLfks5sB3NZgaJpZM4IxCCS>\n> .\n>\n", "The link from benjrossi doesn't work anymore, here is an updated link to the same page: https://www.twosigma.com/insights/a-workaround-for-non-determinism-in-tensorflow.", "Reductions are now deterministic.", "Although I was not using TensorFlow, I think my experience may be useful for some case.\r\n\r\nI found that **multi-thread pre-fetching** training samples also introduces randomness. In the multi-thread way, in a new run the samples are put into the queue in a new order, determined by the relative speed of the threads. I had to set the number of pre-fetching threads to `1` to solve the problem.\r\n\r\nBTW, I implemented the multi-threading in my own way using package `threading`.", "@ekelsen which tensorflow version is deterministic?", "It's been in head for a few weeks now.", "so currently only master has deterministic GPU reductions; the next release, 1.4 should have them.", "@ekelsen Could you please elaborate on how it went?", "As far as I can tell, TF 1.5 still shows non-deterministic results on GPU (and deterministic on CPU). However, I'm using reduce_mean not reduce_sum. I assume it is the same problem.\r\nI would love to know if there are concrete plans to improve the reproduciblity of training.", "From my own experiences non-determinism can be found in the backward pass of the convolution filter and in the computation of the softmax cross-entropy loss function.\r\n\r\nThe convolution issue can be solved by using a deterministic algorithm from cudnn instead of letting tensorflow decide the \"best\" algorithm to use.", "Perhaps `export TF_CUDNN_USE_AUTOTUNE=0`", "As far as I can tell TF 1.6 also shows non-deterministic results on GPU. Has this been fixed in the master branch or a later release? If not, are there any plans to \"fix\" it? Even if not, in the mean time it seems very necessary to at least note this in the official documentation. Sorry if it is already there, but I was not able to find it (which suggests others might also not be able to).", "@ekelsen is this still expected? \r\n\r\nIt's fine for these to be non-deterministic, but we should mention it in the docs if so.", "Happy to re-open this PR and refine it: https://github.com/tensorflow/tensorflow/pull/10636. Thoughts @martinwicke?", "Sounds good. Thank you!", "https://github.com/tensorflow/tensorflow/pull/10636 is outdated so I won't be refining it.\r\n\r\nIt seems like both `tf.reduce_sum` and `tf.reduce_mean` are deterministic now. @nikonikolov do you have a small example to reproduce the non-deterministic behaviour?", "@jkschin Unfortunately I do not have a simple one. Are **all** operators determinisitc on GPU now? If they are supposed to be, I can dig into it and compose an example. I am using much more operators than `tf.reduce_sum` and `tf.reduce_mean` so there is a chance there is another operator causing the non-deterministic behavior. \r\n\r\nAlso which version exactly has the deterministic behavior?", "@jkschin Assuming that `tf.reduce_sum` and `tf.reduce_mean` are now deterministic, are all gradient based operations deterministic too (assuming the same data is passed to the neural network). I am still experiencing the problem in TF 1.8 and the non-determinism happens after a gradient step is taken. I can try to provide example, but first wanted to make sure determinism is expected with gradients.", "The gradient pass often contains ops that are rare in the forward pass. Is there a way to narrow this down further? There may be a reduction or something else which is non-deterministic. \r\n\r\nWe probably cannot claim that all of TF is deterministic yet.", "Hey, so here is a relatively simple example to reproduce. I tried it on CPU and results were the same. However, on GPU they were not. When I decreased number of iterations to 1 or 2, all the time I was able to get the same results (although I did not do a lot of runs). When the number of iterations is higher, say >10, I almost always get different results. Also when there is only 1 dense layer, the results are almost always reproducible.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nITERATIONS=20\r\n\r\ntf.set_random_seed(42)\r\nnp.random.seed(42)\r\n\r\nx_data = np.random.normal(size=[32, 10])\r\ny_data = np.random.normal(size=[32, 1])\r\nx_test = np.random.normal(size=[32, 10])\r\n\r\nx_in  = tf.placeholder(tf.float32, [None, 10])\r\ny_in  = tf.placeholder(tf.float32, [None, 1])\r\nx     = x_in\r\nx     = tf.layers.dense(x, 200, tf.nn.relu)\r\nx     = tf.layers.dense(x, 1, tf.nn.relu)\r\nloss  = tf.losses.mean_squared_error(y_in, x)\r\n\r\nmvars = tf.get_default_graph().get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\n\r\nopt   = tf.train.AdamOptimizer(use_locking=True)\r\ntrain = opt.minimize(loss)\r\nconfig= tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\r\nsess  = tf.Session(config=config)\r\n\r\nallvars = tf.get_default_graph().get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\r\n\r\nsess.run(tf.global_variables_initializer())\r\ninit_vals = sess.run(allvars)\r\n\r\ndef run():\r\n  for val, v in zip(init_vals, allvars):\r\n    sess.run(tf.assign(v, val))\r\n  \r\n  ivals = sess.run(allvars)\r\n  out = []\r\n  allvals = []\r\n\r\n  for i in range(ITERATIONS):\r\n    l, _ = sess.run([loss, train], feed_dict={x_in: x_data, y_in: y_data})\r\n    out.append(sess.run(x, feed_dict={x_in: x_test}))\r\n    allvals.append(sess.run(allvars))\r\n\r\n  fvals = sess.run(allvars)\r\n  # return np.asarray(ivals), np.asarray(fvals), np.asarray(out)\r\n  return np.asarray(ivals), np.asarray(fvals), np.asarray(out), allvals\r\n\r\nivals1, fvals1, out1, all1 = run()\r\nivals2, fvals2, out2, all2 = run()\r\n\r\nsame_init = [np.all(v1 == v2) for v1, v2 in zip(ivals1, ivals2)] \r\nsame_fin = [np.all(v1 == v2) for v1, v2 in zip(fvals1, fvals2)] \r\nprint(\"Forward passes were the same: {}\".format( np.all(out1 == out2) ))\r\nprint(\"Final value of variables are the same: {}\".format( np.all(same_fin) ))\r\nprint(\"Variables initialized to same values: {}\".format( np.all(same_init) ))\r\n```\r\nUnfortunately I am really busy with experiments at the moment and do not have the time to narrow this down further, but I hope it will be a good starting point. I tested with python 3.6.5, CUDA 9.0, cudnn 7.1\r\n and TF 1.8.\r\n\r\nIt will be great if we manage to make all operations deterministic. I am running some Reinforcement Learning algorithms and over time I get huge difference in performance even if I use the same seed.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "GPU reductions are now deterministic.", "@ekelsen Which ops are deterministic exactly? Since when / which TF version? Is this documented?", "@ekelsen, I still experience non-deterministic GPU operations. In comparison, same code is absolutely deterministic when it runs on CPU. I am using TF 1.12.0 and CUDA 9.0", "Reductions are deterministic.  Many other things are not:\r\n\r\n0) Forward passes of anything that uses auto-tune (convolutions are the prime culprit, but I think also gemm now)\r\n1) Backward passes of convolutions\r\n2) Backward passes of bias_add (sometimes depending on auto-tune)\r\n3) I think cross entropy still hasn't been switched to use the faster and deterministic reductions under the hood.\r\n4) unsorted_segment_sum (possibly also sorted_segment_sum)\r\n5) scatter_add/sub\r\n6) backward pass of depthwise conv, some less common ops used in preprocessing\r\nPossibly more I've forgotten or never knew about.", "Thanks for clarifying. Are there any plans to make all these deterministic? PyTorch can be completely deterministic on GPU which can often be very useful. In particular, while the non-determinism might be acceptable in supervised learning, in Reinforcement Learning it has very non-trivial impact on results, sometimes leading to big variance due to the constantly shifting data distribution.", "For anyone still encountering this issue, @duncanriach has done great work on this problem which he presented at NVIDIA's GTC 2019 conference.\r\nHis findings being that there are a multitude of reasons for the non-determinism we see\r\nHe has set-up a [repository](https://github.com/NVIDIA/tensorflow-determinism) to track the issue which also contains a link to his talk", "Thanks @Robbie-Palmer. The current high-level status is that there are now solutions for TensorFlow determinism when running on GPUs related to cuDNN (convolutions and max-pooling) and bias_add. Please see the following repo for up-to-date status: https://github.com/NVIDIA/tensorflow-determinism"]}, {"number": 2731, "title": "Feature Request: Exporting Model to Eigen Only Environments", "body": "This would be insanely useful. I understand that some operations don't map cleanly to Eigen operations. \n\n[https://github.com/riga/tfdeploy](https://github.com/riga/tfdeploy) addresses a similar goal and has a reasonable approach to handle custom ops. \n\nAn API where the eigen only computational graph is generated and the ability to \"plug in\" missing ops?\n", "comments": ["Given limited resources, this is not something which we are currently planning to work on.\n"]}, {"number": 2730, "title": "TensorBoard feature request: Search for Node", "body": "I have been trying to retrain the inception v3 network and TensorBoard has been the only real way to introspect the network and it's implementation so far.\n\nIt would be very useful to be able to find nodes in the graph, by name or other properties. E.g. in my case I am interested in seeing the total_loss node that is being defined here: https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py#L120 To get a greater idea of what it is linked to.\n", "comments": ["Does `tf.Graph.get_operations` or `tf.Graph.get_operation_by_name` work for you?\n", "This is for the TensorBoard GUI, rather than the tensorflow API.\n", "@danmane Care to comment?  Should we mark this contributions welcome? \n", "I'm assigning nikhil and daniel to take a look, since it's a graph explorer question. But I also pre-emptively marked it contributions welcome...\n", "This issue has been migrated to tensorflow/tensorboard#76 because TensorBoard has migrated to a new repo (outside of tensorflow/tensorflow)."]}, {"number": 2729, "title": "TensorBoard inception v3 visualization is horizontal and almost unusable", "body": "### Environment info\n\nOperating System: Linux x64 GPU\n\nInstalled version of CUDA and cuDNN: CUDA 7.5, cuDNN v4\n\nIf installed from binary pip package, provide: 0.8.0\n\nI've been trying to retrain the inception v3 network using the image_retraining example, and then the inception v3 model in the models repo.\n\nThe image_retraining model[1] seems to visualize vertically and it relatively straight forward to look at.\n\nThe model from the models repo[2] visualizes horizontally, making zooming in and exploring the model really tricky.\n\n[1] http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n[2] http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\n", "comments": ["Thanks for the report. In the coming few months we're planning to prioritize making the graph visualizer more useful on a wider range of models; we'll revisit this issue then. For now, I'm going to close this to keep issue tracking manageable. \n"]}, {"number": 2728, "title": "Memory leak (on cpu) in 0.9rc (vs. 0.7.1rc)", "body": "Hi,\nusing the provided wheel vor 0.9rc (also reproduced with a fresh install from source) I'm seeing a lot of memory consumption (on the cpu) during training a model (on a gpu (TITAN X) which gets its data from a `tf.FIFOQueue` pinned with `tf.device()` to a cpu). I tested two different dataset sizes and these lead to 600MB and 1.2GB leaked memory respectively accumulated during the training loop (GPU memory consumption stays constant during training time).\n\nThis memory leak does not happen at all when running the same code using 0.7.1rc. Additionally,\nwith 0.9rc setting up the whole graph takes about 600MB vs 400MB with 0.7.1rc.\n\nThe code is rather large so it will take a bit to get a MWE to (hopefully) reproduce this issue. I suspect it is tied to using a queue (queues, actually, because the validation set is also feed through a different queue, relying on `tf.template()` to share model parameters). Has something like that been observed beforehand? Any hints how I could try figure out more of the problem myself?\n### Environment info\n\nOperating System:\nUbuntu 15.10\n\nInstalled version of CUDA and cuDNN: \nCUDA7.5/cuDNN4.0.4\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 Jan  1 23:25 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1 23:25 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally 0.9.0rc0\n```\n### Steps to reproduce\n1. Working on MWE...\n### What have you tried?\n\nThe described behavior does not happen with 0.7.1rc (UPDATE: also not observed with 0.8rc).\n", "comments": ["I think I'm seeing something similar. Haven't been able to confirm that it's a memory leak, but progress grinds to a halt (`session.run` never returns) after a few iterations of training and memory usage is quite high. I'm also making use of queues, and my setup is almost identical to the one above.\n\nEdit: I should mention that I'm training on a GPU (Titan X).\n", "I am experiencing the same issue. In python interpreter, `session.run` hangs and it even cannot be stopped until I press Ctrl+\\ and terminate the entire process.\n", "Updated the original with a bit of more info (training on GPU, GPU memory consumption stays constant). `session.run` does not hang for me, training works fine. Though this might be due to the fact that the training set itself is rather small in this case. One minibatch is of size 128x19 (float32).\n", "Same issue here. Using two Tesla K20c GPU's for training/evaluation. Each training session takes up to a GB of CPU memory which is never returned to the system.\n", "I'm experiencing a similar problem.\nWhen launching a graph, around 16 GB memory is consumed, but only ~3 GB is release upon exiting the process.\n\nUsing `top` command, lost 13 GB is only in total used memory, not on any of listed processes.\nI.e., summing up every process's memory usage results in a fewer number by ~13 GB.\n\nMemory consumption is constant during training, and re-runing the same graph within a process does not increase the consumption either.\nOn the other hand, after re-launching another process, the first path consumes another 16 GB.\nThe memory seems to never go to swap, and when it exhausts the available main memory, Linux hangs.\n\nI'm running the graph on a Titan X on a machine with two Titan X and 64 GB main memory.\nI've experienced this for at least master branch at 6/8 and 5/24.\nThe OS is Ubuntu 16.04.\nWhen I used 0.9rc without GPU support, the leak didn't happen.\n\nInterestingly, when I run very similar graph at home with single GTX 960 machine and 16 GB main memory, this kind of leak never happens.\nThe batch size fed is much smaller due to GPU memory size, though.\nI've tested this on master branch at 6/4 and 6/12.\nThe OS is ArchLinux.\n\nCUDA7.5/cuDNN5.0.5 for both of the above.\nAlso I should note that I compiled tensorflow with GPU support using GCC 5.\n", "It will be very helpful if someone can post a small case to reproduce the problem. \n", "I adapted CIFAR multi gpu for my project. In my case **both my code and CIFAR** cause memory leaks on 0.8.0rc version. I rolled back to 0.7.1 and I can confirm that there is no memory leak.\nI can provide my code if it is till needed.\n", "@abezuglov, does the memory leak only happen with multi-GPU? \n\nIt will be great to have a repro with single GPU. If not, it is okay to have a repro case with multi-GPU. \n", "If I remember correctly, it did happen with both single and multi GPU's.\n\nPlease, find the main training loop below. The complete code is here: [https://github.com/abezuglov/ANN/tree/master/code], where ilt_one_layer.py, ilt_two_layers.py, etc. are the models; ilt_default_feed.py -- training on a single device, ilt_multi_gpu_feed.py -- training on multiple devices. Let me know if I can be of further help.\n\ndef train():\n    # Read datasets \n    train_dataset, valid_dataset, test_dataset = ld.read_data_sets()\n\n```\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n\n    # Prepare global step and learning rate for optimization...\n\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    outputs = ilt.inference(x)\n    loss = ilt.loss(outputs, y_)\n\n    # Calculate gradients and apply them\n    grads = optimizer.compute_gradients(loss)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step = global_step)\n\n    # Smoothen variables after gradient applications\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_avg_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    merged = tf.merge_all_summaries()\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session(config = tf.ConfigProto(\n        allow_soft_placement = False, # allows to utilize GPU's & CPU's\n        log_device_placement = False)) # shows GPU/CPU allocation\n\n    # Finish graph creation. Below is the code for running graph\n    sess.run(init)\n    tf.train.start_queue_runners(sess=sess)\n\n    # Main training loop\n    for step in xrange(FLAGS.max_steps):\n        start_time = time.time()\n\n        _, train_loss, summary, lr = sess.run(\n            [train_op, loss, merged, learning_rate], feed_dict=fill_feed_dict(train_dataset, x, y_, train = True))\n\n        duration = time.time()-start_time\n        if step%(FLAGS.max_steps//20) == 0:\n            feed_dict = fill_feed_dict(valid_dataset, x, y_, train = False)\n            valid_loss, summary = sess.run([loss, merged], feed_dict = feed_dict)\n            print('Step %d (%.2f op/sec): Training MSE: %.5f, Validation MSE: %.5f' % (\n                step, 1.0/duration, np.float32(train_loss).item(), np.float32(valid_loss).item()))\n\n    feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n    test_loss = sess.run([loss], feed_dict = feed_dict)\n    print('Test MSE: %.5f' % (np.float32(test_loss).item()))\n    sess.close()\n```\n", "@osdf @neggert @abezuglov @jihunchoi @nsuke Just to gather some info here: What is your NVIDIA driver version (see nvidia-smi output)?\n", "NVIDIA-SMI 352.93     Driver Version: 352.93                      \n", "Ran in to this thread just now. @osdf since you're also suspecting the queue, this might be related to #2942.\n", "While @zheng-xq investigates the issue, may I suggest the following? We have seen cases in which similar memory leaks occur for NVIDIA driver 352, but doesn't occur for NVIDIA driver 361. Can you try upgrading the NVIDIA driver and see if the problem persists?\n", "@caisq thanks ! This solved the problem for me.\n\nThe leak was happening while using 361.42 from Ubuntu 16.04 official repository.\nI've run `apt-get purge nvidia-*` then installed 367.27 from a [PPA](https://launchpad.net/~graphics-drivers/+archive/ubuntu/ppa).\nThe memory consumption went down from 16+GB to around 6 GB which is now correctly released when exiting the process.\n", "I'm closing this as fixed. Thanks for tracking down the driver issue!\n"]}, {"number": 2727, "title": "tf learn bugs when running DNNClassifier examples", "body": "from sklearn import datasets, metrics\n\niris = datasets.load_iris()\nclassifier = learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\nclassifier.fit(iris.data, iris.target)\nscore = metrics.accuracy_score(iris.target, classifier.predict(iris.data))\nprint(\"Accuracy: %f\" % score)\n\n```\nTraceback (most recent call last):\n\n  File \"<ipython-input-73-ae8986fb272b>\", line 1, in <module>\n    classifier.fit(iris.data, iris.target)\n\n  File \"/home/wenjian/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 181, in fit\n    monitors=monitors)\n\n  File \"/home/wenjian/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 498, in _train_model\n    monitors=monitors)\n\n  File \"/home/wenjian/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 225, in train\n    monitor.begin(max_steps=start_step + steps)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n```\n", "comments": ["seems solved in the master branch now\n", "Looks like this problem is resolved. Closing the issue.\n"]}, {"number": 2726, "title": "Modifying MNIST example to distributed version: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR", "body": "Operating System: 2 servers(each have 4 GPUs, Titan-x), ubuntu14.04\nCUDA 7.5, cuDNN 4.0.7\nTensorflow has been installed with source file with bazel.\n\nMNIST example works well on each single server.\n~/tensorflow/tensorflow/models/image/mnist$ python convolutional.py\n\nTo make MNIST example as distributed version, I modified convolutional.py as follows.\nI refer \"Putting it all together: example trainer program\" in \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md\n\n```\n#Simple, end-to-end, LeNet-5-like convolutional MNIST model example.\n#This should achieve a test error of 0.7%. Please keep this model as simple and\n#linear as possible, it is meant as a tutorial for simple convolutional models.\n#Run with --self_test on the command line to execute a short self-test.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\nimport time\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\nWORK_DIRECTORY = 'data'\nIMAGE_SIZE = 28\nNUM_CHANNELS = 1\nPIXEL_DEPTH = 255\nNUM_LABELS = 10\nVALIDATION_SIZE = 5000  # Size of the validation set.\nSEED = 66478  # Set to None for random seed.\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nEVAL_BATCH_SIZE = 64\nEVAL_FREQUENCY = 100  # Number of steps between evaluations.\n\n  ######################################################################################\n  #Flags for defining the tf.train.ClusterSpec\ntf.app.flags.DEFINE_string(\"ps_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\ntf.app.flags.DEFINE_string(\"worker_hosts\", \"\",\n                           \"Comma-separated list of hostname:port pairs\")\n\n  #Flags for defining the tf.train.Server\ntf.app.flags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\ntf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n  ######################################################################################\n\ntf.app.flags.DEFINE_boolean(\"self_test\", False, \"True if running a self test.\")\nFLAGS = tf.app.flags.FLAGS\n\n\ndef maybe_download(filename):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not tf.gfile.Exists(WORK_DIRECTORY):\n    tf.gfile.MakeDirs(WORK_DIRECTORY)\n  filepath = os.path.join(WORK_DIRECTORY, filename)\n  if not tf.gfile.Exists(filepath):\n    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n    with tf.gfile.GFile(filepath) as f:\n      size = f.Size()\n    print('Successfully downloaded', filename, size, 'bytes.')\n  return filepath\n\n\ndef extract_data(filename, num_images):\n  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  \"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n    return data\n\n\ndef extract_labels(filename, num_images):\n  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_images)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n  return labels\n\n\ndef fake_data(num_images):\n  \"\"\"Generate a fake dataset that matches the dimensions of MNIST.\"\"\"\n  data = numpy.ndarray(\n      shape=(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS),\n      dtype=numpy.float32)\n  labels = numpy.zeros(shape=(num_images,), dtype=numpy.int64)\n  for image in xrange(num_images):\n    label = image % 2\n    data[image, :, :, 0] = label - 0.5\n    labels[image] = label\n  return data, labels\n\n\ndef error_rate(predictions, labels):\n  \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n  return 100.0 - (\n      100.0 *\n      numpy.sum(numpy.argmax(predictions, 1) == labels) /\n      predictions.shape[0])\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  ##########################################################################################\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  ##########################################################################################\n  if FLAGS.self_test:\n    print('Running self-test.')\n    train_data, train_labels = fake_data(256)\n    validation_data, validation_labels = fake_data(EVAL_BATCH_SIZE)\n    test_data, test_labels = fake_data(EVAL_BATCH_SIZE)\n    num_epochs = 1\n  else:\n    # Get the data.\n    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n\n    # Extract it into numpy arrays.\n    train_data = extract_data(train_data_filename, 60000)\n    train_labels = extract_labels(train_labels_filename, 60000)\n    test_data = extract_data(test_data_filename, 10000)\n    test_labels = extract_labels(test_labels_filename, 10000)\n\n    # Generate a validation set.\n    validation_data = train_data[:VALIDATION_SIZE, ...]\n    validation_labels = train_labels[:VALIDATION_SIZE]\n    train_data = train_data[VALIDATION_SIZE:, ...]\n    train_labels = train_labels[VALIDATION_SIZE:]\n    num_epochs = NUM_EPOCHS\n  ##########################################################################################\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n  ##########################################################################################\n      train_size = train_labels.shape[0]\n\n      # This is where training samples and labels are fed to the graph.\n      # These placeholder nodes will be fed a batch of training data at each\n      # training step using the {feed_dict} argument to the Run() call below.\n      train_data_node = tf.placeholder(\n          tf.float32,\n          shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n      train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n      eval_data = tf.placeholder(\n          tf.float32,\n          shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n\n      # The variables below hold all the trainable weights. They are passed an\n      # initial value which will be assigned when we call:\n      # {tf.initialize_all_variables().run()}\n      conv1_weights = tf.Variable(\n          tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n                              stddev=0.1,\n                              seed=SEED))\n      conv1_biases = tf.Variable(tf.zeros([32]))\n      conv2_weights = tf.Variable(\n          tf.truncated_normal([5, 5, 32, 64],\n                              stddev=0.1,\n                              seed=SEED))\n      conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n      fc1_weights = tf.Variable(  # fully connected, depth 512.\n          tf.truncated_normal(\n              [IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n              stddev=0.1,\n              seed=SEED))\n      fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n      fc2_weights = tf.Variable(\n          tf.truncated_normal([512, NUM_LABELS],\n                              stddev=0.1,\n                              seed=SEED))\n      fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n\n      # We will replicate the model structure for the training subgraph, as well\n      # as the evaluation subgraphs, while sharing the trainable parameters.\n      def model(data, train=False):\n        \"\"\"The Model definition.\"\"\"\n        # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n        # the same size as the input). Note that {strides} is a 4D array whose\n        # shape matches the data layout: [image index, y, x, depth].\n        conv = tf.nn.conv2d(data,\n                            conv1_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        # Bias and rectified linear non-linearity.\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n        # Max pooling. The kernel size spec {ksize} also follows the layout of\n        # the data. Here we have a pooling window of 2, and a stride of 2.\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        conv = tf.nn.conv2d(pool,\n                            conv2_weights,\n                            strides=[1, 1, 1, 1],\n                            padding='SAME')\n        relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n        pool = tf.nn.max_pool(relu,\n                              ksize=[1, 2, 2, 1],\n                              strides=[1, 2, 2, 1],\n                              padding='SAME')\n        # Reshape the feature map cuboid into a 2D matrix to feed it to the\n        # fully connected layers.\n        pool_shape = pool.get_shape().as_list()\n        reshape = tf.reshape(\n            pool,\n            [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n        # Fully connected layer. Note that the '+' operation automatically\n        # broadcasts the biases.\n        hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n        # Add a 50% dropout during training only. Dropout also scales\n        # activations such that no rescaling is needed at evaluation time.\n        if train:\n          hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n        return tf.matmul(hidden, fc2_weights) + fc2_biases\n\n      # Training computation: logits + cross-entropy loss.\n      logits = model(train_data_node, True)\n      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits, train_labels_node))\n\n      # L2 regularization for the fully connected parameters.\n      regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n                      tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n      # Add the regularization term to the loss.\n      loss += 5e-4 * regularizers\n\n      # Optimizer: set up a variable that's incremented once per batch and\n      # controls the learning rate decay.\n      batch = tf.Variable(0)\n      # Decay once per epoch, using an exponential schedule starting at 0.01.\n      learning_rate = tf.train.exponential_decay(\n          0.01,                # Base learning rate.\n          batch * BATCH_SIZE,  # Current index into the dataset.\n          train_size,          # Decay step.\n          0.95,                # Decay rate.\n          staircase=True)\n      # Use simple momentum for the optimization.\n      optimizer = tf.train.MomentumOptimizer(learning_rate,\n                                             0.9).minimize(loss,\n                                                           global_step=batch)\n\n      # Predictions for the current training minibatch.\n      train_prediction = tf.nn.softmax(logits)\n\n      # Predictions for the test and validation, which we'll compute less often.\n      eval_prediction = tf.nn.softmax(model(eval_data))\n\n      # Small utility function to evaluate a dataset by feeding batches of data to\n      # {eval_data} and pulling the results from {eval_predictions}.\n      # Saves memory and enables this to run on smaller GPUs.\n      def eval_in_batches(data, sess):\n        \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n        size = data.shape[0]\n        if size < EVAL_BATCH_SIZE:\n          raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n        predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n        for begin in xrange(0, size, EVAL_BATCH_SIZE):\n          end = begin + EVAL_BATCH_SIZE\n          if end <= size:\n            predictions[begin:end, :] = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[begin:end, ...]})\n          else:\n            batch_predictions = sess.run(\n                eval_prediction,\n                feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n            predictions[begin:, :] = batch_predictions[begin - size:, :]\n        return predictions\n      # Run all the initializers to prepare the trainable parameters.\n      #saver = tf.train.Saver()#dist\n      summary_op = tf.merge_all_summaries()#dist\n      init_op = tf.initialize_all_variables()#dist\n      #tf.initialize_all_variables().run()\n      print('Initialized!')\n    ###################################################################################\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             #logdir=\"/home/user/tmp\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             #saver=saver,\n                             global_step=batch)#,\n                             #save_model_secs=600)\n    ###################################################################################\n    # Create a local session to run the training.\n    start_time = time.time()\n    #with tf.Session() as sess:\n    #with sv.managed_session(server.target) as sess:\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n      # Loop through training steps.\n      for step in xrange(int(num_epochs * train_size) // BATCH_SIZE):\n      #step = 0\n      #while not sv.should_stop() and step < (int(num_epochs * train_size) // BATCH_SIZE):\n        # Compute the offset of the current minibatch in the data.\n        # Note that we could use better randomization across epochs.\n        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n        # This dictionary maps the batch data (as a numpy array) to the\n        # node in the graph it should be fed to.\n        feed_dict = {train_data_node: batch_data,\n                     train_labels_node: batch_labels}\n        # Run the graph and fetch some of the nodes.\n        _, l, lr, predictions = sess.run(\n            [optimizer, loss, learning_rate, train_prediction],\n            feed_dict=feed_dict)\n        if step % EVAL_FREQUENCY == 0:\n          elapsed_time = time.time() - start_time\n          start_time = time.time()\n          print('Step %d (epoch %.2f), %.1f ms' %\n                (step, float(step) * BATCH_SIZE / train_size,\n                 1000 * elapsed_time / EVAL_FREQUENCY))\n          print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n          print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n          print('Validation error: %.1f%%' % error_rate(\n              eval_in_batches(validation_data, sess), validation_labels))\n          sys.stdout.flush()\n      # Finally print the result!\n      test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n      print('Test error: %.1f%%' % test_error)\n      if FLAGS.self_test:\n        print('test_error', test_error)\n        assert test_error == 0.0, 'expected 0.0 test_error, got %.2f' % (\n            test_error,)\n    # Ask for all the services to stop.\n    sv.stop()\n\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n# Commands\n## on server1, terminal1\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=ps --task_index=0\n## on server1, terminal2\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=0\n## on server2, terminal2\n\n$python convolutional.py --ps_hosts=user-81:2222 --worker_hosts=user-81:2223,user-70:2223 --job_name=worker --task_index=1\n# Message & Error\n## Common\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:0b:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:09:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 2 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:06:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 3 with properties:\nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:05:00.0\nTotal memory: 12.00GiB\nFree memory: 11.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\n```\n## on server1, terminal1\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222, user-70:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, user-70:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 10.14G (10892742656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 9.13G (9803467776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 8.22G (8823120896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n\n...\n## on server1, terminal2\n\n```\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, user-70:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:08.411750480   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:09.412362146   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:11.005026028   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nE0608 20:36:13.191502278   23861 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.70:2224': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\nTraceback (most recent call last):\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 343, in main\n    feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InternalError: cuDNN launch failure : input shape([64,1,28,28]) filter shape([5,5,1,32])\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:worker/replica:0/task:0/gpu:0\"](_recv_Placeholder_0_G6, Variable/read_S53)]]\n         [[Node: add_5_G12 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/gpu:0\", send_device_incarnation=-6674272897051056682, tensor_name=\"edge_106_add_5\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D', defined at:\n  File \"convolutional.py\", line 367, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 254, in main\n    logits = model(train_data_node, True)\n  File \"convolutional.py\", line 220, in model\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n## on server2, terminal2\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:0b:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX TITAN X, pci bus id: 0000:06:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {user-81:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {user-81:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 20:36:48.351331932   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:49.352058170   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\nE0608 20:36:50.939646814   18333 tcp_client_posix.c:173]     failed to connect to 'ipv4:143.248.39.81:2223': socket error: connection refused\n```\n\nCould anybody help me?\n## Errors on server1, terminal1\n\n```\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:289] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:278] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nW tensorflow/stream_executor/stream.cc:301] attempting to perform DNN operation using StreamExecutor without DNN support\n```\n## Errors on server1, terminal2\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 11.27G (12103048448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n```\n", "comments": ["Yes, you need cudNN v4. v5 is not supported yet afaik. See: \nhttps://github.com/tensorflow/tensorflow/issues/1969\n\nHere is a fix:\nhttp://stackoverflow.com/a/36978616\n", "I also have tried cuDNN 4.0.7 and got the same messages.\n", "Well, your script is working on my machines. What do you get if you execute the following command:\n\n```\n(tf-env)worker1:~$ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n#define CUDNN_MAJOR      4\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 7\n```\n\nMake sure to run it on all GPU machines.\n\nI ran your script on 1 paramater server without gpu and 3 workers with each one GTX 960 GPU. \n\nParameter Server:\n\n```\n(tensorflow-cpu)param:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=ps --task_index=0\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n```\n\nWorker 1:\n\n```\n(tf-env)worker1:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=0\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.77GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2222, pc3-012-l.cs.some-university.ac.uk:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:30.327518348    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:30.327551292    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328548987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:31.328627152    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:32.725781597    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:33.036280593    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.500345457    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:35.721717987    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:38.816290592    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.174:2222': socket error: connection refused\nE0608 17:01:39.043082815    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:46.250808712    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.668526778    3902 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nStep 0 (epoch 0.00), 449.4 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 301.8 ms\nMinibatch loss: 3.292, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.5%\nStep 200 (epoch 0.23), 413.2 ms\nMinibatch loss: 3.435, learning rate: 0.010000\nMinibatch error: 7.8%\nValidation error: 3.2%\n```\n\nWorker 2:\n\n```\n(tf-env)worker2:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=1\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.61GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, localhost:2222, pc3-007-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nE0608 17:01:46.733183601    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:47.733565779    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:49.099973593    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:51.845976385    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:01:55.380873638    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nE0608 17:02:02.158287167    5215 tcp_client_posix.c:173]     failed to connect to 'ipv4:138.251.29.169:2222': socket error: connection refused\nStep 0 (epoch 0.00), 592.7 ms\nMinibatch loss: 3.580, learning rate: 0.010000\nMinibatch error: 14.1%\nValidation error: 7.3%\nStep 100 (epoch 0.12), 408.5 ms\nMinibatch loss: 3.029, learning rate: 0.010000\nMinibatch error: 0.0%\nValidation error: 3.2%\nStep 200 (epoch 0.23), 415.5 ms\nMinibatch loss: 2.972, learning rate: 0.010000\nMinibatch error: 1.6%\nValidation error: 2.4%\n```\n\nWorker 3:\n\n```\n(tf-env)worker3:~$ python convolutional.py \\\n> --ps_hosts=pc3-013-l.cs.some-university.ac.uk:2222  \\\n> --worker_hosts=pc3-002-l.cs.some-university.ac.uk:2222,pc3-012-l.cs.some-university.ac.uk:2222,pc3-007-l.cs.some-university.ac.uk:2222  \\\n> --job_name=worker --task_index=2\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 960\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.253\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.53GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960, pci bus id: 0000:01:00.0)\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {pc3-013-l.cs.some-university.ac.uk:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {pc3-002-l.cs.some-university.ac.uk:2222, pc3-012-l.cs.some-university.ac.uk:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nStep 0 (epoch 0.00), 337.0 ms\nMinibatch loss: 3.471, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 7.4%\nStep 100 (epoch 0.12), 407.0 ms\nMinibatch loss: 3.057, learning rate: 0.010000\nMinibatch error: 3.1%\nValidation error: 3.1%\nStep 200 (epoch 0.23), 412.3 ms\nMinibatch loss: 3.086, learning rate: 0.010000\nMinibatch error: 9.4%\nValidation error: 2.3%\n```\n\nI'm actually very happy I found this. I currently learning how to use tensorflow in a distributed setting. So thanks for your detailed and excellent example!\n", "@vrv: Any suggestions here?\n", "What version of tensorflow are you using?  Please fill out the issues template instead of erasing it, particularly when it comes to issues with GPUs.\n", "Actually, I'm not sure what's going on, even if you told me the version of TF you were using.  In one case you are running outof GPU memory, in the other case we're getting an error from cudnn that tells me nothing about what happened (INTERNAL ERROR).  I wish error messages were more informative :(\n", "Sorry for late answer.\nTensorflow version is 0.8.0. \nI find similar OUT OF MEMORY issue here https://github.com/tensorflow/models/issues/72\nMaybe It's because of assigning 2 workers on 1 GPU. \nI'll update if it is solved.\n", "Yeah, without better information from nvidia/cuda libraries, or cuda-memcheck, I'm not sure how much more we can help.  Let us know if you find anything.\n", "I am new to tensorflow and distributed tensorflow. I ran the example on a raspberry pi 3 cluster with one ps node and three worker nodes. I use slurm as the tool to assign four nodes for the task, piw30-33.\n\nI have following error message. \nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw30:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw31:2223, localhost:2223, piw33:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw31:2223, piw32:2223, piw33:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw30:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, piw32:2223, piw33:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw30:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw31:2223, piw32:2223, localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nTraceback (most recent call last):\n  File \"mnist_yetanother.py\", line 351, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"mnist_yetanother.py\", line 310, in main\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 176, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: /job:worker/replica:0/task:0/cpu:0 unknown device.\n     [[Node: truncated_normal_2_S5 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device_incarnation=-2310109875678913912, tensor_name=\"edge_37_truncated_normal_2\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\nsrun: error: piw32: task 0: Exited with exit code 1\n\nAny help or suggestion would be appreciated!\n", "@hellf I believe it is because you assign the ps and one worker on one node. If you split the ps on one node and the workers on the others,  the out-of-memory bug will disappear\n"]}, {"number": 2725, "title": "R0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2724, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for this -- though you should make the fix in the original documentation: https://github.com/tensorflow/tensorflow/blob/9078909131d5e91e8bd1878eac54136885656043/tensorflow/core/ops/array_ops.cc#L1718  since the others are automatically generated.\n", "No problem -- you can drop the change from ops.pbtxt -- we auto generate that afterwards.\n"]}, {"number": 2723, "title": "Enable whole batch whitening.", "body": "The function tf.image.per_image_whitening() can now be applied to a whole batch. `Mean` and `adjusted_stddev` is then computed over all batch dimensions.\n\nI find it very useful to be able to ably tf.image.whitening to an entire batch.\n1) In some situations a batch is given (due to previous computation in the graph). Splitting the batch into individual images, in order to be able to apply whitening seams to be a bit redundant.\n2) Applying whitening to a batch is statistically superior (and more elegant when used in combination with Batch Normalization).\n\nIt might be considered to rename the function in `per_batch_whitening`. As this would now be more accurate.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please. (Alas we lost the results from the last run....)\n", "It would be nice to mention that the name of the attribute has changed in RELEASE.md ... people who use named arguments are going to break (even though it doesn't make sense for them to for a one argument function).\n", "Or maybe it's not worth changing the name right now and just leave it as 'image' -- the documentation should hopefully be sufficient.\n", "Also, shouldn't there be a test of this behavior?\n", "Ping. Any progress on this?\n", "@tensorflow-jenkins test this please\n", "The test failure seems to be an unrelated timeout of //tensorflow/contrib/learn:linear_test\n", "Are you waiting for my response? Whether we should leave the name as `image` or rename to `images` is a design decision. I would leave that up to you guys. Personally I slightly prefer `images`, as this is consistent with the name convention of all other function definition in that file.\n\nI will add a test to `python/ops/image_ops_test.py` tomorrow. I can also add a note to RELEASE.md, if that is desired.\n", "Tests are now added as well as a short note to release.md.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@shlens: do you think we should have a per_image_whitening separate from the batch version, since some people might want whitening done per image in a batch, rather than using the batch statistics?\n", "After staring hard at the PR, I am starting to wonder whether we might already have this functionality in TF.\n\nNamely, if we wish to whiten all images based on the _batch_ mean and _batch_ variance, then this is precisely what `tf.nn.batch_normalization` does. That is, I think you might mimic `per_image_whitening` where the mean and variance are computed over the entire batch via a call to `tf.nn_batch_normalization`. For example, in your PR this call:\n\n``` python\nwhitened_images = per_image_whitening(images)\n```\n\nbecomes\n\n``` python\n# Compute the minimum variance to mimic per_image_whitening()\nnum_pixels = tf.reduce_prod(array_ops.shape(images))\nmin_stddev = tf.inv(tf.sqrt(math_ops.cast(num_pixels, tf.float32)))\n\n# Calculate the mean and variance of the image batch.\naxis = list(range(images.get_shape().ndims - 1))\nmean, variance = nn.moments(images, axis)\n\n# Whiten the images.\nwhitened_images = tf.nn_batch_normalization(images, mean, variance, scale=None, offset=None, variance_epsilon=min_stddev)\n```\n\nTake a look at the API for [tf.nn.batch_normalization](https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#batch_normalization) and please let me know if this works for you. If not, then we should expand the API for `per_image_whitening`.\n", "ping @MarvinTeichmann \n", "Yes, your code should provide a similar / the same result as per_batch_whitening, which is somewhat intentional.\n\nThe use case is somewhat different though. The whitening is supposed to be part of the preprocessing while the batch normalization is used as a regularize. From an API perceptive it might not be the niced to make the user \"abuse\" the batch normalization part. Like, most user would expect this functionality in the `tf.image` package, where all the other preprocessing can be found.\n\nAdditionally, the function `per_image_whitening` is already part of the module. The code you posted would also work for the single image case (just use a batch size of 1). I am not sure, whether it is nice to have completely different approaches for single image/batch size. \n\nI could imagine, that the `tf.nn_batch_normalization` is faster (more code in C++?). In this case it might be a nice option to rewrite the  `tf.image.per_image_whitening` function, to take use of this. I would however vote for keeping the `tf.image.per_image_whitening` functionality.\n", "I guess my only source of confusion is that per_image_whitening makes it sound like whitening is done independently of all other images -- any type of batch statistics no longer makes it 'per image'.  I think I would prefer per_batch_whitening for this new function, and leaving per_image_whitening as is? \n\nWhat do you think @shlens ?\n", "@vrv Yes, you are write, an `per_image_whitening` is not the optimal naming.\n\nOn the other hand, I don't think it is nice to have two functions, `per_image_whitening` and `per_batch_whitening`, as both of them are applying exactly the same tensorflow ops. The _very same computation_ is done in both cases. The only difference is the dimension of the input and output.\n\nKeep in mind, the only real change I have done is to remove the line, which throws an error, if a 4-d tensor is passed to the function. The rest is syntactic sugar (Tests, Parameter Naming, Documentation). This line does violate the general python design idea of being permissive, as it artificially restricts the function to a specific use-case. Therefor I think that this line should not be there.\n\nFunction naming is a different issue, but having two restrictive functions does not sound like a great idea, if it is possible to solve the same think with one permissive function. \n", "The reason I suggest the different name is, as written, the results of the whitening of one image out of a batch of N depends on the contents of the other images in the batch.   per_image_whitening that operated on batches of images would compute a result where each whitened image is computed independently of the other samples in the batch.\n\nSo as written, I would say that this function is per_batch_whitening.  a per_image_whitening that operated on batches could be implemented, by changing the computation to reduce independently of the batch dimension.\n", "I think the @MarvinTeichmann  makes some nice points -- namely that it is a simple code change to permit whitening across multiple images. Likewise, the functionality is very similar to the 3-D version of `tf.image.per_image_whitening`. So, from an API perspective, this is very simple and clean.\n\nFrom a functionality perspective I do share @vrv  concern. This function would be performing in effect a different operation with a different statistical goal then the name of the function suggests (i.e. an operation akin to batch-normalization). I think that the danger of not conveying this subtlety to the user would cause lots of pain down the road so much so that I think it is worth sacrificing some simplicity in the API.\n\nIf adding this functionality is important, then my suggestion would be to create a new Python function, i.e.`tf.image.per_batch_whitening` and have it be a light wrapper around `tf.batch_normalization` per the code snippet above. This would provide an efficient implementation and avoid any ambiguity about how statistics are computed.\n", "@shlens Thanks for your Input.\n\nAdditionally, what do you think about renaming the current `tf.image.per_image_whitening` to `tf.image.per_batch_whitening`?  (i.e remove the current `tf.image.per_image_whitening`).\n\nThis would be clean from both API side and also user side perspective. The only disadvantage is, that it breaks compability.\n\nThis solution would work even if we use `tf.batch_normalization` as backend. (i.e. the posted code snippet should to the same as `tf.image.per_image_whitening` , if it is applied to a single image).\n", "I think we are in agreement then having `tf.image.per_batch_whitening` in the API call `tf.batch_normalization` would be a nice addition to the API.\n\nThe secondary question is what to do with `tf.image.per_image_whitening`. I think that we have some policy about retaining backward compatibility within some limits. @vrv would you be able to comment on our degree of backward compatibility in the API?\n", "If you want to deprecate it (logging warning), and remove all the existing internal users of it (switching them to per_batch_whitening), go ahead :).  \n", "(closing due to lack of activity, but will happily re-open if desired -- maybe a new PR will bump this up in the list).\n"]}, {"number": 2722, "title": "Fixed tensorboard not to use any lambda functions for compatibility with Safari", "body": "Fixed tensorboard not to use any lambda functions for compatibility with Safari\n\nTested on Safari (OS X, iOS), Chrome (Windows, OS X)\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks! We are fixing this internally (upstream) since it requires updates to other source files as well. Will close this issue once the internal change surfaces as a commit here and will reference it.\n\nThanks for finding this and bringing it to our attention!\n"]}, {"number": 2721, "title": "tensorflow  performance of model evaluation using C++ is very lower than  using python. The model was trained by python ,and freezed to loaded by c++.", "body": "tensorflow evaluation using c++  has very lower performance, 100 times lower than using python. \n1.The  model was trained using python,  freezed(using freeze_graph.py tool),  and then loaded by c++. \n2.The evaluation speed :  1K+/s using python ,VS 10/s using C++.\n", "comments": ["Please could you provide more detail about what you are trying to do here, in particular could you provide the code you are trying to 'freeze' and the exact command lines used to build and execute both the 'fast' and 'slow' versions.\n\n@petewarden  - could you please take a look at this once the above information is provided.\n", "@petewarden  I also had the same problem. I tested `tensorflow/examples/label_image/main.cc` and it took 5 seconds or more to predict the result. However, evaluation tested using python only took 0.5 seconds more or less.\nDo you have any idea about the difference about performance?\n", "If compiling the binary using bazel, I suspect you are not compiling your binary with -c opt or --copt=-mavx  on the command lines.  But we'd need more information to debug this anyway.\n", "@vrv Thank you very much! \nIt really helps.\n", "@vrv thanks for you comment,  bazel need build binary with option   --config=cuda to enable GPU.\n", "@vrv Hi can you pls elaborate on the compilation part. I compile using bazel of my c++ code yet it takes more time than my python "]}, {"number": 2720, "title": "added complex128 to release notes", "body": "", "comments": []}, {"number": 2719, "title": "RNN related API documents", "body": "Do you have any plan to release the RNN related API documents? I still cannot find them in TensorFlow R0.9 API doc: https://www.tensorflow.org/versions/r0.9/api_docs/python/index.html.\n\nThis was also asked in StackOverflow: http://stackoverflow.com/questions/37159372/.\n", "comments": ["The API docs will be updated soon; as we _just_ released the RNN docs in HEAD.  Not sure if this will end up in 0.9 or the next version.  @martinwicke?\n", "Can you check the 0.9 docs? If the docs are there they're there.\nOn Sun, Jun 12, 2016 at 20:12 ebrevdo notifications@github.com wrote:\n\n> The API docs will be updated soon; as we _just_ released the RNN docs in\n> HEAD. Not sure if this will end up in 0.9 or the next version.\n> @martinwicke https://github.com/martinwicke?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2719#issuecomment-225481251,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_TyAVXLMbjsEEo2GA64KAcVKLZlBks5qLMqFgaJpZM4IwhYu\n> .\n", "@martinwicke for some reason they don't show up.\n", "I also noticed.\n\nOn Fri, Jun 17, 2016 at 11:28 AM ebrevdo notifications@github.com wrote:\n\n> @martinwicke https://github.com/martinwicke for some reason they don't\n> show up.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2719#issuecomment-226671910,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AA3DV7KRpX8teALsXjf7YF0VLd5H4YZsks5qMhRkgaJpZM4IwhYu\n> .\n", "Should be fixed. Reopen if it's still an issue.\n"]}, {"number": 2718, "title": "Branch 124305034", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2717, "title": "Branch 124290852", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2716, "title": "Add a lite version of libtensorflow-core to Makefile", "body": "The tensorflow iOS libraries created by the makefile have a very large footprint ( > 90 MB ).\n\nTo help reduce the size of apps, there should also be a lite version of the libraries, possibly based on the Bazel android_tensorflow_lib_lite target.\n\nNote that Apple prevents apps larger than 100 MB from being downloaded over the air.\n", "comments": ["@StephenOman we've reduced the footprint to about 11MB per architecture, which we're still working on shrinking but should be a lot more usable. Can you let me know if you're seeing that reduction too?\n", "Thanks @petewarden. Will check it out. \n", "I'm still getting libs between 75MB and 90MB per architecture based on a clean clone of the head last night and running build_all_ios.sh. Has the reduced footprint configuration been added to the main repo?\n", "It has, but one thing I realize I haven't documented well is passing in optimization flags to the build process. After doing that I see a single-architecture executable for the simple example of around 14MB (about 11MB for the lib and 3MB for the app code I think).\n\n``` bash\ntensorflow/contrib/makefile/compile_ios_tensorflow.sh \"-Os\"\n```\n", "That's better than I'm getting. With that compiler option, I'm seeing approx 30/31 MB per lib per architecture.\n\nFor information, this is my gcc:\n\n```\nmakefile $  gcc --version\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include\ndir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.11.sdk/usr/include/c++/4.2.1\nApple LLVM version 7.3.0 (clang-703.0.31)\nTarget: x86_64-apple-darwin15.5.0\nThread model: posix\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\nmakefile $\n```\n\nAlso note that build_all_ios.sh doesn't pass command line arguments to compile_ios_tensorflow.sh. That caught me out when I tried your suggestion!\n", "Good point on the build_all not passing through arguments, I should look at that.\n\nAre you seeing this when you build the Simple example at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/simple ? The reason I'm asking is that the size of the libs on disk and the final size they contribute to an executable can be pretty different, so I want to check how you're building your final binary.\n", "I've followed the instructions as outlined in the docs and at the moment, I'm still seeing 30MB libs on the disk.\n\nThe Simple example generates an app of 68MB on disk (building it in Xcode IDE). I haven't changed anything from the downloaded files.\n", "Are you building with release settings? I see a binary size of 13Mb when building the library with \"-Os\" (now the default with build_all_ios.sh) and then choosing Release in Edit Schemes in Xcode.\n", "Yes thanks that works now. I see Simple App binary is 15.3MB on the disk.\n\nBefore I close this out, what is the purpose of the android_tensorflow_lib_lite collection in the Bazel BUILD in tensorflow/core?\n", "That's to have a version of the library that doesn't include any operators, so we can pick and choose exactly which implementations we use to reduce the code size even further. That's still a work in progress  though.\n"]}, {"number": 2715, "title": "error looking for libcudart.so.7.0 instead of 7.5 when building from sources (r0.9) even though configured to use cuda 7.5", "body": "### Summary\n\nI've configured everything to use cuda 7.5 / cudnn 5.0.5. After building from source cc example is working fine with libcudart.so.7.5. However when I import tensorflow in python the import fails saying it's unable to load libcudart.so.7.0 - which doesn't and shouldn't exist on my system. I'm not sure why it's looking for that. \n### Environment info\n\nOperating System: Linux 3.19.0-59-generic 14.04.1-Ubuntu\n\nInstalled version of CUDA and cuDNN: 7.5.18 & 5.0.5\n\n> memo@MSA-Blade:~$ ls -l /usr/local/cuda/lib64/libcud*\n> -rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\n> lrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\n> lrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n> -rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n> -rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n> -rwxr-xr-x 1 root root 59909104 Jun  7 10:25 /usr/local/cuda/lib64/libcudnn.so\n> -rwxr-xr-x 1 root root 59909104 Jun  7 10:25 /usr/local/cuda/lib64/libcudnn.so.5\n> -rwxr-xr-x 1 root root 59909104 Jun  7 10:25 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n> -rw-r--r-- 1 root root 58775484 Jun  7 10:25 /usr/local/cuda/lib64/libcudnn_static.a\n\nNote: nvcc is reporting v7.5.17 while installed is v7.5.18\n\n> memo@MSA-Blade:~$ nvcc --version\n> nvcc: NVIDIA (R) Cuda compiler driver\n> Copyright (c) 2005-2015 NVIDIA Corporation\n> Built on Tue_Aug_11_14:27:32_CDT_2015\n> Cuda compilation tools, release 7.5, V7.5.17\n> memo@MSA-Blade:~$ which nvcc\n> /usr/local/cuda/bin/nvcc\n> memo@MSA-Blade:~$ ls -l /usr/local/cuda\n> lrwxrwxrwx 1 root root 8 Jun  7 11:07 /usr/local/cuda -> cuda-7.5\n> memo@MSA-Blade:~$ head /usr/local/cuda/version.txt\n> CUDA Version 7.5.18\n\nIf installed from sources, provide the commit hash:\ntried master: a0085c8a689893a116be72ac83774c0cc3fa59f9\nand r0.9: f05f72ee8f6dd91491226fe82f020c00f9fb882d\n### Steps to reproduce\n1. uninstalled tensorflow 0.7 and purged cuda 7.0/cudnn 4.0. In fact purged all instances of cuda (and restarted) before each attempt\n2. followed instructions exactly to install cuda 7.5 (from deb) and cudnn 5.0 (to /usr/local/cuda)\n3. reboot\n4. clone tensorflow repo \n5. ./configure with no cloud platform support, gpu support \n6. build cc:tutorials_example_trainer (-c opt --config=cuda) -> builds and _runs fine_ \n\n> memo@MSA-Blade:~/DEV/tensorflow$ bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5.18 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5.0.5 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5.18 locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\n> I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5.18 locally\n> I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n> name: GeForce GTX 970M\n> major: 5 minor: 2 memoryClockRate (GHz) 1.038\n> pciBusID 0000:01:00.0\n> Total memory: 3.00GiB\n> Free memory: 2.60GiB\n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n> I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:807] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0)\n1. build pip package (-c opt --config=cuda), wheel and sudo pip install -> _FAILS ON IMPORT_\n\n> memo@MSA-Blade:~/DEV/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n> Tue Jun 7 13:08:59 PDT 2016 : === Using tmpdir: /tmp/tmp.wuLJ2u2tY7\n> /tmp/tmp.wuLJ2u2tY7 ~/DEV/tensorflow\n> Tue Jun 7 13:09:00 PDT 2016 : === Building wheel\n> ~/DEV/tensorflow\n> Tue Jun 7 13:09:09 PDT 2016 : === Output wheel file is in: /tmp/tensorflow_pkg\n> memo@MSA-Blade:~/DEV/tensorflow$ ls -al /tmp/tensorflow_pkg/\n> total 47820\n> drwxrwxr-x  2 memo memo     4096 Jun  7 13:09 .\n> drwxrwxrwt 15 root root   118784 Jun  7 13:09 ..\n> -rw-rw-r--  1 memo memo 48839608 Jun  7 13:09 tensorflow-0.9.0rc0-py2-none-any.whl\n> memo@MSA-Blade:~/DEV/tensorflow$ sudo pip install /tmp/tensorflow_pkg/tensorflow-0.9.0rc0-py2-none-any.whl \n> [sudo] password for memo: \n> Unpacking /tmp/tensorflow_pkg/tensorflow-0.9.0rc0-py2-none-any.whl\n> Requirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)\n> Requirement already satisfied (use --upgrade to upgrade): protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)\n> Requirement already satisfied (use --upgrade to upgrade): wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)\n> Requirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0)\n> Installing collected packages: tensorflow\n> Successfully installed tensorflow\n> Cleaning up...\n> memo@MSA-Blade:~/DEV/tensorflow$ cd ..\n> memo@MSA-Blade:~/DEV$ ipython\n> Python 2.7.11 |Anaconda 4.0.0 (64-bit)| (default, Dec  6 2015, 18:08:32) \n> Type \"copyright\", \"credits\" or \"license\" for more information.\n> \n> IPython 4.1.2 -- An enhanced Interactive Python.\n> ?         -> Introduction and overview of IPython's features.\n> %quickref -> Quick reference.\n> help      -> Python's own help system.\n> object?   -> Details about 'object', use 'object??' for extra details.\n> \n> In [1]: import tensorflow\n> ImportError                               Traceback (most recent call last)\n> <ipython-input-1-a649b509054f> in <module>()\n> ----> 1 import tensorflow\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/**init**.py in <module>()\n>      21 from **future** import print_function\n>      22 \n> ---> 23 from tensorflow.python import *\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/**init**.py in <module>()\n>      47 \n>      48 # Import things out of contrib\n> ---> 49 from tensorflow import contrib\n>      50 \n>      51 # Framework\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/**init**.py in <module>()\n>      21 \n>      22 # Add projects here, they will show up under tf.contrib.\n> ---> 23 from tensorflow.contrib import layers\n>      24 from tensorflow.contrib import util\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/**init**.py in <module>()\n>      66 # pylint: disable=unused-import,wildcard-import\n>      67 from tensorflow.contrib.layers.python.framework.tensor_util import *\n> ---> 68 from tensorflow.contrib.layers.python.layers import *\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/**init**.py in <module>()\n>      20 \n>      21 # pylint: disable=wildcard-import\n> ---> 22 from tensorflow.contrib.layers.python.layers.initializers import *\n>      23 from tensorflow.contrib.layers.python.layers.layers import *\n>      24 from tensorflow.contrib.layers.python.layers.regularizers import *\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/initializers.py in <module>()\n>      22 \n>      23 from tensorflow.python.framework import dtypes\n> ---> 24 from tensorflow.python.ops import random_ops\n>      25 \n>      26 \n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py in <module>()\n>      21 \n>      22 from tensorflow.python.framework import dtypes\n> ---> 23 from tensorflow.python.framework import ops\n>      24 from tensorflow.python.framework import tensor_shape\n>      25 from tensorflow.python.framework import tensor_util\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py in <module>()\n>      37 from tensorflow.python.framework import registry\n>      38 from tensorflow.python.framework import tensor_shape\n> ---> 39 from tensorflow.python.framework import versions\n>      40 from tensorflow.python.util import compat\n>      41 from tensorflow.python.platform import logging\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/versions.py in <module>()\n>      20 from **future** import print_function\n>      21 \n> ---> 22 from tensorflow.python import pywrap_tensorflow\n>      23 \n>      24 **version** = pywrap_tensorflow.**version**\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in <module>()\n>      26                 fp.close()\n>      27             return _mod\n> ---> 28     _pywrap_tensorflow = swig_import_helper()\n>      29     del swig_import_helper\n>      30 else:\n> \n> /home/memo/anaconda2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py in swig_import_helper()\n>      22         if fp is not None:\n>      23             try:\n> ---> 24                 _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n>      25             finally:\n>      26                 fp.close()\n> \n> ImportError: libcudart.so.7.0: cannot open shared object file: No such file or directory\n### What have you tried?\n1. tried both master and r0.9 branches\n2. for cuda and cudnn version numbers tried keeping them system default (/usr/local/cuda is symlinked to /usr/local/cuda-7.5) AND tried giving explicit version numbers (cuda: 7.5, 7.5.18; cudnn: 5, 5.0.5 )\n3. If I purge Cuda 7.5 and only use cuda 7.0 / cudnn 4.0.7 it works fine. \n### Logs or other output that would be helpful\n", "comments": ["I just found and fixed the problem, and I'll post it here for other people. Problem is difference between 'sudo pip install' vs 'pip install' and in my case, the fact that I'm using anaconda. \n\n'sudo pip install' installs in system path (/usr/lib/python2.7/dist-packages or similar)\n'pip install' installs in a user path (~/anaconda2/lib/python2.7/site-packages or similar. will be different if not using anaconda)\n\nApparently this computer had tensorflow installed in both user and system paths. So 'sudo pip uninstall tensorflow' uninstalled tensorflow from the system path, but not the user path. I was installing the new tensorflow with 'sudo pip install' (into system path), but there already was a tensorflow (0.7) in user path. _That's_ the version of tensorflow which was being imported, and thus getting the error. \n\nSo I uninstalled all instances of tensorflow from both user path (pip uninstall tensorflow) and system path (sudo pip uninstall tensorflow). And reinstalled the one I compiled into user path only (pip install /tmp/tensorflow_pkg/tensorflow-0.9.0rc0-py2-none-any.whl), notice lack of 'sudo'.\n\nTo see if tensorflow is already installed in system and/or user paths:\n'pip show tensorflow' will show tensorflow version (if it exists) in user path\n'sudo pip show tensorflow' will show tensorflow version (if it exists) in system path\n"]}]