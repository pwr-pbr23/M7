[{"number": 33622, "title": "Pack() used in mixed precision ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: Tesla-V100-SXM2\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI want to use the clip_by_global_norm, but it does not work when the variables are mixed precision: float32 and float16 because it used Pack() function somewhere. \r\nIf I cast all the vars into float16 or float32,  it brings up other problems like 'can not add float32 with float16' in other operations.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@estelll, In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "@estelll, Please provide the minimal code to replicate the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33621, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ABS, ADD, BATCH_TO_SPACE_ND, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, GREATER, LESS, LOGICAL_AND, LOGICAL_NOT, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, NEG, PACK, PAD, RELU, RESHAPE, RSQRT, SELECT, SOFTMAX, SPACE_TO_BATCH_ND, SQUARE, STRIDED_SLICE, SUB, SUM, TOPK_V2. Here is a list of operators for which you will need custom implementations: FIFOQueueV2, QueueDequeueV2, SparseSoftmaxCrossEntropyWithLogits.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["As the error message suggested you will need custom implementations for following operations;\r\n```FIFOQueueV2, QueueDequeueV2, SparseSoftmaxCrossEntropyWithLogits..```\r\n\r\n- To make a custom op you may refer https://www.tensorflow.org/lite/guide/ops_custom\r\n- You can also raise a request to add these features in TF by commenting on this issue thread.\r\n  https://github.com/tensorflow/tensorflow/issues/21526\r\n- You can try converting your model using ```TFLITE_BUILTINS, SELECT_TF_OPS``` flags to reduce custom implementation of ops (only a subset of ops can be implemented by this method while others may still require custom implementation )"]}, {"number": 33620, "title": "Refactor {Window, Zip}DatasetOpTest", "body": "This PR refactors `WindowDatasetOpTest` and `ZipDatasetOpTest`. ", "comments": []}, {"number": 33619, "title": "Unable to execute python program  with Keras / Tensorflow ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nPC\r\n- TensorFlow installed from (source or binary):\r\nKeras                2.3.1\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\ntensorboard          2.0.0\r\ntensorflow           2.0.0\r\ntensorflow-estimator 2.0.1\r\nInstalled Python version from pip within Visual Studio\r\n- TensorFlow version:  above\r\n- Python version:   3.7.4 64-bit\r\n- Installed using virtualenv? pip? conda?:   installed via pip\r\n- Bazel version (if compiling from source):    n/a\r\n- GCC/Compiler version (if compiling from source):  n/a\r\n- CUDA/cuDNN version:   do not have Cuda installed, want to run this on the CPU as my GPA is a Nvid GTS-250\r\n- GPU model and memory:\r\nNvid GTS-250 @ 2 cards\r\nCPU is an i7 Intel 930 @ 2.8GHz, Quad Core\r\nSystem has 12G of RAM\r\n\r\n**Describe the problem**\r\nI am able to import keras and numpy into my python program, however when I attempt to run the program I see the following error...\r\n\r\nH:\\Python\\Projects\\Machine_Learning>C:/Users/Userid/AppData/Local/Programs/Python/Python37/python.exe \"h:/Python/Projects/Machine_Learning/44 - first_deep_learning_example.py\"\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"h:/Python/Projects/Machine_Learning/44 - first_deep_learning_example.py\", line 2, in <module>\r\n    from keras.models import Sequential\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Userid\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code 3221225501\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["perhaps my CPU doesn't support AVX?  apparently everything after tensorFlow 1.6 requires AVX.  I tried installing an older version with:\r\n\r\nH:\\Python\\Projects\\Machine_Learning>pip3 install tensorflow==1.5\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0)\r\nERROR: No matching distribution found for tensorflow==1.5\r\n\r\nI'm also reading some posts that I should perhaps run Anaconda which I'm not really familiar with... \r\n\r\nLink here doesn't have AVX listed on my CPU\r\nhttp://www.cpu-world.com/Compare/387/Intel_Core_i7_i7-4770_vs_Intel_Core_i7_i7-930.html\r\n", "the version of tensorFlow that I have installed should be the CPU version.  As my video card is a GTS250, that would require CUDA and I don't think that it is supported on my GTS250 cards...\r\n\r\nhttps://www.nvidia.com/en-us/geforce/forums/game-ready-drivers/13/227869/geforce-gts-250-cuda-drivers/", "@Belgrath1900  The newest Tensorflow requires AVX2 support from the CPU. You can also refer to the following [link](https://devtalk.nvidia.com/default/topic/1057009/error-code-3221225501-when-trying-to-run-tensorflow/).", "Thanks @gowthamkpr, is there a version of TensorFlow that I could use with either my GPU or CPU or is this machine about to be tossed out?", "You can build tensorflow from source. Please refer to the following [link](url).", "> You can build tensorflow from source. Please refer to the following [link](url).\r\n\r\nI assume that this was the link:  https://www.tensorflow.org/install/source_windows\r\n\r\nSo by building my own version on this PC, it'll bypass any AVX support?  I see further on down in the above link that it talks about Python version 3.5.x is the issue my currently install python 3.7.. I've never compiled from source before so this is a bit of a new adventure :)", "Yes @Belgrath1900 I am gonna close this issue as of now and if you face any issues while building Tensorflow from source, open a new one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33619\">No</a>\n"]}, {"number": 33618, "title": "massive c++14 warning during build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 1903 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.4 x64\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source):   VS 2017 build tools\r\n- CUDA/cuDNN version: 10.1 / 7.6.4\r\n- GPU model and memory: RTX 2080Ti GDDR6 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nmassive unknown option warning from compiler\r\n\r\nthe option '-std=c++14' should not be used\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\ncl : Command line warning D9002 : ignoring unknown option '-std=c++14'\r\n```\r\n", "comments": ["@alanpurple \r\n\r\nCan you please let us know the output of `./configure.py`. Thanks!\r\n", "@ravikyram \r\n```\r\nPS D:\\repo\\tensorflow> .\\configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is C:\\Anaconda3\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 6.1,7.5\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```", "Does your compiler support c++14?", "@mihaimaruseac \r\nas the blog linked below says, yes\r\n\r\nhttps://mariusbancila.ro/blog/2017/03/08/whats-new-in-visual-studio-2017-for-cpp-development/", "I get this also.  I tried both bazel 0.29.1 and 1.2.0.  The problem is that '-std=c++14' is not a valid flag for cl.  The cl flag that should be used to specify the C++ 14 standard  is '/std:c++14'.  \r\n", "yeah ~ above comment seems to be the reason to the issue", "> I get this also. I tried both bazel 0.29.1 and 1.2.0. The problem is that '-std=c++14' is not a valid flag for cl. The cl flag that should be used to specify the C++ 14 standard is '/std:c++14'.\r\n\r\nWhere do you update this? Thanks", "@jevad Can you tell me the build command or the file where '/std:c++14' should be specified?\r\n\r\n> I get this also. I tried both bazel 0.29.1 and 1.2.0. The problem is that '-std=c++14' is not a valid flag for cl. The cl flag that should be used to specify the C++ 14 standard is '/std:c++14'.\r\n", "@alanpurple ,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33618\">No</a>\n"]}, {"number": 33617, "title": "[ROCm] Fix for the broken ROCm CSB.", "body": "The following commit breaks the `--config=rocm` build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/9d08b6bb4fc96073e00c84b0ad7d987a06a53fdb\r\n\r\nThe commit adds the implementation code for T=int32 within `#if GOOGLE_CUDA`, but the code that registers the op+kernel for T=int32 is within `#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM`. This discrepancy leads to build errors. The fix is trivial, which is to put the implementation code within `#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM` as well.\r\n\r\n-------------------------------------------------------------------------\r\n\r\n\r\n@whchung @chsigg \r\n", "comments": ["similar changes are merged internally 5 days ago , thank you "]}, {"number": 33616, "title": "TFLiteConverter StridedSlice Error for Transformer Example Notebook", "body": "**System information**\r\n- Linux Ubuntu 18.04.2\r\n- Installed from binary\r\n- TensorFlow version 2.0.0\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-82-0b556551beac> in <module>()\r\n      3 \r\n      4 converter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\n----> 5 tflite_model = converter.convert()\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    444         input_tensors=input_tensors,\r\n    445         output_tensors=output_tensors,\r\n--> 446         **converter_kwargs)\r\n    447 \r\n    448     if self._is_calibration_quantize():\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-10-22 12:59:50.304931: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 915 operators, 1487 arrays (0 quantized)\r\n2019-10-22 12:59:50.337611: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 915 operators, 1487 arrays (0 quantized)\r\n2019-10-22 12:59:50.397450: F tensorflow/lite/toco/graph_transformations/resolve_strided_slice_attributes.cc:95] Check failed: start_indices_size <= num_input_axes (4 vs. 2)StridedSlice op requires no more than 2 start indices\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f2d4bb1e740 (most recent call first):\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52 in execute\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89 in main\r\n  File \"/home/mattc/anaconda3/envs/main/bin/toco_from_protos\", line 10 in <module>\r\nAborted (core dumped)\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nI am following this notebook: https://www.tensorflow.org/tutorials/text/transformer\r\nI am interested in doing NMT on an Android device with TFLite. The NMT Attention notebook uses an LSTM which is not supported yet for TF Lite conversion so I am looking into this approach that only uses Attention, Embeddings, and Dense layers in hope that it is convertible. \r\n\r\nI haven't edited the code from that notebook at all, but created a new inference concrete function and am trying to convert it. I am getting this vague error about the StridedSlice op.\r\n\r\nLooking for some guidance on what I can do from here.\r\n\r\n```python\r\ntf.random.set_seed(1234)\r\n\r\neval_step_signature = [\r\n    tf.TensorSpec(shape=(BATCH_SIZE, 64), dtype=tf.int64),\r\n    tf.TensorSpec(shape=(BATCH_SIZE, 26), dtype=tf.int64),\r\n]\r\n\r\n@tf.function(input_signature=eval_step_signature)\r\ndef eval_step(inp, tar):\r\n\r\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar)\r\n    \r\n    predictions, _ = transformer(inp, tar, \r\n                                 True, \r\n                                 enc_padding_mask, \r\n                                 combined_mask, \r\n                                 dec_padding_mask)\r\n    return predictions\r\n\r\nckpt.f = eval_step\r\nto_save = ckpt.f.get_concrete_function()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\ntflite_model = converter.convert()\r\n```\r\n\r\nThank you!", "comments": ["I have also encountered the same problem. Looks like it is from create_padding_mask function.", "Hi @mattc-eostar ,\r\n\r\nI could resolve the issue with:\r\n```python\r\nckpt.f = eval_step\r\nto_save = ckpt.f.get_concrete_function()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\nThank you.", "@mattc-eostar \r\n\r\nCan you please check and confirm whether your issue was resolved by adding below 2 lines of code before converting\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nPlease, close this thread if your issue was resolved.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33616\">No</a>\n"]}, {"number": 33615, "title": "Include path problem when compiling TensorFlow from source using compiler in non-standard system location.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0 master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1.1 / 7.5\r\n- GPU model and memory: V100 (32 GB)\r\n\r\n**Describe the problem**\r\nThere is a major issue in installing TensorFlow when using compiler(s) in non-standard system locations. There is no provided tutorial or documentation on how to configure bazel and tensorflow in order to compile TensorFlow using different compilers.\r\n\r\nIn my case,  I have been struggling to compile TensorFlow from source using gcc/8.3.0 which is installed in a non-standard systm locations. I invoke this compiler by using \r\n`module load gcc/8.3.0`\r\nAll the include and library paths to this compiler are set properly as I use it for compiling many other programs. For clarity I display the configuration of this compiler below \r\n\r\n```\r\n(tensorflow2.0-master) -bash-4.2$ module show gcc/8.3.0/gcc-4.8.5 \r\n-------------------------------------------------------------------\r\n/gpfslocalsup/pub/modules-idris/modulefiles/linux-rhel7-x86_64/gcc/8.3.0/gcc-4.8.5:\r\n\r\nmodule-whatis\t The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages. \r\nconflict\t gcc \r\nprepend-path\t PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin \r\nprepend-path\t MANPATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/share/man \r\nprepend-path\t LD_LIBRARY_PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib \r\nprepend-path\t LIBRARY_PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib \r\nprepend-path\t LD_LIBRARY_PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib64 \r\nprepend-path\t LIBRARY_PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib64 \r\nprepend-path\t CPATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include \r\nprepend-path\t CMAKE_PREFIX_PATH /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/ \r\nsetenv\t\t CC /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/gcc \r\nsetenv\t\t CXX /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/g++ \r\nsetenv\t\t FC /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/gfortran \r\nsetenv\t\t F77 /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/gfortran \r\nsetenv\t\t F90 /gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/gfortran \r\n-------------------------------------------------------------------\r\n```\r\nI compile bazel-0.29.1 after modifying ` tools/cpp/cc_toolchain_config.bzl` to set the hardcoded path to `gcc`, `cpp`, `ar` and `nm`. I also add the include directories to `cxx_builtin_include_directories` in the file. \r\n\r\nI compile TensorFlow using the following command : \r\n\r\n`CC=/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/bin/gcc bazel --output_user_root=/tmp/ujjwal-builds build --action_env=PATH --action_env=LD_LIBRARY_PATH --config=opt --config=cuda --config=mkl --config=numa //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\nAnd then I get an error as shown below : \r\n```\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /tmp/ujjwal-builds/7d993f307acf01aa765c32a6dcabd368/external/com_google_absl/absl/strings/BUILD.bazel:83:1: undeclared inclusion(s) in rule '@com_google_absl//absl/strings:internal':\r\nthis rule is missing dependency declarations for the following files included by 'external/com_google_absl/absl/strings/internal/utf8.cc':\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/cstddef'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/c++config.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/os_defines.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/cpu_defines.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/stddef.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/cstdint'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/stdint.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /tmp/ujjwal/tensorflow/tensorflow/tools/pip_package/BUILD:49:1 undeclared inclusion(s) in rule '@com_google_absl//absl/strings:internal':\r\nthis rule is missing dependency declarations for the following files included by 'external/com_google_absl/absl/strings/internal/utf8.cc':\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/cstddef'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/c++config.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/os_defines.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/x86_64-pc-linux-gnu/bits/cpu_defines.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/stddef.h'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/include/c++/8.3.0/cstdint'\r\n  '/gpfslocalsup/spack_soft/gcc/8.3.0/gcc-4.8.5-opnwtdjumg2hxo4ljvnx77ugb6afmvj3/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/stdint.h'\r\nINFO: Elapsed time: 0.745s, Critical Path: 0.26s\r\nINFO: 4 processes: 4 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nBazel is being updated too frequently and there is no documentation or explanation on how to configure it to compile TensorFlow using other compilers. \r\n\r\nThis should be addressed for the benefits of users.\r\n", "comments": ["This issue has finally been solved. A complete description of the whole process can be found at \r\nhttps://medium.com/@ujjwalaryan/compiling-tensorflow-from-the-source-when-your-compiler-is-in-a-non-standard-location-194fecc92153", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33615\">No</a>\n"]}, {"number": 33614, "title": "[TF-Nighly] Keras object serialization breaks for certain object names", "body": "## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but seq2seq maintained by Google\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: All\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: tf-nightly > 20191018\r\n\r\n### Describe the problem\r\nSince b22fd65247633edfbc000c3bd9b9008502aedc33 serialization breaks if the name attribute of an object matches the name of a keras registered object. The check added compares `key,items` in a config and checks if the `item` matches the name of a registered object. This is brittle because `{'name': 'ObjectNameAttribute'}` appears in that config. Below I've attached a way to break the test case. \r\n\r\nThis breaks all Attention mechanism serializations from seq2seq. E.g:\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper.py#L687\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/attention_wrapper_test.py#L155\r\n\r\nBut also our activation layers who's default name matches the registered activation name:\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/layers/sparsemax.py#L27\r\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/sparsemax.py#L27\r\n\r\nI believe adding a check that `key` != `name` would fix this, but haven't thought through all of the implications.\r\n\r\n### Source code / logs\r\nModified test case that breaks. Notice the `name` of Dense Layer matches a `custom_object` and thus breaks. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\n\r\n\r\nclass SerDeTest(tf.test.TestCase):\r\n    def test_nested_serializable_fn(self):\r\n        def serializable_fn(x):\r\n            \"\"\"A serializable function to pass out of a test layer's config.\"\"\"\r\n            return x\r\n\r\n        class SerializableNestedInt(int):\r\n            \"\"\"A serializable object containing a serializable function.\"\"\"\r\n\r\n            def __new__(cls, value, fn):\r\n                obj = int.__new__(cls, value)\r\n                obj.fn = fn\r\n                return obj\r\n\r\n            def get_config(self):\r\n                return {'value': int(self), 'fn': self.fn}\r\n\r\n            @classmethod\r\n            def from_config(cls, config):\r\n                return cls(**config)\r\n\r\n        layer = keras.layers.Dense(\r\n            SerializableNestedInt(3, serializable_fn),\r\n            name='SerializableNestedInt',\r\n            activation='relu',\r\n            kernel_initializer='ones',\r\n            bias_regularizer='l2')\r\n        config = keras.layers.serialize(layer)\r\n        new_layer = keras.layers.deserialize(\r\n            config,\r\n            custom_objects={\r\n                'serializable_fn': serializable_fn,\r\n                'SerializableNestedInt': SerializableNestedInt\r\n            })\r\n        self.assertEqual(new_layer.activation, keras.activations.relu)\r\n        self.assertIsInstance(new_layer.bias_regularizer, keras.regularizers.L1L2)\r\n        self.assertIsInstance(new_layer.units, SerializableNestedInt)\r\n        self.assertEqual(new_layer.units, 3)\r\n        self.assertIs(new_layer.units.fn, serializable_fn)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.test.main()\r\n```\r\n\r\n`TypeError: __new__() missing 2 required positional arguments: 'value' and 'fn'`\r\n\r\ncc @qlzh727 as owner for the seq2seq code that is breaking.\r\n", "comments": ["The fix will reach nightly tomorrow.", "Thanks for the quick fix!", "Since https://github.com/tensorflow/addons/pull/621 is still failing for one edge case, thought I'd re-open this for tracking. ", "Sorry for the late reply, it seems that the issue on addon side has been addressed in https://github.com/tensorflow/addons/pull/669. Closing this issue for now."]}, {"number": 33613, "title": "[TFLite, Bug] Unexpected weights changes after conversion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): VERSION=2.0.0, GIT_VERSION=v1.12.1-15611-g025365a736\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA/cuDNN version: CUDA Version: 10.1\r\n- GPU model and memory: GeForce GTX 1080 Titan 11GB\r\n\r\n**Describe the current behavior**\r\nIn this issue code from tensorflow/models is used (tensorflow/models/official/nlp/bert_modeling.py). After conversion weights of model seriously changes. After some minimal fixes in source code of tensorflow/models it is solved but program mustn't changes. Below I show some cases. Code of keras layer EmbeddingPostprocessor:\r\n```\r\ndef call(self, inputs):\r\n    \"\"\"Implements call() for the layer.\"\"\"\r\n    unpacked_inputs = tf_utils.unpack_inputs(inputs)\r\n    word_embeddings = unpacked_inputs[0]\r\n    token_type_ids = unpacked_inputs[1]\r\n    input_shape = tf_utils.get_shape_list(word_embeddings, expected_rank=3)\r\n    batch_size = input_shape[0]\r\n    seq_length = input_shape[1]\r\n    width = input_shape[2]\r\n\r\n    output = word_embeddings\r\n    if self.use_type_embeddings:\r\n      flat_token_type_ids = tf.reshape(token_type_ids, [-1])\r\n      one_hot_ids = tf.one_hot(\r\n          flat_token_type_ids,\r\n          depth=self.token_type_vocab_size,\r\n          dtype=self.dtype)\r\n      token_type_embeddings = tf.matmul(one_hot_ids, self.type_embeddings)\r\n      token_type_embeddings = tf.reshape(token_type_embeddings,\r\n                                         [batch_size, seq_length, width])\r\n      output += token_type_embeddings\r\n\r\n    if self.use_position_embeddings:\r\n      position_embeddings = tf.expand_dims(\r\n          tf.slice(self.position_embeddings, [0, 0], [seq_length, width]),\r\n          axis=0)\r\n\r\n      output += position_embeddings\r\n\r\n    output = self.output_layer_norm(output)\r\n    output = self.output_dropout(output)\r\n\r\n    return output\r\n```\r\n\r\n1) Without changes in source code. Weights are incorrect.\r\n2) Fixes are shown under this point. In this case weights are correct.\r\n```\r\nif self.use_type_embeddings:\r\n      flat_token_type_ids = tf.reshape(token_type_ids, [-1])\r\n      token_type_embeddings = tf.gather(self.type_embeddings, flat_token_type_embeddings)\r\n      token_type_embeddings = tf.reshape(token_type_embeddings,\r\n                                         [batch_size, seq_length, width])\r\n      output += token_type_embeddings\r\n```\r\n3) Use EmbeddingPostprocessor without position_embeddings. In this case weights are correct. Example with full code is in the part \"Code to reproduce the issue\"\r\n```\r\noutputs = EmbeddingPostprocessor(\r\n    use_type_embeddings=True,\r\n    token_type_vocab_size=2,\r\n    use_position_embeddings=False,\r\n    dtype=tf.float32)(input1, input2)\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom official.nlp.bert_modeling import EmbeddingPostprocessor\r\n\r\nsize = 100\r\n\r\ninput1 = tf.keras.layers.Input(shape=(size, size), dtype=tf.float32, name='1')\r\ninput2 = tf.keras.layers.Input(shape=(size,), dtype=tf.int32, name='2')\r\noutputs = EmbeddingPostprocessor(\r\n    use_type_embeddings=True,\r\n    token_type_vocab_size=2,\r\n    dtype=tf.float32)(input1, input2)\r\nmodel = tf.keras.Model(inputs=[input1, input2], outputs=outputs)\r\n\r\nexample1 = tf.constant(np.random.random_sample(size=(1, size, size)), dtype=tf.float32)\r\nexample2 = tf.constant(np.random.randint(2, size=(1, size), dtype=np.int32))\r\nexample = {'1': example1, '2': example2}\r\n\r\noutput1 = model.predict(example)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.reset_all_variables()\r\ninterpreter.set_tensor(input_details[0]['index'], example[input_details[0]['name']])\r\ninterpreter.set_tensor(input_details[1]['index'], example[input_details[1]['name']])\r\ninterpreter.invoke()\r\n\r\noutput2 = interpreter.get_tensor(output_details[0]['index'])\r\n\r\nprint(np.sum(np.abs(output1 - output2)))\r\n```\r\n\r\n**Other info / logs**\r\n* With error:\r\n```\r\n*** INFO MESSAGES ***\r\n691.0672\r\n```\r\n* Without error:\r\n```\r\n*** INFO MESSAGES ***\r\n0.0021286607\r\n```", "comments": ["@Vooblin, Since the associated PR has been merged. Can we close this issue. ", "@Vooblin, Can you please let us know if you are happy to close if no issue persists. Thanks!", "@gadagashwini Sorry, I missed your last comment and forgot about this issue. Associated PR fixed only code in tensorflow/models. I think that this issue related to problem with tflite converter and PR didn't fix it. So I think that problem wasn't solved but if you think that it's not so important and it doesn't need to be fixed now then I won't mind closing this issue.", "@Vooblin,\r\nIs this still an issue? Could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33613\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33613\">No</a>\n"]}, {"number": 33612, "title": "improve tf.nn.l2_normalize() doc", "body": "Added usage example to improve documentation.", "comments": ["This is the guide you can use https://www.tensorflow.org/community/contribute/docs_ref"]}, {"number": 33611, "title": "An error occurred in build  quantized model to lite ", "body": "tensorflow 1.14.0\r\n\r\nwhen I build quantized SSD MobileNetV2 model, haapen error.  reference documentation: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n```\r\nbazel run -c opt tensorflow/lite/toco:toco -- \\\r\n--input_file=/home/zengpr/lite/model/tflite_graph.pb \\\r\n--output_file=/home/zengpr/lite/model/ssd_mobilenet_v2.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops\r\n```\r\n\r\n```\r\nArray FeatureExtractor/MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator \r\nproducing the output array FeatureExtractor/MobilenetV2/expanded_conv/depthwise/Relu6, is \r\nlacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-\r\nquantized output format, or run quantized training with your model from a floating point checkpoint\r\n to change the input graph to contain min/max information. If you don't care about accuracy, you can \r\npass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n```\r\n\r\n\r\nBut I used  --default_ranges_min= and --default_ranges_max= ,   model canit detection in predict.\r\n```\r\nbazel run -c opt tensorflow/lite/toco:toco -- \\\r\n--input_file=/home/zengpr/lite/model/tflite_graph.pb \\\r\n--output_file=/home/zengpr/lite/model/ssd_mobilenet_v2.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--default_ranges_min=0 \\\r\n--default_ranges_max=6 \\\r\n--allow_custom_ops\r\n```", "comments": ["Try to quantize this model with the post-training integer quantization tool:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba\r\n\r\nWe've tested it on mobilenet-v2-ssd models internally and it worked pretty well.\r\n\r\nFeel free to open another issue when you run into problem with it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33611\">No</a>\n"]}, {"number": 33610, "title": "fix tf.data.Dataset size inferring issue in TF 2.0 keras training loop", "body": "fix #33216 issue", "comments": []}, {"number": 33609, "title": "Arduino compilation fail : error: macro \"max\" requires 2 arguments, but only 1 given  max(0.0f),", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arduino Nano 33 IOT\r\n\r\n**Describe the problem**\r\n\r\nThere is a compilation issue :\r\nDocuments/Arduino/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/schema/schema_generated.h:6761:17: error: macro \"max\" requires 2 arguments, but only 1 given\r\n         max(0.0f),\r\n\r\nDocuments/Arduino/libraries/Arduino_TensorFlowLite/src/tensorflow/lite/schema/schema_generated.h:6778:13: error: macro \"max\" requires 2 arguments, but only 1 given\r\n   float max() const {\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nArduino IDE upload button\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Same problem here. Looks like the tf lib (schema_generated.h) declares min and max itself and the compiler confuses them with the arduino functions (for example: https://www.arduino.cc/reference/en/language/functions/math/min/)\r\n\r\nIf I load the \"Hello World\" example sketch and insert the following right before the \"TensorFlowLite.h\" include, I can compile:\r\n\r\n```\r\n#undef min\r\n#undef max\r\n```\r\nNot exactly elegant but suffices for testing.", "hi the 'correct' fix is to change the BSP package to use the modern arduino api template version of min/max\r\n\r\nhttps://github.com/adafruit/ArduinoCore-samd/commit/841a1b8188d254cd0cecc214fa55d454826bae23", "@digitalwert-developer  the method does solve my problem, thanks  \r\n`#undef min\r\n#undef max\r\n`", "@jujuz777,\r\n\r\nCan you take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/33609#issuecomment-554610960) from @ladyada and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33609\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33609\">No</a>\n"]}, {"number": 33608, "title": "\"Failed to call mkldnn_sgemm. Error code: 3\" Error when training a simple CNN model", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.1.0-dev20191021, 2.0.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\nTrying to build a model that uses CNN over text. \r\nminimal code to reproduce the error:\r\n\r\n```\r\nimport tensorflow\r\nfrom tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding, Lambda, Conv1D, GlobalMaxPool1D, concatenate\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom tensorflow.keras import Input\r\nimport string\r\n\r\nCHARS = string.punctuation + string.digits + string.ascii_letters + string.whitespace\r\nCHAR_MAP = tensorflow.lookup.StaticHashTable(tensorflow.lookup.KeyValueTensorInitializer(list(CHARS), list(range(1, len(CHARS)+1)), 'string', 'int32'), 0)\r\n\r\n\r\ndef string_to_vec(s):\r\n    return tensorflow.ragged.map_flat_values (CHAR_MAP.lookup, tensorflow.strings.unicode_split(s, 'UTF-8')).to_tensor()\r\n\r\n\r\ninput_layer = Input(shape=tuple(), ragged=False, dtype='string')\r\nfeature_layer = Lambda(string_to_vec)(input_layer)\r\nembedding_layer = Embedding(input_dim=len(CHARS)+1, output_dim=10, name='char_embeddings')(feature_layer)\r\ncnn_layer = concatenate([Conv1D(10, kernel_size, activation='relu', padding='same')(embedding_layer) for kernel_size in range(5)])\r\nmax_pool = GlobalMaxPool1D()(cnn_layer)\r\ndense_layer = Dense(1, activation='relu')(max_pool)\r\nmodel = Model(inputs=[input_layer], outputs=[dense_layer])\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\r\n\r\nprint(tensorflow.__version__)\r\n\r\nx = ['hello world', 'this is a test']\r\ny = [0] * len(x)\r\nprint(model.predict(x))\r\nmodel.fit(x,y)\r\n\r\n```\r\n\r\nOutput:\r\n```\r\n2.1.0-dev20191021\r\n[[0.]\r\n [0.]]\r\nTrain on 2 samples\r\n2019-10-22 15:29:13.322033: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call mkldnn_sgemm. Error code: 3\r\n\t [[{{node Conv2DBackpropInput}}]]\r\n2/2 [==============================] - 1s 285ms/sample\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-64494552324e>\", line 4, in <module>\r\n    runfile('/Users/ophir/dev/ophir/cnn_error.py', wdir='/Users/ophir/dev/ophir')\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/Users/ophir/dev/ophir/cnn_error.py\", line 29, in <module>\r\n    model.fit(x,y)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 777, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 337, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 127, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2339, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1589, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1670, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 521, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/miniconda3/envs/zr/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  Failed to call mkldnn_sgemm. Error code: 3\r\n\t [[node Conv2DBackpropInput (defined at /Users/ophir/dev/ophir/cnn_error.py:29) ]] [Op:__inference_distributed_function_1725]\r\nFunction call stack:\r\ndistributed_function\r\n\r\n```\r\n", "comments": ["@ophiry ,\r\nThank you for reporting, when tried running the given code, both in colab and jupyter notebook session seems to get crashed, can you please help me on this!?Thanks", "tried it on colab and got a crash too\r\n\r\nanother way to reproduce this is to run from within docker (tensorflow/tensorflow:latest-py3)\r\nthere I get the error: `Floating point exception`\r\n\r\n", "note also that turning of mkl using\r\n`export TF_DISABLE_MKL=1`\r\ndoesn't change the results", "The issue is that the line:\r\n`concatenate([Conv1D(10, kernel_size, activation='relu', padding='same')(embedding_layer) for kernel_size in range(5)])`\r\ncreates a convolutional layer with kernel size 0, which is meaning less.\r\n\r\nadding a validation for the kernel size can provide a clearer error message", "This is fixed with tf-nightly '2.2.0-dev20200331'. Also [check for 0 kernel_size](1e102f63964365d82d7f22402b7ba21e0e0e64fe) has been added.\r\n```python\r\nValueError: The argument `kernel_size` cannot contain 0(s). Received: 0\r\n```\r\nchanging `kernel_size in range(1,5)`\r\n```python\r\nconcatenate([Conv1D(10, kernel_size, activation='relu', padding='same')(embedding_layer) for kernel_size in range(1,5)])\r\n```\r\noutput:\r\n```python\r\n2.2.0-dev20200331\r\n[[0.]\r\n [0.]]\r\n1/1 [==============================] - 0s 2ms/step - loss: 0.0000e+00\r\n<tensorflow.python.keras.callbacks.History at 0x7fb382378240>\r\n```\r\nThanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "it works now (using 2.2.0rc3)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33608\">No</a>\n"]}, {"number": 33607, "title": "Coral Dev Board runtime error during prediction", "body": "I'm working on a classification problem. I used Inception v3 model in Keras for classification. I did post training quantization and converted my hdf5 model to TFLite. Further, I compiled my TFLite model to use it on TPU instead of CPU on dev board. It's giving runtime error when I run this code during prediction on dev board\r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last):\r\n  File \"inference_try.py\", line 25, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/home/mendel/.local/lib/python3.5/site-packages/tflite_runtime/interpreter_wrapper.py\", line 114, in AllocateTensors\r\n    return _interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Internal: :71 tf_lite_type != kTfLiteUInt8 (9 != 3)Node number 5 (EdgeTpuDelegateForCustomOp) failed to `prepare.`\r\n```\r\n**Describe the expected behavior**\r\nShould predict the outcomes successfully and not give error on `interpreter.allocate_tensors()` statement.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\n\r\ninterpreter = Interpreter(\r\n      model_path=\"/home/mendel/classification/inceptionV3Q_edgetpu.tflite\",\r\n      experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], img)\r\ninterpreter.invoke()\r\noutput_details1 = interpreter.get_output_details()[0]\r\nanswer= np.squeeze(interpreter.get_tensor(output_details1['index']))\r\n```\r\n", "comments": ["See also #33558, and my comment there. https://github.com/tensorflow/tensorflow/issues/33558#issuecomment-545041049. Appears to crash when model or input tensor is large, despite the fact that my model easily fits into TPU on-chip memory.", "The model you mentioned i.e., model.tflite, was that working fine or still giving error while executing? Looks like I'll need to reduce my model size to a huge extent so as to make it run successfully.\r\n```\r\nEdge TPU Compiler version 2.0.267685300\r\n\r\nModel compiled successfully in 2804 ms.\r\n\r\nInput model: inceptionV3Q.tflite\r\nInput size: 21.30MiB\r\nOutput model: inceptionV3Q_edgetpu.tflite\r\nOutput size: 22.49MiB\r\nOn-chip memory available for caching model parameters: 4.75MiB\r\nOn-chip memory used for caching model parameters: 4.74MiB\r\nOff-chip memory used for streaming uncached model parameters: 16.91MiB\r\nNumber of Edge TPU subgraphs: 1\r\nTotal number of operations: 165\r\nOperation log: inceptionV3Q_edgetpu.log\r\n\r\nModel successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\r\nNumber of operations that will run on Edge TPU: 161\r\nNumber of operations that will run on CPU: 4\r\nSee the operation log file for individual operation details.\r\n```", "My model compiled and executed on the TPU without runtime error after I reduced model size (though I don't see why size reduction is/was needed, given the modest size even of my larger model). I did not retrain the smaller model, just used random weights as proof of concept.\r\n\r\nYes, @bhavitvyamalik, your model looks very large. I have no idea what the delays would be like when using \"off-chip memory used for streaming uncached model parameters.\"", "@bhavitvyamalik, Can you try the @mattroos's suggestion and let us know how it progresses. Thanks! ", "Since I was using InceptionV3 as my base model so even after reducing image size to 100x100 my model size was around 85MB which as @mattroos mentioned was too large. Eventually I had to change my base model to MobileNetV2 to bring model size under 3MB and then it worked like a charm! \r\n\r\n@gadagashwini I'm still working on running InceptionV3 model on Edge TPU. Even though official documentation says inceptionV3 is supported still it doesn't work. Please look into if possible. Thanks!\r\n", "I was able to run InceptionV3 model on EdgeTPU. The issue was not with size of the model but with the quantization process. Here is the code on how to quantize and convert the code to TFLite:\r\n```\r\ndef representative_data_gen():\r\n  for input_value in mnist_data[:100]:      //mnist_data list contains images\r\n    data = np.array([input_value])\r\n    yield [data]\r\n\r\nopt = tf.lite.Optimize.DEFAULT\r\nops = tf.lite.OpsSet.TFLITE_BUILTINS_INT8\r\ndtype = tf.uint8\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(model_path)\r\n\r\nconverter.optimizations = [opt]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [ops]\r\nconverter.inference_input_type = dtype\r\nconverter.inference_output_type = dtype\r\n\r\ntflite_quant_model = converter.convert()\r\nopen(\"model_quantised.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\nTensorflow version used was 1.15.0. If you use TF2.0 and use from_keras_model instead, it will give the same error.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33607\">No</a>\n"]}, {"number": 33606, "title": "TFLite_Micro: Implementing version 3 of convolutional layers for Post-training fully-integer quantization", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0, up to date with master\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nBy following the second part of the tutorial on [Post-training integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant),  thus providing also a representative dataset and setting\r\n```python\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\nthe resulting converted model is fully quantized, including inputs and outputs.\r\nHowever, if there are convolutions operations in the model, the converted versions will be \"Version 3\", which is not currently supported by Tflite for Microcontrollers, as shown in all_ops_resolver.cc lines 22 and 30\r\n```c++\r\n  AddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D());\r\n ...\r\n AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D());\r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\nThe current tflite micro api will be changed by including support to fully quantized convolutional layers as currently happens in TFLite\r\n**Who will benefit with this feature?**\r\nEveryone who wants to deploy fully quantized convolutional layers on a microcontroller without floating point support (or in all those situations where due to energy constraints, FPU cannot be used)\r\n**Any Other info.**\r\n", "comments": ["Agreed this is a problem.  Newer versions of tflite ops are being added to micro.  @njeffrie we do have some quantized support checked in for conv and depthwise conv.  What work remains to fully support version 3 of these ops?", "I've been working on updating the reported op versions for each operator.  We currently do support version 3 of conv2d, which is reflected in a CL I created yesterday.  That CL should fix this issue.", "Hi @njeffrie, sorry I am not sure I fully got it. Is this change merged with the master or I find it as pull request somewhere? Because at the moment, despite in conv.cc and depthwise_conv.cc there are the EvalQuantizedPerChannel fuctions, it still does not work. If I try to manually register also version 3 by modifying all_ops_resolver.cc as follows\r\n```python\r\nAddBuiltin(BuiltinOperator_DEPTHWISE_CONV_2D, Register_DEPTHWISE_CONV_2D(),1,3);\r\n```\r\nit crashes at Line 233 of micro/kernels/all_ops_resolver.cc\r\n```python\r\nTF_LITE_ENSURE_STATUS(CalculateOpData(context, node, params, width, height,\r\n                                        filter_width, filter_height, data_type,\r\n                                        &data));\r\n```\r\n", "Sorry for the delay - it's currently an internal change awaiting review.  I plan to land it early next week.  Can you paste the entire error log?  I would like to dig into the error you're seeing once you make the change to AddBuiltin.", "I believe this change has been merged.  Could you retry your model @antofara ?", "@rockyrhodes \r\nIn all_ops_resolver both BuiltinOperator_DEPTHWISE_CONV_2D and BuiltinOperator_CONV_2D are still registered as version 1, thus MicroMutableOpResolver::FindOp still returns, as expected, nullptr.\r\nIf I enforce it by modifying all_ops_resolver, registering version 3 of both operators, the program executes until \r\n`TfLiteStatus invoke_status = registration->invoke(&context_, &node);`\r\n(micro_interpreter.cc:254)\r\nand then it exits because invoke_status is kTfLiteError", "Sorry, you're right.  The change to all_ops_resolver is still stuck here at Google in code review.  I'll work with @njeffrie to get it submitted.", "@rockyrhodes \r\nIt seems that the change has been pushed, thanks! Can I close this issue?"]}, {"number": 33605, "title": "OUT_OF_MEMORY error on Mali GPU.", "body": "Hey there!\r\nIn my project I try to run inference on a simple one input model via GPU delegate and SSBO.\r\n\r\nI have 5 devices: Nexus 5X, Xiaomi Mi A1, Xiaomi Mi T9 Pro, Honor 10 and Huawei Mate 20.\r\n\r\nFirst three devices are on Adreno GPU and everything works perfectly, but in Huawei and Honor it turned out that during the intertpreter invoke method OpenGL ES OUT_OF_MEMORY error tracked on glGetUniformLocation call. I tried out different approaches to avoid this, but it didn't help. \r\n\r\nAt the moment of crash usage of Graphics memory is about 256 MB out of 1 GB.\r\n\r\nMaybe you could help me with that?", "comments": ["Maybe any updates on that?", "Sorry I'm out on a conference. I'll likely into this when I'm back. I'll\ngive an update next week.\n\nOn Fri, Nov 1, 2019 at 17:25 Anton Lashchenko <notifications@github.com>\nwrote:\n\n> Maybe any updates on that?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33605?email_source=notifications&email_token=ACKKUT32UZHAVHSZXXCQLLTQRPRXDA5CNFSM4JDO7CY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC2JIKA#issuecomment-548705320>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUT3MKUO7MALKVMPO7KTQRPRXDANCNFSM4JDO7CYQ>\n> .\n>\n", "Just ping on any update with this issue @impjdi :)", "@wanfranck \r\n\r\nOops, sorry; forgot about this issue.  Thanks for reminding me.\r\n\r\nWe have gotten this bug report (OUT OF MEMORY on Mali devices) with MediaPipe too =/ (I'm involved with both projects).  The current suspicion is that something is not stable with OpenGL contexts on Mali devices.  Does your stuff run without SSBO, the bare minimum `interpreter.run()` with just CPU tensors?  Is it only failing if you're trying to fill in the GPU tensors via SSBO externally and then call `interpreter.run()`?", "@impjdi\r\n\r\nYes, everything works perfectly with CPU tensors and start failing on GPU tensors via SSBO.\r\nSeparately from my problem here we found out there are some problems with sync methods (glFenseSync e.g.) on Mali GPU. Maybe it related to the problem as well, but we are not sure.", "@wanfranck \r\n\r\nAh, okay.  In that case, please check that you maintain the same `GLContext` for the SSBO as the one for the inference.  Assume you create an SSBO with ID 42 and you tell an input / output tensor that ID 42 is used for that tensor.  Inference starts and will look up an SSBO of ID 42, but if that's a different context, it won't be able to find the object and will throw an `OUT_OF_MEMORY`.", "@impjdi \r\n\r\nHi again! Everything's OK with maintaining GLContext, we definitely create SSBO in the same context, where the inference executed.", "@impjdi  Any clue on how to fix this on mediapipe?", "@wanfranck \r\n\r\nThanks for the update!\r\n\r\n@elblogbruno \r\n\r\nI think there's a very subtle bug in MediaPipe.  A couple of engineers have been looking into the issue (and I also checked), but we couldn't spot anything obvious :(", "@impjdi do you think you'll sort it out?", "@elblogbruno Hopefully ;)  However, I'm not directly involved with the MediaPipe team and don't know their agenda or priorities.  I think it's better to follow up with them.", "@impjdi thanks for answering. I'm really interested on implementing it on my product that's why I want it to work. I'll follow up.", "@impjdi I think you got me wrong :) the issue still exists, my message is that the issue isn't caused by GLContext management.", "@wanfranck \r\n\r\nI have a couple of questions:\r\n\r\n1. Are you using C++ or Java APIs?\r\n2. Are you using OpenCL or OpenGL?  If you don't know the answer, which delegate are you using?  IIRC we have like 3 different versions of the GPU delegate.\r\n3. How are you using the GPU delegate?  From master, or from some static snapshot of TF release, or nightly build?", "@impjdi \r\n\r\n1. C++ API\r\n2. OpenGL, gl_delegate from <tensorflow/lite/delegates/gpu/gl_delegate.h> (if it makes sence to you).\r\n3. I use tensorflow as a submodule of my project, with bazel I built (from latest) libtensorflowlite.so and libtensorflowlite_gpu_gl.so and link it to C++ sources. ", "@wanfranck \r\n\r\nThanks.  @NikolayChirkov might have found the root cause of this problem.  I'll keep you posted when his fix goes live.  Maybe you can test and confirm :)", "@wanfranck \r\n\r\nOkay.  Change went live on master.  Can you sync and try out the latest code?", "MEDIAPIPE Now works! awesome! thanks @impjdi and @wanfranck ", "@impjdi I'm on actual origin/master branch. Built gpu_delegate and tensorflowlite via:\r\n```\r\nbazel build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so\r\n\r\nbazel build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --linkopt -s --strip always //tensorflow/lite:libtensorflowlite.so   \r\n```\r\n\r\nAnd currently getting undefined reference to `TfLiteGpuDelegateCreate' on arm64-v8a.\r\nLast commit in branch is 891e3dc6219bc886e5dbd22b268cd097ab2effcd.", "@impjdi maybe some thoughts regarding this? Same problem without stripping though.", "@wanfranck \r\n\r\nSorry for the late reply; was out for thanksgiving.\r\n\r\nStripping shouldn't cause a difference as that's about the symbols for debuggers and stacktraces etc.  Undefined reference is usually because of `-fvisibility` and things around `TFL_CAPI_EXPORT`.  The command line you copy & pasted above is used for our release as well, so I'm a bit puzzled why you run into this issue =/\r\n", "I spy a couple of flags I don't recognize.  The last time I tried the release, it was something like:\r\n\r\n```\r\n# build -c opt --config android_arm64 --copt -Os --copt -DTFLITE_GPU_BINARY_RELEASE --copt -fvisibility=hidden --linkopt -s --strip always :libtensorflowlite_gpu_gl.so\r\ncc_binary(\r\n    name = \"libtensorflowlite_gpu_gl.so\",\r\n    linkopts = select({\r\n        \"//tensorflow:android\": [\r\n            \"-lEGL\",\r\n            \"-lGLESv3\",\r\n        ],\r\n        \"//conditions:default\": [],\r\n    }),\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    deps = [\":gl_delegate\"],\r\n)\r\n```\r\n\r\nYou can also drop `--copt -Os -fvisibility=hidden --strip always`.  I'm not sure whether you can drop `--linkopt -s`; you can try if nothing else helps =/", "@impjdi thank you, I'll give a try and let you know if it helps or not.", "@impjdi unfortunately with your config I get fatal error\r\n```\r\n../../../../../../../../../libs/tensorflow/tensorflow/lite/delegates/gpu/gl_delegate.h:22:10: fatal error: 'absl/base/macros.h' file not found\r\n#include \"absl/base/macros.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~\r\n```", "That's weird.  That file has been existing for quite some time.", "@impjdi I mean, maybe I did something totally wrong. Just to validate the whole process:\r\n1. checked out to master branch, and pulled it.\r\n2. added cc_binary (from youur comment) section to ./tensorflow/BUILD file.\r\n3. added pathes to android sdk and ndk repositories in ./WORKSPACE file.\r\n4. run the command above (from your comment).\r\n5. after that got the following error.", "We fixed build with adding additional include directory with com_google_absl.\r\nThe line with this dependency was added with\r\n```\r\n18c7bad93bf (A. Unique TensorFlower 2019-12-03 00:45:08 -0800  22) #include \"absl/base/macros.h\"\r\n```", "@impjdi with the workaround above I ran tflite gpu delegate on Mali and now it's another problem appeared. When I try to init my input buffers, I try to get input tensor size like this:\r\n\r\n```\r\nstd::size_t get_input_size(std::size_t index) {\r\n        auto input_id = interpreter_->inputs()[index];\r\n        auto input = interpreter_->tensor(input_id); // here I get NULL as a result\r\n\r\n        return input->bytes;\r\n    }\r\n```\r\n\r\nI tried the same code on Adreno device (which worked before update) and ran into the problem where after invoke method I try to read my output and in outputs() method in debug I see that subgraphes member field is empty when the implementation tries to take .front() element of it.\r\n\r\n```\r\nvoid get_output(float* buf, std::size_t index) {\r\n        auto id = interpreter_->outputs()[index]; // problem is here\r\n        interpreter_->EnsureTensorDataIsReadable(id);\r\n        auto output = interpreter_->tensor(id);\r\n\r\n        std::memcpy(buf, output->data.f, output->bytes);\r\n    }\r\n```", "@wanfranck \r\n\r\nIt's quite unlikely that `interpreter_->inputs()` or `interpreter_->outputs()` is broken.  Is your `Interpreter` properly initialized?", "@impjdi I think yes. Anyway here is a general code below.\r\n```\r\nmodel_ = tflite::FlatBufferModel::BuildFromBuffer(char_buf, size, &error_reporter_);\r\nif (!model_) {\r\n    fatal(\"Failed to load TFLite model.\");\r\n    return false;\r\n}\r\n\r\nauto resolver = std::make_unique<tflite::ops::builtin::BuiltinOpResolver>();\r\ntflite::InterpreterBuilder builder(*model_, *resolver);\r\nauto status = builder(&interpreter_);\r\nif (status != kTfLiteOk) {\r\n    fatal(\"Failed to create TFLite interpreter\");\r\n    return false;\r\n}\r\n\r\nconst TfLiteGpuDelegateOptions kDefaultOptions = {\r\n    .metadata = nullptr,\r\n    .compile_options =\r\n        {\r\n            .precision_loss_allowed = 1,  // false\r\n            .preferred_gl_object_type = TFLITE_GL_OBJECT_TYPE_FASTEST,\r\n            .dynamic_batch_enabled = 0,  // false\r\n        },\r\n};\r\n\r\ninterpreter_->SetAllowBufferHandleOutput(true);\r\n\r\ndelegate_ = TfLiteDelegatePtr(TfLiteGpuDelegateCreate(&kDefaultOptions), &TfLiteGpuDelegateDelete);\r\n\r\nauto const Make = [](GLenum target, GLsizeiptr byte_count, const GLvoid* data, GLenum usage) {\r\n    Buffer buffer;\r\n    glGenBuffers(1, &buffer.id);\r\n    glBindBuffer(target, buffer.id);\r\n    glBufferData(target, byte_count, data, usage);\r\n    return buffer;\r\n};\r\n\r\ninput_image_ = Make(GL_SHADER_STORAGE_BUFFER, get_input_size(0),\r\n                                    std::vector<char>(get_input_size(0), 0).data(), GL_STREAM_COPY);\r\nif (TfLiteGpuDelegateBindBufferToTensor(delegate_.get(), input_image_.id, interpreter_->inputs()[0]) != kTfLiteOk) {\r\n    fatal(\"input image SSBO bind failure\");\r\n    return false;\r\n}\r\n\r\nif (interpreter_->ModifyGraphWithDelegate(delegate_.get()) != kTfLiteOk) {\r\n    fatal(\"GPU delegate failure\");\r\n    return false;\r\n}\r\n```", "I don't see anything wrong with the initialization.  Then, I would put a lot of print statements here and there:\r\n\r\n```\r\nsize_t get_input_size(std::size_t index) {\r\n        if (index >= interpreter_->inputs().size()) {\r\n          // print index & interpreter_->inputs().size();\r\n          exit(-1);\r\n        }\r\n        auto input_id = interpreter_->inputs()[index];\r\n        auto input = interpreter_->tensor(input_id); // here I get NULL as a result\r\n        return input->bytes;\r\n    }\r\n```\r\n\r\ndoes your stuff run on CPU?", "As soon as I use SSBO buffer as input, I think it's running on GPU, if only I didn't miss anything else.\r\n", "@wanfranck,\r\n\r\nWe are checking to see if this is still an issue.Can you try updating to latest version of TF i.e `2.6.0`, and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33605\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33605\">No</a>\n"]}, {"number": 33604, "title": "Pip install with --no-binary \"No matching distribution found for tensorflow\" ", "body": "**System information**\r\n- python 3.6\r\n- pip 19.3.1\r\n\r\n**Describe the problem**\r\n\r\nInstalling Tensorflow through Pip with the --no-binary option throws a `No matching distribution found` error.\r\n\r\nThis is, for example, currently causing an issue with Apache Beam which uses the `--no-binary` flag to download external dependencies based on a requirements.txt file.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```pip install tensorflow --no-binary :all:```\r\n\r\n**Any other info / logs**\r\n\r\nFrom Pip:\r\n>ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n>ERROR: No matching distribution found for tensorflow\r\n\r\nFrom Beam:\r\n>subprocess.CalledProcessError: Command '['/usr/bin/python3', '-m', 'pip', 'download', '--dest', '/tmp/dataflow-requirements-cache', '-r', 'config/dataflow/requirements.txt', '--exists-action', 'i', '--no-binary', ':all:']' returned non-zero exit status 1.\\n\"\r\n\r\nSide note, until yesterday this was working fine.", "comments": ["Instead of\r\n```pip install tensorflow --no-binary :all:``` which gave me this\r\n\r\n![ten1](https://user-images.githubusercontent.com/45028982/67290466-bf9ab080-f4fd-11e9-8bc4-8b990204a8dd.JPG)\r\n\r\nI tried\r\n```pip install tensorflow --no-binary :all``` which gave me this\r\n\r\n![ten2](https://user-images.githubusercontent.com/45028982/67290481-c3c6ce00-f4fd-11e9-88bf-da047786b6d5.JPG)\r\n\r\nPS: Not sure ! why this is happening ! \ud83d\ude05\r\nEnv Info: Python 3.7.3, pip@19.3.1\r\n", "Not sure either, but `:all` is not a valid format (https://pip.pypa.io/en/stable/reference/pip_install/#cmdoption-no-binary)", "When tried in local `pip install tensorflow --no-binary :all:` worked fine for me. Thanks!", "Today it's working again, but yesterday there was clearly an issue. Maybe something wrong with Pypi's repository.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33604\">No</a>\n", "@oanush Same issue today, `pip install tensorflow --no-binary :all:` is failing again. Can you confirm it's also failing on your side?", "Please post a full output of `pip debug` and `pip install -vvv tensorflow --no-binary :all:` (not screenshots, actual code with proper markdown format so it can be both read easily and copy-pasted for analysis)", "@mihaimaruseac \r\n\r\n`pip debug`\r\n\r\n```\r\n>pip version: pip 19.3.1 from /Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip (python 3.6)\r\nsys.version: 3.6.9 (default, Sep  1 2019, 16:05:17) \r\n[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)]\r\nsys.executable: /Users/g/.pyenv/versions/3.6.9/bin/python3.6\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: darwin\r\nsys.implementation:\r\n  name: cpython\r\nCompatible tags: 498\r\n  cp36-cp36m-macosx_10_14_x86_64\r\n  cp36-cp36m-macosx_10_14_intel\r\n  cp36-cp36m-macosx_10_14_fat64\r\n  cp36-cp36m-macosx_10_14_fat32\r\n  cp36-cp36m-macosx_10_14_universal\r\n  cp36-cp36m-macosx_10_13_x86_64\r\n  cp36-cp36m-macosx_10_13_intel\r\n  cp36-cp36m-macosx_10_13_fat64\r\n  cp36-cp36m-macosx_10_13_fat32\r\n  cp36-cp36m-macosx_10_13_universal\r\n  ...\r\n  [First 10 tags shown. Pass --verbose to show all.]\r\n```\r\n\r\n`pip install -vvv tensorflow --no-binary :all:` (note: full output is too big for Github, I truncated the `Skipping link` statements):\r\n```\r\nCreated temporary directory: /private/var/folders/q3/p5ssy3y12mnd69v8z98pc8n80000gn/T/pip-ephem-wheel-cache-sjjtbomf\r\nCreated temporary directory: /private/var/folders/q3/p5ssy3y12mnd69v8z98pc8n80000gn/T/pip-req-tracker-mywnfdp5\r\nCreated requirements tracker '/private/var/folders/q3/p5ssy3y12mnd69v8z98pc8n80000gn/T/pip-req-tracker-mywnfdp5'\r\nCreated temporary directory: /private/var/folders/q3/p5ssy3y12mnd69v8z98pc8n80000gn/T/pip-install-n52nse9p\r\n1 location(s) to search for versions of tensorflow:\r\n* https://pypi.org/simple/tensorflow/\r\nGetting page https://pypi.org/simple/tensorflow/\r\nFound index url https://pypi.org/simple\r\nLooking up \"https://pypi.org/simple/tensorflow/\" in the cache\r\nRequest header has \"max_age\" as 0, cache bypassed\r\nStarting new HTTPS connection (1): pypi.org:443\r\nhttps://pypi.org:443 \"GET /simple/tensorflow/ HTTP/1.1\" 304 0\r\nAnalyzing links from page https://pypi.org/simple/tensorflow/\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/90/cf/1d1e12f9f39b6a0ed1c49792ef5ce7615dddc2ce7287fc83ede0dddb9b3c/tensorflow-0.12.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#sha256=feaf06c7df5c0a480654bf1f38dd4d3b809c7315502a7d9f295033f9d2bd9b13 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/f6/2a/e5bb6320a3fc2886f2677ffa0d4396eefb5914cfc19db94e672c650f0700/tensorflow-0.12.0rc0-cp27-cp27mu-manylinux1_x86_64.whl#sha256=d4b6ca2cacb64513350c1544c33a6e9493073f928398407d20ba018d991fb28e (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/64/9c/72aff7713c507f7e6c15df011e0ed18ac85e5bfa16c3763d8cba44585d79/tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl#sha256=58ed3cda954eb6cfe07f87273570de5c568b02dfb0f9ae20ca4b01e3455ef0dd (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/33/4b/20e517870effa573405d30dafc22f330f24c0a8928659b4ad5a44b9a9af2/tensorflow-0.12.0rc0-cp35-cp35m-macosx_10_11_x86_64.whl#sha256=9cd8631971e02969ad17ca68e66e7cf324c888092a9111c8764e6e61187c47e3 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/67/06/15153c48b2281bc59f8a70f2ae681723ece29ebc0015883117fb28abaf68/tensorflow-0.12.0rc0-cp35-cp35m-manylinux1_x86_64.whl#sha256=20e3f558a0522cee625c2c67adefe3e943ceb83545b7c1c9b062eb3d7b2cf5dd (from https://pypi.org/simple/tensorflow/)\r\n [...]\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/2c/72/6b3264aa2889b7dde7663464b99587d95cd6a5f3b9b30181f14d78a63e64/tensorflow-2.0.0-cp37-cp37m-macosx_10_11_x86_64.whl#sha256=73e9bf5a3f850fd8f630ca17c13b79dfbe1283a08b6f15b9c956697e98fb53f3 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/2a/5c/f1d66de5dde6f3ff528f6ea1fd0757a0e594d17debb3ec7f82daa967ea9a/tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl#sha256=d2f99b19e161f7044929c004e98a72789267c7971ca667da6a31afc81a887932 (from https://pypi.org/simple/tensorflow/)\r\n  Skipping link: No binaries permitted for tensorflow: https://files.pythonhosted.org/packages/54/5f/e1b2d83b808f978f51b7ce109315154da3a3d4151aa59686002681f2e109/tensorflow-2.0.0-cp37-cp37m-win_amd64.whl#sha256=f132935755472b77c1bf6d638f32c3101e5d6c04c5d6725dff8b6e27c5f9e15a (from https://pypi.org/simple/tensorflow/)\r\nGiven no hashes to check 0 links for project 'tensorflow': discarding no candidates\r\nCleaning up...\r\nRemoved build tracker '/private/var/folders/q3/p5ssy3y12mnd69v8z98pc8n80000gn/T/pip-req-tracker-mywnfdp5'\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/cli/base_command.py\", line 153, in _main\r\n    status = self.run(options, args)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/commands/install.py\", line 382, in run\r\n    resolver.resolve(requirement_set)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py\", line 201, in resolve\r\n    self._resolve_one(requirement_set, req)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py\", line 365, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/legacy_resolve.py\", line 311, in _get_abstract_dist_for\r\n    req.populate_link(self.finder, upgrade_allowed, self.require_hashes)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/req/req_install.py\", line 225, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip/_internal/index.py\", line 906, in find_requirement\r\n    'No matching distribution found for %s' % req\r\npip._internal.exceptions.DistributionNotFound: No matching distribution found for tensorflow\r\n```", "I think we need `pip debug --verbose`", "@mihaimaruseac \r\n`pip debug --verbose`:\r\n```pip version: pip 19.3.1 from /Users/g/.pyenv/versions/3.6.9/lib/python3.6/site-packages/pip (python 3.6)\r\nsys.version: 3.6.9 (default, Sep  1 2019, 16:05:17) \r\n[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)]\r\nsys.executable: /Users/g/.pyenv/versions/3.6.9/bin/python3.6\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: UTF-8\r\nsys.platform: darwin\r\nsys.implementation:\r\n  name: cpython\r\nCompatible tags: 498\r\n  cp36-cp36m-macosx_10_14_x86_64\r\n  cp36-cp36m-macosx_10_14_intel\r\n  cp36-cp36m-macosx_10_14_fat64\r\n  cp36-cp36m-macosx_10_14_fat32\r\n  cp36-cp36m-macosx_10_14_universal\r\n  cp36-cp36m-macosx_10_13_x86_64\r\n  cp36-cp36m-macosx_10_13_intel\r\n  cp36-cp36m-macosx_10_13_fat64\r\n  cp36-cp36m-macosx_10_13_fat32\r\n  cp36-cp36m-macosx_10_13_universal\r\n  cp36-cp36m-macosx_10_12_x86_64\r\n  cp36-cp36m-macosx_10_12_intel\r\n  cp36-cp36m-macosx_10_12_fat64\r\n  cp36-cp36m-macosx_10_12_fat32\r\n  cp36-cp36m-macosx_10_12_universal\r\n  cp36-cp36m-macosx_10_11_x86_64\r\n  cp36-cp36m-macosx_10_11_intel\r\n  cp36-cp36m-macosx_10_11_fat64\r\n  cp36-cp36m-macosx_10_11_fat32\r\n  cp36-cp36m-macosx_10_11_universal\r\n  cp36-cp36m-macosx_10_10_x86_64\r\n  cp36-cp36m-macosx_10_10_intel\r\n  cp36-cp36m-macosx_10_10_fat64\r\n  cp36-cp36m-macosx_10_10_fat32\r\n  cp36-cp36m-macosx_10_10_universal\r\n  cp36-cp36m-macosx_10_9_x86_64\r\n  cp36-cp36m-macosx_10_9_intel\r\n  cp36-cp36m-macosx_10_9_fat64\r\n  cp36-cp36m-macosx_10_9_fat32\r\n  cp36-cp36m-macosx_10_9_universal\r\n  cp36-cp36m-macosx_10_8_x86_64\r\n  cp36-cp36m-macosx_10_8_intel\r\n  cp36-cp36m-macosx_10_8_fat64\r\n  cp36-cp36m-macosx_10_8_fat32\r\n  cp36-cp36m-macosx_10_8_universal\r\n  cp36-cp36m-macosx_10_7_x86_64\r\n  cp36-cp36m-macosx_10_7_intel\r\n  cp36-cp36m-macosx_10_7_fat64\r\n  cp36-cp36m-macosx_10_7_fat32\r\n  cp36-cp36m-macosx_10_7_universal\r\n  cp36-cp36m-macosx_10_6_x86_64\r\n  cp36-cp36m-macosx_10_6_intel\r\n  cp36-cp36m-macosx_10_6_fat64\r\n  cp36-cp36m-macosx_10_6_fat32\r\n  cp36-cp36m-macosx_10_6_universal\r\n  cp36-cp36m-macosx_10_5_x86_64\r\n  cp36-cp36m-macosx_10_5_intel\r\n  cp36-cp36m-macosx_10_5_fat64\r\n  cp36-cp36m-macosx_10_5_fat32\r\n  cp36-cp36m-macosx_10_5_universal\r\n  cp36-cp36m-macosx_10_4_intel\r\n  cp36-cp36m-macosx_10_4_fat32\r\n  cp36-cp36m-macosx_10_4_universal\r\n  cp36-cp36m-macosx_10_3_fat32\r\n  cp36-cp36m-macosx_10_3_universal\r\n  cp36-cp36m-macosx_10_2_fat32\r\n  cp36-cp36m-macosx_10_2_universal\r\n  cp36-cp36m-macosx_10_1_fat32\r\n  cp36-cp36m-macosx_10_1_universal\r\n  cp36-cp36m-macosx_10_0_fat32\r\n  cp36-cp36m-macosx_10_0_universal\r\n  cp36-abi3-macosx_10_14_x86_64\r\n  cp36-abi3-macosx_10_14_intel\r\n  cp36-abi3-macosx_10_14_fat64\r\n  cp36-abi3-macosx_10_14_fat32\r\n  cp36-abi3-macosx_10_14_universal\r\n  cp36-abi3-macosx_10_13_x86_64\r\n  cp36-abi3-macosx_10_13_intel\r\n  cp36-abi3-macosx_10_13_fat64\r\n  cp36-abi3-macosx_10_13_fat32\r\n  cp36-abi3-macosx_10_13_universal\r\n  cp36-abi3-macosx_10_12_x86_64\r\n  cp36-abi3-macosx_10_12_intel\r\n  cp36-abi3-macosx_10_12_fat64\r\n  cp36-abi3-macosx_10_12_fat32\r\n  cp36-abi3-macosx_10_12_universal\r\n  cp36-abi3-macosx_10_11_x86_64\r\n  cp36-abi3-macosx_10_11_intel\r\n  cp36-abi3-macosx_10_11_fat64\r\n  cp36-abi3-macosx_10_11_fat32\r\n  cp36-abi3-macosx_10_11_universal\r\n  cp36-abi3-macosx_10_10_x86_64\r\n  cp36-abi3-macosx_10_10_intel\r\n  cp36-abi3-macosx_10_10_fat64\r\n  cp36-abi3-macosx_10_10_fat32\r\n  cp36-abi3-macosx_10_10_universal\r\n  cp36-abi3-macosx_10_9_x86_64\r\n  cp36-abi3-macosx_10_9_intel\r\n  cp36-abi3-macosx_10_9_fat64\r\n  cp36-abi3-macosx_10_9_fat32\r\n  cp36-abi3-macosx_10_9_universal\r\n  cp36-abi3-macosx_10_8_x86_64\r\n  cp36-abi3-macosx_10_8_intel\r\n  cp36-abi3-macosx_10_8_fat64\r\n  cp36-abi3-macosx_10_8_fat32\r\n  cp36-abi3-macosx_10_8_universal\r\n  cp36-abi3-macosx_10_7_x86_64\r\n  cp36-abi3-macosx_10_7_intel\r\n  cp36-abi3-macosx_10_7_fat64\r\n  cp36-abi3-macosx_10_7_fat32\r\n  cp36-abi3-macosx_10_7_universal\r\n  cp36-abi3-macosx_10_6_x86_64\r\n  cp36-abi3-macosx_10_6_intel\r\n  cp36-abi3-macosx_10_6_fat64\r\n  cp36-abi3-macosx_10_6_fat32\r\n  cp36-abi3-macosx_10_6_universal\r\n  cp36-abi3-macosx_10_5_x86_64\r\n  cp36-abi3-macosx_10_5_intel\r\n  cp36-abi3-macosx_10_5_fat64\r\n  cp36-abi3-macosx_10_5_fat32\r\n  cp36-abi3-macosx_10_5_universal\r\n  cp36-abi3-macosx_10_4_intel\r\n  cp36-abi3-macosx_10_4_fat32\r\n  cp36-abi3-macosx_10_4_universal\r\n  cp36-abi3-macosx_10_3_fat32\r\n  cp36-abi3-macosx_10_3_universal\r\n  cp36-abi3-macosx_10_2_fat32\r\n  cp36-abi3-macosx_10_2_universal\r\n  cp36-abi3-macosx_10_1_fat32\r\n  cp36-abi3-macosx_10_1_universal\r\n  cp36-abi3-macosx_10_0_fat32\r\n  cp36-abi3-macosx_10_0_universal\r\n  cp36-none-macosx_10_14_x86_64\r\n  cp36-none-macosx_10_14_intel\r\n  cp36-none-macosx_10_14_fat64\r\n  cp36-none-macosx_10_14_fat32\r\n  cp36-none-macosx_10_14_universal\r\n  cp36-none-macosx_10_13_x86_64\r\n  cp36-none-macosx_10_13_intel\r\n  cp36-none-macosx_10_13_fat64\r\n  cp36-none-macosx_10_13_fat32\r\n  cp36-none-macosx_10_13_universal\r\n  cp36-none-macosx_10_12_x86_64\r\n  cp36-none-macosx_10_12_intel\r\n  cp36-none-macosx_10_12_fat64\r\n  cp36-none-macosx_10_12_fat32\r\n  cp36-none-macosx_10_12_universal\r\n  cp36-none-macosx_10_11_x86_64\r\n  cp36-none-macosx_10_11_intel\r\n  cp36-none-macosx_10_11_fat64\r\n  cp36-none-macosx_10_11_fat32\r\n  cp36-none-macosx_10_11_universal\r\n  cp36-none-macosx_10_10_x86_64\r\n  cp36-none-macosx_10_10_intel\r\n  cp36-none-macosx_10_10_fat64\r\n  cp36-none-macosx_10_10_fat32\r\n  cp36-none-macosx_10_10_universal\r\n  cp36-none-macosx_10_9_x86_64\r\n  cp36-none-macosx_10_9_intel\r\n  cp36-none-macosx_10_9_fat64\r\n  cp36-none-macosx_10_9_fat32\r\n  cp36-none-macosx_10_9_universal\r\n  cp36-none-macosx_10_8_x86_64\r\n  cp36-none-macosx_10_8_intel\r\n  cp36-none-macosx_10_8_fat64\r\n  cp36-none-macosx_10_8_fat32\r\n  cp36-none-macosx_10_8_universal\r\n  cp36-none-macosx_10_7_x86_64\r\n  cp36-none-macosx_10_7_intel\r\n  cp36-none-macosx_10_7_fat64\r\n  cp36-none-macosx_10_7_fat32\r\n  cp36-none-macosx_10_7_universal\r\n  cp36-none-macosx_10_6_x86_64\r\n  cp36-none-macosx_10_6_intel\r\n  cp36-none-macosx_10_6_fat64\r\n  cp36-none-macosx_10_6_fat32\r\n  cp36-none-macosx_10_6_universal\r\n  cp36-none-macosx_10_5_x86_64\r\n  cp36-none-macosx_10_5_intel\r\n  cp36-none-macosx_10_5_fat64\r\n  cp36-none-macosx_10_5_fat32\r\n  cp36-none-macosx_10_5_universal\r\n  cp36-none-macosx_10_4_intel\r\n  cp36-none-macosx_10_4_fat32\r\n  cp36-none-macosx_10_4_universal\r\n  cp36-none-macosx_10_3_fat32\r\n  cp36-none-macosx_10_3_universal\r\n  cp36-none-macosx_10_2_fat32\r\n  cp36-none-macosx_10_2_universal\r\n  cp36-none-macosx_10_1_fat32\r\n  cp36-none-macosx_10_1_universal\r\n  cp36-none-macosx_10_0_fat32\r\n  cp36-none-macosx_10_0_universal\r\n  cp35-abi3-macosx_10_14_x86_64\r\n  cp35-abi3-macosx_10_14_intel\r\n  cp35-abi3-macosx_10_14_fat64\r\n  cp35-abi3-macosx_10_14_fat32\r\n  cp35-abi3-macosx_10_14_universal\r\n  cp35-abi3-macosx_10_13_x86_64\r\n  cp35-abi3-macosx_10_13_intel\r\n  cp35-abi3-macosx_10_13_fat64\r\n  cp35-abi3-macosx_10_13_fat32\r\n  cp35-abi3-macosx_10_13_universal\r\n  cp35-abi3-macosx_10_12_x86_64\r\n  cp35-abi3-macosx_10_12_intel\r\n  cp35-abi3-macosx_10_12_fat64\r\n  cp35-abi3-macosx_10_12_fat32\r\n  cp35-abi3-macosx_10_12_universal\r\n  cp35-abi3-macosx_10_11_x86_64\r\n  cp35-abi3-macosx_10_11_intel\r\n  cp35-abi3-macosx_10_11_fat64\r\n  cp35-abi3-macosx_10_11_fat32\r\n  cp35-abi3-macosx_10_11_universal\r\n  cp35-abi3-macosx_10_10_x86_64\r\n  cp35-abi3-macosx_10_10_intel\r\n  cp35-abi3-macosx_10_10_fat64\r\n  cp35-abi3-macosx_10_10_fat32\r\n  cp35-abi3-macosx_10_10_universal\r\n  cp35-abi3-macosx_10_9_x86_64\r\n  cp35-abi3-macosx_10_9_intel\r\n  cp35-abi3-macosx_10_9_fat64\r\n  cp35-abi3-macosx_10_9_fat32\r\n  cp35-abi3-macosx_10_9_universal\r\n  cp35-abi3-macosx_10_8_x86_64\r\n  cp35-abi3-macosx_10_8_intel\r\n  cp35-abi3-macosx_10_8_fat64\r\n  cp35-abi3-macosx_10_8_fat32\r\n  cp35-abi3-macosx_10_8_universal\r\n  cp35-abi3-macosx_10_7_x86_64\r\n  cp35-abi3-macosx_10_7_intel\r\n  cp35-abi3-macosx_10_7_fat64\r\n  cp35-abi3-macosx_10_7_fat32\r\n  cp35-abi3-macosx_10_7_universal\r\n  cp35-abi3-macosx_10_6_x86_64\r\n  cp35-abi3-macosx_10_6_intel\r\n  cp35-abi3-macosx_10_6_fat64\r\n  cp35-abi3-macosx_10_6_fat32\r\n  cp35-abi3-macosx_10_6_universal\r\n  cp35-abi3-macosx_10_5_x86_64\r\n  cp35-abi3-macosx_10_5_intel\r\n  cp35-abi3-macosx_10_5_fat64\r\n  cp35-abi3-macosx_10_5_fat32\r\n  cp35-abi3-macosx_10_5_universal\r\n  cp35-abi3-macosx_10_4_intel\r\n  cp35-abi3-macosx_10_4_fat32\r\n  cp35-abi3-macosx_10_4_universal\r\n  cp35-abi3-macosx_10_3_fat32\r\n  cp35-abi3-macosx_10_3_universal\r\n  cp35-abi3-macosx_10_2_fat32\r\n  cp35-abi3-macosx_10_2_universal\r\n  cp35-abi3-macosx_10_1_fat32\r\n  cp35-abi3-macosx_10_1_universal\r\n  cp35-abi3-macosx_10_0_fat32\r\n  cp35-abi3-macosx_10_0_universal\r\n  cp34-abi3-macosx_10_14_x86_64\r\n  cp34-abi3-macosx_10_14_intel\r\n  cp34-abi3-macosx_10_14_fat64\r\n  cp34-abi3-macosx_10_14_fat32\r\n  cp34-abi3-macosx_10_14_universal\r\n  cp34-abi3-macosx_10_13_x86_64\r\n  cp34-abi3-macosx_10_13_intel\r\n  cp34-abi3-macosx_10_13_fat64\r\n  cp34-abi3-macosx_10_13_fat32\r\n  cp34-abi3-macosx_10_13_universal\r\n  cp34-abi3-macosx_10_12_x86_64\r\n  cp34-abi3-macosx_10_12_intel\r\n  cp34-abi3-macosx_10_12_fat64\r\n  cp34-abi3-macosx_10_12_fat32\r\n  cp34-abi3-macosx_10_12_universal\r\n  cp34-abi3-macosx_10_11_x86_64\r\n  cp34-abi3-macosx_10_11_intel\r\n  cp34-abi3-macosx_10_11_fat64\r\n  cp34-abi3-macosx_10_11_fat32\r\n  cp34-abi3-macosx_10_11_universal\r\n  cp34-abi3-macosx_10_10_x86_64\r\n  cp34-abi3-macosx_10_10_intel\r\n  cp34-abi3-macosx_10_10_fat64\r\n  cp34-abi3-macosx_10_10_fat32\r\n  cp34-abi3-macosx_10_10_universal\r\n  cp34-abi3-macosx_10_9_x86_64\r\n  cp34-abi3-macosx_10_9_intel\r\n  cp34-abi3-macosx_10_9_fat64\r\n  cp34-abi3-macosx_10_9_fat32\r\n  cp34-abi3-macosx_10_9_universal\r\n  cp34-abi3-macosx_10_8_x86_64\r\n  cp34-abi3-macosx_10_8_intel\r\n  cp34-abi3-macosx_10_8_fat64\r\n  cp34-abi3-macosx_10_8_fat32\r\n  cp34-abi3-macosx_10_8_universal\r\n  cp34-abi3-macosx_10_7_x86_64\r\n  cp34-abi3-macosx_10_7_intel\r\n  cp34-abi3-macosx_10_7_fat64\r\n  cp34-abi3-macosx_10_7_fat32\r\n  cp34-abi3-macosx_10_7_universal\r\n  cp34-abi3-macosx_10_6_x86_64\r\n  cp34-abi3-macosx_10_6_intel\r\n  cp34-abi3-macosx_10_6_fat64\r\n  cp34-abi3-macosx_10_6_fat32\r\n  cp34-abi3-macosx_10_6_universal\r\n  cp34-abi3-macosx_10_5_x86_64\r\n  cp34-abi3-macosx_10_5_intel\r\n  cp34-abi3-macosx_10_5_fat64\r\n  cp34-abi3-macosx_10_5_fat32\r\n  cp34-abi3-macosx_10_5_universal\r\n  cp34-abi3-macosx_10_4_intel\r\n  cp34-abi3-macosx_10_4_fat32\r\n  cp34-abi3-macosx_10_4_universal\r\n  cp34-abi3-macosx_10_3_fat32\r\n  cp34-abi3-macosx_10_3_universal\r\n  cp34-abi3-macosx_10_2_fat32\r\n  cp34-abi3-macosx_10_2_universal\r\n  cp34-abi3-macosx_10_1_fat32\r\n  cp34-abi3-macosx_10_1_universal\r\n  cp34-abi3-macosx_10_0_fat32\r\n  cp34-abi3-macosx_10_0_universal\r\n  cp33-abi3-macosx_10_14_x86_64\r\n  cp33-abi3-macosx_10_14_intel\r\n  cp33-abi3-macosx_10_14_fat64\r\n  cp33-abi3-macosx_10_14_fat32\r\n  cp33-abi3-macosx_10_14_universal\r\n  cp33-abi3-macosx_10_13_x86_64\r\n  cp33-abi3-macosx_10_13_intel\r\n  cp33-abi3-macosx_10_13_fat64\r\n  cp33-abi3-macosx_10_13_fat32\r\n  cp33-abi3-macosx_10_13_universal\r\n  cp33-abi3-macosx_10_12_x86_64\r\n  cp33-abi3-macosx_10_12_intel\r\n  cp33-abi3-macosx_10_12_fat64\r\n  cp33-abi3-macosx_10_12_fat32\r\n  cp33-abi3-macosx_10_12_universal\r\n  cp33-abi3-macosx_10_11_x86_64\r\n  cp33-abi3-macosx_10_11_intel\r\n  cp33-abi3-macosx_10_11_fat64\r\n  cp33-abi3-macosx_10_11_fat32\r\n  cp33-abi3-macosx_10_11_universal\r\n  cp33-abi3-macosx_10_10_x86_64\r\n  cp33-abi3-macosx_10_10_intel\r\n  cp33-abi3-macosx_10_10_fat64\r\n  cp33-abi3-macosx_10_10_fat32\r\n  cp33-abi3-macosx_10_10_universal\r\n  cp33-abi3-macosx_10_9_x86_64\r\n  cp33-abi3-macosx_10_9_intel\r\n  cp33-abi3-macosx_10_9_fat64\r\n  cp33-abi3-macosx_10_9_fat32\r\n  cp33-abi3-macosx_10_9_universal\r\n  cp33-abi3-macosx_10_8_x86_64\r\n  cp33-abi3-macosx_10_8_intel\r\n  cp33-abi3-macosx_10_8_fat64\r\n  cp33-abi3-macosx_10_8_fat32\r\n  cp33-abi3-macosx_10_8_universal\r\n  cp33-abi3-macosx_10_7_x86_64\r\n  cp33-abi3-macosx_10_7_intel\r\n  cp33-abi3-macosx_10_7_fat64\r\n  cp33-abi3-macosx_10_7_fat32\r\n  cp33-abi3-macosx_10_7_universal\r\n  cp33-abi3-macosx_10_6_x86_64\r\n  cp33-abi3-macosx_10_6_intel\r\n  cp33-abi3-macosx_10_6_fat64\r\n  cp33-abi3-macosx_10_6_fat32\r\n  cp33-abi3-macosx_10_6_universal\r\n  cp33-abi3-macosx_10_5_x86_64\r\n  cp33-abi3-macosx_10_5_intel\r\n  cp33-abi3-macosx_10_5_fat64\r\n  cp33-abi3-macosx_10_5_fat32\r\n  cp33-abi3-macosx_10_5_universal\r\n  cp33-abi3-macosx_10_4_intel\r\n  cp33-abi3-macosx_10_4_fat32\r\n  cp33-abi3-macosx_10_4_universal\r\n  cp33-abi3-macosx_10_3_fat32\r\n  cp33-abi3-macosx_10_3_universal\r\n  cp33-abi3-macosx_10_2_fat32\r\n  cp33-abi3-macosx_10_2_universal\r\n  cp33-abi3-macosx_10_1_fat32\r\n  cp33-abi3-macosx_10_1_universal\r\n  cp33-abi3-macosx_10_0_fat32\r\n  cp33-abi3-macosx_10_0_universal\r\n  cp32-abi3-macosx_10_14_x86_64\r\n  cp32-abi3-macosx_10_14_intel\r\n  cp32-abi3-macosx_10_14_fat64\r\n  cp32-abi3-macosx_10_14_fat32\r\n  cp32-abi3-macosx_10_14_universal\r\n  cp32-abi3-macosx_10_13_x86_64\r\n  cp32-abi3-macosx_10_13_intel\r\n  cp32-abi3-macosx_10_13_fat64\r\n  cp32-abi3-macosx_10_13_fat32\r\n  cp32-abi3-macosx_10_13_universal\r\n  cp32-abi3-macosx_10_12_x86_64\r\n  cp32-abi3-macosx_10_12_intel\r\n  cp32-abi3-macosx_10_12_fat64\r\n  cp32-abi3-macosx_10_12_fat32\r\n  cp32-abi3-macosx_10_12_universal\r\n  cp32-abi3-macosx_10_11_x86_64\r\n  cp32-abi3-macosx_10_11_intel\r\n  cp32-abi3-macosx_10_11_fat64\r\n  cp32-abi3-macosx_10_11_fat32\r\n  cp32-abi3-macosx_10_11_universal\r\n  cp32-abi3-macosx_10_10_x86_64\r\n  cp32-abi3-macosx_10_10_intel\r\n  cp32-abi3-macosx_10_10_fat64\r\n  cp32-abi3-macosx_10_10_fat32\r\n  cp32-abi3-macosx_10_10_universal\r\n  cp32-abi3-macosx_10_9_x86_64\r\n  cp32-abi3-macosx_10_9_intel\r\n  cp32-abi3-macosx_10_9_fat64\r\n  cp32-abi3-macosx_10_9_fat32\r\n  cp32-abi3-macosx_10_9_universal\r\n  cp32-abi3-macosx_10_8_x86_64\r\n  cp32-abi3-macosx_10_8_intel\r\n  cp32-abi3-macosx_10_8_fat64\r\n  cp32-abi3-macosx_10_8_fat32\r\n  cp32-abi3-macosx_10_8_universal\r\n  cp32-abi3-macosx_10_7_x86_64\r\n  cp32-abi3-macosx_10_7_intel\r\n  cp32-abi3-macosx_10_7_fat64\r\n  cp32-abi3-macosx_10_7_fat32\r\n  cp32-abi3-macosx_10_7_universal\r\n  cp32-abi3-macosx_10_6_x86_64\r\n  cp32-abi3-macosx_10_6_intel\r\n  cp32-abi3-macosx_10_6_fat64\r\n  cp32-abi3-macosx_10_6_fat32\r\n  cp32-abi3-macosx_10_6_universal\r\n  cp32-abi3-macosx_10_5_x86_64\r\n  cp32-abi3-macosx_10_5_intel\r\n  cp32-abi3-macosx_10_5_fat64\r\n  cp32-abi3-macosx_10_5_fat32\r\n  cp32-abi3-macosx_10_5_universal\r\n  cp32-abi3-macosx_10_4_intel\r\n  cp32-abi3-macosx_10_4_fat32\r\n  cp32-abi3-macosx_10_4_universal\r\n  cp32-abi3-macosx_10_3_fat32\r\n  cp32-abi3-macosx_10_3_universal\r\n  cp32-abi3-macosx_10_2_fat32\r\n  cp32-abi3-macosx_10_2_universal\r\n  cp32-abi3-macosx_10_1_fat32\r\n  cp32-abi3-macosx_10_1_universal\r\n  cp32-abi3-macosx_10_0_fat32\r\n  cp32-abi3-macosx_10_0_universal\r\n  py3-none-macosx_10_14_x86_64\r\n  py3-none-macosx_10_14_intel\r\n  py3-none-macosx_10_14_fat64\r\n  py3-none-macosx_10_14_fat32\r\n  py3-none-macosx_10_14_universal\r\n  py3-none-macosx_10_13_x86_64\r\n  py3-none-macosx_10_13_intel\r\n  py3-none-macosx_10_13_fat64\r\n  py3-none-macosx_10_13_fat32\r\n  py3-none-macosx_10_13_universal\r\n  py3-none-macosx_10_12_x86_64\r\n  py3-none-macosx_10_12_intel\r\n  py3-none-macosx_10_12_fat64\r\n  py3-none-macosx_10_12_fat32\r\n  py3-none-macosx_10_12_universal\r\n  py3-none-macosx_10_11_x86_64\r\n  py3-none-macosx_10_11_intel\r\n  py3-none-macosx_10_11_fat64\r\n  py3-none-macosx_10_11_fat32\r\n  py3-none-macosx_10_11_universal\r\n  py3-none-macosx_10_10_x86_64\r\n  py3-none-macosx_10_10_intel\r\n  py3-none-macosx_10_10_fat64\r\n  py3-none-macosx_10_10_fat32\r\n  py3-none-macosx_10_10_universal\r\n  py3-none-macosx_10_9_x86_64\r\n  py3-none-macosx_10_9_intel\r\n  py3-none-macosx_10_9_fat64\r\n  py3-none-macosx_10_9_fat32\r\n  py3-none-macosx_10_9_universal\r\n  py3-none-macosx_10_8_x86_64\r\n  py3-none-macosx_10_8_intel\r\n  py3-none-macosx_10_8_fat64\r\n  py3-none-macosx_10_8_fat32\r\n  py3-none-macosx_10_8_universal\r\n  py3-none-macosx_10_7_x86_64\r\n  py3-none-macosx_10_7_intel\r\n  py3-none-macosx_10_7_fat64\r\n  py3-none-macosx_10_7_fat32\r\n  py3-none-macosx_10_7_universal\r\n  py3-none-macosx_10_6_x86_64\r\n  py3-none-macosx_10_6_intel\r\n  py3-none-macosx_10_6_fat64\r\n  py3-none-macosx_10_6_fat32\r\n  py3-none-macosx_10_6_universal\r\n  py3-none-macosx_10_5_x86_64\r\n  py3-none-macosx_10_5_intel\r\n  py3-none-macosx_10_5_fat64\r\n  py3-none-macosx_10_5_fat32\r\n  py3-none-macosx_10_5_universal\r\n  py3-none-macosx_10_4_intel\r\n  py3-none-macosx_10_4_fat32\r\n  py3-none-macosx_10_4_universal\r\n  py3-none-macosx_10_3_fat32\r\n  py3-none-macosx_10_3_universal\r\n  py3-none-macosx_10_2_fat32\r\n  py3-none-macosx_10_2_universal\r\n  py3-none-macosx_10_1_fat32\r\n  py3-none-macosx_10_1_universal\r\n  py3-none-macosx_10_0_fat32\r\n  py3-none-macosx_10_0_universal\r\n  cp36-none-any\r\n  cp3-none-any\r\n  py36-none-any\r\n  py3-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any", "@mihaimaruseac The problem doesn't seem to be specific to my machine though. I tried on a couple of different machines, and it fails the same way. Including on Dataflow, which is my blocker at the moment because it's the one using that `--no-binary` flag.", "@aaaaahaaaaa same problem here. No matching distribution =/\r\n", "You have the `cp36-cp36m-macosx_10_11_x86_64` tag and that should correspond to [one of the wheels we are posting](https://files.pythonhosted.org/packages/c8/a1/2ab46a175c916b0149ccb9edc06202bce6365455779fa251c1f59a4c7806/tensorflow-2.0.0-cp36-cp36m-macosx_10_11_x86_64.whl)\r\n\r\nWas the `pip debug --verbose` output from the Dataflow machine / from a machine where you cannot install?", "@mihaimaruseac The output came from my local machine where the installation indeed failed. It also fails on Dataflow.\r\nLocally, it works fine without the `--no-binary` option.", "I think a tensorflow installation is always bound to a particular operating system and/or cpu, gpu or tpu. Hence a `pip download tensorflow` with `--no-binary :all:` is deemed to fail. ", "I agree, this doesn't really make sense. I should have detailed the problem further: the problem comes from `tensorflow-transform` which depends on `tensorflow`. So when Beam/Dataflow then tries to install `tensorflow-transform`, it installs both using that `--no-binary` option. That dependency to `tensorflow` seems to be new with `transform v0.15.0`. It seems to work right now with `transform v0.14.0`, but I'm pretty sure there was a problem with that version also earlier.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33604\">No</a>\n"]}, {"number": 33603, "title": "tf.pow() overflow??", "body": "Env:\r\n* Colab (with gpu on)\r\n* TF 2.0\r\n* Python3\r\n\r\nRunning tf.pow() with integer powers greater than 5 returns wrong results and to me it looks like an overflow.\r\n```python\r\na = tf.constant(50)\r\nb = tf.constant(6)\r\ntf.pow(a,b)\r\n```\r\nThe above returns:\r\n```\r\ntf.Tensor(-1554869184, shape=(), dtype=int32)\r\n```\r\nWhich is mathematically wrong. Here is the result (the correct one) when using python's math library.\r\n```python\r\nimport math\r\nmath.pow(50,6)\r\n```\r\nThe above returns ```15625000000.0```\r\n\r\nUsing python's math library gives correct results for higher powers which I'd expect tf.pow() to do for integer inputs. \r\nAny explanation for this discrepancy!!", "comments": ["Issue replicating in TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/bb4b5fe54b25afa435b0055243f63d14/33603.ipynb) of colab.Thanks!", "The range for int32 is from -2,147,483,648 to 2,147,483,647 while the result you want exceeds the limit. Therefore, the solution is\r\n```\r\na = tf.constant(50, tf.int64)\r\nb = tf.constant(6, tf.int64)\r\ntf.pow(a,b)\r\n```\r\n\r\nwhich returns\r\n`<tf.Tensor: id=2, shape=(), dtype=int64, numpy=15625000000>\r\n`\r\n", "Thanks @oanush and @blairhan. I thought the types were applied implicitly.", "@steph-en-m Closing this issue as it has been resolved. Please add additional comments and we can open the issue again", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33603\">No</a>\n", "> Thanks @oanush and @blairhan. I thought the types were applied implicitly.\r\n\r\nActually am more interested in knowing why the tensorflow team chose to implement it this way (where one explicitly sets ```dtypes```) rather than the python ```math.pow()``` style implementation", "Because these types need to be propagated to the C++ core layer. Dynamic typing is not a panacea"]}, {"number": 33602, "title": "evalSpec 'NoneType' object is not subscriptable", "body": "tensorflow=2.0.0\r\npython 3.6.8\r\n\r\ni followed this guide: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator\r\n\r\nI wanted to replicate this code with my data set:\r\n```python\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\ndef input_fn(mode, input_context=None):\r\n  datasets, info = tfds.load(name='mnist',\r\n                                with_info=True,\r\n                                as_supervised=True)\r\n  mnist_dataset = (datasets['train'] if mode == tf.estimator.ModeKeys.TRAIN else\r\n                   datasets['test'])\r\n\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  if input_context:\r\n    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,\r\n                                        input_context.input_pipeline_id)\r\n  return mnist_dataset.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n```\r\n\r\n\r\nmy code is: \r\n```python\r\nbatch_size=16\r\ndef input_fn(mode,input_context=None):\r\n    dataset = (tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train, np.float32), Y_train)).repeat() \r\n               if mode == tf.estimator.ModeKeys.TRAIN else\r\n                   tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_valid, np.float32), Y_valid)))\r\n    if input_context:\r\n      dataset = dataset.shard(input_context.num_input_pipelines,\r\n                                        input_context.input_pipeline_id)\r\n    return dataset.batch(batch_size)\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nconfig = tf.estimator.RunConfig(train_distribute=strategy, save_checkpoints_steps=100)\r\nclassifier=tf.keras.estimator.model_to_estimator(\r\n    keras_model=my_model, model_dir=\"model\")\r\n\r\ntf.estimator.train_and_evaluate(\r\n    classifier,\r\n    train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n    eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\r\n)\r\n```\r\nmy dataset input is this:\r\n\r\n/>> type(X_train)\r\nnumpy.ndarray\r\n/>> X_train.shape\r\n(480, 224, 224, 3)\r\n/>>tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_train, np.float32), Y_train)).shuffle(1000).repeat().batch(batch_size)\r\n<DatasetV1Adapter shapes: ((?, 224, 224, 3), (?, 16)), types: (tf.float32, tf.float32)>\r\n\r\n/>>Y_train.shape\r\n(480, 16)\r\n/>>type(Y_train)\r\nnumpy.ndarray\r\n\r\n/>>X_valid.shape\r\n(160, 224, 224, 3)\r\n/>>tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(X_valid, np.float32), Y_valid)).repeat().batch(batch_size)\r\n<DatasetV1Adapter shapes: ((?, 224, 224, 3), (?, 16)), types: (tf.float32, tf.float32)>\r\n\r\n/>>Y_valid.shape\r\n(160, 16)\r\n/>>type(Y_valid)\r\nnumpy.ndarray\r\n\r\n\r\n\r\n\r\nlogs:\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-46-dfcb7e52174e> in <module>()\r\n     27     classifier,\r\n     28     train_spec=tf.estimator.TrainSpec(input_fn=input_fn),\r\n---> 29     eval_spec=tf.estimator.EvalSpec(input_fn=input_fn)\r\n     30     # eval_spec=tf.estimator.EvalSpec(input_fn=input_fn, exporters=exporter)\r\n     31 )\r\n\r\n14 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py in get_metric_function(metric, output_shape, loss_fn)\r\n   1029 \r\n   1030   if metric in ['accuracy', 'acc']:\r\n-> 1031     if output_shape[-1] == 1 or is_binary_crossentropy:\r\n   1032       return metrics_module.binary_accuracy\r\n   1033     elif is_sparse_categorical_crossentropy:\r\n\r\nTypeError: 'NoneType' object is not subscriptable", "comments": []}, {"number": 33601, "title": "Model loaded with tensorflow.keras.Model.load_model is slower (if GRU cells are used), than the original constructed model", "body": "Tensorflow 2.0, CUDA 10.0, CuDNN 7.6, Python 3.6.8 in a clean environment\r\n\r\nThe model loaded from a checkpoint does not use cudnn. You can see this in the line after `Epoch 1/2` in the outputs. When the model is loaded from checkpoint it misses\r\n`2019-10-22 12:25:01.298965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n`\r\nand the code is slower. (Much slower if I am using real long data, not just the broken down bugreport file)\r\n\r\n\r\nCode to reproduce:\r\n```\r\nimport argparse\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nfrom tensorflow.keras.layers import Activation, Embedding, Dense, Input\r\nfrom tensorflow.keras.layers import SimpleRNN, GRU, LSTM\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport numpy as np\r\n\r\nparser = argparse.ArgumentParser(description='train recurrent net.')\r\nparser.add_argument('--epochs', dest='epochs',  type=int, default=2)\r\nparser.add_argument('--hidden_size', dest='hidden_size',  type=int, default=50)\r\nparser.add_argument('--RNN_type', dest='RNN_type',  type=str, default='GRU')\r\nparser.add_argument('--epoch_size', dest='epoch_size',  type=int, default=1000)\r\nparser.add_argument('--pretrained_name', dest='pretrained_name',  type=str, default=None)\r\n\r\nargs = parser.parse_args()\r\n\r\nRNN_type = {}\r\n\r\nRNN_type['LSTM'] = LSTM\r\nRNN_type['GRU'] = GRU\r\nRNN_type['SimpleRNN'] = SimpleRNN\r\n\r\nLSTM_use = RNN_type[args.RNN_type]\r\n\r\nmax_output = 10\r\nfull_python_file_string = [1,3,2,4,5,3,2,3,4,5]\r\nclass KerasBatchGenerator(object):\r\n    def __init__(self, data_set):\r\n        self.data_set = data_set\r\n            \r\n    def generate(self):\r\n        while True:\r\n            tmp_x = np.array([full_python_file_string], dtype=int)\r\n            tmp_y = np.array([full_python_file_string], dtype=int)\r\n            yield tmp_x, to_categorical(tmp_y, num_classes=max_output)\r\n\r\ntrain_data_generator = KerasBatchGenerator(0)\r\ntest_data_generator = KerasBatchGenerator(0)\r\n\r\nhidden_size = args.hidden_size\r\n\r\nif args.pretrained_name is not None:\r\n  from tensorflow.keras.models import load_model\r\n  model = load_model(args.pretrained_name)\r\nelse:\r\n  inputs = Input(batch_shape=(1,None,))\r\n  embeds = Embedding(max_output, max_output, embeddings_initializer='identity', trainable=True)(inputs)\r\n  lstm1 = LSTM_use(hidden_size, return_sequences=True, stateful = True)(embeds)\r\n  x = Dense(max_output)(lstm1)\r\n  predictions = Activation('softmax')(x)\r\n  model = Model(inputs=inputs, outputs=predictions)\r\n\r\ncheckpointer = ModelCheckpoint(filepath='checkpoints/model-{epoch:02d}.hdf5', verbose=1)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.fit_generator(train_data_generator.generate(), args.epoch_size, args.epochs, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=args.epoch_size / 10, callbacks=[checkpointer])\r\n\r\n```\r\n\r\nYou have to start it two times (you will need a subdirectory checkpoints), the first time generates the checkpoint model file:\r\n\r\n```\r\n(tf2-gpu) detlef@ubuntu-i7:~/AutomaticProgramming$ python BugReport.py\r\n2019-10-22 12:25:00.253047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-22 12:25:00.276364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.277095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\n2019-10-22 12:25:00.277424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:25:00.278925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 12:25:00.279799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 12:25:00.280014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 12:25:00.281396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 12:25:00.282247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 12:25:00.286830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 12:25:00.287072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.287628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.288014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-22 12:25:00.288241: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-22 12:25:00.310514: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3999620000 Hz\r\n2019-10-22 12:25:00.310931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44a3ae0 executing computations on platform Host. Devices:\r\n2019-10-22 12:25:00.310946: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-22 12:25:00.413265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.413738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44a5940 executing computations on platform CUDA. Devices:\r\n2019-10-22 12:25:00.413751: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1\r\n2019-10-22 12:25:00.413908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.414331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\n2019-10-22 12:25:00.414378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:25:00.414387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 12:25:00.414394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 12:25:00.414401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 12:25:00.414408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 12:25:00.414414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 12:25:00.414421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 12:25:00.414467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.414924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.415391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-22 12:25:00.415430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:25:00.416258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-22 12:25:00.416267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-22 12:25:00.416282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-22 12:25:00.416464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.417126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:25:00.417832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6219 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nEpoch 1/2\r\n2019-10-22 12:25:01.298965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 12:25:02.108956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n 999/1000 [============================>.] - ETA: 0s - loss: 1.1246 - categorical_accuracy: 0.7498   \r\nEpoch 00001: saving model to checkpoints/model-01.hdf5\r\n1000/1000 [==============================] - 38s 38ms/step - loss: 1.1238 - categorical_accuracy: 0.7501 - val_loss: 0.3462 - val_categorical_accuracy: 1.0000\r\nEpoch 2/2\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.1313 - categorical_accuracy: 1.0000  \r\nEpoch 00002: saving model to checkpoints/model-02.hdf5\r\n1000/1000 [==============================] - 38s 38ms/step - loss: 0.1312 - categorical_accuracy: 1.0000 - val_loss: 0.0470 - val_categorical_accuracy: 1.0000\r\n(tf2-gpu) detlef@ubuntu-i7:~/AutomaticProgramming$ \r\n\r\n```\r\nthe second time we load a checkpoint:\r\n```\r\n\r\n(tf2-gpu) detlef@ubuntu-i7:~/AutomaticProgramming$ python BugReport.py --pretrained_name checkpoints/model-02.hdf5\r\n2019-10-22 12:27:35.436227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-22 12:27:35.456340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.456789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\n2019-10-22 12:27:35.456974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:27:35.457883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 12:27:35.458700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 12:27:35.458896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 12:27:35.459886: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 12:27:35.460658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 12:27:35.462926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 12:27:35.463040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.463511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.463922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-22 12:27:35.464170: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-22 12:27:35.486520: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3999620000 Hz\r\n2019-10-22 12:27:35.487009: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a23e30 executing computations on platform Host. Devices:\r\n2019-10-22 12:27:35.487026: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-22 12:27:35.597273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.597738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a25c90 executing computations on platform CUDA. Devices:\r\n2019-10-22 12:27:35.597754: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1\r\n2019-10-22 12:27:35.597930: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.598350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\n2019-10-22 12:27:35.598405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:27:35.598420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 12:27:35.598434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 12:27:35.598446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 12:27:35.598460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 12:27:35.598473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 12:27:35.598486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 12:27:35.598535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.599208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.599699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-10-22 12:27:35.599745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 12:27:35.600327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-22 12:27:35.600337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2019-10-22 12:27:35.600342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2019-10-22 12:27:35.600521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.601467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-10-22 12:27:35.602241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6189 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nEpoch 1/2\r\n2019-10-22 12:27:36.894279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.0299 - categorical_accuracy: 1.0000  \r\nEpoch 00001: saving model to checkpoints/model-01.hdf5\r\n1000/1000 [==============================] - 84s 84ms/step - loss: 0.0299 - categorical_accuracy: 1.0000 - val_loss: 0.0195 - val_categorical_accuracy: 1.0000\r\nEpoch 2/2\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.0150 - categorical_accuracy: 1.0000  \r\nEpoch 00002: saving model to checkpoints/model-02.hdf5\r\n1000/1000 [==============================] - 83s 83ms/step - loss: 0.0150 - categorical_accuracy: 1.0000 - val_loss: 0.0116 - val_categorical_accuracy: 1.0000\r\n(tf2-gpu) detlef@ubuntu-i7:~/AutomaticProgramming$ \r\n\r\n```\r\n", "comments": ["@dsmic  Can you please replicate this in google colab setting the runtime to GPU and let me know if the issue is still persisting. Also please share the github gist of that reproduction.Thanks!", "I tried to replicate it and simplified the source code for it, but the slow down does not show up. I do not see, how to check, if cudnn is used: \r\n\r\nhttps://gist.github.com/dsmic/d226c1d9831f483d2e5388cd81901837\r\n\r\nrunning the same code on my machine shows a slow down (with tf 2.0 and the nightly build):\r\n```\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nfrom tensorflow.keras.layers import Activation, Embedding, Dense, Input\r\nfrom tensorflow.keras.layers import SimpleRNN, GRU, LSTM\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport numpy as np\r\n\r\nRNN_type = {}\r\n\r\nRNN_type['LSTM'] = LSTM\r\nRNN_type['GRU'] = GRU\r\nRNN_type['SimpleRNN'] = SimpleRNN\r\n\r\nLSTM_use = RNN_type['GRU']\r\n\r\nmax_output = 10\r\nfull_python_file_string = [1,3,2,4,5,3,2,3,4,5]\r\nclass KerasBatchGenerator(object):\r\n    def __init__(self, data_set):\r\n        self.data_set = data_set\r\n            \r\n    def generate(self):\r\n        while True:\r\n            tmp_x = np.array([full_python_file_string], dtype=int)\r\n            tmp_y = np.array([full_python_file_string], dtype=int)\r\n            yield tmp_x, to_categorical(tmp_y, num_classes=max_output)\r\n\r\ntrain_data_generator = KerasBatchGenerator(0)\r\ntest_data_generator = KerasBatchGenerator(0)\r\n\r\nhidden_size = 50\r\n\r\nif None is not None:\r\n  from tensorflow.keras.models import load_model\r\n  model = load_model('Model-02.hdf5')\r\nelse:\r\n  inputs = Input(batch_shape=(1,None,))\r\n  embeds = Embedding(max_output, max_output, embeddings_initializer='identity', trainable=True)(inputs)\r\n  lstm1 = LSTM_use(hidden\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nfrom tensorflow.keras.layers import Activation, Embedding, Dense, Input\r\nfrom tensorflow.keras.layers import SimpleRNN, GRU, LSTM\r\nfrom tensorflow.keras.utils import to_categorical\r\nimport numpy as np\r\n\r\nRNN_type = {}\r\n\r\nRNN_type['LSTM'] = LSTM\r\nRNN_type['GRU'] = GRU\r\nRNN_type['SimpleRNN'] = SimpleRNN\r\n\r\nLSTM_use = RNN_type['GRU']\r\n\r\nmax_output = 10\r\nfull_python_file_string = [1,3,2,4,5,3,2,3,4,5]\r\nclass KerasBatchGenerator(object):\r\n    def __init__(self, data_set):\r\n        self.data_set = data_set\r\n            \r\n    def generate(self):\r\n        while True:\r\n            tmp_x = np.array([full_python_file_string], dtype=int)\r\n            tmp_y = np.array([full_python_file_string], dtype=int)\r\n            yield tmp_x, to_categorical(tmp_y, num_classes=max_output)\r\n\r\ntrain_data_generator = KerasBatchGenerator(0)\r\ntest_data_generator = KerasBatchGenerator(0)\r\n\r\nhidden_size = 50\r\n\r\nif None is not None:\r\n  from tensorflow.keras.models import load_model\r\n  model = load_model('Model-02.hdf5')\r\nelse:\r\n  inputs = Input(batch_shape=(1,None,))\r\n  embeds = Embedding(max_output, max_output, embeddings_initializer='identity', trainable=True)(inputs)\r\n  lstm1 = LSTM_use(hidden_size, return_sequences=True, stateful = True)(embeds)\r\n  x = Dense(max_output)(lstm1)\r\n  predictions = Activation('softmax')(x)\r\n  model = Model(inputs=inputs, outputs=predictions)\r\n\r\ncheckpointer = ModelCheckpoint(filepath='model-{epoch:02d}.hdf5', verbose=1)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 2, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10, callbacks=[checkpointer])\r\n\r\n\r\nfrom tensorflow.keras.models import load_model\r\nmodel = load_model('model-02.hdf5')\r\n\r\ncheckpointer = ModelCheckpoint(filepath='model-{epoch:02d}.hdf5', verbose=1)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 2, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10, callbacks=[checkpointer])\r\n_size, return_sequences=True, stateful = True)(embeds)\r\n  x = Dense(max_output)(lstm1)\r\n  predictions = Activation('softmax')(x)\r\n  model = Model(inputs=inputs, outputs=predictions)\r\n\r\ncheckpointer = ModelCheckpoint(filepath='model-{epoch:02d}.hdf5', verbose=1)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 2, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10, callbacks=[checkpointer])\r\n\r\n\r\nfrom tensorflow.keras.models import load_model\r\nmodel = load_model('model-02.hdf5')\r\n\r\ncheckpointer = ModelCheckpoint(filepath='model-{epoch:02d}.hdf5', verbose=1)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 2, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10, callbacks=[checkpointer])\r\n\r\n```\r\n\r\n```\r\n\r\nEpoch 1/2\r\n2019-10-23 23:59:44.382975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-23 23:59:45.209938: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n 998/1000 [============================>.] - ETA: 0s - loss: 1.1991 - categorical_accuracy: 0.6322   \r\nEpoch 00001: saving model to model-01.hdf5\r\n1000/1000 [==============================] - 37s 37ms/step - loss: 1.1975 - categorical_accuracy: 0.6329 - val_loss: 0.3753 - val_categorical_accuracy: 1.0000\r\nEpoch 2/2\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.1397 - categorical_accuracy: 1.0000 \r\nEpoch 00002: saving model to model-02.hdf5\r\n1000/1000 [==============================] - 35s 35ms/step - loss: 0.1396 - categorical_accuracy: 1.0000 - val_loss: 0.0486 - val_categorical_accuracy: 1.0000\r\nEpoch 1/2\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.0306 - categorical_accuracy: 1.0000  \r\nEpoch 00001: saving model to model-01.hdf5\r\n1000/1000 [==============================] - 81s 81ms/step - loss: 0.0306 - categorical_accuracy: 1.0000 - val_loss: 0.0198 - val_categorical_accuracy: 1.0000\r\nEpoch 2/2\r\n 999/1000 [============================>.] - ETA: 0s - loss: 0.0151 - categorical_accuracy: 1.0000  \r\nEpoch 00002: saving model to model-02.hdf5\r\n1000/1000 [==============================] - 82s 82ms/step - loss: 0.0151 - categorical_accuracy: 1.0000 - val_loss: 0.0117 - val_categorical_accuracy: 1.0000\r\n\r\n```", "OK, I just realized, that Google Colab does not use tensorflow 2.0 by default. I installed tensorflow 2.0 and was able to reproduce the problem. It is not, as my first thought, a problem loading cudnn, but it appears with cpu, gpu and tpu if you are using GRU cells. They are faster if they are not loaded from a file with load_model:\r\n\r\n[gist](https://gist.github.com/dsmic/fcfbf9e3a433d3aed7c00bea9e810f0c) \r\n\r\n```\r\n!pip install tensorflow==2.0\r\n\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Activation, Embedding, Dense, Input\r\nfrom tensorflow.keras.layers import SimpleRNN, GRU, LSTM\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom tensorflow.keras.models import load_model\r\nimport numpy as np\r\n\r\n# with LSTM and SimpleRNN the problem does not exist ..\r\nLSTM_use = GRU\r\n\r\nmax_output = 10\r\nfull_python_file_string = [1,3,2,4,5,3,2,3,4,5]\r\nclass KerasBatchGenerator(object):\r\n    def generate(self):\r\n        while True:\r\n            tmp_x = np.array([full_python_file_string], dtype=int)\r\n            tmp_y = np.array([full_python_file_string], dtype=int)\r\n            yield tmp_x, to_categorical(tmp_y, num_classes=max_output)\r\n\r\ntrain_data_generator = KerasBatchGenerator()\r\ntest_data_generator = KerasBatchGenerator()\r\n\r\ninputs = Input(batch_shape=(1,None,))\r\nembeds = Embedding(max_output, max_output, embeddings_initializer='identity', trainable=True)(inputs)\r\nlstm1 = LSTM_use(50, return_sequences=True, stateful = True)(embeds)\r\nx = Dense(max_output)(lstm1)\r\npredictions = Activation('softmax')(x)\r\nmodel = Model(inputs=inputs, outputs=predictions)\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer = 'SGD', metrics=['categorical_accuracy'])\r\nmodel.save('start_model.hdf5')\r\n\r\n#model=load_model('start_model.hdf5') #loading here slows down first fit_generator call too\r\n\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 1, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10)\r\n\r\nmodel = load_model('start_model.hdf5') #'model-02.hdf5')\r\n\r\nmodel.fit_generator(train_data_generator.generate(), 1000, 1, \r\n                    validation_data=test_data_generator.generate(), \r\n                    validation_steps=1000 / 10)\r\n```\r\noutput in google colab:\r\n\r\n```\r\nCollecting tensorflow==2.0\r\n  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86.3MB 127kB/s \r\nRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.7)\r\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.15.0)\r\nCollecting tensorboard<2.1.0,>=2.0.0\r\n  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8MB 35.6MB/s \r\nRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\r\nRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\r\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.17.3)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\r\nRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\r\nRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.0)\r\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.33.6)\r\nCollecting tensorflow-estimator<2.1.0,>=2.0.0\r\n  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 450kB 49.9MB/s \r\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\r\nRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (41.4.0)\r\nRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\r\nInstalling collected packages: tensorboard, tensorflow-estimator, tensorflow\r\n  Found existing installation: tensorboard 1.15.0\r\n    Uninstalling tensorboard-1.15.0:\r\n      Successfully uninstalled tensorboard-1.15.0\r\n  Found existing installation: tensorflow-estimator 1.15.1\r\n    Uninstalling tensorflow-estimator-1.15.1:\r\n      Successfully uninstalled tensorflow-estimator-1.15.1\r\n  Found existing installation: tensorflow 1.15.0\r\n    Uninstalling tensorflow-1.15.0:\r\n      Successfully uninstalled tensorflow-1.15.0\r\nSuccessfully installed tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.1\r\n1000/1000 [==============================] - 42s 42ms/step - loss: 1.2335 - categorical_accuracy: 0.6717 - val_loss: 0.4020 - val_categorical_accuracy: 0.9000\r\n1000/1000 [==============================] - 62s 62ms/step - loss: 1.2335 - categorical_accuracy: 0.6717 - val_loss: 0.4020 - val_categorical_accuracy: 0.9000\r\n\r\n<tensorflow.python.keras.callbacks.History at 0x7f61fbb02da0>\r\n```", "Same problem here. Versions: Tf 2.0.0, CUDA: 10.0, CUDNN: 7.6, Python 3.7.4, Ubuntu 18.04.3\r\n\r\nIn addition, when logging the device placement of ops with `tf.debugging.set_log_device_placement(True)`, I can see that in each training step (forward pass and gradient computation) the following logs appear:\r\n\r\n`2019-10-29 10:55:09.199413: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op Unpack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-10-29 10:55:09.463677: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op Pack in device /job:localhost/replica:0/task:0/device:GPU:0\r\n2019-10-29 10:55:09.862140: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op AddN in device /job:localhost/replica:0/task:0/device:GPU:0`\r\n\r\nThey appear in each training step, so once every batch is processed. They do not appear when the model is created from scratch instead of loaded from an h5 file. I don't know if this might be the cause of the problem.\r\n", "@dsmic Is this still an issue? When I used recent `tf-nightly`, I don't see any significant difference between running the `fit_generator` with loaded model or with direct model. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/aa468f554163a4c0c340dc924bcc2000/untitled849.ipynb). I have also imported `time` to show computational time with and without loading model.\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Seems to be ok, thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33601\">No</a>\n"]}, {"number": 33600, "title": "AttributeError: module 'tensorflow.python.keras' has no attribute 'Model'", "body": "Running code from object Detection\r\nnot a custom code\r\nI am using Window 10\r\nTensorflow 1.4 CPU\r\nI am getting this error, while running below code.\r\ni want to rectify the error, without updating Tensorflow,\r\n\r\nimport tensorflow as tf\r\n\r\nBOX_ENCODINGS = 'box_encodings'\r\nCLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'\r\nMASK_PREDICTIONS = 'mask_predictions'\r\n\r\n\r\nclass BoxPredictor(object):\r\n  \"\"\"BoxPredictor.\"\"\"\r\n\r\n  def __init__(self, is_training, num_classes):\r\n    \"\"\"Constructor.\r\n\r\n    Args:\r\n      is_training: Indicates whether the BoxPredictor is in training mode.\r\n      num_classes: number of classes.  Note that num_classes *does not*\r\n        include the background category, so if groundtruth labels take values\r\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\r\n        assigned classification targets can range from {0,... K}).\r\n    \"\"\"\r\n    self._is_training = is_training\r\n    self._num_classes = num_classes\r\n\r\n  @property\r\n  def is_keras_model(self):\r\n    return False\r\n\r\n  @property\r\n  def num_classes(self):\r\n    return self._num_classes\r\n\r\n  def predict(self, image_features, num_predictions_per_location,\r\n              scope=None, **params):\r\n    \"\"\"Computes encoded object locations and corresponding confidences.\r\n\r\n    Takes a list of high level image feature maps as input and produces a list\r\n    of box encodings and a list of class scores where each element in the output\r\n    lists correspond to the feature maps in the input list.\r\n\r\n    Args:\r\n      image_features: A list of float tensors of shape [batch_size, height_i,\r\n      width_i, channels_i] containing features for a batch of images.\r\n      num_predictions_per_location: A list of integers representing the number\r\n        of box predictions to be made per spatial location for each feature map.\r\n      scope: Variable and Op scope name.\r\n      **params: Additional keyword arguments for specific implementations of\r\n              BoxPredictor.\r\n\r\n    Returns:\r\n      A dictionary containing at least the following tensors.\r\n        box_encodings: A list of float tensors. Each entry in the list\r\n          corresponds to a feature map in the input `image_features` list. All\r\n          tensors in the list have one of the two following shapes:\r\n          a. [batch_size, num_anchors_i, q, code_size] representing the location\r\n            of the objects, where q is 1 or the number of classes.\r\n          b. [batch_size, num_anchors_i, code_size].\r\n        class_predictions_with_background: A list of float tensors of shape\r\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\r\n          predictions for the proposals. Each entry in the list corresponds to a\r\n          feature map in the input `image_features` list.\r\n\r\n    Raises:\r\n      ValueError: If length of `image_features` is not equal to length of\r\n        `num_predictions_per_location`.\r\n    \"\"\"\r\n    if len(image_features) != len(num_predictions_per_location):\r\n      raise ValueError('image_feature and num_predictions_per_location must '\r\n                       'be of same length, found: {} vs {}'.\r\n                       format(len(image_features),\r\n                              len(num_predictions_per_location)))\r\n    if scope is not None:\r\n      with tf.variable_scope(scope):\r\n        return self._predict(image_features, num_predictions_per_location,\r\n                             **params)\r\n    return self._predict(image_features, num_predictions_per_location,\r\n                         **params)\r\n\r\n  # TODO(rathodv): num_predictions_per_location could be moved to constructor.\r\n  # This is currently only used by ConvolutionalBoxPredictor.\r\n  @abstractmethod\r\n  def _predict(self, image_features, num_predictions_per_location, **params):\r\n    \"\"\"Implementations must override this method.\r\n\r\n    Args:\r\n      image_features: A list of float tensors of shape [batch_size, height_i,\r\n        width_i, channels_i] containing features for a batch of images.\r\n      num_predictions_per_location: A list of integers representing the number\r\n        of box predictions to be made per spatial location for each feature map.\r\n      **params: Additional keyword arguments for specific implementations of\r\n              BoxPredictor.\r\n\r\n    Returns:\r\n      A dictionary containing at least the following tensors.\r\n        box_encodings: A list of float tensors. Each entry in the list\r\n          corresponds to a feature map in the input `image_features` list. All\r\n          tensors in the list have one of the two following shapes:\r\n          a. [batch_size, num_anchors_i, q, code_size] representing the location\r\n            of the objects, where q is 1 or the number of classes.\r\n          b. [batch_size, num_anchors_i, code_size].\r\n        class_predictions_with_background: A list of float tensors of shape\r\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\r\n          predictions for the proposals. Each entry in the list corresponds to a\r\n          feature map in the input `image_features` list.\r\n    \"\"\"\r\n    pass\r\n\r\n\r\nclass KerasBoxPredictor(tf.keras.Model):\r\n  \"\"\"Keras-based BoxPredictor.\"\"\"\r\n\r\n  def __init__(self, is_training, num_classes, freeze_batchnorm,\r\n               inplace_batchnorm_update, name=None):\r\n    \"\"\"Constructor.\r\n\r\n    Args:\r\n      is_training: Indicates whether the BoxPredictor is in training mode.\r\n      num_classes: number of classes.  Note that num_classes *does not*\r\n        include the background category, so if groundtruth labels take values\r\n        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the\r\n        assigned classification targets can range from {0,... K}).\r\n      freeze_batchnorm: Whether to freeze batch norm parameters during\r\n        training or not. When training with a small batch size (e.g. 1), it is\r\n        desirable to freeze batch norm update and use pretrained batch norm\r\n        params.\r\n      inplace_batchnorm_update: Whether to update batch norm moving average\r\n        values inplace. When this is false train op must add a control\r\n        dependency on tf.graphkeys.UPDATE_OPS collection in order to update\r\n        batch norm statistics.\r\n      name: A string name scope to assign to the model. If `None`, Keras\r\n        will auto-generate one from the class name.\r\n    \"\"\"\r\n    super(KerasBoxPredictor, self).__init__(name=name)\r\n\r\n    self._is_training = is_training\r\n    self._num_classes = num_classes\r\n    self._freeze_batchnorm = freeze_batchnorm\r\n    self._inplace_batchnorm_update = inplace_batchnorm_update\r\n\r\n  @property\r\n  def is_keras_model(self):\r\n    return True\r\n\r\n  @property\r\n  def num_classes(self):\r\n    return self._num_classes\r\n\r\n  def call(self, image_features, **kwargs):\r\n    \"\"\"Computes encoded object locations and corresponding confidences.\r\n\r\n    Takes a list of high level image feature maps as input and produces a list\r\n    of box encodings and a list of class scores where each element in the output\r\n    lists correspond to the feature maps in the input list.\r\n\r\n    Args:\r\n      image_features: A list of float tensors of shape [batch_size, height_i,\r\n      width_i, channels_i] containing features for a batch of images.\r\n      **kwargs: Additional keyword arguments for specific implementations of\r\n            BoxPredictor.\r\n\r\n    Returns:\r\n      A dictionary containing at least the following tensors.\r\n        box_encodings: A list of float tensors. Each entry in the list\r\n          corresponds to a feature map in the input `image_features` list. All\r\n          tensors in the list have one of the two following shapes:\r\n          a. [batch_size, num_anchors_i, q, code_size] representing the location\r\n            of the objects, where q is 1 or the number of classes.\r\n          b. [batch_size, num_anchors_i, code_size].\r\n        class_predictions_with_background: A list of float tensors of shape\r\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\r\n          predictions for the proposals. Each entry in the list corresponds to a\r\n          feature map in the input `image_features` list.\r\n    \"\"\"\r\n    return self._predict(image_features, **kwargs)\r\n\r\n  @abstractmethod\r\n  def _predict(self, image_features, **kwargs):\r\n    \"\"\"Implementations must override this method.\r\n\r\n    Args:\r\n      image_features: A list of float tensors of shape [batch_size, height_i,\r\n        width_i, channels_i] containing features for a batch of images.\r\n      **kwargs: Additional keyword arguments for specific implementations of\r\n              BoxPredictor.\r\n\r\n    Returns:\r\n      A dictionary containing at least the following tensors.\r\n        box_encodings: A list of float tensors. Each entry in the list\r\n          corresponds to a feature map in the input `image_features` list. All\r\n          tensors in the list have one of the two following shapes:\r\n          a. [batch_size, num_anchors_i, q, code_size] representing the location\r\n            of the objects, where q is 1 or the number of classes.\r\n          b. [batch_size, num_anchors_i, code_size].\r\n        class_predictions_with_background: A list of float tensors of shape\r\n          [batch_size, num_anchors_i, num_classes + 1] representing the class\r\n          predictions for the proposals. Each entry in the list corresponds to a\r\n          feature map in the input `image_features` list.\r\n    \"\"\"\r\n    raise NotImplementedError\r\n\r\n", "comments": ["@esakkirajask, Please provide the link of the object detection code. Thanks! ", "@esakkirajask, Is this still issue!", "No it closed.\n\nOn Wed, 6 Nov 2019, 4:44 pm gadagashwini, <notifications@github.com> wrote:\n\n> @esakkirajask <https://github.com/esakkirajask>, Is this still issue!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33600?email_source=notifications&email_token=ANQNUGZ3Y2GAABDK3N2VDIDQSKRINA5CNFSM4JDNL47KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDGFLRY#issuecomment-550262215>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANQNUG4OEAITSIZKPWV5EK3QSKRINANCNFSM4JDNL47A>\n> .\n>\n", "Closing this issue since its resolved. Thanks!\r\n\r\n"]}, {"number": 33599, "title": "C binding for tensorflow 2.0", "body": "Is there a release date for `c binding` for `tf2`?", "comments": ["We are currently building libtensorflow by ourselves, but it would be great if the official one would appear.", "@sjamesr is the Java SIG taking ownership of the libtensorflow releases?", "I am taking ownership of libtensorflow packages, as it looks like all of the previous owners left the team.\r\nWe have noticed some issues with the current packages we have. We will look into fixing them and publishing them again.", "Yay!", "@foxik Could you please share how you are building them yourself. We have tried, but it errors when making predictions. Thanks in advance.", "Does anyone know what version of TensorFlow 2 will have libtensorflow in it?", "2.3 will have libtensorflow. Additionally we have nightly packages on GCS. You can find the links on the main README.", "For everyone who's subscribed, it's arrived: https://www.tensorflow.org/install/lang_c", "I saw that, awesome, thanks everyone!\n\nOn Thu, Jul 30, 2020 at 2:38 AM Andrew Kane <notifications@github.com>\nwrote:\n\n> For everyone who's subscribed, it's arrived:\n> https://www.tensorflow.org/install/lang_c\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33599#issuecomment-666157941>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAQ4TMIDOR7VLJ2HWA3IY3R6EIOLANCNFSM4JDMMLBQ>\n> .\n>\n", "@oak-tree,\r\nClosing this issue as it has been resolved, as per [this Comment](https://github.com/tensorflow/tensorflow/issues/33599#issuecomment-666157941). Please refer this [Tensorflow Documentation](https://www.tensorflow.org/install/lang_c) for more information. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33599\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33599\">No</a>\n"]}, {"number": 33598, "title": "about tf.gradients() API", "body": "The API `tf.gradients` has a parameter `gate_gradients`\r\n```gate_gradients: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.```\r\n\r\nIn case of op has more than one in_grads, it makes a barrier, grads will not propagate until the whole grad subGraph was computed. It make sense.\r\n\r\nBut, in case of op has more than one in_grads and the upstream ops are not Variable, the barrier seems redundant. The OP parallelism can be improved.\r\n\r\nthanks", "comments": ["@gowthamkpr ", "@candyzone,\r\nSorry for the delayed response. In **`Tensorflow 2.x`**, we use [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) over [tf.gradients](https://www.tensorflow.org/api_docs/python/tf/gradients). Can you please let us know if the feature that you are looking for is not present in [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) as well?\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33597, "title": "how to realize batch convolutional operation", "body": "hello,\r\n    i want to use batch convolutional operation, is said:\r\n   input shape:  [batch_size, height, width, channel], and i have a group of kernels:\r\n    kernel shape:  [batch_size, kernel_size_height, kernel_size_width, out_channel]\r\ni want to use this set of kernels to convolute the batch data: for each data in the batch, corresponding a kernel, like:\r\n\r\ndata[0] (shape: [ height, width, channel]) ---use-- ->  kernels[0] (shape: [kernel_size_height, kernel_size_width, out_channel])\r\ndata[1] (shape: [ height, width, channel])  ---use-- ->  kernels[1] (shape: [kernel_size_height, kernel_size_width, out_channel])\r\n\r\nbut, i failed to find the correlative function.   i use tensorflow1.13, does any function realize that??\uff1f\r\nthanks\uff01", "comments": ["@J-zin I am not clear about your question\r\n\r\n> i failed to find the correlative function. i use tensorflow1.13, does any function realize that??\uff1f\r\n\r\nWhat do you mean by that. Can you elaborate what you are exactly trying to solve", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been inactive for more than 14 days. Please add additional  comments and we can open this issue again. Thanks!"]}, {"number": 33596, "title": "Fix for parallel_map_dataset_op_test failure on big endian", "body": "For Tensorflow v2.0.0, `//tensorflow/core/kernels/data:parallel_map_dataset_op_test` module was failing on s390x  with error \"**_Which is: Invalid argument: num_parallel_calls must be greater than zero._**\"\r\n\r\nThe int32 results in a negative value of num_parallel_calls, hence the test case fails on s390x.\r\nWhen the type is changed to int64, behavior is same for both s390x and Intel.  And all the test cases of parallel_map_dataset_op_test run successfully.\r\nEven after changing int32 to int64, behavior on Intel is unaffected.\r\n", "comments": ["I believe the problem is with the test and not with the kernel. The op kernel expects a int32 for `num_parallel_calls` but the failing test is creating int64 tensor. Could you please see if changing the test to use int32 for the num_parallel_calls tensors fixes the problem?", "@anujajakhade Could you try to change `int64` to `int32` at Line [89](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L89), [111](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L111), [134](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L134), [156](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L156),\r\n[179](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L179), [202](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L202), [226](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L226), [250](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L250), [270](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L270)?", "> @anujajakhade Could you try to change `int64` to `int32` at Line [89](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L89), [111](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L111), [134](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L134), [156](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L156),\r\n> [179](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L179), [202](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L202), [226](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L226), [250](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L250), [270](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/parallel_map_dataset_op_test.cc#L270)?\r\n\r\nThe mentioned changes work on s390x. Should I do the changes in the same PR ? ", "Yes, I revert the change to the kernel.", "@jsimsa I have done the required changes. Can you please review?"]}, {"number": 33595, "title": "`tf.summary.FileWriter` overwrites existing events file if reopened in same `logdir`quickly.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: GeForce 940MX, 2GB\r\n\r\n**Current behaviour**\r\nIf `tf.summary.FileWriter` is closed and then reopened in the same `logdir`, `FileWriter` overwrites events file created right before closure. If I add a delay (expr `time.sleep(1)`) before reopening `FileWriter`, reopened `FileWriter` creates new events file. \r\n\r\n**Expected behaviour**\r\nI expect `FileWriter` creating new events file regardless of how recently `logdir` was used.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport os\r\nimport time\r\n\r\nimport tensorflow.compat.v1 as tf\r\n\r\n\r\ndef print_dir_contents(path, msg):\r\n    print()\r\n    print(msg)\r\n    print(\"Summary dir contents:\", os.listdir(path))\r\n    print(\"Size of events files:\", [os.stat(os.path.join(path, f))[-4] for f in os.listdir(path)])\r\n\r\n\r\npath = 'summary_dir'\r\n\r\npl = tf.placeholder(tf.float32)\r\n\r\ns = tf.summary.tensor_summary('tsum', pl)\r\n\r\nos.makedirs(path, exist_ok=True)\r\nprint_dir_contents(path, \"Before summarizing:\")\r\nwith tf.Session() as sess:\r\n    w = tf.summary.FileWriter(logdir=path)\r\n    for i in range(10):\r\n        s_ = sess.run(s, feed_dict={pl: i})\r\n        w.add_summary(s_, global_step=i)\r\n    w.flush()\r\n    w.close()\r\n    print_dir_contents(path, \"After first writer shutdown:\")\r\n    # time.sleep(1)  # If uncomment, snippet works correctly.\r\n    w = tf.summary.FileWriter(logdir=path)\r\n    for i in range(10, 20):\r\n        s_ = sess.run(s, feed_dict={pl: i})\r\n        w.add_summary(s_, global_step=i)\r\n    w.flush()\r\ntime.sleep(1)\r\nprint_dir_contents(path, \"After second flush:\")\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n\r\nBefore summarizing:\r\nSummary dir contents: []\r\nSize of events files: []\r\n2019-10-22 10:22:34.689307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-22 10:22:35.800362: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2019-10-22 10:22:35.800561: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: aenima\r\n2019-10-22 10:22:35.800639: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: aenima\r\n2019-10-22 10:22:35.800881: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.67.0\r\n2019-10-22 10:22:35.801044: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.67.0\r\n2019-10-22 10:22:35.801107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.67.0\r\n2019-10-22 10:22:35.844875: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-22 10:22:36.569070: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2711950000 Hz\r\n2019-10-22 10:22:36.570320: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564451333fb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-10-22 10:22:36.570411: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n\r\nAfter first writer shutdown:\r\nSummary dir contents: ['events.out.tfevents.1571728959.aenima']\r\nSize of events files: [530]\r\n\r\nAfter second flush:\r\nSummary dir contents: ['events.out.tfevents.1571728959.aenima']\r\nSize of events files: [532]\r\n```\r\n", "comments": ["Issue replicating for TF-1.15, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/2fea3d9404fcfd69cf82dd87fe0e625f/untitled.ipynb#scrollTo=IYPi-FxzI_R_) of colab.Thanks!", "This is a known issue with FileWriter in `TF 1.x` and now that TF 2.0 is out, we won't be making further changes to this API.  But the new `tf.summary.create_file_writer()` in TF 2.0 does a better job of avoiding this issue by including the PID and a per-process UID in the filename as well as the timestamp.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33595\">No</a>\n", "Thank you for the response!"]}, {"number": 33594, "title": "graph_transforms build has error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source):0.21,I download deb file\r\n- GCC/Compiler version (if compiling from source):Ubuntu default gcc,gcc version 7.4.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory:no\r\n\r\n\r\n\r\nI want to know,which version with tensorflow and bazel can build sucess?\r\n\r\n\r\n", "comments": ["tf1.4  bazel 0.29\r\nC++ compilation of rule '//tensorflow/core/kernels:tile_ops' failed", "@yyf1986 \r\n\r\nCan you please go through the below [link](https://www.tensorflow.org/install/source#tested_build_configurations) and see if it helps you.Thanks!", "@yyf1986 \r\n\r\nAny updates on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33594\">No</a>\n"]}, {"number": 33593, "title": "distributed training with MirroredStrategy crashes when input shapes are variable", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): tf2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 10.0 cudnn 7.6.4\r\n- GPU model and memory: rtx titan / 24gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow raise error (ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:) when checking input signature of a tf.function if input shape is variable in a distributed training environment. The training works without any error when I train it with single GPU or input with fixed shape, or I delete a path to cuda from my environment path. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport random\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\r\nclass Model(keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.emb = keras.layers.Embedding(51,100)\r\n        self.layer = keras.layers.Dense(51)\r\n    def call(self,x):\r\n        x = self.emb(x)\r\n        x = self.layer(x)\r\n        return x\r\n@tf.function(input_signature=(tf.TensorSpec(shape=[None, None], dtype=tf.int64),\r\n                              tf.TensorSpec(shape=[None, None], dtype=tf.int64)))\r\ndef multi_gpu_step(x,y):\r\n    def example_update_step(x, y):\r\n        with tf.GradientTape() as tape:\r\n            y_ = model(x)\r\n            batch_loss = keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_, from_logits=True)`\r\n            losses = batch_loss / strategy.num_replicas_in_sync\r\n        step_grad = tape.gradient(losses, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(step_grad, model.trainable_variables))\r\n        return tf.reduce_mean(batch_loss,1)\r\n    example_loss = strategy.experimental_run_v2(\r\n        example_update_step, args=(x, y))\r\n    losses_sum = strategy.reduce(\r\n        tf.distribute.ReduceOp.SUM, example_loss, axis=0)\r\n    return losses_sum\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\ndata = [[i for i in range(random.randint(10,50))] for j in range(400)]\r\n\r\n\r\ndef iterator():\r\n    for i in range(len(data)):\r\n        yield data[i], data[i]\r\n\r\n\r\nwith strategy.scope():\r\n    model = Model()\r\n    optimizer = keras.optimizers.Adam()\r\n\r\ndataset = tf.data.Dataset.from_generator(iterator, output_types=(tf.int64, tf.int64))\r\nbatchfier = dataset.padded_batch(4, padded_shapes=([None], [None]))\r\nbatchfier = strategy.experimental_distribute_dataset(batchfier)\r\n\r\nfor x,y in batchfier:\r\n    l = multi_gpu_step(x,y)\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n**Other info / logs**\r\n```ssh://bj1123@121.78.112.79:2222/home/bj1123/anaconda3/bin/python -u /home/bj1123/pycharms/GPT2/multi_test_variable.py\r\n2019-10-22 14:21:01.879166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-10-22 14:21:02.010621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:b2:00.0\r\n2019-10-22 14:21:02.011936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:db:00.0\r\n2019-10-22 14:21:02.012174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 14:21:02.013811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 14:21:02.015315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 14:21:02.015650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 14:21:02.017535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 14:21:02.019038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 14:21:02.023539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 14:21:02.028529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-10-22 14:21:02.028935: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-10-22 14:21:02.058997: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2019-10-22 14:21:02.062760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d0eb1f8ad0 executing computations on platform Host. Devices:\r\n2019-10-22 14:21:02.062799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-22 14:21:02.415212: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d0eb25ba20 executing computations on platform CUDA. Devices:\r\n2019-10-22 14:21:02.415251: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5\r\n2019-10-22 14:21:02.415260: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN RTX, Compute Capability 7.5\r\n2019-10-22 14:21:02.417153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:b2:00.0\r\n2019-10-22 14:21:02.418575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:db:00.0\r\n2019-10-22 14:21:02.418622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 14:21:02.418637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 14:21:02.418652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 14:21:02.418667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 14:21:02.418682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 14:21:02.418697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 14:21:02.418714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 14:21:02.424652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-10-22 14:21:02.424697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 14:21:02.428615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-22 14:21:02.428630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-10-22 14:21:02.428636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N \r\n2019-10-22 14:21:02.428641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N \r\n2019-10-22 14:21:02.432235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22846 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:b2:00.0, compute capability: 7.5)\r\n2019-10-22 14:21:02.434469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 22846 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:db:00.0, compute capability: 7.5)\r\n2019-10-22 14:21:02.445038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:b2:00.0\r\n2019-10-22 14:21:02.448745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: TITAN RTX major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:db:00.0\r\n2019-10-22 14:21:02.448816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-10-22 14:21:02.448856: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-10-22 14:21:02.448907: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-10-22 14:21:02.448953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-10-22 14:21:02.449004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-10-22 14:21:02.449045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-10-22 14:21:02.449095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-10-22 14:21:02.459964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-10-22 14:21:02.460089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-22 14:21:02.460116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-10-22 14:21:02.460136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N \r\n2019-10-22 14:21:02.460156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N \r\n2019-10-22 14:21:02.466303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 22846 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:b2:00.0, compute capability: 7.5)\r\n2019-10-22 14:21:02.468400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 22846 MB memory) -> physical GPU (device: 1, name: TITAN RTX, pci bus id: 0000:db:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\r\nTraceback (most recent call last):\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1704, in _convert_inputs_to_signature\r\n    value, dtype_hint=spec.dtype)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: <tf.Tensor: id=145, shape=(2, 49), dtype=int64, numpy=\r\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\r\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  0,  0,\r\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\r\n         0],\r\n       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\r\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\r\n        32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\r\n        48]])>,\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: <tf.Tensor: id=148, shape=(2, 27), dtype=int64, numpy=\r\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\r\n        16, 17, 18,  0,  0,  0,  0,  0,  0,  0,  0],\r\n       [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\r\n        16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]])>\r\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/bj1123/pycharms/GPT2/multi_test_variable.py\", line 54, in <module>\r\n    l = multi_gpu_step(x,y)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1822, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2107, in _maybe_define_function\r\n    *args, **kwargs)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1650, in canonicalize_function_inputs\r\n    self._flat_input_signature)\r\n  File \"/home/bj1123/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1710, in _convert_inputs_to_signature\r\n    format_error_message(inputs, input_signature))\r\nValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:\r\n  inputs: (\r\n    PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(\r\n[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\r\n   0]\r\n [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\r\n  48]], shape=(2, 49), dtype=int64),\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(\r\n[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18  0  0  0  0  0\r\n   0  0  0]\r\n [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26]], shape=(2, 27), dtype=int64)\r\n},\r\n    PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(\r\n[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26 27 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\r\n   0]\r\n [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\r\n  48]], shape=(2, 49), dtype=int64),\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(\r\n[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18  0  0  0  0  0\r\n   0  0  0]\r\n [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\r\n  24 25 26]], shape=(2, 27), dtype=int64)\r\n})\r\n  input_signature: (\r\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\r\n    TensorSpec(shape=(None, None), dtype=tf.int64, name=None))\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["when I change inputs to be a fixed shape \r\n\r\nwith \r\n`strategy = tf.distribute.MirroredStrategy()`\r\n`x = tf.random.uniform((800,50),maxval=50,dtype=tf.int64)`\r\n`y = tf.random.uniform((800,50),maxval=50,dtype=tf.int64)`\r\n`dataset = tf.data.Dataset.from_tensor_slices((x,y))`\r\n`batchfier = dataset.batch(4)`\r\n\r\nit works perfectly\r\n\r\n\r\n\r\nIn addition, I tried setting experimental_relax_shapes to true, but it didn't work.\r\nNot passing input_signature argument, which means just decorating codes only with @tf.function did not raise error, but it significantly slowed down the training speed. ", "This issue has now been fixed and an example has been provided in #29911 for how to use the element_spec property of distributed datasets/iterators to specify the input_signature. Please reopen this issue as needed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33593\">No</a>\n"]}]