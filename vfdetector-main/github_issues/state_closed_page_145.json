[{"number": 50527, "title": "Fix numpy 1.20 deprecation warnings", "body": "Replaced\r\n* np.bool with np.bool_,\r\n* np.int with np.int64,\r\n* np.float with np.float64,\r\n* np.complex with np.complex128\r\n* np.object with np.object_,\r\n* np.str with np.str_, and\r\n* np.unicode with np.str_.\r\n\r\nRepaired 4 uses of np.complex that should have been np.complexfloating.\r\n\r\nAccording to https://numpy.org/doc/stable/release/1.20.0-notes.html,\r\nthis will have no effect except to suppress deprecation warnings.", "comments": ["Thank you for this PR.\r\n\r\nWouldn't it be more elegant to replace `np.bool` with `bool`, `np.object` with `object`, and `np.string` and `np.unicode` with `str`?\r\n\r\nThe numpy release notes say:\r\n\r\n> To give a clear guideline for the vast majority of cases, for the types bool, object, str (and unicode) using the plain version is shorter and clear, and generally a good replacement.", "> Wouldn't it be more elegant to replace\u2026\r\n\r\nYes, it's definitely a reasonable solution to specify dtypes using the Python types or using dtypes.\r\n\r\nOne of my considerations is to try to remain consistent with the rest of TensorFlow.  Currently, roughly 13000 other places in the code use dtypes whereas about 1200 use Python types.\r\n\r\nAnother reason is that `np.int64` is explicit.  Someone may come along later and wonder whether `int` corresponds to `int64` or `int32` or `int128` (when it exists).  It's nice not having to guess.\r\n\r\nFinally, this is the typical style in every numeric library I've looked at (NumPy itself, SciPy, JAX, etc.)\r\n\r\nRegardless, I'm happy to do things the other way if you prefer.", "I fully agree on the more explicit `int64` -- which is why I did not list it here.\r\n\r\nThe other ones are somewhat about aesthetics and a lot about how much the code will make the reader think, and what it will make them think:\r\n\r\n - Something that ends in an underscore would make me think -- \"ah, looks we are using some internals or stuff we otherwise should not use\".\r\n -  Using `np.unicode_` makes it look like it represents a datatype different from `str` when in reality it is the same datatype (in Python 3).\r\n - Using the `np.` prefix might make readers of the code think that these datatypes are different from Python's built-in ones, when in reality they are not.\r\n\r\nOf course that needs to be weighed against the consistency with the rest of the code base. If the usage was almost 100% in one direction, the decision would be easy :-).\r\n\r\nAnyway, just my opinion with some thoughts around it -- do whatever suits you with it.\r\nI am happy if the deprecation warnings go away.\r\n", "> Something that ends in an underscore would make me think -- \"ah, looks we are using some internals or stuff we otherwise should not use\".\r\n\r\nIn Python, when the underscore *precedes* the word, it indicates a *private variable*.  An underscore *following* a word indicates a *name chosen to avoid a collision*, which is exactly what happened here.\r\n\r\n> Using np.unicode_ makes it look like it represents a datatype different from str when in reality it is the same datatype (in Python 3).\r\n\r\nGood catch.  There was one instance of this, and I changed it to `np.str_`.\r\n\r\n> Using the np. prefix might make readers of the code think that these datatypes are different from Python's built-in ones, when in reality they are not.\r\n\r\nSorry, but you are mistaken.  They are different data types.  Whether you pass `bool` or `np.bool_` as your dtype argument, NumPy will convert it to the dtype `np.bool_` and the array it creates is based on its representation of Boolean values (maybe one byte per element)--and not Python's representation of Boolean values, which is a boxed object.  Similarly, NumPy's representation of integers is `int64` while Python's has arbitrary precision.  It is true that you can pass Python type names as synonyms, but the ultimate internal representation is NumPy dtypes.\r\n\r\n> I am happy if the deprecation warnings go away.\r\n\r\nMe too!!", "@NeilGirdhar  Can you please check build failures. Thanks!", "@gbaned  Happy to.\r\n\r\nThe PyLint error is here:\r\n```python\r\n      x = np.linspace(-np.e, np.e, num=1000, dtype=dtype)\r\n```\r\nPyLint doesn't like the `-np.e`.  I don't know why this is happening, but I don't think it's related to this changelist.\r\n\r\nThe oneDNN CI fails some builds, which has nothing to do with this changelist since I only touched Python.  It also says that this test fails: \"tensorflow/python/kernel_tests/linalg/sparse:conjugate_gradient_test\".  I ran it on my machine and it works fine:\r\n\r\n<details>\r\n\r\n```\r\npython tensorflow/python/kernel_tests/linalg/sparse/conjugate_gradient_test.py                                                                     \ue0b6 \u2714 \ue0ba 4s \uf252 \ue0b4\r\nRunning tests under Python 3.9.5: /home/neil/.pyenv/versions/3.9.5/bin/python\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_False): 0.26s\r\nI0706 14:58:00.242783 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_False): 0.26s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_True): 0.07s\r\nI0706 14:58:00.313689 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_True): 0.07s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_10_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_False): 0.02s\r\nI0706 14:58:00.329952 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_False): 0.02s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_True): 0.01s\r\nI0706 14:58:00.344354 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_True): 0.01s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_1_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_False): 0.03s\r\nI0706 14:58:00.375050 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_False): 0.03s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_True): 0.03s\r\nI0706 14:58:00.406750 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_True): 0.03s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat32_4_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_False): 0.07s\r\nI0706 14:58:00.474509 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_False): 0.07s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_True): 0.06s\r\nI0706 14:58:00.538034 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_True): 0.06s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_10_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_False): 0.01s\r\nI0706 14:58:00.552656 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_False): 0.01s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_True): 0.01s\r\nI0706 14:58:00.566967 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_True): 0.01s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_1_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_False\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_False): 0.03s\r\nI0706 14:58:00.599325 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_False): 0.03s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_False\r\n[ RUN      ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_True\r\nINFO:tensorflow:time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_True): 0.03s\r\nI0706 14:58:00.629496 140491261256320 test_util.py:2102] time(__main__.ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_True): 0.03s\r\n[       OK ] ConjugateGradientTest.test_ConjugateGradient__test_conjugate_gradientfloat64_4_staticshape_True\r\n[ RUN      ] ConjugateGradientTest.test_session\r\n[  SKIPPED ] ConjugateGradientTest.test_session\r\n----------------------------------------------------------------------\r\nRan 13 tests in 0.645s\r\n\r\nOK (skipped=1)\r\n```\r\n\r\n</details>\r\n\r\nThe Internal CI is failing with\r\n```\r\nE   ModuleNotFoundError: No module named 'tensorflow.tools.api'\r\n```\r\nThis is because an `__init__.py` file is missing in that directory.  This should also be in a separate changelist.", "@gbaned Am I misinterpreting the errors?  What can I do to help this along?", "@kkimdev Can you please assist on above comments from @NeilGirdhar. Thanks!\r\n", "@NeilGirdhar It is not so easy to reproduce exactly the Pylint CI environment locally. See my comment at https://github.com/tensorflow/tensorflow/pull/50133#issuecomment-856329703 about using the official TF devel docker image for the pylint github action.", "@NeilGirdhar Can you try with `-float(np.e)`?", "Also please check we had some API changes in random for `numpy.int64`:\r\nhttps://source.cloud.google.com/results/invocations/9dec6c3d-bf74-4f8b-81d8-ffb6c1b4600a/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/log", "@bhack sure I'll take a look now", "Also please note that for the current Github linting action we don't set specific a specific numpy version so it is always running with the last stable available version (currently `numpy 1.21.0`):\r\nhttps://github.com/tensorflow/tensorflow/blob/5ce6132701108cb6550d947927bf6fd373130c2f/.github/workflows/pylint-presubmit.yml#L44", "> Also please check we had some API changes in random for numpy.int64\r\n\r\nYes, that makes sense.  Can we update the default?  Should we just ignore the API change since NumPy was always internally converting `int` to `np.int64` anyway.\r\n\r\n> @NeilGirdhar It is not so easy to reproduce exactly the Pylint CI environment locally. See my comment at #50133 (comment) about using the official TF devel docker image for the pylint github action.\r\n\r\nSorry, is this pointing to the right comment?  I'm not sure what I should do?  Sorry, this is new to me.\r\n\r\n> Also please note that for the current Github linting action we don't set specific a specific numpy version so it is always running with the last stable available version (currently numpy 1.21.0):\r\n\r\nGreat, I'm running that too!", "> @NeilGirdhar Can you try with -float(np.e)?\r\n\r\nAs soon I know how to test things locally, I'm happy to try that.  It's weird that `-np.e` is failing, but `-np.euler_gamma` in the same file passes.  Also, that doesn't seem like a great solution.  It might be better to `# pylint: disable=` the problem until it's fixed by `NumPy` or `pylint`--not sure who's to blame.", "> > @NeilGirdhar Can you try with -float(np.e)?\r\n> \r\n> As soon I know how to test things locally, I'm happy to try that. It's weird that `-np.e` is failing, but `-np.euler_gamma` in the same file passes. Also, that doesn't seem like a great solution. It might be better to `# pylint: disable=` the problem until it's fixed by `NumPy` or `pylint`--not sure who's to blame.\r\n\r\nOk please add the inline ignore. \r\nAs I told you we don't have a single command to reproduce this locally as we are not running the pylint in the Docker image we distribute for developer or any other officially distributed Docker image.\r\n\r\nSo please commit and we will run CI linting here on Github.\r\n\r\nFor the API change I think we need a quick check from @tensorflow/api-owners  /cc @mihaimaruseac ", "> As I told you we don't have a single command to reproduce this locally as we are not running the pylint in the Docker image we distribute for developer or any other officially distributed Docker image.\r\n\r\nSorry, I didn't understand.  I don't have a lot of experience with Docker.\r\n\r\n> Ok please add the inline ignore.\r\n\r\nI will add the ignore and force push now.", "> Sorry, I didn't understand. I don't have a lot of experience with Docker.\r\n\r\nIn this context It is just a quick solution to have aligned environments (e.g. python , pylint, numpy versions etc..). \r\nIf we run `pylint` here with the Github Action in the same Docker container that you can run on your machine we are on the same page and we could expect exactly the same results.\r\n\r\n> I will add the ignore and force push now.\r\n\r\nOk please resolve also the conflict.", "(Done)", "OK now pylint has green light. \r\nThe problem is that we don't know what is fixed in 1.20 with this PR as the CI is not running with numpy 1.20.\r\nSee:\r\nhttps://github.com/tensorflow/tensorflow/pull/48935\r\nhttps://github.com/tensorflow/tensorflow/pull/48918\r\n", "@bhack Yes, I've been watching both of those PRs since they were submitted.  I'm not trying to make tensorflow compatible with 1.21 in this PR; I just want to resolve the warnings :smile:   I am looking forward to tensorflow eventually being compatible with 1.21 though.", "What I meant is that the tests that are running now in the CI are just checking what is going wrong with numpy < 1.20.\r\nThey don't tell us anything about  your PR changes with numpy>=1.20 that is what your a solving locally on your env.", "> What is the earliest version of numpy that has these symbols?\r\n\r\nLooking at https://github.com/numpy/numpy/blob/main/numpy/core/numerictypes.py, these types have been in NumPy for at least 16 years.\r\n\r\n> What is the compatibility story?\r\n\r\nGood question.  This change will have no effect on people running older versions of NumPy that are compatible with TensorFlow.\r\n\r\n> Will these get renamed again in a later numpy version?\r\n\r\nI think that's _extremely_ unlikely to happen in the next decade.  But in the unlikely event that it does happen, there will be at least a few years of deprecation time, after which a similar search and replace would fix everything again.", "Thank you. Just wanted to make sure."]}, {"number": 50526, "title": "Numpy Version", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 2.5, 2.6, etc\r\n- Python version: 3.9\r\n\r\nWhere can I find the nupmy version requirements for a Tensorflow build?\r\nPlease write requirements for numpy as you write for cuda.\r\n", "comments": ["@Expert73 \r\n\r\nCould you please refer the similar issue [#31249](https://github.com/tensorflow/tensorflow/issues/31249),also refer this [documentation](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform-release-notes/tf-jetson-rel.html) ,let us  know if it helps.Thanks", "Before doing a build from tensorflow source where can I find out the REQUIRED VERSION (technical requirements) of numpy?\r\nI am interested in the question of which version of numpy is compatible with a specific version of tensorflow.", "I would select the appropriate tag (version) in code view then check `tensorflow/tools/pip_package/setup.py` file for required packages.", "> I would select the appropriate tag (version) in code view\r\n\r\nwhat do you mean? how to do it?", "@Expert73 \r\n\r\nTensorFlow NumPy is built on top of TensorFlow and hence interoperates seamlessly with TensorFlow. Please refer this [documentation](https://www.tensorflow.org/guide/tf_numpy#tf_numpy_and_tensorflow) and let us know if it helps.Thanks\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "`pip install` is able to download the other dependencies as needed.\r\n\r\nWe cannot list the versions for all dependencies. We list CUDA as these are external and must be present on the host as there is no way the pip installer would be able to install them. But everything else is downloaded as needed.\r\n\r\nIf you want to see the version in a static way, without installing, then look at `tensorflow/tools/pip_package/setup.py`.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50526\">No</a>\n"]}, {"number": 50525, "title": "AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'", "body": "def input_parser(self, case, slice_num):\r\n\t# read image and select the desire slice\r\n\tcase = case.decode(\"utf-8\")\r\n\r\n\r\nFile \"D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 241, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 130, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"D:\\Vnet2\\NiftiDataset2D.py\", line 122, in input_parser\r\n    case = case.decode(\"utf-8\")\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\r\n\r\nI have encountered this trouble, I hope someone can help me, thank you very much", "comments": ["@liuzhiwei1997 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the tensorflow version,complete code and dataset to reproduce the issue reported here.Thanks!", "Also please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/29972#issuecomment-504591877) for the similar error.It helps.Thanks!", "tensorflow-gpu=2.1\r\ncode\uff1a\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\nimport SimpleITK as sitk\r\nimport tensorflow as [tf](url)\r\ntf = tf.compat.v1\r\ntf.disable_v2_behavior()\r\nimport numpy as np\r\nimport math\r\nimport random\r\nimport multiprocessing\r\nfrom tqdm import tqdm\r\nimport NiftiDataset3D\r\nimport threading\r\n\r\n\r\nclass NiftiDataset(object):\r\n\r\n\tdef __init__(self,\r\n\t\tdata_dir ='D:\\Vnet2\\data\\training',\r\n\t\timage_filenames ='',\r\n\t\tlabel_filename ='',\r\n\t\ttransforms3D=None,\r\n\t\ttransforms2D=True,\r\n\t\ttrain=True,\r\n\t\tlabels=(0,1),\r\n\t\tmin_pixel=5,\r\n\t\tdrop_ratio=0.1):\r\n\r\n\t\t# Init membership variables\r\n\t\tself.data_dir = data_dir\r\n\t\tself.image_filenames = image_filenames\r\n\t\tself.label_filename = label_filename\r\n\t\tself.transforms3D = transforms3D\r\n\t\tself.transforms2D = transforms2D\r\n\t\tself.train = train\r\n\t\tself.labels = labels\r\n\t\tself.min_pixel = min_pixel\r\n\t\tself.drop_ratio = drop_ratio\r\n\r\n\tdef read_image(self,path):\r\n\t\treader = sitk.ImageFileReader()\r\n\t\treader.SetFileName(path)\r\n\t\treturn reader.Execute()\r\n\r\n\tdef get_dataset(self):\r\n\t\tslices_list = []\r\n\r\n\t\t# read all images to generate the candidate slice list\r\n\t\tpbar = tqdm(os.listdir(self.data_dir))\r\n\r\n\t\tignore_files = [\r\n\t\t\t\".DS_Store\",\r\n\t\t\t\"@eaDir\"\r\n\t\t]\r\n\t\tfor case in pbar:\r\n\t\t\t#if case in ignore_files:\r\n\t\t\t\t#continue\r\n\t\t\tpbar.set_description(\"Loading {}...\".format(case))\r\n\r\n\t\t\tlabel = self.read_image(os.path.join(self.data_dir,case,self.label_filename))\r\n\r\n\t\t\tfor i in range(label.GetSize()[2]):\r\n\t\t\t\t# check if the slice contains label\r\n\t\t\t\textractor = sitk.ExtractImageFilter()\r\n\t\t\t\tsize = [label.GetSize()[0],label.GetSize()[1],0]\r\n\t\t\t\tindex = [0,0,i]\r\n\t\t\t\textractor.SetSize(size)\r\n\t\t\t\textractor.SetIndex(index)\r\n\t\t\t\tlabel_ = extractor.Execute(label)\r\n\r\n\t\t\t\tbinaryThresholdFilter = sitk.BinaryThresholdImageFilter()\r\n\t\t\t\tbinaryThresholdFilter.SetLowerThreshold(1)\r\n\t\t\t\tbinaryThresholdFilter.SetUpperThreshold(255)\r\n\t\t\t\tbinaryThresholdFilter.SetInsideValue(1)\r\n\t\t\t\tbinaryThresholdFilter.SetOutsideValue(0)\r\n\t\t\t\tlabel_ = binaryThresholdFilter.Execute(label_)\r\n\r\n\t\t\t\tstatFilter = sitk.StatisticsImageFilter()#\u5f52\u4e00\u5316\uff0c\u50cf\u7d20\u503c\u5728[0,1]\u4e4b\u95f4\r\n\t\t\t\tstatFilter.Execute(label_)\r\n\r\n\t\t\t\tif statFilter.GetSum() > 1:\r\n\t\t\t\t\tslices_list.append([case,i])\r\n\t\t\t\telse:\r\n\t\t\t\t\tslices_list.append([case,i])\r\n\t\t\t\t\tcontinue\r\n\r\n\t\t# randomize the slices\r\n\t\trandom.shuffle(slices_list)\r\n\t\tslices_list_1 = []\r\n\t\tslices_list_2 = []\r\n\r\n\r\n\r\n\t\tprint(\"slices_list:\", slices_list)\r\n\t\tfor value in slices_list:\r\n\t\t\tslices_list_1.append(value[0])\r\n\t\t\tslices_list_2.append(value[1])\r\n\t\tprint(\"slices_list_1:\", slices_list_1)\r\n\t\tprint(\"slices_list_2:\", slices_list_2)\r\n\r\n\t\tdataset = tf.data.Dataset.from_tensor_slices((slices_list_1,slices_list_2))#\u52a0\u8f7d\u5207\u7247\r\n\t\tdataset = dataset.map(lambda case, slice_num: tuple(tf.py_function(self.input_parser, [case, slice_num], [tf.float32,tf.int32])),num_parallel_calls=1)#\u9884\u5904\u7406,slice_num\u53c2\u6570\uff0ctuple\uff08\u5143\u7ec4\uff09\uff1a\u8fd4\u56de\u7684\u53c2\u6570\uff0c\r\n\t\t\t# num_parallel_calls=multiprocessing.cpu_count())#           \u81ea\u5b9a\u4e49\u51fd\u6570input_parser   \u51e0\u4e2atensor\u7ec4\u6210\u7684list  \u6570\u636e\u7c7b\u578b\u7ec4\u6210\u7684list       \u6570\u636e\u5e76\u884c\u5904\u7406\u7684\u7ea7\u522b\r\n\r\n\r\n\t\t#case_list = os.listdir(self.data_dir)\r\n\t\t#dataset = tf.data.Dataset.from_tensor_slices(case_list)\r\n\t\t#dataset = dataset.map(lambda case: tuple(tf.py_function(self.input_parser, [case], [tf.float32,tf.int32])),num_parallel_calls=multiprocessing.cpu_count())\r\n\r\n\t\tself.dataset = dataset\r\n\t\tself.data_size = len(slices_list)\r\n\r\n\t\tprint(\"len(slices_list):\", len(slices_list))\r\n\t\tprint(\"case:\",case)\r\n\r\n\t\treturn self.dataset\r\n\r\n\tdef input_parser(self, case, slice_num):\r\n\t\t#read image and select the desire slice\r\n\t\tcase = case.decode(\"utf-8\")\r\n\r\n\t\tslice_num = int(slice_num)\r\n\r\n\t\timage_paths = []\r\n\t\tfor channel in range(len(self.image_filenames)):\r\n\t\t\timage_paths.append(os.path.join(self.data_dir,case,self.image_filenames[channel]))\r\n\r\n\t\t# read images\r\n\t\timages = []\r\n\t\tfor channel in range(len(image_paths)):\r\n\t\t\timages.append(self.read_image(image_paths[channel]))\r\n\r\n\t\t# cast image\r\n\t\tfor channel in range(len(images)):\r\n\t\t\tcastImageFilter = sitk.CastImageFilter()\r\n\t\t\tcastImageFilter.SetOutputPixelType(sitk.sitkFloat32)\r\n\t\t\timages[channel] = castImageFilter.Execute(images[channel])\r\n\t\t\t# check header consistency\r\n\t\t\tsameSize = images[channel].GetSize() == images[0].GetSize()\r\n\t\t\tsameSpacing = images[channel].GetSpacing() == images[0].GetSpacing()\r\n\t\t\tsameDirection = images[channel].GetDirection() == images[0].GetDirection()\r\n\r\n\t\t\tif sameSize and sameSpacing and sameDirection:\r\n\t\t\t\tcontinue\r\n\t\t\telse:\r\n\t\t\t\traise Exception('Header info inconsistent: {}'.format(source_paths[channel]))\r\n\t\t\t\texit()\r\n\r\n\t\tlabel = sitk.Image(images[0].GetSize(), sitk.sitkInt32)\r\n\t\tlabel.SetOrigin(images[0].GetOrigin())\r\n\t\tlabel.SetSpacing(images[0].GetSpacing())\r\n\t\tlabel.SetDirection(images[0].GetDirection())\r\n\r\n\t\tif self.train:\r\n\t\t\tlabel_ = self.read_image(os.path.join(self.data_dir, case, self.label_filename))\r\n\r\n\t\t\t# check header consistency\r\n\t\t\tsameSize = label_.GetSize() == images[0].GetSize()\r\n\t\t\tsameSpacing = label_.GetSpacing() == images[0].GetSpacing()\r\n\t\t\tsameDirection = label_.GetDirection() == images[0].GetDirection()\r\n\t\t\tif not (sameSize and sameSpacing and sameDirection):\r\n\t\t\t\traise Exception('Header info inconsistent: {}'.format(os.path.join(self.data_dir,case, self.label_filename)))\r\n\t\t\t\texit()\r\n\r\n\t\t\tfor channel in range(len(self.labels)):\r\n\t\t\t\tthresholdFilter = sitk.BinaryThresholdImageFilter()\r\n\t\t\t\tthresholdFilter.SetOutsideValue(0)\r\n\t\t\t\tthresholdFilter.SetInsideValue(1)\r\n\t\t\t\tthresholdFilter.SetLowerThreshold(self.labels[channel])\r\n\t\t\t\tthresholdFilter.SetUpperThreshold(self.labels[channel])\r\n\t\t\t\tone_hot_label_image = thresholdFilter.Execute(label_)\r\n\t\t\t\tmultiFilter = sitk.MultiplyImageFilter()\r\n\t\t\t\tone_hot_label_image = multiFilter.Execute(one_hot_label_image, channel)\r\n\t\t\t\t# cast one_hot_label to sitkInt32\r\n\t\t\t\tcastImageFilter = sitk.CastImageFilter()\r\n\t\t\t\tcastImageFilter.SetOutputPixelType(sitk.sitkInt32)\r\n\t\t\t\tone_hot_label_image = castImageFilter.Execute(one_hot_label_image)\r\n\t\t\t\tone_hot_label_image.SetSpacing(images[0].GetSpacing())\r\n\t\t\t\tone_hot_label_image.SetDirection(images[0].GetDirection())\r\n\t\t\t\tone_hot_label_image.SetOrigin(images[0].GetOrigin())\r\n\t\t\t\taddFilter = sitk.AddImageFilter()\r\n\t\t\t\tlabel = addFilter.Execute(label,one_hot_label_image)\r\n\r\n\t\tsample = {'image':images, 'label':label}\r\n\r\n\t\tif self.transforms3D:\r\n\t\t\tfor transform in self.transforms3D:\r\n\t\t\t\ttry:\r\n\t\t\t\t\t# print(case, transform.name)\r\n\t\t\t\t\tsample =transform(sample)\r\n\t\t\t\t\t# print(case, transform.name,\"3d transform complete\")\r\n\t\t\t\texcept:\r\n\t\t\t\t\tprint(\"Dataset preprocessing error: {}\".format(os.path.dirname(image_paths[0])))\r\n\t\t\t\t\texit()\r\n\r\n\t\t# extract the desire slice\r\n\t\timages = sample['image']\r\n\t\tlabel = sample['label']\r\n\r\n\t\tsize = [images[0].GetSize()[0],images[0].GetSize()[1],0]\r\n\t\tindex = [0,0,int(slice_num)]\r\n\t\tfor channel in range(len(images)):\r\n\t\t\textractor = sitk.ExtractImageFilter()\r\n\t\t\textractor.SetSize(size)\r\n\t\t\textractor.SetIndex(index)\r\n\t\t\timages[channel] = extractor.Execute(images[channel])\r\n\r\n\t\textractor = sitk.ExtractImageFilter()\r\n\t\textractor.SetSize(size)\r\n\t\textractor.SetIndex(index)\r\n\t\tlabel = extractor.Execute(label)\r\n\r\n\t\tsample = {'image':images, 'label':label}\r\n\r\n\t\tif self.transforms2D:\r\n\t\t\tfor transform in self.transforms2D:\r\n\t\t\t\ttry:\r\n\t\t\t\t\t# print(case, transform.name)\r\n\t\t\t\t\tsample = transform(sample)\r\n\t\t\t\t\t# print(case, transform.name,\"2d transform complete\")\r\n\t\t\t\texcept:\r\n\t\t\t\t\tprint(\"Dataset preprocessing error: {}\".format(os.path.dirname(image_paths[0])))\r\n\t\t\t\t\texit()\r\n\r\n\t\t# convert sample to tf tensors\r\n\t\tfor channel in range(len(sample['image'])):\r\n\t\t\timage_np_ = sitk.GetArrayFromImage(sample['image'][channel])\r\n\t\t\timage_np_ = np.asarray(image_np_,np.float32)\r\n\t\t\tif channel == 0:\r\n\t\t\t\timage_np = image_np_[:,:,np.newaxis]\r\n\t\t\telse:\r\n\t\t\t\timage_np = np.append(image_np,image_np_[:,:,np.newaxis],axis=-1)\r\n\r\n\t\tlabel_np = sitk.GetArrayFromImage(sample['label'])\r\n\t\tlabel_np = np.asarray(label_np,np.int32)\r\n\r\n\t\t# print(case, \"convert sitk image to np array complete\")\r\n\r\n\t\treturn image_np, label_np\r\n\r\nclass ManualNormalization(object):\r\n\t\"\"\"\r\n\tNormalize an image by mapping intensity with given max and min window level\r\n\t\"\"\"\r\n\r\n\tdef __init__(self,windowMin, windowMax):\r\n\t\tself.name = 'ManualNormalization'\r\n\t\tassert isinstance(windowMax, (int,float))\r\n\t\tassert isinstance(windowMin, (int,float))\r\n\t\tself.windowMax = float(windowMax)\r\n\t\tself.windowMin = float(windowMin)\r\n\r\n\tdef __call__(self, sample):\r\n\t\timage, label = sample['image'], sample['label']\r\n\t\t\r\n\t\tfor channel in range(len(image)):\r\n\t\t\tintensityWindowingFilter = sitk.IntensityWindowingImageFilter()\r\n\t\t\tintensityWindowingFilter.SetOutputMaximum(255)\r\n\t\t\tintensityWindowingFilter.SetOutputMinimum(0)\r\n\t\t\tintensityWindowingFilter.SetWindowMaximum(self.windowMax);\r\n\t\t\tintensityWindowingFilter.SetWindowMinimum(self.windowMin);\r\n\t\t\timage[channel] = intensityWindowingFilter.Execute(image[channel])\r\n\r\n\t\treturn {'image': image, 'label': label}\r\n\r\nclass Resample(object):\r\n\t\"\"\"\r\n\tResample the volume in a sample to a given voxel size\r\n\r\n\tArgs:\r\n\t\tvoxel_size (float or tuple): Desired output size.\r\n\t\tIf float, output volume is isotropic.\r\n\t\tIf tuple, output voxel size is matched with voxel size\r\n\t\tCurrently only support linear interpolation method\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, voxel_size):\r\n\t\tself.name = 'Resample'\r\n\r\n\t\tassert isinstance(voxel_size, (int, float, tuple, list))\r\n\t\tif isinstance(voxel_size, float):\r\n\t\t\tself.voxel_size = (voxel_size, voxel_size)\r\n\t\telse:\r\n\t\t\tassert len(voxel_size) == 2\r\n\t\t\tself.voxel_size = voxel_size\r\n\r\n\tdef __call__(self, sample):\r\n\t\timage, label = sample['image'], sample['label']\r\n\r\n\t\tfor image_channel in range(len(image)):\r\n\t\t\told_spacing = image[image_channel].GetSpacing()\r\n\t\t\told_size = image[image_channel].GetSize()\r\n\r\n\t\t\tnew_spacing = self.voxel_size\r\n\r\n\t\t\tnew_size = []\r\n\t\t\tfor i in range(2):\r\n\t\t\t\tnew_size.append(int(math.ceil(old_spacing[i]*old_size[i]/new_spacing[i])))\r\n\t\t\tnew_size = tuple(new_size)\r\n\r\n\t\t\tresampler = sitk.ResampleImageFilter()\r\n\t\t\tresampler.SetInterpolator(sitk.sitkLinear)\r\n\t\t\tresampler.SetOutputSpacing(new_spacing)\r\n\t\t\tresampler.SetSize(new_size)\r\n\r\n\t\t\t# resample on image\r\n\t\t\tresampler.SetOutputOrigin(image[image_channel].GetOrigin())\r\n\t\t\tresampler.SetOutputDirection(image[image_channel].GetDirection())\r\n\t\t\t# print(\"Resampling image...\")\r\n\t\t\timage[image_channel] = resampler.Execute(image[image_channel])\r\n\r\n\t\t# resample on segmentation\r\n\t\tresampler = sitk.ResampleImageFilter()\r\n\t\tresampler.SetInterpolator(sitk.sitkLinear)\r\n\t\tresampler.SetOutputSpacing(new_spacing)\r\n\t\tresampler.SetSize(new_size)\r\n\t\tresampler.SetInterpolator(sitk.sitkNearestNeighbor)\r\n\t\tresampler.SetOutputOrigin(label.GetOrigin())\r\n\t\tresampler.SetOutputDirection(label.GetDirection())\r\n\t\t# print(\"Resampling segmentation...\")\r\n\t\tlabel = resampler.Execute(label)\r\n\r\n\t\treturn {'image': image, 'label': label}\r\n\r\nclass Padding(object):\r\n\t\"\"\"\r\n\tAdd padding to the image if size is smaller than patch size\r\n\r\n\tArgs:\r\n\t\toutput_size (tuple or int): Desired output size. If int, a cubic volume is formed\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, output_size):\r\n\t\tself.name = 'Padding'\r\n\r\n\t\tassert isinstance(output_size, (int, tuple, list))\r\n\t\tif isinstance(output_size, int):\r\n\t\t\tself.output_size = (output_size, output_size, output_size)\r\n\t\telse:\r\n\t\t\tassert len(output_size) == 2\r\n\t\t\tself.output_size = output_size\r\n\r\n\t\tassert all(i > 0 for i in list(self.output_size))\r\n\r\n\tdef __call__(self,sample):\r\n\t\timage, label = sample['image'], sample['label']\r\n\r\n\t\tsize_old = image[0].GetSize()\r\n\r\n\t\tif (size_old[0] >= self.output_size[0]) and (size_old[1] >= self.output_size[1]):\r\n\t\t\treturn sample\r\n\t\telse:\r\n\t\t\toutput_size = list(self.output_size)\r\n\t\t\tif size_old[0] > self.output_size[0]:\r\n\t\t\t\toutput_size[0] = size_old[0]\r\n\t\t\tif size_old[1] > self.output_size[1]:\r\n\t\t\t\toutput_size[1] = size_old[1]\r\n\r\n\t\t\toutput_size = tuple(output_size)\r\n\r\n\t\t\tfor image_channel in range(len(image)):\r\n\t\t\t\tresampler = sitk.ResampleImageFilter()\r\n\t\t\t\tresampler.SetOutputSpacing(image[image_channel].GetSpacing())\r\n\t\t\t\tresampler.SetSize(output_size)\r\n\r\n\t\t\t\t# resample on image\r\n\t\t\t\tresampler.SetInterpolator(sitk.sitkLinear)\r\n\t\t\t\tresampler.SetOutputOrigin(image[image_channel].GetOrigin())\r\n\t\t\t\tresampler.SetOutputDirection(image[image_channel].GetDirection())\r\n\t\t\t\timage[image_channel] = resampler.Execute(image[image_channel])\r\n\r\n\t\t\t# resample on label\r\n\t\t\tresampler = sitk.ResampleImageFilter()\r\n\t\t\tresampler.SetOutputSpacing(image[image_channel].GetSpacing())\r\n\t\t\tresampler.SetSize(output_size)\r\n\t\t\tresampler.SetInterpolator(sitk.sitkNearestNeighbor)\r\n\t\t\tresampler.SetOutputOrigin(label.GetOrigin())\r\n\t\t\tresampler.SetOutputDirection(label.GetDirection())\r\n\r\n\t\t\tlabel = resampler.Execute(label)\r\n\r\n\t\t\treturn {'image': image, 'label': label}\r\n\r\nclass RandomCrop(object):\r\n\t\"\"\"\r\n\tCrop randomly the image in a sample. This is usually used for data augmentation.\r\n\tDrop ratio is implemented for randomly dropout crops with empty label. (Default to be 0.2)\r\n\tThis transformation only applicable in train mode\r\n\r\n\tArgs:\r\n\toutput_size (tuple or int): Desired output size. If int, cubic crop is made.\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, output_size, drop_ratio=0.1, min_pixel=1):\r\n\t\tself.name = 'Random Crop'\r\n\r\n\t\tassert isinstance(output_size, (int, tuple, list))\r\n\t\tif isinstance(output_size, int):\r\n\t\t\tself.output_size = (output_size, output_size, output_size)\r\n\t\telse:\r\n\t\t\tassert len(output_size) == 2\r\n\t\t\tself.output_size = output_size\r\n\r\n\t\tassert isinstance(drop_ratio, (int,float))\r\n\t\tif drop_ratio >=0 and drop_ratio<=1:\r\n\t\t\tself.drop_ratio = drop_ratio\r\n\t\telse:\r\n\t\t\traise RuntimeError('Drop ratio should be between 0 and 1')\r\n\r\n\t\tassert isinstance(min_pixel, int)\r\n\t\tif min_pixel >=0 :\r\n\t\t\tself.min_pixel = min_pixel\r\n\t\telse:\r\n\t\t\traise RuntimeError('Min label pixel count should be integer larger than 0')\r\n\r\n\tdef __call__(self,sample):\r\n\t\timage, label = sample['image'], sample['label']\r\n\t\tsize_old = image[0].GetSize()\r\n\t\tsize_new = self.output_size\r\n\r\n\t\tcontain_label = False\r\n\r\n\t\troiFilter = sitk.RegionOfInterestImageFilter()\r\n\t\troiFilter.SetSize([size_new[0],size_new[1]])\r\n\r\n\t\t# statFilter = sitk.StatisticsImageFilter()\r\n\t\t# statFilter.Execute(label)\r\n\t\t# print(statFilter.GetMaximum(), statFilter.GetSum())\r\n\r\n\t\tbinaryThresholdFilter = sitk.BinaryThresholdImageFilter()\r\n\t\tbinaryThresholdFilter.SetLowerThreshold(1)\r\n\t\tbinaryThresholdFilter.SetUpperThreshold(255)\r\n\t\tbinaryThresholdFilter.SetInsideValue(1)\r\n\t\tbinaryThresholdFilter.SetOutsideValue(0)\r\n\t\tlabel_ = binaryThresholdFilter.Execute(label)\r\n\r\n\t\t# check if the whole slice contain label > minimum pixel\r\n\t\tstatFilter = sitk.StatisticsImageFilter()\r\n\t\tstatFilter.Execute(label_)\r\n\t\tif statFilter.GetSum() < self.min_pixel:\r\n\t\t\tcontain_label = True\r\n\r\n\t\twhile not contain_label: \r\n\t\t\t# get the start crop coordinate in ijk\r\n\t\t\tif size_old[0] <= size_new[0]:\r\n\t\t\t\tstart_i = 0\r\n\t\t\telse:\r\n\t\t\t\tstart_i = np.random.randint(0, size_old[0]-size_new[0])\r\n\r\n\t\t\tif size_old[1] <= size_new[1]:\r\n\t\t\t\tstart_j = 0\r\n\t\t\telse:\r\n\t\t\t\tstart_j = np.random.randint(0, size_old[1]-size_new[1])\r\n\r\n\t\t\troiFilter.SetIndex([start_i,start_j])\r\n\r\n\t\t\tlabel_crop = roiFilter.Execute(label_)\r\n\r\n\t\t\tstatFilter.Execute(label_crop)\r\n\r\n\t\t\t# will iterate until a sub volume containing label is extracted\r\n\t\t\t# pixel_count = seg_crop.GetHeight()*seg_crop.GetWidth()*seg_crop.GetDepth()\r\n\t\t\t# if statFilter.GetSum()/pixel_count<self.min_ratio:\r\n\t\t\tif statFilter.GetSum()<self.min_pixel:\r\n\t\t\t\tcontain_label = self.drop(self.drop_ratio) # has some probabilty to contain patch with empty label\r\n\t\t\telse:\r\n\t\t\t\tcontain_label = True\r\n\r\n\t\tfor image_channel in range(len(image)):\r\n\t\t\timage[image_channel] = roiFilter.Execute(image[image_channel])\r\n\t\tlabel = roiFilter.Execute(label)\r\n\r\n\t\treturn {'image': image, 'label': label}\r\n\r\n\tdef drop(self,probability):\r\n\t\treturn random.random() <= probability\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"D:\\anaconda\\envs\\tf21\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 234, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"D:\\anaconda\\envs\\tf21\\lib\\site-packages\\tensorflow_core\\python\\ops\\script_ops.py\", line 123, in __call__\r\n    ret = self._func(*args)\r\n\r\n  File \"D:\\Vnet2\\NiftiDataset2D.py\", line 120, in input_parser\r\n    case = case.decode(\"utf-8\")\r\n\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'", "@liuzhiwei1997 ,\r\n\r\nCan you please try executing the code in latest tf v2.5 and let us know if you are facing same issue.Also the code provided is fairly complex hence it would be difficult for us to pinpoint the issue. Could you please get the example down to the simplest possible repro? That will allow us to determine the source of the issue easily. Thanks!", "Can you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/29972#issuecomment-504591877) for the similar error.It helps.Thanks!\r\n", "I still don\u2019t know how to solve this problem, can I talk about it in detail", "@liuzhiwei1997 ,\r\n\r\nIf you cannot simplify the code, please consider posting this issue in Stackoverflow where there is a large community to help and support each other. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50525\">No</a>\n"]}, {"number": 50524, "title": "Does tf.data.Dataset.from_tensor_slices() and tf.cast() creates extra memory", "body": "Suppose \"imageX\", \"imageY\" are numpy arrays of size 20GB, is\r\ndataset = tf.data.Dataset.from_tensor_slices((imageX, imageY))\r\ncreating an extra dataset using 40GB memory, or it will re-use the numpy array memory occupied by \"imageX\" and \"imageY\"?\r\n\r\nAlso, does tf.cast() create extra memory or re-use exist numpy memory?", "comments": ["@llodds \r\n it creates a dataset that on-the-fly consumes numpy arrays and provides a batch at a time. It doesn't increase memory\r\nThat is the goal of dataset.\r\n\r\nKindly create Stackoverflow/Tensorflow discussion group and move this to closed status.", "Thanks for confirming this!"]}, {"number": 50523, "title": "remove unnecessary cudaFree(nullptr) call", "body": "Corresponding issue: https://github.com/tensorflow/tensorflow/issues/50232\r\nAs commented here: https://github.com/tensorflow/tensorflow/issues/50232#issuecomment-862034257, this cudaFree call could be unnecessary. \r\n\r\nThe behavior of `cudaSetDevice` remains for CUDA version 9+.\r\n\r\n[gpu_device_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device_test.cc) contains comprehensive test cases to prevent regression, so not writing unit tests here.", "comments": ["@sanjoy mind taking a look?"]}, {"number": 50522, "title": "[Colab] [TF2.5] Reshaping images and labels after augmentation pipeline generates error during training", "body": "**System information**\r\n- Have I written custom code: \r\n  - Adapting this [tutorial](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=LtAVr-4CP1rp) to run over custom implementation of model training example.\r\n- OS Platform and Distribution:\r\n  - Google colab enviroment (Tensorflow 2.5.0, **TPU & GPU backends**)\r\n- Python version:\r\n  - Python 3.7.10\r\n\r\n### I have a tf.data pipeline (adapted to run on tpu according to the tutorial mentioned above).\r\n\r\n### After parsing the tfrecord, it basically maps to an augmentation pipeline (model) that produces 32 augmented versions of every input image. Then it tile the labels and reshape the data (transform it into batches). \r\n\r\nTraining runs fine untill  step 249 from first epoch when reshape generates an error. \r\nThe \"Requested shape\" is equal to 1233125376 which corresponds to batch_size x num_generated_images (augmentation) x image_dimensions, namely: (32x8 x 32 x 224 x 224 x 3), whereas the input tensor which is equal to 125239296, actually corresponds to (26 x 32 x 224 x 224 x 3)  i don't have a clue why this is happening.  \r\n\r\n### Error:\r\n\r\n```python\r\nEpoch 1/30\r\n 249/2007 [==>...........................] - ETA: 32:46 - loss: 14.8993 - class_1_loss: 5.9974 - class_2_loss: 5.1770 - class_3_loss: 3.7249 - class_1_categorical_accuracy: 0.0472 - class_2_categorical_accuracy: 0.0721 - class_3_categorical_accuracy: 0.1524\r\n\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-23-c13881470e34> in <module>()\r\n      4 \r\n      5 resnet_50V2.summary()\r\n----> 6 history = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)\r\n      7 plot_training_history3(history)\r\n      8 \r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_214827}} Input to reshape is a tensor with 125239296 values, but the requested shape has 1233125376\r\n\t [[{{node Reshape}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_214827}} Input to reshape is a tensor with 125239296 values, but the requested shape has 1233125376\r\n\t [[{{node Reshape}}]]\r\n\t [[MultiDeviceIteratorGetNextFromShard]]\r\n\t [[RemoteCall]]\r\n\t [[IteratorGetNext]]\r\n\t [[cluster_train_function/_execute_2_0/_59]]\r\n0 successful operations.\r\n7 derived errors ignored.\r\n```\r\n``` python\r\ndef train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, model):\r\n  data_augmentation = preprocessing_model()\r\n\r\n  train_filenames = get_filenamesTPU(train_path)\r\n  random.shuffle(train_filenames\r\n\r\n  validation_filenames = get_filenamesTPU(validation_path)\r\n  random.shuffle(validation_filenames)\r\n\r\n  dataset_length = 91758  \r\n  train_size =  dataset_length * 0.7\r\n  validation_size = dataset_length - train_size\r\n\r\n  batch_size = 32 * tpu_strategy.num_replicas_in_sync # (32 x 8)\r\n  \r\n  tpu_arg_0 = 32 * batch_size \r\n  \r\n  data_reshape = lambda x,y: (tf.reshape(x,shape=(tpu_arg_0,224,224,3)), (tf.reshape(y[0],shape=(tpu_arg_0,1000)), tf.reshape(y[1],shape=(tpu_arg_0,516)),tf.reshape(y[2],shape=(tpu_arg_0,124))))\r\n  \r\n  augmentation_pipeline = lambda x,y: (data_augmentation(tf.expand_dims(x,axis=0)),(tf.tile(tf.reshape(y[0],[1,1000]),[32,1]),tf.tile(tf.reshape(y[1],[1,516]),[32,1]),tf.tile(tf.reshape(y[2],[1,124]),[32,1])))\r\n  \r\n  AUTO = tf.data.AUTOTUNE\r\n  train_dataset = tf.data.TFRecordDataset(buffer_size=int(1e+8),num_parallel_reads=AUTO,filenames=train_filenames).map(parsing_fn,num_parallel_calls=AUTO).shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\r\n  train_dataset = train_dataset.map(augmentation_pipeline, num_parallel_calls=AUTO).batch(batch_size)\r\n  train_dataset = train_dataset.map(data_reshape,num_parallel_calls=AUTO)\r\n  train_dataset = train_dataset.repeat()\r\n  train_dataset = train_dataset.prefetch(AUTO)\r\n\r\n  # Create a validation dataset\r\n  validation_dataset = tf.data.TFRecordDataset(num_parallel_reads=AUTO,filenames=validation_filenames).map(parsing_fn,num_parallel_calls=AUTO)\r\n  validation_dataset = validation_dataset.map(augmentation_pipeline,num_parallel_calls=AUTO).batch(batch_size).map(data_reshape,num_parallel_calls=AUTO)\r\n  validation_dataset = validation_dataset.prefetch(AUTO)\r\n  validation_dataset = validation_dataset.repeat(1)\r\n\r\n  validation_steps = validation_size / batch_size\r\n  history = model.fit(x=train_dataset,\r\n                          epochs=epochs,\r\n                          steps_per_epoch=steps_per_epoch,                        \r\n                          validation_data=validation_dataset,\r\n                          validation_steps=validation_steps)\r\n  return history\r\n\r\n```\r\n\r\n### Training fn:\r\n\r\n```python\r\nloss={\r\n        \"class1\": 'CategoricalCrossentropy',\r\n        \"class2\": 'CategoricalCrossentropy',\r\n        \"class3\": 'CategoricalCrossentropy',\r\n},\r\n\r\nmetrics = ['categorical_accuracy']\r\noptimizer = Adam(lr=5e-3)\r\n\r\nweight_file = None\r\nwith tpu_strategy.scope():\r\n  resnet_50V2 = load_and_configure_model(optimizer, loss, metrics, weight_file)\r\n\r\nbase_directory = 'gs://2015_tfrecords'\r\n\r\ntrain_path = base_directory+'/train/'\r\nvalidation_path = base_directory+'/validation/'\r\nbuffer_size = 10240\r\nepochs = 30\r\nsteps_per_epoch = 2007\r\n\r\nresnet_50V2.summary()\r\nhistory = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)\r\nplot_training_history3(history)\r\n```\r\n\r\n\r\nI also accept suggestions on how to vectorize the data augmentation pipeline, since when i input a batch of images instead of a single tensor, it runs on parallel making the generated images come in a random sequence, hence making tiling the labels unfeasible.", "comments": ["Tried removing prefetch(AUTO) as I thought that it maybe was triggering this error, due to dinamic changes on the number of prefetched images but samething happened.", "Modified the code to run on GPU and produced the same error on the end of training routine:\r\n\r\n```python\r\ndef train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, model):\r\n  data_augmentation = preprocessing_model()\r\n\r\n  train_filenames = get_filenames(train_path)\r\n  random.shuffle(train_filenames)\r\n\r\n  validation_filenames = get_filenames(validation_path)\r\n  random.shuffle(validation_filenames)\r\n\r\n  dataset_length = 91758  \r\n  train_size =  dataset_length * 0.7\r\n  validation_size = dataset_length - train_size\r\n\r\n  batch_size = 18\r\n\r\n  data_reshape = lambda x,y: (tf.reshape(x,shape=(18*32,224,224,3)), (tf.reshape(y[0],shape=(18*32,1000)), tf.reshape(y[1],shape=(18*32,516)),tf.reshape(y[2],shape=(18*32,124))))\r\n  augmentation_pipeline = lambda x,y: (data_augmentation(tf.expand_dims(x,axis=0)),(tf.tile(tf.reshape(y[0],[1,1000]),[32,1]),tf.tile(tf.reshape(y[1],[1,516]),[32,1]),tf.tile(tf.reshape(y[2],[1,124]),[32,1])))\r\n  \r\n  AUTO = tf.data.AUTOTUNE\r\n  train_dataset = tf.data.TFRecordDataset(buffer_size=int(1e+8),num_parallel_reads=AUTO,filenames=train_filenames).map(parsing_fn,num_parallel_calls=AUTO).shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\r\n  train_dataset = train_dataset.map(augmentation_pipeline, num_parallel_calls=AUTO).batch(batch_size)\r\n  train_dataset = train_dataset.map(data_reshape,num_parallel_calls=AUTO)#.cache('/cache/train_cache')\r\n  train_dataset = train_dataset.repeat()\r\n  train_dataset = train_dataset.prefetch(AUTO)\r\n\r\n  # Create a validation dataset\r\n  validation_dataset = tf.data.TFRecordDataset(num_parallel_reads=AUTO,filenames=validation_filenames).map(parsing_fn,num_parallel_calls=AUTO)\r\n  validation_dataset = validation_dataset.map(augmentation_pipeline,num_parallel_calls=AUTO).batch(batch_size).map(data_reshape,num_parallel_calls=AUTO)\r\n  validation_dataset = validation_dataset.prefetch(AUTO)\r\n  validation_dataset = validation_dataset.repeat(1)\r\n\r\n  validation_steps = validation_size / batch_size # \"This ensures that the same validation samples are used every time\"\r\n  history = model.fit(x=train_dataset,\r\n                          epochs=epochs,\r\n                          steps_per_epoch=steps_per_epoch,                        \r\n                          validation_data=validation_dataset,\r\n                          validation_steps=validation_steps)\r\n  return history\r\n```\r\n```python\r\nloss={\r\n        \"class1\": 'CategoricalCrossentropy',\r\n        \"class2\": 'CategoricalCrossentropy',\r\n        \"class3\": 'CategoricalCrossentropy',\r\n},\r\n\r\nmetrics = ['categorical_accuracy']\r\noptimizer = Adam(lr=5e-3)\r\n\r\nweight_file = None\r\nwith tpu_strategy.scope():\r\n  resnet_50V2 = load_and_configure_model(optimizer, loss, metrics, weight_file)\r\n\r\ntrain_path = base_directory+'/train/'\r\nvalidation_path = base_directory+'/validation/'\r\nbuffer_size = 10240\r\nepochs = 30\r\nsteps_per_epoch = 3569\r\n\r\nresnet_50V2.summary()\r\nhistory = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)\r\nplot_training_history3(history)\r\n```\r\n### Output:\r\n```\r\nTotal params: 26,925,160\r\nTrainable params: 7,825,000\r\nNon-trainable params: 19,100,160\r\n__________________________________________________________________________________________________\r\nEpoch 1/30\r\n3542/3569 [============================>.] - ETA: 1:06 - loss: 12.9579 - class_1_loss: 5.2126 - class_2_loss: 4.5014 - class_3_loss: 3.2439 - class_1_categorical_accuracy: 0.1300 - class_2_categorical_accuracy: 0.1560 - class_3_categorical_accuracy: 0.2230\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Input to reshape is a tensor with 55552 values, but the requested shape has 71424\r\n\t [[{{node Reshape_3}}]]\r\n\t [[IteratorGetNext]]\r\n  (1) Invalid argument:  Input to reshape is a tensor with 55552 values, but the requested shape has 71424\r\n\t [[{{node Reshape_3}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[IteratorGetNext/_8]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_15021]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n\r\n```", "After further investigation i found that is related to the issue #43094, although i accept suggestions on how to deal with a fixed batch_size, more details: https://stackoverflow.com/questions/68201815/unexpected-behavior-of-tensorflow-tfrecorddataset-batch.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50522\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50522\">No</a>\n"]}, {"number": 50520, "title": "[CherryPick:r2.6][lite] Update release notes to mention - New builtin ops for variables. - Converter:   * To enable resource through conversion, set experimental_enable_resource_variables = True.   * TOCO / Old converter has been deprecated for few releases. It will go away in the coming release.", "body": "PiperOrigin-RevId: 381523046\nChange-Id: Ie155ad723d9f5d41152e881f4e897d2deb8e4503", "comments": ["Integrated in #50469"]}, {"number": 50519, "title": "GPU Symmetric Fake Quantization", "body": "Extension of PR #48580 which adds GPU unit tests for symmetric fake quantization.\r\n\r\nAttn: @Xhark.", "comments": ["Looks like this has been rolled back again in e4f883065966d41ce6e04061100c3c5d8e0eec7a.", "@Xhark do you have any information on why this has been rolled back in e4f883065966d41ce6e04061100c3c5d8e0eec7a?", "@philipphack Do you mind reopening the PR as it looks like it has been rolled back in e4f883065966d41ce6e04061100c3c5d8e0eec7a?", "Hi @lgeiger, It was rollbacked because it broken some internal tests. sorry for the late notice. I'm trying to find a way to handle that. Thanks.", "> It was rollbacked because it broken some internal tests. sorry for the late notice. I'm trying to find a way to handle that.\r\n\r\nThanks for looking into it \ud83d\udc4d "]}, {"number": 50518, "title": "Label_image always giving same outputs", "body": "Hi all !\r\n\r\nWhile testing our models with basic [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) example, we faced with strange results.\r\n\r\nWe tried _label_image_ with 2 different YOLO models with their corresponding label.txt. However regardless the input images and labels, the test output is always in the same order as [2 3 0 1]. Finally we also gave the Grace Hopper image from official source and still we faced with same issue. You can see the outputs in the image below.\r\n\r\nNormally our models are working smoothly on PC. But, it is important for us to test them in our iMX6 device with this basic sample. Do you have any idea or solution about this problem ?\r\nThank you in advance.\r\n\r\n![image](https://user-images.githubusercontent.com/56031118/123631875-8b6dac00-d817-11eb-8ba0-17047e522da4.png)\r\n\r\n", "comments": ["Can you shows the input and output description of your models?\r\nNote that label_image example assumes there is one output and that is the probabilities of each labels.", "> Can you shows the input and output description of your models?\r\n\r\nInput and output are defined as below in our YOLO model.\r\n![image](https://user-images.githubusercontent.com/56031118/123957548-39aa5a80-d9ac-11eb-914e-9a0b87420b30.jpg)\r\n\r\n\r\n> Note that label_image example assumes there is one output and that is the probabilities of each labels.\r\n\r\nNormally in terms of classification our model performs well on our tests in Colab, but basicly we also want to see how it works on label_image sample. \r\nWhen we try label_image with MobileNet model and grace hopper image, it works properly. So we want to figure out what we are doing wrong in our model...\r\n\r\n", "`Normally in terms of classification our model performs well on our tests in Colab`\r\nDid you run it with Tensorflow or Tensorflow Lite in Colab?", "> Did you run it with Tensorflow or Tensorflow Lite in Colab?\r\n\r\nWe run it with Tensorflow.\r\n", "Could you try to run the TFLite model in Colab and check its result there.\r\n\r\nThe reason is you fully understand and control the input/output processing there (since you have the code working for the TF model) so it will be a good start to debug it there.\r\n\r\nIn my experience, most of these unexpected accuracy problems is in input/output handling.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50518\">No</a>\n"]}, {"number": 50517, "title": "tensorflow2.0.0 compatibility with scipy ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["i'm using tensorflow 2.0.0 what version of scipy should i install ", "@saida11000 ,\r\n\r\nCan you please try to install 1.4.x version for tf v2.0.Also please take a look at the issue link for more information.[Link1](https://github.com/tensorflow/tensorflow/issues/35709).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50517\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50517\">No</a>\n"]}, {"number": 50516, "title": "Recover callback history when training is interrupted", "body": "I could not find a solution to my demand, so I believe it should be a feature to be implemented.\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI was wondering if it would be possible to checkpoint and recover the callback history, so that my callbacks can continue whatever they were tracking when training is interrupted for any reason.\r\n\r\n**Will this change the current api? How?**\r\nI don't know.\r\n\r\n**Who will benefit with this feature?**\r\nWhoever performs long training sessions.\r\n\r\n**Any Other info.**\r\nNA", "comments": ["@dennymarcels Thanks for creating the issue. keras moved to a new repository https://github.com/keras-team/keras/issues and that repo is dedicated for keras development. Earlier keras team published about the move in TF forum. The link is given below.\r\n\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nRegarding this feature, you could write custom callback to checkpoint and recover anytime. For example, you can use `on_train_begin` to checkpoint at the start of training and you can use other methods available to checkpoint at different times in the model training/testing as described here https://keras.io/guides/writing_your_own_callbacks/.\r\n\r\nPlease let me know what you think. Also, provide any use-case for further discussion. Thanks!", "Hey @jvishnuvardhan thank you for replying!\r\n\r\nI don't think writing a custom callback would work because my model is customized itself, and I could only save its weights. I can surely load the weights if the training was interrupted, but all callbacks will be reset, meaning none knows which was the best loss so far, nor in which epoch it happened.\r\n\r\nAlso, if you feel that is more appropriate and you have the power to do so, would you mind moving this request to the Keras repository?", "@dennymarcels When you write custom callback, you can inherit ModelCheckpoint callback and write the weights based on the performance of a metric. Please check [this example](https://github.com/tensorflow/tensorflow/issues/33163#issuecomment-833383468). When you want to load weights, you can choose weights of last iteration/batch/epoch or best weights as mentioned in [this TF tutorial](https://www.tensorflow.org/tutorials/keras/save_and_load). \r\n\r\nIf this is still an issue, please open in `keras-team/keras`. I cannot be able to move the issue to that repo because it is not part of `tensorflow/tensorflow` repository and I don't have permission. It is easy for you to open there and reference this issue. Thanks!\r\n\r\nAs entire Keras team is focussed on that repo, you would get faster response and resolution. Thanks!", "I did [post an issue there](https://github.com/keras-team/keras/issues/14862), and while writing it I figured I was not that clear here. Thank you nonetheless. Closing."]}, {"number": 50515, "title": "Split segment_reduction_ops_gpu.cu.cc compilation", "body": "- Refactors the file into a header and several separate source files to reduce critical path compile time. (This is done pre-emptively\r\n  because future changes are expected to increase the compile time of this code).\r\n- No functional change.\r\n\r\ncc @nluehr @sanjoy ", "comments": []}, {"number": 50514, "title": "tflite-runtime (2.5) needs GLIBC_2.29 with Python 3.8", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mendel Linux (Coral Dev Board)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: tflite-runtime 2.5\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: --- \r\n- GPU model and memory: ---\r\n\r\nHey,\r\n\r\nI'm not sure if this is the right place but I'm having problems with the tflite-runtime and python 3.8 on the coral dev board.\r\nI've upgraded to python 3.8 due to some other requirements and installed tflite-runtime with version 2.5.\r\nNow when I try to import the runtime I get this error:\r\n\r\n```\r\nPython 3.8.0 (default, May 17 2021, 08:51:42) \r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tflite_runtime.interpreter as tflite\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mendel/env3.8/lib/python3.8/site-packages/tflite_runtime/interpreter.py\", line 36, in <module>\r\n    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper\r\nImportError: /lib/aarch64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/mendel/env3.8/lib/python3.8/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.cpython-38-aarch64-linux-gnu.so)\r\n```\r\n\r\nAny suggestions how to fix this?\r\n\r\nBruno\r\n", "comments": ["Any ideas?", "You might need to build PIP by yourself. Check the following pages.\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_pip", "No way around it with Python3.8?", "Is there any difference between building the binaries and building the wheel (and installing it with pip) for the final usage?", "It depends on programming language you want to use.\r\nPIP is for [Python API](https://www.tensorflow.org/lite/guide/python) and other binaries are for [C++](https://www.tensorflow.org/lite/api_docs/cc) or C.", "Thanks @terryheo. When I run the docker container to cross-compile the wheel it does not find docker. Any idea how to fix this? I cloned the latest version of the tensorflow repo.\r\n\r\nError Message:\r\n```\r\n\r\nthon3.8/dist-packages/pybind11/include     CMakeFiles/cmTC_41885.dir/testCCompiler.c.o  -o cmTC_41885 \r\n    aarch64-linux-gnu-gcc: fatal error: -fuse-linker-plugin, but liblto_plugin.so not found\r\n    compilation terminated.\r\n    CMakeFiles/cmTC_41885.dir/build.make:89: recipe for target 'cmTC_41885' failed\r\n    make[1]: *** [cmTC_41885] Error 1\r\n    make[1]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeTmp'\r\n    Makefile:124: recipe for target 'cmTC_41885/fast' failed\r\n    make: *** [cmTC_41885/fast] Error 2\r\n    \r\n    \r\n\r\n  \r\n\r\n  CMake will not be able to correctly generate this project.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:40 (project)\r\n\r\n\r\nCMake Error at CMakeLists.txt:40 (project):\r\n  The CMAKE_CXX_COMPILER:\r\n\r\n    /workspace/tensorflow/lite/tools/cmake/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++\r\n\r\n  is not a full path to an existing compiler tool.\r\n\r\n  Tell CMake where to find the compiler by setting either the environment\r\n  variable \"CXX\" or the CMake cache entry CMAKE_CXX_COMPILER to the full path\r\n  to the compiler, or to the compiler name if it is in the PATH.\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeError.log\".\r\n```", "Solved by building myself. Thanks for the help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50514\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50514\">No</a>\n"]}, {"number": 50513, "title": "please help me ", "body": "[\r\nimport pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense,Conv1D,Flatten\r\nfrom tensorflow.keras.models import Sequential, Model\r\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelEncoder\r\nimport seaborn as sns\r\nimport keras\r\nfrom sklearn.metrics import accuracy_score,confusion_matrix\r\nimport plotly.offline as py\r\nimport xgboost as xgb\r\n%matplotlib inline\r\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\r\nsc = StandardScaler()\r\nx_train = sc.fit_transform(x_train)\r\nx_test = sc.transform(x_test)\r\nmodel=Sequential()\r\nmodel.add(Flatten(input_shape=(10,)))\r\nmodel.add(Dense(100,activation='relu'))\r\nmodel.add(Dense(1,activation='sigmoid'))\r\nmodel.compile(optimizer='adam',metrics=['accuracy'],loss='SparseCategoricalCrossentropy')\r\nmodel.fit(x_train,y_train,batch_size=64,validation_split=0.1,epochs=25)\r\npreds=model.predict(x_test)\r\npreds=np.where(preds>0.5,1,0)\r\naccuracy_score(y_test,press)]\r\nAttributeErrorTraceback (most recent call last)\r\n<ipython-input-18-a68cc7861501> in <module>()\r\n      3 model.add(Dense(100,activation='relu'))\r\n      4 model.add(Dense(1,activation='sigmoid'))\r\n----> 5 model.compile(optimizer='adam',metrics=['accuracy'],loss='SparseCategoricalCrossentropy')\r\n      6 model.fit(x_train,y_train,batch_size=64,validation_split=0.1,epochs=25)\r\n      7 preds=model.predict(x_test)\r\n\r\n3 frames\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.pyc in get_loss_function(loss)\r\n   1186   return losses.LossFunctionWrapper(\r\n   1187       loss_fn,\r\n-> 1188       name=loss_fn.__name__,\r\n   1189       reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)\r\n   1190 \r\n\r\nAttributeError: 'SparseCategoricalCrossentropy' object has no attribute '__name__'\r\n", "comments": ["Did you load the dataset? After importing the modules, if you run:\r\n\r\n```python\r\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\r\n```\r\n\r\nyou will get:\r\n\r\n```\r\n...\r\n----> 1 x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\r\n...\r\nNameError: name 'X' is not defined\r\n```\r\n\r\nUpdate: join the community at https://discuss.tensorflow.org/ where you can ask questions and discuss all things TF \ud83d\udc4d ", "You may try with\r\n```python\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=losses_utils.ReductionV2.AUTO,\r\n                            name='sparse_categorical_crossentropy')\r\nmodel.compile(optimizer='adam',metrics=['accuracy'],loss=loss)\r\n```", "[model.add(Flatten())\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Dense(8))\r\nmodel.add(tf.keras.layers.Dense(1))\r\nmodel.compile(optimizer='sgd', loss='mse')\r\n# This builds the model for the first time:\r\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\r\n\r\n\r\n\r\nValueErrorTraceback (most recent call last)\r\n<ipython-input-10-333e6d95c148> in <module>()\r\n      5 model.compile(optimizer='sgd', loss='mse')\r\n      6 # This builds the model for the first time:\r\n----> 7 model.fit(x_train, y_train, batch_size=32, epochs=10)\r\n\r\n6 frames\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.pyc in validate_input_types(inp, orig_inp, allow_dict, field_name)\r\n   1243     raise ValueError(\r\n   1244         'Please provide as model inputs either a single array or a list of '\r\n-> 1245         'arrays. You passed: {}={}'.format(field_name, orig_inp))\r\n   1246 \r\n   1247 \r\n\r\nValueError: Please provide as model inputs either a single array or a list of arrays.](url)", "> [model.add(Flatten())\r\n> model = tf.keras.Sequential()\r\n> model.add(tf.keras.layers.Dense(8))\r\n> model.add(tf.keras.layers.Dense(1))\r\n> model.compile(optimizer='sgd', loss='mse')\r\n> \r\n> # This builds the model for the first time:\r\n> model.fit(x_train, y_train, batch_size=32, epochs=10)\r\n> \r\n> ValueErrorTraceback (most recent call last)\r\n> in ()\r\n> 5 model.compile(optimizer='sgd', loss='mse')\r\n> 6 # This builds the model for the first time:\r\n> ----> 7 model.fit(x_train, y_train, batch_size=32, epochs=10)\r\n> \r\n> 6 frames\r\n> /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.pyc in validate_input_types(inp, orig_inp, allow_dict, field_name)\r\n> 1243 raise ValueError(\r\n> 1244 'Please provide as model inputs either a single array or a list of '\r\n> -> 1245 'arrays. You passed: {}={}'.format(field_name, orig_inp))\r\n> 1246\r\n> 1247\r\n> \r\n> ValueError: Please provide as model inputs either a single array or a list of arrays.](url)\r\n\r\nhow I can solve this problem?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50513\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50513\">No</a>\n"]}, {"number": 50512, "title": "InvalidArgumentError:  indices[10] = -1 is not in [0, 16081) \t [[node model_1/embedding_1/embedding_lookup (defined at <ipython-input-34-8168eb82f052>:1) ]] [Op:__inference_test_function_10607]", "body": "I am building a Wide & Deep model with Keras. However, after training and testing the model I get this error. I find the -1 very strange.\r\n\r\nThis is my code:\r\n\r\n```\r\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\r\n\r\n\r\ndef encode_inputs(inputs, use_embedding=False):\r\n    encoded_features = []\r\n    num_oov_indices = 10\r\n    for feature_name in inputs:\r\n        if feature_name in CATEGORICAL_FEATURE_NAMES:\r\n            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\r\n            # Create a lookup to convert string values to an integer indices.\r\n            # Since we are not using a mask token nor expecting any out of vocabulary\r\n            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\r\n            lookup = StringLookup(\r\n                vocabulary=vocabulary,\r\n                mask_token=None,\r\n                num_oov_indices=num_oov_indices,\r\n                output_mode=\"int\" if use_embedding else \"binary\",\r\n            )\r\n            if use_embedding:\r\n                # Convert the string input values into integer indices.\r\n                encoded_feature = lookup(inputs[feature_name])\r\n                #embedding_dims = int(math.sqrt(len(vocabulary)))\r\n                embedding_dims = int(np.ceil(len(vocabulary) ** 0.25))\r\n                \r\n                # Create an embedding layer with the specified dimensions.\r\n                embedding = layers.Embedding(\r\n                    input_dim=len(vocabulary), output_dim=embedding_dims\r\n                )\r\n                # Convert the index values to embedding representations.\r\n                encoded_feature = embedding(encoded_feature)\r\n            else:\r\n                # Convert the string input values into a one hot encoding.\r\n                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\r\n        else:\r\n            # Use the numerical features as-is.\r\n            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\r\n\r\n        encoded_features.append(encoded_feature)\r\n\r\n    all_features = layers.concatenate(encoded_features)\r\n    return all_features\r\n```\r\n\r\n\r\nThe model creation:\r\n```\r\ndef create_wide_and_deep_model():\r\n\r\n    inputs = create_model_inputs()\r\n    wide = encode_inputs(inputs)\r\n    wide = layers.BatchNormalization()(wide)\r\n\r\n    deep = encode_inputs(inputs, use_embedding=True)\r\n    for units in hidden_units:\r\n        deep = layers.Dense(units)(deep)\r\n        deep = layers.BatchNormalization()(deep)\r\n        deep = layers.ReLU()(deep)\r\n        deep = layers.Dropout(dropout_rate)(deep)\r\n\r\n    merged = layers.concatenate([wide, deep])\r\n    outputs = layers.Dense(units=1, activation=\"linear\")(merged)\r\n    model = keras.Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n```\r\n\r\nWhen I try: ```wide_and_deep_model.fit(train_dataset, epochs=num_epochs)```\r\n\r\nI get the following error:\r\n```\r\nInvalidArgumentError:  indices[119] = 16083 is not in [0, 16081)\r\n\t [[node model/embedding_1/embedding_lookup (defined at <ipython-input-28-db18f4de269d>:1) ]] [Op:__inference_train_function_2502]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model/embedding_1/embedding_lookup:\r\n model/embedding_1/embedding_lookup/1869 (defined at /usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:117)\r\n\r\nFunction call stack:\r\ntrain_function```\r\n\r\n\r\n", "comments": ["@nielsnpo \r\nCan you please refer to [this link](https://stackoverflow.com/questions/54176051/invalidargumenterror-indicesi-0-x-is-not-in-0-x-in-keras) and let us know. [[link1](https://github.com/tensorflow/tensorflow/issues/23698)]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I have a similar Issue:\r\n\r\n`  (0) Invalid argument:  indices[4,0] = -1 is not in [0, 8)\r\n\t [[node model_y_dnn/condition_embed_layer/embedding_lookup (defined at /tmp/tmpb36iffpd/dnn_model.py:163) ]]\r\n  (1) Invalid argument:  indices[4,0] = -1 is not in [0, 8)\r\n\t [[node model_y_dnn/condition_embed_layer/embedding_lookup (defined at /tmp/tmpb36iffpd/dnn_model.py:163) ]]\r\n\t [[model_y_dnn/condition_embed_layer/embedding_lookup/_72]]\r\n`\r\n\r\nfor a lookup that is defined as len(vocab)+1\r\n"]}, {"number": 50511, "title": "ValueError: Python inputs incompatible with input_signature:", "body": "environment\uff1atf2.2\\ linux \\2080Ti \\cuda:10.2\r\n\r\nI use **tf.kerns.model** and **tf.gradienttape** to build my (BERT-CRF) custom model, and now I want to use **tensorflow_ model_ Serving** deploys my model, so the first thing I need to do is save the model as a **\". Pb\" file**\r\n\r\nI tried to use **tf. saved_ model. save()** to save the model, but I encountered the problem of **dtype**, because in tf2.2, I can't change the dtype of tensor to tf.int32, whether I use **constant, cast or conver_to_tensor and variable** can't change dtype to tf.int32, and their output forms are changed to int32\r\n\r\nI saw from the Internet that by setting **@tf. function (tf. TensorSpec())** before the **call()** function, I still encountered the problem that dtype could not be converted to tf. Int32\r\n\r\nWhat I want to ask is, if you use tf.keras.model to build your own model, how can you save the model as \". Pb\", or can you tell me how to change the tensor of tf2.2 to tf.int32?\r\n\r\n[https://github.com/1148330040/nlp_cvs/blob/main/Models/bert_crf.py](url)\r\n\r\n`\r\nmodel:\r\n\r\n\r\n class MyBertCrf(tf.keras.Model):\r\n\r\n    def __init__(self, use_crf, input_dim, output_dim):\r\n        super(MyBertCrf, self).__init__(use_crf, input_dim, output_dim)\r\n        self.use_crf = use_crf\r\n        self.input_dim = input_dim\r\n        self.output_dim = output_dim\r\n        self.bert = TFBertModel.from_pretrained('hfl/chinese-bert-wwm-ext')\r\n        self.dropout = tf.keras.layers.Dropout(0.3)\r\n        self.dense = tf.keras.layers.Dense(self.output_dim)\r\n        self.other_params = tf.Variable(tf.random.uniform(shape=(output_dim, output_dim)))\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, 128], name='ids', dtype=tf.int32),\r\n                                  tf.TensorSpec([None, 128], name='mask', dtype=tf.int32),\r\n                                  tf.TensorSpec([None, 128], name='tokens', dtype=tf.int32),\r\n                                  tf.TensorSpec([None, 128], name='target', dtype=tf.int32),\r\n                                  tf.TensorSpec([1], name='input_seq_len', dtype=tf.int32)])\r\n    def call(self, ids, masks, tokens, target, input_seq_len):\r\n        hidden = self.bert(ids, masks, tokens)[0]\r\n        dropout_inputs = self.dropout(hidden, 1)\r\n        logistic_seq = self.dense(dropout_inputs)\r\n        print(hidden)\r\n        print(ids, masks, tokens, target, input_seq_len)\r\n        if self.use_crf:\r\n            log_likelihood, self.other_params = tfa.text.crf.crf_log_likelihood(logistic_seq,\r\n                                                                                target,\r\n                                                                                input_seq_len,\r\n                                                                                self.other_params )\r\n            decode_predict, crf_scores = tfa.text.crf_decode(logistic_seq, self.other_params , input_seq_len)\r\n\r\n            return decode_predict, log_likelihood, crf_scores\r\n        else:\r\n            prob_seq = tf.nn.softmax(logistic_seq)\r\n\r\n            return prob_seq, None, None`\r\n\r\nerror:\r\n\r\n`    \r\nraise ValueError(\"Python inputs incompatible with input_signature:\\n%s\" %\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[ 101 6163 1906 ...    0    0    0]\r\n [ 101  872 4761 ...    0    0    0]\r\n ...\r\n [ 101 1259 6163 ...    0    0    0]\r\n [ 101 2769 2682 ...    0    0    0]], shape=(16, 128), dtype=int32),\r\n    tf.Tensor(\r\n[[1 1 1 ... 0 0 0]\r\n ...\r\n [1 1 1 ... 0 0 0]], shape=(16, 128), dtype=int32),\r\n    tf.Tensor(\r\n[[0 0 0 ... 0 0 0]\r\n ...\r\n [0 0 0 ... 0 0 0]], shape=(16, 128), dtype=int32),\r\n    tf.Tensor(\r\n[[2 2 2 ... 0 0 0]\r\n ...\r\n [1 1 1 ... 0 0 0]], shape=(16, 128), dtype=int32),\r\n    tf.Tensor([19 19 28 28 21 27 19 20 32 22 23 29 27 21 29 25], shape=(16,), dtype=int32))\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 128), dtype=tf.int32, name='ids'),\r\n    TensorSpec(shape=(None, 128), dtype=tf.int32, name='mask'),\r\n    TensorSpec(shape=(None, 128), dtype=tf.int32, name='tokens'),\r\n    TensorSpec(shape=(None, 128), dtype=tf.int32, name='target'),\r\n    TensorSpec(shape=(1,), dtype=tf.int32, name='input_seq_len'))\r\n\r\nProcess finished with exit code 1\r\n`", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50511\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50511\">No</a>\n", "I'm facing a similar problem, did your error got resolved?\r\n See https://github.com/tensorflow/tensorflow/issues/55403 for more details.", "> \r\n\r\n\r\n\r\n> I'm facing a similar problem, did your error got resolved? See #55403 for more details.\r\n\r\nSolved\r\n\r\nChange it to the following form\uff08One more '()'\uff09:\r\n\r\n![1648532978(1)](https://user-images.githubusercontent.com/34124260/160542276-7819e7af-39db-4005-8b81-43bf8d973dd7.png)\r\n", "@1148330040 can you see https://github.com/tensorflow/tensorflow/issues/55403 and find out what exactly is causing the problem?", "> @1148330040 can you see #55403 and find out what exactly is causing the problem?\r\n\r\nsorry, i cant open your code, i use tf2.2, My mistake is missing a bracket. You can try it", "@1148330040 I tried adding a bracket, but it's still not working", "show me your model code\uff0cnot web\uff0cI can't open it\r\n\r\n---Original---\r\nFrom: \"Yelchuri venkata sai ***@***.***&gt;\r\nDate: Tue, Mar 29, 2022 14:10 PM\r\nTo: ***@***.***&gt;;\r\nCc: ***@***.******@***.***&gt;;\r\nSubject: Re: [tensorflow/tensorflow] ValueError: Python inputs incompatible with input_signature: (#50511)\r\n\r\n\r\n\r\n\r\n \r\n@1148330040 I tried adding a bracket, but it's still not working\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "This is model code:\r\ndef __init__(self):\r\n    self.model = tf.keras.Sequential([\r\n        tf.keras.layers.Dense(24,input_shape=(2,), activation='relu'),\r\n        tf.keras.layers.Dense(12,activation='relu'),\r\n        tf.keras.layers.Dense(1,activation='sigmoid')])\r\n\r\nThis is error:\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    tf.Tensor(\r\n[[ 9.       17.229916]\r\n [15.       10.601508]\r\n [15.       10.517618]\r\n [ 7.       26.893027]\r\n [15.       10.572687]\r\n [ 7.       27.393882]\r\n [15.        9.859101]\r\n [14.       11.724992]\r\n [12.       13.421819]\r\n [ 7.       28.229916]], shape=(10, 2), dtype=float64),\r\n    tf.Tensor([1. 1. 1. 0. 1. 0. 1. 1. 1. 0.], shape=(10,), dtype=float64))\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 2), dtype=tf.float64, name=None),\r\n    TensorSpec(shape=(None, 1), dtype=tf.float64, name=None)).\r\n\r\n\r\nThis is the class code:\r\nclass Model(tf.Module):\r\n\r\n  def __init__(self):\r\n    self.model = tf.keras.Sequential([\r\n        tf.keras.layers.Dense(24,input_shape=(2,), activation='relu'),\r\n        tf.keras.layers.Dense(12,activation='relu'),\r\n        tf.keras.layers.Dense(1,activation='sigmoid')])\r\n    self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n    self.optimizer = tf.keras.optimizers.Adam()\r\n    self.train_loss_results = []\r\n    self.train_accuracy_results = []\r\n    self.epoch_loss_avg = tf.keras.metrics.Mean()\r\n    self.epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\r\n\r\n\r\n  def loss(self,model, x, y, training):\r\n    # training=training is needed only if there are layers with different\r\n    # behavior during training versus inference (e.g. Dropout).\r\n    y_ = self.model(x, training=False)\r\n    return self.loss_object(y_true=y, y_pred=y_)\r\n\r\n  def grad(self,model, inputs, targets):\r\n    with tf.GradientTape() as tape:\r\n      loss_value = self.loss(model, inputs, targets, training=False)\r\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n\r\n  # The `train` function takes a batch of input images and labels.\r\n  @tf.function(input_signature=[(tf.TensorSpec([None, 2], tf.float64)),\r\n                                (tf.TensorSpec([None, 1], tf.float64))])\r\n  def train(self, x, y):\r\n    loss_value, grads = self.grad(self.model, x, y)\r\n    self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n    result = {\"loss\": loss_value}\r\n    self.epoch_loss_avg.update_state(loss_value)  \r\n    self.epoch_accuracy.update_state(y, self.model(x, training=True))\r\n    return result\r\n\r\n  @tf.function(input_signature=[\r\n      tf.TensorSpec([None, 2], tf.float32),\r\n  ])\r\n\r\n  def infer(self, x):\r\n    logits = self.model(x)\r\n    prediction=tf.math.round(logits)\r\n    return {\r\n        \"output\": prediction,\r\n        \"logits\": logits\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def save(self, checkpoint_path):\r\n    tensor_names = [weight.name for weight in self.model.weights]\r\n    tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n    tf.raw_ops.Save(\r\n        filename=checkpoint_path, tensor_names=tensor_names,\r\n        data=tensors_to_save, name='save')\r\n    return {\r\n        \"checkpoint_path\": checkpoint_path\r\n    }\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n  def restore(self, checkpoint_path):\r\n    restored_tensors = {}\r\n    for var in self.model.weights:\r\n      restored = tf.raw_ops.Restore(\r\n          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n          name='restore')\r\n      var.assign(restored)\r\n      restored_tensors[var.name] = restored\r\n    return restored_tensors", "# you can try this one:\r\n# ==============================\r\nmax_len = 12\r\nclass MyModel(tf.keras.Model):\r\n    def init(self, output_dim):\r\n        super(MyModel, self).__init__(output_dim)\r\n        self.dense1 = tf.keras.layers.Dense(24,input_shape=(2,), activation='relu')\r\n        self.dense2 = tf.keras.layers.Dense(12,activation='relu')\r\n        self.dense3 = tf.keras.layers.Dense(1,activation='sigmoid')\r\n\r\n    @tf.function(input_signature=[(tf.TensorSpec([None, max_len], name='input_ds', dtype=tf.int32))])\r\n    def call(self, batch_ds):\r\n        input_ids = batch_ds\r\n        hidden1 = self.dense1(input_ids)\r\n        hidden1 = self.dense2(hidden1)\r\n        predict = self.dense3(hidden1)\r\n        return predict\r\n\r\ndef loss(self, x, y,):\r\n    # training=training is needed only if there are layers with different\r\n    # behavior during training versus inference (e.g. Dropout).\r\n    return tf.keras.losses.BinaryCrossentropy(x, y)\r\n\r\ndef data_generator():\r\n    # Here is your own data iterator\r\n    return 1\r\n\r\ndef train():\r\n    model = MyModel(1)\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\r\n    def grad(inputs, targets):\r\n        with tf.GradientTape() as tape:\r\n            loss_value = loss(model, inputs, targets)\r\n\r\n        params = tape.gradient(loss_value, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(params, model.trainable_variables))\r\n        return loss_value\r\n\r\n    for ds in data_generator:\r\n        input_ds = ds[0]\r\n        label = ds[1]\r\n        loss = grad(input_ds, label)\r\n\r\n        model.save('your_model.weights')\r\n\r\n# you can try like this one\r\n# ========================\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(24,input_shape=(2,), activation='relu'),\r\n    tf.keras.layers.Dense(12,activation='relu'),\r\n    tf.keras.layers.Dense(1,activation='sigmoid')])\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    loss=tf.keras.losses.BinaryCrossentropy # 'binary_crossentropy'\r\n)\r\n\r\nmodel.fit()\r\n\r\nmodel.save()\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    ***@***.***&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2022\u5e743\u670829\u65e5(\u661f\u671f\u4e8c) \u4e0b\u53482:24\r\n***@***.***&gt;;\r\n***@***.******@***.***&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] ValueError: Python inputs incompatible with input_signature: (#50511)\r\n\r\n\r\n\r\n\r\n\r\n \r\nThis is the class code:\r\n \r\nclass Model(tf.Module):\r\n \r\ndef init(self):\r\n self.model = tf.keras.Sequential([\r\n tf.keras.layers.Dense(24,input_shape=(2,), activation='relu'),\r\n tf.keras.layers.Dense(12,activation='relu'),\r\n tf.keras.layers.Dense(1,activation='sigmoid')])\r\n self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n self.optimizer = tf.keras.optimizers.Adam()\r\n self.train_loss_results = []\r\n self.train_accuracy_results = []\r\n self.epoch_loss_avg = tf.keras.metrics.Mean()\r\n self.epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\r\n \r\ndef loss(self,model, x, y, training):\r\n # training=training is needed only if there are layers with different\r\n # behavior during training versus inference (e.g. Dropout).\r\n y_ = self.model(x, training=False)\r\n return self.loss_object(y_true=y, y_pred=y_)\r\n \r\ndef grad(self,model, inputs, targets):\r\n with tf.GradientTape() as tape:\r\n loss_value = self.loss(model, inputs, targets, training=False)\r\n return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n \r\nThe train function takes a batch of input images and labels.\r\n \r\n@tf.function(input_signature=[(tf.TensorSpec([None, 2], tf.float64)),\r\n (tf.TensorSpec([None, 1], tf.float64))])\r\n def train(self, x, y):\r\n loss_value, grads = self.grad(self.model, x, y)\r\n self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n result = {\"loss\": loss_value}\r\n self.epoch_loss_avg.update_state(loss_value)\r\n self.epoch_accuracy.update_state(y, self.model(x, training=True))\r\n return result\r\n \r\n@tf.function(input_signature=[\r\n tf.TensorSpec([None, 2], tf.float32),\r\n ])\r\n \r\ndef infer(self, x):\r\n logits = self.model(x)\r\n prediction=tf.math.round(logits)\r\n return {\r\n \"output\": prediction,\r\n \"logits\": logits\r\n }\r\n \r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n def save(self, checkpoint_path):\r\n tensor_names = [weight.name for weight in self.model.weights]\r\n tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n tf.raw_ops.Save(\r\n filename=checkpoint_path, tensor_names=tensor_names,\r\n data=tensors_to_save, name='save')\r\n return {\r\n \"checkpoint_path\": checkpoint_path\r\n }\r\n \r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n def restore(self, checkpoint_path):\r\n restored_tensors = {}\r\n for var in self.model.weights:\r\n restored = tf.raw_ops.Restore(\r\n file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n name='restore')\r\n var.assign(restored)\r\n restored_tensors[var.name] = restored\r\n return restored_tensors\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "TF. No suggestion first keras.Sequential () and self built inherit from TF.Moudle is used in combination. I don't understand. Is this the code in the tensorflow tutorial?\r\n\r\n\r\nThis is very different. As for the reason, you can find it yourself. It's a process of progress for you.\r\n\r\n\r\n\r\n\r\nNext @ tf.function (input_signature=() is to specify the format of input data of the model\r\n\r\n\r\nIt won't be with TF.keras.Sequential () appears together, so your code gives me the feeling that it is very messy, a simplified process version TF keras. Seq and customized versions are mixed.\r\n\r\n\r\n\r\nSo you can take a look at the code examples I wrote for you first, and please take a closer look at the basic tutorial of tensorflow:\r\n\u521d\u5b66\u8005\u7684 TensorFlow 2.0 \u6559\u7a0b &nbsp;|&nbsp; TensorFlow Core (google.cn)\r\n\u9488\u5bf9\u4e13\u4e1a\u4eba\u5458\u7684 TensorFlow 2.0 \u5165\u95e8 &nbsp;|&nbsp; TensorFlow Core (google.cn)\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    ***@***.***&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2022\u5e743\u670829\u65e5(\u661f\u671f\u4e8c) \u4e0b\u53482:24\r\n***@***.***&gt;;\r\n***@***.******@***.***&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] ValueError: Python inputs incompatible with input_signature: (#50511)\r\n\r\n\r\n\r\n\r\n\r\n \r\nThis is the class code:\r\n \r\nclass Model(tf.Module):\r\n \r\ndef init(self):\r\n self.model = tf.keras.Sequential([\r\n tf.keras.layers.Dense(24,input_shape=(2,), activation='relu'),\r\n tf.keras.layers.Dense(12,activation='relu'),\r\n tf.keras.layers.Dense(1,activation='sigmoid')])\r\n self.loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\n self.optimizer = tf.keras.optimizers.Adam()\r\n self.train_loss_results = []\r\n self.train_accuracy_results = []\r\n self.epoch_loss_avg = tf.keras.metrics.Mean()\r\n self.epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\r\n \r\ndef loss(self,model, x, y, training):\r\n # training=training is needed only if there are layers with different\r\n # behavior during training versus inference (e.g. Dropout).\r\n y_ = self.model(x, training=False)\r\n return self.loss_object(y_true=y, y_pred=y_)\r\n \r\ndef grad(self,model, inputs, targets):\r\n with tf.GradientTape() as tape:\r\n loss_value = self.loss(model, inputs, targets, training=False)\r\n return loss_value, tape.gradient(loss_value, model.trainable_variables)\r\n \r\nThe train function takes a batch of input images and labels.\r\n \r\n@tf.function(input_signature=[(tf.TensorSpec([None, 2], tf.float64)),\r\n (tf.TensorSpec([None, 1], tf.float64))])\r\n def train(self, x, y):\r\n loss_value, grads = self.grad(self.model, x, y)\r\n self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\r\n result = {\"loss\": loss_value}\r\n self.epoch_loss_avg.update_state(loss_value)\r\n self.epoch_accuracy.update_state(y, self.model(x, training=True))\r\n return result\r\n \r\n@tf.function(input_signature=[\r\n tf.TensorSpec([None, 2], tf.float32),\r\n ])\r\n \r\ndef infer(self, x):\r\n logits = self.model(x)\r\n prediction=tf.math.round(logits)\r\n return {\r\n \"output\": prediction,\r\n \"logits\": logits\r\n }\r\n \r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n def save(self, checkpoint_path):\r\n tensor_names = [weight.name for weight in self.model.weights]\r\n tensors_to_save = [weight.read_value() for weight in self.model.weights]\r\n tf.raw_ops.Save(\r\n filename=checkpoint_path, tensor_names=tensor_names,\r\n data=tensors_to_save, name='save')\r\n return {\r\n \"checkpoint_path\": checkpoint_path\r\n }\r\n \r\n@tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n def restore(self, checkpoint_path):\r\n restored_tensors = {}\r\n for var in self.model.weights:\r\n restored = tf.raw_ops.Restore(\r\n file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\r\n name='restore')\r\n var.assign(restored)\r\n restored_tensors[var.name] = restored\r\n return restored_tensors\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "Thanks for the suggestions, but hen I changed the input signature from (None,1) to (None)  the program started working\r\n", "congratulations\uff01~\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    ***@***.***&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2022\u5e743\u670829\u65e5(\u661f\u671f\u4e8c) \u4e0b\u53483:16\r\n***@***.***&gt;;\r\n***@***.******@***.***&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] ValueError: Python inputs incompatible with input_signature: (#50511)\r\n\r\n\r\n\r\n\r\n\r\n \r\nThanks for the suggestions, but hen I changed the input signature from (None,1) to (None)  the program started working\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "Thank you!!"]}, {"number": 50510, "title": "The Phi definition of GELU is incorrect", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/addons/api_docs/python/tfa/activations/gelu?hl=en\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nCurrently, the definition of `\\Phi(x)` in the GELU document is as the follow:\r\n\r\n```\r\n    \\Phi(x) = \\frac{x}{2} \\left[ 1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)) \\right]\r\n```\r\n\r\nBut it seems that this is the definition of `gelu(x)`, is not it of `\\Phi(x)` in the original paper.\r\n\r\nI expect that a definition of `\\Phi(x)` was as the follow:\r\n\r\n```\r\n    \\Phi(x) = \\frac{1}{2} \\left[ 1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)) \\right]\r\n```\r\n\r\nOr, the document defined `gelu(x)` directly when `approximate` is `True` like the follow:\r\n\r\n```\r\n    gelu(x) = \\frac{x}{2} \\left[ 1 + \\tanh(\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)) \\right]\r\n```", "comments": ["@RATATATO \r\nCan you please share the original paper as proof of the doc change requested.", "The original paper is here:\r\n\r\n- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)\r\n\r\nThis paper is also linked in the [GELU page](https://www.tensorflow.org/addons/api_docs/python/tfa/activations/gelu?hl=en).\r\n\r\nIn the section 2 of the paper, an approximation of `gelu(x)` is shown as the follow:\r\n\r\n```\r\nWe can approximate the GELU with\r\n   0.5x(1 + tanh( ...    (Same definition of \\Phi(x) on the Tensorflow page currently)\r\n```\r\n\r\nSo from the paper, I expect that an approximation of `gelu(x)` and `\\Phi(x)` is as the follow:\r\n\r\n```\r\ngelu(x) = x * \\Phi(x) = 0.5x(1 + tanh( ...\r\n\\Phi(x) = 0.5(1 + tanh( ...\r\n```\r\n\r\n### See also\r\nAlso on other sites, the current definition of `\\Phi(x)` is used as an approximation of `gelu(x)`, not as it of `\\Phi(x)`\r\n\r\n- https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation\r\n- etc", "@RATATATO  Perhaps you can raise PR to fix this issue \r\nThe docs are generated from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/gelu.py\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50509, "title": "For Keras training, can we relax the steps checking when distributed dataset is passed in", "body": "Recently, when we tried to use MultiWorkerMirroredStrategy with Keras, we found:\r\n1) When Keras wrap our passed in dataset with experimental distributed dataset, we found we cannot scale over x nodes because it needs us pass in a global batch size and global batch size needs to take number of workers into consideration (global batch size = batch size * num of workers * num of replica). Therefore, when we have a lot of workers, compared with Mirrored strategy, we start seeing job failure due to OOM\r\n2) We try to get around this issue by passing in distribute_datasets_from_function that we can have full control over per replica batch and sharding logic (and get around OOM issue). Then our job failed at: https://github.com/tensorflow/tensorflow/blob/1923123d32ea41d92b70a27a3f6ecf0763b56f6c/tensorflow/python/keras/engine/data_adapter.py#L733\r\nWhen we passed in normal dataset, it has UNKNOWN cardinality and leverage https://github.com/tensorflow/tensorflow/blob/1923123d32ea41d92b70a27a3f6ecf0763b56f6c/tensorflow/python/keras/engine/data_adapter.py#L710 to recreate iterator for every epoch. Our use case is to have validation step to exhaust our dataset instead of hard coding steps. I wonder if we can relax check in L733 altogether with change to L714. Then we can support no steps input from users? If you agree, I can submit the PR to make the change. \r\n\r\nPlease let me know if any downside of doing so.\r\n\r\nThanks", "comments": ["Excellent question. You may also be able to get feedback at https://discuss.tensorflow.org/ - try Title: \"MultiWorkerMirroredStrategy with Keras: can we relax the steps checking when distributed dataset is passed in?\" and maybe someone from the community/TF team will get back to you \ud83d\udc4d ", "@annyan09023 It looks like the issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Asked this question in keras repo instead ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50509\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50509\">No</a>\n"]}, {"number": 50508, "title": "[Go] Fix segfault on string tensors with mismatched dimensions", "body": "This PR fixes a segmentation violation that may occur during GC on string tensors. The segfault results in a flaky test for `TestNewTensor` at the error test for mismatched dimensions for strings\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c837cf8963d4ef9cb3b3b9e8787cb35f21b68f9d/tensorflow/go/tensor_test.go#L86-L87\r\n\r\nFor string tensors, `C.TF_TString_Dealloc` is called during garbage collection within a finalizer function.  However, tensor structure isn't checked until encoding to avoid a performance penalty.  The current method for dealloc assumes that encoding succeeded, but segfaults when a string tensor is garbage collected whose encoding failed (e.g., due to mismatched dimensions).\r\n\r\nTo fix this, the call to set the finalizer function is deferred until `NewTensor` returns and, if encoding failed for a string tensor, deallocs are determined based on bytes written.\r\n\r\n", "comments": []}, {"number": 50506, "title": "support integer types for FloorMod on GPU", "body": "Some types of FloorMod operators cannot be placed on the gpu, from the timeline.\r\nSupport integer types for FloorMod on GPU: int8, int16, int32, int64, uint8, uint16, uint32, uint64", "comments": []}, {"number": 50505, "title": "[determinism] Add d9m-unimplemented exception-throwing to fused batch-norm", "body": "This PR adds determinism-unimplemented exception-throwing to `tf.compat.v1.nn.fused_batch_norm`. If an attempt is made to run the GPU kernel that implements backprop to `x`, `scale`, or `offset` when `is_training=False` (when fine-tuning) and when determinism is expected (i.e. when `TF_DETERMINISTIC_OPS` is set to `\"true\"` or `\"1\"`), then a `tf.errors.UnimplementedError` will be thrown.\r\n\r\nThis PR tests that the exception is thrown as appropriate and also tests that the other paths through the op (on both CPU and GPU) operate deterministically.\r\n\r\nThis PR is related to [RFC: Enabling Determinism in TensorFlow](https://github.com/tensorflow/community/blob/master/rfcs/20210119-determinism.md). For status and history of GPU-determinism for this op, see [here](https://github.com/NVIDIA/framework-determinism/blob/master/tensorflow_status.md#fused-batch-norm).\r\n\r\nCC @reedwm, @sanjoy, @nluehr", "comments": []}, {"number": 50504, "title": "Is there a way to update a dynamic variable used to filter a part of your training data during training?", "body": "```\r\ndef myfilter(x, my_var):\r\n    return tf.equal(x['vars'], my_var)\r\n\r\n\r\ndata = tf.data.TFRecordDataset(tf.io.match_filenames_once('part-*'))\r\nmy_var  = tf.Variable(1, trainable=False, name='my_var', dtype=tf.int64)\r\n\r\ndata = data.map(parsing_func, num_parallel_calls=multiprocessing.cpu_count() - 1)\r\ndata= data.filter(lambda x : myfilter(x, my_var) )\r\ndata = data.batch(batch_size=32)\r\n\r\n```\r\n\r\nHere, using static `my_var`, I am able to filter the data. However, I want to keep updating the var value from [1, 2, .... n]. Any ideas on how to do this during training? \r\n\r\nI was trying something like this:\r\n\r\n```\r\nclass CustomVarScheduler(tf.keras.callbacks.Callback):\r\n    def __init__(self, my_var):\r\n        super(CustomPhaseScheduler, self).__init__()\r\n        self.var = my_var\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        # Set the value back to the optimizer before this epoch starts\r\n        tf.keras.backend.set_value(self.my_var, tf.math.add(self.my_var, 1))\r\n        print(\"\\nEpoch %05d: my_var is %6.4f.\" % (epoch, self.my_var))\r\n\r\n```\r\nNot able to get it to work properly. Any help? or Possible Documentation/Feature. \r\n\r\nAlso, willing to contribute to such documentation.  \r\n\r\nThanks.\r\n", "comments": ["Or do I need something like this: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/utils/dataset_creator.py", "@harsh306 \r\nIs this still an issue", "Yeah, I didn't find a proper way to do this. ", "This question is better asked on [TensorFlow Forum](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 50503, "title": "Is there a plan to support horizontal scaling for KPL?", "body": "Based on current preprocessing layer code: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/engine/base_preprocessing_layer.py#L237\r\n\r\nhorizontal scaling is not supported for KPL. For example, I cannot have TextVectorization run on different machines with different sharded data and merge at the end. I wonder is there any plan for KPL to support horizontal scaling so that it can adapt with my large dataset?", "comments": ["@annyan09023 Thanks for creating the issue. keras moved to a new repository and that repo is dedicated for keras development. Earlier keras team published about the move in TF forum. The link is given below.\r\n\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nCan you please create a new issue in that repository. Thanks!", "done creating another ticket under keras repo. Feel free to close this one"]}, {"number": 50502, "title": "The TFLite Converter has errors when \"training=True\" is used in Dropout and BatchNormalization layers", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Windows 10):\r\n- TensorFlow installation (pip package):\r\n- TensorFlow library (TF 2.5.0):\r\n\r\n### 2. Code\r\n\r\nWhen training GANs, some Dropout and BatchNormalization layers need to have \"training=True\" set for the training to work correctly.  This is as per a [book on GANs](https://machinelearningmastery.com/generative_adversarial_networks/) where the chapter on pix2pix translation is based on [this article](https://phillipi.github.io/pix2pix/).   Unfortunately \"training=True\" trips up the TensorFlow Lite Converter with the errors shown in the sample code below.  It would be good if the TensorFlow Lite Converter ignored the \"training=True\" arguments.\r\n\r\nThe bug can be recreated with the following code.\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Input\r\nfrom keras.models import Model\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import BatchNormalization\r\n\r\n# Create minimalist model to show the errors (actual model is much bigger)\r\nin_image = Input(shape=(256,256,3))\r\nout_image = Conv2D(64, (4,4))(in_image)\r\n# In the following two lines, the arguments \"training=True\" cause errors in the the TensorFlow Lite Converter.\r\n# Removing both \"training=True\" arguments below makes the conversion work fine, but including the \"training=True\" causes the errors shown.\r\nout_image = Dropout(0.5)(out_image, training=True)          # Causes error: \"TF Select ops: RandomUniform, Details: tf.RandomUniform {device = \"\", seed = 0 : i64, seed2 = 0 : i64}\" below\r\nout_image = BatchNormalization()(out_image, training=True)  # Causes error: \"0 incompatible with expected resource\" below\r\nmodel = Model(in_image, out_image)\r\nmodel.compile()\r\n\r\n# Convert model to TensorFlow Lite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nwith open('model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nThere might be other layers which can have \"training=True\" set.", "comments": ["For BatchNorm with training=True it requires variables.\r\nYou can enable variable support on saved model using\r\n\r\n```\r\nconverter.experimental_enable_resource_variables = True\r\n```\r\n\r\nFlag will be enabled by default later, and currently you can do this only when using from_saved_model - other entry points will be coming soon too.\r\n\r\nNote that this will still require TF_Select for the FusedBatchNorm op to run.\r\n\r\nFor DropOuts:\r\nThis requires RandomUniform op, we are working on adding it as a builtin op, for now you can use TF_Select model to run the model.\r\n\r\nThanks", "Sorry, but the problem is still there!\r\n\r\nAfter waiting a few days, I then installed the latest nightly build ( tf_nightly-2.7.0.dev20210704-cp37-cp37m-win_amd64.whl ) into a separate virtual environment.\r\n\r\nAfter implementing the changes suggested, the error message when including \"training=True\" for BatchNormalization() has changed to a different one  (TF Select ops: FusedBatchNormV3\r\nDetails: tf.FusedBatchNormV3(tensor<?x253x253x64xf32>, ...  exponential_avg_factor = 0.00999999977 : f32, is_training = true}).  \r\n\r\nThe error message for Dropout() is the same.\r\n\r\nThe test code now uses \"converter.experimental_enable_resource_variables = True\" and \"from_saved_model()\" as specified, and is now as follows:-\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Input\r\nfrom keras.models import Model\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import BatchNormalization\r\n\r\n# Create minimalist model to show the errors (actual model is much bigger)\r\nin_image = Input(shape=(256,256,3))\r\nout_image = Conv2D(64, (4,4))(in_image)\r\n# In the following two lines, the arguments \"training=True\" cause errors in the the TensorFlow Lite Converter.\r\n# Removing both \"training=True\" arguments below makes the conversion work fine, but including the \"training=True\" causes the errors shown.\r\nout_image = Dropout(0.5)(out_image, training=True)          # Causes error: \"TF Select ops: RandomUniform, Details: tf.RandomUniform {device = \"\", seed = 0 : i64, seed2 = 0 : i64}\" below\r\nout_image = BatchNormalization()(out_image, training=True)  # Causes error: \"TF Select ops: FusedBatchNormV3 Details: tf.FusedBatchNormV3 ... : f32, is_training = true}\" below\r\nmodel = Model(in_image, out_image)\r\nmodel.compile()\r\n\r\n# Save the model \r\nmodel.save(\"test_model\")\r\n\r\n# Convert model to TensorFlow Lite\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"test_model\")\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nwith open('test_model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nCould someone have a look into it.\r\nThanks\r\n\r\n", "In your sample code, you didn't enable TF Select. You need to enable them since we don't support these ops natively.\r\n\r\nSee instructions [here](https://www.tensorflow.org/lite/guide/ops_select#convert_a_model)\r\n\r\nBasically for conversion you need to add\r\n```\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n```", "Thanks, I've added that bit of code.  It looks as though we are going in the right direction but am now getting a different error again \"TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s): Flex ops: FlexFusedBatchNormV3, FlexRandomUniform\".\r\n\r\nThe code is now as follows:-\r\n```\r\nimport tensorflow as tf\r\nfrom keras.models import Input\r\nfrom keras.models import Model\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import Dropout\r\nfrom keras.layers import BatchNormalization\r\n\r\n# Create minimalist model to show the errors (actual model is much bigger)\r\nin_image = Input(shape=(256,256,3))\r\nout_image = Conv2D(64, (4,4))(in_image)\r\n# In the following two lines, the arguments \"training=True\" cause errors in the the TensorFlow Lite Converter.\r\n# Removing both \"training=True\" arguments below makes the conversion work fine, but including the \"training=True\" causes the errors shown.\r\nout_image = Dropout(0.5)(out_image, training=True)          # Causes error: \"TF Select ops: RandomUniform, Details: tf.RandomUniform {device = \"\", seed = 0 : i64, seed2 = 0 : i64}\" below\r\nout_image = BatchNormalization()(out_image, training=True)  # Causes error: \"0 incompatible with expected resource\" below\r\nmodel = Model(in_image, out_image)\r\nmodel.compile()\r\n\r\n# Save the model \r\nmodel.save(\"test_model\")\r\n\r\n# Convert model to TensorFlow Lite\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"test_model\")\r\nprint(\"tf.lite.TFLiteConverter.from_saved_model done\")\r\nconverter.experimental_enable_resource_variables = True\r\nprint(\"experimental_enable_resource_variables done\")\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS    # enable TensorFlow ops.\r\n]\r\nprint(\"TF Select done\")\r\ntflite_model = converter.convert()\r\nprint(Convert done\")\r\nwith open('test_model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nThe error occurs on the line \"tflite_model = converter.convert()\".\r\n\r\nSorry, but could you have a look at it again.\r\nThanks\r\nEric", "IMPORTANT UPDATE\r\n\r\nIn my initial submission I said that \"It would be good if the TensorFlow Lite Converter ignored the \"training=True\" arguments\".\r\n\r\nOn some reflection and investigation, I now realize that the Dropout() and BatchNormalization() layers which are normally not trainable are now trainable.   The training component for those two layers therefore needs to be implemented in TFLite.  Ignoring \"training=True\" may prevent an error coming up but won't produce the correct output."]}, {"number": 50501, "title": "test", "body": "<em>Please make sure that this is an issue related to keras.\r\ntag:keras_template</em>\r\n\r\n## **Important Notice**\r\n\r\nPlease note that `tf.keras` code was moved entirely to\r\n[keras-team/keras](https://github.com/keras-team/keras) repository\r\n\r\nYou can open any code/doc bugs, performance issues, and feature requests\r\n in [keras-team/keras](https://github.com/keras-team/keras/issues) repository\r\n\r\n`tf.keras` related issues opened in\r\n[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may\r\nnot get attention as [keras-team/keras](https://github.com/keras-team/keras)\r\nrepository is dedicated for the development of `keras` code\r\n", "comments": []}, {"number": 50500, "title": "[CherryPick:r2.6]Rollback breaking change", "body": "PiperOrigin-RevId: 381578821\nChange-Id: I08d1c7896d76a19c6e693d8e178b89f5fb49af15", "comments": []}, {"number": 50499, "title": "[CherryPick:r2.6]Disable test on macos and debug.", "body": "PiperOrigin-RevId: 381563828\nChange-Id: I8199401a23494d9c0f08e4818198ef07c51cf969", "comments": []}, {"number": 50498, "title": "r2.6 cherry-pick request: Add the missing kernels_experimental header file to pip package", "body": "PR #49717 adds [Resource Variable ops support to kernel C API](https://github.com/tensorflow/community/pull/387) in a new `kernel_experimental` file. However, it failed to add the new header file to the pip package, so PluggableDevices aren't able to use it yet. This PR fixed that. With this fix, TensorFlow can train models on PluggableDevices.\r\n\r\nOriginal PR (merged into master on 6/26): #50462 \r\ncc: @kulinseth @saxenasaurabh ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50498) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes as the changes are from an already merged PR.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50498) for more info**.\n\n<!-- need_author_consent -->", "Thank you for the quick review and the fix, @mihaimaruseac!"]}, {"number": 50497, "title": "tensorflow lite (C++) crashes in tflite::reference_ops::Gather", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina, Linux Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4, 2.5, current\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A (tensorflow-lite was compiled w/ cmake)\r\n- GCC/Compiler version (if compiling from source): clang-12.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nAfter a few iterations over the input data tensorflow-lite crashes in tflite::reference_ops::Gather<>\r\n\r\n**Describe the expected behavior**\r\nNot to crash!\r\n\r\n**Standalone code to reproduce the issue**\r\n[standalone.tar.gz](https://www.dropbox.com/s/dwkypf5skrxzfqa/standalone.tar.gz?dl=0)\r\n\r\n**Other info / logs** \r\nBacktrace (using lldb on Mac OS Catalina):\r\n```\r\n(lldb) bt\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT\r\n  * frame #0: 0x00007fff69bb033a libsystem_kernel.dylib`__pthread_kill + 10\r\n    frame #1: 0x00007fff69c6ce60 libsystem_pthread.dylib`pthread_kill + 430\r\n    frame #2: 0x00007fff69b37808 libsystem_c.dylib`abort + 120\r\n    frame #3: 0x000000010049ec05 standalone`void tflite::reference_ops::Gather<float, int>(op_params=0x00007ffeefbfeee0, input_shape=0x00007ffeefbfef48, input_data=0x0000000104136ce4, coords_shape=0x00007ffeefbfef28, coords_data=0x000000011261a000, output_shape=0x00007ffeefbfef08, output_data=0x000000011261d6c0) at gather.h:76:9\r\n    frame #4: 0x000000010049cdc6 standalone`TfLiteStatus tflite::ops::builtin::gather::Gather<float, int>(params=0x000000010302a780, input=0x000000010380a310, positions=0x000000010380a000, output=0x000000010380a770) at gather.cc:125:3\r\n    frame #5: 0x000000010049c9b9 standalone`tflite::ops::builtin::gather::Eval(context=0x0000000103038eb8, node=0x000000010381e000) at gather.cc:166:16\r\n    frame #6: 0x0000000100039543 standalone`tflite::Subgraph::OpInvoke(this=0x0000000103038e90, op_reg=0x000000010381e050, node=0x000000010381e000) at subgraph.h:434:12\r\n    frame #7: 0x000000010003919b standalone`tflite::Subgraph::Invoke(this=0x0000000103038e90) at subgraph.cc:1104:9\r\n    frame #8: 0x00000001006b91af standalone`tflite::Interpreter::Invoke(this=0x0000000103038da0) at interpreter.cc:273:3\r\n    frame #9: 0x0000000100004b2a standalone`main((null)=<unavailable>, (null)=<unavailable>) at main.cc:139:5 [opt]\r\n    frame #10: 0x00007fff69a68cc9 libdyld.dylib`start + 1\r\n    frame #11: 0x00007fff69a68cc9 libdyld.dylib`start + 1\r\n(lldb) \r\n```", "comments": ["The attached model does not have any issues with running with the TFLite model benchmark tool. Please make sure that the given input in your code is compatible with the model's input requirement.", "I'm not sure what can possibly be incompatible here. Model accepts 3500 integers on input. At some point it crashes.", "I stand corrected, there was an issue w/ input data. Thank you for pointing it out!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50497\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50497\">No</a>\n"]}, {"number": 50496, "title": "Update setup.py", "body": "", "comments": []}]