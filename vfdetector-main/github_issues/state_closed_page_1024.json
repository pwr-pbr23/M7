[{"number": 22618, "title": "cannot make learning rate as variable when using keras model to estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.11.0-cpu-py3\r\n- **Python version**:3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA \r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: run the code provided below \r\n\r\n### Describe the problem\r\nI was trying to make learning rate a variable and control it using hook during training, however, it gives me the following error <Tensor(\"Variable/read:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"dense/kernel:0\", shape=(), dtype=resource).>\r\n\r\nPS: when I define the estimator through the generic \"model_fn\" instead of keras, variable learning rate can work.  Therefore, I believe the issue comes from tf.keras.estimator.model_to_estimator\r\n\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef estimator_fn():\r\n    x_in = tf.keras.layers.Input(shape=[10])\r\n    x = tf.keras.layers.Dense(16, activation='relu')(x_in)\r\n    x_out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    model = tf.keras.models.Model(x_in, x_out)\r\n    lr = tf.Variable(0.1, trainable=False, dtype=tf.float32)\r\n    optimizer = tf.train.GradientDescentOptimizer(lr)\r\n    model.compile(loss='binary_crossentropy', optimizer= optimizer)\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model = model)\r\n    return estimator\r\n\r\ndef input_fn():\r\n    np.random.seed(100)\r\n    x = np.random.random((1024, 10))\r\n    y = np.random.randint(2, size=(1024, 1))\r\n    x = tf.cast(x, tf.float32)\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(1024)\r\n    return dataset\r\n  \r\nmodel_estimator = estimator_fn()\r\nmodel_estimator.train(input_fn=input_fn, steps=1000)\r\n```", "comments": ["@vbvg2008 You can make learning rate a variable, try using `keras.optimizers` instead. Just modified your code as below and executed it without running into error.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras import optimizers\r\n\r\ndef estimator_fn():\r\n    x_in = tf.keras.layers.Input(shape=[10])\r\n    x = tf.keras.layers.Dense(16, activation='relu')(x_in)\r\n    x_out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    model = tf.keras.models.Model(x_in, x_out)\r\n    lr = tf.Variable(0.1, trainable=False, dtype=tf.float32)\r\n    optimizer = keras.optimizers.SGD(lr, momentum=0.0, decay=0.0, nesterov=False)\r\n    model.compile(loss='binary_crossentropy', optimizer= optimizer)\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model = model)\r\n    return estimator\r\n\r\ndef input_fn():\r\n    np.random.seed(100)\r\n    x = np.random.random((1024, 10))\r\n    y = np.random.randint(2, size=(1024, 1))\r\n    x = tf.cast(x, tf.float32)\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(1024)\r\n    return dataset\r\n  \r\nmodel_estimator = estimator_fn()\r\nmodel_estimator.train(input_fn=input_fn, steps=1000)\r\n```", "@wt-huang Thanks for your reply.  But the thing is: I am trying to use mirrored strategy to do multi-gpu training. Only optimizers from tf.train are supported.  \r\n\r\nIf there can be some fix on the model_to_estimator side, it would be greatly appreciated.", "@vbvg2008 For multi-gpu training you can add the following to the code:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.keras.utils import multi_gpu_model\r\nimport numpy as np\r\n\r\ndef estimator_fn():\r\n    x_in = tf.keras.layers.Input(shape=[10])\r\n    x = tf.keras.layers.Dense(16, activation='relu')(x_in)\r\n    x_out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n    with tf.device('/cpu:0'):\r\n        model = tf.keras.models.Model(x_in, x_out)\r\n    num_gpu = 2\r\n    parallel_model = multi_gpu_model(model, gpus=num_gpu)\r\n    \r\n    lr = tf.Variable(0.1, trainable=True, dtype=tf.float32)\r\n    optimizer = tf.keras.optimizers.SGD(0.01, momentum=0.0, decay=0.0, nesterov=False)\r\n    parallel_model.model.compile(loss='binary_crossentropy', optimizer= optimizer)\r\n    estimator = tf.keras.estimator.model_to_estimator(keras_model =  parallel_model)\r\n    return estimator\r\n\r\ndef input_fn():\r\n    np.random.seed(100)\r\n    x = np.random.random((1024, 10))\r\n    y = np.random.randint(2, size=(1024, 1))\r\n    x = tf.cast(x, tf.float32)\r\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(1024)\r\n    return dataset\r\n  \r\nmodel_estimator = estimator_fn()\r\nmodel_estimator.train(input_fn=input_fn, steps=1000)\r\n```", "@wt-huang Hi, thanks again for the reply. However, I don't think your solution works.\r\n\r\nExecuting your fixed code gives me error message saying <ValueError: ('Could not interpret optimizer identifier:', <keras.optimizers.SGD object at 0x7fb12460f310>)>. I believe it is bad practice to mix the usage of keras and tf.keras.\r\n\r\nAlso, even I use tf.keras.optimizers.SGD instead, it gives me the error message saying <alueError: Only TensorFlow native optimizers are supported with DistributionStrategy.>.  I believe only Only optimizers from tf.train are supported. \r\n\r\nThe current tensorflow 's model_to_estimator is not compatible with  tf.train optimizers with variable learning rate.  I believe the right direction should be focusing on the model_to_estimator code. ", "@vbvg2008 thought I could trust copy/paste. Edited the above code, which you can use as a guideline. Essentially `tf.keras.utils.multi_gpu_model` could get what you need, refer to this [page](https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model) for more details.", "I'm having this same issue. It seems like there needs to be a way to retrieve a global_step tensor in a compatible manner. I think there are some advantages to the TensorFlor distributed estimator method that the keras multi_gpu_model doesn't capture (i.e. distributing the entire model over multiple GPUs [estimator]rather than running computations in parallel over multiple GPUs [multi_gpu_model]), so it would still be relevant to find a compatible method.", "I am having the same issue.  I would like to be able to use Keras to define a model, convert to TPUEstimator, and use an adaptive learning rate policy.  So far, I can't figure out a good solution and it all seems to point back to this issue.\r\n\r\nAny good workarounds?", "+1 multi_gpu_model is not a substitute for estimator (it's slow and not fully support multi-gpu), this issue should be reopen.", "Hi, \r\n\r\nDid anyone found a workaround ?"]}, {"number": 22617, "title": "No module named '_tensorflow_wrap_toco'", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (through pip)\r\n- **TensorFlow version (use command below)**:\r\n1.11.0\r\n- **Python version**:\r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNone (CPU only)\r\n- **GPU model and memory**:\r\nNone\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI saved my keras model to file and tried to use \"lite.TFLiteConverter.from_keras_model_file(...)\"  and \"tflite_model = converter.convert()\" to get a lite model, but got error \"No module named '_tensorflow_wrap_toco'\". This is only a \"tensorflow_wrap_toco.py\" in \"\\tensorflow\\contrib\\lite\\toco\\python\", and no \"_tensorflow_wrap_toco\" in that file.\r\nI have updated my tensorflow through \"pip install tensorflow --upgrade\"\r\n\r\n### Source code / logs\r\n```\r\nmodel= get_testing_model(input_shape=(160,140)) \r\nmodel.load_weights(keras_weights_file)\r\nmodel.save(\"kerasModel.h5\")\r\nconverter = lite.TFLiteConverter.from_keras_model_file(\"kerasModel.h5\")\r\ntflite_model = converter.convert() # bug happens here\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nLogs:\r\nFutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nUsing TensorFlow backend.\r\n2018-09-29 21:03:55.936260: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/wang/Desktop/OpenPoseApp/camera-openpose-keras/demo_camera.py\", line 325, in <module>\r\n    save_tf_lite_model()\r\n  File \"C:/Users/wang/Desktop/OpenPoseApp/camera-openpose-keras/demo_camera.py\", line 311, in save_tf_lite_model\r\n    tflite_model = converter.convert()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\", line 453, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 342, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\h5py\\\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\r\\n  from ._conv import register_converters as _register_converters\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\ProgramData\\\\Anaconda3\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n", "comments": ["Please refer this for exporting a [tf.keras file into tensorflow lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#exporting-a-tfkeras-file-)\r\n", "@ymodak  I've run the code in [ tf.keras file into tensorflow lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#exporting-a-tfkeras-file-), but get the same error.\r\n![qq 20181003100649](https://user-images.githubusercontent.com/25495799/46386448-16b3c480-c6f4-11e8-9e37-8c1822f65dff.png)\r\n", "Thanks for the information.\r\nCan you please try running your model with the tf-nightly build (pip install tf-nightly) instead of 1.11 and see if you get the same results?", "@ymodak  I've tried tf-nightly and tf-nightly_gpu, but it didn't work, got same error.", "I haven't been able to replicate the results using `virtualenv` and `pip` on Python 3.6.3. I ran the following steps to run the code and the [tf.keras file into tensorflow lite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/python_api.md#exporting-a-tfkeras-file-) example ran without error:\r\n```\r\nvirtualenv -p python3 venv-tf-nightly\r\nsource venv-tf-nightly/bin/activate\r\npip install tf-nightly\r\n```\r\nWhen I run `pip freeze` it says I'm on `tf-nightly==1.12.0.dev20180929`.\r\n\r\nCan you try running your code in a virtual environment with a fresh installation of TensorFlow (preferably through `pip`). It seems like there might be some issues with your installation. Otherwise, can you provide reproducible steps outlining how you installed your version of TensorFlow.", "+1. I was able to build successfully using tf-nightly too.", "I created a completely new python env from Pycharm and run \"pip install tf-nightly\", then got the same error.\r\nSo I tried to use tf.lite on Ubuntu 18.04, it succeed.\r\nWhen I open \"...site-packages/tensorflow/contrib/lite/toco/python\" on Ubuntu, I found a \"_tensorflow_wrap_toco.so\", but that file doesn't exist on my Win 10.\r\nI don't know if I haven't compile that file, or is it just not support for Windows?\r\nAnyway, I avoided that bug by using linux.\r\n", "Same issue on windows 10", "I have also the same issue on Windows 10. I set up a new virtualenv and installed tensorflow nightly with `pip install tf-nightly`. The code I ran to convert my model is:\r\n```\r\nimport tensorflow as tf\r\n\r\ngraph_def_file = \"graph_optimized.pb\"\r\ninput_arrays = [\"Placeholder\"]\r\noutput_arrays = [\"output\"]\r\n\r\nconverter = tf.contrib.lite.TFLiteConverter.from_frozen_graph(\r\n    graph_def_file, input_arrays, output_arrays, input_shapes={\"Placeholder\" : [1, 227, 227, 3]})\r\ntflite_model = converter.convert()\r\nopen(\"save_path/converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\n```\r\nI get the same error with:\r\n `import _tensorflow_wrap_tocoModuleNotFoundError: No module named  '_tensorflow_wrap_toco'`", "@Noltibus Thanks for opening a new issue and expressing your problem. I will close this issue so that we can focus on the newly created one #22897 .", "i have the same problem ,when i run in Unbuntu \r\n`Traceback (most recent call last):\r\n  File \"quant.py\", line 17, in <module>\r\n    mobilenet_tflite_file.write_bytes(converter.convert())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 317, in toco_convert_graph_def\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/lite/python/convert.py\", line 135, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'2018-10-12 13:08:32.732548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1109] Converting unsupported operation: TFLite_Detection_PostProcess\\n2018-10-12 13:08:32.738803: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1182] Unable to determine output type for op: TFLite_Detection_PostProcess\\n2018-10-12 13:08:32.783094: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:08:32.844203: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1352 arrays (0 quantized)\\n2018-10-12 13:08:33.944841: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:08:33.948440: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 111 operators, 220 arrays (0 quantized)\\n2018-10-12 13:08:33.953645: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 11520000 bytes, theoretical optimal value: 11520000 bytes.\\n2018-10-12 13:08:33.954096: I tensorflow/contrib/lite/toco/toco_tooling.cc:397] Estimated count of arithmetic ops: 2.49483 billion (note that a multiply-add is counted as 2 ops).\\n2018-10-12 13:08:33.954530: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954549: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954557: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954565: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954572: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_2_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954579: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954588: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_3_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954597: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954606: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_4_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954615: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954624: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_5_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954633: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954641: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_6_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954650: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954659: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_7_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954668: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954676: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_8_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954685: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954694: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_9_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954703: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954712: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_10_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954721: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954730: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_11_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954739: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954747: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_12_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954756: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_depthwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954765: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_13_pointwise/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954774: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_2_1x1_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954783: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_2_3x3_s2_512/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954792: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_3_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954801: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_3_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954810: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_4_1x1_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954819: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_4_3x3_s2_256/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954828: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_1_Conv2d_5_1x1_64/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954836: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output FeatureExtractor/MobilenetV1/Conv2d_13_pointwise_2_Conv2d_5_3x3_s2_128/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954845: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954854: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_0/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954863: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954872: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_1/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954881: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954890: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_2/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954899: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954906: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_3/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954913: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954921: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_4/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954930: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/BoxEncodingPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954939: W tensorflow/contrib/lite/toco/tflite/export.cc:423] FAKE_QUANT operation {FakeQuant operator with output BoxPredictor_5/ClassPredictor/act_quant/FakeQuantWithMinMaxVars} was not converted. If running quantized make sure you are passing --inference_type=QUANTIZED_UINT8 and values for --std_values and --mean_values.\\n2018-10-12 13:08:33.954950: F tensorflow/contrib/lite/toco/tflite/export.cc:460] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TFLiteConverter(). Here is a list of operators for which  you will need custom implementations: TFLite_Detection_PostProcess.\\nAborted (core dumped)\\n'\r\nNone\r\n`", "@cjr0106: Please file a new issue with reproducible instructions.", "You forget to install tf_nightly. I am using windows 10. In CMD, please input this:\r\n`pip3 install tf_nightly`"]}, {"number": 22616, "title": "Comment fix: cudnn output tensor data_format conversion.", "body": "The output tensor data_format conversion in core/kernels/conv_ops.cc should be \"from NCHW to NHWC\" since the called function is functor::NCHWToNHWC<GPUDevice, T, 4>().", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 22615, "title": "AudioSpectrogram not support batch_size mode and no fbank op ", "body": "AudioSpectrogram not support audios with `batch_size` dim.\r\n\r\n```\r\nValueError: Shape must be rank 2 but is rank 3 for 'AudioSpectrogram' (op: 'AudioSpectrogram') with input shapes: [32,?,1].\r\n```\r\n\r\n\r\nThere have `Mfcc` and `Spectrogram` api but no `Fbank` api export .", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "OS Platform and Distribution\r\ncentos 7\r\n\r\nTensorFlow installed from\r\nsource\r\n\r\nTensorFlow version\r\ntf.VERSION = 1.9.0\r\ntf.GIT_VERSION = v1.9.0-0-g25c197e023\r\ntf.COMPILER_VERSION = v1.9.0-0-g25c197e023\r\n\r\nBazel version\r\n\r\nCUDA/cuDNN version\r\ncuda 9.0\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nN/A\r\nMobile device\r\nN/A", "Looks like @petewarden added this OP.", "@gunan  Does not find `melfbank_op` in `core/kernels`.  And there is also no introduction about audio_ops in c++ API.", "Since this is a feature request that we aren't likely to get to soon, closing for now."]}, {"number": 22614, "title": "what is operator name for python corresponding to kReducePrecision", "body": "I am wondering what is the operator in python corresponding to the opcode kReducePrecision. Thanks for any hint.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Have I written custom code: yes\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device: N/A", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22613, "title": "Is official 1.11.0 pip package for windows build by bazel or cmake?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: vs2017 15.8 / msvc 19.15.26726\r\n- **CUDA/cuDNN version**: 10.0.130_411.31/7.3.0.29\r\n- **GPU model and memory**: 1080ti 11GB\r\n- **Exact command to reproduce**: pip install tensorflow-gpu\r\n\r\n### Describe the problem\r\n\r\nIn 1.10 release notes, you say `Starting from TensorFlow 1.11, Windows builds will use Bazel. Therefore, we will drop official support for cmake.`, but table from https://www.tensorflow.org/install/source_windows indicate that 1.11 is build by cmake.\r\n\r\nThe cuDNN version described in the table from https://www.tensorflow.org/install/source_windows or https://www.tensorflow.org/install/source still 7, not 7.2 or 7.x.x. Which version of cudnn do you used to build pip package?\r\n\r\nNvidia's website didn't provide cudnn 7.2 for cuda 9.0 download, please provide more clarify documentation.\r\n\r\nThanks\r\n\r\n### Source code / logs\r\n\r\n```\r\npip install tensorflow-gpu==1.11.0\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.11.0-rc2-4-gc19e29306c' 1.11.0\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "@tensorflowbutler \r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A", "We have switched over to using bazel rather than cmake for Windows. The cmake build will still be supported by the community. \r\n\r\nI have updated the [release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.11.0) We had mentioned this in the 1.10 release, but forgot to mention it now. Thanks @fo40225 for bringing this to our attention."]}, {"number": 22612, "title": "A Error about \"from tensorflow.examples.tutorials.mnist import input_data\"", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code:YES ,I have\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:windows10 64 bit\r\n- **Mobile device  if the issue happens on mobile device**:no ,it's not work on Mobile device\r\n- **TensorFlow installed from (source or binary)**:installed from command\r\n- **TensorFlow version (use command below)**:tensorflow 1.10.0\r\n- **Python version**:python3.6.5\r\n- **Bazel version (if compiling from source)**:no,it's not compiling from source \r\n- **GCC/Compiler version (if compiling from source)**:no,it's not compiling from source \r\n- **CUDA/cuDNN version**:CUDA9.0/cuDNN7.1\r\n- **GPU model and memory**:GTX1080/8192\r\n- **Exact command to reproduce**:command\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nIn python, I type:\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nit got error as follows:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-6-127a3aab68d9> in <module>()\r\n----> 1 from tensorflow.examples.tutorials.mnist import input_data\r\n      2 import tensorflow as tf\r\n      3 mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\__init__.py in <module>()\r\n     19 from __future__ import print_function\r\n     20 \r\n---> 21 from tensorflow.examples.tutorials.mnist import input_data\r\n     22 from tensorflow.examples.tutorials.mnist import mnist\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py in <module>()\r\n     28 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n     29 import tensorflow as tf\r\n---> 30 from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n     31 # pylint: enable=unused-import\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\__init__.py in <module>()\r\n     35 from tensorflow.contrib import crf\r\n     36 from tensorflow.contrib import cudnn_rnn\r\n---> 37 from tensorflow.contrib import data\r\n     38 from tensorflow.contrib import deprecated\r\n     39 from tensorflow.contrib import distribute\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\data\\__init__.py in <module>()\r\n     76 from tensorflow.contrib.data.python.ops.counter import Counter\r\n     77 from tensorflow.contrib.data.python.ops.enumerate_ops import enumerate_dataset\r\n---> 78 from tensorflow.contrib.data.python.ops.error_ops import ignore_errors\r\n     79 from tensorflow.contrib.data.python.ops.get_single_element import get_single_element\r\n     80 from tensorflow.contrib.data.python.ops.grouping import bucket_by_sequence_length\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\error_ops.py in <module>()\r\n     18 from __future__ import print_function\r\n     19 \r\n---> 20 from tensorflow.contrib.data.python.ops import contrib_op_loader  # pylint: disable=unused-import\r\n     21 from tensorflow.contrib.data.python.ops import gen_dataset_ops\r\n     22 from tensorflow.python.data.ops import dataset_ops\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\contrib_op_loader.py in <module>()\r\n     22 \r\n     23 _dataset_ops = loader.load_op_library(\r\n---> 24     resource_loader.get_path_to_datafile(\"../../_dataset_ops.so\"))\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py in load_op_library(path)\r\n     54       return None\r\n     55   path = resource_loader.get_path_to_datafile(path)\r\n---> 56   ret = load_library.load_op_library(path)\r\n     57   assert ret, 'Could not load %s' % path\r\n     58   return ret\r\n\r\nD:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\load_library.py in load_op_library(library_filename)\r\n     54     RuntimeError: when unable to load the library or get the python wrappers.\r\n     55   \"\"\"\r\n---> 56   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\n     57 \r\n     58   op_list_str = py_tf.TF_GetOpList(lib_handle)\r\n\r\nNotFoundError: D:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops\\..\\..\\_dataset_ops.so not found\r\n\r\nand It also pops up a window saying:\r\nThe procedure entry point ?addcleanup@arenaimpl@internal@protobuf@google@@QEAAXPEAXP6AX0@Z@Z could not be located in the dynamic link library _pywarp_tensorflow_internal.pyd\r\n\r\nwhat should I do to solve it?by the way ,I Use it OK in tensorflow 1.5.have it changed in tensorflow?where  can I find guid about it?\r\nplease help ! thank you very much in advance!\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@noticeable Hi, thanks for  your post. Can you delete the file _dataset_ops.so from the below path and try it again. Thank you!\r\n\r\nNotFoundError: D:\\software\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\data\\python\\ops...._dataset_ops.so not found", "@noticeable Does the issue still exist ?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "its having version issue. \r\ninstall this and your problem would be solved\r\npip install tensorflow==1.14"]}, {"number": 22611, "title": "compiler/tests/stateless_random_ops_test.py:testRandomNormalIsFinite doesn't", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: n/a\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: n/a\r\n- **TensorFlow version (use command below)**: n/a\r\n- **Python version**: n/a\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\n@hawkinsp wrote a test called [`testRandomNormalIsFinite`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tests/stateless_random_ops_test.py#L94), but it uses `stateless_random_uniform` instead of `stateless_random_normal`.\r\n\r\n### Source code / logs\r\n\r\nN/A.", "comments": ["Oops! Good catch. I'll fix the test."]}, {"number": 22610, "title": "Here is a list of operators for which  you will need custom implementations: ListDiff", "body": "i use tf.keras to save my h5 model, and convert to tflite, this op can't be supported", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "i have solved this bug\uff0c i will close this issue", "> i have solved this bug\uff0c i will close this issue\r\n\r\nCould you tell me how you solved the problem? I've got the same but i cant locate where Listdiff had been used"]}, {"number": 22609, "title": "tf.keras.BatchNormalization  Cannot be quantified", "body": "Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "i have solved this bug ", "how?", "@dlml I got the same problem. How did you solve it?"]}, {"number": 22608, "title": "tensorflow new installation method", "body": "about a month agp, there are two ways to install tensorflow one of which is through anaconda. But now when i visit the tensorflow website, anaconda is not an option already. and now that I have changed laptop , I have to reinstall it. Installing under windows, i have to install bazel, MSYS2, visual c++ build tools 2015 which I have never heard of when I initially installed in my old laptop. Do I have to install bazel etc.?\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\nAnd must i install the dependencies listed in the setup.py file under REQUIRED_PACKAGES ? if so, how do i go about it?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@hadias  The link you are referring is to build tensorflow from source. Please go through [this](https://www.tensorflow.org/install/pip?lang=python3) link which has steps to install tensorflow on Windows. This is installing tensorflow with pip.", "@hadias Are you looking to install tensorflow from binary or to build it from source ?", "It sounds like you're trying to install the binary and just missed the \"Windows\" and \"Conda\" buttons on the pip install page. \r\n\r\nThat `source_windows` page is for advanced users. Only if you want to compile from scratch on windows, and it sounds like that's not what you want.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22607, "title": "attention score shape", "body": "from equation 4, the attention score should not be a rank-3 tensor object.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "updated code to sync with the pseudo code and equations in the figure. \r\n\r\nIn short, attention score should not be rank-3 tensor, but should be a rank-2 tensor if applying tf.squeeze", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 22606, "title": "[INTEL MKL] Fix concat primitive descriptor creation bug", "body": "When there is tensor format mismatch between tf tensor and mkl tensor, the primitive descriptor creation will error out. This fix allows MKL concat op to decide what format to use on its own.\r\nIt first creates concat w/o dst_md, then queries dst_pd from concat.", "comments": ["Hi @caisq,  will you be able to review this PR this week? It only has a few lines of change. Thanks.", "Pinging @tatianashp for a review.", "Hi @case540, \r\nCould you start the merging process of this PR?  I have another PR that is dependent on this one.  \r\nI notice that the automatic merge is blocked by Windows Bazel GPU internal CI failure.  Our code change shouldn't break any GPU CI, because this code is inside \"#ifdef INTEL_MKL\" block. \r\nThanks", "@case540, just want to follow-up, it's been ready to pull for a few days.", "Hello,\r\n\r\nAfter applying this patch on my TF build, inference crashes somewhere else (using the same method so I'm pretty sure it's related)\r\n\r\n```\r\nAbortedError (see above for traceback): Operation received an exception:Status: 5, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:652\r\n\t [[Node: conv1/BiasAdd = _MklConv2DWithBias[T=DT_FLOAT, _kernel=\"MklOp\", data_format=\"NHWC\", padding=\"SAME\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_1_0_0, conv1/kernel/read, conv1/bias/read, DMT/_0, DMT/_1, DMT/_2)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/usr/lib/python3/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 5, message: could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:652\r\n\t [[Node: conv1/BiasAdd = _MklConv2DWithBias[T=DT_FLOAT, _kernel=\"MklOp\", data_format=\"NHWC\", padding=\"SAME\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_input_1_0_0, conv1/kernel/read, conv1/bias/read, DMT/_0, DMT/_1, DMT/_2)]]\r\n```\r\n\r\n@HuiyangFei you were talking about a second PR, could you link this one here ?"]}, {"number": 22605, "title": "[Closed] Fix concat primitive descriptor creation bug", "body": "When there is tensor format mismatch between tf tensor and mkl tensor, the primitive descriptor creation will error out. This fix allows MKL concat op to decide what format to use on its own. \r\nIt first creates concat w/o dst_md, then queries dst_pd from concat.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 22604, "title": "[Closed] Fix concat primitive descriptor creation error", "body": "This fix allows MKL concat op to decide what tensor format to use automatically. It first creates concat w/o dst_md, then queries dst_pd from concat.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@HuiyangFei Can you sign the CLA please?", "@caisq I should have CLA, and actually used the same account to submit PR before, https://github.com/tensorflow/tensorflow/pull/21694\r\n\r\nLet me check what's going on with my CLA.\r\n", "@HuiyangFei Is it possible that your github email address and the one you used in your git commit are different? ", "@caisq I created a new P #22606, which has CLA working properly. Thanks for checking. "]}, {"number": 22603, "title": "[BUG] possible memory corruption when multiplying large matices", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (through pip)\r\n- **TensorFlow version (use command below)**:\r\n1.11.0\r\n- **Python version**:\r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.1.85\r\n- **GPU model and memory**:\r\nTITAN Xp 12GB\r\n- **Exact command to reproduce**:\r\n\r\nmore system information in [tf_env.txt](https://github.com/tensorflow/tensorflow/files/2429099/tf_env.txt)\r\n\r\n\r\n### Describe the problem\r\nFor large matrix multiplications, part of the result is not correctly computed or its memory is partially overwritten. Not sure what exactly the problem is. I created as small as possible reproducible code that can be run and visualise the problem. \r\n\r\nHere are also visual outputs that I run. The result should be array of constant value 3.\r\nwith tf.tile:\r\nsampling = 6\r\n![sampling = 6](https://user-images.githubusercontent.com/8818326/46223432-45741880-c34b-11e8-8d0e-88c4a4682073.png)\r\nsampling = 8\r\n![sampling = 8](https://user-images.githubusercontent.com/8818326/46223453-591f7f00-c34b-11e8-9fcc-6edf42434fc3.png)\r\nwith broadcasting:\r\nsampling = 6\r\n![sampling = 6](https://user-images.githubusercontent.com/8818326/46223487-73595d00-c34b-11e8-9314-179848bb2f96.png)\r\nsampling = 8\r\n![sampling = 8](https://user-images.githubusercontent.com/8818326/46223500-7b190180-c34b-11e8-9122-31fb2e0991c7.png)\r\n\r\n### Source code / logs\r\nby varying the sampling variable from 6 to 12 (OOM) you can observe the behaviour by plotting the result (as done in the code)\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\r\n\r\n\r\n# On Nvidia Titan XP 12GB:\r\n# 1) MEMORY BUG - for shape [960, 480] and sampling > 6 the output \r\n#                 variable is partially overwritten (see figure) \r\n# 2) OOM error when sampling >= 12\r\nsampling = 8  \r\nshape = [480, 960]\r\n\r\n# \"random\" input vectors\r\ni1 = np.ones(shape=(shape[0]*shape[1], 1, 3), dtype=np.float)\r\ni2 = np.ones(shape=(1, sampling**3, 3), dtype=np.float)\r\n\r\ninput1 = tf.placeholder(shape=(i1.shape[0], i1.shape[1], i1.shape[2]), dtype=tf.float32)\r\ninput2 = tf.placeholder(shape=(i2.shape[0], i2.shape[1], i2.shape[2]), dtype=tf.float32)\r\n\r\n# Operation v1:  \r\n# tile before multiplying to have same dimensions\r\n# vectors (batch size, shape[0]*shape[1], sampling^2 , 3) and\r\n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\r\ni1_tile = tf.tile(input1, (1, sampling**3, 1))\r\ni2_tile = tf.tile(input2, (shape[0]*shape[1], 1, 1))\r\nres1 = tf.reduce_sum(i1_tile * i2_tile, axis=-1)\r\n\r\n# Operation v2:  \r\n# use broadcasting to multiply \r\n# vectors i1 = (batch size, shape[0]*shape[1],     1     , 3) and\r\n#         i2 = (batch size,         1        , sampling^3, 3) \r\n# to get (after summation) vector (barch size, shape[0]*shape[1], sampling^3)\r\nres2 = tf.reduce_sum(input1*input2, axis=-1)\r\n\r\nres = res1 #res2\r\n\r\nwith tf.Session() as sess:\r\n    pred_out = sess.run(res, feed_dict={input1: i1, input2: i2})\r\n\r\n# result should be image of size shape with all values equal to 3\r\nfig = plt.figure()\r\npred_out = np.reshape(np.mean(pred_out, axis=-1), (shape[0], shape[1]))\r\nimgplot = plt.imshow(pred_out)\r\nimgplot.set_cmap('jet')\r\ndivider = make_axes_locatable(plt.gca())\r\ncax1 = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\r\nfig.colorbar(imgplot, cax=cax1)\r\nplt.clim(0, 3.1)\r\n\r\nplt.savefig(\"result.png\", dpi=100)\r\n\r\n```\r\n\r\n**Console output of the run of the script:**\r\n/home/mifs/tv278/.virtualenvs/tf-keras/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-09-28 18:22:45.596062: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-09-28 18:22:45.865097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.74GiB\r\n2018-09-28 18:22:45.865123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-28 18:22:46.099514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-28 18:22:46.099541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-28 18:22:46.099546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-28 18:22:46.099791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11359 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:17:00.0, compute capability: 6.1)\r\n\r\n\r\n\r\n", "comments": ["could this be related to: https://github.com/tensorflow/tensorflow/issues/22123 ?", "probably related to  #19155", "Seems like the same problem as #19155", "Nagging Assignees @chsigg, @azaks2: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I've reproduced the issue. The problem seems to be with the reduction, not with the matrix multiplication. I'm closing this issue and will provide updates in #22123."]}, {"number": 22602, "title": "[BUG] Tensorflow branch 1.11 test. ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.11 (git branch 1.11)\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 9.0/7.3\r\n- **GPU model and memory**: Titan Xp\r\n- **Exact command to reproduce**: ```bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...```\r\n\r\n### Describe the problem\r\nFound a missing imports (pandas and dask) in file tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py at line 295.\r\n\r\n### Source code / logs\r\nError1 log:\r\n```\r\n//tensorflow/contrib/learn:data_feeder_test                              FAILED in 1.9s\r\n======================================================================\r\nERROR: test_dask_data_feeder (__main__.SetupPredictDataFeederTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py\", line 295, in test_dask_data_feeder\r\n    x = pd.DataFrame(\r\nNameError: global name 'pandas' is not defined\r\n```\r\n\r\nSolution1:\r\nin file tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py add:\r\n```\r\nimport pandas as pd\r\nimport dask.dataframe as dd\r\n```\r\n\r\nError2 log:\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/contrib/learn:data_feeder_test\r\n-----------------------------------------------------------------------------\r\nWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:307: __init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease feed input to tf.data to support dask.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nEWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:246: __init__ (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tensorflow/transform or tf.data.\r\nWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:340: check_array (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease convert numpy dtypes explicitly.\r\n...2018-09-28 16:50:14.407888: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n....WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\n.............WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nE.......WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\n..........WARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py:363: setup_predict_data_feeder (from tensorflow.contrib.learn.python.learn.learn_io.data_feeder) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tensorflow/transform or tf.data.\r\nWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:215: extract_dask_data (from tensorflow.contrib.learn.python.learn.learn_io.dask_io) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease feed input to tf.data to support dask.\r\nWARNING:tensorflow:From /home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py:217: extract_pandas_data (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease access pandas data directly.\r\n....\r\n======================================================================\r\nERROR: test_dask_data_feeder (__main__.DataFeederTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py\", line 307, in test_dask_data_feeder\r\n    df = data_feeder.DaskDataFeeder(x, y, n_classes=2, batch_size=2)\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 306, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\", line 872, in __init__\r\n    self._output_dtype = _check_dtype(self._y.dtypes[self._y_columns])\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\", line 281, in _check_dtype\r\n    if dtypes.as_dtype(dtype) == dtypes.float64:\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/framework/dtypes.py\", line 681, in as_dtype\r\n    return _INTERN_TABLE[type_value]\r\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py\", line 1045, in __hash__\r\n    ' hashed'.format(self.__class__.__name__))\r\nTypeError: 'Series' objects are mutable, thus they cannot be hashed\r\n\r\n======================================================================\r\nERROR: test_dask_data_feeder (__main__.SetupPredictDataFeederTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder_test.py\", line 307, in test_dask_data_feeder\r\n    df = data_feeder.DaskDataFeeder(x, y, n_classes=2, batch_size=2)\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 306, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\", line 872, in __init__\r\n    self._output_dtype = _check_dtype(self._y.dtypes[self._y_columns])\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\", line 281, in _check_dtype\r\n    if dtypes.as_dtype(dtype) == dtypes.float64:\r\n  File \"/home/nikita/.cache/bazel/_bazel_nikita/992bfa23a6d33fa546f39f500a08968c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/data_feeder_test.runfiles/org_tensorflow/tensorflow/python/framework/dtypes.py\", line 681, in as_dtype\r\n    return _INTERN_TABLE[type_value]\r\n  File \"/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py\", line 1045, in __hash__\r\n    ' hashed'.format(self.__class__.__name__))\r\nTypeError: 'Series' objects are mutable, thus they cannot be hashed\r\n\r\nRan 43 tests in 0.184s\r\n\r\nFAILED (errors=2)\r\ntf.estimator package not installed.\r\n```\r\n\r\n\r\nWith second error I don`t know what to do.", "comments": ["@ispirmustafa @martinwicke do we still support contrib/learn?", "No. It's been deprecated for a while.\n", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I will close this issue as it is an issue in a deprecated component."]}, {"number": 22601, "title": "Tensorflow 1.11.0 incompatible with keras2.2.2?", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.11.0\r\n\r\n- **Python version**:\r\n3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.17.2\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 10\r\ncuDNN 7.3\r\n\r\n- **GPU model and memory**:\r\nGTX1070\r\n\r\n- **Exact command to reproduce**:\r\n```\r\npip3 --no-cache-dir install ./tensorflow-1.11.0-cp35-cp35m-linux_x86_64.whl --user\r\n```\r\n\r\nThen  pip complains that\r\n```\r\nkeras 2.2.2 has requirement keras-applications==1.0.4, but you'll have keras-applications 1.0.5 which is incompatible.\r\nkeras 2.2.2 has requirement keras-preprocessing==1.0.2, but you'll have keras-preprocessing 1.0.3 which is incompatible\r\n```\r\nBut keras 2.2.2 is already the newest and it requires  keras-applications 1.0.4 and  keras-preprocessing 1.0.2.\r\n\r\nWhy does tensorflow require incompatible versions of keras-applications and keras-preprocessing? \r\n\r\nI reinstalled keras and it removed keras-applications 1.0.5 and and keras-processing 1.0.3 and installed \r\n keras-applications 1.0.4 and keras-preprocessing 1.0.2 instead . I did a few tests with tensorflow  and it worked without problem.\r\n\r\nIf In tensorflow/tools/pip_package/setup.py change line 54 and 55 to\r\n\r\n```\r\n'keras_applications >= 1.0.4',\r\n'keras_preprocessing >= 1.0.2',\r\n```\r\nbefore bazel build pip package then I think pip3 install would happily go along without complaint.   Are these incompatible requirements really necessary??\r\n\r\n\r\n", "comments": ["Running this on Windows and I have this same notification as well when installing these versions of keras and tensorflow", "tensorflow 1.11.0 has requirement keras-applications>=1.0.5, but you'll have keras-applications 1.0.4 which is incompatible.\r\nbut when i install 1.0.5 it required for 1.0.4", "Is this still an issue?"]}, {"number": 22600, "title": "print error information, when the os is not supported", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "\uff20asimshankar I think that's ok"]}, {"number": 22599, "title": "Skip record reader for cloud file system", "body": "Addressing this https://github.com/tensorflow/tensorflow/issues/22598\r\n\r\nThe behaviour of gcs(google cloud file storage) file system is different than local file system.\r\n\r\nas\r\n```\r\n tf.gfile.Open('local_empty.tfrecord').read(100000000) == ''\r\n```\r\nwhile \r\n```\r\ntf.gfile.Open('gcs_empty_file.tfrecord').read(1000000)=='Out[11]: \"<?xml version='1.0'\r\n encoding='UTF-8'?><Error>\r\n<Code>InvalidRange</Code>\r\n<Message>The requested range cannot be satisfied. \r\n</Message>\r\n<Details>bytes=some number</Details></Error>\"'\r\n```\r\nso when we use tf.data.TFRecordDataset to process the tfrecord in the gcs, there would be\r\nDataLossError\r\n", "comments": ["@mrry  do you mind help me have a look at this  `//tensorflow/contrib/data/python/kernel_tests:writer_ops_test` my local bazel build have no problems. ", "Look at what? If you have a test failure log, that would be helpful.", "sorry here https://source.cloud.google.com/results/invocations/ee0efbcc-a378-40e1-8be5-56bf6be6831b/targets/%2F%2Ftensorflow%2Fcontrib%2Fdata%2Fpython%2Fkernel_tests:writer_ops_test/log \r\n\r\n```\r\n======================================================================\r\nERROR: testWrite (__main__.TFRecordWriterTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.py\", line 71, in testWrite\r\n    for i, r in enumerate(tf_record.tf_record_iterator(self._outputFilename())):\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/python/lib/io/tf_record.py\", line 163, in tf_record_iterator\r\n    compat.as_bytes(path), 0, compat.as_bytes(compression_type), status)\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 526, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: eof\r\n======================================================================\r\nERROR: testWriteZLIB (__main__.TFRecordWriterTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.py\", line 84, in testWriteZLIB\r\n    tf_record.tf_record_iterator(self._outputFilename(), options=options)):\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/python/lib/io/tf_record.py\", line 163, in tf_record_iterator\r\n    compat.as_bytes(path), 0, compat.as_bytes(compression_type), status)\r\n  File \"/b/f/w/run/bazel-out/k8-opt/bin/tensorflow/contrib/data/python/kernel_tests/writer_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 526, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: eof\r\n----------------------------------------------------------------------\r\nRan 7 tests in 0.194s\r\n```", "This problem seems like it should be fixed in the `GcsFileSystem` object, so that all reader implementations will benefit. Can you look into doing that?\r\n\r\nSince the problem seems to be specific to GCS, I'm going to transfer the review to @saeta, who is more familiar with that code (or who at least might know who knows it best).", "Thanks for connecting me, yeah I can look into it. It would be ever better if there is suggestions from the gcs filesystem people. ", "Closing because this isn't the correct fix. We should fix the filesystem's interpretation of that error message instead.\r\n\r\nI'll reassign the issue.", "@yupbank For context, we'd prefer to fix the implementation here:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/platform/cloud\r\n\r\nTo make what it returns for reading an empty file the same as other filesystems.", "make sense, thank you"]}, {"number": 22597, "title": "Feature request: Documentation tf.keras.applications missing for TF 1.11", "body": "\r\n### Describe the problem\r\nThe documentation for tf.keras.applications is missing for version 1.11 and I am getting a page not found error (see below image). Documentation is available for 1.10. While both have the same documentation, 1.11 comes with `mobilenet_V2` which was not available in 1.10.\r\n\r\n### Source code / logs\r\n![image](https://user-images.githubusercontent.com/25213730/46215172-50ef2180-c302-11e8-9cd4-8e12ba62c1e1.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "I can confirm. [Link](https://www.tensorflow.org/api_docs/python/tf/keras/applications) @MarkDaoust any ideas? @sibyjackgrove you can ignore the @tensorflowbutler request for more information.", "Hi @sibyjackgrove,\r\n\r\nYes. This is a known issue.\r\n\r\nI'm working on a fix so this doesn't happen again.\r\nI'll just go patch it in the mean time.\r\n\r\nThe problem is that tf.keras.preprocessing and tf.keras.applications are now just wrappers for the keras_preprocessing and keras_applications packages.", "Thanks, for the update.", "@MarkDaoust Hi Mark, any update on the fix ? Please share the PR # if it's created. Thanks !", "They're partially fixed.\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/applications\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing\r\n\r\nThose main pages don't 404 anymore, but the contents could be better.", "Hi @sibyjackgrove ! The above links are working fine now in TF 2.6 .  You can find the same  from here .[Link1](https://www.tensorflow.org/api_docs/python/tf/keras/applications) . Closing this issue as it seem to be resolved  in latest versions. Feel free to create a new issue if you face any problems further. Thank you!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22596, "title": "TFGAN support in tensorflow 2.0", "body": "Hi everyone,\r\n\r\nas written in the [roadmap](https://www.tensorflow.org/community/roadmap) `tf.contrib` will be deprecated/removed and some parts of it will be merged inside tensorflow core or moved to a separate repository and some removed.\r\n\r\nSince in my team we're currently using tensorflow and TFGAN (and we like it a lot, we even hold a tutorial at EuroSciPy 2018 about GANs in Tensorflow [[1]]) we're looking closely at the Tensorflow 2.0 release and we would like to move our stack to this new version in the fastest and smoothest possible way.\r\n\r\nIn order to do this, a couple of questions (that I can't ask on StackOverflow, since you're the only one that knows what will happen on Tensorflow 2.0 and the community, outside of the maintainers, can't help a lot in this phase):\r\n\r\n- Will TFGAN be moved to a separate project? If so, will the repo be available at the day-one?\r\n- Since `tf.layer` will be removed in favor of `tf.keras.layers`: the implementation of our models into TFGAN, that are currently defined using `tf.layers` will work just switching to `tf.keras.layers` or have we to refactor all the existing codebase?\r\n\r\n\r\nThank you\r\n\r\n[1]: https://github.com/zurutech/gans-from-theory-to-production", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@galeone Hi, you can find more details and ask question at https://github.com/tensorflow/community/pull/18/files#diff-eef567fcebc6a5988c0fa92c7bb38526R192 \r\n\r\nI'll close the issue, please feel free to reopen it if needed."]}, {"number": 22595, "title": "is it tensorflow example bug ?", "body": "\r\n\r\n### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): window10 and jupyter\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone X\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.11\r\nPython version: 3.6\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 9.1 / 7.1\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\n\r\n### Describe the problem\r\nI used the tensorflow github example form [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb#scrollTo=Cffg2i257iMS) change the CNN model \r\n`img = tf.keras.applications.inception_v3.preprocess_input(img)\r\n--->img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)`\r\n\r\n`image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\r\n---->image_model =tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet')`\r\n\r\nthe output\r\n```\r\nEpoch 1 Batch 0 Loss nan\r\nEpoch 1 Batch 100 Loss nan\r\nEpoch 1 Batch 200 Loss nan\r\nEpoch 1 Batch 300 Loss nan\r\nEpoch 1 Loss nan\r\n```\r\ni have tried to change the learning rate and loss function,even others CNN model (vgg19,vgg16) but it was same \r\n\r\n### Source code / logs\r\nthe Source code is tensorflow example [from here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb#scrollTo=Cffg2i257iMS)\r\nyou just need to change the CNN model and wait a few minute  run it.", "comments": ["@drpngx Hi, could you please look into this issue.", "@harshini-gadige did you try without modifications?", "@drpngx Yes I tried the original one and it took long time to load the data and then the system crashed.", "@MarkDaoust Is that a known documentation problem?", " @drpngx  So this example only can use InceptionV3 Model? have any solution?\r\n\r\nthanks!\r\n\r\n", "I think it's probably that the docs need to be refreshed. I'm not sure what's the plan for this.", "@MarkDaoust Hi, could you please look into this ?", "hi, this problem was figure out in new tensorflow 1.12 , but it stall have another problem.\r\nIn this code will print error\r\n` \"provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor\"`\r\nadd it happen in use Vgg16 model.\r\n\r\nThis is the  code[ here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb#scrollTo=io7ws3ReRPGv)\r\nchange the CNN model\r\n```\r\nimg = tf.keras.applications.inception_v3.preprocess_input(img) --->img = tf.keras.applications.vgg16.preprocess_input(img)\r\n\r\nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet') ---->image_model =tf.keras.applications.Vgg16(include_top=False, weights='imagenet')\r\n\r\n img = tf.image.resize_images(img, (299, 299))---> img = tf.image.resize_images(img, (224, 224))\r\n\r\n```\r\nchange the image features_shape & attention_features_shape\r\n```\r\nfeatures_shape = 2048 -->512\r\nattention_features_shape = 64-->81\r\n```\r\n\r\n\r\nproblem code in here\r\n```\r\n# captions on the validation set\r\nrid = np.random.randint(0, len(img_name_val))\r\nimage = img_name_val[rid]\r\nreal_caption = ' '.join([index_word[i] for i in cap_val[rid] if i not in [0]])\r\nresult, attention_plot = evaluate(image)\r\n\r\nprint ('Real Caption:', real_caption)\r\nprint ('Prediction Caption:', ' '.join(result))\r\nplot_attention(image, result, attention_plot)\r\n# opening the image\r\n`Image.open(img_name_val[rid])`\r\n```\r\n and error here\r\n\r\n```\r\nFallbackException                        Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6494         _ctx._context_handle, _ctx._eager_context.device_name, \"Reshape\",\r\n-> 6495         name, _ctx._post_execution_callbacks, tensor, shape)\r\n   6496       return _result\r\n\r\n_FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-31-98d70c233c4f> in <module>()\r\n      2 image = img_name_val[rid]\r\n      3 real_caption = ' '.join([index_word[i] for i in cap_val[rid] if i not in [0]])\r\n----> 4 result, attention_plot = evaluate(image)\r\n      5 \r\n      6 print ('Real Caption:', real_caption)\r\n\r\n<ipython-input-29-55980383ccd6> in evaluate(image)\r\n      4     hidden = decoder.reset_state(batch_size=1)\r\n      5 \r\n----> 6     temp_input = tf.expand_dims(load_image(image)[0], 0)\r\n      7     img_tensor_val = image_features_extract_model(temp_input)\r\n      8     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\r\n\r\n<ipython-input-5-28f690dfd033> in load_image(image_path)\r\n      3     img = tf.image.decode_jpeg(img, channels=3)\r\n      4     img = tf.image.resize_images(img, (224, 224))\r\n----> 5     img = tf.keras.applications.vgg16.preprocess_input(img)\r\n      6     return img, image_path\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/__init__.py in wrapper(*args, **kwargs)\r\n     68       kwargs['models'] = models\r\n     69       kwargs['utils'] = utils\r\n---> 70     return base_fun(*args, **kwargs)\r\n     71   return wrapper\r\n     72 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/vgg16.py in preprocess_input(*args, **kwargs)\r\n     42 @keras_modules_injection\r\n     43 def preprocess_input(*args, **kwargs):\r\n---> 44   return vgg16.preprocess_input(*args, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_applications/imagenet_utils.py in preprocess_input(x, data_format, mode, **kwargs)\r\n    193     else:\r\n    194         return _preprocess_symbolic_input(x, data_format=data_format,\r\n--> 195                                           mode=mode, **kwargs)\r\n    196 \r\n    197 \r\n\r\n/usr/local/lib/python3.6/dist-packages/keras_applications/imagenet_utils.py in _preprocess_symbolic_input(x, data_format, mode, **kwargs)\r\n    149             data_format=data_format)\r\n    150     else:\r\n--> 151         x = backend.bias_add(x, _IMAGENET_MEAN, data_format)\r\n    152     if std is not None:\r\n    153         x /= std\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in bias_add(x, bias, data_format)\r\n   4595     elif data_format == 'channels_last':\r\n   4596       if len(bias_shape) == 1:\r\n-> 4597         x = x + reshape(bias, (1, 1, bias_shape[0]))\r\n   4598       else:\r\n   4599         x = x + reshape(bias, (1,) + bias_shape)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in reshape(x, shape)\r\n   2279       A tensor.\r\n   2280   \"\"\"\r\n-> 2281   return array_ops.reshape(x, shape)\r\n   2282 \r\n   2283 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in reshape(tensor, shape, name)\r\n   6497     except _core._FallbackException:\r\n   6498       return reshape_eager_fallback(\r\n-> 6499           tensor, shape, name=name, ctx=_ctx)\r\n   6500     except _core._NotOkStatusException as e:\r\n   6501       if name is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in reshape_eager_fallback(tensor, shape, name, ctx)\r\n   6516   _attrs = (\"T\", _attr_T, \"Tshape\", _attr_Tshape)\r\n   6517   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\r\n-> 6518                              ctx=_ctx, name=name)\r\n   6519   _execute.record_gradient(\r\n   6520       \"Reshape\", _inputs_flat, _attrs, _result, name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\r\n     59                                                op_name, inputs, attrs,\r\n---> 60                                                num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nTypeError: provided list of inputs contains objects other than 'EagerTensor'. Item 0 is Tensor\r\n```\r\n\r\nTHX!!", "Can you try this out: https://www.tensorflow.org/tutorials/text/image_captioning\r\n\r\nI am closing this. Please reopen if still failing."]}, {"number": 22594, "title": "TensorFlow 1.11.0 with GPU support now requires CUDA Compute Capability 3.7?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, [cifar10_train.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Version 1803 OS Build 17134.320\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0\r\n- **Python version**: Python 3.6.6 :: Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0/cuDNN v7.3.0\r\n- **GPU model and memory**: GeForce GTX 780 Ti 3GB\r\n- **Exact command to reproduce**: `python cifar10_train.py`\r\n\r\n### Describe the problem\r\nTensorFlow 1.11.0 now requires CUDA Compute Capability 3.7. But this was not mentioned in [\r\nGPU support](https://www.tensorflow.org/install/gpu) or [Release Notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.11.0). I want to ask whether this is intentional.\r\n\r\n### Source code / logs\r\n\r\n```\r\n2018-09-28 18:31:27.274915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0715\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.00GiB freeMemory: 2.45GiB\r\n2018-09-28 18:31:27.278399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1461] Ignoring visible gpu device (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:01:00.0, compute capability: 3.5) with Cuda compute capability 3.5. The minimum required Cuda capability is 3.7.\r\n```", "comments": ["Yes, They changed minimum CUDA Compute Capability to 3.7 from 3.0. Try to use 1.10 it will work as changes were made after that", "Yes this was intentional as part of our transition from cmake to bazel. @angersson maybe add this to the release notes?", "I guess you could always use customized-build for your GPU. After all, it\u2019s still supported by CUDA and NV drivers.", "I have a Tesla 63C K40C. The highest compute capability is 3.5. So, do you mean I have to buy a new GPU or install the older version tensorflow? Is there any setting I can use to make the current tensorflow also support GPU with capacity 3.5?", "@nimning AFAIK 3.5 or K40 series is always supported.", "@nimning You can always build from source. If you don't want to do that then unfortunately you have to install one of the older versions"]}, {"number": 22593, "title": "[XLA/tfcompile] fix type of enum for build with MSVC", "body": " cpu_function_runtime.h: When using bitfields, MSVC only merges same-sized types. Therefore I changed the type of the \"Kind\" enum to match the type of size_.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@cgrail Can you sign the CLA, please?", "@caisq did you mean @cgrati ?", "@cgrail Oops, sorry. Yes.\r\n@cgrati ", "I signed the CLA! Hopefully using the correct email ...", "CLAs look good, thanks!\n\n<!-- ok -->", "@sanjoy added the static_assert - the explanation therein should suffice in place of a comment - is this ok for you? I'm sorry the commit history is such a mess ... should we merge this into one commit?", "@sanjoy Is it up to me to trigger the tests (Ubuntu CC Expected, Ubuntu Sanity, import/copybara), so my pull request gets merged? If yes, how do I do that?\r\n", "@cgrati There seem to be some compilation errors in builds:\r\n\r\nIn file included from tensorflow/compiler/xla/service/cpu/buffer_info_util.cc:16:\r\nIn file included from ./tensorflow/compiler/xla/service/cpu/buffer_info_util.h:20:\r\n./tensorflow/compiler/tf2xla/cpu_function_runtime.h:73:26: error: use of undeclared identifier '_size'\r\n    static_assert(sizeof(_size) == sizeof(Kind),\r\n\r\nE.g,. see https://source.cloud.google.com/results/invocations/46ec77f6-0b22-44e7-8e3c-bc35f462cbaf/log\r\n\r\nCan you fix them?", "It has been 18 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 46 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22592, "title": "Unable to convert ops to .tflite model", "body": " tf.VERSION = 1.9.0\r\n tf.GIT_VERSION = v1.9.0-0-g25c197e023\r\n tf.COMPILER_VERSION = v1.9.0-0-g25c197e023\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)\r\ncentos 7\r\n\r\n```\r\nConverting unsupported operation: RandomStandardNormal\\n\r\n2018-09-28 14:53:49.252475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: SplitV\\n\r\n2018-09-28 14:53:49.252515: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\\n\r\n2018-09-28 14:53:49.252535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Size\\n\r\n2018-09-28 14:53:49.252944: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Cos\\n\r\n2018-09-28 14:53:49.253000: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: RFFT\\n\r\n2018-09-28 14:53:49.253019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ComplexAbs\\n\r\n2018-09-28 14:53:49.253067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LinSpace\\n\r\n2018-09-28 14:53:49.253189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: LinSpace\\n\r\n2018-09-28 14:53:49.253302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: SplitV\\n\r\n2018-09-28 14:53:49.253618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:147] Unsupported data type in placeholder op: 2\\n\r\n2018-09-28 14:53:49.253703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\\n\r\n2018-09-28 14:53:49.253763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.253790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\\n\r\n2018-09-28 14:53:49.271105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\\n\r\n2018-09-28 14:53:49.271561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271777: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: ListDiff\\n\r\n2018-09-28 14:53:49.271830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.271866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1366] Converting unsupported operation: Prod\\n\r\n2018-09-28 14:53:49.274806: F tensorflow/contrib/lite/toco/tooling_util.cc:921] Check failed: array->has_shape() \\n'\r\nNone\r\n```\r\n\r\nWhat can I do for these errors?\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "\r\nOS Platform and Distribution\r\ncentos 7\r\n\r\nTensorFlow installed from\r\nsource\r\n\r\nTensorFlow version\r\ntf.VERSION = 1.9.0\r\ntf.GIT_VERSION = v1.9.0-0-g25c197e023\r\ntf.COMPILER_VERSION = v1.9.0-0-g25c197e023\r\n\r\nBazel version\r\n\r\nCUDA/cuDNN version\r\ncuda 9.0\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nN/A\r\nMobile device\r\nN/A", "When to support `ListDiff` op or  `tensordot` op ?", "Please refer [TensorFlow Operations Compatibility Guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) to know more about all the supported operations.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 22591, "title": "Fix some documentation errors", "body": "", "comments": []}, {"number": 22590, "title": "Fast with eager execution, but slow without it for the same code?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.5 LTS (Xenial Xerus)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX GeForce 1080Ti 11MB (11177MiB)\r\n- **Exact command to reproduce**: Source code below\r\n\r\n### Describe the problem\r\nThe same code runs very slowly using the classic graph model (around 7 minutes) while it runs very quickly using the new eager execution mode (not profiled exactly, but around 10 seconds). The code just tries to compute the SSIM value of the two images loaded from a file.\r\n\r\n### Source code / logs\r\nThe codes are pretty much the same, the only difference being whether `ssim_result` is run in a session or eagerly executed.\r\n\r\n-- Code redacted by request --", "comments": ["@hsgkim Hi, eager execution is meant for faster computation without constructing any graph. Hence this behavior is as expected.", "@harshini-gadige  I don't think so. The discrepancy is huge and the graph mode is supposed to be faster.\r\nMaybe this issue should be considered more carefully.\r\nI tried to run the code on Colab. Unfortunately, a 'ssimtest.mat' is required.\r\n@hsgkim  could you make a simpler toy example which can reproduce the issue. ", "@nairouz This can be reproduced with `img1` and `img2` as `np.random.normal(size=(256, 224, 176)).astype(np.float32)` as well.", "@hsgkim why are you using numpy based operations for the gkernel function. I think you should use Tensorflow operations. That could be the issue causing the abnormal runtime.", "In colab:\r\n7.3 minutes for the graph mode.\r\n3.1 sec for the eager mode.\r\n\r\nThat's abnormal.", "Wait, I didn't notice that part was written in NumPy as this was a part of a huge file. I think that might be the reason why :x", "@hsgkim maybe you can re-run the code without the `gkernel` function and check if it is the cause of the issue."]}, {"number": 22589, "title": "Error when re-initializing the model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: 1.11.0-rc2\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nUsed Google Colab.\r\n\r\n### Describe the problem\r\nI defined a model in terms of CuDNNLSTM, then re-defined it again in terms of LSTM. The model crushed with error message suggesting that it was looking for the CUDA optimized function no matter that the second model used standard LSTM.\r\n\r\nBelow I post a reproducible code and a ~~[Colab link](https://colab.research.google.com/drive/1fhsUfxAdtG2lTTYndhcr8Y9924IPYkiJ)~~ [Colab link](https://colab.research.google.com/drive/1fhsUfxAdtG2lTTYndhcr8Y9924IPYkiJ):\r\n\r\n```python\r\nfrom __future__ import print_function\r\n\r\nfrom keras.preprocessing import sequence\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Embedding\r\nfrom keras.layers import LSTM, CuDNNLSTM\r\nfrom keras.datasets import imdb\r\n\r\nmax_features = 20000\r\nmaxlen = 80\r\nbatch_size = 32\r\n\r\nprint('Loading data...')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\n\r\n### ========== IMPORTANT STUFF STARTS HERE ================\r\n\r\n# define the model\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128))\r\nmodel.add(CuDNNLSTM(128))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\n# it will crush because not using GPUs, so re-define it in terms of standard LSTM:\r\n\r\n# ===============================================================\r\n#          The below part starts in a DIFFERENT notebook cell\r\n# ===============================================================\r\n\r\nmodel = Sequential()\r\nmodel.add(Embedding(max_features, 128))\r\nmodel.add(LSTM(128))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\n\r\n# it crushes again, with the same error\r\n\r\n### ================ END HERE ======================\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=15,\r\n          validation_data=(x_test, y_test))\r\nscore, acc = model.evaluate(x_test, y_test,\r\n                            batch_size=batch_size)\r\n```\r\n\r\n### Source code / logs\r\n\r\nI get the following error on the very first iteration:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1291     try:\r\n-> 1292       return fn(*args)\r\n   1293     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1274       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1275       self._extend_graph()\r\n   1276       return self._call_tf_sessionrun(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1311     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1312       tf_session.ExtendSession(self._session)\r\n   1313 \r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-60cbd0c8ff19> in <module>()\r\n     53           batch_size=batch_size,\r\n     54           epochs=15,\r\n---> 55           validation_data=(x_test, y_test))\r\n     56 score, acc = model.evaluate(x_test, y_test,\r\n     57                             batch_size=batch_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1000                               initial_epoch=initial_epoch,\r\n   1001                               steps_per_epoch=steps_per_epoch,\r\n-> 1002                               validation_steps=validation_steps)\r\n   1003 \r\n   1004     def evaluate(self, x=None, y=None,\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1703                               initial_epoch=initial_epoch,\r\n   1704                               steps_per_epoch=steps_per_epoch,\r\n-> 1705                               validation_steps=validation_steps)\r\n   1706 \r\n   1707     def evaluate(self, x=None, y=None,\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/engine/training.py in _fit_loop(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n   1234                         ins_batch[i] = ins_batch[i].toarray()\r\n   1235 \r\n-> 1236                     outs = f(ins_batch)\r\n   1237                     if not isinstance(outs, list):\r\n   1238                         outs = [outs]\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in __call__(self, inputs)\r\n   2478             feed_dict[tensor] = value\r\n   2479         fetches = self.outputs + [self.updates_op] + self.fetches\r\n-> 2480         session = get_session()\r\n   2481         updated = session.run(fetches=fetches, feed_dict=feed_dict,\r\n   2482                               **self.session_kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py in get_session()\r\n    191                 # not already marked as initialized.\r\n    192                 is_initialized = session.run(\r\n--> 193                     [tf.is_variable_initialized(v) for v in candidate_vars])\r\n    194                 uninitialized_vars = []\r\n    195                 for flag, v in zip(is_initialized, candidate_vars):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    885     try:\r\n    886       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 887                          run_metadata_ptr)\r\n    888       if run_metadata:\r\n    889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1108     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1109       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1110                              feed_dict_tensor, options, run_metadata)\r\n   1111     else:\r\n   1112       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1284     if handle is None:\r\n   1285       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1286                            run_metadata)\r\n   1287     else:\r\n   1288       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1306           self._config.experimental.client_handles_error_formatting):\r\n   1307         message = error_interpolation.interpolate(message, self._graph)\r\n-> 1308       raise type(e)(node_def, op, message)\r\n   1309 \r\n   1310   def _extend_graph(self):\r\n\r\nInvalidArgumentError: No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n\r\nCaused by op 'cu_dnnlstm_1/CudnnRNN', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-1-60cbd0c8ff19>\", line 32, in <module>\r\n    model.add(CuDNNLSTM(128))\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/models.py\", line 522, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 500, in __call__\r\n    return super(RNN, self).__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\", line 619, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/cudnn_recurrent.py\", line 90, in call\r\n    output, states = self._process_batch(inputs, initial_state)\r\n  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/cudnn_recurrent.py\", line 510, in _process_batch\r\n    is_training=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1544, in __call__\r\n    input_data, input_h, input_c, params, is_training=is_training)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 1435, in __call__\r\n    seed=self._seed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 922, in _cudnn_rnn\r\n    outputs, output_h, output_c, _ = gen_cudnn_rnn_ops.cudnn_rnn(**args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py\", line 116, in cudnn_rnn\r\n    is_training=is_training, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'CudnnRNN' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[{{node cu_dnnlstm_1/CudnnRNN}} = CudnnRNN[T=DT_FLOAT, direction=\"unidirectional\", dropout=0, input_mode=\"linear_input\", is_training=true, rnn_mode=\"lstm\", seed=87654321, seed2=0](cu_dnnlstm_1/transpose, cu_dnnlstm_1/ExpandDims_1, cu_dnnlstm_1/ExpandDims_2, cu_dnnlstm_1/concat_1)]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@twolodzko `CuDNNLSTM` does work in general but you may need to check the shape of the embedding layer or specify input shape inside `CuDNNLSTM` along with other args. Regardless `LSTM` and `CuDNNLSTM` should have minimal difference except for speed. Thanks for bringing this to our attention. ", "@wt-huang This does not seem to relate to the problem... The problem is that I initialized model on non-GPU machine using `CuDNNLSTM`, then initialized it again replacing `CuDNNLSTM` with `LSTM` and got error saying that I need GPU for model using non-GPU `LSTM`. So Keras/TensorFlow still \"remembered\" that I was using the `CuDNNLSTM` for my `model` no matter that I defined the `model` again using regular `LSTM` layer. I guess that this means that something with graph initialization went wrong and the graph \"remembered\" my previous model definition.", "@twolodzko Yes, I understood your problem. The error message is triggered by the first `CuDNNLSTM` model. If you have another `LSTM` or other working model as the first model, the model initialization should work fine.  ", "@wt-huang OK, I messed the example up, so it was unclear: the code above is ran from Colab notebook, so first the model `CuDNNLSTM` is initialized in one cell, I get an error, then I initialize the regular `LSTM` model *in another cell* and I get an error from the re-initialized model. See the [relevant Colab notebook (fixed example)](https://colab.research.google.com/drive/1fhsUfxAdtG2lTTYndhcr8Y9924IPYkiJ).", "@twolodzko When the first model `CuDNNLSTM` got an error, to re-initialize the second model you can either go to the command line to kill the process or use a different name for the new model. ", "@wt-huang Right, this is a possible and reasonable workaround. However, what I would normally expect is that when the previous model is not used any more (it gets detached from the `model` name), then some kind of destructor cleans it up, so that it does not interfere with the new model under the same name. This seems to de issue with having a destructor and garbage collector.\r\n\r\nMoreover, in this case I get an error because of doing something wrong, but **what if I re-used the name and didn't get the error**? Would I be working with a polluted namespace that stores all the history of everything that was previously assigned to this name? If so, this seems to be a straight way to really ugly and hard to trace bugs.\r\n\r\nWhat I'm saying is that the described problem is not much bother for me, as I could use different name, but it seems to be a symptom of something really ugly and serious beneath.", "@twolodzko You would need to add this before initiating a model, or before LSTM model in the above code snippet: `tf.reset_default_graph()`\r\n\r\nIt should work without triggering any errors.\r\n\r\n\r\n", "@wt-huang Thanks, but as I mentioned above, this may need to happen automatically to prevent bugs, or at least be better documented.", "@twolodzko This is part of the current set up, we will try to make further improvements\r\n and document it better.\r\n\r\n"]}, {"number": 22588, "title": "Update the relative file paths in the comments of tf.data kernel files", "body": "`tf.data` kernel files are moved down one directory in [this commit](https://github.com/tensorflow/tensorflow/commit/a5b2a0c9a3335d10c4dd3dfdff96149f74a4d120#diff-1362ec26d9480143607b5c5609959280). Accordingly, this PR updates the relative paths in several comments of `tf.data` kernel files that need to be changed.\r\n\r\nFixes #22576\r\n\r\n  ", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@caisq This PR has been a while since the approval. Is there anything I need to do?", "@feihugis The change is being tested and reviewed internally. When it is submitted internally, this GH PR will be automatically merged. No actions is required from you at this time. Thank you for your patience.", "And sorry for the delay.", "@caisq Thanks for your quick reply. The tests for `MacOS Python2 and CC` failed, but I think it may be not related as this PR only changes the documentation. Do you know why `feedback/copybara` failed?", "@feihugis internal tests are running... crossing fingers.", "Passes. Waiting for internal approval."]}]