[{"number": 29844, "title": "tf.image.decode_and_crop_jpeg parameter crop_window changed", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/io/decode_and_crop_jpeg\r\n\r\n## Description of issue :\r\nDocumentation of parameter type - crop_window\r\n\r\n### Clear description\r\nThe type of parameter is list of int and in the documentation it says:\r\n\r\n>     crop_window: A `Tensor` of type `int32`.\r\n\r\n", "comments": ["I made a toy example and observed that ```crop_window``` argument can take a tensor as well as a list. Perhaps we can make changes to the existing documentation.", "Ya know, I don't know where this docstring lives :/\r\nFound it in this wrappers.go file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/op/wrappers.go\r\nbut I'm not certain that's the source of truth.", "This is a hard one to track down the SOT for.\r\n\r\nThe builtin ops's docstrings are usuallt set by these protos:\r\n https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_DecodeJpeg.pbtxt \r\n\r\nThere isn't one for \"decode_and_crop\", it's not implemented as a stand alone op. It's an overload of DecodeImage?:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/kernels/decode_image_op.cc\r\n\r\n---------------------------\r\n\r\nAnyway, enough spelunking. \r\n\r\nI believe the correct answer here is:\r\n\r\n> **Any function that takes a `Tensor` argument will also accept anything that can be converted to a tensor by `tf.convert_to_tensor`**\r\n\r\nThis is/should be true for every op, not just this one. If there's an action to take here it's that we need to better publicize the above fact. \r\n\r\nHow about we add it to the top of: https://www.tensorflow.org/guide/tensors \r\n\r\nWDYT?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "The current docs say: \r\n\r\n```\r\ncrop_window:  A\u00a0Tensor\u00a0of type\u00a0int32. 1-D. The crop window: [crop_y, crop_x, crop_height, crop_width].\r\n```\r\n\r\nSo this is fixed."]}, {"number": 29843, "title": "beta-1: tf.keras.layers.Conv2D with dilation_rate > 1 returns tensor with shape None", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): tensorflow-gpu==2.0.0-beta1 colab\r\n- TensorFlow version (use command below):tensorflow-gpu==2.0.0-beta1\r\n- Python version: pip3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: colab\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nif tf.keras.layers.Conv2D is used with dilation_rate param > 1 it returns tensors with shape None!\r\n", "comments": ["@flobotics In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "you can run the code directly in colab. \r\n\r\nhttps://github.com/flobotics/colab/blob/master/tf2_ganimorph.ipynb\r\n\r\ngithub now shows directly the notebook with the error message in the def Discriminator function. You can also run it directly with colab.\r\n\r\nthe error message is\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-34-4221abe9b435> in <module>()\r\n----> 1 discriminator = Discriminator()\r\n      2 \r\n      3 if not TEST:\r\n      4   noise = tf.random.normal([1, 3, 128, 128])\r\n      5 \r\n\r\n<ipython-input-33-8aedbb5db62a> in Discriminator()\r\n    105                         bias_initializer=None)(atrous)\r\n    106 \r\n--> 107   atrous2 = tfa.layers.InstanceNormalization()(atrous2)\r\n    108   atrous2 = layers.ReLU()(atrous2)\r\n    109 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    614           # Build layer if applicable (if the `build` method has been\r\n    615           # overridden).\r\n--> 616           self._maybe_build(inputs)\r\n    617 \r\n    618           # Wrapping `call` function in autograph to allow for dynamic control\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   1964         # operations.\r\n   1965         with tf_utils.maybe_init_scope(self):\r\n-> 1966           self.build(input_shapes)\r\n   1967       # We must set self.built since user defined build functions are not\r\n   1968       # constrained to set self.built.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_addons/layers/normalizations.py in build(self, input_shape)\r\n    102     def build(self, input_shape):\r\n    103 \r\n--> 104         self._check_if_input_shape_is_none(input_shape)\r\n    105         self._set_number_of_groups_for_instance_norm(input_shape)\r\n    106         self._check_size_of_dimensions(input_shape)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_addons/layers/normalizations.py in _check_if_input_shape_is_none(self, input_shape)\r\n    203                              'input tensor should have a defined dimension '\r\n    204                              'but the layer received an input with shape ' +\r\n--> 205                              str(input_shape) + '.')\r\n    206 \r\n    207     def _set_number_of_groups_for_instance_norm(self, input_shape):\r\n\r\nValueError: Axis -1 of input tensor should have a defined dimension but the layer received an input with shape (None, 512, None, None).\r\n```\r\n\r\nIf you run it with  tensorflow-gpu==2.0.0-alpha0 it works ?\r\n\r\nhope that is good enough ,or do you need more?", "Was able to reproduce the issue on colab link with Tensorflow 2.0.0.beta1. Thanks! ", "are there some news/updates ?", "The shape information gets lost somewhere in `__call__`. `call` itself is fine.\r\n\r\nI debugged a little, trying to find where exactly the shape is list, but I haven't found it: https://colab.sandbox.google.com/drive/1IKVe8cJhmnOV0SHztLmryYT9xvkJTIAm#scrollTo=_rbSDPnQH9ak\r\n\r\nA simple workaround is to call `output.set_shape(layer.compute_output_shape(input.shape))` on the output, but there's clearly a bug here.", "I'm closing this as a duplicate of #29542", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29843\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29843\">No</a>\n"]}, {"number": 29842, "title": "native python logging broken with TF 1.14", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.0 stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): v1.12.1-2376-gf5ce1c00d4 1.14.0-rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.6\r\n- GPU model and memory: NVIDIA Titan X (Pascal) 12Gb\r\n\r\n**Describe the current behavior**\r\nlogging from the python's standard library stopped working in TF 1.14. Log file is not created and the output supposed to be written there is instead redirected to the stdout, which results in each logging message appear twice in the console.\r\n\r\n**Describe the expected behavior**\r\nWhen run on TF 1.12 , log.txt is created and the logging is being recorded correctly.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport logging\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(_):\r\n    FORMAT = '%(asctime)-15s %(message)s'\r\n    logging.basicConfig(filename='log.txt', filemode='w',\r\n                        datefmt='%Y-%m-%d %H:%M:%S',\r\n                        level=logging.INFO, format=FORMAT)\r\n    console = logging.StreamHandler()\r\n    console.setLevel(logging.INFO)\r\n    logging.getLogger('').addHandler(console)\r\n\r\n    sess = tf.Session()\r\n\r\n    logging.info(\"test write\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run()\r\n```\r\n\r\nThis yields the following output:\r\n\r\n```\r\n2019-06-16 17:54:24.047410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10711 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0, compute capability: 6.1)\r\nI0616 17:54:24.051054 140454010144128 test_logging.py:15] test write\r\ntest write\r\n```\r\n\r\nNotice that `test write` appears twice in the output. And log.txt is not created. When run with TF 1.12 this code snippets behaves as expected.", "comments": ["I could able to reproduce the above mentioned issue with tensorflow 1.14.0-rc0 on colab. Thanks!", "Similar problem here, @gadagashwini. Just importing tensorflow intrusively adds a logging handler to the root logger:\r\n```python\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nif __name__ == '__main__':\r\n    logger.warning(\"Hi!\")\r\n\r\n(stderr)>>>\r\nHi!\r\n```\r\nvs\r\n```python\r\nimport logging\r\n\r\nimport tensorflow as tf\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\nif __name__ == '__main__':\r\n    logger.warning(\"Hi!\")\r\n\r\n(stderr)>>>\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0628 20:51:13.387141 140355075146048 test.py:9] Hi!\r\n```\r\n\r\nThis is due to absl adding its own handler to the root logger when importing tensorflow.", "Having dug a bit more, I have found the reason for this issue. Compare the difference between `tf.app` in 1.13 and 1.14:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/platform/app.py\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/platform/app.py\r\n\r\nI have traced the unwanted addition of an `ABSLHandler` down to this import in 1.14:\r\nhttps://github.com/tensorflow/tensorflow/blob/f98fa7beca612279019a788d2719a3ebaaeaff85/tensorflow/python/platform/app.py#L23", "Just a quick ping to say that my work is also affected by this.", "I have the same problem", "@revan has better context about this issue than I do. I'm assigning the bug to him.", "Closing as duplicate of #26691.\r\nSee that bug for more, but TL;DR: upstream absl logging might be a touch too magical, but you can disable it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29842\">No</a>\n"]}, {"number": 29841, "title": "tf.keras.layers.Conv3DTranspose ignores dilation_rate parameter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: Any\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n `Conv3DTranspose` layer ignores `dilation_rate` parameter. The shape of the output tensor stays the same regardless of the `dilation_rate` parameter.\r\n\r\n**Describe the expected behavior**\r\n\r\n `Conv3DTranspose` should behave like `Conv2DTranspose` and handle dilated transposed convolution. Note that `Conv3D` handles dilated convolutions all right.\r\n\r\n**Code to reproduce the issue**\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow.keras.layers import Conv3DTranspose\r\n\r\n    sess = tf.Session()\r\n    x = tf.placeholder(tf.float32, shape=(None,3,3,3,1))\r\n    layer = Conv3DTranspose(filters=1, kernel_size=3, dilation_rate=2)\r\n    result = layer(x)\r\n    sess.run(tf.global_variables_initializer())\r\n    output = sess.run(result, {x:np.ones((2,3,3,3,1))})\r\n\r\n    ## Expected output shape = (2,7,7,7,1)\r\n    ## Actual output shape = (2,5,5,5,1)\r\n\r\n**Other info / logs**\r\nThe problematic code seems to be here:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/convolutional.py#L890\r\n", "comments": ["Have tried on Colab with TF 1.13.1 and was able to reproduce the issue.", "Perhaps this is a feature request rather than a bug. I checked that `Conv2DTranspose` uses custom implementation from Keras submodule (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L4633) which is not implemented for 3D transposed convolutions at all...", "Conv3DTranspose does not have a dilation rate for now", "Would you like to make a PR out of it? I will make this as contribution welcome.", "Not sure if this is directly relevant to this thread, but is `keras.backend.conv3d_transpose` still a relevant function worth keeping? It seems like `keras.layers.Conv3DTranspose` calls on `nn_ops` directly instead of routing to `keras.backend`. This contrasts with similar layers like `keras.layers.Conv2DTranspose`, which calls on `keras.backend.conv2d_transpose` internally (which then eventually calls on `nn_ops`).\r\n\r\nAt any rate, I'll be editing the docstrings to erase the `dilation_rate` argument as of now since it is an unsupported feature as of yet.", "Fixed here: https://github.com/tensorflow/tensorflow/commit/46c79db67db33d7a14e124fc0b5cf23e0910d039", "The commit linked above does _not_ fix the problem:\r\n\r\nWhile dilation_rate is now passed to the parent constructor, it is not actually used - neither for output shape calculation in the call to `conv_utils.deconv_output_length` nor for the actual computation in the call to `nn.conv3d_transpose`.\r\n\r\nThe test case introduced in the commit is also wrong in that regard - the output shape with a dilation rate of 2 for an input of `(5, 7, 6)` should be `(9, 11, 10)`.\r\n\r\nBesides, dilation_rate is still not even stored in the config dict."]}, {"number": 29839, "title": "TPU train_on_batch stride size error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5\r\n- GPU model and memory: TPU v3-8\r\n\r\n**Describe the current behavior**\r\n\r\nCode and data which **runs fine on CPU**, _throws error on TPU_. This only happens if I use train_on_batch instead of fit.\r\n\r\nI have 2 versions of same model. One is with fit with 2 loops and with **train_on_batch** with 3 loops (epoch, day worth of data, batch within day)\r\n\r\ntrain_on_batch throws error: `slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.`\r\n\r\n0111 data is label provided in y2. and the size of 4 is correct. Why is computed input tensor is size 3 I don't understand. It looks like a bug very much.\r\n\r\nModel\r\n```\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) \r\nmodel.add(layers.LSTM(neurons))\r\nmodel.add(layers.Dense(outputs_n, activation='sigmoid'))\r\n\r\nopt = tf.train.AdamOptimizer(0.001)\r\n\r\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\r\n \r\ntpu_model = tf.contrib.tpu.keras_to_tpu_model(model, \r\n        strategy=tf.contrib.tpu.TPUDistributionStrategy(\r\n            tf.contrib.cluster_resolver.TPUClusterResolver(tpu = [TPU_ADDRESS1])))\r\n```\r\nShapes\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlstm_input (InputLayer)      (None, 1024, 7)           0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 1024, 128)         69632     \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 128)               131584    \r\n_________________________________________________________________\r\ndense (Dense)                (None, 4)                 516       \r\n=================================================================\r\n```\r\nTraining:\r\n```\r\nfor epoch in epochs:\r\n    for d in days : \r\n        # get arrays for the day\r\n        features = np.asarray(d[1])[:,2:9].astype(dtype = 'float32')\r\n        labels = np.asarray(d[1])[:, 9:13].astype(dtype = 'int32')\r\n        \r\n        X,y = split_sequence(features, labels_buy, window_size)\r\n\r\n        # train \r\n        for slide in range(window_size):\r\n            try:\r\n                x1, y1 = X[slide], y[slide]\r\n                x2, y2 = x1.reshape(1,1024,7), y1.reshape(1, 4)\r\n                H = tpu_model.train_on_batch(x2,y2)\r\n            except Exception as e:\r\n                print('** train exception **', e)\r\n                continue\r\n```\r\n**Describe the expected behavior**\r\n\r\ntrain_on_batch trains without exception\r\n", "comments": ["@maxima120 Please provide the complete code snippet to reproduce the issue reported here. And also include the error log. Thanks!", "how do I send it to you? data file is big", "@maxima120 Will it be possible to provide minimal code snippet with all the operations to expedite the trouble-shooting process. Thanks!\r\n", "Ok but it's there already. Run \"model\" excerpt and then \"training\" ", "The code snippet attached here is complete. Can you please make a GitHub gist or Google Colab with your example? Thanks!", "this reproduces the issue:\r\nhttps://gist.github.com/maxima120/d1057a0e4bbf2ae2a1434dad57999a3f", "@maxima120 I've reassigned it to folks more familiar with the code, but with TensorFlow 1.14, can you try distribution strategy instead? That is a more complete implementation: https://www.tensorflow.org/guide/distribute_strategy#using_tfdistributestrategy_with_keras", "Sorry for the delay. It looks like `train_on_batch` hasn't been overridden properly in the `keras_to_tpu_model` code. As Frank mentions, it would great if you can try using the DistributionStrategy approach instead: that has better support as of TF 1.14.", "Got **error** using tf.distribute instead of contribute: **`train_on_batch` is not supported for models distributed with tf.distribute.Strategy.**\r\n\r\nUsed GCP image: Debian with TF 1.14.0-rc0\r\n\r\nCode (_not sure if I am doing it right_):\r\n```\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver([TPU_ADDRESS1])\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\n# build model\r\nwith tpu_strategy.scope():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) \r\n    model.add(layers.LSTM(neurons))\r\n    model.add(layers.Dense(neurons, activation='relu'))\r\n    model.add(layers.Dense(outputs_n, activation=activation))\r\n\r\n    opt = tf.train.RMSPropOptimizer(learning_rate)\r\n\r\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\r\n\r\n    X,y = split_sequence(features, labels, window_size)\r\n    print('X shape:', X.shape, 'Y shape:', y.shape)\r\n    for slide in range(window_size):\r\n        x1, y1 = X[slide], y[slide]\r\n        x2, y2 = x1.reshape(1,window_size,inputs_n), y1.reshape(1,outputs_n)\r\n        model.train_on_batch(x2,y2)\r\n```\r\ngot output:\r\n```\r\nUsing TensorFlow backend.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0709 11:29:14.795013 140573481281280 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nX shape: (9000, 1000, 7) Y shape: (9000, 4)\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-5-0f3e39e3b63e> in <module>\r\n     33         x1, y1 = X[slide], y[slide]\r\n     34         x2, y2 = x1.reshape(1,window_size,inputs_n), y1.reshape(1,outputs_n)\r\n---> 35         model.train_on_batch(x2,y2)\r\n     36 \r\n     37 print(model.metrics_names)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)\r\n   1146     if (self._distribution_strategy and\r\n   1147         distribution_strategy_context.in_cross_replica_context()):\r\n-> 1148       raise NotImplementedError('`train_on_batch` is not supported for models '\r\n   1149                                 'distributed with tf.distribute.Strategy.')\r\n   1150     # Validate and standardize user data.\r\n\r\nNotImplementedError: `train_on_batch` is not supported for models distributed with tf.distribute.Strategy.\r\n```", "@rjpower - I tried your suggestion but got similar error (_train_on_batch` is not supported.. with tf.distribute.Strategy_) - see above.. any other suggestions?", "@rjpower @frankchn train_on_batch is broken using with tpu distribution strategy on tensorflow 1.14.0\r\nplease check this link https://stackoverflow.com/questions/56741126/using-train-on-batch-on-colab-tpu-new-tensorflow-version-1-14", "Thank you. But this is about TF 1.13\r\nI am not going to use 1.14 ever. I am just waiting for 2.0 being mature and TPUs are implemented..\r\nThey are moving like facebook now - move fast break things (translation to English - no quality just ticking checkboxes in managerial reports)", "Sadly, the original Keras API you're using was always intended as an experimental stop-gap (hence being marked as `@experimental` in the API signature). DistributionStrategy is intended as it's replacement. I'm not sure at the moment if the missing support for `train_on_batch` is \"we haven't gotten to it yet\", or if it's \"we can't get to it\".\r\n\r\nI can check on that, but I think in the meantime, the easiest thing to do (albeit a bit of a kludge)  is to try `.fit` API and give it a single batch. For more custom use cases, I believe the intended idea is a \"custom training loop\". It looks something like this [[code](https://github.com/tensorflow/tpu/blob/4d7f7bf76df200a6dd74b01a6dbb95d8b013ecfe/models/common/distributed_executor.py#L165)]:\r\n\r\n```\r\n   def train_one_step(inputs):\r\n        inputs, labels = inputs\r\n\r\n        with tf.GradientTape() as tape:\r\n          logits = model(inputs, training=True)\r\n          prediction_loss = loss_fn(labels, logits)\r\n          logging.info('debug prediction_loss %s', prediction_loss)\r\n          loss = tf.reduce_mean(prediction_loss)\r\n          loss = loss / strategy.num_replicas_in_sync\r\n          return loss\r\n\r\n        grads = tape.gradient(loss, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n      per_replica_losses = strategy.experimental_run_v2(_replicated_step, args=(next(iterator),))\r\n      return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses, axis=None)\r\n```\r\n\r\nRE timelines: I'm inclined to agree -- TF 2.0 should have a more mature distribution strategy and better support for Keras TPUs. FWIW, Keras TPU support has never been a supported feature prior to TF 2.0 (though I can see good arguments for why it should have been prioritized earlier).", "Thank you I will try it.\r\nRe: TF 2.0 - I would gladly switch but I read 2.0 beta doesn't have tpu support whatsoever. Is it wrong? ", "I know that 2.0 final is expected to have full TPU support. That said,\ndevelopment is still under way and many things aren't quite ready yet. I'd\nwait until at least the first RC before trying anything TPU+2.0 related in\nearnest.\n\n(I don't work closely with the teams so take this with a grain of salt...)\n", "Apologies for the long delay: TensorFlow 2.1 is now out, and now supports TPUs via DistributionStrategy. Documentation is somewhat sparse, but here's an example:\r\n\r\nhttps://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29839\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29839\">No</a>\n"]}, {"number": 29838, "title": "Error using OpenMPI together with self-compiled version of Tensorflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 1.13.1\r\n- Python version: Bazel used 2.7 as default, however I have also access to 3.6\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.26\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - (no GPU)\r\n\r\n\r\n**Describe the problem**\r\nI am using Tensorflow's C API within a C++ simulation software for fluid dynamics (OpenFOAM). So far I used the standard C API available here (https://www.tensorflow.org/install/lang_c). Everything worked well, both doing my computations on one core as well as using OpenMPI to parallelize the fluid dynamics part. As I wanted to speed up my inference I wanted to try out AVX / AVX2 support, as my CPU supports AVX / AVX2. To do so I compiled Tensorflow from source:\r\n1) Installation of Bazel 0.26.1\r\n2) clone from Github: git clone https://github.com/tensorflow/tensorflow.git\r\n3) cd tensorflow -> ./configure\r\n4) I chose these steps for configuration:\r\n- Please specify the location of python. [Default is /usr/bin/python]: default\r\n-Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]: default\r\n- Do you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\n- Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\n- Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n- Do you wish to build TensorFlow with CUDA support? [y/N]: N\r\n- Do you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\n- Do you wish to build TensorFlow with MPI support? [y/N]: y\r\n- Please specify the MPI toolkit folder. [Default is /usr]: default\r\n- Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: default\r\n- Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n5) Test: bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n6) Build: bazel build --config opt //tensorflow/tools/lib_package:libtensorflow\r\n7) Included the header files and linked the libraries\r\n\r\nUnfortunately, although everything seemed to be fine, I get an error. As I mentioned, using the standard C API downloaded from the homepage everything was fine. Now i can still to my computations if i do not parallelize my fluid dynamics software. However if I run my fluid dynamics simulation in parallel, this happens:\r\n`\r\n[node134:18796] *** Process received signal ***\r\n\r\n[node134:18796] Signal: Segmentation fault (11)\r\n\r\n[node134:18796] Signal code: Address not mapped (1)\r\n\r\n[node134:18796] Failing at address: (nil)\r\n\r\n[node134:18796] [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7fec1c96ff20]\r\n\r\n[node134:18796] [ 1] /home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt32Opt/lib/libtensorflow_framework.so(hwloc_bitmap_and+0x14)[0x7fec01c21534]\r\n\r\n[node134:18796] [ 2] /usr/lib/x86_64-linux-gnu/libopen-pal.so.20(opal_hwloc_base_filter_cpus+0x380)[0x7febe59d6b80]\r\n\r\n[node134:18796] [ 3] /usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_pmi.so(+0x2b4e)[0x7febe4902b4e]\r\n\r\n[node134:18796] [ 4] /usr/lib/x86_64-linux-gnu/libopen-rte.so.20(orte_init+0x22e)[0x7febe5c2a1de]\r\n\r\n[node134:18796] [ 5] /usr/lib/x86_64-linux-gnu/libmpi.so.20(ompi_mpi_init+0x30e)[0x7febffdbc27e]\r\n\r\n[node134:18796] [ 6] /usr/lib/x86_64-linux-gnu/libmpi.so.20(MPI_Init+0x6b)[0x7febffddd2ab]\r\n\r\n[node134:18796] [ 7] /opt/OpenFOAM/OpenFOAM-4.1/platforms/linux64GccDPInt32Opt/lib/openmpi-system/libPstream.so(_ZN4Foam8UPstream4initERiRPPc+0x1f)[0x7fec1c72843f]\r\n\r\n[node134:18796] [ 8] /opt/OpenFOAM/OpenFOAM-4.1/platforms/linux64GccDPInt32Opt/lib/libOpenFOAM.so(_ZN4Foam7argListC1ERiRPPcbbb+0x719)[0x7fec1db36ed9]\r\n\r\n[node134:18796] [ 9] tabulatedCombustionFoam(+0x279b8)[0x55fe6eb489b8]\r\n\r\n[node134:18796] [10] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xe7)[0x7fec1c952b97]\r\n\r\n[node134:18796] [11] tabulatedCombustionFoam(+0x30a0a)[0x55fe6eb51a0a]\r\n\r\n[node134:18796] *** End of error message ***`\r\n\r\nI do not think that my code has any errors, as with the C API I downloaded directly from the Tensorflow homepage everything worked fine. More probably it seems to be a collision between OpenMPI called from Tensorflow and OpenMPI called from OpenFOAM (the fluid dynamics software). Moreover the run crashes immediately after I start the simulation and far before it reaches my own code.\r\n\r\nMy questions:\r\n1) Is there a procedure I can follow to get rid of the bug?\r\n2) What would I have to do to reproduce exactly the version of the C API that one can download from your homepage? Because as mentioned using this version everything worked fine. Can I find out where the precompiled C API looks for openmpi? Or can you tell me the path?\r\n3) From this post (https://www.cfd-online.com/Forums/openfoam-solving/133913-problems-running-openfoam-2-3-parallel.html) I learned that OpenFOAM uses an own OpenMPI version, however this one worked perfectly fine together with my initial Tensorflow C API stuff that i downloaded. Can you maybe define what the correct MPI toolkit path must contain? Then I could possibly use the one also used by OpenFOAM.\r\n", "comments": ["Duplicated with https://github.com/horovod/horovod/issues/1123 and has been fixed with https://github.com/tensorflow/tensorflow/pull/29807.", "cc @gunan @alsrgv FYI", "Hi @byronyi,\r\n\r\nthanks for your support, my issue is solved. Just had to reclone the Tensorflow git repository! "]}, {"number": 29837, "title": "Tensorflow for Kotlin Multiplatform", "body": "Currently Tensorflow supports [Javascript](https://www.tensorflow.org/js), [Java](https://www.tensorflow.org/install/lang_java), and [C](https://www.tensorflow.org/install/lang_c). These are all the target programming languages of [Kotlin Multiplatform](https://kotlinlang.org/docs/reference/multiplatform.html). Therefore, it would be theoretically possible to port Tensorflow to be a Kotlin Multiplatform Library by just reusing the existing APIs. \r\nAre there any plans for this? If not, what do you think about the idea?", "comments": ["duplicate #27064"]}, {"number": 29836, "title": "R2.0", "body": "Nothing changed, just to join the contributers community...", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29836) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!\n\nVon meinem iPhone gesendet\n\n> Am 16.06.2019 um 12:34 schrieb googlebot <notifications@github.com>:\n> \n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> \ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.\n> \n> Once you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.\n> \n> What to do if you already signed the CLA\n> \n> Individual signers\n> \n> It's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.\n> Corporate signers\n> \n> Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to go/cla#troubleshoot (Public version).\n> The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.\n> The email used to register you as an authorized contributor must also be attached to your GitHub account.\n> \u2139\ufe0f Googlers: Go here for more info.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n"]}, {"number": 29835, "title": "GPU Support - Shared Object Exclusion ", "body": "[UPDATE]:  This issue is about Maven artifact organization / dependency management and documentation.  \r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from Maven, version 1.13.1\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.7\r\n- Installed using virtualenv\r\n- CUDA/cuDNN version:  NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1\r\n- GPU model and memory: 4xNvidia 1080Ti, \r\n\r\n**Describe the problem**\r\n\r\nI have created a TensorFlow model in Python and saved it to the disk using the standard method:\r\n\r\n    builder = tf.saved_model.builder.SavedModelBuilder(model_directory) \r\n    builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING]) \r\n    builder.save(False)\r\n\r\nNext I am loading the model in Java:\r\n\r\n    SavedModelBundle savedModelBundle = SavedModelBundle.loader(modelDir)\r\n        .withTags(\"serve\")\r\n        .withConfigProto(\r\n            ConfigProto.newBuilder()\r\n                .setGpuOptions(\r\n                    GPUOptions.newBuilder()\r\n                        .setPerProcessGpuMemoryFraction(1.0).build())\r\n                .setLogDevicePlacement(true)\r\n                .build().toByteArray())\r\n        .load();\r\n    Session session = savedModelBundle.session();\r\n\r\nIn my pom.xml I have\r\n\r\n    <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>tensorflow</artifactId>\r\n        <version>${tensorflow.version}</version>\r\n    </dependency>\r\n    <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>proto</artifactId>\r\n        <version>${tensorflow.version}</version>\r\n    </dependency>\r\n    <dependency>\r\n        <groupId>org.tensorflow</groupId>\r\n        <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        <version>${tensorflow.version}</version>\r\n    </dependency>\r\n\r\nHowever, this fails to use my GPU's on startup\r\n\r\n    2019-06-15 23:48:38.130731: I tensorflow/compiler/xla/service/service.cc:150] \r\n        XLA service 0x7fbfc4656e10 executing computations on platform Host. Devices:\r\n    2019-06-15 23:48:38.130768: I tensorflow/compiler/xla/service/service.cc:158]   \r\n        StreamExecutor device (0): <undefined>, <undefined>\r\n        Device mapping:\r\n            /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    2019-06-15 23:48:38.131168: I tensorflow/core/common_runtime/direct_session.cc:317]    \r\n       Device mapping:\r\n        /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n\r\nPlease advise on how to use GPU with SaveModelBundle.  ", "comments": ["One interesting thing to note here, is that when I download the shared objects manually:\r\n\r\n    https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-1.13.1.tar.gz\r\n\r\nand place them in my shaded jar resources\r\n\r\n    19798441 Fri Nov 30 04:08:00 PST 1979 org/tensorflow/native/linux-x86_64/libtensorflow_framework.so\r\n    281091096 Fri Nov 30 04:08:00 PST 1979 org/tensorflow/native/linux-x86_64/libtensorflow_jni.so\r\n\r\n then I am able to use my GPUs\r\n\r\n    2019-06-16 07:07:16.919142: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7ff270e69e80 executing computations on platform CUDA. Devices:\r\n    2019-06-16 07:07:16.919178: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n    2019-06-16 07:07:16.919192: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n    2019-06-16 07:07:16.919204: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n    2019-06-16 07:07:16.919215: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n    2019-06-16 07:07:16.942508: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3598270000 Hz\r\n\r\nIs it possible that the Maven artifacts are out of date in Maven Central?  What is the difference between the shared objects hosted in Maven vs storage.googleapis?", "Update, the solution to this issue is related to the Maven shade plugin.  With Maven shade, an exclusion must be made to enable GPU support:\r\n\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>tensorflow</artifactId>\r\n            <exclusions>\r\n                <exclusion>\r\n                    <groupId>org.tensorflow</groupId>\r\n                    <artifactId>libtensorflow_jni</artifactId>\r\n                </exclusion>\r\n            </exclusions>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>proto</artifactId>\r\n        </dependency>\r\n        <dependency>\r\n            <groupId>org.tensorflow</groupId>\r\n            <artifactId>libtensorflow_jni_gpu</artifactId>\r\n        </dependency>\r\n\r\nIt would be nice if this was documented on the [Java installation guideline](https://www.tensorflow.org/install/lang_java), or better would be to publish Maven artifacts which explicitly provide the required native support and not bundle libtensorflow_jni with the [tensorflow dependency](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/maven/tensorflow/pom.xml). ", "Thanks @achandraa for adding the docs tag.  Here's another interesting thing to highlight in the solution.  If you want seamless Mac OSX support, plus automatic Linux support for GPU, then any Maven Shade Plugin solution for creating wide jars will also need filter exclusions:\r\n\r\n\t<filters>\r\n\t    <filter>\r\n\t        <artifact>org.tensorflow:libtensorflow_jni</artifact>\r\n\t        <excludes>\r\n\t            <exclude>org/tensorflow/native/linux-x86_64/**</exclude>\r\n\t            <exclude>org/tensorflow/native/windows-x86_64/**</exclude>\r\n\t        </excludes>\r\n\t    </filter>\r\n\t    <filter>\r\n\t        <artifact>org.tensorflow:libtensorflow_jni_gpu</artifact>\r\n\t        <includes>\r\n\t            <include>org/tensorflow/native/linux-x86_64/**</include>\r\n\t            <include>org/tensorflow/native/windows-x86_64/**</include>\r\n\t        </includes>\r\n\t    </filter>\r\n\t</filters>\r\n\r\nAs an aside, since the shared objects in `org.tensorflow:libtensorflow_jni_gpu` will downgrade to CPU when there is no GPU, why not bundle these with `org.tensorflow:libtensorflow_jni`? ", "@ghost We see you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29834, "title": "lite: fix buffer overrun in object detection postprocess.", "body": "In NonMaxSuppressionMultiClassRegularHelper() function,\r\nThe size of sorted_indices is max_detections.\r\nIt could overrun and get corrupted.\r\nSo I added the guard.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29834) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "I checked my CLA data.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29834) for more info**.\n\n<!-- ok -->", "Because I found a issue in my patch, I will cancel this pull request.\r\nAfter I revise the patch, I will request again.\r\nSorry for bothering you."]}, {"number": 29833, "title": "lite: fix buffer overrun at object detection postprocess.", "body": "The size of sorted_indices is max_detections.\r\nIt could overrun and get memory corrupted.\r\nSo I added the guard.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29833) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 29832, "title": "Using TF estimator with tf.data fetched from tfds", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.00-beta\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to use a pre-made estimator with a dataset fetched from tensorflow_dataset. I have followed the steps from the guide, but it gives an error:\r\n\r\n```\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n```\r\n\r\nI have also looked at this other issue ([issue #24592](https://github.com/tensorflow/tensorflow/issues/24592)) but the solution provided there does not apply to me.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nI am trying to use the pre-made estimator `tf.estimator.DNNClassifier` to use on the MNIST dataset. I load the dataset from `tensorflow_dataset`.\r\n\r\nI pursue the following four steps: first building the dataset pipeline and defining the input function:\r\n\r\n```python\r\n## Step 1\r\nmnist, info = tfds.load('mnist', with_info=True)\r\n\r\nds_train_orig, ds_test = mnist['train'], mnist['test']\r\n\r\ndef train_input_fn(dataset, batch_size):\r\n    dataset = dataset.map(lambda x:({'image-pixels':tf.reshape(x['image'], (-1,))}, \r\n                                    x['label']))\r\n    return dataset.shuffle(1000).repeat().batch(batch_size)\r\n```\r\n\r\nThen, in step 2, I define the feature column with a single key, and shape 784:\r\n\r\n```python\r\n## Step 2:\r\nimage_feature_column = tf.feature_column.numeric_column(key='image-pixels',\r\n                                                        shape=(28*28))\r\n\r\nimage_feature_column\r\nNumericColumn(key='image-pixels', shape=(784,), default_value=None, dtype=tf.float32, normalizer_fn=None)\r\n```\r\n\r\nStep 3, I instantiated the estimator as follows:\r\n\r\n```python\r\n## Step 3:\r\ndnn_classifier = tf.estimator.DNNClassifier(\r\n    feature_columns=image_feature_column,\r\n    hidden_units=[16, 16],\r\n    n_classes=10)\r\n```\r\n\r\nAnd finally, step 4 using the estimator by calling the `.train()` method:\r\n\r\n```python\r\n## Step 4:\r\ndnn_classifier.train(\r\n    input_fn=lambda:train_input_fn(ds_train_orig, batch_size=32),\r\n    #lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\r\n    steps=20)\r\n```\r\n", "comments": ["Solved, based on the answer from Stackoverflow: https://stackoverflow.com/questions/56612386/defining-the-input-function-for-tensorflow-pre-made-estimator/56615591#56615591 \r\n\r\nFor eager execution, I should define the tfds dataset inside the function. ", "@vmirly same error ...... Thanks for your answer , but I think ```For eager execution, I should define the tfds dataset inside the function.``` should be written in tensorflow estimator document ......"]}, {"number": 29831, "title": "ImportError: Could not find 'cudart64_.dll'.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro (Build 17763)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: v2.0.0-beta1\r\n- Python version: Python 3.6.8 (Anaconda)\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0, 7.3\r\n- GPU model and memory: GTX 1080, 8gb\r\n\r\n**Describe the problem**\r\nI get the following error during build:\r\n\r\nERROR: C:/tools/tensorflow/tensorflow/python/keras/api/BUILD:46:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/cover/_bazel_cover/cchquwgb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n\r\n\u2026 bunch of stuff \u2026 then\u2026\r\n\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_edzmzyvw\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_sfe42hqk\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_sfe42hqk\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_sfe42hqk\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_sfe42hqk\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 379.181s, Critical Path: 375.70s\r\nINFO: 572 processes: 572 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nAll of my paths to CUDA are included in %PATH% and I've ensured cudart64_100.dll is located in a directory in my path.  Its interesting that the error doesn't include the correct name of the file.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n![image](https://user-images.githubusercontent.com/33430083/59557596-815f1480-8f92-11e9-84a6-a794cbbfc7e5.png)\r\n*Using python 3.6.8 instead of 3.7 as above\r\n\r\nI'm using all defaults for configure.py, aside from setting cuda to yes, and my compute capability to 6.1.\r\n\r\nbazel build --config=v2 --config=mkl --config==cuda -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I'm a bit of a noob, so this might actually be a problem with my own setup... looking at the error, I have to ask, could this be a conflict between msys64 and WSL on my system?  If that's the case, is there an easy way to fix that conflict?", "@CovertKoala  You're not noob! I've been doing this stuff for like 7 years, and I'm going to be blunt. This bazel, msys, vscode nonsense is the worst build process I've ever had to deal with. The whole ecosystem is just an absolute nightmare. \r\n\r\nIn any case, I was able to get past this error by exporting:\r\n```\r\nexport TF_CUDNN_VERSION=7\r\n# You might want to change your cuda version according to your own\r\nexport TF_CUDA_VERSION=10.1\r\n```\r\nI'm building again right now. Who knows what will be next in the cavalcade of errors. \r\n", "@rlewkowicz ,  Thanks for the affirmation and the tip.  I've spent WAY too much time trying to get things to work.  Setting those variables did help me get past that error, but now I'm getting stuck on a separate issue regarding nvcc and permission errors...  I may just switch back to CNTK.  It was way faster for my model than TF out of the box with no need for a build or additional components outside of CUDA.", "I lied apparently... setting those variable didn't fix anything... I just got hung up on other errors before ever getting to see it on this latest attempt.  Thanks though!", "This string of errors might be helpful...???\r\n\r\nbazel build --config=opt --config=cuda --config=v2 -k --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=199\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tools\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'build' from c:\\tools\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/lib/site-packages --python_path=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:cuda in file c:\\tools\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\tools\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:monolithic in file c:\\tools\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:opt in file c:\\tools\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\tools\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\tools\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:v2 in file c:\\tools\\tensorflow\\.tf_configure.bazelrc: --define=tf_api_version=2\r\nWARNING: C:/tools/tensorflow/tensorflow/python/BUILD:3577:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: C:/tools/tensorflow/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/tools/tensorflow/tensorflow/python/keras/api/BUILD:29:1: Couldn't build file tensorflow/python/keras/api/_v1/__init__.py: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/cover/_bazel_cover/cchquwgb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Program Files\\Microsoft MPI\\Bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.3.0;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Windows\\System32\\WindowsPowerSh;C:\\tools;C:\\msys64;C:\\msys64\\usr\\bin;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\lib;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen_compat_v1.exe  --apidir=bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api_v1/ --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api._v1 bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/activations/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/backend/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/callbacks/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/constraints/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/estimator/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/initializers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/layers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/losses/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/metrics/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/models/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/optimizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/regularizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/utils/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/wrappers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v1/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles__40bn074\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles__40bn074\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles__40bn074\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles__40bn074\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles__40bn074\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nERROR: C:/tools/tensorflow/tensorflow/python/keras/api/BUILD:46:1: Couldn't build file tensorflow/python/keras/api/_v2/__init__.py: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/cover/_bazel_cover/cchquwgb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Program Files\\Microsoft MPI\\Bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.3.0;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Windows\\System32\\WindowsPowerSh;C:\\tools;C:\\msys64;C:\\msys64\\usr\\bin;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\lib;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.exe  --apidir=bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api_v2/ --apiname=keras --apiversion=2  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api._v2 bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/activations/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/backend/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/callbacks/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/constraints/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/estimator/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/initializers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/losses/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/metrics/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/models/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/regularizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/utils/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/_v2/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_uzkhmihn\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_uzkhmihn\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_uzkhmihn\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_uzkhmihn\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_uzkhmihn\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nERROR: C:/tools/tensorflow/tensorflow/BUILD:797:1: Couldn't build file tensorflow/_api/v2/v2.py: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/cover/_bazel_cover/cchquwgb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Program Files\\Microsoft MPI\\Bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.3.0;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Windows\\System32\\WindowsPowerSh;C:\\tools;C:\\msys64;C:\\msys64\\usr\\bin;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\lib;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/bin/tensorflow/tf_python_api_gen_v2.genrule_script.sh\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_f793v21q\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_f793v21q\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_f793v21q\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_f793v21q\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_f793v21q\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nERROR: C:/tools/tensorflow/tensorflow/python/keras/api/BUILD:13:1: Couldn't build file tensorflow/python/keras/api/__init__.py: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit 1): bash.exe failed: error executing command\r\n  cd C:/users/cover/_bazel_cover/cchquwgb/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Program Files\\Microsoft MPI\\Bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\iCLS;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\Intel\\WiFi\\bin;C:\\Program Files\\Common Files\\Intel\\WirelessCommon;C:\\Program Files\\Git\\cmd;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.3.0;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\mingw-w64\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\usr\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Library\\bin;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\Scripts;C:\\Users\\Cover\\anaconda3\\envs\\fast_tf2\\bin;C:\\Users\\Cover\\anaconda3\\condabin;C:\\Windows\\System32\\WindowsPowerSh;C:\\tools;C:\\msys64;C:\\msys64\\usr\\bin;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\lib;C:\\tools\\TensorRT-5.1.5.0.Windows10.x86_64.cuda-10.1.cudnn7.5\\TensorRT-5.1.5.0\\bin\r\n    SET PYTHON_BIN_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Cover/anaconda3/envs/tf2fast/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\n    SET TF_NEED_CUDA=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_1_keras_python_api_gen.exe  --apidir=bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api --apiname=keras --apiversion=1  --package=tensorflow.python,tensorflow.python.keras --output_package=tensorflow.python.keras.api bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/activations/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/backend/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/callbacks/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/constraints/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/estimator/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/initializers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/layers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/layers/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/losses/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/metrics/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/mixed_precision/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/mixed_precision/experimental/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/models/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/optimizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/optimizers/schedules/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/regularizers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/utils/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/wrappers/__init__.py bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras/wrappers/scikit_learn/__init__.py\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_lzyx__wr\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Users\\Cover\\anaconda3\\envs\\tf2fast\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_lzyx__wr\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_lzyx__wr\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_lzyx__wr\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"\\\\?\\C:\\Users\\Cover\\AppData\\Local\\Temp\\Bazel.runfiles_lzyx__wr\\runfiles\\org_tensorflow\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA  from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 16.667s, Critical Path: 6.58s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "I think this message is the key\r\n```\r\nImportError: Could not find 'cudart64_.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA from this URL: https://developer.nvidia.com/cuda-90-download-archive\r\n```\r\n\r\nIf you installed CUDA through conda, it is possible that it did not update your `PATH` environment variable to include the cuda libraries. You may need to find which directory these dlls are in, and then update your `PATH` to include that directory.", "Thank you for the response.  I installed Cuda using NVIDIA's installer (https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exenetwork)\r\n\r\nI did verify that the location (C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin) was included in my PATH variable (as also verified in the error message above).\r\n\r\nIt appears that the tensorflow\\python\\platform\\self_check.py is receiving an incomplete cudart64 dll name... specifically, the version number isn't being inserted into the file name (ie, it needs \"cudart64_100.dll\" vice \"cudart64_.dll\").  There's an unset variable somewhere that's preventing the name from being created correctly.  Since Googling the issue doesn't come up with any definitive answers or similar problems, I'm assuming it is likely an issue with my machine.  I've been digging to see if I can find where it should be set without much luck.", "That is interesting. Did you run configure before building?\r\nhttps://www.tensorflow.org/install/source_windows#configure_the_build", "I did.", "Looking closer to your command, I see that you are using these flags:\r\n`-k --define=no_tensorflow_py_deps=true ` these two should not be used for building the pip package.\r\nMay I ask which guide you are using to see if I can follow the guide and reproduce the problem?", "I'm using your guide...\r\nhttps://www.tensorflow.org/install/source_windows#bazel_build_options\r\n\r\nPlus https://docs.bazel.build/versions/master/command-line-reference.html for the \"-k\" option.  I had been getting errors in a random order after long waits, which made it difficult to troubleshoot.  Using \"-k\" helped me build as much as possible, so that when I attempted fixes on my own, I'd get fast feedback on their effects.\r\n\r\nModified as I understood it to work for building 2.0.", "adding @meteorcloudy @laszlocsomor, bazel windows team to help.\r\nNot sure if the flag is still useful for building the pip package.\r\n\r\n@chsigg about the windows CUDA library name issues.", "I'm attempting another rebuild with those two flags removed as well as manually setting:\r\n![image](https://user-images.githubusercontent.com/33430083/59941816-0030d300-9413-11e9-9893-eab242ecad07.png)\r\n\r\nThose variable are kind of a shot in the dark (due to my inexperience) based on what I saw (~Line 560) in a CMakeList.txt file found here \"C:\\tools\\tensorflow\\tensorflow\\contrib\\cmake\" \r\n\r\nAfter running \"python configure.py\", running \"echo %short_CUDA_VER%\" didn't return a value.", "Those changes did not fix the issue.  I got the same:\r\n\"ImportError: Could not find 'cudart64_.dll'.\"", "Quick and dirty solution:\r\n1) Avoid the cudart64_.dll not found error\r\nCreate a copy of:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudart64_100.dll as cudart64_.dll in the same folder\r\ni.e. I kept both the original file and the renamed file in the same folder.\r\n\r\n2) Avoid the cudnn64_.dll not found error\r\nCreate a copy of:\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_100.dll as cudnn64_.dll in the same folder\r\ni.e. I kept both the original file and the renamed file in the same folder.\r\n", "> If you installed CUDA through conda, it is possible that it did not update your PATH environment variable to include the cuda libraries. You may need to find which directory these dlls are in, and then update your PATH to include that directory.\r\n\r\n@gunan It doesn't appear to be the issue here, but this advice is going to break in Python 3.8 (and potentially earlier versions of Anaconda for the same reason, as they have hit even more issues than CPython and made other fixes).\r\n\r\nIt is already broken for Python 3.7 from the Microsoft Store package, as the operating system enforces the DLL loading restrictions unconditionally. Again, doesn't appear to be the cause here, but you will certainly see it coming up more often.\r\n\r\nThe details and workaround for 3.8 are at https://docs.python.org/3.8/whatsnew/3.8.html#bpo-36085-whatsnew", "I'm having the same problem. It appears that build_info.py is supposed to be generated by calling gen_build_info and passing in cuda/cudnn versions via environment variables TF_CUDA_VERSION and TF_CUDNN_VERSION ([here](https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/tensorflow.bzl#L2290-L2307)?). However when gen_build_info is called, neither environment variable is set. More over, if I set TF_CUDA_VERSION to e.g. 101 before invoking the build, it will fail to find cuda.h - there is inconsistency between path of cuda.h (10.1) and how the dll is named (cudart64_101.dll).", "Experienced the same problem. After running configure.py script I had to manually add the following lines to the generated **.tf.configure.bazelrc** file:\r\n\r\n```\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\n```\r\nAfter this the build process succeeded.\r\nThe environment: \r\nWindows 10, tensorflow v1.14.0,  CUDA Toolkit 10.1.168, CUDNN - 7.6.2.24, Bazel - 0.25.2\r\n\r\nHope this helps someone.", "I am running into this issue with the conda/pip install of tensorflow-gpu in a Windows10 environment.  Any chance the fix above can be incorporated for that pkg so I can avoid having to build from source?\r\nTIA", "@CovertKoala Is this still an issue? Can you check with `TF2.0` and let us know whether the issue persists with latest TF version. Thanks!", "Hi @CovertKoala , Could you please check with similar issues [link1,](https://github.com/tensorflow/tensorflow/issues/30539),l[ink2](https://github.com/tensorflow/tensorflow/issues/43776#issuecomment-705999765) and  Also Try with Latest version 2.6 with instructions from [here ](https://www.tensorflow.org/install) to see if issue persists.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29831\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29831\">No</a>\n"]}, {"number": 29830, "title": "lite: extending SparseToDense op", "body": "* enable int8/uint8 for sparse2dense\r\n* enable quantization convert in toco for sparse2dense", "comments": ["a gentle ping @jianlijianli @suharshs :)"]}, {"number": 29829, "title": "Segmentation fault using tf.lite.TFLiteConverter with representative_dataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 2.0beta1\r\n- **Python version**: 3.6.8\r\n- **Bazel version (if compiling from source)**:  0.24.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.4.0\r\n- **CUDA/cuDNN version**: 10.0\r\n- **GPU model and memory**: GTX2080TI 11 GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to quantize (post-training) my model, using a representative dataset as explained in the latest documentation. I tried to build the representative dataset by loading some relevant PNGs from a folder. But I get a segmentation fault error when I set the representative dataset (no error if I do not set it or use mnist as dataset).\r\n\r\n### Source code / logs\r\nThis works:\r\n\r\n```\r\nmnist_train, _ = tf.keras.datasets.mnist.load_data()\r\nimages = tf.cast(mnist_train[0], tf.float32)/255.0\r\nmnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in mnist_ds.take(100):\r\n    yield [input_value]\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\nThis doesn't (SEGFAULT):\r\n\r\n```\r\ntrain = []\r\nfiles = glob.glob ('/myPath/data/images/test/*.png')\r\n\r\nfor myFile in files:\r\n    image = cv2.imread (myFile)\r\n    train.append (image)\r\ntrain = np.array(train,dtype='float32') #as mnist\r\nimages = tf.cast(train[0], tf.float32)/255.0\r\nmy_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n    \r\ndef representative_data_gen():\r\n  for input_value in my_ds.take(100):\r\n    yield [input_value]    \r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n....\r\n2019-06-15 20:40:32.261287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-15 20:40:32.261634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10246 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nW0615 20:40:32.319736 140096456644416 deprecation_wrapper.py:118] From quantize.py:47: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f6abf885740 (most recent call first):\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 197 in _calibrate_quantize_model\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 922 in convert\r\n  File \"quantize.py\", line 50 in <module>\r\n\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n\r\nIf I use ciphar instead of mnist as representative dataset I also get the same SEGFAULT.\r\n\r\n```cifar_train, _ = tf.keras.datasets.cifar10.load_data()\r\nimages = tf.cast(cifar_train[0], tf.float32) / 255.0\r\ncifar_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in cifar_ds.take(100):\r\n    yield [input_value]`\r\n\r\nAny ideas? Thank you.\r\n", "comments": ["Are you also setting `converter.representative_dataset = representative_data_gen` in both examples?\r\n\r\nCould you provide the complete code for the working MNIST case and the failing cifar test case (the ones where you are using tf.keras.datasets), to help reproduce the error. Thanks!", "Hello suharshs. Yes, I am setting converter.representative_dataset = representative_data_gen, that's what triggers the SEGFAULT (if I do not set that the process doesn't crash at least). I'll post the code you ask later. Thank you.\r\n\r\n", "I'm sorry, but it seems I accidentally deleted that line in the mnist example that seemed to work (converter.representative_dataset = representative_data_gen).\r\n\r\nSo now I get SEGFAULT with mnist also, and here it is one example of the code that crashes (I got this idea from here: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb)\r\n\r\n```\r\nimport glob\r\nimport cv2\r\nimport numpy as np\r\nimport os\r\nfrom PIL import Image\r\nimport random\r\nimport tensorflow as tf\r\nimport sys\r\nimport faulthandler; faulthandler.enable()\r\n\r\nif sys.version_info.major >= 3:\r\n    import pathlib\r\nelse:\r\n    import pathlib2 as pathlib\r\nfrom pathlib import Path\r\n\r\nos.environ['PYTHONPATH'] = ':/content/models/research/:/content/models/research/slim/'\r\ntf.enable_eager_execution()\r\ntf.logging.set_verbosity(tf.logging.DEBUG)\r\n\r\nfrozen_graph='/somePath/tflite/saved_model.pb'\r\ninput_arrays=[\"normalized_input_image_tensor\"]\r\noutput_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\ninput_shapes={\"normalized_input_image_tensor\":[1,300,300,3]}\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_graph,input_arrays,output_arrays,input_shapes=input_shapes)\r\nconverter.allow_custom_ops=True\r\n\r\nmnist_train, _ = tf.keras.datasets.mnist.load_data()\r\nimages = tf.cast(mnist_train[0], tf.float32)/255.0\r\nmnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in mnist_ds.take(100):\r\n    yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset=representative_data_gen\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = \"/somePath/tflite/quantized.tflite\"\r\np = Path(tflite_model_quant_file)\r\np.write_bytes(tflite_quant_model)\r\n```\r\n### Output:\r\n\r\n```\r\npython quantize2.py \r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0617 17:23:37.896252 140342202963776 deprecation_wrapper.py:118] From quantize2.py:18: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\r\n\r\nW0617 17:23:37.896475 140342202963776 deprecation_wrapper.py:118] From quantize2.py:19: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\r\n\r\nW0617 17:23:37.896584 140342202963776 deprecation_wrapper.py:118] From quantize2.py:19: The name tf.logging.DEBUG is deprecated. Please use tf.compat.v1.logging.DEBUG instead.\r\n\r\n2019-06-17 17:23:37.903709: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-06-17 17:23:43.597809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.598136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:01:00.0\r\n2019-06-17 17:23:43.598281: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-17 17:23:43.598920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-17 17:23:43.599496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-17 17:23:43.599652: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-17 17:23:43.600447: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-17 17:23:43.601066: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-17 17:23:43.602911: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-17 17:23:43.602994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.603347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.603640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-17 17:23:43.628186: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz\r\n2019-06-17 17:23:43.629547: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b49b80 executing computations on platform Host. Devices:\r\n2019-06-17 17:23:43.629572: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-17 17:23:43.766065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.766403: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x37b6d40 executing computations on platform CUDA. Devices:\r\n2019-06-17 17:23:43.766415: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2019-06-17 17:23:43.766522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.766808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:01:00.0\r\n2019-06-17 17:23:43.766834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-17 17:23:43.766843: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-17 17:23:43.766851: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-17 17:23:43.766859: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-17 17:23:43.766867: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-17 17:23:43.766876: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-17 17:23:43.766885: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-17 17:23:43.766912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.767203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.767469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-17 17:23:43.767488: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-17 17:23:43.768066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-17 17:23:43.768074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-06-17 17:23:43.768080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-06-17 17:23:43.768133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.768426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.768716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10311 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2019-06-17 17:23:43.982463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.982782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65\r\npciBusID: 0000:01:00.0\r\n2019-06-17 17:23:43.982815: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-06-17 17:23:43.982825: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-06-17 17:23:43.982835: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-06-17 17:23:43.982860: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-06-17 17:23:43.982868: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-06-17 17:23:43.982875: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-06-17 17:23:43.982897: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-06-17 17:23:43.982967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.983295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.983592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-06-17 17:23:43.983609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-17 17:23:43.983613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-06-17 17:23:43.983616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-06-17 17:23:43.983688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.984070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-17 17:23:43.984359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10311 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nW0617 17:23:44.392145 140342202963776 deprecation_wrapper.py:118] From quantize2.py:39: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007fa3f7279740 (most recent call first):\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 197 in _calibrate_quantize_model\r\n  File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 922 in convert\r\n  File \"quantize2.py\", line 42 in <module>\r\nSegmentation fault (`core' dump)\r\n```\r\nCiphar example (also crashes)\r\n\r\n[https://github.com/tensorflow/tensorflow/issues/29443](https://github.com/tensorflow/tensorflow/issues/29443)", "Hey hvico, the segfault of mnist seems strange. I am investigating with the code you provided. Will post here once I have any update. Thanks.", "Hi hvico, the representative_data_gen part is reasonable and I suspect the error is from some mismatch between the data and the model. Is it possible for you to share the model? Thanks.", "representative_data_gen should(better) be the images that used to train our model? \r\n@jianlijianli ", "I ran into a similar problem and debugged my script using PDB, caught an exception related to custom operators. I then removed the custom operator from the model. Then the conversion finished without a problem.", " @jianlijianli  hello,have you solved the issue ? I have the same problem as yours. But it seems to work when you downgrade your tensorflow version ``tensorflow-gpu==1.13.1``, but then you should use ``converter.post_training_quantize=True`` instead of ``convert.optimizations= [tf.lite.Optimize.DEFAULT]``, because the last one haven't exist in the early version yet . \r\nThen I noticed that the size of my graph is about 1/4 as the original one, thus when test in the mobile the fps don't look like improve, do you have any idea of this ?\r\n", "@chenyuZha I think by default a lot of the operations in your graph is still floating point-based hence there might not be a speed advantage. But your weights are stored as uint8 instead of float32 hence you see 1/4 size reduction. For speed improvement, you need to set converter.inference_type = tf.lite.constants.QUANTIZED_UINT8. This will require you to have added fake quant nodes in the training step. See tutorial https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize.", "@LiyouZhou  Thanks a lot for your response. But I have a question when I converted my model to type QUANTIZED_UINT8, which is as I took a **non-quantized** model for training(ex: ssd_mobilenet_v2_coco), so I must give a the **min** and **max** values for activations(``default_ranges_stats`` in  TFLiteConverter). But for the training aware models, these values are calculated during training. I wonder how can I get the correct range when I convert my float model to quantized uint8..?Because once I gave  like ``min=0`` and ``max=255``, I got my quantized model, but it fails to predict bounding-boxes during inference.. \r\n\r\n", "I have not figured out how to convert model fully to 8 bits post-training. I suspect it cannot be done. The tool always prompts warning saying I should insert fake quant into training graph. For an example of quantized training https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config#L195-L201", "@LiyouZhou  Ok I see.. So the best option to 'quantize' a float model is using ``representative_dataset`` to define the dynamic range of activations, which returns back to the topic of this issue:**segmentation fault**. I wonder if it is related the version of tensorflow: as I trained my model with ``tensorflow-gpu==1.13.1`` which is not as same as the version required(``tensorflow-gpu==1.14.0``) to process representative_dataset option...", "#26412", "So it is confirmed, it does not support custom ops.\r\n", "@LiyouZhou  so I've rerun ``export_tflite_ssd_graph.py`` with flag ``add_postprocessing_op=false``, in the end I got 3 raw outputs :``'raw_outputs/box_encodings','raw_outputs/class_predictions','anchors' ``. By using the TFliteConverter I've succeeded to get my tflite file( I didn't set **representative_dataset** yet in order to see if with these kind of outputs I can run inference with mobile). Now I get another problem of output size:  `` Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 10, 4].``\r\nI suppose is that as I didn't add post processing operations, the 1917 corresponds the num of anchors which is not the num of max_boxes detections. ", "@chenyuZha You basically have to now implement what TFLite_Detection_PostProcess https://github.com/tensorflow/tensorflow/blob/f34d571332336f325223be6030368390283818cf/tensorflow/lite/kernels/detection_postprocess.cc#L716 does yourself. \r\n\r\nThis looks messy but involves the following steps:\r\n1. Get a list of the anchor boxes. When you ran the export, they are written into a tf.constant node. This means you can extract its value like this:\r\n    ```\r\n    anchors = tf.get_default_graph().get_tensor_by_name(\"anchors:0\")\r\n    output = sess.run(anchors)\r\n    np.save(\"ssd_anchors.npy\", output)\r\n    ``` \r\n     In your case there should be 1917 anchor boxes.\r\n2. The box_encodings are coded and you need to decode them using anchor box to get the true values. You have now 1917 box_encodings, one for each anchor box.\r\n3. The exact way to decode is defined in your training config in the box_coder section, you can find the python code to decode in this folder https://github.com/tensorflow/models/tree/master/research/object_detection/box_coders\r\n4. Do a bit of reverse engineer and try to find the exact sequence to decode your boxes. In my case the trick bit was that anchor box values are scaled by the image width and height.\r\n\r\n\r\nfor class_predictions you should get an array of [1, 1917, num_classes+1] this is the score of each class for each box_encodings. You are mostly just interested in the class with the best score for each row. I don't think class_predictions[1, :,  0] actually mean anything. ", "@LiyouZhou  Thanks a lot for your advice.. So all what I should do is to find the way to decode the bbox then add the bloc to the train config, then I re-run the export_ssdlite_graph.py with this new config ? \r\n ", "@chenyuZha I think so, try it and let me know how it goes.", "I'm running into the same issue trying for an 8-bit quantized model post training. Was re-implementing the postprocess succesful? \r\n\r\nEDIT:\r\nI was able to get past my segfault by modifiying the number of classes output by the object_detection/export_tflite_ssd_graph.py that I had been using to convert my graph to a tflite format. It was defaulting to outputting probability of one class per bounding box, once I modified it to output probability for all classes, the maths stopped crashing. Then I ran into RESIZE_NEAREST_NEIGHBOR not being supported - but atleast I got the segfault figured out. ", "> ### System information\r\n> * **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n> * **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n> * **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n> * **TensorFlow installed from (source or binary)**: Source\r\n> * **TensorFlow version (use command below)**: 2.0beta1\r\n> * **Python version**: 3.6.8\r\n> * **Bazel version (if compiling from source)**:  0.24.1\r\n> * **GCC/Compiler version (if compiling from source)**: 7.4.0\r\n> * **CUDA/cuDNN version**: 10.0\r\n> * **GPU model and memory**: GTX2080TI 11 GB\r\n> * **Exact command to reproduce**:\r\n> \r\n> ### Describe the problem\r\n> I am trying to quantize (post-training) my model, using a representative dataset as explained in the latest documentation. I tried to build the representative dataset by loading some relevant PNGs from a folder. But I get a segmentation fault error when I set the representative dataset (no error if I do not set it or use mnist as dataset).\r\n> \r\n> ### Source code / logs\r\n> This works:\r\n> \r\n> ```\r\n> mnist_train, _ = tf.keras.datasets.mnist.load_data()\r\n> images = tf.cast(mnist_train[0], tf.float32)/255.0\r\n> mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n> def representative_data_gen():\r\n>   for input_value in mnist_ds.take(100):\r\n>     yield [input_value]\r\n> \r\n> tf.logging.set_verbosity(tf.logging.INFO)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> tflite_quant_model = converter.convert()\r\n> ```\r\n> \r\n> This doesn't (SEGFAULT):\r\n> \r\n> ```\r\n> train = []\r\n> files = glob.glob ('/myPath/data/images/test/*.png')\r\n> \r\n> for myFile in files:\r\n>     image = cv2.imread (myFile)\r\n>     train.append (image)\r\n> train = np.array(train,dtype='float32') #as mnist\r\n> images = tf.cast(train[0], tf.float32)/255.0\r\n> my_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n>     \r\n> def representative_data_gen():\r\n>   for input_value in my_ds.take(100):\r\n>     yield [input_value]    \r\n> \r\n> tf.logging.set_verbosity(tf.logging.INFO)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> tflite_quant_model = converter.convert()\r\n> ```\r\n> \r\n> Output:\r\n> \r\n> ```\r\n> ....\r\n> 2019-06-15 20:40:32.261287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2019-06-15 20:40:32.261634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10246 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n> W0615 20:40:32.319736 140096456644416 deprecation_wrapper.py:118] From quantize.py:47: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\r\n> \r\n> Fatal Python error: Segmentation fault\r\n> \r\n> Current thread 0x00007f6abf885740 (most recent call first):\r\n>   File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n>   File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 197 in _calibrate_quantize_model\r\n>   File \"/usr/tf2/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 922 in convert\r\n>   File \"quantize.py\", line 50 in <module>\r\n> \r\n> Segmentation fault (core dumped)\r\n> ```\r\n> \r\n> If I use ciphar instead of mnist as representative dataset I also get the same SEGFAULT.\r\n> \r\n> ```\r\n> images = tf.cast(cifar_train[0], tf.float32) / 255.0\r\n> cifar_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n> def representative_data_gen():\r\n>   for input_value in cifar_ds.take(100):\r\n>     yield [input_value]`\r\n> \r\n> Any ideas? Thank you.\r\n> ```\r\n\r\nHi.Can you solve it?\r\nI need that", "@Davari393 Maybe it is to do with input shape not what the network is expecting?", "@Jconn I got it working with the custom posts processing op (tf 1.14.0), This is what I did to convert:\r\n```\r\npython $WORKINGDIR/object_detection/export_tflite_ssd_graph.py \\\r\n    --pipeline_config_path=ssd_mobilenet_v1_coco.config \\\r\n    --trained_checkpoint_prefix=models/train/model.ckpt-50000 \\\r\n    --output_directory=tmp/ \\\r\n    --add_postprocessing_op=true\r\n\r\ntflite_convert \\\r\n    --graph_def_file=${TF_FL32_GRAPH_FN} \\\r\n    --output_file=models/export/ssd_mobilenet_v1_bstld.tflite \\\r\n    --input_shapes=\"1,265,960,3\" \\\r\n    --input_arrays=normalized_input_image_tensor \\\r\n    --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n    --change_concat_input_ranges=false \\\r\n    --mean_values=\"128\" \\\r\n    --std_dev_values=\"127\" \\\r\n    --inference_type=QUANTIZED_UINT8 \\\r\n    --allow_custom_ops\\\r\n\r\n", "> @Davari393 Maybe it is to do with input shape not what the network is expecting?\r\n\r\nMy model is mobilenetv2 ssdlite that its input shape is 300x300x3\r\nmy code is:\r\n\r\ncoco_train = []\r\ndirectory_images = \"/home/davari/detection_on_mobile/val_coco_dataset/coco2014/output/kam_images/\"\r\nfor img in os.listdir(directory_images):\r\n  coco_data = cv2.imread(os.path.join(directory_images,img))\r\n  coco_data = cv2.resize(coco_data , (300 , 300))\r\n  coco_train.append(coco_data)\r\nprint(tuple(coco_train))\r\n\r\ndef representative_data_gen():\r\n  for input_value in tuple(coco_train):\r\n    yield [input_value]\r\n\r\n\r\nconverter.representative_dataset = representative_data_gen\r\ntflite_model_quant = converter.convert()\r\nopen(\"/home/davari/detection_on_mobile/MobilenetV2SSDLite_model_quant.tflite\", \"wb\").write(tflite_model_quant)\r\n\r\nbut after run :\r\n\r\n32412 segmentation fault (core dumped)", "Some times you need to supply the input size of (1, 300, 300, 3). But with a segfault, there isn't really much information to go on. Try tracking down the failure point with pdb, that will tell you more information.", "Sorry to piggyback on an old issue. I'm using tf==1.15.3 and I'm hitting a segmentation fault attempting int8 quantization using `converter.representative_dataset`. I set a breakpoint at the beginning of my representative_dataset_gen function and it never hits it.\r\n\r\nI get a segmentation fault here when running the conversion with the `--add_postprocessing_op=true` as suggested:\r\n```\r\n.../tensorflow_core/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n```\r\nThe source code for the exact line that is causing the segmentation fault is [here in tensorflow/lite/python/optimize/calibration_wrapper.cc](https://github.com/tensorflow/tensorflow/blob/v1.15.3/tensorflow/lite/python/optimize/calibration_wrapper.cc#L216), I believe.\r\n\r\nThe full stack trace is here:\r\n```\r\n> .../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py(51)__init__()\r\n-> .CreateWrapperCPPFromBuffer(model_content))\r\n(Pdb) s\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007ff40ee9f740 (most recent call first):\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 51 in __init__\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 236 in _calibrate_quantize_model\r\n  File \".../python3.6/site-packages/tensorflow_core/lite/python/lite.py\", line 993 in convert\r\n  File \".../convert_model_to_tflite_int8.py\", line 97 in <module>\r\n  File \"<string>\", line 1 in <module>\r\n  File \"/usr/lib/python3.6/bdb.py\", line 434 in run\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1548 in _runscript\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1667 in main\r\n  File \"/usr/lib/python3.6/pdb.py\", line 1694 in <module>\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85 in _run_code\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193 in _run_module_as_main\r\n[1]    17668 segmentation fault (core dumped)  python -m pdb convert_model_to_tflite_int8.py  --add_postprocessing_op=true\r\n```\r\nLet me know if this is unrelated and I will open a new issue. I can't find any other issues more similar to my problem than this issue.\r\n\r\nAny other debugging tips to get to the root of my problem would be appreciated :)\r\n\r\nHere is my conversion code:\r\n```python\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n  graph_def_file=pb_model_path,\r\n  input_arrays=[\"device_0/input_node_name:1\"],\r\n  output_arrays=[\"device_0/output_node_name\"],\r\n  input_shapes={\"device_0/input_node_name:1\": [100, 16384]}\r\n)\r\nconverter.allow_custom_ops = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8  # or tf.uint8\r\nconverter.inference_output_type = tf.int8  # or tf.uint8\r\n\r\ndef test():\r\n  pdb.set_trace()\r\n  print(' ! ! ! representative_dataset_gen ! ! ! ')\r\n  zeros = np.zeros(shape=(1, 100, 16384), dtype='float64')\r\n  ds = tf.data.Dataset.from_tensor_slices((zeros)).batch(1)\r\n  for input_value in ds.take(1):\r\n    yield [input_value]\r\nconverter.representative_dataset = test\r\n\r\npdb.set_trace()\r\ntflite_model = converter.convert()\r\n\r\ntflite_model_size = open(model_name, 'wb').write(tflite_model)\r\nprint('TFLite Model is %d bytes' % tflite_model_size)\r\n```\r\n\r\nFWIW my model conversion works for `tf.float16` (not using `representative_dataset` there, though).\r\n\r\nEDIT: Upgrading my tf version to 2.3 solved the segmentation fault. My model code isn't compatible with tf==2.x yet, but luckily the conversion code is independent from that so the upgrade went smoothly.", "@hvico We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is not actively supported. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29828, "title": "Having problem modifying sample_distorted_bounding_box", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.14\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0/7.0\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHi all. I am trying to modify the behavior of tf.image.sample_distorted_bounding_box so I digged into sample_distorted_bounding_box_op.cc and made some changes including several lines of `cout << \"Hi\" << endl;` for logging. Then I recompiled this library using `bazel build //tensorflow/core/kernels/sample_distorted_bounding_box_op`. However when I reran tf.image.sample_distorted_bounding_box, it seems nothing changed. May I know what else should I do to modify the TensorFlow built-in api?\r\n\r\nThanks in advance!\r\n\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide faster and better help for such issues. Thanks!\r\n"]}, {"number": 29827, "title": "update core.py", "body": "fix typo in deprecation warning", "comments": ["Oh, this should go against master, we're only updating the other release branches on security patches if there has been already a final release.", "@mihaimaruseac Thanks for the information. I will send a new one against master."]}, {"number": 29826, "title": "Can't import Tensorflow GPU 2.0 beta on Win 7: DLL load failed with error code -1073741795", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Win 7 SP1**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version: **2.0 GPU Beta**\r\n- Python version: **3.6.8**\r\n- Installed using virtualenv? pip? conda?: **pip with no virtualenv**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **10.0**\r\n- GPU model and memory: **Geforce GTX 1050, 2GB memory**\r\n\r\n\r\n**Describe the problem**\r\nI am trying to install Tensorflow 2.0 GPU beta but it fails to download a DLL when trying to import the library.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Installed latest NVIDIA drivers\r\n- Installed CUDA Toolkit 10.0 (Sept 2018)\r\n- Installed cuDNN v7.6.0 (May 20, 2019), for CUDA 10.0\r\n- Made sure that the following path are in the PATH variable:\r\n```\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\include;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%\r\n```\r\n- Installed Tensorflow:\r\n```\r\npip install tensorflow-gpu==2.0.0-beta1\r\n```\r\n- Ran Python in command line and tried to import Tensorflow:\r\n```\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nScreenshots of my CUDA folders:\r\n![cuda_bin_folder](https://user-images.githubusercontent.com/31602326/59554008-3e3d7b00-8f6b-11e9-8018-a52c0052192e.png)\r\n![cuda_cupti_folder](https://user-images.githubusercontent.com/31602326/59554011-45648900-8f6b-11e9-83b8-d7ac84670695.png)\r\n![cuda_include_path](https://user-images.githubusercontent.com/31602326/59554014-47c6e300-8f6b-11e9-9170-0400c4004478.png)\r\n![cudnn_folder](https://user-images.githubusercontent.com/31602326/59554016-4a293d00-8f6b-11e9-8da2-8a8a6952b564.png)\r\n![cudnn64_7](https://user-images.githubusercontent.com/31602326/59554039-9d02f480-8f6b-11e9-95de-c1738edc9ddd.png)\r\n", "comments": ["Just to verify did you try to follow instructions from [TensorFlow website](https://www.tensorflow.org/install/pip) and downloaded [Microsoft Visual C++](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015)  .Also you can have a look on this [issue](https://github.com/tensorflow/tensorflow/issues/28848#issuecomment-495057676) and let us know if that helps. Thanks!", "This is what I tried:\r\n\r\n- I installed the Microsoft Visual C++ Redistributables and Build Tools: **Ok**\r\n\r\n- I tried to install the Microsoft Visual C++ 2015 Redistributable Update 3 but it fails, the log revealed that a newer version already exist (not sure if it is really an issue?):\r\n\r\n![vc_redist](https://user-images.githubusercontent.com/31602326/59606470-3dc5f100-90df-11e9-8841-3cd4e3f95b46.png)\r\n\r\n```\r\n[0624:1BF0][2019-06-17T08:34:09]i001: Burn v3.7.3813.0, Windows v6.1 (Build 7601: Service Pack 1), path: D:\\Downloads\\vc_redist.x64.exe, cmdline: ''\r\n[0624:1BF0][2019-06-17T08:34:09]i000: Setting string variable 'WixBundleLog' to value 'C:\\Users\\Sam\\AppData\\Local\\Temp\\dd_vcredist_amd64_20190617083409.log'\r\n[0624:1BF0][2019-06-17T08:34:09]i000: Setting string variable 'WixBundleOriginalSource' to value 'D:\\Downloads\\vc_redist.x64.exe'\r\n[0624:1BF0][2019-06-17T08:34:09]i000: Setting string variable 'WixBundleOriginalSourceFolder' to value 'D:\\Downloads\\'\r\n[0624:1BF0][2019-06-17T08:34:09]i000: Setting string variable 'WixBundleName' to value 'Microsoft Visual C++ 2015 Redistributable (x64) - 14.0.24215'\r\n[0624:1BF0][2019-06-17T08:34:10]i100: Detect begin, 10 packages\r\n[0624:1BF0][2019-06-17T08:34:10]i000: Setting version variable 'windows_uCRT_DetectKey' to value '10.0.10240.16390'\r\n[0624:1BF0][2019-06-17T08:34:10]i000: Setting numeric variable 'windows_uCRT_DetectKeyExists' to value 1\r\n[0624:1BF0][2019-06-17T08:34:10]i102: Detected related bundle: {5fb2083a-f3cc-4b78-93ff-bd9788b5de01}, type: Upgrade, scope: PerMachine, version: 14.16.27024.1, \r\n\r\noperation: Downgrade\r\n[0624:1BF0][2019-06-17T08:34:10]i108: Detected compatible package: vcRuntimeMinimum_x64, provider: Microsoft.VS.VC_RuntimeMinimumVSU_amd64,v14, installed: {F1B0FB3A-\r\n\r\nE0EA-47A6-9383-3650655403B0}, version: 14.16.27024, chained: {50A2BC33-C9CD-3BF1-A8FF-53C10A0B183C}\r\n[0624:1BF0][2019-06-17T08:34:10]i103: Detected related package: {F1B0FB3A-E0EA-47A6-9383-3650655403B0}, scope: PerMachine, version: 14.16.27024.0, language: 0 operation: \r\n\r\nDowngrade\r\n[0624:1BF0][2019-06-17T08:34:10]i108: Detected compatible package: vcRuntimeAdditional_x64, provider: Microsoft.VS.VC_RuntimeAdditionalVSU_amd64,v14, installed: \r\n\r\n{9D29FC96-9EEE-4253-943F-96B3BBFDD0B6}, version: 14.16.27024, chained: {EF1EC6A9-17DE-3DA9-B040-686A1E8A8B04}\r\n[0624:1BF0][2019-06-17T08:34:10]i103: Detected related package: {9D29FC96-9EEE-4253-943F-96B3BBFDD0B6}, scope: PerMachine, version: 14.16.27024.0, language: 0 operation: \r\n\r\nDowngrade\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.3 AND NOT VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.3 AND VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.2 AND NOT VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.2 AND VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.1 AND NOT VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.1 AND VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to true.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.0 AND NOT VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition '(VersionNT = v6.0 AND VersionNT64) AND (windows_uCRT_DetectKeyExists AND windows_uCRT_DetectKey >= v10.0.10240.0)' \r\n\r\nevaluates to false.\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: vcRuntimeMinimum_x64, state: Obsolete, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: vcRuntimeAdditional_x64, state: Obsolete, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows81_x86, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows81_x64, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows8_x86, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows8_x64, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows7_MSU_x86, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: Windows7_MSU_x64, state: Present, cached: Complete\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: WindowsVista_MSU_x86, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i101: Detected package: WindowsVista_MSU_x64, state: Absent, cached: None\r\n[0624:1BF0][2019-06-17T08:34:10]i052: Condition 'VersionNT64 >= v6.0 OR (VersionNT64 = v5.2 AND ServicePackLevel >= 1)' evaluates to true.\r\n[0624:1BF0][2019-06-17T08:34:10]i199: Detect complete, result: 0x0\r\n[0624:197C][2019-06-17T08:34:12]e000: Error 0x80070666: Cannot install a product when a newer version is installed.\r\n```\r\n\r\n- Make sure long paths are enabled on Windows.\u201cEnable NTFS long paths policy\u201d option does not exist on Windows 7, however I tried the suggestion to create a registry key at HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\\LongPathsEnabled] and setting the value (DWORD) to \"1\", didn't help:\r\nhttps://social.msdn.microsoft.com/Forums/en-US/fc85630e-5684-4df6-ad2f-5a128de3deef/260-character-explorer-path-length-limit?forum=windowsgeneraldevelopmentissues\r\n\r\n- pip3 install -U pip virtualenv\r\n\r\n```\r\nC:\\>pip3 install -U pip virtualenv\r\nRequirement already up-to-date: pip in c:\\program files\\python36\\lib\\site-packages (19.1.1)\r\nCollecting virtualenv\r\n  Using cached https://files.pythonhosted.org/packages/c4/9a/a3f62ac5122a65dec34ad4b5ed8d802633dae4bc06a0fc62e55fe3e96fe1/virtualenv-16.6.1-py2.py3-none-any.whl\r\nInstalling collected packages: virtualenv\r\nSuccessfully installed virtualenv-16.6.1\r\n```\r\n\r\n- dependency walker:\r\n\r\n![depends](https://user-images.githubusercontent.com/31602326/59606460-330b5c00-90df-11e9-891a-63b3a2d1f2a4.png)\r\n", "Looks like Microsoft Visual C++ is already installed on your system due to which lower version failed to install. Can you try uninstalling the software from your system and then try installing it through steps provided in [TensorFlow website](https://www.tensorflow.org/install/pip). Let us know. Thanks! ", "Which Microsoft Visual C++ should I uninstall? I have the regular Visual Studio Community 2017 and the Visual Studio Code along with their redists. See screenshot and please let me know. Also, if I uninstall something, some other program might break, this is one of my workstations.\r\n\r\n![visual](https://user-images.githubusercontent.com/31602326/59764821-95449800-926a-11e9-949c-0f92432c41ee.png)\r\n", "Since from the comment it looks newer version of Microsoft Visual C++ is there while installing version 2015 and because of which the error appeared, you can try uninstalling 2017 and installing 2015 from the TensorFlow website. Let us know if that helps. Thanks! ", "I have uninstalled Visual Studio 2017 and both its rediststributables. I was then able to install the Microsoft Visual C++ 2015 Redistributable Update 3 successfully this time. However, I still have the same issue (DLL load failed). \r\nIs there a way to make Tensorflow more verbose, i.e. print out the DLL that failed to load? This seems to be a common issue and that would help significantly.", "Now, we can try with [Dependency Walker](http://www.dependencywalker.com/) to find out which DLL is causing problem. For reference check this [issue](https://github.com/tensorflow/tensorflow/issues/28848#issuecomment-495057676) and let us know if you are able to move forward. Thanks!", "I tried dependency walker and the result looks the same as in the screenshot above that was done several days ago.", "TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets. This means on any CPU that do not have these instruction sets either CPU or GPU version of TF will fail to load. I suspect that your CPU model does not support AVX instruction set. Can you please confirm?. Thanks!", "Yes, I think you nailed the problem. My CPU is an Intel Core i7, but it's the first generation, from around 2011. I was hoping to use my GPU because of that. So I guess it won't work?", "You have couple of options;\r\n1) The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed stable TF version on it (currently running TF 1.14). Also you can use pip install to install any other preferred TF version if required.\r\nIt has added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. All you need is good internet connection and you are all set.\r\n2) You have to build TF from sources by changing CPU optimization flags.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 29825, "title": "Shape change during TPU training", "body": "Once I start train my model on TPU v3-8 I get this output and model start recompiling and continue train with different shape:\r\n```\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_target_30')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for lstm_input\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 5.74567985534668 secs\r\nINFO:tensorflow:Setting weights on TPU model.\r\n 9216/11126 [=======================>......] - ETA: 4s - loss: nan - categorical_accuracy: 0.0250INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(110,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(110, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(110, 4), dtype=tf.float32, name='dense_target_30')]\r\nINFO:tensorflow:Overriding default placeholder.\r\nINFO:tensorflow:Remapping placeholder for lstm_input\r\nINFO:tensorflow:Started compiling\r\nINFO:tensorflow:Finished compiling. Time elapsed: 8.460586786270142 secs\r\n```\r\n\r\nCompare shapes before train kicks in and after:\r\n```\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(128, 4), dtype=tf.float32, name='dense_target_30')]\r\nINFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(110,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(110, 1024, 7), dtype=tf.float32, name='lstm_input_10'), TensorSpec(shape=(110, 4), dtype=tf.float32, name='dense_target_30')]\r\n```\r\nModel shapes:\r\n```\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlstm_input (InputLayer)      (None, 1024, 7)           0         \r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 1024, 256)         270336    \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 1024, 256)         0         \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 256)               525312    \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 256)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 4)                 1028      \r\n=================================================================\r\n```\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- TensorFlow installed from (source or binary): 1.13.1\r\n- Python version: 3.5\r\n- GPU model and memory: TPU v3-8\r\n\r\n**Describe the current behavior**\r\n\r\nshapes change from 128 to 110 during first few batches training\r\n\r\n**Describe the expected behavior**\r\n\r\nshapes don't change\r\n\r\n", "comments": ["@maxima120 In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "I will need to send you data and code for this. I don't think you can use just a snippet. Please let me know how to contact you. Thanks", "@maxima120 Will it be possible you to share code and data in zip file. If it is not private. Thanks!", "code is no secret but data I don't want to put on the open. hope you understand.", "```\r\ndef split_sequence(features, labels, window_size):\r\n    X, y = list(), list()\r\n    rng = len(features) - window_size\r\n    for i in range(rng):\r\n        last_ix = i + window_size\r\n        # gather input and output parts of the pattern\r\n        seq_x, seq_y = features[i:last_ix], labels[last_ix]\r\n        X.append(seq_x)\r\n        y.append(seq_y)\r\n    return np.asarray(X), np.asarray(y)\r\n```\r\n```\r\nimport tensorflow as tf\r\nimport keras\r\n\r\nwindow_size = 1024\r\nn_steps = len(data[0]) - window_size\r\ninputs_n = 7\r\noutputs_n = 4\r\nneurons = 128\r\nlearning_rate = 0.00001\r\nactivation = 'softmax'\r\n\r\nfrom keras.layers import Dense, Activation, Dropout, LSTM, Dropout\r\nfrom tensorflow.keras import layers\r\n\r\ntf.keras.backend.clear_session()\r\ntf.keras.backend.get_session().run(tf.global_variables_initializer())\r\n\r\nmodel = tf.keras.Sequential()\r\nmodel.add(layers.LSTM(neurons, input_shape=(window_size, inputs_n), return_sequences=True)) \r\nmodel.add(layers.LSTM(neurons))\r\nmodel.add(layers.Dense(neurons, activation='relu'))\r\nmodel.add(layers.Dropout(0.2))\r\nmodel.add(layers.Dense(outputs_n, activation=activation))\r\n\r\nopt = tf.train.RMSPropOptimizer(learning_rate)\r\n\r\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\r\n\r\nprint(model.summary())\r\n```\r\n```\r\nimport time\r\n# loop by day - separate features and labels\r\n\r\nbuy_sell = 'sell'\r\nprint('Training.. LABEL:', buy_sell, 'neurons:', neurons, 'learning rate:', learning_rate, 'activation:', activation)\r\n\r\nepochs_n = 1\r\nepochs =  range(epochs_n)\r\n\r\nhistories = list()\r\nday_count = 0\r\n\r\nstart_time = time.time()\r\n\r\n# d is tuple from groupby - d[0] = date, d[1] = values\r\nfor epoch in epochs:\r\n    for d in data : \r\n        # get arrays for the day\r\n        features = np.asarray(d)[:,2:9].astype(dtype = 'float32')\r\n        labels = np.asarray(d)[:, 9:13].astype(dtype = 'int8')\r\n        \r\n        X,y = split_sequence(features, labels, window_size)\r\n\r\n        try:\r\n            H = model.fit(X,y, batch_size = window_size)\r\n            histories.append(H.history)\r\n        except Exception as e:\r\n            print('** train exception :', e)\r\n            continue\r\n        \r\n    #for days\r\n#for epoch\r\n\r\nprint('DONE..')\r\n```\r\nData split by days. All days of the same shape 11000, 11 - 7 features and 4 one-hot labels\r\n\r\nData sample:\r\n```\r\n0.322791712104689,0.323336968375136,0.00109051254089421,6.4610961249576E-05,0.746954076850984,0.7572633552015,0.746954076850984,0,1,0,0\r\n0.323882224645583,0.323882224645583,0,6.4610961249576E-05,0.751640112464855,0.801312089971884,0.751640112464855,0,0,0,1\r\n0.323882224645583,0.324427480916031,0.00109051254089421,0.00928782567962655,0.792877225866917,0.817244611059044,0.792877225866917,0,0,1,0\r\n0.323882224645583,0.324427480916031,0,0.00568576458996269,0.837863167760075,0.837863167760075,0.808809746954077,0,0,1,0\r\n0.322791712104689,0.323336968375136,0.00109051254089421,0.000516887689996608,0.820056232427366,0.820056232427366,0.799437675726336,0,0,0,0\r\n0.323882224645583,0.323882224645583,0,3.2305480624788E-05,0.799437675726336,0.817244611059044,0.792877225866917,0,0,0,1\r\n\r\n```", "> code is no secret but data I don't want to put on the open. hope you understand.\r\n\r\nThanks for sharing the code. ", "this has everything you need to reproduce:\r\nhttps://gist.github.com/maxima120/08241b5e00088901f5068ba1d444e82e", "The re-compilation is expected.\r\n\r\nKeras `model.fit` passes over the entire dataset once for each epoch. If the number of examples in the dataset is not divisible by the batch size, the last batch will be of a different size than the other training batches. (This works similarly on a GPU, you just don't see this message).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29825\">No</a>\n", "Ok please clarify further. If say a regular batch was compiled for 128, then the last was recompiled for 110 for first epoch.. **Can I be sure that all the following epoch will use 128** for all the batched but last?", "Yes, this should be the standard Keras behavior: it uses the requested batch size until the last batch of each epoch.\r\n\r\nAs compilation is cached for each new batch size, you shouldn't see a recompile after the first epoch."]}, {"number": 29824, "title": "tf.reduce_max returns wrong value", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **1.12.2**\r\n- Python version: **3.6.7**\r\n- Bazel version (if compiling from source): **0.17.2**\r\n- GCC/Compiler version (if compiling from source): **gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04)**\r\n- CUDA/cuDNN version: **no gpu**\r\n- GPU model and memory: **no gpu**\r\n\r\n**The current behavior**\r\nI have implemented a computation graph, consisting of a custom keras layer _GaussianSimilaritiesLayer_ and a _tf.reduce_max_ function. You can find the code below.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass GaussianSimilaritiesLayer(tf.keras.layers.Layer):\r\n    def __init__(self, reference_values, covariance_matrix):\r\n        super(GaussianSimilaritiesLayer, self).__init__()\r\n        self._reference_values = tf.convert_to_tensor(np.vstack(reference_values).astype(np.float32))\r\n        self._cov_inv = tf.convert_to_tensor(covariance_matrix.astype(np.float32))\r\n\r\n    def call(self, inputs):\r\n        diffs = self._reference_values - inputs\r\n        A = tf.matmul(diffs, self._cov_inv)\r\n        B = tf.multiply(A, diffs)\r\n        dist = tf.reduce_sum(B, axis=1)\r\n        exp_arg = -0.5 * dist\r\n        # return 1 * tf.math.exp(exp_arg)  # call() returns desired value\r\n        return tf.math.exp(exp_arg)  # call() returns wrong value\r\n\r\n\r\nclass Potential:\r\n    def __init__(self, session, demonstrations, covariance_matrix):\r\n        self._in = tf.keras.layers.Input(shape=(3,))\r\n        similarities = GaussianSimilaritiesLayer(demonstrations,\r\n                                                 covariance_matrix)(self._in)\r\n        max_similarity = tf.keras.layers.Lambda(tf.reduce_max)(similarities)\r\n\r\n        self._model = tf.keras.Model(inputs=[self._in],\r\n                                     outputs=[max_similarity])\r\n        self._session = session\r\n\r\n    def __call__(self, s):\r\n        return self._model.output.eval(session=self._session, feed_dict={\r\n            self._in: s\r\n        })\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    with tf.Session() as sess:\r\n        sa_demonstrations = [np.array([1, 2, 3], dtype=np.float32),\r\n                             np.array([4, 5, 6], dtype=np.float32)]\r\n        covariance_matrix = np.array([[1, 0, 0],\r\n                                    [0, 2, 0],\r\n                                    [0, 0, 3]], dtype=np.float32)\r\n        phi = Potential(sess, sa_demonstrations, covariance_matrix)\r\n        sample_s = np.array([1, 2, 2.7], dtype=np.float32)\r\n        print(phi([sample_s]))\r\n```\r\nWhen `GaussianSimilaritiesLayer.call` return statement looks like below\r\n\r\n```        return tf.math.exp(exp_arg) ```\r\n\r\nthe script outputs:\r\n\r\n> -0.13499996\r\n\r\nThis is the value of `exp_arg` from `GaussianSimilaritiesLayer.call`. The function should return e^-0.13499996.\r\n\r\n**The expected behavior**\r\nWhen `GaussianSimilaritiesLayer.call` return statement looks like below\r\n\r\n```        return 1 * tf.math.exp(exp_arg) ```\r\n\r\nthe script outputs:\r\n\r\n> 0.87371594\r\n\r\nwhich is the desired value.", "comments": ["Seems to be a bug in tf.reduce_max(); if i replace tf.reduce_max() with for example tf.nn.relu() output is correct. I will look into the tf.reduce_max() implementation.", "The behavior is really weird; I guess that multiplying by 1 changes the dtype of the tensor somehow. I get the desired result also when returning\r\n```\r\nreturn tf.cast(tf.cast(tf.exp(exp_arg), tf.float64), tf.float32)\r\n```\r\n\r\n", "I could reproduce the issue with Tf 1.12.2 on Google Colab.Thanks!", "This works as expected in 1.14", "@pwasows Can you upgrade to TF 1.14 as it is working as expected? Thanks!", "> @pwasows Can you upgrade to TF 1.14 as it is working as expected? Thanks!\r\n\r\nI cannot upgrade to 1.14, but can live with the `1 * tf.math.exp(exp_arg)` workaround ;)", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29824\">No</a>\n"]}, {"number": 29823, "title": "Using a `tf.Tensor` as a Python `bool` is not allowed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): official site\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI built a custom activation function in Keras' and call tf functions. When I call the custom function, I encountered error:\r\n\r\n  TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.\r\n\r\nThe line of code where the error happened is this one\r\n`  inputs = tf.where(orig > 0 and orig <= 0.25, 0.25 / (1+K.exp(-self.sharp*((inputs-0.125)/0.5))), inputs)`\r\n**Describe the expected behavior**\r\nI tested all variables, all of them are tf.Tensor\r\n**Code to reproduce the issue**\r\n`import tensorflow as tf\r\n\r\nclass QPWCX(Layer):\r\n\r\n\r\n    def __init__(self, sharp=100, **kwargs):\r\n        super(QPWCX, self).__init__(**kwargs)\r\n        self.supports_masking = True\r\n        self.sharp = K.cast_to_floatx(sharp)\r\n\r\n    def call(self, inputs):\r\n        orig = inputs\r\n        inputs = tf.where(orig <= 0, tf.zeros_like(inputs), inputs)\r\n        inputs = tf.where(orig > 0 and orig <= 0.25, 0.25 / (1+K.exp(-self.sharp*((inputs-0.125)/0.5))), inputs)\r\n        inputs = tf.where(orig > 0.25 and orig <= 0.5, 0.25 / (1+K.exp(-self.sharp*((inputs-0.5)/0.5))) + 0.25, inputs)\r\n        inputs = tf.where(orig > 0.5 and orig <= 0.75, 0.25 / (1+K.exp(-self.sharp*((inputs-0.75)/0.5))) + 0.5, inputs)\r\n        return K.where(orig > 0.75, 1, inputs)\r\n\r\n\r\n    def get_config(self):\r\n        config = {'sharp': float(self.sharp)}\r\n        base_config = super(QPWCX, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n`\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a full code snippet to reproduce the issue reported here. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "hi,\r\n\r\nI have a problem in the last step and i dont know how to fix the bug. below is the error and the code. Could anybody help me?\r\n\r\nimport os\r\nimg_dir = '/tmp/nst'\r\nif not os.path.exists(img_dir):\r\n    os.makedirs(img_dir)\r\n\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\nmpl.rcParams['figure.figsize'] = (10,10)\r\nmpl.rcParams['axes.grid'] = False\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport time\r\nimport functools\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\nfrom tensorflow.python.keras.preprocessing import image as kp_image\r\nfrom tensorflow.python.keras import models \r\nfrom tensorflow.python.keras import losses\r\nfrom tensorflow.python.keras import layers\r\nfrom tensorflow.python.keras import backend as K\r\n\r\n#subimoos las imagenes\r\ncontent_path = (r'C:\\Users\\Casa\\Desktop\\Puerto_Madero.jpg')\r\nstyle_path = (r'C:\\Users\\Casa\\Desktop\\Van_Gogh.jpg')\r\n\r\n#visualize input\r\ndef load_img(path_to_img):\r\n  max_dim = 512\r\n  img = Image.open(path_to_img)\r\n  long = max(img.size)\r\n  scale = max_dim/long\r\n  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\r\n  \r\n  img = kp_image.img_to_array(img)\r\n  \r\n  # We need to broadcast the image array such that it has a batch dimension \r\n  img = np.expand_dims(img, axis=0)\r\n  return img\r\n\r\n\r\ndef imshow(img, title=None):\r\n  # Remove the batch dimension\r\n  out = np.squeeze(img, axis=0)\r\n  # Normalize for display \r\n  out = out.astype('uint8')\r\n  plt.imshow(out)\r\n  if title is not None:\r\n    plt.title(title)\r\n  plt.imshow(out)\r\n  \r\nplt.figure(figsize=(10,10))\r\n\r\ncontent = load_img(content_path).astype('uint8')\r\nstyle = load_img(style_path).astype('uint8')\r\n\r\nplt.subplot(1, 2, 1)\r\nimshow(content, 'Content Image')\r\n\r\nplt.subplot(1, 2, 2)\r\nimshow(style, 'Style Image')\r\nplt.show()\r\n\r\n\r\n#prepare the data\r\ndef load_and_process_img(path_to_img):\r\n  img = load_img(path_to_img)\r\n  img = tf.keras.applications.vgg19.preprocess_input(img)\r\n  return img\r\n\r\ndef deprocess_img(processed_img):\r\n  x = processed_img.copy()\r\n  if len(x.shape) == 4:\r\n    x = np.squeeze(x, 0)\r\n  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\r\n                             \"dimension [1, height, width, channel] or [height, width, channel]\")\r\n  if len(x.shape) != 3:\r\n    raise ValueError(\"Invalid input to deprocessing image\")\r\n  \r\n  # perform the inverse of the preprocessiing step\r\n  x[:, :, 0] += 103.939\r\n  x[:, :, 1] += 116.779\r\n  x[:, :, 2] += 123.68\r\n  x = x[:, :, ::-1]\r\n\r\n  x = np.clip(x, 0, 255).astype('uint8')\r\n  return x\r\n\r\ncontent_layers = ['block5_conv2'] \r\n\r\n# Style layer we are interested in\r\nstyle_layers = ['block1_conv1',\r\n                'block2_conv1',\r\n                'block3_conv1', \r\n                'block4_conv1', \r\n                'block5_conv1'\r\n               ]\r\n\r\nnum_content_layers = len(content_layers)\r\nnum_style_layers = len(style_layers)\r\n\r\ndef get_model():\r\n  \"\"\" Creates our model with access to intermediate layers. \r\n  \r\n  This function will load the VGG19 model and access the intermediate layers. \r\n  These layers will then be used to create a new model that will take input image\r\n  and return the outputs from these intermediate layers from the VGG model. \r\n  \r\n  Returns:\r\n    returns a keras model that takes image inputs and outputs the style and \r\n      content intermediate layers. \r\n  \"\"\"\r\n  # Load our model. We load pretrained VGG, trained on imagenet data\r\n  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\r\n  vgg.trainable = False\r\n  # Get output layers corresponding to style and content layers \r\n  style_outputs = [vgg.get_layer(name).output for name in style_layers]\r\n  content_outputs = [vgg.get_layer(name).output for name in content_layers]\r\n  model_outputs = style_outputs + content_outputs\r\n  # Build model \r\n  return models.Model(vgg.input, model_outputs)\r\n\r\ndef get_content_loss(base_content, target):\r\n  return tf.reduce_mean(tf.square(base_content - target))\r\n\r\ndef gram_matrix(input_tensor):\r\n  # We make the image channels first \r\n  channels = int(input_tensor.shape[-1])\r\n  a = tf.reshape(input_tensor, [-1, channels])\r\n  n = tf.shape(a)[0]\r\n  gram = tf.matmul(a, a, transpose_a=True)\r\n  return gram / tf.cast(n, tf.float32)\r\n\r\ndef get_style_loss(base_style, gram_target):\r\n  \"\"\"Expects two images of dimension h, w, c\"\"\"\r\n  # height, width, num filters of each layer\r\n  # We scale the loss at a given layer by the size of the feature map and the number of filters\r\n  height, width, channels = base_style.get_shape().as_list()\r\n  gram_style = gram_matrix(base_style)\r\n  \r\n  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)\r\n\r\n\r\n\r\ndef get_feature_representations(model, content_path, style_path):\r\n  \"\"\"Helper function to compute our content and style feature representations.\r\n\r\n  This function will simply load and preprocess both the content and style \r\n  images from their path. Then it will feed them through the network to obtain\r\n  the outputs of the intermediate layers. \r\n  \r\n  Arguments:\r\n    model: The model that we are using.\r\n    content_path: The path to the content image.\r\n    style_path: The path to the style image\r\n    \r\n  Returns:\r\n    returns the style features and the content features. \r\n  \"\"\"\r\n  # Load our images in \r\n  content_image = load_and_process_img(content_path)\r\n  style_image = load_and_process_img(style_path)\r\n  \r\n  # batch compute content and style features\r\n  style_outputs = model(style_image)\r\n  content_outputs = model(content_image)\r\n  \r\n  \r\n  # Get the style and content feature representations from our model  \r\n  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\r\n  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\r\n  return style_features, content_features\r\n\r\ndef compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\r\n  \"\"\"This function will compute the loss total loss.\r\n  \r\n  Arguments:\r\n    model: The model that will give us access to the intermediate layers\r\n    loss_weights: The weights of each contribution of each loss function. \r\n      (style weight, content weight, and total variation weight)\r\n    init_image: Our initial base image. This image is what we are updating with \r\n      our optimization process. We apply the gradients wrt the loss we are \r\n      calculating to this image.\r\n    gram_style_features: Precomputed gram matrices corresponding to the \r\n      defined style layers of interest.\r\n    content_features: Precomputed outputs from defined content layers of \r\n      interest.\r\n      \r\n  Returns:\r\n    returns the total loss, style loss, content loss, and total variational loss\r\n  \"\"\"\r\n  style_weight, content_weight = loss_weights\r\n  \r\n  # Feed our init image through our model. This will give us the content and \r\n  # style representations at our desired layers. Since we're using eager\r\n  # our model is callable just like any other function!\r\n  model_outputs = model(init_image)\r\n  \r\n  style_output_features = model_outputs[:num_style_layers]\r\n  content_output_features = model_outputs[num_style_layers:]\r\n  \r\n  style_score = 0\r\n  content_score = 0\r\n\r\n  # Accumulate style losses from all layers\r\n  # Here, we equally weight each contribution of each loss layer\r\n  weight_per_style_layer = 1.0 / float(num_style_layers)\r\n  for target_style, comb_style in zip(gram_style_features, style_output_features):\r\n    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\r\n    \r\n  # Accumulate content losses from all layers \r\n  weight_per_content_layer = 1.0 / float(num_content_layers)\r\n  for target_content, comb_content in zip(content_features, content_output_features):\r\n    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\r\n  \r\n  style_score *= style_weight\r\n  content_score *= content_weight\r\n\r\n  # Get total loss\r\n  loss = style_score + content_score \r\n  return loss, style_score, content_score\r\n\r\n\r\ndef compute_grads(cfg):\r\n  with tf.GradientTape() as tape: \r\n    all_loss = compute_loss(**cfg)\r\n  # Compute gradients wrt input image\r\n  total_loss = all_loss[0]\r\n  return tape.gradient(total_loss, cfg['init_image']), all_loss\r\n\r\nimport IPython.display\r\n\r\ndef run_style_transfer(content_path, \r\n                       style_path,\r\n                       num_iterations=1000,\r\n                       content_weight=1e3, \r\n                       style_weight=1e-2): \r\n  # We don't need to (or want to) train any layers of our model, so we set their\r\n  # trainable to false. \r\n  model = get_model() \r\n  for layer in model.layers:\r\n    layer.trainable = False\r\n  \r\n  # Get the style and content feature representations (from our specified intermediate layers) \r\n  style_features, content_features = get_feature_representations(model, content_path, style_path)\r\n  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\r\n  \r\n  # Set initial image\r\n  init_image = load_and_process_img(content_path)\r\n  init_image = tfe.Variable(init_image, dtype=tf.float32)\r\n  # Create our optimizer\r\n  opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\r\n\r\n  # For displaying intermediate images \r\n  iter_count = 1\r\n  \r\n  # Store our best result\r\n  best_loss, best_img = float('inf'), None\r\n  \r\n  # Create a nice config \r\n  loss_weights = (style_weight, content_weight)\r\n  cfg = {\r\n      'model': model,\r\n      'loss_weights': loss_weights,\r\n      'init_image': init_image,\r\n      'gram_style_features': gram_style_features,\r\n      'content_features': content_features\r\n  }\r\n    \r\n  # For displaying\r\n  num_rows = 2\r\n  num_cols = 5\r\n  display_interval = num_iterations/(num_rows*num_cols)\r\n  start_time = time.time()\r\n  global_start = time.time()\r\n  \r\n  norm_means = np.array([103.939, 116.779, 123.68])\r\n  min_vals = -norm_means\r\n  max_vals = 255 - norm_means   \r\n  \r\n  imgs = []\r\n  for i in range(num_iterations):\r\n    grads, all_loss = compute_grads(cfg)\r\n    loss, style_score, content_score = all_loss\r\n    opt.apply_gradients([(grads, init_image)])\r\n    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\r\n    init_image.assign(clipped)\r\n    end_time = time.time() \r\n    \r\n    if loss < best_loss:\r\n      # Update best loss and best image from total loss. \r\n      best_loss = loss\r\n      best_img = deprocess_img(init_image.numpy())\r\n\r\n    if i % display_interval== 0:\r\n      start_time = time.time()\r\n      \r\n      # Use the .numpy() method to get the concrete numpy array\r\n      plot_img = init_image.numpy()\r\n      plot_img = deprocess_img(plot_img)\r\n      imgs.append(plot_img)\r\n      IPython.display.clear_output(wait=True)\r\n      IPython.display.display_png(Image.fromarray(plot_img))\r\n      print('Iteration: {}'.format(i))        \r\n      print('Total loss: {:.4e}, ' \r\n            'style loss: {:.4e}, '\r\n            'content loss: {:.4e}, '\r\n            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\r\n  print('Total time: {:.4f}s'.format(time.time() - global_start))\r\n  IPython.display.clear_output(wait=True)\r\n  plt.figure(figsize=(14,4))\r\n  for i,img in enumerate(imgs):\r\n      plt.subplot(num_rows,num_cols,i+1)\r\n      plt.imshow(img)\r\n      plt.xticks([])\r\n      plt.yticks([])\r\n      \r\n  return best_img, best_loss\r\n\r\n\r\nbest, best_loss = run_style_transfer(content_path, \r\n                                     style_path, num_iterations=1000)\r\n\r\nTypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor."]}, {"number": 29822, "title": "[TF 2.0 API Docs] tf.estimator.SessionRunHook", "body": "URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/SessionRunHook\r\nDescription of issue (what needs changing):\r\nClear description\r\n\r\nYes clear description\r\nCorrect links\r\n\r\nThe link to the source code correct.\r\nParameters defined\r\n\r\nAll parameters defined and formatted correctly.\r\nReturns defined\r\n\r\nReturn values are defined.\r\nRaises listed and defined\r\n\r\nNo raises listed and defined.\r\nUsage example\r\n\r\nNo usage examples provided\r\nRequest visuals, if applicable\r\n\r\nNo visuals included", "comments": ["I think this was resolved with the update docs which list couple of guides and tutorials to migrate `SessionRunHook` to recent and more advanced methods.\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/SessionRunHook\r\n\r\n> This guide demonstrates how to migrate from SessionRunHook to TensorFlow 2's custom callbacks with the tf.keras.callbacks.Callback API\r\n\r\nI am closing this issue as this was resolved. Thanks!"]}, {"number": 29821, "title": "[TF 2.0 API Docs] tf.estimator.StepCounterHook", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/SessionRunHook\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nYes clear description\r\n\r\n### Correct links\r\n\r\nThe link to the source code correct.\r\n\r\n### Parameters defined\r\n\r\nAll parameters defined and formatted correctly.\r\n\r\n### Returns defined\r\nReturn values are defined.\r\n\r\n### Raises listed and defined\r\nNo raises listed and defined.\r\n\r\n### Usage example\r\n\r\nNo usage examples provided\r\n\r\n### Request visuals, if applicable\r\n\r\nNo visuals included\r\n\r\n\r\n", "comments": ["duplicate #29822 "]}, {"number": 29820, "title": "framework api int64 compatible problem", "body": "It seems tensorflow have compatible problem for Shard and int64\r\n```\r\nauto job = [&](int64 start, int64 limit)\r\n{\r\n            for (int64 i = start; i<limit; i++)\r\n            {\r\n                std::cout << start << std::endl; // Segmentation fault (core dumped)\r\n            }\r\n};\r\n\r\nauto job = [&](int64 start, int64 limit)\r\n{\r\n            for (int i=0; i<4; i++)\r\n            {\r\n                std::cout << start << std::endl; // ok\r\n            }\r\n};\r\n\r\nconst DeviceBase::CpuWorkerThreads& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\r\nconst int64_t shard_cost = 10000;\r\nShard(worker_threads.num_threads, worker_threads.workers,\r\n              (int64)4, shard_cost, job);\r\n\r\n```\r\nsystem info:\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux ubuntu 18.04\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.13 v1.13.1-0-g6612da8951\r\n- Python version:3.5\r\n- CUDA/cuDNN version:10/7.5\r\n- GPU model and memory:7.5/24gb\r\n\r\n", "comments": ["Will it be possible to provide a full code snippet that we can use to reproduce the issue. This will help us to proceed faster. Thanks!", "sure\r\ntestbug.cc:\r\n```\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/framework/tensor_shape.h\"\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/util/work_sharder.h\"\r\n\r\n#include <iostream>\r\n#include <cmath>\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"TestBug\")\r\n    .Output(\"dummy: float\");\r\n\r\nclass TestBugOp : public OpKernel\r\n{\r\npublic:\r\nexplicit TestBugOp(OpKernelConstruction* context)\r\n        : OpKernel(context)\r\n{\r\n\r\n}\r\n\r\nvoid Compute(OpKernelContext* context) override\r\n{\r\n    Tensor* dummy = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, {1},\r\n                                                     &dummy));\r\n    auto job = [&](int64 start, int64 limit)\r\n    {\r\n        for (int64 i = start; i<limit; i++)\r\n        {\r\n            std::cout << start << std::endl; // Segmentation fault (core dumped)\r\n        }\r\n    };\r\n    \r\n    /*\r\n    auto job = [&](int64 start, int64 limit)\r\n    {\r\n        for (int i = 0; i<4; i++)\r\n        {\r\n            std::cout << i << std::endl; // ok\r\n        }\r\n    };\r\n    */\r\n\r\n    const DeviceBase::CpuWorkerThreads& worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\r\n    const int64_t shard_cost = 10000;\r\n    Shard(worker_threads.num_threads, worker_threads.workers,\r\n              (int64)100, shard_cost, job);\r\n\r\n}\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(\r\n  Name(\"TestBug\").Device(DEVICE_CPU),\r\n  TestBugOp\r\n);\r\n    \r\n```\r\ntestbug.py:\r\n```\r\nimport tensorflow as tf\r\nextr_module = tf.load_op_library('./build/libextr_module.so')\r\n\r\ndummy = extr_module.test_bug()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run([dummy])\r\n```\r\nCMakeLists.txt:\r\n```\r\nCMAKE_MINIMUM_REQUIRED(VERSION 2.8)\r\nPROJECT(extr_module)\r\n\r\n\r\n# compiler flags\r\nSET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -O2 ${OpenMP_CXX_FLAGS} -Wall -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 -DGOOGLE_CUDA=1\")\r\n\r\n# TensorFlow dependencies\r\nEXECUTE_PROCESS(COMMAND python3.5 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_include(), end='', flush=True)\"  OUTPUT_VARIABLE TF_INC)\r\n\r\nEXECUTE_PROCESS(COMMAND python3.5 -c \"import os; os.environ['TF_CPP_MIN_LOG_LEVEL']='3'; import tensorflow as tf; print(tf.sysconfig.get_lib(), end='', flush=True)\"  OUTPUT_VARIABLE TF_LIB)\r\n\r\n\r\nMESSAGE(STATUS \"Found TF_INC: \" ${TF_INC})\r\n#MESSAGE(STATUS \"Found TF_INC_EXTERNAL: \" ${TF_INC}/external/nsync/public)\r\nMESSAGE(STATUS \"Found TF_LIB: \" ${TF_LIB})\r\n\r\n\r\nINCLUDE_DIRECTORIES(${TF_INC})\r\n#INCLUDE_DIRECTORIES(${TF_INC}/external/nsync/public)\r\nLINK_DIRECTORIES(${TF_LIB})\r\n\r\n\r\nADD_LIBRARY(extr_module SHARED\r\n  testbug.cc\r\n)\r\n\r\nTARGET_LINK_LIBRARIES(extr_module tensorflow_framework)\r\n```\r\ntest:\r\n```\r\nmkdir build\r\ncd build\r\ncmake ..\r\nmake\r\ncd ..\r\npython3 testbug.py\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "problem solved, I changed g++ to 4.8 version, and all bug solved. solution is referenced to https://github.com/tensorflow/tensorflow/issues/13308", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29820\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29820\">No</a>\n"]}, {"number": 29819, "title": "Internal Compiler Error cross-compiling v2.0.0-beta0 for Raspberry Pi", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\nBuild container: Ubuntu 14.04\r\nDocker version: 18.09.2 (Docker engine) / 2.0.0.3 (31259) (Docker Desktop for Mac)\r\nHost platform: macOS 10.14.4 (Mojave)\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n```\r\nBuild target: Raspberry Pi B, ARMv7l\r\n```\r\n- TensorFlow installed from (source or binary): Source (instructions taken from https://www.tensorflow.org/install/source_rpi)\r\n- TensorFlow version: `v2.0.0-beta0`\r\n- Python version: Python 3.4 \r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\n(via `build_raspberry_pi.sh`)\r\nhttps://github.com/raspberrypi/tools/blob/master/arm-bcm2708/arm-rpi-4.9.3-linux-gnueabihf/bin/arm-linux-gnueabihf-gcc\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.0.0-beta0/tensorflow/tools/ci_build/pi/build_raspberry_pi.sh#L60\r\n```\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nI'm seeing an internal compiler error when compiling `tensorflow/core/kernels/data/experimental/stats_dataset_ops.cc`. Full log is attached as `v2.0.0-beta0-build.log`\r\n\r\nI also noticed this build process is using the `gnu11` dialect, where the preferred dialect in `tensorflow/core/kernels` and other modules is c++11. \r\n\r\n```bash\r\nINFO: From Compiling tensorflow/core/kernels/data/experimental/stats_dataset_ops.cc:\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\nERROR: /workspace/tensorflow/core/kernels/BUILD:3301:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 4): arm-linux-gnueabihf-gcc failed: error executing command\r\n  (cd /Users/ljohnson/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_ljohnson/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  /Users/ljohnson/repos/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_ljohnson/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python3.4 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_square_root_op/matrix_square_root_op.pic.d '-frandom-seed=bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_square_root_op/matrix_square_root_op.pic.o' -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -iquote . -iquote bazel-out/armeabi-opt/genfiles -iquote bazel-out/armeabi-opt/bin -iquote external/com_google_absl -iquote bazel-out/armeabi-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/armeabi-opt/genfiles/external/eigen_archive -iquote bazel-out/armeabi-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/armeabi-opt/genfiles/external/local_config_sycl -iquote bazel-out/armeabi-opt/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/armeabi-opt/genfiles/external/nsync -iquote bazel-out/armeabi-opt/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/armeabi-opt/genfiles/external/gif_archive -iquote bazel-out/armeabi-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/armeabi-opt/genfiles/external/jpeg -iquote bazel-out/armeabi-opt/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/armeabi-opt/genfiles/external/protobuf_archive -iquote bazel-out/armeabi-opt/bin/external/protobuf_archive -iquote external/zlib_archive -iquote bazel-out/armeabi-opt/genfiles/external/zlib_archive -iquote bazel-out/armeabi-opt/bin/external/zlib_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/armeabi-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/armeabi-opt/genfiles/external/farmhash_archive -iquote bazel-out/armeabi-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/armeabi-opt/genfiles/external/fft2d -iquote bazel-out/armeabi-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote bazel-out/armeabi-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/armeabi-opt/genfiles/external/double_conversion -iquote bazel-out/armeabi-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/armeabi-opt/genfiles/external/snappy -iquote bazel-out/armeabi-opt/bin/external/snappy -iquote external/hwloc -iquote bazel-out/armeabi-opt/genfiles/external/hwloc -iquote bazel-out/armeabi-opt/bin/external/hwloc -isystem external/eigen_archive -isystem bazel-out/armeabi-opt/genfiles/external/eigen_archive -isystem bazel-out/armeabi-opt/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/armeabi-opt/genfiles/external/nsync/public -isystem bazel-out/armeabi-opt/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/armeabi-opt/genfiles/external/gif_archive/lib -isystem bazel-out/armeabi-opt/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/armeabi-opt/bin/external/protobuf_archive/src -isystem external/zlib_archive -isystem bazel-out/armeabi-opt/genfiles/external/zlib_archive -isystem bazel-out/armeabi-opt/bin/external/zlib_archive -isystem external/farmhash_archive/src -isystem bazel-out/armeabi-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/armeabi-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/armeabi-opt/genfiles/external/double_conversion -isystem bazel-out/armeabi-opt/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/hwloc -isystem bazel-out/armeabi-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/armeabi-opt/genfiles/external/hwloc/include -isystem bazel-out/armeabi-opt/bin/external/hwloc/include '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -fno-tree-pre -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -DTENSORFLOW_MONOLITHIC_BUILD -pthread -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/matrix_square_root_op.cc -o bazel-out/armeabi-opt/bin/tensorflow/core/kernels/_objs/matrix_square_root_op/matrix_square_root_op.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ncc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++\r\narm-linux-gnueabihf-gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://gcc.gnu.org/bugs.html> for instructions.\r\nINFO: Elapsed time: 43029.508s, Critical Path: 986.76s\r\nINFO: 3265 processes: 3265 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\ngit checkout v2.0.0-beta0 \\\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" \\\r\n    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh > v2.0.0-beta0-build.log\r\n```\r\n\r\n[v2.0.0-beta0-build001.log](https://github.com/tensorflow/tensorflow/files/3292954/v2.0.0-beta0-build001.log)", "comments": ["@petewarden was kind enough to share a patch with me, and I was able to build a wheel successfully after cherry-picking this commit into a branch based from v2.0.0-beta0. https://github.com/tensorflow/tensorflow/commit/c6a9456b5d766adb058d81b52f832acd7397c735\r\n\r\nFor anyone who finds this, here are the steps I took:\r\n\r\n```bash\r\ngit checkout v2.0.0-beta0 && git cherry-pick c6a9456b5d766adb058d81b52f832acd7397c735\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.5\" \\\r\n    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \\\r\n    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh > v2.0.0-beta0-build002.log\r\n```\r\n\r\nNote: Pete's patch updates the build container's platform from Ubuntu 14.04 to 16.04, so I updated `CROSSTOOL_PYTHON_INCLUDE_PATH` to python3.5 includes, as python 3.5 is the default version of Python that ships with Ubuntu 16.04. \r\n\r\nZip and copy artifacts from the build machine to Raspberry Pi (hostname `theia.local` in the example below)\r\n\r\n```bash\r\nscp output-artifacts.zip pi@theia.local:/home/pi/tensorflow-v2.0.0-beta0.zip\r\n...\r\npi@theia:~ $ unzip tensorflow-v2.0.0-beta0.zip\r\npi@theia:~ $ virtualenv -p python3 tmp/venv\r\npi@theia:~ $ source tmp/venv/bin/activate\r\n(venv) pi@theia:~ $ python3 --version\r\nPython 3.5.3\r\npip install /home/pi/output-artifacts/tensorflow-2.0.0b0-cp35-none-linux_armv7l.whl\r\n(venv) pi@theia:~ $ pip install /home/pi/output-artifacts/tensorflow-2.0.0b0-cp35-none-linux_armv7l.whl\r\n(venv) pi@theia:~ $ python\r\nPython 3.5.3 (default, Sep 27 2018, 17:25:39)\r\n[GCC 6.3.0 20170516] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n```\r\n\r\n\ud83d\udc83 \ud83c\udf89 ", "Great to see you made progress on this, thanks for trying out the patch! I wonder if the original internal compiler error had anything to do with this other issue I hacked around separately?\r\nhttps://github.com/tensorflow/tensorflow/commit/f8b2d0b39a843de470e80f1ddaf9d769a3dbebcc\r\n\r\nIt's probably not the same, since the 2.0 branch includes the above fix I think, but if the compiler crash returns I might have to do some more binary searching to figure out the offending options.", "Closing the issue because it is resolved.Thanks"]}, {"number": 29818, "title": "Having problem to use the Reshape in Keras", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-beta0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 750 Ti\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using Reshape layer of Keras and -1 is used to infer the shape, the output shape is incorrect.\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**Code to reproduce the issue**\r\n`from tensorflow.python.keras.layers import Input, Lambda, Conv2D, Reshape, TimeDistributed`\r\n`from tensorflow.python.keras.models import *`\r\n`import tensorflow.keras as keras`\r\n\r\n`a = Input(shape=(12,), dtype = 'int32')`\r\n`# a.shape == (None, 12)`\r\n`d = Reshape((-1, 2, 2))(a)`\r\n`# actual result\uff1a d.shape == (None, None, 2, 2)`\r\n`# expected result\uff1a d.shape == (None, 3, 2, 2)`\r\n\r\n", "comments": ["I am able to reproduce the reported issue on Colab with Tensorflow 2.0.0.beta0. Thanks!", "@Ray322417 I am closing this as this is a duplicate of this issue https://github.com/keras-team/keras/issues/12964. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29818\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29818\">No</a>\n"]}, {"number": 29817, "title": "[INTEL MKL]  1.14 cherrypick request. Moving compat date for fused_bacth_norm_v3.", "body": "Moving compatibility date of fused_batch_norm_v3 to 2019/11/10 so it won't trigger in v1.14. ", "comments": ["@penpornk Thanks. I will look at the tests.", "@penpornk  @bananabowl  The PR has been updated to reflect review comments. Thanks.", "Quick note: I'm out of office today and won't be able to review PRs again until in the afternoon. So please don't wait for me if things can be resolved before noon.", "> Please change the dates in tensorflow/contrib/quantize/python/fold_batch_norms_test.py as well. Please see the same commit I referred to in my first review.\r\n> \r\n> @bananabowl The PR you referenced sets the date to 11/1 not 11/11. Is 11/1 correct? If you plan to make it 11/11 at some point, should Ramesh change the date to 11/12 or is 11/11 okay?\r\n\r\nGood catch! It is Nov 1: https://github.com/tensorflow/tensorflow/commit/7bca6c7cce6a2f7dae12f47b21138a6f83244d50#diff-43c90835f88ddfcb818c9ed9f2594b1c\r\n\r\n11/11 is fine. Thanks!", "@Penpornk tensorflow/contrib/quantize/python/fold_batch_norms_test.py did not fail in our testing so I kept same date, will change that also.  Thanks.", "@penpornk @bananabowl We have updated the compat date as suggested for the test. We had to keep the compat date as 11/10 in nn_impl.py. If we change it to 11/11, the //tensorflow/contrib/quantize:quantize_graph_test will fail. ", "> @penpornk @bananabowl We have updated the compat date as suggested for the test. We had to keep the compat date as 11/10 in nn_impl.py. If we change it to 11/11, the //tensorflow/contrib/quantize:quantize_graph_test will fail.\r\n\r\nThanks for the changes - now pending internal review.", "This will only be pulled into 1.14: https://github.com/tensorflow/tensorflow/pull/29924"]}, {"number": 29816, "title": "Sess.run() leads to unexpected memory leaky in CPU version", "body": "Hi~\r\n\r\nA simplest example is shown as below:\r\n```\r\ngraph = tf.Graph()\r\n\r\nsess = tf.InteractiveSession(graph=graph)\r\n\r\nwith graph.as_default_graph():\r\n       # Here is my code build the model\r\n       model  = Model(inputs=,outputs=)\r\n\r\n# frozen graph to avoid any other ops adding to the graph\r\ngraph.finalize()\r\n\r\n# prepare test data\r\nimg = np.ones(model.input_shape)\r\n\r\n# run session in loop\r\nwhile True:\r\n      sess.run(model.output,feed_dict={model.input:img}\r\n     # Here is my code for watching memory state:\r\n      print(psutil.Process(os.getpid()).memory_info().rss)\r\n``` \r\n\r\nAnd the result of `psutil` shows the process memory usage increases after each iteration and could not collected by gc in python. I eagerly wonder why this phenomena appears and how to solve it.\r\n\r\nThanks a lot!", "comments": ["@CloudFlyCN Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\n If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29815, "title": "Fix grad of variable size on extract_image_patches", "body": "This fixes #11651.\r\n\r\nThe unit test is taken from #17420.", "comments": ["@rthadur Is it possible this PR to be included with 1.14? ", "Sadly no, 1.14 is about to be released.\n\nOn Tue, Jun 18, 2019, 06:51 Vasileios Lioutas <notifications@github.com>\nwrote:\n\n> @rthadur <https://github.com/rthadur> Is it possible this PR to be\n> included with 1.14?\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29815?email_source=notifications&email_token=AAABHROMUBPLFHLJMYCD7PTP3DR63A5CNFSM4HYOFJOKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODX6VCXY#issuecomment-503140703>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRM7VWLBHNGQA457T73P3DR63ANCNFSM4HYOFJOA>\n> .\n>\n", "@alextp Okay thank you for letting me know.", "Yes\n\nOn Tue, Jun 18, 2019 at 1:12 PM Vasileios Lioutas <notifications@github.com>\nwrote:\n\n> *@lioutasb* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/kernel_tests/extract_image_patches_grad_test.py\n> <https://github.com/tensorflow/tensorflow/pull/29815#discussion_r295003548>\n> :\n>\n> > +        in_val = array_ops.placeholder(\n> +          shape=test_shape,\n> +          dtype=dtypes.float32)\n> +\n> +        feed_dict = {in_val: np.random.random(in_shape)}\n> +        for padding in ['VALID', 'SAME']:\n> +          out_val = array_ops.extract_image_patches(in_val, test_case['ksizes'],\n> +                                                    test_case['strides'],\n> +                                                    test_case['rates'], padding)\n> +          out_val_tmp = out_val.eval(feed_dict=feed_dict)\n> +          out_shape = out_val_tmp.shape\n> +\n> +          err = gradient_checker.compute_gradient_error(in_val, in_shape,\n> +                                                        out_val, out_shape)\n> +\n> +          print('extract_image_patches gradient err: %.4e' % err)\n>\n> I see the same print used earlier here\n> <https://github.com/tensorflow/tensorflow/blob/e67dc75850fe3a54071fdd9db6ad91906ad51797/tensorflow/python/kernel_tests/extract_image_patches_grad_test.py#L104>,\n> should I remove that as well?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/29815?email_source=notifications&email_token=AAABHRMW2JA4MT2QNLMWME3P3E6SRA5CNFSM4HYOFJOKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB35P2ZA#discussion_r295003548>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLV2XFCB5JDAHP7DSTP3E6SRANCNFSM4HYOFJOA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp I fixed the pylint issue. Now it should be ok to merge automatically once you approve it again."]}, {"number": 29814, "title": "grappler: Also recompute LeakyRelu", "body": "", "comments": []}]