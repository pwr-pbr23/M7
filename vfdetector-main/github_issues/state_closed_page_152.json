[{"number": 50281, "title": "ValueError: image not in JPEG format for Custom Object Detection using TF Lite Model Maker", "body": "## URL(s) with the issue:\r\nCodelabs Colab Notebook link for custom object detection using TF Lite Model Maker:\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=p79NHCx0xFqb\r\n\r\n## Description of issue (what needs changing):\r\nI have been following the above mentioned Codelabs for training a model for custom object detection using TF Lite Model Maker. However, when I try importing the image dataset as a CSV from local path, I face an error that says, `ValueError: image not in JPEG format`; although all the training, validation and test images are of the said format i.e. JPEG.  \r\n\r\n### Clear description\r\nBelow is the screenshot of the same.\r\n![image](https://user-images.githubusercontent.com/37960279/122074390-ce428380-ce16-11eb-915e-703b84aed02e.png)\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\nYes\r\n\r\n### Parameters defined\r\nCSV file.\r\n\r\nAre all parameters defined and formatted correctly?\r\nYes\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://github.com/sozercan/tensorflow-object-detection/blob/master/tensorflow-object-detection/generic_create_pascal_tf_record.py\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\nYes, the example is taken from the following Codelabs: https://codelabs.developers.google.com/tflite-object-detection-android#8\r\n", "comments": ["Awaiting a response on this issue.", "I see you have replaced the dataset from tutorial with your dataset. Can you confirm if all Images are in `jpeg` format. Also is it possible for you to provide gist as reference. It can help to debug quicker.", "@ymodak, it is confirmed that all the images are in `jpeg` format. Below is a gist as a reference as requested by you.\r\n\r\nhttps://gist.github.com/NSTiwari/d2ae2b8af600d3865a2fd810691ed8f2\r\n", "Waiting for an update on this.", "I've resolved the issue. I had to explicitly and programmatically convert all the images into 'RGB' format using PIL library and it worked just fine afterwards. Thanks for the support, closing this issue here."]}, {"number": 50280, "title": "Remove a deadlock in cudaMallocAsync", "body": "The call GetStats() a few lines bellow also ask for the lock. But we already own it. So it deadlock.\r\n@akuegel\r\nThis was mentioned here: https://github.com/tensorflow/tensorflow/pull/49173/#issuecomment-861011964", "comments": []}, {"number": 50279, "title": "[tf.data] graduate OptimizationOptions from experimental to tf.data", "body": "This PR graduates `tf.data.experimental.OptimizationOptions` to `tf.data.OptimizationOptions`.\r\n\r\n- [x] deprecate the old endpoint\r\n- [x] update docstring of the class\r\n- [x] Modify the `__getattr__` and `__setattr__` implementation of `dataset_ops.Options` for backward-compatibility\r\n- [x] regenerate golden APIs.\r\n- [x] add/modify relevant test cases for the new endpoint.\r\n- [x] update RELEASE.md\r\n\r\nTEST LOG\r\n```\r\nINFO: Elapsed time: 32.496s, Critical Path: 28.35s\r\nINFO: 354 processes: 133 internal, 221 local.\r\nINFO: Build completed successfully, 354 total actions\r\n//tensorflow/python/data/kernel_tests:options_test                       PASSED in 4.3s\r\n\r\nINFO: Build completed successfully, 354 total actions\r\n```\r\n\r\ncc: @jsimsa ", "comments": ["@kvignesh1420 thank you for the PR. FYI, I am working on removing some unused experimental optimization options that I would prefer to merge before your CL (so that we avoid moving some options only to remove them soon after).\r\n\r\nOn a related note, the branch cut for TF 2.6 is scheduled for 6/24. I am hoping to get my changes in in the next couple of days (and then your PR in before the branch cut).\r\n\r\nAlso, please take a look at the changes I made to your ThreadingOptions PR (https://github.com/tensorflow/tensorflow/commit/120ae92ab198392c7092176fd60a5049e0b7ac5a). The original approach (of maintaining both `threading` and `experimental_threading`) broke internal tests and so I switched to only keeping `threading` and having `experimental_threading` being re-routed to `threading`.", "@jsimsa I have made the initial python level changes for the new `AutotuneOptions` class and `autotune_options` Options attribute. Will continue with the changes after your CL.", "could we separate the `AutotuneOptions` change into a separate PR (which we could merge ahead of this PR) that would also include the C++ changes? thanks", "@jsimsa sure will create a new PR for AutotuneOptions change.", "@jsimsa  created the PR https://github.com/tensorflow/tensorflow/pull/50291 for addressing `AutotuneOptions`", "@kvignesh1420 Can you please resolve conflicts? Thanks!", "@kvignesh1420  Any update on this PR? Please. Thanks!", "@gbaned this PR is currently on hold until the 2.6.0 release. I will resolve the conflicts. Thanks!", "@jsimsa as you have covered the autotune options promotion in https://github.com/tensorflow/tensorflow/commit/c64634eb47f6581a7cf0cb01cf30284ff480d330, shall I continue with this PR? Please let me know.\r\n\r\ncc: @aaudiber ", "I would prefer to handle options related changes myself. My suggestion is to close this PR and prioritize other work suggested by @aaudiber (e.g. migrating `sample_from_datasets` to core API so that `rejection_resampling` implementation can stop depending on experimental API). ", "@jsimsa okay, sure! "]}, {"number": 50277, "title": "Bug: Bazel 3.7.2 fails to build with GCC 11.1.0 on ARMv8", "body": "Hi, \r\n`Bazel 3.7.2.` is failing to build on  `ARMv8`, hopefully, in the latest Bazel, they have fixed it.\r\nAny plan to use upgraded Bazel?\r\n\r\n", "comments": ["What error do you get?", "With GCC-11 we need to explicitely add `#include <limits>` and `#include <stdexcept>`. ", "This issue was fixed upstream. See #50176 and link therein.\r\n", "@samcom12 ,\r\n\r\nLooks like this is duplicate of issue #50303.Can you please close this issue, since it is already being tracked there? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50277\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50277\">No</a>\n"]}, {"number": 50276, "title": "TypeError: An op outside of the function building code is being passed ", "body": "Hi, i am trying to replicate this https://www.kaggle.com/ratthachat/image-captioning-by-effnet-attention-in-tf2-1/notebook but when i try to run the below code \r\n\r\n```\r\n# captions on the train set\r\nrid = np.random.randint(0, len(img_name_train))\r\nimage = img_name_train[rid]\r\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_train[rid] if i not in [0]])\r\nresult, attention_plot = evaluate(image)\r\n\r\nprint ('Real Caption:', real_caption)\r\nprint ('Prediction Caption:', ' '.join(result))\r\nplot_attention(image, result, attention_plot)\r\n```\r\ni am getting this below error\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\n\r\ntensorflow version i am using is 2.4.1.\r\n\r\n", "comments": ["@imnimesh \r\n\r\nLooking at the error log seems it is similar [#32889](https://github.com/tensorflow/tensorflow/issues/32889),and let us know if it helps.Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50275, "title": "Conversion of tensor shape values to None in a keras model", "body": "## Suppose I have a simple keras model as follows, \r\n\r\n    Model: \"sequential_1\"\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d (Conv2D)              (None, 16, 16, 8)         224       \r\n    _________________________________________________________________\r\n    conv2d_1                     (None, 7, 7, 8)           584       \r\n    _________________________________________________________________\r\n    conv2d_2                     (None, 3, 3, 2)           584       \r\n    =================================================================\r\n    Total params: 1,392\r\n    Trainable params: 1,392\r\n    Non-trainable params: 0\r\n\r\n## Now I want the final output layer shape as follows,\r\n\r\n\r\n    Model: \"sequential_1\"\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    conv2d (Conv2D)              (None, 16, 16, 8)         224       \r\n    _________________________________________________________________\r\n    conv2d_1                     (None, 7, 7, 8)           584       \r\n    _________________________________________________________________\r\n    conv2d_2                     (None, None, None, 2)     584       \r\n    =================================================================\r\n    Total params: 1,392\r\n    Trainable params: 1,392\r\n    Non-trainable params: 0\r\n\r\n## Code\r\n\r\n    m = Sequential()\r\n    \r\n    m.add(layers.InputLayer(input_shape=(32, 32, 1)))\r\n    m.add(Conv2D(8, (3,3), padding='same', strides=2))\r\n    m.add(Conv2D(8, (3,3), strides=2))\r\n    m.add(Conv2D(2, (3,3), strides=2))\r\n\r\n## Details\r\n> I hope you got an understanding of the issue.\r\n> Please try to make your own versions of the code. This is just a demo code.\r\n\r\nI am working on image colorization problem where we input an image with one color channel and get output with 2 color channels.\r\nThe original code is huge and different from any codes on the internet. So please don't be concerned about suggesting me any image colorization codes. \r\n\r\nI really appreciate it if you can help me with this issue.", "comments": ["@AdityaNikhil ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the tensorflow version, complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "@tilakrayal \r\n\r\nHere's the code I am working on. I've also mentioned what kind of architecture I expect at the end. \r\nPlease look into this thoroughly. It'll be a huge help for my research. \r\nhttps://colab.research.google.com/drive/1Tj1hRejjSjd5lq-Zxm15_LhdmKXOqLwY?usp=sharing\r\n\r\nThank you", "@AdityaNikhil ,\r\n\r\nCan you please take a look at links with similar error.Hope it helps.[Link1](https://github.com/tensorflow/tensorflow/issues/48513),[Link2](https://datascience.stackexchange.com/questions/64072/input-0-is-incompatible-with-layer-conv2d-2-expected-ndim-4-found-ndim-3-i-get),[Link3](https://stackoverflow.com/questions/47665391/keras-valueerror-input-0-is-incompatible-with-layer-conv2d-1-expected-ndim-4).\r\nThanks!", "@tilakrayal \r\nThank you for the links but still, they don't fix the issue.\r\n", "@ymodak ,\r\n\r\nI was able to reproduce the issue in tf [v2.4](https://colab.research.google.com/gist/tilakrayal/10912edf8c5533b186dc9ed7e5bfe993/2-4colorization_using_transformers.ipynb),[v2.5](https://colab.research.google.com/gist/tilakrayal/c286bd2249ee22486f70cd3f594494f8/2-5colorization_using_transformers.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/69f6786833831094d854def11ebdceeb/tf-nightlycolorization_using_transformers.ipynb).Please find the gist here.", "This question is better asked on [TensorFlow Forum](https://discuss.tensorflow.org/) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 50273, "title": "CUPTI could not be loaded & cupti.dll not found", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Windows 10\r\n- TensorFlow installed from: Source\r\n- TensorFlow version:  2.5.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version:  11.2 / 8.1.0 I believe\r\n- GPU model and memory: RTX 2080\r\nThis is the error I've get;\r\nI had [this issue](https://github.com/tensorflow/tensorflow/issues/43030#issuecomment-799969225). Once I did that copy trick, I started to get this issue...\r\n![Ekran g\u00f6r\u00fcnt\u00fcs\u00fc 2021-06-15 065118](https://user-images.githubusercontent.com/67585935/121990242-1d040500-cda6-11eb-9a79-b32762356f17.png)\r\n", "comments": ["Can you try the solution mentioned [here](https://stackoverflow.com/questions/56860180/).", "Nope, my paths are same;\r\n\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\r\n> C:\\ProgramData\\NVIDIA Corporation\\CUDA Samples\\v11.2\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\bin\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\libnvvp\r\n> C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\\extras\\CUPTI\\lib64\r\n> C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2020.3.0\\\r\n> C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR\r\n\r\nAnd I have `cupti64_2020.3.0.dll`, `cupti64_110.dll` in both `/CUDA/v11.2/bin` & `/CUDA/v11.2/extras/CUPTI/lib64`", "Create a copy of `cupti64_2020.3.0.dll ` and  rename the `cupti64_2020.3.0.dll `to `cupti64_112.dll` and try again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50273\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50273\">No</a>\n"]}, {"number": 50272, "title": "Improved lookup function", "body": "In python 3.x checking f_name in self._op_defs is faster than performing on a view object (i.e. self._op_defs.keys()), while in 2.x it's faster than creating a list which avoids linear time lookups.", "comments": []}, {"number": 50271, "title": "[MLIR][DISC] Bufferize GatherOp and DynamicGatherOp", "body": "support hlo-to-lhlo conversion for GatherOp and DynamicGatherOp", "comments": []}, {"number": 50270, "title": "[Intel MKL] Fixing a failure in run_eager_op_as_function_test", "body": "This PR fixes a failure in run_eager_op_as_function_test when MKL eager code is enabled. Failure started to happen after this [commit](https://github.com/tensorflow/tensorflow/commit/72e63b74a241c855c540997fd8f7cd2408c37016)", "comments": ["@mahmoud-abuzaina Could you please help fix [Android Demo App](https://source.cloud.google.com/results/invocations/c0927a03-7bbb-4882-b277-f79cad512d2d) and [MacOS CPU Python 3](https://source.cloud.google.com/results/invocations/f3b83e7a-b03b-4970-8332-0b8d0e0b69de) errors? Thank you!\r\n```\r\ntensorflow/core/common_runtime/eager/execute.cc:760:38: error: use of undeclared identifier 'mkl_op_registry'\r\n        absl::StartsWith(op->Name(), mkl_op_registry::kMklOpPrefix)) {\r\n                                     ^\r\ntensorflow/core/common_runtime/eager/execute.cc:764:25: error: use of undeclared identifier 'mkl_op_registry'\r\n      attr_kernel.set_s(mkl_op_registry::kMklNameChangeOpLabel);\r\n```", "Looks like it is still failing. I think even the header file has to be under #ifdef INTEL_MKL. Let me do that.", "Closing this as it is merged. "]}, {"number": 50269, "title": "Caching of Pointers to Redzone Checker Kernel", "body": "Introduces a cache for pointers to the Redzone Checker kernel which prevents the repeated loading and unloading of the kernel.", "comments": ["Unassign myself as I don't have time and I'm already reviewing other patches. @gbaned can you reassign?", "@philipphack  Can you please resolve conflicts? Thanks!", "@cheshire I think you are more familiar with this code, so I have added you as a reviewer, too.", "This breaks ROCm build:\r\n\r\n```\r\n./tensorflow/stream_executor/gpu/asm_compiler.h:24:10: fatal error: tensorflow/stream_executor/cuda/cuda_driver.h: No such file or directory\r\n #include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nNote that asm_compiler is used for both ROCm and CUDA.", "@philipphack  Can you please check @cheshire's comments and keep us posted ? Thanks!"]}, {"number": 50268, "title": "[INTEL MKL] Fix Pooling3D test case for --config=cuda build setting", "body": "This PR is for the Google requirement that all tests should pass with --config=cuda and when MKL is enabled.\r\nMKL pooling ops do not support NCDHW data format. The change disables the test execution when\r\n\r\n\r\n1. gpu is available, and\r\n\r\n\r\n2. MKL (oneDNN) is enabled.\r\n\r\n\r\nThe updated test has no impact on actual gpu execution (cuda_only=True) and MKL is disabled.", "comments": ["will reopen after internal review"]}, {"number": 50267, "title": "Fix UT failure config_test and context_test", "body": "Disable two unit tests in config_test.py and context_test.py\r\nbecause  get_memory_usage() and get_memory_usage() do not throw exception for CPU device in case of MKL is enabled. \r\n\r\nMore details: \r\n\r\n1. For default cpu allocator (framework/cpu_allocator_impl.cc), the global Boolean variable  cpu_allocator_collect_stats is set to true by direct_session by invoking EnableCPUAllocatorStats(). \r\n\r\n2. However, for MKL cpu allocator (common_runtime/mkl_cpu_allocator.h/.cc), its static Boolean variable mkl_small_allocator_collect_stats cannot be set to true, as the MKL cpu allocator cannot re-define the method EnableCPUAllocatorStats(). \r\n\r\nTODO: work with Google team to address the limitation in framework/allocator.h, or find a workaround. \r\n\r\n", "comments": ["will reopen after internal check", "Reopen after pinning down true root cause. This PR fixes the two test cases for now. ", "@gzmkl Gentle ping. The TF 2.6 branch cut is this Thursday at 5pm PT."]}, {"number": 50266, "title": "[INTEL MKL] Fix a unit test failure with mkl_fused_batch_norm_test", "body": "Fix a unit test failure with mkl_fused_batch_norm_test.cc, as one of the tests can only be executed with MKL blocked format.", "comments": []}, {"number": 50264, "title": "add out-of-class definition for FinalizeDatasetOp::kHasCapturedRef", "body": "", "comments": []}, {"number": 50263, "title": "Make DataFormatVecPermute's validation and implementation consistent with each other", "body": "There's currently a mismatch between what the validation of DataFormatVecPermute and its implementation is doing. While the validation allows src_format and dst_format to be of length 4 or 5, it only allows the input to be of shape [4] or [2, 4]. This leads to a gap in the validation where the following is allowed, but results in undefined behavior or uninitialized garbage data:\r\n\r\n`tf.raw_ops.DataFormatVecPermute(x=[1,2,3,4], src_format=\"NCDHW\", dst_format=\"NDHWC\")`", "comments": ["@mihaimaruseac How can I figure out what the issue is with the Windows Bazel GPU Internal CI build?", "Windows is failing internally at the moment, please ignore for now", "@mihaimaruseac Do I need to do anything to satisfy the feedback/copybara check?", "Looks like these tests are failing in their XLA+GPU configurations (`//tensorflow/python:nn_test_xla_gpu`). The tf2xla kernel likely needs updating: https://github.com/tensorflow/tensorflow/blob/db23048cd06b7ed1887fd3bb7ef99bdfd9ce3962/tensorflow/compiler/tf2xla/kernels/data_format_ops.cc#L106\r\n\r\nAlternatively you can use `@test_util.disable_xla('reason')` on the new tests for now (but please file a bug for the discrepancy to be fixed if so).", "@PatriceVignola  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "@gbaned @allenlavoie I copied the changes to the XLA kernel and re-ran the tests.", "Looks like clang-tidy identifies two issues. Can you take a look? https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#c-coding-style\r\n\r\nJust in core/kernels/data_format_ops.h; `explicit` on a single-argument constructor and no trailing underscore for struct members.", "Sure I'll fix it, but this is preexisting. I didn't touch this header besides changing the array size."]}, {"number": 50262, "title": "[TFLite]: Add CXXSTANDARD to Makefile", "body": "This PR adds a new variable in the Tensorflow Lite Makefile: `CXXSTANDARD`.\nThis variable defaults to the current `c++11` standard used by Tensorflow\nLite, but also allows users to use a different standard if they desire.\n", "comments": ["@abattery Looking at the build logs for MacOS CPU Python3 and Ubuntu CPU, there's a doctest failure. Am I correct in assuming that's unrelated to this PR?"]}, {"number": 50258, "title": "URL of tf.raw_ops.SparseReduceSum comprises Irrelevant/Garbage Values", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum?hl=ko.Souten%20Ki%20Beti%20HD%20Jeetendra%20Rekha%20Jaya%20Prada%20Hindi%20Full%20MovieShyam\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe **`Hyperlink`** of the Op, [**`tf.raw_ops.SparseReduceSum`**](https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum?hl=ko.Souten%20Ki%20Beti%20HD%20Jeetendra%20Rekha%20Jaya%20Prada%20Hindi%20Full%20MovieShyam) has some garbage/irrelevant values like **Souten**, **Beti**, **Jeetendra**, **Rekha**, etc..", "comments": ["@worldpeaceaspirer \r\nCould you please share more detailed info, a screenshot where these terms appear, as we cannot find it in the link shared.", "Please see the Hyperlink in the below screenshot:\r\n\r\n![image](https://user-images.githubusercontent.com/77092296/122087508-6c881680-ce22-11eb-984c-faaf4af32bec.png)\r\n", "These extra terms are in the parameters, not the URL itself.\r\n\r\nTypically any wabsite will still show the page even if you pass nonsense `name=value` parameters after the?\r\n\r\nOn tensorlfow.org the `hl` parameter is a method to set the language, and it looks like someone may have accidentally pasted some extra values onto the end of the url.\r\n\r\nUnless this bizarre url is in the source of a page somewhere, there's nothing to fix here.\r\n\r\nThe URL is: https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum"]}, {"number": 50257, "title": "Fix for CVE-2021-29515", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.2\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe implementation of `MatrixDiag*` operations(https://github.com/tensorflow/tensorflow/blob/4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33/tensorflow/core/kernels/linalg/matrix_diag_op.cc#L195-L197) does not validate that the tensor arguments are non-empty. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.\r\n\r\n**Will this change the current api? How?** No\r\n\r\n**Who will benefit with this feature?** Vulnerability fix.\r\n\r\n**Any Other info.**\r\nWhen can we expect a fix on Tensorflow-2.3.3.\r\n", "comments": ["@PranitModak,\r\nYou mentioned \r\n\r\n> does not validate that the tensor arguments are non-empty. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range. \r\n\r\nCan you please let us know the source of that information and what exactly you need.", "Wheel packages landed yesterday", "@mihaimaruseac The 2.3.3 wheel doesn't contain fix for the below CVEs\r\n\r\nCVE-2021-29558\r\nCVE-2021-29595\r\n\r\n\r\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29558\r\nhttps://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-29595, although here it is mentioned that the fixes will be cherry picked to 2.3.3\r\n\r\n2.3.3 only includes fix for CVE-2021-29515.\r\n\r\nCan you please reopen the issue.", "Are you sure?\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/c73a74de9f4bbf9c0cee6bc318030c289514e415 and https://github.com/tensorflow/tensorflow/commit/6ec87e02d582e555d403223d50a0281e31f8939e are the equivalents of https://github.com/tensorflow/tensorflow/commit/106d8f4fb89335a2c52d7c895b7a7485465ca8d9 and https://github.com/tensorflow/tensorflow/commit/8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31 (from the 2 CVEs) on the `r2.3` branch and included in the release", "And testing the reproducer code shows that the fix indeed exists:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n2.3.3\r\n>>> shape_dims = tf.constant(0, dtype=tf.int64)\r\n>>> indices = tf.ones([1, 1], dtype=tf.int64)\r\n>>> values = tf.ones([1], dtype=tf.int64)\r\n>>> shape = tf.ones([1], dtype=tf.int64)\r\n>>> tf.raw_ops.SparseSplit(split_dim=shape_dims, indices=indices, values=values, shape=shape, num_split=1)\r\n2021-06-18 09:31:27.922908: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at sparse_split_op.cc:86 : Invalid argument: Slice index 1 is larger than num_split.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/data/tf-test/venv/lib64/python3.8/site-packages/tensorflow/python/util/tf_export.py\", line 404, in wrapper\r\n    return f(**kwargs)\r\n  File \"/data/tf-test/venv/lib64/python3.8/site-packages/tensorflow/python/ops/gen_sparse_ops.py\", line 2910, in sparse_split\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/data/tf-test/venv/lib64/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Slice index 1 is larger than num_split. [Op:SparseSplit]\r\n```\r\n"]}, {"number": 50256, "title": "Add GPU kernel for SparseSegment[Sum,Mean,SqrtN]Grad", "body": "Adds a GPU kernel and an additional test for these grad ops.\r\nIt uses the existing `SegmentReduceVectorKernel` CUDA kernel with a slight modification.\r\n\r\ncc @nluehr ", "comments": ["Wait did this get reverted in https://github.com/tensorflow/tensorflow/commit/a996de56e81be87846def7622cf15a8b9208f0b8 ? It doesn't say it's a revert, but that's what the diff is...\r\n@sanjoy ", "Yes, that was a revert.  I'm working on rolling this forward (one of our buildbots did not enjoy the `auto` return type on  `MakeLookupAndScaleAndCastInputsIterator`.", "Landed again at https://github.com/tensorflow/tensorflow/commit/a8556737c92cb7d920eda6c3c5c7c4310a76a1df."]}, {"number": 50255, "title": "Reading shape of tf.Tensor fails after sliced using an input inside tf.function", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: pip installation\r\n- TensorFlow version: 2.5\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: RTX 2060, 6GB\r\n\r\nRunning the following block:\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef fun(sample, pos):\r\n    x = sample[:10-pos]\r\n    l = tf.constant(x.shape[0])\r\n    return l\r\n\r\nsample = tf.random.uniform([10])\r\npos = tf.constant(2)\r\nout = fun(sample, pos)\r\n```\r\nThis results in the following error message:\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-15-e85c7a447ebb> in <module>\r\n      7 sample = tf.random.uniform([10])\r\n      8 pos = tf.constant(2)\r\n----> 9 out = fun(sample, pos)\r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    887 \r\n    888       with OptionalXlaContext(self._jit_compile):\r\n--> 889         result = self._call(*args, **kwds)\r\n    890 \r\n    891       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    931       # This is the first call of __call__, so we have to initialize.\r\n    932       initializers = []\r\n--> 933       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    934     finally:\r\n    935       # At this point we know that the initialization is complete (or less\r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    761     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\r\n    762     self._concrete_stateful_fn = (\r\n--> 763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n    764             *args, **kwds))\r\n    765 \r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   3048       args, kwargs = None, None\r\n   3049     with self._lock:\r\n-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   3051     return graph_function\r\n   3052 \r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3442 \r\n   3443           self._function_cache.missed.add(call_context_key)\r\n-> 3444           graph_function = self._create_graph_function(args, kwargs)\r\n   3445           self._function_cache.primary[cache_key] = graph_function\r\n   3446 \r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3277     arg_names = base_arg_names + missing_arg_names\r\n   3278     graph_function = ConcreteFunction(\r\n-> 3279         func_graph_module.func_graph_from_py_func(\r\n   3280             self._name,\r\n   3281             self._python_function,\r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    997         _, original_func = tf_decorator.unwrap(python_func)\r\n    998 \r\n--> 999       func_outputs = python_func(*func_args, **func_kwargs)\r\n   1000 \r\n   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    670         # the function a weak reference to itself to avoid a reference cycle.\r\n    671         with OptionalXlaContext(compile_with_xla):\r\n--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    673         return out\r\n    674 \r\n\r\n~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    984           except Exception as e:  # pylint:disable=broad-except\r\n    985             if hasattr(e, \"ag_error_metadata\"):\r\n--> 986               raise e.ag_error_metadata.to_exception(e)\r\n    987             else:\r\n    988               raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-15-e85c7a447ebb>:4 fun  *\r\n        l = tf.constant(x_l.shape[0])\r\n    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant  **\r\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl\r\n        tensor_util.make_tensor_proto(\r\n    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto\r\n        raise ValueError(\"None values not supported.\")\r\n\r\n    ValueError: None values not supported.\r\n```\r\nAs I understand, this means the shape of x after being sliced from a tensor using a value input to the autographed function is somehow set to None. Note that this code throws the same error message (sample is defined inside rather than as an argument):\r\n```python\r\n@tf.function\r\ndef fun(pos):\r\n    sample = tf.random.uniform([10])\r\n    x = sample[:10-pos]\r\n    l = tf.constant(x.shape[0])\r\n    return l\r\n\r\n#sample = tf.random.uniform([10])\r\npos = tf.constant(2)\r\nout = fun(pos)\r\n```\r\nHowever, this runs without any errors (pos is defined inside the function):\r\n```python\r\n@tf.function\r\ndef fun(sample):\r\n    pos = tf.constant(2)\r\n    x = sample[:10-pos]\r\n    l = tf.constant(x.shape[0])\r\n    return l\r\n\r\nsample = tf.random.uniform([10])\r\n#pos = tf.constant(2)\r\nout = fun(sample)\r\n```", "comments": ["@Saduf2019 \r\n\r\nI was able to replicate the reported here,Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/546b7ade56d36c098116e0776d9471fc/untitled100.ipynb).Thanks", "@dinesh110598 \r\nWhen a function is decorated with @tf  it is considered as a graph function.Which means that the shape of the pos will be evaluated lazily, If you try printing pos , you will see it will have None as shape.", "@Saduf2019 \r\nThings seem to be goofier in tf.function. When I tried something like this (as you've suggested):\r\n```python\r\n@tf.function\r\ndef fun(sample, pos):\r\n    x = sample[:10-pos]\r\n    tf.print(pos.shape)\r\n    return x\r\n\r\nsample = tf.random.uniform([10])\r\npos = tf.constant(2)\r\nout = fun(sample, pos)\r\n```\r\nThe output is `TensorShape([])` just as expected, not None. Even `tf.print(sample.shape)` outputs `TensorShape([10])` as expected. It's only the the shape of x (tensor obtained after slicing sample with pos) that's `TensorShape([None])`. In other words, while the shapes of inputs like sample and pos are evaluated properly, it's pretty weird that only a quantity that's after slicing one of the inputs using another has its shape handled lazily. I mean it's a somewhat inconsistent behavior.", "@dinesh110598 \r\nLazy eval means you do not have access to the tensor at the moment but you will know it value at execution . The output shape of x after slicing is not known  before you actually execute it hence it appears as none. This is not a bug or Feature Request, please move this issue to closed status and open an issue in tf.discussion or stackoverflow for any further queries.", "@dinesh110598 \r\nYou can print when you already know the `input_signature`\r\nE.g.\r\n\r\n```\r\nimport tensorflow as tf\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(10,), dtype=tf.float32)])\r\ndef fun(sample):\r\n    x = sample[:10-2]\r\n    tf.print(x.shape)\r\n    print(x)\r\n    return sample\r\n\r\nsample = tf.random.uniform([10])\r\nprint(sample)\r\nout = fun(sample)\r\nprint(out.shape)\r\n```", "@Saduf2019 @bhack \r\nHey thanks, think I get what you mean...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50255\">No</a>\n"]}, {"number": 50254, "title": "Bug in keras custom layer", "body": "System: Python 3.8.3 64-bit | Qt 5.12.9 | PyQt5 5.12.3 | Linux 5.8.0-55-generic \r\n              keras (newest version)\r\n```#!/usr/bin/env python3\r\nfrom keras.layers import Layer\r\nimport numpy as np\r\nfrom keras.layers import Input\r\nfrom keras.models import Model  \r\n\r\nInputTensor = [0.1,0.2,0.3]      # Attention! Different tensors cause errors. \r\n                                 # See  below the code!\r\n\r\nInputTensor = np.array(InputTensor) \r\n\r\nclass MyLayer(Layer):\r\n    \r\n    def __init__(self, **kwargs):\r\n        super(MyLayer, self).__init__(**kwargs)\r\n    def build(self, input_shape):\r\n        self.built = True\r\n        super(MyLayer,self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        return x\r\n\r\ninputlayer = Input(shape = InputTensor.shape)\r\nlayer = MyLayer()(inputlayer)\r\n\r\nmodel = Model(inputlayer, layer ) \r\nmodel.summary()\r\n\r\nOutput = model.predict(InputTensor)\r\noutput = np.array(Output)\r\nprint(\"dim(Output) = \"+str(output.ndim))\r\nprint(\"Output = \"+str(Output))\r\n```\r\n\r\n1. Input tensor [0.1,0.2,0.3] cause no problems. Output is [[0.1][0.2][0.2]]. Access to tensor elements are possible. model.summary is (None,2), ndim is 2.\r\n2. Input tensor [[0.1, 0.2,0.3],[0.2,0.3,0.4],[0.3,0.4,0.5]] has an output as [[0.1 0.2 0.3][0.2 0.3 0.4][0.3 0.4 0.5]] and model.summary indicate a shape like (None,3,3) so that a tensor of tree dimensions is expected. ndim should be 3 instead of 2! Access to tensor elements is possible if you consider 2 dimensions instead of 3.\r\n3. Input tensor [[0.1,0.2,0.3],[0.2,0.3,0.4]] generates an error message. It expected a shape of (None,2,3) and found only (None,3) although model.summary shows (None,2,3). \r\n\r\nIt shouldn't have any influence how many dimensions a tensor has, nor how big its elements are!\r\nMy task is, to get access to single elements of a tensor to generate an output tensor like\r\n```\r\nclass MyLayer(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MyLayer, self).__init__(**kwargs)\r\n    def build(self, input_shape):\r\n        self.built = True\r\n        super(MyLayer,self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        OutputTensor = [[0.0, 0.0],\r\n                        [0.0, 0.0]]\r\n        OutputTensor[0][0] = x[0][0]\r\n        OutputTensor[1][1] = x[1][0]\r\n        OutputTensor[1][0] = x[2][0]\r\n        return OutputTensor\r\n```\r\n\r\n", "comments": ["@Oezkan23 ,\r\n\r\nPlease, fill issue template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose).Also please let us know the tensorflow version and confirm whether you are facing similar error(**AttributeError: 'list' object has no attribute 'ndim')** or different error while executing the code.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/d107929a3c8ee2f1d65e0d7641311f35/50254.ipynb) of it here.\r\n\r\nThanks!", "Sorry, I was mistaken because I had German expressions in the code.\r\n\r\nThe end of the code should be:\r\n```\r\nOutput = model.predict(InputTensor)\r\noutput = np.array(Output)\r\nprint(\"dim(Output) = \"+str(output.ndim))\r\nprint(\"Output = \"+str(Output))\r\n```"]}, {"number": 50253, "title": "update all values of dict like in session.run tensorflow", "body": "This function takes the pre-trained imagenet-vgg-verydeep-19.mat model and using its weights and input returns the graph(model)\r\n\r\n```\r\ndef load_vgg_model(path):\r\n    ...\r\n    ...\r\n    ...\r\n    # Constructs the graph model.\r\n    graph = {}\r\n    graph['input']   = tf.Variable(np.zeros((1, 400, 300, 3)), dtype = 'float32')\r\n    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')      # returns tf.nn.relu\r\n    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\r\n    graph['avgpool1'] = _avgpool(graph['conv1_2'])                    # returns tf.nn.avg_pool\r\n    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\r\n    ....\r\n    ....\r\n    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\r\n    graph['avgpool5'] = _avgpool(graph['conv5_4'])\r\n    \r\n    return graph\r\n```\r\n\r\n\r\n\r\nHere is the implementation of the same in TensorFlow 1\r\n\r\n```\r\nmodel = load_vgg_model(\"imagenet-vgg-verydeep-19.mat\")\r\nsess = tf.compat.v1.InteractiveSession()\r\ncontent_image = imread(\"images/louvre_small.jpg\")\r\n\r\nsess.run(model['input'].assign(content_image))\r\nout = model['conv4_2']\r\na_C = sess.run(out)\r\na_G = out\r\n```\r\n\r\nBut I wanted to know the implementation in TensorFlow 2.x\r\n\r\nI have read these documents https://www.tensorflow.org/guide/effective_tf2\r\n\r\nhttps://www.tensorflow.org/guide/migrate\r\n", "comments": []}, {"number": 50252, "title": "tf.data.experimental.service does not support from_generator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Tensorflow docker image (tensorflow/tensorflow:2.5.0-gpu)\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nGet error message and trace;\r\n```\r\n2021-06-14 09:56:03.953422: W tensorflow/core/framework/op_kernel.cc:1755] Invalid argument: ValueError: callback pyfunc_0 is not found\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 238, in __call__\r\n    raise ValueError(\"callback %s is not found\" % token)\r\n\r\nValueError: callback pyfunc_0 is not found\r\n```\r\n**Describe the expected behavior**\r\nI expect the dataset to handle data from generators. \r\n\r\nIn this instance, I would expect to get the same output as if `tf.data.experimental.service.distribute` was not applied to the dataset, or if `tf.data.Dataset.from_generator` was replaced with `tf.data.Dataset.range(5)`.\r\n\r\nWhile the documentation states explicitly that `processing_mode=\"distributed_epoch\"` has limitations, this is not the case for  `processing_mode=\"parallel_epochs\"` where it is stated that you can [\"Create the dataset however you were before using the tf.data service.\"](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#using_the_tfdata_service_from_your_training_job)\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nminimal dataset:\r\n```\r\n    import tensorflow as tf\r\n\r\n    ds = (\r\n        tf.data.Dataset.from_generator(\r\n            range,\r\n            output_signature=(tf.TensorSpec(shape=(), dtype=tf.int32)),\r\n            args=[5])\r\n        .apply(tf.data.experimental.service.distribute(\r\n            processing_mode=\"parallel_epochs\",\r\n            service='grpc://0.0.0.0:32121')\r\n        )\r\n    )\r\n\r\n    for data in ds:\r\n        print(data)\r\n```\r\nminimal worker code (run on same device):\r\n```\r\nimport tensorflow as tf\r\ndispatch_config = tf.data.experimental.service.DispatcherConfig(port=32121)\r\ndispatcher = tf.data.experimental.service.DispatchServer(dispatch_config)\r\nworker_config = tf.data.experimental.service.WorkerConfig(\r\n        port=32122,\r\n        dispatcher_address=\"0.0.0.0:32121\",\r\n        worker_address=\"0.0.0.0:32122\",\r\n)\r\nworker = tf.data.experimental.service.WorkerServer(worker_config)\r\ndispatcher.join()\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mikael-epigram \r\n\r\nLooking at the error log seems it is similar to [#23885](https://github.com/tensorflow/tensorflow/issues/23885),and let us know if it helps.Thanks", "@UsharaniPagadala \r\n\r\nI don't think that issue is relevant.\r\n\r\nMy issue is with using `tf.data.Dataset.from_generator` together with `tf.data.experimental.service` servers.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "In the upcoming version of Tensorflow 2.6, there are some changes made on` tf.data` side as mentioned in the release note.\r\nCheck if any of the below update help your cause. \r\n\r\n`Added target_workers param to data_service_ops.from_dataset_id and data_service_ops.distribute. Users can specify \"AUTO\", \"ANY\", or \"LOCAL\" (case insensitive). If \"AUTO\", tf.data service runtime decides which workers to read from. If \"ANY\", TF workers read from any tf.data service workers. If \"LOCAL\", TF workers will only read from local in-processs tf.data service workers. \"AUTO\" works well for most cases, while users can specify other targets. For example, \"LOCAL\" would help avoid RPCs and data copy if every TF worker colocates with a tf.data service worker. Currently, \"AUTO\" reads from any tf.data service workers to preserve existing behavior. The default value is \"AUTO\".`  \r\n\r\n", "@sachinprasadhs \r\n\r\nThose changes deal with optimizing which workers to read data from. This is not relevant for the issue. ", "@mikael-epigram Thanks for reporting this. As you found, tf.data service does not work with datasets created using `from_generator`, even though this limitation wasn't called out in the docs. https://github.com/tensorflow/tensorflow/commit/c9eafd47de1385390a659da5d97e7602b7f62185 will update the documentation to call out the limitation until we have a way to address it. For now, is it possible to write your dataset in a way that doesn't use `from_generator`?", "The dataset is stored in a database, and requires some non-trivial computation to extract data, which then comes in large chunks. This is made efficient by storing some intermediate state, hence the generator. To not use `from_generator` is possible, eg. by using `map`, but it would mean it would be harder to reuse the intermediate state, making the input pipeline very inefficient.\r\n\r\nI think for now I will have to pursue a non-tensorflow solution for the input pipeline \ud83d\ude14", "To make this work well with TF, we would need TF kernels for reading the data from the specific type of database you are using. [tensorflow/io](https://github.com/tensorflow/io/tree/master/docs/tutorials) has many custom dataset implementations that you could use for reference.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50252\">No</a>\n"]}, {"number": 50251, "title": "Use custom model_spec for object detection", "body": "**System information**\r\n- MacOS\r\n- TensorFlow from COLAB\r\n\r\n\r\nI would like to use TF Lite Model Maker for transfer learning using a different model from EfficentDet[0-4].\r\n\r\nso, is possible to use a different model spec?\r\nif yes, how can I get model_spec for different model like ssd mobile net?\r\n\r\nhere is the problem:\r\n spec = model_spec.get('efficientdet_lite0')\r\n", "comments": ["@mrjoelc,\r\nCan you please go through the [Tutorial of Object Detection with TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection) and let us know if it helps? Thanks!", "> @mrjoelc,\r\n> Can you please go through the [Tutorial of Object Detection with TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection) and let us know if it helps? Thanks!\r\n\r\nI came directly from this tutorial. If I want to do transfer learning using model maker but for a net different from EfficentDet how can I do? for what I understood I need a \"model_spec\" but I don't know where to find a model_spec different from EfficientDet", "Hi @mrjoelc ! I have attached a relevant [thread](https://blog.tensorflow.org/2021/06/easier-object-detection-on-mobile-with-tf-lite.html) for usage of ssd mobilenet in model_spec.get() . Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50251\">No</a>\n"]}, {"number": 50250, "title": "The performance of the saved model and the performance of the trained model seem to be different ", "body": "I use tensorflow2.4 and generate images by using the GAN method.\r\n\r\nDuring the training stage, the generated model at one certain step is good. And then I save it by .tf. However, when I load the model again and plan to generate new images, these images are pretty terrible. (I have tried many times). This should not be a common issue for the GAN method. So could you please tell me the reason why there are huge differences after saving the model?", "comments": ["@RayGuo-C ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50249, "title": "mixed keras import levels lead to untracked variables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5\r\n- Python version: Ubuntu 20.04.2 \r\n- CUDA/cuDNN version: 11.2/8.1.0\r\n- GPU model and memory: RTX 2080 TI\r\n\r\n**Describe the current behavior**\r\nMixing layers (e.g. Input, Dense) which are imported via\r\n`from tensorflow import keras`\r\n`Input = keras.layers.Input()`\r\nand \r\n`from keras.layers import Dense`\r\n`Linear = Dense()`\r\nleads to variables not being tracked\r\n**Describe the expected behavior**\r\nvariables should be tracked\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nfrom tensorflow import keras\r\nfrom keras.layers import Input, Dense\r\n\r\n\r\ninput1 = keras.layers.Input(shape=(2, 2))\r\ninput2 = Input(shape=(2, 2))\r\nlinear_layer = Dense(4)\r\n\r\ny = linear_layer(input1)\r\nmodel1 = keras.Model(input1, y)\r\n\r\ny = linear_layer(input2)\r\nmodel2 = keras.Model(input2, y)\r\nprint(model1.trainable_variables)\r\n#outputs:\r\n#[]\r\nprint(model2.trainable_variables)\r\n#outputs:\r\n#[<tf.Variable 'dense/kernel:0' shape=(2, 4) dtype=float32, numpy=\r\n#array([[ 0.26048756,  0.08715534,  0.3931222 , -0.02270007],\r\n#       [-0.17754197, -0.7752099 , -0.3470106 ,  0.85839653]],\r\n#      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\r\n```\r\n**Any Other info**\r\nTensorflow issues a warning related to Lambda layers: \r\n```WARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (tf.tensordot), but\r\nare not present in its tracked objects:\r\n  <tf.Variable 'dense/kernel:0' shape=(2, 4) dtype=float32>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.'```\r\n", "comments": ["@qp-qp \r\n\r\nI reproduced the code shared with minimal code changes with no errors.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/03744c3054a6f4eae40ae6d907a811df/untitled92.ipynb) here, and let us know if it  helps.Thanks", "@UsharaniPagadala \r\nHey thanks for the reply. I know that it is possible to change the code such that the variables are being tracked (Hence I included a minimal example of model2 with tracked variables).\r\nMy problem is that I think that the behavior exhibited in the minmal code example is quite prone to cause bugs.\r\nIn my case, I used code from different projects for my model and the unexpected behavior led to my model not training properly and the warning message by tensorflow did not really help me in tracking down the problem either.", "@qp-qp Looks like some layers were imported from `keras` and others from `tf.keras`. I changed one line in your code and everything works as expected.\r\n\r\n```\r\n# from keras.layers import Input, Dense # this imports Dense from keras and results in [] variables for model 1\r\nfrom tensorflow.keras.layers import Input, Dense \r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/d2b0a3660f882aa5aff1533b4a8d15a0/untitled92.ipynb).\r\n\r\nPlease verify once and close the issue if this is resolved for you. Thanks!", "@jvishnuvardhan\r\nI am aware that the problem is caused by mixing layer imports. This was the reason for raising this issue and including it in my code example. I thought that this behavior was a bug. If it is the intended behavior I will close this issue.", "@qp-qp Looks like this is a bug. What I gave above was a workaround, so we will keep this open. Thanks!", "Briefly, keras and tf.keras referred to different packages, so there are some integration errors. In Tensorflow 2.6 (release soon) this error is fixed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50249\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50249\">No</a>\n"]}, {"number": 50248, "title": "Android studio:Error occurred when detecting the image", "body": " I had trained my custom model with different objects for the detection purpose. I had used ssd_mobilenet_v1_coco for my training. I could pull out the object detection on desktop with the customised trained model by using the webcam or passing an image. After converted my tf model to tflite model, adding metadata and quantized my model I obtained the following error when I run my application in Android Studio:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 28456\r\n    java.lang.AssertionError: Error occurred when detecting the image: Label map does not contain enough elements: model returned class index 79 but label map only contains 2 elements.\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(ObjectDetector.java:421)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(ObjectDetector.java:401)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:94)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:246)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\nI/Process: Sending signal. PID: 28456 SIG: 9\r\n\r\n\r\nMy application detect the objects, but after approximately 1 minute, the application stop every time I run the program .\r\nHas anyone encountered this error before ?\r\n", "comments": ["@raulpopa98 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the tensorflow version,complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50248\">No</a>\n"]}, {"number": 50247, "title": "Cannot save custom model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the current behaviour**\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/root/rl-toolkit/rl_toolkit/__main__.py\", line 155, in <module>\r\n    agent.save()\r\n  File \"/root/rl-toolkit/rl_toolkit/policy/learner.py\", line 215, in save\r\n    self.model.save(os.path.join(self._save_path, \"model\"))\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 2111, in save\r\n    save.save_model(self, filepath, overwrite, include_optimizer, save_format,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/save.py\", line 150, in save_model\r\n    saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 89, in save\r\n    saved_nodes, node_paths = save_lib.save_and_return_nodes(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 1103, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 1290, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def,\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 1207, in _build_meta_graph_impl\r\n    signatures = signature_serialization.find_function_to_export(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/signature_serialization.py\", line 99, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py\", line 154, in list_functions\r\n    obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\", line 2713, in _list_functions_for_serialization\r\n    functions = super(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 3016, in _list_functions_for_serialization\r\n    return (self._trackable_saved_model_saver\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py\", line 92, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 73, in functions_to_serialize\r\n    return (self._get_serialized_attributes(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py\", line 89, in _get_serialized_attributes\r\n    object_dict, function_dict = self._get_serialized_attributes_internal(\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py\", line 48, in _get_serialized_attributes_internal\r\n    default_signature = save_impl.default_save_signature(self.obj)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save_impl.py\", line 215, in default_save_signature\r\n    fn = saving_utils.trace_model_call(layer)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saving_utils.py\", line 119, in trace_model_call\r\n    raise_model_input_error(model)\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saving_utils.py\", line 90, in raise_model_input_error\r\n    raise ValueError(\r\nValueError: Model <rl_toolkit.networks.actor_critic.ActorCritic object at 0x7f6d059b2ee0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling `.fit()` or `.predict()`. To manually set the shapes, call `model.build(input_shape)`.\r\n```\r\n\r\nBut before I'm saving the model, I ran:\r\n```python\r\n# Actor network (for learner)\r\nself.model = ActorCritic(\r\n    num_of_outputs=tf.reduce_prod(self._env.action_space.shape),\r\n    gamma=gamma,\r\n    tau=tau,\r\n)\r\nself.model.build((None,) + self._env.observation_space.shape)\r\nself.model.compile(\r\n    actor_optimizer=Adam(learning_rate=actor_learning_rate),\r\n    critic_optimizer=Adam(learning_rate=critic_learning_rate),\r\n    alpha_optimizer=Adam(learning_rate=alpha_learning_rate),\r\n)\r\n\r\nself.model.summary()\r\n```\r\n\r\nThe result of `summary()`:\r\n```bash\r\nModel: \"actor_critic\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nactor (Actor)                multiple                  139108\r\n_________________________________________________________________\r\nmulti_critic (MultiCritic)   multiple                  270802\r\n_________________________________________________________________\r\nmulti_critic_1 (MultiCritic) multiple                  270802\r\n=================================================================\r\nTotal params: 680,714\r\nTrainable params: 678,313\r\nNon-trainable params: 2,401\r\n_________________________________________________________________\r\n```\r\n\r\nDefinition of custom model:\r\n```python\r\nclass ActorCritic(Model):\r\n    def __init__(self, num_of_outputs: int, gamma: float, tau: float, **kwargs):\r\n        super(ActorCritic, self).__init__(**kwargs)\r\n\r\n        self.gamma = tf.constant(gamma)\r\n        self.tau = tf.constant(tau)\r\n\r\n        # init param 'alpha' - Lagrangian constraint\r\n        self.log_alpha = tf.Variable(0.0, trainable=True, name=\"log_alpha\")\r\n        self.alpha = tf.Variable(0.0, trainable=False, name=\"alpha\")\r\n        self.target_entropy = tf.cast(-num_of_outputs, dtype=tf.float32)\r\n\r\n        # Actor\r\n        self.actor = Actor(num_of_outputs)\r\n\r\n        # Critic\r\n        self.critic = MultiCritic(2)\r\n        self.critic_target = MultiCritic(2)\r\n        self._train_target(self.critic, self.critic_target, tau=tf.constant(1.0))\r\n\r\n    def train_step(self, data):\r\n        with tf.GradientTape(persistent=True) as tape:\r\n\r\n        # ....\r\n\r\n        return {\r\n            \"actor_loss\": actor_loss,\r\n            \"critic_loss\": Q_loss,\r\n            \"alpha_loss\": alpha_loss,\r\n        }\r\n\r\n    def _train_target(self, source, target, tau):\r\n        for source_weight, target_weight in zip(\r\n            source.trainable_variables, target.trainable_variables\r\n        ):\r\n            target_weight.assign(tau * source_weight + (1.0 - tau) * target_weight)\r\n\r\n    def call(self, inputs):\r\n        action, log_pi = self.actor(inputs, with_log_prob=True)\r\n        Q_value = tf.reduce_min(self.critic([inputs, action]), axis=1)\r\n        Q_value_target = tf.reduce_min(self.critic_target([inputs, action]), axis=1)\r\n        return [Q_value, Q_value_target, action, log_pi]\r\n\r\n    def compile(self, actor_optimizer, critic_optimizer, alpha_optimizer):\r\n        super(ActorCritic, self).compile()\r\n        self.actor_optimizer = actor_optimizer\r\n        self.critic_optimizer = critic_optimizer\r\n        self.alpha_optimizer = alpha_optimizer\r\n```\r\n\r\nHow can I use `self.model.save()` correctly on a custom model?\r\nThanks.", "comments": ["@markub3327 Custom models are little different. Before saving the model, You need to call them on some input as shown in [this section](https://www.tensorflow.org/guide/keras/save_and_serialize#how_savedmodel_handles_custom_objects) of the `save and load` tutorial. Thanks!\r\n\r\nShare a simple standalone to reproduce the issue if the above tutorial didn't help you. Thanks!", "Yeah, I call only layers individually (inside `train_step(self, data)`), but the model's `call()` method is never used. The model's call() method is unnecessary for me ... So, can I call it on `tf.zeros` before saving (the content of the model's call method is only for illustration to init all layers contained in the model with input_shape)?  Why is not it enough to use only `model.build(input_shape)` before saving? I thought that the `model.build(input_shape)` method used `call()` automatically when was called. Thanks.", "@markub3327 Yes, you can call the model with your input (any input, just to set the random weights) or run `model.build()` method. Can you try that and share a standalone code to reproduce any issues. Thanks!\r\n", "[Here is the reproducible code](https://colab.research.google.com/gist/markub3327/6fd3075359b21e8708dcbcd356874c83/untitled2.ipynb)\r\n\r\n```\r\nValueError: Model <__main__.ActorCritic object at 0x7f7a1544b550> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling `.fit()` or `.predict()`. To manually set the shapes, call `model.build(input_shape)`.\r\n``` \r\nWhat this means when I called `model.build(input_shape)` before saving (please look at snippet)? I thought that this error talks about the possibility of using `model.build(input_shape)` before saving the model. \r\n\r\nAnd can you describe to me please what means: \r\n```\r\nWARNING:absl:Found untraced functions such as layer_normalization_33_layer_call_fn, layer_normalization_33_layer_call_and_return_conditional_losses, fc1_a_layer_call_fn, fc1_a_layer_call_and_return_conditional_losses, layer_normalization_34_layer_call_fn while saving (showing 5 of 155). These functions will not be directly callable after loading.\r\n\r\nINFO:tensorflow:Assets written to: model/assets\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  category=CustomMaskWarning)\r\n```\r\n\r\nAnd my last question is about generating an image of the model's topology... The `tf.keras.utils.plot_model()` draws me only one block (that present all my model). I want to see all layers contained in my model.\r\n\r\n![model](https://user-images.githubusercontent.com/74611856/122300008-0624f600-ceff-11eb-8f55-4122c54fd39b.png)\r\n\r\nThanks!", "```\r\nValueError: Could not find matching function to call loaded from the SavedModel. Got:\r\n  Positional arguments (4 total):\r\n    * Tensor(\"inputs:0\", shape=(1, 24), dtype=float32)\r\n    * False\r\n    * None\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (4 total):\r\n    * TensorSpec(shape=(None, 24), dtype=tf.float32, name='inputs')\r\n    * False\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (4 total):\r\n    * TensorSpec(shape=(None, 24), dtype=tf.float32, name='inputs')\r\n    * True\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\nHow can I use the saved model with other parameters, that weren't used during training? I have only 2 option to use `call()`, but I need the third option like `call(inputs, None, None, True)`\r\n\r\nReproducible code is [here](https://colab.research.google.com/gist/markub3327/6fd3075359b21e8708dcbcd356874c83/untitled2.ipynb).\r\n\r\nThanks.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@markub3327 I tried with the options you are looking (call(inputs, None, None, True)) and was able to run it without any issue. Only last `model.save` was throwing error as we need to call the model on some data or run `model.predict`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/782843967723cb5309a830970726f153/untitled2.ipynb). Thanks!\r\n\r\nGenerally we don't like to debug long codes. So, in future, please post a code as short as possible to find the root-cause. Thanks!", "Oh sorry, I don't know what part of my model definition has been bad so I post it all, for the future, I will be shorter.\r\n\r\n1. Yes, it's necessary to run call() before saving the model and **use only build() is not enough**!\r\n2. With parameters that I need It's still not working: model.actor(tf.random.normal([1, 24]),training=False, with_log_prob=False, deterministic=True). You tested it on one of the possible configurations like here is described:\r\n```\r\nExpected these arguments to match one of the following 2 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (4 total):\r\n    * TensorSpec(shape=(None, 24), dtype=tf.float32, name='inputs')\r\n    * False\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n\r\nOption 2:\r\n  Positional arguments (4 total):\r\n    * TensorSpec(shape=(None, 24), dtype=tf.float32, name='inputs')\r\n    * True\r\n    * True\r\n    * None\r\n  Keyword arguments: {}\r\n```\r\nIs this the issue:\r\n```\r\nWARNING:absl:Found untraced functions such as fc1_layer_call_fn, fc1_layer_call_and_return_conditional_losses, activation_6_layer_call_fn, activation_6_layer_call_and_return_conditional_losses, layer_normalization_6_layer_call_fn while saving (showing 5 of 140). These functions will not be directly callable after loading.\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\r\n  category=CustomMaskWarning)\r\n```\r\n???\r\n\r\n\r\n3. How can I show the graph of this custom model (`tf.keras.utils.plot_model()`), because I see the model like one node?\r\n![model](https://user-images.githubusercontent.com/74611856/122300008-0624f600-ceff-11eb-8f55-4122c54fd39b.png)\r\n[Code here](https://colab.research.google.com/gist/markub3327/6fd3075359b21e8708dcbcd356874c83/untitled2.ipynb)\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50247\">No</a>\n"]}, {"number": 50245, "title": "[MLIR][DISC] place shape calculation on host", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis PR is about placing shape calculating op on CPU. We use \"mhlo_device_type\" attr to tell if Op is placed on CPU. To achieve this, we add two OP h2d and d2h in mhlo dialect temporarily(These two Ops should be put into a separate dedicated dialect like mhlo-disc dialect).\r\nThere will be 3 PRs to process memory allocation for dynamic shape. 1) Mark shape calculation Ops by Op attribute and insert D2H and H2D Op in mhlo dialect; 2) Add device type info to memref by address space in lmhlo dialect; 3) Convert for device alloc/dealloc with memref's address space.\r\n       \r\nThis PR mainly place shape calculation Ops to Host(cpu), place rest Ops to Device(gpu or cpu) and insert H2D and D2H Op(Host and Device could be cpu).\r\n\r\nRelated discussion is [here](https://llvm.discourse.group/t/placement-representation-in-tensor-world-buffer-world/3502/6).", "comments": ["@azazhu  Can you please check @joker-eph's comments and resolve conflicts?. Thanks!", "@joker-eph, we plan to refactor this PR and split it to two passes: a)  tell the shape calculation OP from data computation OP and insert these info into mhlo OP by attr; b) add placement attr into mhlo OP according to shape calculation or data computation, and then insert H2D and D2H OP.  \r\nWe are refactoring(based on current llvm/MLIR api) DISC internally and make sure that the refactored DISC can e2e run. It will take 3~5 weeks. \r\nFor this PR, I will just keep: 1) add mhlo_disc dialect with H2D&D2H Op and; 2) insert attr for mhlo OP to tell shape calc op and data computation OP. \r\nFor placement and H2D/H2D insert, I will submit another PR.  Is it OK for you? ", "> Flushing comments (not finished reading it all though).\r\n> \r\n> Are you sure that the current test is enough to provide all the coverage?\r\n\r\nAdd one more UT to test Tuple\r\n", "It's best if you don't update this PR anymore right now, I'm integrating it (with modification), it's best if you follow up separately later.", "> It's best if you don't update this PR anymore right now, I'm integrating it (with modification), it's best if you follow up separately later.\r\n\r\nOk", "This merged with some changes, looks at:\r\n\r\n`git diff 9d9ef40ef1c0ea51c381f5b6b434b64e00724a3f..f28661687857a5e2b695a356589a6e0c24695772 tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_mark_shape_calc.cc`\r\n\r\nAnd I left some TODO for you in the test that require some follow-up:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/tests/mhlo_mark_shape_calc_op.mlir#L8\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/tests/mhlo_mark_shape_calc_op.mlir#L41\r\n\r\nI updated the CHECK in the test to be both simpler, but also to actually test what you meant (you missed some CHECK-NOT for example). Have a look that may help for the future!\r\n", "> This merged with some changes, looks at:\r\n> \r\n> `git diff 9d9ef40ef1c0ea51c381f5b6b434b64e00724a3f..f28661687857a5e2b695a356589a6e0c24695772 tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_mark_shape_calc.cc`\r\n> \r\n> And I left some TODO for you in the test that require some follow-up:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/tests/mhlo_mark_shape_calc_op.mlir#L8\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/hlo/tests/mhlo_mark_shape_calc_op.mlir#L41\r\n> \r\n> I updated the CHECK in the test to be both simpler, but also to actually test what you meant (you missed some CHECK-NOT for example). Have a look that may help for the future!\r\n\r\nThanks, I will refine them later."]}]