[{"number": 32647, "title": "TF2.0 Convolution  [10,128,128,9] -> [10,128,128,32]->[10,128,128,3] doesn't work.", "body": "**System information**\r\n\r\nPython version: 3.6.8\r\nTensorFlow-gpu version 2.0.rc0\r\nCUDA/cuDNN version: 10.0\r\nGPU model and memory: GeForce GTX 1080 Ti\r\n**Describe the current behavior**\r\nRunning the provided code on GPUs leads to error message tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'lambda_26_1/Identity:0' shape=(50, 128, 128, 3) dtype=float32>].\r\n\r\nI created **keras** model with multiple keras layers and write  images in summary for tensorboard in function with attribute @tf.function: \r\n```\r\n@tf.function\r\ndef write_to_summary(outputs, targets, writer)\r\n    with writer.as_default():\r\n        tf.summary.image(name=\"losses_aux/log_outputs\",\r\n                         data=outputs)\r\n   ...\r\n   ...\r\n```\r\nand get the error with such stack:\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:/Work/1.0/tf2/train.2.0.py\", line 255, in <module>\r\n    _main()\r\n  File \"D:/Work/1.0/tf2/train.2.0.py\", line 84, in _main\r\n    _train_and_eval(config, train_dataset, eval_dataset, model, model_path)\r\n  File \"D:/Work/1.0/tf2/train.2.0.py\", line 170, in _train_and_eval\r\n    loss_value = loss_fn(outputs, target_batch_train, writer)\r\n  File \"D:\\Work\\1.0\\tf2\\autoencoder_losses2.py\", line 62, in loss_fn\r\n    gamma=losses_config.get_float('hfmae.gamma')),\r\n  File \"D:\\Work\\1.0\\tf2\\loss2.py\", line 47, in high_frequency_mean_abs_error\r\n    outputs, targets, writer, size, sigma, gamma)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 445, in __call__\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 75, in quick_execute\r\n    \"tensors, but found {}\".format(keras_symbolic_tensors))\r\ntensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'lambda_26_1/Identity:0' shape=(50, 128, 128, 3) dtype=float32>]\r\n```\r\n\r\nWhy keras tensor can't be input to eager execution function?!! \r\nPlease, fix this bug, because images are very important for tensorboard.\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Hi ! \r\nwhen i created standalone code (analog our production code), i get such error : \r\n```\r\n2019-09-27 10:40:35.222352: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at conv_ops.cc:1069 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"D:/Work/1.0/tf2/standalone_example.py\", line 44, in <module>\r\n    outputs = model(img)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 851, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 697, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 842, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 851, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\convolutional.py\", line 197, in call\r\n    outputs = self._convolution_op(inputs, self.kernel)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 1134, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 639, in __call__\r\n    return self.call(inp, filter)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 238, in __call__\r\n    name=self.name)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\", line 2010, in conv2d\r\n    name=name)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1031, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\", line 1130, in conv2d_eager_fallback\r\n    ctx=_ctx, name=name)\r\n  File \"C:\\venv_rc2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked! [Op:Conv2D]\r\n```\r\nso i can't get error exactly same in start topic.\r\n\r\nStandalone code for this example: \r\n```\r\nimport tensorflow as tf\r\n\r\nH, W, C = 128, 128, 9\r\nimgs = tf.zeros([10, H, W, C])\r\nds = tf.data.Dataset.from_tensor_slices(imgs).batch(2)\r\nprint(ds)\r\n\r\n\r\ndef construstor(inputs):\r\n    shortcuts = []\r\n    weights_initializer = tf.compat.v1.initializers.truncated_normal(\r\n        mean=0.0,\r\n        stddev=tf.math.sqrt(\r\n            2.0 / ((3 ** 2) * 32))\r\n    )\r\n    activation_fn = tf.keras.layers.LeakyReLU()\r\n\r\n    def _operation_convolutional(_filters):\r\n        return tf.keras.layers.Conv2D(\r\n            filters=_filters,\r\n            strides=[1, 1],\r\n            activation=activation_fn,\r\n            kernel_initializer=weights_initializer,\r\n            kernel_size=[3, 3],\r\n            padding='SAME',\r\n            kernel_regularizer=tf.keras.regularizers.l2(1.0),\r\n            bias_regularizer=tf.keras.regularizers.l2(1.0))\r\n\r\n    outputs = _operation_convolutional(32)(inputs)\r\n    return _operation_convolutional(3)(outputs)\r\n\r\ninput_tensor = tf.keras.Input(shape=[None, None, 3])\r\nmodel = tf.keras.Model(inputs=input_tensor,\r\n                       outputs=construstor(input_tensor))\r\ndef run(img):\r\n    tf.summary.image('img', img)\r\n\r\nwriter = tf.summary.create_file_writer(r\"D:\\tmp\")\r\nwith writer.as_default():\r\n    for i, img in enumerate(ds):\r\n        tf.summary.experimental.set_step(i)\r\n        print('iteration')\r\n        outputs = model(img)\r\n        run(outputs)\r\n\r\n```\r\nhere you can see convolutional for tensor [10,128,128,9] -> [10,128,128,32]->[10,128,128,3] doesn't work.\r\n\r\n\r\n", "I have tried on colab with TF version 2.0.rc0,2.0.0-rc2  and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c1171a6578fe3ab1dc77a2170d4086ca/untitled228.ipynb).Thanks!", "@MaryRodina Sorry for the delay in my response. I think there is a shape mismatch between input_data and the model_Expectation. Please check the error trace I am getting with `TF2.0`.\r\n\r\n```\r\n<BatchDataset shapes: (None, 128, 128, 9), types: tf.float32>\r\niteration\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-8-a1cee438968d> in <module>()\r\n     41         tf.summary.experimental.set_step(i)\r\n     42         print('iteration')\r\n---> 43         outputs = model(img)\r\n     44         print(outputs)\r\n     45         run(outputs)\r\n\r\n12 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnimplementedError: The Conv2D op currently does not support grouped convolutions on the CPU. A grouped convolution was attempted to be run because the input depth of 9 does not match the filter input depth of 3 [Op:Conv2D]\r\n```\r\n\r\nWhen I changed input channels from `9` to `3`, everything worked without any issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/d5558fac1ff57e44ef36ab1d9288f2bf/untitled228.ipynb) for your reference. Thanks!\r\n\r\nPlease close the issue if it was already resolved. Thanks!\r\n", "I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32647\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32647\">No</a>\n", "Usually those errors come when a change on channels happen in two different model calls. For example calling the model on (?, 28, 28, 1) followed by (?, 28, 28, 3). MNIST grayscale to MNIST colored."]}, {"number": 32646, "title": "Tried to convert 'y' to a tensor and failed. Error: None values not supported.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0rc1\r\n\r\n**Describe the current behavior**\r\n\r\nCrash when fit. It works just fine in `1.14.0`, `2.0.0b0` & `2.0.0b1`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNot to crash when fit.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.optimizers import Adam\r\n\r\nmodel = Sequential([\r\n    Dense(24, input_dim=4, activation='relu'),\r\n    Dense(24, activation='relu'),\r\n    Dense(2, activation='linear')\r\n])\r\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001))\r\n\r\nx = np.array([[-0.08623559, -0.79897248,  0.03606475,  1.09068178]])\r\ny = np.array([[ 1.0449973,  -0.14471795]])\r\nmodel.fit(x, y)\r\n```\r\n\r\nHere is the [colab link](https://colab.research.google.com/drive/1HevwenWthBQAnRRj8z7sJzBjuZV9F_JZ)\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    526                 as_ref=input_arg.is_ref,\r\n--> 527                 preferred_dtype=default_dtype)\r\n    528           except TypeError as err:\r\n\r\n22 frames\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    543               raise ValueError(\r\n    544                   \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\r\n--> 545                   (input_name, err))\r\n    546             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\r\n    547                       (input_name, op_type_name, observed))\r\n\r\nValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.\r\n```\r\n", "comments": ["It works fine with TF 1.15.0.rc0.\r\nI could reproduce the issue with Tensorflow 2.0.0.rc1. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/d6a907491917bcc4e36f7c8e08fefe54/untitled160.ipynb). Thanks!  ", "This also happens in 2.0.0.rc0.\r\nA \"workaround\" is not using the class for the optimizer but using the name as string, of course, then you can't adjust the learning rate.\r\n\r\nInstead of:\r\n`model.compile(optimizer=keras.optimizers.SGD(), loss='mse')`\r\nyou would write:\r\n`model.compile(optimizer='sgd', loss='mse')`", "Changing your imports can help resolve this issue. Tested in ```'2.0.0-rc1'```\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nmodel = Sequential([\r\n    Dense(24, input_dim=4, activation='relu'),\r\n    Dense(24, activation='relu'),\r\n    Dense(2, activation='linear')\r\n])\r\nmodel.compile(loss='mse', optimizer=Adam(lr=0.001))\r\n\r\nx = np.array([[-0.08623559, -0.79897248,  0.03606475,  1.09068178]])\r\ny = np.array([[ 1.0449973,  -0.14471795]])\r\nmodel.fit(x, y)\r\n```\r\noutput:\r\n```python\r\nTrain on 1 samples\r\n1/1 [==============================] - 0s 239ms/sample - loss: 0.5793\r\n<tensorflow.python.keras.callbacks.History at 0x7fa54c4f6630>\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32646\">No</a>\n", "I have this problem ,too. my code works well in 2.0b1, not work in tf 2.0rc1", "> Changing your imports can help resolve this issue. Tested in `'2.0.0-rc1'`\r\n\r\n\r\nwhat is the difference between `tensorflow.python.keras` and `tensorflow.keras`? Why this trick works?\r\n\r\n\r\n", "`from tensorflow.keras.optimizers import Adam`\r\nin place of \r\n`from tensorflow.python.keras.optimizers import Adam`\r\nhelps", "> `from tensorflow.keras.optimizers import Adam`\r\n> in place of\r\n> `from tensorflow.python.keras.optimizers import Adam`\r\n> helps\r\n\r\nIt works for me", "Worked for me!\r\n\r\n> `from tensorflow.keras.optimizers import Adam`\r\n> in place of\r\n> `from tensorflow.python.keras.optimizers import Adam`\r\n> helps\r\n\r\n"]}, {"number": 32645, "title": "import source code in Clion failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:r1.15\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):0.24.1\r\n- GCC/Compiler version (if compiling from source):7\r\n- CUDA/cuDNN version:10.4\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI want to try to import the tensorflow source using clion's bazel plugin, and tried many versions, but none of them succeeded. Has anyone succeeded? Please give me advice.\r\n\r\n![image](https://user-images.githubusercontent.com/33537489/65225648-1b85aa00-daf8-11e9-863d-954f0f248f78.png)\r\n\r\n", "comments": ["This isn't something that we support on GitHub, sorry. Try asking a question on Stack Overflow.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32645\">No</a>\n"]}, {"number": 32644, "title": "TF 2.0 Cherrypick for TensorBoard 2.0 dependency", "body": "Update tensorboard dependency to 2.0.x\r\n\r\nTensorBoard release: https://pypi.org/project/tensorboard/2.0.0/", "comments": []}, {"number": 32643, "title": "fix typo and fix comment to FoldConjugateIntoTranspose", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32643) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32643) for more info**.\n\n<!-- ok -->", "@Agoniii Could you please check reviewer comments and keep us posted. Thanks!", "@Agoniii Could you please check failed build errors? Thanks!", "@gbaned I only fixed typo in comment.  These build errors are not related to the code change in this PR. Thanks.", "@Agoniii Could you please address Ubuntu Sanity errors? Thanks!", "@gbaned One error in Ubuntu Sanity is `ERROR: /tmpfs/src/github/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:659:12: in deps attribute of cc_library rule //tensorflow/compiler/xla/service/cpu:runtime_single_threaded_fft: target '//tensorflow/compiler/xla:xla_data_cc_proto' does not exist` \r\nbut in [BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/cpu/BUILD#L659) , it is `\"//tensorflow/compiler/xla:xla_data_proto\"`. The codes are different.\r\nThe other error is similar."]}, {"number": 32642, "title": "Unable to convert .ckpt to .pb using tensorflow", "body": "I have been trying to use BERT for question answer prediction but the response time of it quite high.\r\n\r\nThe model size is too heavy and is in .ckpt format.\r\n\r\nI want to save it to .pb format but I am struglling with the output_node_name encountered with below error:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'InfeedEnqueueTuple' used by node input_pipeline_task0/while/InfeedQueue/enqueue/0 (defined at Tensorflowckpt2pb.py:98) with these attrs: [shapes=[[1], [1,384], [1,384], [1,384], [1], [1]], device_ordinal=0, layouts=[], dtypes=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _class=[\"loc:@input_pipeline_task0/while/IteratorGetNext\"]]\r\nRegistered devices: [CPU, XLA_CPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n         [[input_pipeline_task0/while/InfeedQueue/enqueue/0]]\r\n\r\nThe script code is as below:\r\n\r\n# Copyright 2015 Google Inc. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Converts checkpoint variables into Const ops in a standalone GraphDef file.\r\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\r\nvariable values stored in a checkpoint file, and output a GraphDef with all of\r\nthe variable ops converted into const ops containing the values of the\r\nvariables.\r\nIt's useful to do this when we need to load a single file in C++, especially in\r\nenvironments like mobile or embedded where we may not have access to the\r\nRestoreTensor ops and file loading calls that they rely on.\r\nAn example of command-line usage is:\r\nbazel build tensorflow/python/tools:freeze_graph && \\\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=some_graph_def.pb \\\r\n--input_checkpoint=model.ckpt-8361242 \\\r\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\nYou can also look at freeze_graph_test.py for an example of how to use it.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nfrom google.protobuf import text_format\r\n#from tensorflow.python.client import graph_util\r\nfrom tensorflow.python.framework import graph_util\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ntf.app.flags.DEFINE_string(\"input_graph\", \"\",\r\n                           \"\"\"TensorFlow 'GraphDef' file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"input_saver\", \"\",\r\n                           \"\"\"TensorFlow saver file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"input_checkpoint\", \"\",\r\n                           \"\"\"TensorFlow variables file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"output_graph\", \"\",\r\n                           \"\"\"Output 'GraphDef' file name.\"\"\")\r\ntf.app.flags.DEFINE_boolean(\"input_binary\", False,\r\n                            \"\"\"Whether the input files are in binary format.\"\"\")\r\ntf.app.flags.DEFINE_string(\"output_node_names\", \"\",\r\n                           \"\"\"The name of the output nodes, comma separated.\"\"\")\r\ntf.app.flags.DEFINE_string(\"restore_op_name\", \"save/restore_all\",\r\n                           \"\"\"The name of the master restore operator.\"\"\")\r\ntf.app.flags.DEFINE_string(\"filename_tensor_name\", \"save/Const:0\",\r\n                           \"\"\"The name of the tensor holding the save path.\"\"\")\r\ntf.app.flags.DEFINE_boolean(\"clear_devices\", True,\r\n                            \"\"\"Whether to remove device specifications.\"\"\")\r\ntf.app.flags.DEFINE_string(\"initializer_nodes\", \"\", \"comma separated list of \"\r\n                           \"initializer nodes to run before freezing.\")\r\n\r\n\r\ndef freeze_graph(input_graph, input_saver, input_binary, input_checkpoint,\r\n                 output_node_names, restore_op_name, filename_tensor_name,\r\n                 output_graph, clear_devices, initializer_nodes):\r\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\"\r\n\r\n  if not tf.gfile.Exists(input_graph):\r\n    print(\"Input graph file '\" + input_graph + \"' does not exist!\")\r\n    return -1\r\n\r\n  if input_saver and not tf.gfile.Exists(input_saver):\r\n    print(\"Input saver file '\" + input_saver + \"' does not exist!\")\r\n    return -1\r\n\r\n  if not tf.gfile.Glob(input_checkpoint):\r\n    print(\"Input checkpoint '\" + input_checkpoint + \"' doesn't exist!\")\r\n    return -1\r\n\r\n  if not output_node_names:\r\n    print(\"You need to supply the name of a node to --output_node_names.\")\r\n    return -1\r\n\r\n  input_graph_def = tf.GraphDef()\r\n  mode = \"rb\" if input_binary else \"r\"\r\n  with tf.gfile.FastGFile(input_graph, mode) as f:\r\n    if input_binary:\r\n      input_graph_def.ParseFromString(f.read())\r\n    else:\r\n      text_format.Merge(f.read(), input_graph_def)\r\n  # Remove all the explicit device specifications for this node. This helps to\r\n  # make the graph more portable.\r\n  if clear_devices:\r\n    for node in input_graph_def.node:\r\n      node.device = \"\"\r\n  _ = tf.import_graph_def(input_graph_def, name=\"\")\r\n\r\n  with tf.Session() as sess:\r\n    if input_saver:\r\n      with tf.gfile.FastGFile(input_saver, mode) as f:\r\n        saver_def = tf.train.SaverDef()\r\n        if input_binary:\r\n          saver_def.ParseFromString(f.read())\r\n        else:\r\n          text_format.Merge(f.read(), saver_def)\r\n        saver = tf.train.Saver(saver_def=saver_def)\r\n        saver.restore(sess, input_checkpoint)\r\n    else:\r\n      sess.run([restore_op_name], {filename_tensor_name: input_checkpoint})\r\n      if initializer_nodes:\r\n        sess.run(initializer_nodes)\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, input_graph_def, output_node_names.split(\",\"))\r\n\r\n  with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\n  print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n\r\ndef main(unused_args):\r\n  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,\r\n               FLAGS.input_checkpoint, FLAGS.output_node_names,\r\n               FLAGS.restore_op_name, FLAGS.filename_tensor_name,\r\n               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()\r\n\r\n", "comments": ["@tusharsh23,\r\nPlease provide the information asked in the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). It will indeed help us debug the root cause. Thanks!", "I have provided all the details in the description it self. Do you need any specific information?", "@tusharsh23, Please provide details about TensorFlow version. Also, did you compile from source or install a binary?\r\nIn order to expedite the trouble-shooting process, please provide the proper intended code snippet to reproduce the issue reported here. Thanks!\r\n", "tensorflow-1.14.0\r\nBelow is the entire script.\r\n\r\n` Copyright 2015 Google Inc. All Rights Reserved.\r\n\r\n Licensed under the Apache License, Version 2.0 (the \"License\");\r\n you may not use this file except in compliance with the License.\r\n You may obtain a copy of the License at\r\n\r\n     http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n>  Unless required by applicable law or agreed to in writing, software\r\n> distributed under the License is distributed on an \"AS IS\" BASIS,\r\n>  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n>  See the License for the specific language governing permissions and\r\n>  limitations under the License.\r\n\r\n====================================================================\r\n\"\"\"Converts checkpoint variables into Const ops in a standalone GraphDef file.\r\nThis script is designed to take a GraphDef proto, a SaverDef proto, and a set of\r\nvariable values stored in a checkpoint file, and output a GraphDef with all of\r\nthe variable ops converted into const ops containing the values of the\r\nvariables.\r\nIt's useful to do this when we need to load a single file in C++, especially in\r\nenvironments like mobile or embedded where we may not have access to the\r\nRestoreTensor ops and file loading calls that they rely on.\r\nAn example of command-line usage is:\r\nbazel build tensorflow/python/tools:freeze_graph && \\\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n--input_graph=some_graph_def.pb \\\r\n--input_checkpoint=model.ckpt-8361242 \\\r\n--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax\r\nYou can also look at freeze_graph_test.py for an example of how to use it.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\nfrom google.protobuf import text_format\r\n#from tensorflow.python.client import graph_util\r\nfrom tensorflow.python.framework import graph_util\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ntf.app.flags.DEFINE_string(\"input_graph\", \"\",\r\n                           \"\"\"TensorFlow 'GraphDef' file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"input_saver\", \"\",\r\n                           \"\"\"TensorFlow saver file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"input_checkpoint\", \"\",\r\n                           \"\"\"TensorFlow variables file to load.\"\"\")\r\ntf.app.flags.DEFINE_string(\"output_graph\", \"\",\r\n                           \"\"\"Output 'GraphDef' file name.\"\"\")\r\ntf.app.flags.DEFINE_boolean(\"input_binary\", False,\r\n                            \"\"\"Whether the input files are in binary format.\"\"\")\r\ntf.app.flags.DEFINE_string(\"output_node_names\", \"\",\r\n                           \"\"\"The name of the output nodes, comma separated.\"\"\")\r\ntf.app.flags.DEFINE_string(\"restore_op_name\", \"save/restore_all\",\r\n                           \"\"\"The name of the master restore operator.\"\"\")\r\ntf.app.flags.DEFINE_string(\"filename_tensor_name\", \"save/Const:0\",\r\n                           \"\"\"The name of the tensor holding the save path.\"\"\")\r\ntf.app.flags.DEFINE_boolean(\"clear_devices\", True,\r\n                            \"\"\"Whether to remove device specifications.\"\"\")\r\ntf.app.flags.DEFINE_string(\"initializer_nodes\", \"\", \"comma separated list of \"\r\n                           \"initializer nodes to run before freezing.\")\r\n\r\n\r\ndef freeze_graph(input_graph, input_saver, input_binary, input_checkpoint,\r\n                 output_node_names, restore_op_name, filename_tensor_name,\r\n                 output_graph, clear_devices, initializer_nodes):\r\n  \"\"\"Converts all variables in a graph and checkpoint into constants.\"\"\"\r\n\r\n  if not tf.gfile.Exists(input_graph):\r\n    print(\"Input graph file '\" + input_graph + \"' does not exist!\")\r\n    return -1\r\n\r\n  if input_saver and not tf.gfile.Exists(input_saver):\r\n    print(\"Input saver file '\" + input_saver + \"' does not exist!\")\r\n    return -1\r\n\r\n  if not tf.gfile.Glob(input_checkpoint):\r\n    print(\"Input checkpoint '\" + input_checkpoint + \"' doesn't exist!\")\r\n    return -1\r\n\r\n  if not output_node_names:\r\n    print(\"You need to supply the name of a node to --output_node_names.\")\r\n    return -1\r\n\r\n  input_graph_def = tf.GraphDef()\r\n  mode = \"rb\" if input_binary else \"r\"\r\n  with tf.gfile.FastGFile(input_graph, mode) as f:\r\n    if input_binary:\r\n      input_graph_def.ParseFromString(f.read())\r\n    else:\r\n      text_format.Merge(f.read(), input_graph_def)\r\n  # Remove all the explicit device specifications for this node. This helps to\r\n  # make the graph more portable.\r\n  if clear_devices:\r\n    for node in input_graph_def.node:\r\n      node.device = \"\"\r\n  _ = tf.import_graph_def(input_graph_def, name=\"\")\r\n\r\n  with tf.Session() as sess:\r\n    if input_saver:\r\n      with tf.gfile.FastGFile(input_saver, mode) as f:\r\n        saver_def = tf.train.SaverDef()\r\n        if input_binary:\r\n          saver_def.ParseFromString(f.read())\r\n        else:\r\n          text_format.Merge(f.read(), saver_def)\r\n        saver = tf.train.Saver(saver_def=saver_def)\r\n        saver.restore(sess, input_checkpoint)\r\n    else:\r\n      sess.run([restore_op_name], {filename_tensor_name: input_checkpoint})\r\n      if initializer_nodes:\r\n        sess.run(initializer_nodes)\r\n    output_graph_def = graph_util.convert_variables_to_constants(\r\n        sess, input_graph_def, output_node_names.split(\",\"))\r\n  \r\n  out_node_name = []\r\n  for i in range(len(output_graph_def.outputs)):\r\n      output_name = net_model.outputs[i].name.split(':')[0]\r\n      tf.identity(net_model.outputs[i],output_name)\r\n      out_node_name.append(output_name)\r\n      print(out_node_name)\r\n\r\n  with tf.gfile.GFile(output_graph, \"wb\") as f:\r\n    f.write(output_graph_def.SerializeToString())\r\n  print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\n\r\ndef main(unused_args):\r\n  freeze_graph(FLAGS.input_graph, FLAGS.input_saver, FLAGS.input_binary,\r\n               FLAGS.input_checkpoint, FLAGS.output_node_names,\r\n               FLAGS.restore_op_name, FLAGS.filename_tensor_name,\r\n               FLAGS.output_graph, FLAGS.clear_devices, FLAGS.initializer_nodes)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.app.run()`", "I compile it from source.", "@tusharsh23 Can you provide a simple standalone code to reproduce the issue? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32641, "title": "Tensorflow Memory Error", "body": "Dear Team,\r\n\r\ni have 8 GB RAM in my system is it possible to train it in divided rule so that it would take less RAM to train it to large data set with less RAM i mean 8GB with 1GPU .i am going to train open_images_v4  data set ", "comments": ["@DeveloperRachit ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Thanks!", "I am using tensorflow version 14.0 and GPU 8 GB RAM Linux 18.0 version ", "@DeveloperRachit ,\r\nCan you share a simple and standalone code to reproduce the issue?Thanks!\r\n", "Train.py\r\n====================================================================\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom scipy.io import loadmat\r\nfrom urllib.request import urlretrieve\r\nfrom os.path import isfile, isdir\r\nfrom tqdm import tqdm\r\nimport os\r\nimport cv2\r\n\r\n\r\nfrom model import GAN\r\nfrom dataset import Dataset,view_samples\r\n\r\ndata_dir = '../train_00/'\r\nreal_size = (32,32,3)\r\nz_size = 100\r\nlearning_rate = 0.0002\r\nbatch_size = 128\r\nepochs = 1000\r\nalpha = 0.2\r\nbeta1 = 0.5\r\n\r\nif not isdir(data_dir):\r\n    raise Exception(\"Data directory doesn't exist!\")\r\n\r\nimages=np.array([cv2.resize((cv2.imread(\"../train_00/\"+tmp, cv2.IMREAD_COLOR)),(32, 32))\r\n                                               for tmp in os.listdir(\"../train_00/\")])\r\ntrainset = images[:3000]\r\ntestset = images[3000:]\r\n\r\n\r\nidx = np.random.randint(0, len(trainset), size=36)\r\nfig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5),)\r\nfor ii, ax in zip(idx, axes.flatten()):\r\n ax.imshow(trainset[ii], aspect='equal')\r\n    ax.xaxis.set_visible(False)\r\n    ax.yaxis.set_visible(False)\r\n\r\nplt.subplots_adjust(wspace=0, hspace=0)\r\nplt.show()\r\n\r\n\r\ndef train(net, dataset, epochs, batch_size, print_every=10, show_every=100, figsize=(5,5)):\r\n    saver = tf.train.Saver()\r\n    sample_z = np.random.uniform(-1, 1, size=(72, z_size))\r\n\r\n    samples, losses = [], []\r\n    steps = 0\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        for e in range(epochs):\r\n            for x in dataset.batches(batch_size):\r\n                steps += 1\r\n\r\n                # Sample random noise for G\r\n                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\r\n\r\n                # Run optimizers\r\n                _ = sess.run(net.d_opt, feed_dict={net.input_real: x, net.input_z: batch_z})\r\n                _ = sess.run(net.g_opt, feed_dict={net.input_z: batch_z, net.input_real: x})\r\n\r\n                if steps % print_every == 0:\r\n                    # At the end of each epoch, get the losses and print them out\r\n  train_loss_d = net.d_loss.eval({net.input_z: batch_z, net.input_real: x})\r\n                    train_loss_g = net.g_loss.eval({net.input_z: batch_z})\r\n\r\n                    print(\"Epoch {}/{}...\".format(e+1, epochs),\r\n                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\r\n                          \"Generator Loss: {:.4f}\".format(train_loss_g))\r\n                    # Save losses to view after training\r\n                    losses.append((train_loss_d, train_loss_g))\r\n\r\n                if steps % show_every == 0:\r\n                    gen_samples = sess.run(\r\n                                   generator(net.input_z, 3, reuse=True, training=False),\r\n                                   feed_dict={net.input_z: sample_z})\r\n                    samples.append(gen_samples)\r\n                    _ = view_samples(-1, samples, 6, 12, figsize=figsize)\r\n                    plt.show()\r\n\r\n        saver.save(sess, './checkpoints/generator.ckpt')\r\n\r\n    with open('samples.pkl', 'wb') as f:\r\n        pkl.dump(samples, f)\r\n\r\n    return losses, samples\r\nnet = GAN(real_size, z_size, learning_rate, alpha=alpha, beta1=beta1)\r\n\r\ndataset = Dataset(trainset, testset)\r\n\r\nlosses, samples = train(net, dataset, epochs, batch_size, figsize=(10,5))\r\n\r\n_ = view_samples(0, samples, 4, 4, figsize=(10,5))\r\n\r\n", "datasets.py\r\n========================\r\nimport numpy as np\r\n\r\n\r\ndef view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\r\n    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols,\r\n                             sharey=True, sharex=True)\r\n    for ax, img in zip(axes.flatten(), samples[epoch]):\r\n        ax.axis('off')\r\n        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\r\n        ax.set_adjustable('box-forced')\r\n        im = ax.imshow(img, aspect='equal')\r\n\r\n    plt.subplots_adjust(wspace=0, hspace=0)\r\n    return fig, axes\r\n\r\n\r\ndef scale(x, feature_range=(-1, 1)):\r\n    # scale to (0, 1)\r\n    x = ((x - x.min())/(255 - x.min()))\r\n\r\n    # scale to feature_range\r\n    min, max = feature_range\r\n    x = x * (max - min) + min\r\n    return x\r\n\r\n\r\n\r\nclass Dataset:\r\n    def __init__(self, train, test, val_frac=0.5, shuffle=False, scale_func=None):\r\n        split_idx = int(len(test)*(1 - val_frac))\r\n        self.test_x, self.valid_x = test[:split_idx], test[split_idx:]\r\n\r\n        self.train_x = train\r\n\r\n        #self.train_x = np.rollaxis(self.train_x, 3)\r\n                                                                                                                            1,13          Top\r\n        if scale_func is None:\r\n            self.scaler = scale\r\n        else:\r\n            self.scaler = scale_func\r\n        self.shuffle = shuffle\r\n\r\n    def batches(self, batch_size):\r\n        if self.shuffle:\r\n            idx = np.arange(len(self.train_x))\r\n            np.random.shuffle(idx)\r\n            self.train_x = self.train_x[idx]\r\n\r\n\r\n        n_batches = len(self.train_x)//batch_size\r\n        for ii in range(0, len(self.train_x), batch_size):\r\n            x = self.train_x[ii:ii+batch_size]\r\n\r\n\r\n            yield self.scaler(x)\r\n", "model.py\r\n==================================================\r\nimport tensorflow as tf\r\n\r\n\r\ndef model_inputs(real_dim, z_dim):\r\n    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\r\n    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\r\n\r\n    return inputs_real, inputs_z\r\n\r\n\r\ndef generator(z, output_dim, reuse=False, alpha=0.2, training=True):\r\n    with tf.variable_scope('generator', reuse=reuse):\r\n        # First fully connected layer\r\n        x1 = tf.layers.dense(z, 4*4*512)\r\n        # Reshape it to start the convolutional stack\r\n        x1 = tf.reshape(x1, (-1, 4, 4, 512))\r\n        x1 = tf.layers.batch_normalization(x1, training=training)\r\n        x1 = tf.maximum(alpha * x1, x1)\r\n        # 4x4x512 now\r\n\r\n        x2 = tf.layers.conv2d_transpose(x1, 256, 5, strides=2, padding='same')\r\n        x2 = tf.layers.batch_normalization(x2, training=training)\r\n        x2 = tf.maximum(alpha * x2, x2)\r\n        # 8x8x256 now\r\n\r\n        x3 = tf.layers.conv2d_transpose(x2, 128, 5, strides=2, padding='same')\r\n        x3 = tf.layers.batch_normalization(x3, training=training)\r\n        x3 = tf.maximum(alpha * x3, x3)\r\n        # 16x16x128 now\r\n\r\n        # Output layer\r\n        logits = tf.layers.conv2d_transpose(x3, output_dim, 5, strides=2, padding='same')\r\n        # 32x32x3 now\r\n\r\n        out = tf.tanh(logits)\r\n                                                                                                                            1,1           Top\r\n\r\n        return out\r\n\r\ndef discriminator(x, reuse=False, alpha=0.2):\r\n    with tf.variable_scope('discriminator', reuse=reuse):\r\n        # Input layer is 32x32x3\r\n        x1 = tf.layers.conv2d(x, 64, 5, strides=2, padding='same')\r\n        relu1 = tf.maximum(alpha * x1, x1)\r\n        # 16x16x64\r\n\r\n        x2 = tf.layers.conv2d(relu1, 128, 5, strides=2, padding='same')\r\n        bn2 = tf.layers.batch_normalization(x2, training=True)\r\n        relu2 = tf.maximum(alpha * bn2, bn2)\r\n        # 8x8x128\r\n\r\n        x3 = tf.layers.conv2d(relu2, 256, 5, strides=2, padding='same')\r\n        bn3 = tf.layers.batch_normalization(x3, training=True)\r\n        relu3 = tf.maximum(alpha * bn3, bn3)\r\n        # 4x4x256\r\n\r\n        # Flatten it\r\n        flat = tf.reshape(relu3, (-1, 4*4*256))\r\n        logits = tf.layers.dense(flat, 1)\r\n        out = tf.sigmoid(logits)\r\n\r\n        return out, logits\r\n\r\ndef model_loss(input_real, input_z, output_dim, alpha=0.2):\r\n    \"\"\"\r\n    Get the loss for the discriminator and generator\r\n    :param input_real: Images from the real dataset\r\n    :param input_z: Z input\r\n    :param out_channel_dim: The number of channels in the output image\r\n    :return: A tuple of (discriminator loss, generator loss)\r\n    g_model = generator(input_z, output_dim, alpha=alpha)\r\n    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\r\n    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, alpha=alpha)\r\n\r\n    d_loss_real = tf.reduce_mean(\r\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real)))\r\n    d_loss_fake = tf.reduce_mean(\r\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\r\n    g_loss = tf.reduce_mean(\r\n        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\r\n\r\n    d_loss = d_loss_real + d_loss_fake\r\n\r\n    return d_loss, g_loss\r\n\r\ndef model_opt(d_loss, g_loss, learning_rate, beta1):\r\n    \"\"\"\r\n    Get optimization operations\r\n    :param d_loss: Discriminator loss Tensor\r\n    :param g_loss: Generator loss Tensor\r\n    :param learning_rate: Learning Rate Placeholder\r\n    :param beta1: The exponential decay rate for the 1st moment in the optimizer\r\n    :return: A tuple of (discriminator training operation, generator training operation)\r\n    \"\"\"\r\n    # Get weights and bias to update\r\n    t_vars = tf.trainable_variables()\r\n    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\r\n    g_vars = [var for var in t_vars if var.name.startswith('generator')]\r\n\r\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\r\n        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\r\n\r\n    return d_train_opt, g_train_opt\r\n\r\n\r\nclass GAN:\r\n    def __init__(self, real_size, z_size, learning_rate, alpha=0.2, beta1=0.5):\r\n        tf.reset_default_graph()\r\n\r\n        self.input_real, self.input_z = model_inputs(real_size, z_size)\r\n\r\n        self.d_loss, self.g_loss = model_loss(self.input_real, self.input_z,\r\n                                              real_size[2], alpha=alpha)\r\n\r\n        self.d_opt, self.g_opt = model_opt(self.d_loss, self.g_loss, learning_rate, beta1)\r\n\r\n                                                                                                                            118,0-1       Bot\r\n", "look into this code ", "Dear Team,,\r\n\r\ni have CPU with this configuration Intel(R) Xeon(R) CPU           X5650  @ 2.67GHz after running code wit Tf this going to give me error(core dumped ) is it mean not supportinh cpu either having any issue.  and GPU configuration is GeForce GTX 1050 with 8GB Ram", "@DeveloperRachit ,\r\nThank you for the info, could you please try sending a simple and standalone code to replicate the issue reported here?thanks.", "i already share code but still i am not getting any answer regarding this", "Please prefix code lines with triple quotes, per the [markdown syntax](https://guides.github.com/features/mastering-markdown/) so that it is easier to read.", "tell me", "what is the reason tell me ", "@DeveloperRachit ,\r\n\r\nLooks like the indentation is wrong in the code given by you\r\n\r\n> Please prefix code lines with triple quotes, per the [markdown syntax](https://guides.github.com/features/mastering-markdown/) so that it is easier to read.\r\n\r\nPlease follow the instructions provided in the link and provide us the proper standalone code so that we can try replicating it from our end and help you fixing the issue faced.\r\n", "Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32640, "title": "How to build tf1.14 then build tf2.0?", "body": "I try to build 2 different pip package with 2 versions. First:\r\n\r\n```\r\nbazel build --config=opt  //tensorflow/tools/pip_package:build_pip_package\r\n# this generates 1.14 by default\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n# then I try build v2\r\nbazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n```\r\n\r\nBut anyway it generates only 1.14, not 2.0 in the next step......", "comments": ["1.14 comes from the `r1.14` branch, 2.0 from the `r2.0` one so you need to switch branches using `git checkout`.\r\n\r\n`--config=v2` just enables 2.0 behavior but the version number stays the same", "@jinfagang, Did you try the @mihaimaruseac's solution and also please follow the instructions mentioned [here](https://www.tensorflow.org/install/source#download_the_tensorflow_source_code). Thanks!\r\n", "@jinfagang, Is this still an issue? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32640\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32640\">No</a>\n"]}, {"number": 32639, "title": "slightly optimize tflite Concatenate operation", "body": "Current implementation of tflite Concatenation calculates copy_size by accessing member varibales in innermost loop. It is redundant and causes performance loss.\r\n\r\nThis pull request reduces redundant member accesses, and get 20% performance gain.\r\n\r\nThe method I use for performance estimation is explained below.\r\nThis python script outputs example tflite model:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import backend as K\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import *\r\n\r\n# construct model\r\na = Input(shape=(300, 300, 8))\r\nb = Input(shape=(300, 300, 8))\r\ny = Concatenate()([a, b])\r\nmodel = Model(inputs=[a, b], outputs=[y])\r\n\r\n# model path\r\nkeras_path = \"model.h5\"\r\ntflite_path = \"model.tflite\"\r\n\r\n# keras -> keras h5\r\nkeras.models.save_model(model, keras_path, include_optimizer=False)\r\n\r\n# keras h5 -> tflite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_path)\r\ntflite_model = converter.convert()\r\nopen(tflite_path, \"wb\").write(tflite_model)\r\n```\r\n\r\nHere is output of benchmark_model with current implementation:\r\n\r\n```\r\n$./benchmark_model --graph=model.tflite --num_runs=10000\r\nSTARTING!\r\nMin num runs: [10000]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [model.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse gpu : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nLoaded model model.tflite\r\nresolved reporter\r\nInitialized session in 0.145ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=516 first=2018 curr=542 min=541 max=2018 avg=598.953 std=100\r\n\r\nRunning benchmark for at least 10000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=10000 first=608 curr=563 min=539 max=2371 avg=612.409 std=83\r\n\r\nAverage inference timings in us: Warmup: 598.953, Init: 145, no stats: 612.409\r\n```\r\n\r\nand with optimized implementation:\r\n```\r\n$./benchmark_model --graph=model.tflite --num_runs=10000\r\nSTARTING!\r\nMin num runs: [10000]\r\nMin runs duration (seconds): [1]\r\nMax runs duration (seconds): [150]\r\nInter-run delay (seconds): [-1]\r\nNum threads: [1]\r\nBenchmark name: []\r\nOutput prefix: []\r\nMin warmup runs: [1]\r\nMin warmup runs duration (seconds): [0.5]\r\nGraph: [model.tflite]\r\nInput layers: []\r\nInput shapes: []\r\nUse gpu : [0]\r\nAllow fp16 : [0]\r\nRequire full delegation : [0]\r\nEnable op profiling: [0]\r\nMax profiling buffer entries: [1024]\r\nLoaded model model.tflite\r\nresolved reporter\r\nInitialized session in 0.143ms\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=543 first=1852 curr=457 min=450 max=1852 avg=541.483 std=91\r\n\r\nRunning benchmark for at least 10000 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=10000 first=462 curr=458 min=447 max=2558 avg=481.11 std=87\r\n\r\nAverage inference timings in us: Warmup: 541.483, Init: 143, no stats: 481.11\r\n```\r\n", "comments": ["> Hi, thanks for this change! It looks good. If you want, there appears to be a similar issue with ConcatenationWithScaling, so there might be a similar gain there.\r\n\r\n@talumbau \r\nI agree, but I don't understand the way for testing and checking performance.\r\nIs there a simple example for this?"]}, {"number": 32638, "title": "load custom op in C API got error", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\nI have added my op by following [https://www.tensorflow.org/guide/extend/op](url)\r\nand I can load it in python by `tf.load_op_library(\"./myop.so\")`, and then I freeze the graph and saved to `graph.pb`. Now I want to load the graph.pb in C API,and do inference, by the following codes to load the op \r\n`TF_Library* lib_handle = TF_LoadLibrary(\"/home/yx.wang/hello_tf/myop.so\", status);\r\nif(TF_GetCode(status) != TF_OK)\r\n    {\r\n        printf(\"Can't encode string: %s\\n\", TF_Message(status));\r\n        return NULL;\r\n    }\r\n`\r\nBut I got an error `2019-09-19 10:39:14.505244: F tensorflow/core/framework/variant_op_registry.cc:102] Check failed: existing == nullptr (0x1e80700 vs. nullptr)Unary VariantDecodeFn for type_name: tensorflow::Tensor already registered`\r\nI donot know what's wrong, anyone can help me to ckeck the question?", "comments": ["@wangyunxiaa \r\n\r\nCan you please go through the [link](https://github.com/tensorflow/custom-op) and see if it helps you. Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@wangyunxiaa \r\n\r\nCan you please let us know if the issue still persists.Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 32637, "title": "No such file or directory in bazel build", "body": "- OS Platform and Distribution ( Linux Ubuntu 18.04, python3.5):\r\n- TensorFlow installed from (tensorflow-master,that is, I download the https://github.com/tensorflow/tensorflow)\r\n- TensorFlow version (source,I don't know, this is the link https://github.com/tensorflow/tensorflow):\r\n\r\n`tensorflow-master$ bazel build tensorflow/lite/toco:toco`\r\n`Starting local Bazel server and connecting to it...\r\nFATAL: Cannot get start time of process 0: (error: 2): No such file or directory`\r\n\r\nSo what's wrong ??\r\nthx", "comments": ["in another platform and same OS,\r\nsame `tensorflow-master$ bazel build tensorflow/lite/toco:toco`\r\nbut ups\r\n`INFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=107\r\nINFO: Reading rc options for 'build' from /./tensorflow-master/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/bazel-toolchains/archive/92dd8a7a518a2fb7ba992d47c8b38299fe0be825.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - /home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /./tensorflow-master/WORKSPACE:35:1\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nTraceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\nINFO: Call stack for the definition of repository 'local_config_git' which is a git_configure (rule definition at /data/rnnoise/src/data/mss/pb2tflite/tensorflow-master/third_party/git/git_configure.bzl:63:17):\r\n - /./tensorflow-master/tensorflow/workspace.bzl:74:5\r\n - /./tensorflow-master/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_git':\r\n   Traceback (most recent call last):\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Call stack for the definition of repository 'gemmlowp' which is a tf_http_archive (rule definition at /./tensorflow-master/third_party/repo.bzl:121:19):\r\n - /./tensorflow-master/tensorflow/workspace.bzl:239:5\r\n - /./tensorflow-master/WORKSPACE:19:1\r\nINFO: Repository 'gemmlowp' used the following cache hits instead of downloading the corresponding file.\r\n * Hash '6678b484d929f2d0d3229d8ac4e3b815a950c86bb9f17851471d143f6d4f7834' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/12fed0cd7cfcd9e169bf1925bc3a7a58725fdcc3.zip\r\nIf the definition of 'gemmlowp' was updated, verify that the hashes were also updated.\r\nINFO: Call stack for the definition of repository 'fft2d' which is a tf_http_archive (rule definition at /./tensorflow-master/third_party/repo.bzl:121:19):\r\n - /./tensorflow-master/tensorflow/workspace.bzl:605:5\r\n - /./tensorflow-master/WORKSPACE:19:1\r\nINFO: Call stack for the definition of repository 'zlib_archive' which is a tf_http_archive (rule definition at /./tensorflow-master/third_party/repo.bzl:121:19):\r\n - /./tensorflow-master/tensorflow/workspace.bzl:593:5\r\n - /./tensorflow-master/WORKSPACE:19:1\r\nERROR: /./tensorflow-master/tensorflow/core/BUILD:2741:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /./tensorflow-master/tensorflow/core/BUILD:2741:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/head in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /./tensorflow-master/tensorflow/core/BUILD:2741:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/branch_ref in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: Analysis of target '//tensorflow/lite/toco:toco' failed; build aborted: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/home/./.cache/bazel/_bazel_xulm1/b172144b72497b6793e5891257276c30/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Elapsed time: 0.588s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)`", "the` /./` is some path in my dirs", "Doesn't appear to be specific to TensorFlow Lite:\r\n\r\n> ERROR: /./tensorflow-master/tensorflow/core/BUILD:2741:1: //tensorflow/core:version_info_gen depends on @local_config_git//:gen/spec.json in repository @local_config_git which failed to fetch. no such package '@local_config_git//': Traceback (most recent call last):\r\n> File \"/./tensorflow-master/third_party/git/git_configure.bzl\", line 61\r\n> _fail(result.stderr)", "This is a download issue:\r\n```\r\nhttps://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/bazel-toolchains/archive/92dd8a7a518a2fb7ba992d47c8b38299fe0be825.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n```\r\n\r\n\r\n", "have solved the ups\r\nbut get the down,\r\n`tensorflow-master$ bazel-bin/tensorflow/lite/toco/toco \r\n--graph_def_file=../my_freeze12xshell.pb --output_file=../my12.tflite \r\n--output_format=TFLITE --input_shape=2,4,513 --input_array=x_mixed \r\n--output_array=y_out1,y_out2 --inference_type=QUANTIZED_UINT8 \r\n--inference_input_type=QUANTIZED_UINT8 --std_dev_values=1 --mean_values=0\r\n \r\n-bash: bazel-bin/tensorflow/lite/toco/toco: No such file or directory`\r\n\r\nwhen I deal the up,and get the https://github.com/tensorflow/tensorflow/issues/32650,\r\nhow to ?\r\n", "Looks like you solved the build issue reported here?\r\nI see that the issue you linked, you were able to run toco.", "Closing the issue, because after the updates I see that original issue was resolved.\r\nWill let the new issue be handled in the linked github issue."]}, {"number": 32636, "title": "pip3.ipynb", "body": "https://github.com/tensorflow/tensorflow/commit/495b6197b4de40cc8951c3b147167787545246c8", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/32636\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32636) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32636) for more info**.\n\n<!-- ok -->", "What is this? The commit message has no description and the random jupyter notebook should not just be in the root of the repo.", "i signed it!"]}, {"number": 32635, "title": "[Intel Mkl] Updating README.md for 1.14.0 release", "body": "", "comments": []}, {"number": 32634, "title": "[ROCm] Fix for the broken ROCm Nightly Community Supported Build", "body": "The following commit causes the ` `//tensorflow/c/eager:c_api_experimental_test_gpu` test to fail leading to breakage in the ROCm Nightly Community Supported Build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/263200e6a10dce513baf938f47005f5b2ea2cf69\r\n\r\nThe diff at this line is the cause:\r\nhttps://github.com/tensorflow/tensorflow/commit/263200e6a10dce513baf938f47005f5b2ea2cf69#diff-14a3327e71c141f7761cbfcf221cd4b9R331\r\n\r\nThe failure occurs (in the `ExecuteWithTracing` subtest) because the string \"GPU:0\" does not show up in the profiling result (even though the MatMul op is successfully placed on the GPU).\r\n\r\nThough profiler is not enabled in ROCm mode, this test used to pass because the \"label\" associated with op-kernel execution event used to include the device name (which had \"GPU:0\" in it).\r\n\r\nThe change in the commit above drops the device name from the \"label\" and hence the test starts to fail.\r\n\r\nDisabling the \"GPU:0\" check in ROCm mode for now. It should be re-enabled when profiling support is added in ROCm mode.\r\n\r\n-----------------------------------------\r\n\r\n@whchung @chsigg ", "comments": []}, {"number": 32633, "title": "[r1.15 CherryPick]: [INTEL MKL] Fixing spurious omp thread spawning", "body": "This PR fixes the core over-utilization issue when Eigen and OpenMP threadings crash (in eager mode). (Cherry-picked to r2.0 in PR #32514.)\r\n\r\nOriginal PR #32485:\r\nIn some models that use eager/imperative code where we don\u2019t rewrite a few operators to MKL, a lot (intra_op_threads*OMP_NUM_THREADS) of threads are spawned by Eigen matmul code. This is because -fopenmp flag is passed to Eigen as part of the build configuration. This PR fixes the configuration to not pass -fopenmp to Eigen under --config=mkl.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32633) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because all the co-authored commits are from PR #32485 that is already merged in master.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32633) for more info**.\n\n<!-- cla_yes -->", "Hi @penpornk , @gunan and @goldiegadde , quick ping to check if we are okay to make the 1.15 cutoff for this PR.", "@Srini511 The PR wasn't merged yesterday because many tests failed in `Linux GPU`, `MacOS Python2 and CC`, and `Ubuntu CPU`. I believe they are unrelated to the PR, but I don't know what's going to happen to this PR either. Will have to wait to hear from @goldiegadde and @gunan.", "Thanks @penpornk for the update."]}, {"number": 32632, "title": "[r1.15-CherryPick]:forward_compatible env variable caching perf optimization.", "body": "PiperOrigin-RevId: 269832823\r\n\r\nLow-risk change with about 15% perf gain on a tight, small-tensor eager execution.", "comments": ["Are we picking this only into 1.15, or also 2.0?\r\n\r\nIt's a great change, but probably not worth it for 1.15 unless we make the same change in 2.0.\r\n\r\nIn fact, because forward compat dates are fixed, it would be better to completely expunge the forward compatibility tooling in the releases, but that's a bit more involved.", "Cherry-pick requested to both 1.15 and 2.0 ( https://github.com/tensorflow/tensorflow/pull/32631 )", "Another way is monkey-patching the function at run-time.\r\n```python\r\nclass Foo:\r\n  def path1(self):\r\n    print('path 1')\r\n  def path2(self):\r\n    print('path 2')\r\n  func = path1 if forward_compatible(2019, 10, 1) else path2\r\n```\r\nThough now since `forward_compatible` function got down to two function calls with several bitwise operations, I feel like monkey-patching or a separate stripping tooling is a bit overkill.  If we reach a point that reducing Python `if` and function calls beneficial, probably we should convert that part to C++."]}, {"number": 32631, "title": "[r2.0-CherryPick]:forward_compatible env variable caching perf optimization.", "body": "PiperOrigin-RevId: 269832823\r\n\r\nLow-risk change with about 15% perf gain on a tight, small-tensor eager execution.", "comments": []}, {"number": 32630, "title": "Cannot import tensorflow.image since 2019-09-06", "body": "I am running Linux. \r\n\r\nExecuting the following works:\r\n```\r\npip3 install tf-nightly-gpu-2.0-preview==2.0.0.dev20190903\r\npython3 -c \"import tensorflow.image\"\r\n```\r\n\r\nThere were no nightly builds on 4th and 5th. The following does not work:\r\n```\r\npip3 install tf-nightly-gpu-2.0-preview==2.0.0.dev20190906\r\npython3 -c \"import tensorflow.image\"\r\n```\r\n\r\nI get\r\n\r\n```\r\nModuleNotFoundError: No module named 'tensorflow.image'\r\n```\r\n\r\nThis still reproduces on the most recent nightly 2.0.0.dev20190918.", "comments": ["The following fixes the import problem: add\r\n\r\n```python\r\nfrom tensorflow.python.util import module_wrapper as _module_wrapper\r\n\r\nif not isinstance(_sys.modules[__name__], _module_wrapper.TFModuleWrapper):\r\n  _sys.modules[__name__] = _module_wrapper.TFModuleWrapper(\r\n      _sys.modules[__name__], \"\", public_apis=None, deprecation=False,\r\n      has_lite=False)\r\n```\r\n\r\nbefore `# WRAPPER_PLACEHOLDER` in `tensorflow_core/__init__.py`. This code was present on 3rd of September.", "I believe that this is a \"featured\" bug, but I will happily PR the fix if needed.", "Just to check, does the following work?\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.image)\r\n```", "That works:\r\n```\r\n<module 'tensorflow_core._api.v2.image' from '/usr/local/lib/python3.7/dist-packages/tensorflow_core/_api/v2/image/__init__.py'>\r\n```", "Perfect. That is the only supported way to use TF API, all the other ways can break as we progress our API. Unfortunately, Python allows you to do all kinds of imports, accessing private stuff, etc but this is also brittle.\r\n\r\nPlease use the `import tensorflow as tf; tf.image` approach", "Closing this issue since its resolved. \r\nFeel free to reopen if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32630\">No</a>\n", "Issue persists  on Colab\r\n \r\n> \r\n---------------------------------------------------------------------------\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n\r\n<ipython-input-12-4c38243be73f> in <module>()\r\n      3 import numpy as np\r\n      4 \r\n----> 5 import tensorflow.image.encode_jpeg\r\n      6 import tensorflow.image.decode_jpeg\r\n      7 import tensorflow.image.adjust_jpeg_quality\r\n\r\nModuleNotFoundError: No module named 'tensorflow.image'\r\n\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\r\n\r\n", "@kmamine,\r\nPlease use the imports as follows:\r\n```\r\nimport tensorflow as tf\r\nencode = tf.image.encode_jpeg\r\ndecode = tf.image.decode_jpeg\r\nadjust_quality = tf.image.adjust_jpeg_quality\r\n```\r\nThanks!"]}, {"number": 32629, "title": "tflite_runtime supports for arm (pi3 model b+)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinuz rpi 4.19.66-v7+ #1253 SMP Thu Ag 15 11:49:46 BST 2019 arm71 GNU/Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary):\r\nI installed tflite_runtime via \r\npip3 install tflite_runtime-1.15.0rc0-cp35-cp35m-linux_armv7l.whl package\r\n- TensorFlow version:\r\n1.15.0\r\n- Python version:\r\n3.5.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the problem**\r\n$ pip3 install tflite_runtime-1.15.0rc0-cp35-cp35m-linux_armv7l.whl\r\nin my script, I have:\r\n\r\nfrom tflite_runtime.interpreter import Interpreter\r\nfrom tflite_runtime.interpreter import load_delegate\r\n// more codes here\r\nfile_name = 'my_model.tflite'\r\ninterpreter = Interpreter(file_name, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n\r\nRunning the script:\r\n$ python3 script.py\r\n...\r\nOSError: /usr/bin/arm-linux-gnueabihf/libc++abi.so.1: undefined symbol: _Unwind_SetIP\r\nException ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter..Delegate object at 0x769be550>>\r\nTraceback (most recent call last):\r\n  File \"/home/pi/.local/lib/python3.5/site-packages/tflite_runtime/iterpreter.py\", line 124, in __del__\r\n    if self.__library is not None:\r\nAttributeError: 'Delegate' object has no attribute '_library'\r\n\r\nKeep in mind that I've been able to run the same exact code on my x86_64 Ubunbu 18.10 host machine, downloading the package from here:\r\nhttps://www.tensorflow.org/lite/guide/python#install_just_the_interpreter\r\nand ran pip install on the package.whl\r\n\r\n", "comments": ["Hi guys - any updates on this? \r\nThis issue isn't for me, personally but I know some people are waiting to get load_delegate working on their pi. Thanks! ", "I have the same exact issue!, even tf2 is not working", "Same issue here.\r\nIn the meanwhile the images from [google-coral/edgetpu-platforms](https://github.com/google-coral/edgetpu-platforms) work fine.\r\n", "@andcarnivorous \r\nI have already tried it on Raspberry Pi 3, Stretch Lite, Edgetpu 2.11.1, but not worked for me.", "@andcarnivorous are you using the edgetpu api or the tflite_runtime api?\r\nI have not been able to use the tflite_runtime api at all on the pi3 b+", "> @andcarnivorous are you using the edgetpu api or the tflite_runtime api?\r\n> I have not been able to use the tflite_runtime api at all on the pi3 b+\r\n\r\n  \r\n@Namburger Oops, I was running the edgetpu api, tflite_runtime is still not working even on the images.\r\n\r\n", "Hi\r\nI am using a Google Coral Mini PCIe on Banana PI 64 and have a similiar issue\r\nMy linux distribution is\r\nLinux bpi-iot-ros-ai 5.4.0-bpi-r64 #1 SMP PREEMPT Mon Dec 16 16:00:08 IST 2019 aarch64 aarch64 aarch64 GNU/Linux\r\nTensorFlow installed\r\ntflite_runtime-1.14.0-cp35-cp35m-linux_aarch64.whl\r\nPython3 Version : 3.5.2\r\n\r\nI have loaded the libedgetpu as per the instructions here https://coral.ai/news/updates-04-2019/.\r\nI am running the Demo model https://coral.ai/docs/m2/get-started/#4-run-a-model-using-the-tensorflow-lite-api which gives me the following error\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"classify_image.py\", line 122, in <module>\r\n    main()\r\n  File \"classify_image.py\", line 98, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"classify_image.py\", line 71, in make_interpreter\r\n    {'device': device[0]} if device else {})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 165, in load_delegate\r\n    delegate = Delegate(library, options)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 89, in __init__\r\n    self._library = ctypes.pydll.LoadLibrary(library)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 425, in LoadLibrary\r\n    return self._dlltype(name)\r\n  File \"/usr/lib/python3.5/ctypes/__init__.py\", line 347, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: /usr/lib/aarch64-linux-gnu/libc++abi.so.1: undefined symbol: _Unwind_GetRegionStart\r\nException ignored in: <bound method Delegate.__del__ of <tflite_runtime.interpreter.Delegate object at 0x7f99af6550>>\r\nTraceback (most rent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tflite_runtime/interpreter.py\", line 124, in __del__\r\n    if self._library is not None:\r\nAttributeError: 'Delegate' object has no attribute '_library'\r\n```\r\n\r\nI have updated the g++ and gcc to version 6 and installed libunwind8. but the same error is still present.", "same here. My setup: Raspberry pi 4, Raspbian,  tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl for tflite and Google Coral.\r\n\r\nAny leads will be appreciated.", "I managed to install tf2 from here (\nhttps://github.com/PINTO0309/Tensorflow-bin  ), and it's working :)\n*            Best Regards*\n\n\n\nOn Wed, Dec 25, 2019 at 10:02 PM Amit Dingare <notifications@github.com>\nwrote:\n\n> same here. My setup: Raspberry pi 4, Raspbian,\n> tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl for tflite and Google\n> Coral.\n>\n> Any leads will be appreciated.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32629?email_source=notifications&email_token=ABJSVGOX2B5NTSFBVJ4JGZ3Q2PJ67A5CNFSM4IYCCSTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHUT67Y#issuecomment-568934271>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABJSVGOXR7JAPLU2GSCDRATQ2PJ67ANCNFSM4IYCCSTA>\n> .\n>\n", "Same problem on a rpi 3B+, buster raspbian, running on this docker (python 3.5.3) https://hub.docker.com/r/sgtwilko/rpi-raspbian-opencv, with tflite_runtime-1.14.0-cp35-cp35m-linux_armv7l.whl", "> Same problem on a rpi 3B+, buster raspbian, running on this docker (python 3.5.3) https://hub.docker.com/r/sgtwilko/rpi-raspbian-opencv, with tflite_runtime-1.14.0-cp35-cp35m-linux_armv7l.whl\r\n\r\nSolved it using: \r\napt-get install apt-transport-https\r\napt-get install usbutils (check if you detect Google Inc. with lsusb)\r\napt-get update\r\napt-get install curl\r\necho \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | tee /etc/apt/sources.list.d/coral-edgetpu.list\r\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\r\napt-get update\r\napt-get install libedgetpu1-std", "Hey guys, sorry for not checking this sooner...\r\nUpdating the `tflite_runtime` package from [here](https://www.tensorflow.org/lite/guide/python) fixed it. \r\nThe current package is `tflite_runtime-2.1.0-cp37-cp37m*.whl`, my rpi run consistently with no issues. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32629\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32629\">No</a>\n"]}, {"number": 32628, "title": "add new variables to a pretrain model in keras", "body": "I encounter some problems when using keras. I get a network which has been pretrained, such as vgg16 or resnet34. The network use the keras default naming for all variables. Now i want to change some layers of the network which will change the variables of current layers. But when i directly change the layers, the name of variables in other layers change, too. Then i can not load the variables from the pretrained model. Can anyone tell me how to solve the problem.", "comments": ["@Womcos-Shu Please take a look at this [issue](https://github.com/keras-team/keras/issues/3465) which should be able to answer your questions and let me know if it helps. Thanks!", "Closing this issue as it has been inactive for more than 5 days. Please add additional comments and we can open the issue again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32628\">No</a>\n"]}, {"number": 32627, "title": " Support for 32 bits architecture #32315 ", "body": "Several users are still using Python32 bits and they cannot install TensorFlow. For them, pip install TensorFlow fails as no wheel matches the tags expected by their environment (to debug, pip debug --verbose shows only tags that don't math the filenames of our wheels).\r\n\r\nThere is some requests to support 32 bits, see for example #31431\r\n\r\nThis is not going to be easy as we need to also compile the C++ codebase in 32 bits mode and that would cause issues with code written assuming types have a certain bit width.\r\n\r\nThere is no change in the user visible API, just a new set of wheels to support more users.\r\n\r\nOpening this to reference in all similar issues.\r\n\r\nAfter closing my thread, i just reopen it. Google employees trying to tell me this is not the roadmap for the tool. The day i am going to care about a Google employee opinion is the day hell froze. \r\n\r\nIf you close this issue I will keep posting daily, until issue is fixed. \r\n\r\n", "comments": ["Hi there @reliefs \r\n\r\nTensorFlow is an open source project: while the people paid to work on it by Google have limited bandwidth, this doesn't mean other people can't join in and help. If you want to improve the situation, please consider joining the SIG Build project in which many community collaborators are working to build TF for many architectures.\r\n\r\nRepeatedly pinging the same issue for something we're aware of makes things worse. Your tone in this issue and in #32315 is unhelpful: there are real people working on this project who care. Please be civil and constructive, for the sake of everyone who works hard and contributes, and abide by our [Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md). \r\n\r\nThank you", "> Hi there @reliefs\r\n> \r\n> TensorFlow is an open source project: while the people paid to work on it by Google have limited bandwidth, this doesn't mean other people can't join in and help. If you want to improve the situation, please consider joining the SIG Build project in which many community collaborators are working to build TF for many architectures.\r\n> \r\n> Repeatedly pinging the same issue for something we're aware of makes things worse. Your tone in this issue and in #32315 is unhelpful: there are real people working on this project who care. Please be civil and constructive, for the sake of everyone who works hard and contributes, and abide by our [Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md).\r\n> \r\n> Thank you\r\n\r\nThanks you for your reply and understanding. People just ask me daily on how to fix this issue, it really bothers me and take time out of my work day.. \r\n\r\nHave a great weekend. ", "I'm interested in why you are getting asked this daily? Excuse my ignorance of your job/other projects, but I'm very curious as to which users and use cases mean you get this repeated question? It might give us clues or data that will help.\r\n", "Face detection is the new thing around script kitties.\r\n\r\nBut I have many people in last thread and other who use my work, TensorFlow is a good framework, if people want to use it in their package, and they should eliminate all 32bit servers and PCs, it's really crazy.\r\n\r\nNot all people have as fast pcs as us. ", "When im try to install from raspberry (ubuntu 18) says error:\r\n\r\nCollecting tensorflow==2.0.0rc0 (from -r requirements.txt (line 3))\r\n  Could not find a version that satisfies the requirement tensorflow==2.0.0rc0 (from -r requirements.txt (line 3)) (from versions: )\r\nNo matching distribution found for tensorflow==2.0.0rc0 (from -r requirements.txt (line 3))\r\n\r\nWhat happen?", " @alexzruiz \r\nNo offense, but your issue has nothing to do with the current thread. That being said, it seems that there is a typo in the requirements file you are using: it should be `tensorflow==2.0.0-rc0`.", "I will build a work around to fix this for my users, i feel bad for the rest of the community.. Good luck :/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32627\">No</a>\n", "If possible, please release the work around back to the community, in true spirit of open source development", "would love to see another wheels for 32bit system too.. eg. Hololens 1, although hololens 2 is x64 but i am sure a lot other produkt will still be in x32\r\n", "[https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.8.0/py36/CPU/sse2](url) is a possible functional wheel for 32bit system.. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32627\">No</a>\n", "Closing based on #32315 and https://discuss.tensorflow.org/t/how-to-build-32bit-tensorflow-on-windows/5284?u=mihaimaruseac"]}, {"number": 32626, "title": "Cherrypicks for estimator 2.0 release", "body": "", "comments": []}, {"number": 32625, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not application\r\n- TensorFlow installed from (source or binary): PIP Command\r\n- TensorFlow version:1.14.0\r\n- Python version:Python 3.7.3 (default, Apr 24 2019, 15:29:51)\r\n- Installed using virtualenv? pip? conda?: PIP\r\n- Bazel version (if compiling from source): Not applicable\r\n- GCC/Compiler version (if compiling from source): Not applicable\r\n- CUDA/cuDNN version: cudnn-10.1-windows10-x64-v7.6.3.30\r\n- GPU model and memory:NVIDIA GetForce GTX 1660 Ti anf 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I am try to import the tensorflow I am getting the error ImportError: DLL load failed: The specified module could not be found.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFollowed the installation guide provided for CUDA Installation Guide for Microsoft Windows\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n>>> import tensorflow as t\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "Does your CPU support AVX? See also #32352", "I have installed:\r\ncuda_10.1.243_win10_network version of CUDA Toolkit\r\nI have already verified the path on windows\r\nThanks", "The answer is to a different question.\r\n\r\nCan you check if your CPU supports AVX extension?", "> Does your CPU support AVX? See also #32352\r\n\r\nYes my CPU support AVX\r\n\r\nOutout of python -v -c \"import tensorflow\"\r\n\r\nimport _frozen_importlib # frozen\r\nimport _imp # builtin\r\nimport '_thread' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport '_warnings' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport '_weakref' # <class '_frozen_importlib.BuiltinImporter'>\r\n# installing zipimport hook\r\nimport 'zipimport' # <class '_frozen_importlib.BuiltinImporter'>\r\n# installed zipimport hook\r\nimport '_frozen_importlib_external' # <class '_frozen_importlib.FrozenImporter'>\r\nimport '_io' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'marshal' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'nt' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport _thread # previously loaded ('_thread')\r\nimport '_thread' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport _weakref # previously loaded ('_weakref')\r\nimport '_weakref' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'winreg' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\encodings\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\encodings\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\encodings\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\codecs.cpython-37.pyc matches D:\\Anaconda\\lib\\codecs.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\codecs.cpython-37.pyc'\r\nimport '_codecs' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'codecs' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7994A8>\r\n# D:\\Anaconda\\lib\\encodings\\__pycache__\\aliases.cpython-37.pyc matches D:\\Anaconda\\lib\\encodings\\aliases.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\encodings\\\\__pycache__\\\\aliases.cpython-37.pyc'\r\nimport 'encodings.aliases' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7AAE48>\r\nimport 'encodings' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7886D8>\r\n# D:\\Anaconda\\lib\\encodings\\__pycache__\\utf_8.cpython-37.pyc matches D:\\Anaconda\\lib\\encodings\\utf_8.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\encodings\\\\__pycache__\\\\utf_8.cpython-37.pyc'\r\nimport 'encodings.utf_8' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7BAEF0>\r\nimport '_signal' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\encodings\\__pycache__\\latin_1.cpython-37.pyc matches D:\\Anaconda\\lib\\encodings\\latin_1.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\encodings\\\\__pycache__\\\\latin_1.cpython-37.pyc'\r\nimport 'encodings.latin_1' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7BE358>\r\n# D:\\Anaconda\\lib\\__pycache__\\io.cpython-37.pyc matches D:\\Anaconda\\lib\\io.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\io.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\abc.cpython-37.pyc matches D:\\Anaconda\\lib\\abc.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\abc.cpython-37.pyc'\r\nimport '_abc' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'abc' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7BE8D0>\r\nimport 'io' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7BE518>\r\n# D:\\Anaconda\\lib\\__pycache__\\site.cpython-37.pyc matches D:\\Anaconda\\lib\\site.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\site.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\os.cpython-37.pyc matches D:\\Anaconda\\lib\\os.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\os.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\stat.cpython-37.pyc matches D:\\Anaconda\\lib\\stat.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\stat.cpython-37.pyc'\r\nimport '_stat' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'stat' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC917B70>\r\n# D:\\Anaconda\\lib\\__pycache__\\ntpath.cpython-37.pyc matches D:\\Anaconda\\lib\\ntpath.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\ntpath.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\genericpath.cpython-37.pyc matches D:\\Anaconda\\lib\\genericpath.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\genericpath.cpython-37.pyc'\r\nimport 'genericpath' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC924A58>\r\nimport 'ntpath' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC91F278>\r\n# D:\\Anaconda\\lib\\__pycache__\\_collections_abc.cpython-37.pyc matches D:\\Anaconda\\lib\\_collections_abc.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_collections_abc.cpython-37.pyc'\r\nimport '_collections_abc' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC928080>\r\nimport 'os' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7CD780>\r\n# D:\\Anaconda\\lib\\__pycache__\\_sitebuiltins.cpython-37.pyc matches D:\\Anaconda\\lib\\_sitebuiltins.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_sitebuiltins.cpython-37.pyc'\r\nimport '_sitebuiltins' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7CDB70>\r\n# D:\\Anaconda\\lib\\__pycache__\\_bootlocale.cpython-37.pyc matches D:\\Anaconda\\lib\\_bootlocale.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_bootlocale.cpython-37.pyc'\r\nimport '_locale' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport '_bootlocale' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC912208>\r\n# D:\\Anaconda\\lib\\encodings\\__pycache__\\cp1252.cpython-37.pyc matches D:\\Anaconda\\lib\\encodings\\cp1252.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\encodings\\\\__pycache__\\\\cp1252.cpython-37.pyc'\r\nimport 'encodings.cp1252' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC940FD0>\r\n# D:\\Anaconda\\lib\\__pycache__\\types.cpython-37.pyc matches D:\\Anaconda\\lib\\types.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\types.cpython-37.pyc'\r\nimport 'types' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA5D320>\r\n# D:\\Anaconda\\lib\\importlib\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\importlib\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\importlib\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\warnings.cpython-37.pyc matches D:\\Anaconda\\lib\\warnings.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\warnings.cpython-37.pyc'\r\nimport 'warnings' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA6A2B0>\r\nimport 'importlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA5D780>\r\n# D:\\Anaconda\\lib\\importlib\\__pycache__\\util.cpython-37.pyc matches D:\\Anaconda\\lib\\importlib\\util.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\importlib\\\\__pycache__\\\\util.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\importlib\\__pycache__\\abc.cpython-37.pyc matches D:\\Anaconda\\lib\\importlib\\abc.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\importlib\\\\__pycache__\\\\abc.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\importlib\\__pycache__\\machinery.cpython-37.pyc matches D:\\Anaconda\\lib\\importlib\\machinery.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\importlib\\\\__pycache__\\\\machinery.cpython-37.pyc'\r\nimport 'importlib.machinery' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA73F28>\r\nimport 'importlib.abc' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA736D8>\r\n# D:\\Anaconda\\lib\\__pycache__\\contextlib.cpython-37.pyc matches D:\\Anaconda\\lib\\contextlib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\contextlib.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\collections\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\collections\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\collections\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\operator.cpython-37.pyc matches D:\\Anaconda\\lib\\operator.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\operator.cpython-37.pyc'\r\nimport '_operator' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'operator' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACBAB518>\r\n# D:\\Anaconda\\lib\\__pycache__\\keyword.cpython-37.pyc matches D:\\Anaconda\\lib\\keyword.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\keyword.cpython-37.pyc'\r\nimport 'keyword' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACBB4860>\r\n# D:\\Anaconda\\lib\\__pycache__\\heapq.cpython-37.pyc matches D:\\Anaconda\\lib\\heapq.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\heapq.cpython-37.pyc'\r\nimport '_heapq' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'heapq' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACBB5160>\r\nimport 'itertools' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\__pycache__\\reprlib.cpython-37.pyc matches D:\\Anaconda\\lib\\reprlib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\reprlib.cpython-37.pyc'\r\nimport 'reprlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACBB5780>\r\nimport '_collections' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'collections' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA8C710>\r\n# D:\\Anaconda\\lib\\__pycache__\\functools.cpython-37.pyc matches D:\\Anaconda\\lib\\functools.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\functools.cpython-37.pyc'\r\nimport '_functools' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'functools' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA8CB00>\r\nimport 'contextlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA73D68>\r\nimport 'importlib.util' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACA6A940>\r\n# possible namespace for D:\\Anaconda\\lib\\site-packages\\mpl_toolkits\r\n# possible namespace for D:\\Anaconda\\lib\\site-packages\\google\r\n# destroy sphinxcontrib\r\n# destroy sphinxcontrib\r\n# destroy sphinxcontrib\r\n# destroy sphinxcontrib\r\n# destroy sphinxcontrib\r\n# destroy sphinxcontrib\r\nimport 'site' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAC7C53C8>\r\nPython 3.7.3 (default, Apr 24 2019, 15:29:51) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\__future__.cpython-37.pyc matches D:\\Anaconda\\lib\\__future__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\__future__.cpython-37.pyc'\r\nimport '__future__' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC10278>\r\n# D:\\Anaconda\\lib\\distutils\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\distutils\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\distutils\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\nimport 'distutils' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC107B8>\r\n# D:\\Anaconda\\lib\\__pycache__\\inspect.cpython-37.pyc matches D:\\Anaconda\\lib\\inspect.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\inspect.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\dis.cpython-37.pyc matches D:\\Anaconda\\lib\\dis.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\dis.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\opcode.cpython-37.pyc matches D:\\Anaconda\\lib\\opcode.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\opcode.cpython-37.pyc'\r\nimport '_opcode' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'opcode' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC30978>\r\nimport 'dis' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC2B6D8>\r\n# D:\\Anaconda\\lib\\collections\\__pycache__\\abc.cpython-37.pyc matches D:\\Anaconda\\lib\\collections\\abc.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\collections\\\\__pycache__\\\\abc.cpython-37.pyc'\r\nimport 'collections.abc' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC3D080>\r\n# D:\\Anaconda\\lib\\__pycache__\\enum.cpython-37.pyc matches D:\\Anaconda\\lib\\enum.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\enum.cpython-37.pyc'\r\nimport 'enum' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC3D0F0>\r\n# D:\\Anaconda\\lib\\__pycache__\\linecache.cpython-37.pyc matches D:\\Anaconda\\lib\\linecache.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\linecache.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\tokenize.cpython-37.pyc matches D:\\Anaconda\\lib\\tokenize.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\tokenize.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\re.cpython-37.pyc matches D:\\Anaconda\\lib\\re.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\re.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\sre_compile.cpython-37.pyc matches D:\\Anaconda\\lib\\sre_compile.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\sre_compile.cpython-37.pyc'\r\nimport '_sre' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\__pycache__\\sre_parse.cpython-37.pyc matches D:\\Anaconda\\lib\\sre_parse.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\sre_parse.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\sre_constants.cpython-37.pyc matches D:\\Anaconda\\lib\\sre_constants.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\sre_constants.cpython-37.pyc'\r\nimport 'sre_constants' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC7C710>\r\nimport 'sre_parse' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC6DEB8>\r\nimport 'sre_compile' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC6A860>\r\n# D:\\Anaconda\\lib\\__pycache__\\copyreg.cpython-37.pyc matches D:\\Anaconda\\lib\\copyreg.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\copyreg.cpython-37.pyc'\r\nimport 'copyreg' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC8AC18>\r\nimport 're' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC62978>\r\n# D:\\Anaconda\\lib\\__pycache__\\token.cpython-37.pyc matches D:\\Anaconda\\lib\\token.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\token.cpython-37.pyc'\r\nimport 'token' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC8AF98>\r\nimport 'tokenize' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC54208>\r\nimport 'linecache' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC3EFD0>\r\nimport 'inspect' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACC108D0>\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\ctypes\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\ctypes\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\ctypes\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# extension module '_ctypes' loaded from 'D:\\\\Anaconda\\\\DLLs\\\\_ctypes.pyd'\r\n# extension module '_ctypes' executed from 'D:\\\\Anaconda\\\\DLLs\\\\_ctypes.pyd'\r\nimport '_ctypes' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AACCB4A20>\r\n# D:\\Anaconda\\lib\\__pycache__\\struct.cpython-37.pyc matches D:\\Anaconda\\lib\\struct.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\struct.cpython-37.pyc'\r\nimport '_struct' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'struct' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCBD0B8>\r\n# D:\\Anaconda\\lib\\ctypes\\__pycache__\\_endian.cpython-37.pyc matches D:\\Anaconda\\lib\\ctypes\\_endian.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\ctypes\\\\__pycache__\\\\_endian.cpython-37.pyc'\r\nimport 'ctypes._endian' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCCA4A8>\r\nimport 'ctypes' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCAAC88>\r\n# D:\\Anaconda\\lib\\__pycache__\\traceback.cpython-37.pyc matches D:\\Anaconda\\lib\\traceback.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\traceback.cpython-37.pyc'\r\nimport 'traceback' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCAC390>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\_globals.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\_globals.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\_globals.cpython-37.pyc'\r\nimport 'numpy._globals' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCF7A90>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\__config__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\__config__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\__config__.cpython-37.pyc'\r\nimport 'numpy.__config__' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCFE0F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\version.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\version.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\version.cpython-37.pyc'\r\nimport 'numpy.version' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCFE2E8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\_distributor_init.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\_distributor_init.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\_distributor_init.cpython-37.pyc'\r\n# extension module 'numpy._mklinit' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\_mklinit.cp37-win_amd64.pyd'\r\n# extension module 'numpy._mklinit' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\_mklinit.cp37-win_amd64.pyd'\r\nimport 'numpy._mklinit' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AACCFE748>\r\nimport 'numpy._distributor_init' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCFE390>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\info.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\info.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\info.cpython-37.pyc'\r\nimport 'numpy.core.info' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD02278>\r\n# D:\\Anaconda\\lib\\__pycache__\\glob.cpython-37.pyc matches D:\\Anaconda\\lib\\glob.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\glob.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\fnmatch.cpython-37.pyc matches D:\\Anaconda\\lib\\fnmatch.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\fnmatch.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\posixpath.cpython-37.pyc matches D:\\Anaconda\\lib\\posixpath.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\posixpath.cpython-37.pyc'\r\nimport 'posixpath' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD02F28>\r\nimport 'fnmatch' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD02940>\r\nimport 'glob' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD02400>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\multiarray.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\multiarray.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\multiarray.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\overrides.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\overrides.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\overrides.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\datetime.cpython-37.pyc matches D:\\Anaconda\\lib\\datetime.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\datetime.cpython-37.pyc'\r\nimport 'time' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'math' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport '_datetime' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'datetime' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD17D30>\r\n# extension module 'numpy.core._multiarray_umath' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\_multiarray_umath.cp37-win_amd64.pyd'\r\n# extension module 'numpy.core._multiarray_umath' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\_multiarray_umath.cp37-win_amd64.pyd'\r\nimport 'numpy.core._multiarray_umath' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AACD17B00>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\compat\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\__pycache__\\_inspect.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\_inspect.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\compat\\\\__pycache__\\\\_inspect.cpython-37.pyc'\r\nimport 'numpy.compat._inspect' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD230B8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\__pycache__\\py3k.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\compat\\py3k.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\compat\\\\__pycache__\\\\py3k.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\pathlib.cpython-37.pyc matches D:\\Anaconda\\lib\\pathlib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\pathlib.cpython-37.pyc'\r\nimport 'errno' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\urllib\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\urllib\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\urllib\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\nimport 'urllib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDA8198>\r\n# D:\\Anaconda\\lib\\urllib\\__pycache__\\parse.cpython-37.pyc matches D:\\Anaconda\\lib\\urllib\\parse.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\urllib\\\\__pycache__\\\\parse.cpython-37.pyc'\r\nimport 'urllib.parse' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDA8320>\r\nimport 'pathlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD1FAC8>\r\nimport 'numpy.compat.py3k' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD1F588>\r\nimport 'numpy.compat' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD23F98>\r\nimport 'numpy.core.overrides' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD17550>\r\nimport 'numpy.core.multiarray' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD02D30>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\umath.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\umath.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\umath.cpython-37.pyc'\r\nimport 'numpy.core.umath' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD0BEB8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\numerictypes.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\numerictypes.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\numerictypes.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\numbers.cpython-37.pyc matches D:\\Anaconda\\lib\\numbers.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\numbers.cpython-37.pyc'\r\nimport 'numbers' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDC26D8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_string_helpers.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_string_helpers.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_string_helpers.cpython-37.pyc'\r\nimport 'numpy.core._string_helpers' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDCD278>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_type_aliases.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_type_aliases.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_type_aliases.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_dtype.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_dtype.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_dtype.cpython-37.pyc'\r\nimport 'numpy.core._dtype' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDD98D0>\r\nimport 'numpy.core._type_aliases' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACDCD9B0>\r\nimport 'numpy.core.numerictypes' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD0BD68>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\numeric.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\numeric.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\numeric.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_internal.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_internal.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_internal.cpython-37.pyc'\r\nimport 'numpy.core._internal' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACFF3320>\r\n# D:\\Anaconda\\lib\\__pycache__\\pickle.cpython-37.pyc matches D:\\Anaconda\\lib\\pickle.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\pickle.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\_compat_pickle.cpython-37.pyc matches D:\\Anaconda\\lib\\_compat_pickle.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_compat_pickle.cpython-37.pyc'\r\nimport '_compat_pickle' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD018AC8>\r\nimport '_pickle' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'pickle' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0005F8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\fromnumeric.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\fromnumeric.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_methods.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_methods.cpython-37.pyc'\r\nimport 'numpy.core._methods' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD038908>\r\nimport 'numpy.core.fromnumeric' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0188D0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\arrayprint.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\arrayprint.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\arrayprint.cpython-37.pyc'\r\nimport 'numpy.core.arrayprint' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0387B8>\r\nimport 'numpy.core.numeric' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACFE2978>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\defchararray.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\defchararray.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\defchararray.cpython-37.pyc'\r\nimport 'numpy.core.defchararray' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACFF30F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\records.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\records.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\records.cpython-37.pyc'\r\nimport 'numpy.core.records' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD06DB38>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\memmap.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\memmap.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\memmap.cpython-37.pyc'\r\nimport 'numpy.core.memmap' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD079B38>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\function_base.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\function_base.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\function_base.cpython-37.pyc'\r\nimport 'numpy.core.function_base' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD07F048>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\machar.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\machar.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\machar.cpython-37.pyc'\r\nimport 'numpy.core.machar' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD07F588>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\getlimits.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\getlimits.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\getlimits.cpython-37.pyc'\r\nimport 'numpy.core.getlimits' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD07FC18>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\shape_base.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\shape_base.cpython-37.pyc'\r\nimport 'numpy.core.shape_base' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD085B70>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\einsumfunc.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\einsumfunc.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\einsumfunc.cpython-37.pyc'\r\nimport 'numpy.core.einsumfunc' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD08EAC8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_add_newdocs.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_add_newdocs.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_add_newdocs.cpython-37.pyc'\r\n# extension module 'numpy.core._multiarray_tests' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\r\n# extension module 'numpy.core._multiarray_tests' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\_multiarray_tests.cp37-win_amd64.pyd'\r\nimport 'numpy.core._multiarray_tests' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD098F98>\r\nimport 'numpy.core._add_newdocs' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD098BE0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\core\\__pycache__\\_dtype_ctypes.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\core\\_dtype_ctypes.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\__pycache__\\\\_dtype_ctypes.cpython-37.pyc'\r\nimport 'numpy.core._dtype_ctypes' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD098EB8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\_pytesttester.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\_pytesttester.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\_pytesttester.cpython-37.pyc'\r\nimport 'numpy._pytesttester' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0A70B8>\r\nimport 'numpy.core' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCFE5F8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\info.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\info.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\info.cpython-37.pyc'\r\nimport 'numpy.lib.info' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0A7470>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\type_check.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\type_check.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\type_check.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\ufunclike.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\ufunclike.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\ufunclike.cpython-37.pyc'\r\nimport 'numpy.lib.ufunclike' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0A7E10>\r\nimport 'numpy.lib.type_check' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0A7438>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\index_tricks.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\index_tricks.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\index_tricks.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\matrixlib\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\matrixlib\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\matrixlib\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\matrixlib\\__pycache__\\defmatrix.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\matrixlib\\\\__pycache__\\\\defmatrix.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\ast.cpython-37.pyc matches D:\\Anaconda\\lib\\ast.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\ast.cpython-37.pyc'\r\nimport '_ast' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'ast' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0CFAC8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\__pycache__\\info.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\info.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\__pycache__\\\\info.cpython-37.pyc'\r\nimport 'numpy.linalg.info' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD105A90>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\__pycache__\\linalg.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\__pycache__\\\\linalg.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\twodim_base.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\twodim_base.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\twodim_base.cpython-37.pyc'\r\nimport 'numpy.lib.twodim_base' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1146A0>\r\n# extension module 'numpy.linalg.lapack_lite' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\lapack_lite.cp37-win_amd64.pyd'\r\n# extension module 'numpy.linalg.lapack_lite' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\lapack_lite.cp37-win_amd64.pyd'\r\nimport 'numpy.linalg.lapack_lite' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD118320>\r\n# extension module 'numpy.linalg._umath_linalg' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\_umath_linalg.cp37-win_amd64.pyd'\r\n# extension module 'numpy.linalg._umath_linalg' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\linalg\\\\_umath_linalg.cp37-win_amd64.pyd'\r\nimport 'numpy.linalg._umath_linalg' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD118438>\r\nimport 'numpy.linalg.linalg' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD105B38>\r\nimport 'numpy.linalg' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1057B8>\r\nimport 'numpy.matrixlib.defmatrix' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0CB940>\r\nimport 'numpy.matrixlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0CB6A0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\function_base.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\function_base.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\function_base.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\utils.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\utils.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\utils.cpython-37.pyc'\r\nimport 'numpy.lib.utils' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD155668>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\histograms.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\histograms.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\histograms.cpython-37.pyc'\r\nimport 'numpy.lib.histograms' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD15F278>\r\nimport 'numpy.lib.function_base' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0CFA58>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\stride_tricks.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\stride_tricks.cpython-37.pyc'\r\nimport 'numpy.lib.stride_tricks' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1554E0>\r\nimport 'numpy.lib.index_tricks' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD0C3320>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\mixins.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\mixins.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\mixins.cpython-37.pyc'\r\nimport 'numpy.lib.mixins' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD16E438>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\nanfunctions.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\nanfunctions.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\nanfunctions.cpython-37.pyc'\r\nimport 'numpy.lib.nanfunctions' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1767B8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\shape_base.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\shape_base.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\shape_base.cpython-37.pyc'\r\nimport 'numpy.lib.shape_base' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD17B550>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\scimath.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\scimath.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\scimath.cpython-37.pyc'\r\nimport 'numpy.lib.scimath' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1852B0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\polynomial.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\polynomial.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\polynomial.cpython-37.pyc'\r\nimport 'numpy.lib.polynomial' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD185978>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\arraysetops.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraysetops.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\arraysetops.cpython-37.pyc'\r\nimport 'numpy.lib.arraysetops' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD195128>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\npyio.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\npyio.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\weakref.cpython-37.pyc matches D:\\Anaconda\\lib\\weakref.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\weakref.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\_weakrefset.cpython-37.pyc matches D:\\Anaconda\\lib\\_weakrefset.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_weakrefset.cpython-37.pyc'\r\nimport '_weakrefset' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1B8908>\r\nimport 'weakref' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1AD198>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\format.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\format.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\format.cpython-37.pyc'\r\nimport 'numpy.lib.format' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1C0A20>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\_datasource.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\_datasource.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\_datasource.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\shutil.cpython-37.pyc matches D:\\Anaconda\\lib\\shutil.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\shutil.cpython-37.pyc'\r\nimport 'zlib' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\__pycache__\\bz2.cpython-37.pyc matches D:\\Anaconda\\lib\\bz2.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\bz2.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\_compression.cpython-37.pyc matches D:\\Anaconda\\lib\\_compression.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\_compression.cpython-37.pyc'\r\nimport '_compression' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1E09B0>\r\n# D:\\Anaconda\\lib\\__pycache__\\threading.cpython-37.pyc matches D:\\Anaconda\\lib\\threading.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\threading.cpython-37.pyc'\r\nimport 'threading' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1E90B8>\r\n# extension module '_bz2' loaded from 'D:\\\\Anaconda\\\\DLLs\\\\_bz2.pyd'\r\n# extension module '_bz2' executed from 'D:\\\\Anaconda\\\\DLLs\\\\_bz2.pyd'\r\nimport '_bz2' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD1FADD8>\r\nimport 'bz2' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1E00F0>\r\n# D:\\Anaconda\\lib\\__pycache__\\lzma.cpython-37.pyc matches D:\\Anaconda\\lib\\lzma.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\lzma.cpython-37.pyc'\r\n# extension module '_lzma' loaded from 'D:\\\\Anaconda\\\\DLLs\\\\_lzma.pyd'\r\n# extension module '_lzma' executed from 'D:\\\\Anaconda\\\\DLLs\\\\_lzma.pyd'\r\nimport '_lzma' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD20A6D8>\r\nimport 'lzma' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1E9630>\r\nimport 'shutil' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1D0160>\r\nimport 'numpy.lib._datasource' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1C8208>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\_iotools.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\_iotools.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\_iotools.cpython-37.pyc'\r\nimport 'numpy.lib._iotools' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD1D0278>\r\nimport 'numpy.lib.npyio' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD195DA0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\financial.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\financial.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\financial.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\decimal.cpython-37.pyc matches D:\\Anaconda\\lib\\decimal.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\decimal.cpython-37.pyc'\r\n# extension module '_decimal' loaded from 'D:\\\\Anaconda\\\\DLLs\\\\_decimal.pyd'\r\n# extension module '_decimal' executed from 'D:\\\\Anaconda\\\\DLLs\\\\_decimal.pyd'\r\nimport '_decimal' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD2417F0>\r\nimport 'decimal' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD241588>\r\nimport 'numpy.lib.financial' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2360F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\arrayterator.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\arrayterator.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\arrayterator.cpython-37.pyc'\r\nimport 'numpy.lib.arrayterator' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2416D8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\arraypad.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\arraypad.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\arraypad.cpython-37.pyc'\r\nimport 'numpy.lib.arraypad' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2488D0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\__pycache__\\_version.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\lib\\_version.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\lib\\\\__pycache__\\\\_version.cpython-37.pyc'\r\nimport 'numpy.lib._version' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD254BA8>\r\nimport 'numpy.lib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACD027F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\__pycache__\\info.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\info.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__pycache__\\\\info.cpython-37.pyc'\r\nimport 'numpy.fft.info' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD25D2B0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\__pycache__\\fftpack.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\fftpack.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__pycache__\\\\fftpack.cpython-37.pyc'\r\n# extension module 'numpy.fft.fftpack_lite' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\fftpack_lite.cp37-win_amd64.pyd'\r\n# extension module 'numpy.fft.fftpack_lite' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\fftpack_lite.cp37-win_amd64.pyd'\r\nimport 'numpy.fft.fftpack_lite' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD25DF98>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\__pycache__\\helper.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\fft\\helper.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__pycache__\\\\helper.cpython-37.pyc'\r\nimport 'numpy.fft.helper' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD264128>\r\nimport 'numpy.fft.fftpack' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD25D390>\r\n# D:\\Anaconda\\lib\\site-packages\\mkl_fft\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\mkl_fft\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\mkl_fft\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# extension module 'mkl_fft._pydfti' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\mkl_fft\\\\_pydfti.cp37-win_amd64.pyd'\r\n# extension module 'mkl_fft._pydfti' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\mkl_fft\\\\_pydfti.cp37-win_amd64.pyd'\r\nimport 'mkl_fft._pydfti' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD264CF8>\r\n# D:\\Anaconda\\lib\\site-packages\\mkl_fft\\__pycache__\\_version.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\mkl_fft\\_version.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\mkl_fft\\\\__pycache__\\\\_version.cpython-37.pyc'\r\nimport 'mkl_fft._version' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD26FB38>\r\nimport 'mkl_fft' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD264A90>\r\n# D:\\Anaconda\\lib\\site-packages\\mkl_fft\\__pycache__\\_numpy_fft.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\mkl_fft\\_numpy_fft.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\mkl_fft\\\\__pycache__\\\\_numpy_fft.cpython-37.pyc'\r\nimport 'mkl_fft._numpy_fft' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD264B38>\r\nimport 'numpy.fft' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD25D0B8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\polynomial.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\polynomial.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\polynomial.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\polyutils.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\polyutils.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\polyutils.cpython-37.pyc'\r\nimport 'numpy.polynomial.polyutils' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2AD710>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\_polybase.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\_polybase.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\_polybase.cpython-37.pyc'\r\nimport 'numpy.polynomial._polybase' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2AD780>\r\nimport 'numpy.polynomial.polynomial' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2A80F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\chebyshev.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\chebyshev.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\chebyshev.cpython-37.pyc'\r\nimport 'numpy.polynomial.chebyshev' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2BA9B0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\legendre.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\legendre.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\legendre.cpython-37.pyc'\r\nimport 'numpy.polynomial.legendre' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2C4E48>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\hermite.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\hermite.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\hermite.cpython-37.pyc'\r\nimport 'numpy.polynomial.hermite' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2D0048>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\hermite_e.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\hermite_e.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\hermite_e.cpython-37.pyc'\r\nimport 'numpy.polynomial.hermite_e' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2DD0B8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\__pycache__\\laguerre.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\polynomial\\laguerre.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\polynomial\\\\__pycache__\\\\laguerre.cpython-37.pyc'\r\nimport 'numpy.polynomial.laguerre' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD2DDEF0>\r\nimport 'numpy.polynomial' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD25D160>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\random\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\random\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\random\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# extension module 'numpy.random.mtrand' loaded from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\random\\\\mtrand.cp37-win_amd64.pyd'\r\n# extension module 'numpy.random.mtrand' executed from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\random\\\\mtrand.cp37-win_amd64.pyd'\r\nimport 'numpy.random.mtrand' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD6EA518>\r\nimport 'numpy.random' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD6EA048>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\__pycache__\\ctypeslib.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\ctypeslib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\__pycache__\\\\ctypeslib.cpython-37.pyc'\r\nimport 'numpy.ctypeslib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD6EA470>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\ma\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\__pycache__\\core.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\core.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\ma\\\\__pycache__\\\\core.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\textwrap.cpython-37.pyc matches D:\\Anaconda\\lib\\textwrap.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\textwrap.cpython-37.pyc'\r\nimport 'textwrap' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD753748>\r\nimport 'numpy.ma.core' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD6FA6D8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\__pycache__\\extras.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\ma\\extras.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\ma\\\\__pycache__\\\\extras.cpython-37.pyc'\r\nimport 'numpy.ma.extras' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD6FACF8>\r\nimport 'numpy.ma' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD6FA2B0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\testing\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\result.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\result.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\result.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\util.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\util.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\util.cpython-37.pyc'\r\nimport 'unittest.util' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD78AA20>\r\nimport 'unittest.result' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD78A2B0>\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\case.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\case.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\case.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\difflib.cpython-37.pyc matches D:\\Anaconda\\lib\\difflib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\difflib.cpython-37.pyc'\r\nimport 'difflib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD7ADD30>\r\n# D:\\Anaconda\\lib\\logging\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\logging\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\logging\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\string.cpython-37.pyc matches D:\\Anaconda\\lib\\string.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\string.cpython-37.pyc'\r\nimport '_string' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'string' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD7E1390>\r\nimport 'atexit' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'logging' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD7B9F60>\r\n# D:\\Anaconda\\lib\\__pycache__\\pprint.cpython-37.pyc matches D:\\Anaconda\\lib\\pprint.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\pprint.cpython-37.pyc'\r\nimport 'pprint' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD7EC208>\r\nimport 'unittest.case' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD78AD30>\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\suite.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\suite.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\suite.cpython-37.pyc'\r\nimport 'unittest.suite' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD7AD6D8>\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\loader.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\loader.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\loader.cpython-37.pyc'\r\nimport 'unittest.loader' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8090F0>\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\main.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\main.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\main.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\argparse.cpython-37.pyc matches D:\\Anaconda\\lib\\argparse.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\argparse.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\gettext.cpython-37.pyc matches D:\\Anaconda\\lib\\gettext.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\gettext.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\locale.cpython-37.pyc matches D:\\Anaconda\\lib\\locale.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\locale.cpython-37.pyc'\r\nimport 'locale' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD841EF0>\r\nimport 'gettext' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD839438>\r\nimport 'argparse' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8164A8>\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\runner.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\runner.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\runner.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\unittest\\__pycache__\\signals.cpython-37.pyc matches D:\\Anaconda\\lib\\unittest\\signals.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\unittest\\\\__pycache__\\\\signals.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\signal.cpython-37.pyc matches D:\\Anaconda\\lib\\signal.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\signal.cpython-37.pyc'\r\nimport 'signal' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD86D358>\r\nimport 'unittest.signals' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD85EDD8>\r\nimport 'unittest.runner' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD85E748>\r\nimport 'unittest.main' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD809C88>\r\nimport 'unittest' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD780F98>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\testing\\\\_private\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\nimport 'numpy.testing._private' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD85EC50>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\__pycache__\\utils.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\testing\\\\_private\\\\__pycache__\\\\utils.cpython-37.pyc'\r\nimport 'gc' # <class '_frozen_importlib.BuiltinImporter'>\r\n# D:\\Anaconda\\lib\\__pycache__\\tempfile.cpython-37.pyc matches D:\\Anaconda\\lib\\tempfile.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\tempfile.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\random.cpython-37.pyc matches D:\\Anaconda\\lib\\random.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\random.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\hashlib.cpython-37.pyc matches D:\\Anaconda\\lib\\hashlib.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\hashlib.cpython-37.pyc'\r\n# extension module '_hashlib' loaded from 'D:\\\\Anaconda\\\\DLLs\\\\_hashlib.pyd'\r\n# extension module '_hashlib' executed from 'D:\\\\Anaconda\\\\DLLs\\\\_hashlib.pyd'\r\nimport '_hashlib' # <_frozen_importlib_external.ExtensionFileLoader object at 0x0000018AAD8A33C8>\r\nimport '_blake2' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport '_sha3' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'hashlib' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD89D9B0>\r\n# D:\\Anaconda\\lib\\__pycache__\\bisect.cpython-37.pyc matches D:\\Anaconda\\lib\\bisect.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\bisect.cpython-37.pyc'\r\nimport '_bisect' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'bisect' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD89DF98>\r\nimport '_random' # <class '_frozen_importlib.BuiltinImporter'>\r\nimport 'random' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD894518>\r\nimport 'tempfile' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD883A58>\r\nimport 'numpy.testing._private.utils' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD85ECF8>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\__pycache__\\decorators.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\decorators.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\testing\\\\_private\\\\__pycache__\\\\decorators.cpython-37.pyc'\r\nimport 'numpy.testing._private.decorators' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8830F0>\r\n# D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\__pycache__\\nosetester.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\numpy\\testing\\_private\\nosetester.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\numpy\\\\testing\\\\_private\\\\__pycache__\\\\nosetester.cpython-37.pyc'\r\nimport 'numpy.testing._private.nosetester' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8A3FD0>\r\nimport 'numpy.testing' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD780E48>\r\nimport 'numpy' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AACCCAF28>\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__pycache__\\pywrap_tensorflow.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\__pycache__\\\\pywrap_tensorflow.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\__pycache__\\__init__.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\__init__.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\platform\\\\__pycache__\\\\__init__.cpython-37.pyc'\r\nimport 'tensorflow.python.platform' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8C5EF0>\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\__pycache__\\self_check.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\platform\\\\__pycache__\\\\self_check.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\__pycache__\\build_info.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\platform\\build_info.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\platform\\\\__pycache__\\\\build_info.cpython-37.pyc'\r\nimport 'tensorflow.python.platform.build_info' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8CE278>\r\nimport 'tensorflow.python.platform.self_check' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8C5FD0>\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__pycache__\\pywrap_dlopen_global_flags.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_dlopen_global_flags.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\__pycache__\\\\pywrap_dlopen_global_flags.cpython-37.pyc'\r\nimport 'tensorflow.python.pywrap_dlopen_global_flags' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD8CE048>\r\n# D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__pycache__\\pywrap_tensorflow_internal.cpython-37.pyc matches D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\__pycache__\\\\pywrap_tensorflow_internal.cpython-37.pyc'\r\n# D:\\Anaconda\\lib\\__pycache__\\imp.cpython-37.pyc matches D:\\Anaconda\\lib\\imp.py\r\n# code object from 'D:\\\\Anaconda\\\\lib\\\\__pycache__\\\\imp.cpython-37.pyc'\r\nimport 'imp' # <_frozen_importlib_external.SourceFileLoader object at 0x0000018AAD998FD0>\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"<frozen importlib._bootstrap>\", line 1035, in _handle_fromlist\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"D:\\Anaconda\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n# clear builtins._\r\n# clear sys.path\r\n# clear sys.argv\r\n# clear sys.ps1\r\n# clear sys.ps2\r\n# clear sys.last_type\r\n# clear sys.last_value\r\n# destroy tensorflow.python.pywrap_tensorflow_internal\r\n# clear sys.last_traceback\r\n# destroy tensorflow.python.pywrap_tensorflow\r\n# destroy tensorflow.python\r\n# destroy tensorflow\r\n# clear sys.path_hooks\r\n# clear sys.path_importer_cache\r\n# clear sys.meta_path\r\n# clear sys.__interactivehook__\r\n# clear sys.flags\r\n# clear sys.float_info\r\n# restore sys.stdin\r\n# restore sys.stdout\r\n# restore sys.stderr\r\n# cleanup[2] removing sys\r\n# cleanup[2] removing builtins\r\n# cleanup[2] removing _frozen_importlib\r\n# cleanup[2] removing _imp\r\n# cleanup[2] removing _thread\r\n# cleanup[2] removing _warnings\r\n# cleanup[2] removing _weakref\r\n# cleanup[2] removing zipimport\r\n# cleanup[2] removing _frozen_importlib_external\r\n# cleanup[2] removing _io\r\n# cleanup[2] removing marshal\r\n# cleanup[2] removing nt\r\n# cleanup[2] removing winreg\r\n# cleanup[2] removing encodings\r\n# cleanup[2] removing codecs\r\n# cleanup[2] removing _codecs\r\n# cleanup[2] removing encodings.aliases\r\n# cleanup[2] removing encodings.utf_8\r\n# cleanup[2] removing _signal\r\n# cleanup[2] removing __main__\r\n# destroy __main__\r\n# cleanup[2] removing encodings.latin_1\r\n# cleanup[2] removing io\r\n# cleanup[2] removing abc\r\n# cleanup[2] removing _abc\r\n# cleanup[2] removing site\r\n# destroy site\r\n# cleanup[2] removing os\r\n# cleanup[2] removing stat\r\n# cleanup[2] removing _stat\r\n# cleanup[2] removing ntpath\r\n# cleanup[2] removing genericpath\r\n# cleanup[2] removing os.path\r\n# cleanup[2] removing _collections_abc\r\n# cleanup[2] removing _sitebuiltins\r\n# cleanup[2] removing _bootlocale\r\n# destroy _bootlocale\r\n# cleanup[2] removing _locale\r\n# cleanup[2] removing encodings.cp1252\r\n# cleanup[2] removing types\r\n# cleanup[2] removing importlib\r\n# cleanup[2] removing importlib._bootstrap\r\n# cleanup[2] removing importlib._bootstrap_external\r\n# cleanup[2] removing warnings\r\n# cleanup[2] removing importlib.util\r\n# cleanup[2] removing importlib.abc\r\n# cleanup[2] removing importlib.machinery\r\n# cleanup[2] removing contextlib\r\n# cleanup[2] removing collections\r\n# cleanup[2] removing operator\r\n# cleanup[2] removing _operator\r\n# cleanup[2] removing keyword\r\n# destroy keyword\r\n# cleanup[2] removing heapq\r\n# cleanup[2] removing _heapq\r\n# cleanup[2] removing itertools\r\n# cleanup[2] removing reprlib\r\n# destroy reprlib\r\n# cleanup[2] removing _collections\r\n# cleanup[2] removing functools\r\n# cleanup[2] removing _functools\r\n# cleanup[2] removing mpl_toolkits\r\n# destroy mpl_toolkits\r\n# cleanup[2] removing google\r\n# destroy google\r\n# cleanup[2] removing sphinxcontrib\r\n# destroy sphinxcontrib\r\n# cleanup[2] removing __future__\r\n# destroy __future__\r\n# cleanup[2] removing distutils\r\n# destroy distutils\r\n# cleanup[2] removing inspect\r\n# destroy inspect\r\n# cleanup[2] removing dis\r\n# cleanup[2] removing opcode\r\n# destroy opcode\r\n# cleanup[2] removing _opcode\r\n# cleanup[2] removing collections.abc\r\n# cleanup[2] removing enum\r\n# cleanup[2] removing linecache\r\n# cleanup[2] removing tokenize\r\n# cleanup[2] removing re\r\n# cleanup[2] removing sre_compile\r\n# cleanup[2] removing _sre\r\n# cleanup[2] removing sre_parse\r\n# cleanup[2] removing sre_constants\r\n# destroy sre_constants\r\n# cleanup[2] removing copyreg\r\n# cleanup[2] removing token\r\n# cleanup[2] removing ctypes\r\n# cleanup[2] removing _ctypes\r\n# cleanup[2] removing struct\r\n# destroy struct\r\n# cleanup[2] removing _struct\r\n# cleanup[2] removing ctypes._endian\r\n# cleanup[2] removing traceback\r\n# cleanup[2] removing numpy\r\n# cleanup[2] removing numpy._globals\r\n# cleanup[2] removing numpy.__config__\r\n# cleanup[2] removing numpy.version\r\n# cleanup[2] removing numpy._distributor_init\r\n# cleanup[2] removing numpy._mklinit\r\n# cleanup[2] removing numpy.core\r\n# cleanup[2] removing numpy.core.info\r\n# cleanup[2] removing glob\r\n# cleanup[2] removing fnmatch\r\n# cleanup[2] removing posixpath\r\n# cleanup[2] removing numpy.core.multiarray\r\n# cleanup[2] removing numpy.core.overrides\r\n# cleanup[2] removing datetime\r\n# destroy datetime\r\n# cleanup[2] removing time\r\n# cleanup[2] removing math\r\n# cleanup[2] removing _datetime\r\n# cleanup[2] removing numpy.core._multiarray_umath\r\n# cleanup[2] removing numpy.compat\r\n# cleanup[2] removing numpy.compat._inspect\r\n# cleanup[2] removing numpy.compat.py3k\r\n# cleanup[2] removing pathlib\r\n# destroy pathlib\r\n# cleanup[2] removing errno\r\n# cleanup[2] removing urllib\r\n# destroy urllib\r\n# cleanup[2] removing urllib.parse\r\n# destroy urllib.parse\r\n# cleanup[2] removing numpy.core.umath\r\n# cleanup[2] removing numpy.core.numerictypes\r\n# cleanup[2] removing numbers\r\n# cleanup[2] removing numpy.core._string_helpers\r\n# cleanup[2] removing numpy.core._type_aliases\r\n# cleanup[2] removing numpy.core._dtype\r\n# cleanup[2] removing numpy.core.numeric\r\n# cleanup[2] removing numpy.core._internal\r\n# cleanup[2] removing pickle\r\n# cleanup[2] removing _compat_pickle\r\n# cleanup[2] removing _pickle\r\n# cleanup[2] removing numpy.core.fromnumeric\r\n# cleanup[2] removing numpy.core._methods\r\n# cleanup[2] removing numpy.core.arrayprint\r\n# cleanup[2] removing numpy.core.defchararray\r\n# cleanup[2] removing numpy.core.records\r\n# cleanup[2] removing numpy.core.memmap\r\n# destroy numpy.core.memmap\r\n# cleanup[2] removing numpy.core.function_base\r\n# cleanup[2] removing numpy.core.machar\r\n# cleanup[2] removing numpy.core.getlimits\r\n# cleanup[2] removing numpy.core.shape_base\r\n# cleanup[2] removing numpy.core.einsumfunc\r\n# cleanup[2] removing numpy.core._add_newdocs\r\n# cleanup[2] removing numpy.core._multiarray_tests\r\n# cleanup[2] removing numpy.core._dtype_ctypes\r\n# cleanup[2] removing numpy._pytesttester\r\n# cleanup[2] removing numpy.lib\r\n# cleanup[2] removing numpy.lib.info\r\n# destroy numpy.lib.info\r\n# cleanup[2] removing numpy.lib.type_check\r\n# cleanup[2] removing numpy.lib.ufunclike\r\n# cleanup[2] removing numpy.lib.index_tricks\r\n# cleanup[2] removing numpy.matrixlib\r\n# cleanup[2] removing numpy.matrixlib.defmatrix\r\n# cleanup[2] removing ast\r\n# cleanup[2] removing _ast\r\n# cleanup[2] removing numpy.linalg\r\n# cleanup[2] removing numpy.linalg.info\r\n# cleanup[2] removing numpy.linalg.linalg\r\n# cleanup[2] removing numpy.lib.twodim_base\r\n# cleanup[2] removing numpy.linalg.lapack_lite\r\n# cleanup[2] removing numpy.linalg._umath_linalg\r\n# cleanup[2] removing numpy.lib.function_base\r\n# cleanup[2] removing numpy.lib.utils\r\n# cleanup[2] removing numpy.lib.histograms\r\n# cleanup[2] removing numpy.lib.stride_tricks\r\n# cleanup[2] removing numpy.lib.mixins\r\n# cleanup[2] removing numpy.lib.nanfunctions\r\n# cleanup[2] removing numpy.lib.shape_base\r\n# cleanup[2] removing numpy.lib.scimath\r\n# cleanup[2] removing numpy.lib.polynomial\r\n# cleanup[2] removing numpy.lib.arraysetops\r\n# cleanup[2] removing numpy.lib.npyio\r\n# cleanup[2] removing weakref\r\n# cleanup[2] removing _weakrefset\r\n# destroy _weakrefset\r\n# cleanup[2] removing numpy.lib.format\r\n# cleanup[2] removing numpy.lib._datasource\r\n# cleanup[2] removing shutil\r\n# cleanup[2] removing zlib\r\n# cleanup[2] removing bz2\r\n# destroy bz2\r\n# cleanup[2] removing _compression\r\n# cleanup[2] removing threading\r\n# cleanup[2] removing _bz2\r\n# cleanup[2] removing lzma\r\n# destroy lzma\r\n# cleanup[2] removing _lzma\r\n# cleanup[2] removing numpy.lib._iotools\r\n# cleanup[2] removing numpy.lib.financial\r\n# cleanup[2] removing decimal\r\n# destroy decimal\r\n# cleanup[2] removing _decimal\r\n# cleanup[2] removing numpy.lib.arrayterator\r\n# cleanup[2] removing numpy.lib.arraypad\r\n# cleanup[2] removing numpy.lib._version\r\n# cleanup[2] removing numpy.fft\r\n# cleanup[2] removing numpy.fft.info\r\n# cleanup[2] removing numpy.fft.fftpack\r\n# cleanup[2] removing numpy.fft.fftpack_lite\r\n# cleanup[2] removing numpy.fft.helper\r\n# cleanup[2] removing mkl_fft\r\n# destroy mkl_fft\r\n# cleanup[2] removing mkl_fft._pydfti\r\n# cleanup[2] removing _cython_0_29_7\r\n# destroy _cython_0_29_7\r\n# cleanup[2] removing cython_runtime\r\n# cleanup[2] removing mkl_fft._version\r\n# destroy mkl_fft._version\r\n# cleanup[2] removing mkl_fft._numpy_fft\r\n# destroy mkl_fft._numpy_fft\r\n# cleanup[2] removing numpy.polynomial\r\n# cleanup[2] removing numpy.polynomial.polynomial\r\n# cleanup[2] removing numpy.polynomial.polyutils\r\n# cleanup[2] removing numpy.polynomial._polybase\r\n# cleanup[2] removing numpy.polynomial.chebyshev\r\n# cleanup[2] removing numpy.polynomial.legendre\r\n# cleanup[2] removing numpy.polynomial.hermite\r\n# cleanup[2] removing numpy.polynomial.hermite_e\r\n# cleanup[2] removing numpy.polynomial.laguerre\r\n# cleanup[2] removing numpy.random\r\n# cleanup[2] removing numpy.random.mtrand\r\n# cleanup[2] removing mtrand\r\n# cleanup[2] removing numpy.ctypeslib\r\n# cleanup[2] removing numpy.ma\r\n# cleanup[2] removing numpy.ma.core\r\n# cleanup[2] removing textwrap\r\n# cleanup[2] removing numpy.ma.extras\r\n# cleanup[2] removing numpy.testing\r\n# cleanup[2] removing unittest\r\n# cleanup[2] removing unittest.result\r\n# cleanup[2] removing unittest.util\r\n# cleanup[2] removing unittest.case\r\n# cleanup[2] removing difflib\r\n# cleanup[2] removing logging\r\n# cleanup[2] removing string\r\n# destroy string\r\n# cleanup[2] removing _string\r\n# cleanup[2] removing atexit\r\n# cleanup[2] removing pprint\r\n# cleanup[2] removing unittest.suite\r\n# cleanup[2] removing unittest.loader\r\n# cleanup[2] removing unittest.main\r\n# destroy unittest.main\r\n# cleanup[2] removing argparse\r\n# cleanup[2] removing gettext\r\n# destroy gettext\r\n# cleanup[2] removing locale\r\n# cleanup[2] removing unittest.runner\r\n# cleanup[2] removing unittest.signals\r\n# cleanup[2] removing signal\r\n# cleanup[2] removing numpy.testing._private\r\n# cleanup[2] removing numpy.testing._private.utils\r\n# cleanup[2] removing gc\r\n# cleanup[2] removing tempfile\r\n# destroy tempfile\r\n# cleanup[2] removing random\r\n# destroy random\r\n# cleanup[2] removing hashlib\r\n# destroy hashlib\r\n# cleanup[2] removing _hashlib\r\n# cleanup[2] removing _blake2\r\n# cleanup[2] removing _sha3\r\n# cleanup[2] removing bisect\r\n# cleanup[2] removing _bisect\r\n# cleanup[2] removing _random\r\n# cleanup[2] removing numpy.testing._private.decorators\r\n# cleanup[2] removing numpy.testing._private.nosetester\r\n# cleanup[2] removing tensorflow.python.platform\r\n# destroy tensorflow.python.platform\r\n# cleanup[2] removing tensorflow.python.platform.self_check\r\n# destroy tensorflow.python.platform.self_check\r\n# cleanup[2] removing tensorflow.python.platform.build_info\r\n# cleanup[2] removing tensorflow.python.pywrap_dlopen_global_flags\r\n# destroy tensorflow.python.pywrap_dlopen_global_flags\r\n# cleanup[2] removing imp\r\n# destroy imp\r\n# destroy _sha3\r\n# destroy _blake2\r\n# destroy _bz2\r\n# destroy _ast\r\n# destroy _datetime\r\n# destroy zipimport\r\n# destroy _sitebuiltins\r\n# destroy importlib.abc\r\n# destroy dis\r\n# destroy token\r\n# destroy _opcode\r\n# destroy zlib\r\n# destroy _compression\r\n# destroy _lzma\r\n# destroy decimal\r\n# destroy tensorflow.python.platform.build_info\r\n# destroy importlib.machinery\r\n# destroy importlib.util\r\n# destroy importlib\r\n# cleanup[3] wiping _frozen_importlib\r\n# destroy _frozen_importlib_external\r\n# cleanup[3] wiping _imp\r\n# cleanup[3] wiping _thread\r\n# cleanup[3] wiping _warnings\r\n# cleanup[3] wiping _weakref\r\n# cleanup[3] wiping _io\r\n# cleanup[3] wiping marshal\r\n# cleanup[3] wiping nt\r\n# cleanup[3] wiping winreg\r\n# cleanup[3] wiping encodings\r\n# destroy encodings.aliases\r\n# destroy encodings.utf_8\r\n# destroy encodings.latin_1\r\n# destroy encodings.cp1252\r\n# cleanup[3] wiping codecs\r\n# cleanup[3] wiping _codecs\r\n# cleanup[3] wiping _signal\r\n# cleanup[3] wiping io\r\n# cleanup[3] wiping abc\r\n# cleanup[3] wiping _abc\r\n# cleanup[3] wiping os\r\n# cleanup[3] wiping stat\r\n# cleanup[3] wiping _stat\r\n# cleanup[3] wiping ntpath\r\n# cleanup[3] wiping genericpath\r\n# cleanup[3] wiping os.path\r\n# cleanup[3] wiping _collections_abc\r\n# cleanup[3] wiping _locale\r\n# cleanup[3] wiping types\r\n# cleanup[3] wiping importlib._bootstrap\r\n# cleanup[3] wiping warnings\r\n# cleanup[3] wiping contextlib\r\n# cleanup[3] wiping collections\r\n# destroy heapq\r\n# cleanup[3] wiping operator\r\n# cleanup[3] wiping _operator\r\n# cleanup[3] wiping _heapq\r\n# cleanup[3] wiping itertools\r\n# cleanup[3] wiping _collections\r\n# destroy _collections\r\n# cleanup[3] wiping functools\r\n# destroy _abc\r\n# cleanup[3] wiping _functools\r\n# cleanup[3] wiping collections.abc\r\n# cleanup[3] wiping enum\r\n# cleanup[3] wiping linecache\r\n# destroy tokenize\r\n# cleanup[3] wiping re\r\n# destroy enum\r\n# destroy sre_compile\r\n# destroy copyreg\r\n# cleanup[3] wiping _sre\r\n# cleanup[3] wiping sre_parse\r\n# cleanup[3] wiping ctypes\r\n# destroy ctypes._endian\r\n# cleanup[3] wiping _ctypes\r\n# cleanup[3] wiping _struct\r\n# cleanup[3] wiping traceback\r\n# destroy linecache\r\n# cleanup[3] wiping numpy\r\n# destroy numpy._globals\r\n# destroy numpy._distributor_init\r\n# destroy mklinit\r\n# destroy numpy._pytesttester\r\n# destroy numpy.__config__\r\n# destroy numpy.version\r\n# destroy numpy.compat\r\n# destroy numpy.compat._inspect\r\n# destroy numpy.compat.py3k\r\n# destroy numpy.core\r\n# destroy numpy.lib\r\n# destroy numpy.lib.ufunclike\r\n# destroy numpy.lib.type_check\r\n# destroy numpy.lib.twodim_base\r\n# destroy numpy.lib.utils\r\n# destroy numpy.lib.histograms\r\n# destroy numpy.lib.stride_tricks\r\n# destroy numpy.lib.index_tricks\r\n# destroy numpy.lib.mixins\r\n# destroy numpy.lib.nanfunctions\r\n# destroy numpy.lib.shape_base\r\n# destroy numpy.lib.polynomial\r\n# destroy numpy.lib.arraysetops\r\n# destroy numpy.lib._datasource\r\n# destroy numpy.lib._iotools\r\n# destroy numpy.lib.npyio\r\n# destroy numpy.lib.financial\r\n# destroy numpy.lib.arrayterator\r\n# destroy numpy.lib.arraypad\r\n# destroy numpy.lib._version\r\n# destroy numpy.lib.scimath\r\n# destroy numpy.fft\r\n# destroy numpy.polynomial\r\n# destroy numpy.polynomial._polybase\r\n# destroy numpy.polynomial.polynomial\r\n# destroy numpy.polynomial.chebyshev\r\n# destroy numpy.polynomial.legendre\r\n# destroy numpy.polynomial.hermite\r\n# destroy numpy.polynomial.hermite_e\r\n# destroy numpy.polynomial.laguerre\r\n# destroy numpy.random\r\n# destroy numpy.ctypeslib\r\n# destroy numpy.ma\r\n# destroy numpy.ma.extras\r\n# destroy numpy.testing\r\n# destroy numpy.testing._private\r\n# destroy numpy.testing._private.utils\r\n# destroy numpy.testing._private.nosetester\r\n# destroy numpy.testing._private.decorators\r\n# cleanup[3] wiping numpy.core.info\r\n# cleanup[3] wiping glob\r\n# cleanup[3] wiping fnmatch\r\n# cleanup[3] wiping posixpath\r\n# destroy genericpath\r\n# cleanup[3] wiping numpy.core.multiarray\r\n# cleanup[3] wiping numpy.core.overrides\r\n# cleanup[3] wiping time\r\n# cleanup[3] wiping math\r\n# cleanup[3] wiping numpy.core._multiarray_umath\r\n# cleanup[3] wiping errno\r\n# cleanup[3] wiping numpy.core.umath\r\n# cleanup[3] wiping numpy.core.numerictypes\r\n# cleanup[3] wiping numbers\r\n# cleanup[3] wiping numpy.core._string_helpers\r\n# cleanup[3] wiping numpy.core._type_aliases\r\n# cleanup[3] wiping numpy.core._dtype\r\n# cleanup[3] wiping numpy.core.numeric\r\n# cleanup[3] wiping numpy.core._internal\r\n# cleanup[3] wiping pickle\r\n# destroy _compat_pickle\r\n# destroy _struct\r\n# cleanup[3] wiping _pickle\r\n# destroy _pickle\r\n# cleanup[3] wiping numpy.core.fromnumeric\r\n# cleanup[3] wiping numpy.core._methods\r\n# cleanup[3] wiping numpy.core.arrayprint\r\n# cleanup[3] wiping numpy.core.defchararray\r\n# cleanup[3] wiping numpy.core.records\r\n# cleanup[3] wiping numpy.core.function_base\r\n# cleanup[3] wiping numpy.core.machar\r\n# cleanup[3] wiping numpy.core.getlimits\r\n# cleanup[3] wiping numpy.core.shape_base\r\n# cleanup[3] wiping numpy.core.einsumfunc\r\n# cleanup[3] wiping numpy.core._add_newdocs\r\n# cleanup[3] wiping numpy.core._multiarray_tests\r\n# cleanup[3] wiping numpy.core._dtype_ctypes\r\n# cleanup[3] wiping numpy.matrixlib\r\n# destroy numpy.matrixlib.defmatrix\r\n# cleanup[3] wiping ast\r\n# cleanup[3] wiping numpy.linalg\r\n# destroy numpy.linalg.info\r\n# destroy numpy.linalg.linalg\r\n# cleanup[3] wiping numpy.linalg.lapack_lite\r\n# cleanup[3] wiping numpy.linalg._umath_linalg\r\n# cleanup[3] wiping numpy.lib.function_base\r\n# cleanup[3] wiping weakref\r\n# cleanup[3] wiping numpy.lib.format\r\n# cleanup[3] wiping shutil\r\n# destroy stat\r\n# cleanup[3] wiping threading\r\n# cleanup[3] wiping numpy.fft.info\r\n# cleanup[3] wiping numpy.fft.fftpack\r\n# cleanup[3] wiping numpy.fft.fftpack_lite\r\n# cleanup[3] wiping numpy.fft.helper\r\n# cleanup[3] wiping mkl_fft._pydfti\r\n# cleanup[3] wiping cython_runtime\r\n# cleanup[3] wiping numpy.polynomial.polyutils\r\n# cleanup[3] wiping numpy.random.mtrand\r\n# cleanup[3] wiping mtrand\r\n# cleanup[3] wiping numpy.ma.core\r\n# destroy textwrap\r\n# cleanup[3] wiping unittest\r\n# destroy unittest.signals\r\n# cleanup[3] wiping unittest.result\r\n# cleanup[3] wiping unittest.util\r\n# cleanup[3] wiping unittest.case\r\n# destroy difflib\r\n# destroy logging\r\n# cleanup[3] wiping _string\r\n# cleanup[3] wiping atexit\r\n# cleanup[3] wiping pprint\r\n# cleanup[3] wiping unittest.suite\r\n# cleanup[3] wiping unittest.loader\r\n# destroy unittest.case\r\n# destroy unittest.suite\r\n# destroy unittest.util\r\n# cleanup[3] wiping argparse\r\n# cleanup[3] wiping locale\r\n# destroy _collections_abc\r\n# destroy encodings\r\n# destroy _locale\r\n# cleanup[3] wiping unittest.runner\r\n# destroy unittest.result\r\n# cleanup[3] wiping signal\r\n# destroy _signal\r\n# cleanup[3] wiping gc\r\n# cleanup[3] wiping _hashlib\r\n# cleanup[3] wiping bisect\r\n# cleanup[3] wiping _bisect\r\n# destroy _bisect\r\n# cleanup[3] wiping _random\r\n# cleanup[3] wiping sys\r\n# cleanup[3] wiping builtins\r\n# destroy _heapq\r\n# destroy numpy.core.info\r\n# destroy glob\r\n# destroy numpy.core.multiarray\r\n# destroy numpy.core._string_helpers\r\n# destroy numpy.core._dtype\r\n# destroy numpy.core._type_aliases\r\n# destroy numpy.core._internal\r\n# destroy numpy.core._methods\r\n# destroy numpy.core.fromnumeric\r\n# destroy numpy.core.arrayprint\r\n# destroy numpy.core.defchararray\r\n# destroy numpy.core.records\r\n# destroy numpy.core.function_base\r\n# destroy numpy.core.machar\r\n# destroy numpy.core.getlimits\r\n# destroy numpy.core.shape_base\r\n# destroy numpy.core.einsumfunc\r\n# destroy numpy.core._multiarray_tests\r\n# destroy numpy.core._add_newdocs\r\n# destroy numpy.core._dtype_ctypes\r\n# destroy numpy.lib.format\r\n# destroy pickle\r\n# destroy numpy.core.numerictypes\r\n# destroy numpy.matrixlib\r\n# destroy numpy.lib.function_base\r\n# destroy numpy.fft.info\r\n# destroy numpy.fft.fftpack_lite\r\n# destroy numpy.fft.helper\r\n# destroy numpy.fft.fftpack\r\n# destroy ctypes\r\n# destroy ast\r\n# destroy _sre\r\n# destroy sre_parse\r\n# destroy types\r\n# destroy _operator\r\n# destroy _functools\r\n# destroy abc\r\n# destroy numpy.core.numeric\r\n# destroy numpy.core.overrides\r\n# destroy numpy.linalg.lapack_lite\r\n# destroy numpy.linalg._umath_linalg\r\n# destroy numbers\r\n# destroy numpy.linalg\r\n# destroy numpy.polynomial.polyutils\r\n# destroy numpy.ma.core\r\n# destroy numpy.core.umath\r\n# destroy gc\r\n# destroy operator\r\n# destroy contextlib\r\n# destroy pprint\r\n# destroy numpy.core._multiarray_umath\r\n# destroy unittest\r\n# destroy collections.abc\r\n# destroy numpy\r\n# destroy fnmatch\r\n# destroy _stat\r\n# destroy functools\r\n# destroy shutil\r\n# destroy errno\r\n# destroy ntpath\r\n# destroy posixpath\r\n# destroy io\r\n# destroy traceback\r\n# destroy warnings\r\n# destroy threading\r\n# destroy atexit\r\n# destroy signal\r\n# destroy weakref\r\n# destroy time\r\n# destroy math\r\n# destroy _hashlib\r\n# destroy bisect\r\n# destroy _random\r\n# destroy argparse\r\n# destroy unittest.loader\r\n# destroy unittest.runner\r\n# destroy collections\r\n# destroy itertools\r\n# destroy _string\r\n# destroy locale\r\n# destroy os\r\n# destroy re\r\n\r\nand output of pip debug --verbose\r\n\r\nERROR: unknown command \"debug\"", "> Does your CPU support AVX? See also #32352\r\n\r\nSee the below info which says that it supports the AVX\r\n\r\n\r\nCoreinfo v3.31 - Dump information on system CPU and memory topology\r\nCopyright (C) 2008-2014 Mark Russinovich\r\nSysinternals - www.sysinternals.com\r\n\r\nIntel(R) Core(TM) i7-9750H CPU @ 2.60GHz\r\nIntel64 Family 6 Model 158 Stepping 10, GenuineIntel\r\nMicrocode signature: 000000AA\r\nHTT             *       Hyperthreading enabled\r\nHYPERVISOR      -       Hypervisor is present\r\nVMX             *       Supports Intel hardware-assisted virtualization\r\nSVM             -       Supports AMD hardware-assisted virtualization\r\nX64             *       Supports 64-bit mode\r\n\r\nSMX             -       Supports Intel trusted execution\r\nSKINIT          -       Supports AMD SKINIT\r\n\r\nNX              *       Supports no-execute page protection\r\nSMEP            *       Supports Supervisor Mode Execution Prevention\r\nSMAP            *       Supports Supervisor Mode Access Prevention\r\nPAGE1GB         *       Supports 1 GB large pages\r\nPAE             *       Supports > 32-bit physical addresses\r\nPAT             *       Supports Page Attribute Table\r\nPSE             *       Supports 4 MB pages\r\nPSE36           *       Supports > 32-bit address 4 MB pages\r\nPGE             *       Supports global bit in page tables\r\nSS              *       Supports bus snooping for cache operations\r\nVME             *       Supports Virtual-8086 mode\r\nRDWRFSGSBASE    *       Supports direct GS/FS base access\r\n\r\nFPU             *       Implements i387 floating point instructions\r\nMMX             *       Supports MMX instruction set\r\nMMXEXT          -       Implements AMD MMX extensions\r\n3DNOW           -       Supports 3DNow! instructions\r\n3DNOWEXT        -       Supports 3DNow! extension instructions\r\nSSE             *       Supports Streaming SIMD Extensions\r\nSSE2            *       Supports Streaming SIMD Extensions 2\r\nSSE3            *       Supports Streaming SIMD Extensions 3\r\nSSSE3           *       Supports Supplemental SIMD Extensions 3\r\nSSE4a           -       Supports Streaming SIMDR Extensions 4a\r\nSSE4.1          *       Supports Streaming SIMD Extensions 4.1\r\nSSE4.2          *       Supports Streaming SIMD Extensions 4.2\r\n\r\nAES             *       Supports AES extensions\r\nAVX             *       Supports AVX intruction extensions\r\nFMA             *       Supports FMA extensions using YMM state\r\nMSR             *       Implements RDMSR/WRMSR instructions\r\nMTRR            *       Supports Memory Type Range Registers\r\nXSAVE           *       Supports XSAVE/XRSTOR instructions\r\nOSXSAVE         *       Supports XSETBV/XGETBV instructions\r\nRDRAND          *       Supports RDRAND instruction\r\nRDSEED          *       Supports RDSEED instruction\r\n\r\nCMOV            *       Supports CMOVcc instruction\r\nCLFSH           *       Supports CLFLUSH instruction\r\nCX8             *       Supports compare and exchange 8-byte instructions\r\nCX16            *       Supports CMPXCHG16B instruction\r\nBMI1            *       Supports bit manipulation extensions 1\r\nBMI2            *       Supports bit manipulation extensions 2\r\nADX             *       Supports ADCX/ADOX instructions\r\nDCA             -       Supports prefetch from memory-mapped device\r\nF16C            *       Supports half-precision instruction\r\nFXSR            *       Supports FXSAVE/FXSTOR instructions\r\nFFXSR           -       Supports optimized FXSAVE/FSRSTOR instruction\r\nMONITOR         *       Supports MONITOR and MWAIT instructions\r\nMOVBE           *       Supports MOVBE instruction\r\nERMSB           *       Supports Enhanced REP MOVSB/STOSB\r\nPCLMULDQ        *       Supports PCLMULDQ instruction\r\nPOPCNT          *       Supports POPCNT instruction\r\nLZCNT           *       Supports LZCNT instruction\r\nSEP             *       Supports fast system call instructions\r\nLAHF-SAHF       *       Supports LAHF/SAHF instructions in 64-bit mode\r\nHLE             -       Supports Hardware Lock Elision instructions\r\nRTM             -       Supports Restricted Transactional Memory instructions\r\n\r\nDE              *       Supports I/O breakpoints including CR4.DE\r\nDTES64          *       Can write history of 64-bit branch addresses\r\nDS              *       Implements memory-resident debug buffer\r\nDS-CPL          *       Supports Debug Store feature with CPL\r\nPCID            *       Supports PCIDs and settable CR4.PCIDE\r\nINVPCID         *       Supports INVPCID instruction\r\nPDCM            *       Supports Performance Capabilities MSR\r\nRDTSCP          *       Supports RDTSCP instruction\r\nTSC             *       Supports RDTSC instruction\r\nTSC-DEADLINE    *       Local APIC supports one-shot deadline timer\r\nTSC-INVARIANT   *       TSC runs at constant rate\r\nxTPR            *       Supports disabling task priority messages\r\n\r\nEIST            *       Supports Enhanced Intel Speedstep\r\nACPI            *       Implements MSR for power management\r\nTM              *       Implements thermal monitor circuitry\r\nTM2             *       Implements Thermal Monitor 2 control\r\nAPIC            *       Implements software-accessible local APIC\r\nx2APIC          *       Supports x2APIC\r\n\r\nCNXT-ID         -       L1 data cache mode adaptive or BIOS\r\n\r\nMCE             *       Supports Machine Check, INT18 and CR4.MCE\r\nMCA             *       Implements Machine Check Architecture\r\nPBE             *       Supports use of FERR#/PBE# pin\r\n\r\nPSN             -       Implements 96-bit processor serial number\r\n\r\nPREFETCHW       *       Supports PREFETCHW instruction\r\n\r\nMaximum implemented CPUID leaves: 00000016 (Basic), 80000008 (Extended).\r\n\r\nLogical to Physical Processor Map:\r\n**----------  Physical Processor 0 (Hyperthreaded)\r\n--**--------  Physical Processor 1 (Hyperthreaded)\r\n----**------  Physical Processor 2 (Hyperthreaded)\r\n------**----  Physical Processor 3 (Hyperthreaded)\r\n--------**--  Physical Processor 4 (Hyperthreaded)\r\n----------**  Physical Processor 5 (Hyperthreaded)\r\n\r\nLogical Processor to Socket Map:\r\n************  Socket 0\r\n\r\nLogical Processor to NUMA Node Map:\r\n************  NUMA Node 0\r\n\r\nNo NUMA nodes.\r\n\r\nLogical Processor to Cache Map:\r\n**----------  Data Cache          0, Level 1,   32 KB, Assoc   8, LineSize  64\r\n**----------  Instruction Cache   0, Level 1,   32 KB, Assoc   8, LineSize  64\r\n**----------  Unified Cache       0, Level 2,  256 KB, Assoc   4, LineSize  64\r\n************  Unified Cache       1, Level 3,   12 MB, Assoc  16, LineSize  64\r\n--**--------  Data Cache          1, Level 1,   32 KB, Assoc   8, LineSize  64\r\n--**--------  Instruction Cache   1, Level 1,   32 KB, Assoc   8, LineSize  64\r\n--**--------  Unified Cache       2, Level 2,  256 KB, Assoc   4, LineSize  64\r\n----**------  Data Cache          2, Level 1,   32 KB, Assoc   8, LineSize  64\r\n----**------  Instruction Cache   2, Level 1,   32 KB, Assoc   8, LineSize  64\r\n----**------  Unified Cache       3, Level 2,  256 KB, Assoc   4, LineSize  64\r\n------**----  Data Cache          3, Level 1,   32 KB, Assoc   8, LineSize  64\r\n------**----  Instruction Cache   3, Level 1,   32 KB, Assoc   8, LineSize  64\r\n------**----  Unified Cache       4, Level 2,  256 KB, Assoc   4, LineSize  64\r\n--------**--  Data Cache          4, Level 1,   32 KB, Assoc   8, LineSize  64\r\n--------**--  Instruction Cache   4, Level 1,   32 KB, Assoc   8, LineSize  64\r\n--------**--  Unified Cache       5, Level 2,  256 KB, Assoc   4, LineSize  64\r\n----------**  Data Cache          5, Level 1,   32 KB, Assoc   8, LineSize  64\r\n----------**  Instruction Cache   5, Level 1,   32 KB, Assoc   8, LineSize  64\r\n----------**  Unified Cache       6, Level 2,  256 KB, Assoc   4, LineSize  64\r\n\r\nLogical Processor to Group Map:\r\n************  Group 0", "Please repost using proper markdown format as it is hard to read this way.\r\n\r\nIf blocks of code/error are short, prefix them with a line containing 3 backticks and end the block with a line containing 3 backticks again.\r\n\r\nIf they are long, please use the attach functionality to attach a plaintext or post a link to a gist.", "Pleas\r\n\r\n> Please repost using proper markdown format as it is hard to read this way.\r\n> \r\n> If blocks of code/error are short, prefix them with a line containing 3 backticks and end the block with a line containing 3 backticks again.\r\n> \r\n> If they are long, please use the attach functionality to attach a plaintext or post a link to a gist.\r\n\r\nPlease find attached the Output of python -v -c \"import tensorflow\" in the file named as \"Output of python import tensorflow.txt\"", "[Outout of python import tensorflow.txt](https://github.com/tensorflow/tensorflow/files/3628896/Outout.of.python.import.tensorflow.txt)\r\n", "> lease find attached the Output of python -v -c \"import tensorflow\" in the file named as \"Output of python import tensorflow.txt\"\r\n\r\nIf you need any other information,please let me know. ", "TF 1.14 supports cuda 10.0 Please switch to cuda 10.0 and update cuda paths.\r\nSee [software requirements](https://www.tensorflow.org/install/gpu#software_requirements).\r\n\r\nPlease see if #28848 helps you.Thanks!", "> TF 1.14 supports cuda 10.0 Please switch to cuda 10.0 and update cuda paths.\r\n> See [software requirements](https://www.tensorflow.org/install/gpu#software_requirements).\r\n> \r\n> Please see if #28848 helps you.Thanks!\r\nI have already installed CUDA 10.1 Update 2. Can you help me with the steps how can we downgrade to CUDA 10.0. \r\n", "After following mentioned steps l am able to import the tensorflow successfully. But for small dataset itself I am getting the following error:\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[50,8,20,100,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node training_1/Adam/gradients/max_pooling3d_4/MaxPool3D_grad/MaxPool3DGrad}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nI have tried various sizes for this but this error is not resolving. Please help.\r\n\r\nPlease guide how to solve this error.\r\n", "`50 * 8 * 20 * 100 * 100 * sizeof(float) ~= 300MB` Try reducing shape parameters or using block methods. But then this is no longer an issue with TF code, should get asked on StackOverflow instead.\r\n\r\nClosing this issue as the original question has been resolved, import succeeds", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32625\">No</a>\n"]}, {"number": 32624, "title": "Inference in C++ Tensorflow: Session->Run hangs when ran with input and output parameters supplied", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source C++ API\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):0.18.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI trained a FasterRCNN for point detection using TensorFlow and its python API. I was able to do the inference successfully in the same without any issues.\r\nNow for production requirements, I need to build a standalone application or library which can perform the inference in C++.\r\n\r\nIn order to achieve that I successfully managed to build the TensorFlow library from scratch for C++ API and ran a few test programs.\r\n\r\nSteps followed for inference on my trained model:\r\n\r\n 1. Method #1: freeze the graph to *.pb file using modified [freeze_graph.py][1] and restore it.\r\n 2. Method #2: Directly restore from checkpoint using MetaGraphDef and ReadBinaryProto.\r\n\r\nI tried both the methods and the loading of the model seems to happen without any error. I printed the node names as well.\r\n\r\nAfter loading the graph I created a handler for image input, previously the input during training was getting handled by QueueRunner, but because it's just inference I created the handler myself with appropriate modifications to it as can be seen below.\r\n\r\n        Status fasterRcnn::CreateGraphForImage(bool unstack)\r\n    {\r\n        fileNameVar=Placeholder(iRoot.WithOpName(\"input\"),DT_STRING);\r\n        auto fileReader=ReadFile(iRoot.WithOpName(\"fileReader\"),fileNameVar);\r\n        auto imageReader=DecodeJpeg(iRoot.WithOpName(\"jpegReader\"),fileReader,DecodeJpeg::Channels(imageChannels));\r\n        auto floatCaster=Cast(iRoot.WithOpName(\"floatCaster\"),imageReader,DT_FLOAT);\r\n        auto dimsExpander=ExpandDims(iRoot.WithOpName(\"dim\"),floatCaster,0);\r\n        //auto resized=ResizeBilinear(iRoot.WithOpName(\"size\"),dimsExpander,Const(iRoot,{imageSide,imageSide}));\r\n        //auto div=Div(iRoot.WithOpName(\"normalized\"),resized,{255.f});\r\n        imageTensorVar=dimsExpander;//div;\r\n        return iRoot.status();\r\n\r\n    }\r\nNOTE: iRoot is the private variable of fasterRcnn class-> Scope used by the graph for loading images into tensors\r\n\r\n \r\n\r\n       class fasterRcnn\r\n    {\r\n    private:\r\n        Scope iRoot;//graph for loading images into tensors\r\n        const int imageSide;\r\n        const int imageChannels;//rgb\r\n        //load image vars\r\n        Output fileNameVar;\r\n        Output imageTensorVar;\r\n        //\r\n        std::unique_ptr<Session> fSession;//file session\r\n        std::unique_ptr<GraphDef> graphDef;\r\n    public:\r\n        fasterRcnn(int side,int channels):iRoot(Scope::NewRootScope()),imageSide(side),imageChannels(channels){}\r\n        Status CreateGraphForImage(bool unstack);\r\n        Status LoadSavedModel(string &fileName);//via frozen graph\r\n        Status LoadSavedModel(std::string graph_fn,std::string checkpoint_fn);//via checkpoints\r\n        Status PredictFromFrozen(Tensor &image,int&results);//\r\n        Status ReadTensorFromImageFile(string& file_name, Tensor& outTensor);\r\n        Status ReadFileTensors(string& folder_name,vector<Tensor>& file_tensors);\r\n    \r\n    };\r\n\r\n**Now the when the time comes for Prediction when I run Session->Run with particular input node name and output node name the program hangs**:\r\n\r\n    Status fasterRcnn::PredictFromFrozen(Tensor& image, int& result)\r\n    {\r\n        vector<Tensor> outTensors;\r\n        cout<<\"Prediction about to start\"<<endl;\r\n        TF_CHECK_OK(fSession->Run({{\"fasterrcnn/truncated_base_network/sub\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors));\r\n        cout<<\"Prediction done\"<<endl;\r\n        auto output_c = outTensors[0].scalar<float>();\r\n        for (int i=0;i<outTensors.size();i++)\r\n        {\r\n            cout << \"Output dimension of the image\" << outTensors[i].DebugString()<<\"\\n\";\r\n        }\r\n        cout << \"Output dimension of the image\" << outTensors[0].DebugString()<<\"\\n\";\r\n        return Status::OK();\r\n    }\r\n\r\n**TF_CHECK_OK(fSession->Run({{\"fasterrcnn/truncated_base_network/sub\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors));**\r\n\r\n***It hangs at the above statement.***\r\n\r\n**Up until a few Debug thoughts:**\r\n\r\n 1. I thought maybe the QueueRunner might be creating the issue, but according to [this answer][2] I handled that as I created the image handler graph.\r\n 2. I thought maybe I didn't freeze the graph well, that's I loaded directly from the checkpoints, but still, it hangs.\r\n 3. The input to the VGG_16 truncated network in graph expects a 4D float tensor, which I am providing, double-checked that.\r\n\r\n\r\n\r\n**Links to Code and Graph:**\r\n\r\n 1. [CodeBase and GraphPng and GIF hosted on GDrive][3]\r\n\r\n**Question:**\r\n\r\n 1. Is there any step that I missing, for successful inferencing in C++.\r\n 2. How to verify if my input and output node names are correct? I referred to the python code and they seem right.\r\n 3. Any steps for debugging that I should follow to provide more info on this?\r\n\r\n  [1]: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\r\n  [2]: https://stackoverflow.com/a/35346656/8250471\r\n  [3]: https://drive.google.com/drive/folders/1iO7JIitD_BfHnnLqgzvUygFwAAW6v_5G?usp=sharing\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@amitpandey2194 ,\r\nThanks for reporting the issue, can you please provide a standalone code so that we can replicate the  same from our end ?Thanks!", "predicting.h #headerfile\r\n```\r\n#include <iostream>\r\n#include <map>\r\n#include <string>\r\n#include <fstream>\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/cc/framework/gradients.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/contrib/tensorboard/db/summary_file_writer.h\"\r\n#include \"tensorflow/cc/tools/freeze_saved_model.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include <tensorflow/core/protobuf/meta_graph.pb.h>\r\nusing namespace std;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\n\r\nclass fasterRcnn\r\n{\r\nprivate:\r\n    Scope iRoot;//graph for loading images into tensors\r\n    const int imageSide;\r\n    const int imageChannels;//rgb\r\n    //load image vars\r\n    Output fileNameVar;\r\n    Output imageTensorVar;\r\n    //\r\n    std::unique_ptr<Session> fSession;//file session\r\n    std::unique_ptr<GraphDef> graphDef;\r\npublic:\r\n    fasterRcnn(int side,int channels):iRoot(Scope::NewRootScope()),imageSide(side),imageChannels(channels){}\r\n    Status CreateGraphForImage(bool unstack);\r\n    Status LoadSavedModel(string &fileName);//via frozen graph\r\n    Status LoadSavedModel(std::string graph_fn,std::string checkpoint_fn);//via checkpoints\r\n    Status PredictFromFrozen(Tensor &image,int&results);//\r\n    Status ReadTensorFromImageFile(string& file_name, Tensor& outTensor);\r\n    Status ReadFileTensors(string& folder_name,vector<Tensor>& file_tensors);\r\n\r\n};\r\n\r\n```\r\n#predicting.cpp #definition file\r\n```\r\n#include \"predicting.h\"\r\ntypedef std::vector<std::pair<std::string, tensorflow::Tensor>> tensor_dict;\r\nStatus fasterRcnn::CreateGraphForImage(bool unstack)\r\n{\r\n    fileNameVar=Placeholder(iRoot.WithOpName(\"input\"),DT_STRING);\r\n    auto fileReader=ReadFile(iRoot.WithOpName(\"fileReader\"),fileNameVar);\r\n    auto imageReader=DecodeJpeg(iRoot.WithOpName(\"jpegReader\"),fileReader,DecodeJpeg::Channels(imageChannels));\r\n    auto floatCaster=Cast(iRoot.WithOpName(\"floatCaster\"),imageReader,DT_FLOAT);\r\n    auto dimsExpander=ExpandDims(iRoot.WithOpName(\"dim\"),floatCaster,0);\r\n    //auto resized=ResizeBilinear(iRoot.WithOpName(\"size\"),dimsExpander,Const(iRoot,{imageSide,imageSide}));\r\n    //auto div=Div(iRoot.WithOpName(\"normalized\"),resized,{255.f});\r\n    imageTensorVar=dimsExpander;//div;\r\n    return iRoot.status();\r\n}\r\nStatus fasterRcnn::LoadSavedModel(string& fileName)\r\n{\r\n\r\n    SessionOptions options;\r\n    fSession.reset(NewSession(options));\r\n    graphDef.reset(new GraphDef);\r\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(),fileName,graphDef.get()));\r\n    return fSession->Create(*graphDef.get());\r\n}\r\nStatus fasterRcnn::LoadSavedModel(std::string graph_fn,std::string checkpoint_fn)\r\n{\r\n  tensorflow::Status status;\r\n  SessionOptions options;\r\n  fSession.reset(NewSession(options));\r\n     // Read in the protobuf graph we exported\r\n  tensorflow::MetaGraphDef graph_def;\r\n  status = ReadBinaryProto(tensorflow::Env::Default(), graph_fn, &graph_def);\r\n  if (status != tensorflow::Status::OK()) return status;\r\n\r\n  // create the graph in the current session\r\n  status = fSession->Create(graph_def.graph_def());\r\n  if (status != tensorflow::Status::OK()) return status;\r\n\r\n  // restore model from checkpoint, iff checkpoint is given\r\n  if (checkpoint_fn != \"\") {\r\n    const std::string restore_op_name = graph_def.saver_def().restore_op_name();\r\n    const std::string filename_tensor_name =\r\n        graph_def.saver_def().filename_tensor_name();\r\n\r\n    tensorflow::Tensor filename_tensor(tensorflow::DT_STRING,\r\n                                       tensorflow::TensorShape());\r\n    filename_tensor.scalar<std::string>()() = checkpoint_fn;\r\n\r\n    tensor_dict feed_dict = {{filename_tensor_name, filename_tensor}};\r\n    status = fSession->Run(feed_dict, {}, {restore_op_name}, nullptr);\r\n    if (status != tensorflow::Status::OK()) return status;\r\n  } else {\r\n    // virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,\r\n    //                  const std::vector<string>& output_tensor_names,\r\n    //                  const std::vector<string>& target_node_names,\r\n    //                  std::vector<Tensor>* outputs) = 0;\r\n    status = fSession->Run({}, {}, {\"init\"}, nullptr);\r\n    if (status != tensorflow::Status::OK()) return status;\r\n  }\r\n\r\n  return tensorflow::Status::OK();\r\n}\r\nStatus fasterRcnn::PredictFromFrozen(Tensor& image, int& result)\r\n{\r\n    //auto inputlayer = graphDef->node(0).name();\r\n    //auto outputlayer = graphDef->node(graphDef->node_size()-2).name();\r\n    //cout<<inputlayer<<endl;\r\n    //cout<<outputlayer<<endl;\r\n    //get the appropriate input and out layer names from the graph/mode to execute\r\n    vector<Tensor> outTensors;\r\n    cout<<\"Prediction about to start\"<<endl;\r\n    TF_CHECK_OK(fSession->Run({{\"fasterrcnn/truncated_base_network/sub\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors));\r\n    cout<<\"Prediction done\"<<endl;\r\n    auto output_c = outTensors[0].scalar<float>();\r\n    for (int i=0;i<outTensors.size();i++)\r\n    {\r\n        cout << \"Output dimension of the image\" << outTensors[i].DebugString()<<\"\\n\";\r\n    }\r\n    cout << \"Output dimension of the image\" << outTensors[0].DebugString()<<\"\\n\";\r\n    return Status::OK();\r\n}\r\nStatus fasterRcnn::ReadTensorFromImageFile(string& file_name, Tensor& outTensor)\r\n{\r\n    if(!iRoot.ok())\r\n        return iRoot.status();\r\n    if (!str_util::EndsWith(file_name, \".jpg\") && !str_util::EndsWith(file_name, \".jpeg\"))\r\n    {\r\n        return errors::InvalidArgument(\"Image must be jpeg encoded\");\r\n    }\r\n    vector<Tensor> out_tensors;\r\n    ClientSession session(iRoot);\r\n    cout<<\"About to do Session.Run\"<<endl;\r\n    TF_CHECK_OK(session.Run({{fileNameVar, file_name}}, {imageTensorVar}, &out_tensors));\r\n\r\n    outTensor = out_tensors[0]; // shallow copy\r\n    cout<<\"done Session.Run\"<<endl;\r\n    return Status::OK();\r\n}\r\nStatus fasterRcnn::ReadFileTensors(string& base_folder_name,vector<Tensor>& file_tensors)\r\n{\r\n    //validate the folder\r\n    Env* penv = Env::Default();//from tensorflow\r\n    TF_RETURN_IF_ERROR(penv->IsDirectory(base_folder_name));\r\n    //get the files\r\n    bool b_shuffle = false;\r\n\r\n    string folder_name = io::JoinPath(base_folder_name, \"\");\r\n    TF_RETURN_IF_ERROR(penv->IsDirectory(folder_name));\r\n    vector<string> file_names;\r\n    TF_RETURN_IF_ERROR(penv->GetChildren(folder_name, &file_names));\r\n    for(string file: file_names)\r\n    {\r\n        string full_path = io::JoinPath(folder_name, file);\r\n        Tensor i_tensor;\r\n        cout<<full_path<<endl;\r\n        TF_RETURN_IF_ERROR(ReadTensorFromImageFile(full_path, i_tensor));\r\n        size_t s = file_tensors.size();\r\n        if(b_shuffle)\r\n        {\r\n            //suffle the images\r\n            int i = rand() % s;\r\n            file_tensors.emplace(file_tensors.begin()+i, i_tensor);\r\n        }\r\n        else\r\n            file_tensors.push_back(i_tensor);\r\n    }\r\n    b_shuffle = true;\r\n    return Status::OK();\r\n}\r\n\r\n```\r\n\r\n#main.cpp \r\n```\r\n#include \"predicting.h\"\r\n#include <chrono>\r\n#include <iomanip>\r\nusing namespace std;\r\nusing namespace chrono;\r\nusing namespace tensorflow;\r\nusing namespace tensorflow::ops;\r\nint main(int argc,const char*argv[])\r\n{\r\n    int imageSide=512;\r\n    int imageChannels=3;\r\n    fasterRcnn model(imageSide,imageChannels);\r\n    Status s;\r\n    string protoName=\"frozen/frozen_model.pb\";\r\n    const std::string graph_fn = \"model/model.ckpt-14260114.meta\";\r\n    const std::string checkpoint_fn = \"model/model.ckpt-14260114\";\r\n    s=model.LoadSavedModel(graph_fn,checkpoint_fn);\r\n    TF_CHECK_OK(s);\r\n    s = model.CreateGraphForImage(false);//rebuild the image loading model without unstacking\r\n    TF_CHECK_OK(s);\r\n    string base_folder = \"data\";\r\n    vector<Tensor> all_files_tensors;\r\n    s = model.ReadFileTensors(base_folder, all_files_tensors);\r\n    TF_CHECK_OK(s);\r\n    int result;\r\n    s=model.PredictFromFrozen(all_files_tensors[0],result);\r\n    TF_CHECK_OK(s);\r\n    cout<<\"All Good\"<<endl;\r\n}\r\n```\r\nLinks to Code and Graph:\r\n\r\n[CodeBase and GraphPng and GIF hosted on GDrive\r\n](https://drive.google.com/drive/folders/1iO7JIitD_BfHnnLqgzvUygFwAAW6v_5G)\r\n\r\n\r\n\r\nThanks, @oanush for responding to this. As the code is specific to the layers of my graph, I don't think so this is a standalone code. But, still, let me know if you need anything else. The graph on the drive would show you appropriate inputs and outputs.\r\n\r\nNote: In my python code I was using QueueRunners for training, but for inference, I was not using it.\r\nSimilarly, on the C++ side, I managed my own input pipeline without using the QueueRunner, as mentioned in this answer by @mrry [answer](https://stackoverflow.com/q/35346117/8250471).\r\n\r\n**@oanush if you can suggest any ideas on debugging ideas that I can try while you replicate it on your side,  as the entire program runs fine but hangs on the fSession->Run line in the PredictFromFrozen() function without giving any error.**\r\n\r\n**Update**\r\nGDB: output for the program: Maybe this might help with any further lead on this.\r\n```\r\n(gdb) run\r\nThe program being debugged has been started already.\r\nStart it from the beginning? (y or n) y\r\nStarting program: /home/rnd/Desktop/Amit/MinutiaeDetection_DL_Master/mnt_detector_cc/predict_cc \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff102d700 (LWP 15750)]\r\n[New Thread 0x7ffff082c700 (LWP 15751)]\r\n[New Thread 0x7ffff002b700 (LWP 15752)]\r\n[New Thread 0x7fffef82a700 (LWP 15753)]\r\n[New Thread 0x7fffef029700 (LWP 15754)]\r\n[New Thread 0x7fffee828700 (LWP 15755)]\r\n[New Thread 0x7fffee027700 (LWP 15756)]\r\n[New Thread 0x7fffed826700 (LWP 15757)]\r\n[New Thread 0x7fffed025700 (LWP 15758)]\r\n[New Thread 0x7fffec824700 (LWP 15759)]\r\n[New Thread 0x7fffec023700 (LWP 15760)]\r\n[New Thread 0x7fffeb822700 (LWP 15761)]\r\n[New Thread 0x7fffeb021700 (LWP 15762)]\r\n[New Thread 0x7fffea820700 (LWP 15763)]\r\n[New Thread 0x7fffea01f700 (LWP 15764)]\r\n[New Thread 0x7fffe981e700 (LWP 15765)]\r\n[New Thread 0x7fffe901d700 (LWP 15766)]\r\n[New Thread 0x7fffe881c700 (LWP 15767)]\r\n[New Thread 0x7fffe801b700 (LWP 15768)]\r\n[New Thread 0x7fffe781a700 (LWP 15769)]\r\n[New Thread 0x7fffe7019700 (LWP 15770)]\r\n[New Thread 0x7fffe6818700 (LWP 15771)]\r\n[New Thread 0x7fffe6017700 (LWP 15772)]\r\n[New Thread 0x7fffe5816700 (LWP 15773)]\r\n[New Thread 0x7fffe5015700 (LWP 15774)]\r\n[New Thread 0x7fffe4814700 (LWP 15775)]\r\n[New Thread 0x7fffe4013700 (LWP 15776)]\r\n[New Thread 0x7fffe3812700 (LWP 15777)]\r\n[New Thread 0x7fffe3011700 (LWP 15778)]\r\n[New Thread 0x7fffe2810700 (LWP 15779)]\r\n[New Thread 0x7fffe200f700 (LWP 15780)]\r\n[New Thread 0x7fffe180e700 (LWP 15781)]\r\n[New Thread 0x7fffe100d700 (LWP 15782)]\r\n[New Thread 0x7fffe080c700 (LWP 15783)]\r\n[New Thread 0x7fffe000b700 (LWP 15784)]\r\n[New Thread 0x7fffdf80a700 (LWP 15785)]\r\n[New Thread 0x7fffdf009700 (LWP 15786)]\r\n[New Thread 0x7fffde808700 (LWP 15787)]\r\n[New Thread 0x7fffde007700 (LWP 15788)]\r\n[New Thread 0x7fffdd806700 (LWP 15789)]\r\n[New Thread 0x7fffdd005700 (LWP 15790)]\r\n[New Thread 0x7fffdc804700 (LWP 15791)]\r\n[New Thread 0x7fffdc003700 (LWP 15792)]\r\n[New Thread 0x7fffdb802700 (LWP 15793)]\r\n[New Thread 0x7fffdb001700 (LWP 15794)]\r\n[New Thread 0x7fffda800700 (LWP 15795)]\r\n[New Thread 0x7fffd9fff700 (LWP 15796)]\r\n[New Thread 0x7fffd97fe700 (LWP 15797)]\r\n[New Thread 0x7fffd8ffd700 (LWP 15798)]\r\n[New Thread 0x7fffd87fc700 (LWP 15799)]\r\n[Thread 0x7fffd87fc700 (LWP 15799) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 15800)]\r\n[Thread 0x7fffd87fc700 (LWP 15800) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 15801)]\r\n[Thread 0x7fffd87fc700 (LWP 15801) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 15802)]\r\n[New Thread 0x7fffd7ffb700 (LWP 15803)]\r\n[New Thread 0x7fffd77fa700 (LWP 15804)]\r\n[Thread 0x7fffd77fa700 (LWP 15804) exited]\r\n[Thread 0x7fffd87fc700 (LWP 15802) exited]\r\n[Thread 0x7fffd7ffb700 (LWP 15803) exited]\r\n[New Thread 0x7fffd77fa700 (LWP 15805)]\r\n[New Thread 0x7fffd7ffb700 (LWP 15806)]\r\n[New Thread 0x7fffd87fc700 (LWP 15807)]\r\n[New Thread 0x7fffd6ff9700 (LWP 15808)]\r\n[New Thread 0x7fffd67f8700 (LWP 15809)]\r\n[New Thread 0x7fffd5ff7700 (LWP 15810)]\r\n[New Thread 0x7fffd57f6700 (LWP 15811)]\r\n[New Thread 0x7fffd4ff5700 (LWP 15812)]\r\n[Thread 0x7fffd4ff5700 (LWP 15812) exited]\r\n[Thread 0x7fffd57f6700 (LWP 15811) exited]\r\n[Thread 0x7fffd5ff7700 (LWP 15810) exited]\r\n[Thread 0x7fffd87fc700 (LWP 15807) exited]\r\n[Thread 0x7fffd67f8700 (LWP 15809) exited]\r\n[Thread 0x7fffd77fa700 (LWP 15805) exited]\r\n[Thread 0x7fffd6ff9700 (LWP 15808) exited]\r\n[Thread 0x7fffd7ffb700 (LWP 15806) exited]\r\ndata/L002gi_1.jpg\r\nAbout to do Session.Run\r\n[New Thread 0x7fffd4ff5700 (LWP 15813)]\r\n[Thread 0x7fffd4ff5700 (LWP 15813) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 15814)]\r\n[Thread 0x7fffd4ff5700 (LWP 15814) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 15815)]\r\n[Thread 0x7fffd4ff5700 (LWP 15815) exited]\r\ndone Session.Run\r\nPrediction about to start\r\n\r\nThread 1 \"predict_cc\" hit Breakpoint 1, fasterRcnn::PredictFromFrozen (this=0x7fffffffd5c0, image=..., result=@0x7fffffffd564: 0)\r\n    at /home/rnd/Desktop/Amit/MinutiaeDetection_DL_Master/mnt_detector_cc/utils/predicting.cpp:75\r\n75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n(gdb) continue\r\nContinuing.\r\n[New Thread 0x7fffd4ff5700 (LWP 15816)]\r\n[Thread 0x7fffd4ff5700 (LWP 15816) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 15817)]\r\n[Thread 0x7fffd4ff5700 (LWP 15817) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 15818)]\r\n[Thread 0x7fffd4ff5700 (LWP 15818) exited]\r\n```\r\n**nexti results gdb**:\r\n```\r\n(gdb) run\r\nThe program being debugged has been started already.\r\nStart it from the beginning? (y or n) y\r\nStarting program: /home/rnd/Desktop/Amit/MinutiaeDetection_DL_Master/mnt_detector_cc/predict_cc \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n[New Thread 0x7ffff102d700 (LWP 16636)]\r\n[New Thread 0x7ffff082c700 (LWP 16637)]\r\n[New Thread 0x7ffff002b700 (LWP 16638)]\r\n[New Thread 0x7fffef82a700 (LWP 16639)]\r\n[New Thread 0x7fffef029700 (LWP 16640)]\r\n[New Thread 0x7fffee828700 (LWP 16641)]\r\n[New Thread 0x7fffee027700 (LWP 16642)]\r\n[New Thread 0x7fffed826700 (LWP 16643)]\r\n[New Thread 0x7fffed025700 (LWP 16644)]\r\n[New Thread 0x7fffec824700 (LWP 16645)]\r\n[New Thread 0x7fffec023700 (LWP 16646)]\r\n[New Thread 0x7fffeb822700 (LWP 16647)]\r\n[New Thread 0x7fffeb021700 (LWP 16648)]\r\n[New Thread 0x7fffea820700 (LWP 16649)]\r\n[New Thread 0x7fffea01f700 (LWP 16650)]\r\n[New Thread 0x7fffe981e700 (LWP 16651)]\r\n[New Thread 0x7fffe901d700 (LWP 16652)]\r\n[New Thread 0x7fffe881c700 (LWP 16653)]\r\n[New Thread 0x7fffe801b700 (LWP 16654)]\r\n[New Thread 0x7fffe781a700 (LWP 16655)]\r\n[New Thread 0x7fffe7019700 (LWP 16657)]\r\n[New Thread 0x7fffe6818700 (LWP 16658)]\r\n[New Thread 0x7fffe6017700 (LWP 16659)]\r\n[New Thread 0x7fffe5816700 (LWP 16660)]\r\n[New Thread 0x7fffe5015700 (LWP 16661)]\r\n[New Thread 0x7fffe4814700 (LWP 16662)]\r\n[New Thread 0x7fffe4013700 (LWP 16663)]\r\n[New Thread 0x7fffe3812700 (LWP 16664)]\r\n[New Thread 0x7fffe3011700 (LWP 16665)]\r\n[New Thread 0x7fffe2810700 (LWP 16666)]\r\n[New Thread 0x7fffe200f700 (LWP 16667)]\r\n[New Thread 0x7fffe180e700 (LWP 16668)]\r\n[New Thread 0x7fffe100d700 (LWP 16669)]\r\n[New Thread 0x7fffe080c700 (LWP 16670)]\r\n[New Thread 0x7fffe000b700 (LWP 16671)]\r\n[New Thread 0x7fffdf80a700 (LWP 16672)]\r\n[New Thread 0x7fffdf009700 (LWP 16673)]\r\n[New Thread 0x7fffde808700 (LWP 16674)]\r\n[New Thread 0x7fffde007700 (LWP 16675)]\r\n[New Thread 0x7fffdd806700 (LWP 16676)]\r\n[New Thread 0x7fffdd005700 (LWP 16677)]\r\n[New Thread 0x7fffdc804700 (LWP 16678)]\r\n[New Thread 0x7fffdc003700 (LWP 16679)]\r\n[New Thread 0x7fffdb802700 (LWP 16680)]\r\n[New Thread 0x7fffdb001700 (LWP 16681)]\r\n[New Thread 0x7fffda800700 (LWP 16682)]\r\n[New Thread 0x7fffd9fff700 (LWP 16683)]\r\n[New Thread 0x7fffd97fe700 (LWP 16684)]\r\n[New Thread 0x7fffd8ffd700 (LWP 16685)]\r\n[New Thread 0x7fffd87fc700 (LWP 16686)]\r\n[Thread 0x7fffd87fc700 (LWP 16686) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 16687)]\r\n[Thread 0x7fffd87fc700 (LWP 16687) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 16688)]\r\n[Thread 0x7fffd87fc700 (LWP 16688) exited]\r\n[New Thread 0x7fffd87fc700 (LWP 16689)]\r\n[New Thread 0x7fffd7ffb700 (LWP 16690)]\r\n[New Thread 0x7fffd77fa700 (LWP 16691)]\r\n[Thread 0x7fffd77fa700 (LWP 16691) exited]\r\n[Thread 0x7fffd87fc700 (LWP 16689) exited]\r\n[Thread 0x7fffd7ffb700 (LWP 16690) exited]\r\n[New Thread 0x7fffd77fa700 (LWP 16692)]\r\n[New Thread 0x7fffd7ffb700 (LWP 16693)]\r\n[New Thread 0x7fffd87fc700 (LWP 16694)]\r\n[New Thread 0x7fffd6ff9700 (LWP 16695)]\r\n[New Thread 0x7fffd67f8700 (LWP 16696)]\r\n[New Thread 0x7fffd5ff7700 (LWP 16697)]\r\n[New Thread 0x7fffd57f6700 (LWP 16698)]\r\n[New Thread 0x7fffd4ff5700 (LWP 16699)]\r\n[Thread 0x7fffd4ff5700 (LWP 16699) exited]\r\n[Thread 0x7fffd57f6700 (LWP 16698) exited]\r\n[Thread 0x7fffd5ff7700 (LWP 16697) exited]\r\n[Thread 0x7fffd67f8700 (LWP 16696) exited]\r\n[Thread 0x7fffd7ffb700 (LWP 16693) exited]\r\n[Thread 0x7fffd77fa700 (LWP 16692) exited]\r\n[Thread 0x7fffd6ff9700 (LWP 16695) exited]\r\n[Thread 0x7fffd87fc700 (LWP 16694) exited]\r\ndata/L002gi_1.jpg\r\nAbout to do Session.Run\r\n[New Thread 0x7fffd4ff5700 (LWP 16701)]\r\n[Thread 0x7fffd4ff5700 (LWP 16701) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 16702)]\r\n[Thread 0x7fffd4ff5700 (LWP 16702) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 16703)]\r\n[Thread 0x7fffd4ff5700 (LWP 16703) exited]\r\ndone Session.Run\r\nPrediction about to start\r\n\r\nThread 1 \"predict_cc\" hit Breakpoint 1, fasterRcnn::PredictFromFrozen (this=0x7fffffffd5c0, image=..., result=@0x7fffffffd564: 0)\r\n    at /home/rnd/Desktop/Amit/MinutiaeDetection_DL_Master/mnt_detector_cc/utils/predicting.cpp:75\r\n75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c006 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+118>:\tmov    -0x280(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c00d\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c00d <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+125>:\tadd    $0x60,%rax\r\n(gdb) nexti\r\n0x000055555555c011\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c011 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+129>:\tmov    %rax,%rdi\r\n(gdb) \r\n0x000055555555c014\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c014 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+132>:\t\r\n    callq  0x55555555f2cc <std::unique_ptr<tensorflow::Session, std::default_delete<tensorflow::Session> >::operator->() const>\r\n(gdb) nexti\r\n0x000055555555c019\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c019 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+137>:\tmov    %rax,%rbx\r\n(gdb) nexti\r\n0x000055555555c01c\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c01c <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+140>:\tmov    (%rbx),%rax\r\n(gdb) nexti\r\n0x000055555555c01f\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c01f <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+143>:\tadd    $0x20,%rax\r\n(gdb) nexti\r\n0x000055555555c023\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c023 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+147>:\tmov    (%rax),%rax\r\n(gdb) nexti\r\n0x000055555555c026\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c026 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+150>:\tmov    %rax,-0x298(%rbp)\r\n(gdb) nexti\r\n0x000055555555c02d\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c02d <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+157>:\tlea    -0x200(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c034\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c034 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+164>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c037\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c037 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+167>:\t\r\n    callq  0x55555555f9ae <std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::vector()>\r\n(gdb) nexti\r\n0x000055555555c03c\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c03c <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+172>:\tlea    -0x26e(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c043\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c043 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+179>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c046\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c046 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+182>:\tcallq  0x555555558360 <_ZNSaIcEC1Ev@plt>\r\n(gdb) nexti\r\n0x000055555555c04b\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c04b <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+187>:\tlea    -0x26e(%rbp),%rdx\r\n(gdb) nexti\r\n0x000055555555c052\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c052 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+194>:\tlea    -0x1e0(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c059\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c059 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+201>:\tlea    0xa850(%rip),%rsi        # 0x5555555668b0\r\n(gdb) nexti\r\n0x000055555555c060\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c060 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+208>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c063\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c063 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+211>:\tcallq  0x555555558610 <_ZNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEC1EPKcRKS3_@plt>\r\n(gdb) nexti\r\n0x000055555555c068\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c068 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+216>:\tlea    -0x1e0(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c06f\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c06f <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+223>:\tmov    %rax,%r14\r\n(gdb) nexti\r\n0x000055555555c072\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c072 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+226>:\tmov    $0x1,%r15d\r\n(gdb) nexti\r\n0x000055555555c078\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c078 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+232>:\tlea    -0x26d(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c07f\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c07f <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+239>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c082\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c082 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+242>:\t\r\n    callq  0x55555555f9ca <std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >::allocator()>\r\n(gdb) nexti\r\n0x000055555555c087\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c087 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+247>:\tlea    -0x26d(%rbp),%rdx\r\n(gdb) nexti\r\n0x000055555555c08e\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c08e <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+254>:\tmov    %r14,%r8\r\n(gdb) nexti\r\n0x000055555555c091\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c091 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+257>:\tmov    %r15,%r9\r\n(gdb) nexti\r\n0x000055555555c094\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c094 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+260>:\tmov    %r14,%rsi\r\n(gdb) nexti\r\n0x000055555555c097\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c097 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+263>:\tmov    %r15,%rdi\r\n(gdb) nexti\r\n0x000055555555c09a\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c09a <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+266>:\tlea    -0x220(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c0a1\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0a1 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+273>:\tmov    %rdx,%rcx\r\n(gdb) nexti\r\n0x000055555555c0a4\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0a4 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+276>:\tmov    %r8,%rsi\r\n(gdb) nexti\r\n0x000055555555c0a7\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0a7 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+279>:\tmov    %rdi,%rdx\r\n(gdb) nexti\r\n0x000055555555c0aa\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0aa <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+282>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c0ad\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0ad <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+285>:\t\r\n    callq  0x55555555fa4a <std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::vector(std::initializer_list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > const&)>\r\n(gdb) nexti\r\n0x000055555555c0b2\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0b2 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+290>:\tmov    -0x288(%rbp),%rdx\r\n(gdb) nexti\r\n0x000055555555c0b9\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0b9 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+297>:\tlea    -0x1c0(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c0c0\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0c0 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+304>:\tlea    0xa812(%rip),%rsi        # 0x5555555668d9\r\n(gdb) nexti\r\n0x000055555555c0c7\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0c7 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+311>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c0ca\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0ca <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+314>:\t\r\n    callq  0x55555555ec28 <std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>::pair<char const (&) [22], tensorflow::Tensor&, true>(char const (&) [22], tensorflow::Tensor&)>\r\n(gdb) nexti\r\n0x000055555555c0cf\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0cf <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+319>:\tlea    -0x1c0(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c0d6\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0d6 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+326>:\tmov    %rax,%r12\r\n(gdb) nexti\r\n0x000055555555c0d9\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0d9 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+329>:\tmov    $0x1,%r13d\r\n(gdb) nexti\r\n0x000055555555c0df\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0df <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+335>:\tlea    -0x26f(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c0e6\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0e6 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+342>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c0e9\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0e9 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+345>:\t\r\n    callq  0x55555555f830 <std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> >::allocator()>\r\n(gdb) nexti\r\n0x000055555555c0ee\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0ee <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+350>:\tlea    -0x26f(%rbp),%rdx\r\n(gdb) nexti\r\n0x000055555555c0f5\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0f5 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+357>:\tmov    %r12,%r8\r\n(gdb) nexti\r\n0x000055555555c0f8\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0f8 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+360>:\tmov    %r13,%r9\r\n(gdb) nexti\r\n0x000055555555c0fb\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0fb <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+363>:\tmov    %r12,%rsi\r\n(gdb) nexti\r\n0x000055555555c0fe\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c0fe <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+366>:\tmov    %r13,%rdi\r\n(gdb) nexti\r\n0x000055555555c101\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c101 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+369>:\tlea    -0x240(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c108\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c108 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+376>:\tmov    %rdx,%rcx\r\n(gdb) nexti\r\n0x000055555555c10b\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c10b <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+379>:\tmov    %r8,%rsi\r\n(gdb) nexti\r\n0x000055555555c10e\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c10e <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+382>:\tmov    %rdi,%rdx\r\n(gdb) nexti\r\n0x000055555555c111\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c111 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+385>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c114\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c114 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+388>:\t\r\n    callq  0x55555555f8b0 <std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > >::vector(std::initializer_list<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > const&)>\r\n(gdb) nexti\r\n0x000055555555c119\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c119 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+393>:\tlea    -0x268(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c120\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c120 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+400>:\tlea    -0x260(%rbp),%rdi\r\n(gdb) nexti\r\n0x000055555555c127\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c127 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+407>:\tlea    -0x200(%rbp),%rsi\r\n(gdb) nexti\r\n0x000055555555c12e\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c12e <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+414>:\tlea    -0x220(%rbp),%rcx\r\n(gdb) nexti\r\n0x000055555555c135\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c135 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+421>:\tlea    -0x240(%rbp),%rdx\r\n(gdb) nexti\r\n0x000055555555c13c\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c13c <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+428>:\tmov    %rdi,%r9\r\n(gdb) nexti\r\n0x000055555555c13f\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c13f <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+431>:\tmov    %rsi,%r8\r\n(gdb) nexti\r\n0x000055555555c142\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c142 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+434>:\tmov    %rbx,%rsi\r\n(gdb) nexti\r\n0x000055555555c145\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c145 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+437>:\tmov    %rax,%rdi\r\n(gdb) nexti\r\n0x000055555555c148\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c148 <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+440>:\tmov    -0x298(%rbp),%rax\r\n(gdb) nexti\r\n0x000055555555c14f\t75\t    Status runStatus=fSession->Run({{\"fasterrcnn/ExpandDims\",image}},{\"fasterrcnn/rcnn/rcnn_proposal/GatherV2_1\"},{},&outTensors);\r\n1: x/i $pc\r\n=> 0x55555555c14f <fasterRcnn::PredictFromFrozen(tensorflow::Tensor&, int&)+447>:\tcallq  *%rax\r\n(gdb) nexti\r\n^[[A[New Thread 0x7fffd4ff5700 (LWP 16705)]\r\n[Thread 0x7fffd4ff5700 (LWP 16705) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 16706)]\r\n[Thread 0x7fffd4ff5700 (LWP 16706) exited]\r\n[New Thread 0x7fffd4ff5700 (LWP 16707)]\r\n[Thread 0x7fffd4ff5700 (LWP 16707) exited]\r\n```\r\n", "Hey, @jvishnuvardhan or @ezhulenev any update or suggestions on this?", "Don't call PredictFromFrozen(). Instead replace s=model.PredictFromFrozen(all_files_tensors[0],result); by contents of PredictFromFrozen() into main.cpp.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32624\">No</a>\n"]}, {"number": 32623, "title": "\"failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\" unless running with sudo", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ClearLinux 31030\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tensorflow-gpu 1.14.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pyenv's pip\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: cuda_10.0.130_410.48_linux, cudnn-10.0-linux-x64-v7.6.3.30\r\n- GPU model and memory: Geforce RTX 2060, 6GB RAM\r\n\r\n\r\n**Describe the problem**\r\n\r\nThe error \"failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\" is thrown when initializing tensorflow-gpu, falling back to CPU instead of GPU.\r\n\r\nWhen running python with sudo, GPU is detected but libraries cannot be opened. A subsequent run without sudo works, enabling GPU being used. I don't understand why running with sudo is needed to enable future calls without sudo work.\r\n\r\nThe specific output is:\r\n\r\n    $ python -c \"import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))\"\r\n    2019-09-18 17:49:26.342297: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n    2019-09-18 17:49:26.364789: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n    2019-09-18 17:49:26.365673: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b4f7008c70 executing computations on platform Host. Devices:\r\n    2019-09-18 17:49:26.365685: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n    2019-09-18 17:49:26.387440: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n    2019-09-18 17:49:26.397897: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n    2019-09-18 17:49:26.397918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: linux\r\n    2019-09-18 17:49:26.397923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: linux\r\n    2019-09-18 17:49:26.397950: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0\r\n    2019-09-18 17:49:26.397965: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0\r\n    2019-09-18 17:49:26.397969: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0\r\n    2019-09-18 17:49:26.399634: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n\r\n    $ sudo python -c \"import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))\"\r\n    2019-09-18 17:49:33.476640: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n    2019-09-18 17:49:33.492804: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n    2019-09-18 17:49:33.493345: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8b43539a0 executing computations on platform Host. Devices:\r\n    2019-09-18 17:49:33.493356: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n    2019-09-18 17:49:33.494037: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n    2019-09-18 17:49:33.525519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:33.525827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\n    name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\n    pciBusID: 0000:01:00.0\r\n    2019-09-18 17:49:33.525900: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.525942: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.525980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.526017: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.526055: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.526092: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.526130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n    2019-09-18 17:49:33.526135: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n    2019-09-18 17:49:33.614070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n    2019-09-18 17:49:33.614090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n    2019-09-18 17:49:33.614095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n    2019-09-18 17:49:33.615437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:33.615752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8b71df500 executing computations on platform CUDA. Devices:\r\n    2019-09-18 17:49:33.615761: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5\r\n    2019-09-18 17:49:33.616523: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n    Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n\r\n    $ python -c \"import tensorflow as tf; tf.Session(config=tf.ConfigProto(log_device_placement=True))\"\r\n    2019-09-18 17:49:38.343247: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n    2019-09-18 17:49:38.359840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192000000 Hz\r\n    2019-09-18 17:49:38.360790: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d522b77c70 executing computations on platform Host. Devices:\r\n    2019-09-18 17:49:38.360803: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n    2019-09-18 17:49:38.361478: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n    2019-09-18 17:49:38.377924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.378224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\n    name: GeForce RTX 2060 major: 7 minor: 5 memoryClockRate(GHz): 1.755\r\n    pciBusID: 0000:01:00.0\r\n    2019-09-18 17:49:38.382955: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n    2019-09-18 17:49:38.426461: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n    2019-09-18 17:49:38.452107: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n    2019-09-18 17:49:38.468904: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n    2019-09-18 17:49:38.517258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n    2019-09-18 17:49:38.545852: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n    2019-09-18 17:49:38.660617: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n    2019-09-18 17:49:38.660684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.661018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.661283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n    2019-09-18 17:49:38.661304: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n    2019-09-18 17:49:38.727136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n    2019-09-18 17:49:38.727157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n    2019-09-18 17:49:38.727164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n    2019-09-18 17:49:38.727262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.727564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.727848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n    2019-09-18 17:49:38.728115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5451 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n    2019-09-18 17:49:38.729363: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d525ddbe50 executing computations on platform CUDA. Devices:\r\n    2019-09-18 17:49:38.729373: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2060, Compute Capability 7.5\r\n    2019-09-18 17:49:38.730199: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n    Device mapping:\r\n    /job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n    /job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n    /job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n\r\n**Any other info / logs**\r\n\r\nCUDA and Nvidia drivers installed at /opt following [ClearLinux guide](https://docs.01.org/clearlinux/latest/tutorials/nvidia.html).\r\n\r\n    $ echo $LD_LIBRARY_PATH\r\n    /usr/local/cuda/lib64:\r\n\r\n    $ nvcc --version\r\n    nvcc: NVIDIA (R) Cuda compiler driver\r\n    Copyright (c) 2005-2018 NVIDIA Corporation\r\n    Built on Sat_Aug_25_21:08:01_CDT_2018\r\n    Cuda compilation tools, release 10.0, V10.0.130\r\n\r\n    $ nvidia-smi\r\n    Wed Sep 18 17:58:36 2019       \r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce RTX 2060    Off  | 00000000:01:00.0  On |                  N/A |\r\n    |  0%   50C    P8     9W / 170W |    152MiB /  5931MiB |      0%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n                                                                                   \r\n    +-----------------------------------------------------------------------------+\r\n    | Processes:                                                       GPU Memory |\r\n    |  GPU       PID   Type   Process name                             Usage      |\r\n    |=============================================================================|\r\n    |    0       651      G   /usr/bin/X                                    39MiB |\r\n    |    0       789      G   /usr/bin/gnome-shell                         111MiB |\r\n    +-----------------------------------------------------------------------------+", "comments": ["@josei ,\r\nCould you check this [issue](https://github.com/tensorflow/tensorflow/issues/19266#issuecomment-399686258) which is similar to your issue. If those solutions doesn't work, could you uninstall and reinstall CUDA and cuDNN? Please let us know how it progresses. Also, try to uninstall and reinstall tensorflow-gpu and try following the instructions from [TensorFlow](https://www.tensorflow.org/install/source) website. Thanks!", "Yes, things I've already tried are:\r\n- Install nvidia-modprobe.\r\n- Uninstall and install CUDA and cuDNN.\r\n- Uninstall and install tensorflow-gpu.\r\n- Uninstall and install different Nvidia driver versions.\r\n\r\nI can't follow TensorFlow instructions right away, as they're not focussed on ClearLinux distribution. Probably I'd switch distribution, as this could be a ClearLinux issue unless someone knows how to narrow it down.", "@josei Can you try TF1.15.0rc1 and let us know if the issue persists or not. Thanks!", "See https://devtalk.nvidia.com/default/topic/749939/cuda-is-not-active-unless-i-run-it-with-sudo-privillages-/", "@mihaimaruseac Thanks, I'll research in that direction and report here", "The problem was that only some `/dev/nvidia*` files are present before running Python with sudo:\r\n\r\n    $ ls /dev/nvidia*\r\n    /dev/nvidia0  /dev/nvidiactl  /dev/nvidia-modeset\r\n\r\nAfter running the [Device Node Verification script](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#runfile-verifications), the `/dev/nvidia-uvm` file gets added, enabling Tensorflow to work on GPU:\r\n\r\n    $ ls /dev/nvidia*\r\n    /dev/nvidia0  /dev/nvidiactl  /dev/nvidia-modeset  /dev/nvidia-uvm", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32623\">No</a>\n", "Just expanding on @josei suggestion.. run the following:\r\n\r\ncd /dev\r\nsudo nano script.sh (copy/write contents of \"Device Node Verification Script\")\r\nsudo chmod 777 script.sh\r\nsudo ./script.sh\r\n"]}, {"number": 32622, "title": "[TF 2.0] tf.assign fails with int32 tensors using a multi-device distribution strategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0rc0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: 2 GPUs\r\n\r\n**Describe the current behavior**\r\n\r\nIf you create a distribution strategy like MultiWorkerMirroredDistributionStrategy, then create a tf.int32 variable var, then call var.assign(some_int32_tensor_to_assign), TF will crash. It crashes in values.py, line 1220,\r\n\r\n```\r\n # To preserve the sum across save and restore, we have to divide the\r\n        # total across all devices when restoring a variable that was summed\r\n        # when saving.\r\n        tensor = args[0]\r\n  if self._aggregation == vs.VariableAggregation.SUM:\r\n          tensor *= 1. / len(self.devices)\r\n```\r\nSo what's happening here appears to be that TF tries to divide the int32 tensor by a float with no cast and blows up.\r\n\r\n**Describe the expected behavior**\r\nIt seems like being able to assign integers to tensors while using distributed training is something that should be possible, given the goal of abstracting away distribution as much as possible. I know the distribution strategies may be kind of in flux right now, and I'm mainly just posting to make more knowledgeable people away of the issue; I'm just going to make all my tensors floats in the meantime :). I don't have the context to know how this should be approached anyway. Cast everything to floats? Make it so that the tensors don't always have to be divided? Divide integer tensors by assigning everything to one worker and giving zeros to everyone else? Who knows.\r\n\r\n", "comments": ["I believe this has been fixed by (in one way, perhaps not the best): https://github.com/tensorflow/tensorflow/commit/438fc52e81ead89799067a2b961f6990d9b4f5f6#diff-580627c9b7904095019167ef005a72de. Please re-open if not, and thank you for filing! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32622\">No</a>\n"]}, {"number": 32621, "title": "Estimator 1.x nightlies break tf.contrib.*", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-9365-gff401a6 1.15.0-dev20190821\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nOn TF 1.x nightlies, `tf.contrib.summary` fails to import due to an error\r\nimporting something from estimator:\r\n\r\n```\r\n$ cd \"$(mktemp -d)\"\r\n$ virtualenv -q -p python3.6 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install -q tf-nightly==1.15.0.dev20190821\r\n(ve) $ pip freeze | grep -e tensor -e tf- -e tb-\r\ntb-nightly==1.15.0a20190911\r\ntf-estimator-nightly==1.14.0.dev2019091801\r\ntf-nightly==1.15.0.dev20190821\r\n(ve) $ python -c 'import tensorflow as tf; tf.contrib.summary'\r\nWARNING:tensorflow:\r\n\r\n  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\r\n\r\n  Please upgrade your code to TensorFlow 2.0:\r\n    * https://www.tensorflow.org/beta/guide/migration_guide\r\n\r\n  Or install the latest stable TensorFlow 1.X release:\r\n    * `pip install -U \"tensorflow==1.*\"`\r\n\r\n  Otherwise your code may be broken by the change.\r\n\r\n  \r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 63, in __getattr__\r\n    return getattr(module, item)\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/contrib/__init__.py\", line 48, in <module>\r\n    from tensorflow.contrib import estimator\r\n  File \"/tmp/tmp.pTt6GXwXDC/ve/lib/python3.6/site-packages/tensorflow_core/contrib/estimator/__init__.py\", line 30, in <module>\r\n    from tensorflow_estimator.contrib import estimator\r\nModuleNotFoundError: No module named 'tensorflow_estimator.contrib'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be possible to force `tf.contrib`, as was possible with yesterday\u2019s\r\n`tensorflow-estimator-nightly`.\r\n\r\n```\r\n$ pip freeze | grep -e tensor -e tf- -e tb-\r\ntb-nightly==1.15.0a20190911\r\ntf-estimator-nightly==1.14.0.dev2019091701\r\ntf-nightly==1.15.0.dev20190821\r\n$ python -c 'import tensorflow as tf; tf.contrib.summary'  # works\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\npython -c 'import tensorflow as tf; tf.contrib.summary'\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis is blocking TensorBoard builds and nightlies.\r\n\r\nGooglers, see <http://b/140485848#comment35>.\r\n", "comments": ["Sorry about these breakages.", "I think the problem here is caused by main TF blocking new nightlies, but estimator still building nightlies.\r\nShould we stop estimator from pushing 1.x nightlies?", "Estimator nightly packages now seem to be versioned as 2.x, which should\r\nbe sufficient for Pip to install the right things in the future. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32621\">No</a>\n"]}, {"number": 32620, "title": "Use tf.Tensor as input to tf.io.gfile.GFile", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0rc1\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, file readers such as `tf.io.gfile.GFile` expect plain strings as input. As a consequence, one can hardly use those file readers inside `tf.data.Dataset` pipelines which yield string Tensors (typically through `tf.data.Dataset.list_files`)\r\n\r\nDoing so raises `TypeError: Expected binary or unicode string, got <tf.Tensor 'EagerPyFunc:0' shape=<unknown> dtype=string>`, at the line `compat.as_bytes(self.__name)` (tensorflow_core/python/lib/io/file_io.py:84).\r\n\r\n**Will this change the current api? How?**\r\nFrom a user point of view, the current API would not see any significant changes.\r\n\r\n**Who will benefit with this feature?**\r\nAny one eager to read custom file formats, where it is for instance needed to decode part of the header file (first few bytes) to decode the actual content. For instance, image formats such as TIFF where the dtype and the image shape are encoded inside the header.\r\n\r\nBelow pipelines would then be easier to build:\r\n```\r\nfiles = tf.data.Dataset.list_files('/**/regex*')\r\ntensors = files.map(MyCustomReader)\r\n```\r\n\r\n**Any Other info.**\r\nSee below an example of such custom format file reader:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef CustomReader(path):\r\n    # path must here a string in the current API\r\n    with tf.io.gfile.GFile(name=path, mode='rb') as f:\r\n        \r\n        # Shape information is encoded  on 16 bytes starting at position 40\r\n        f.seek(40)\r\n        shape = tf.io.decode_raw(f.read(16), out_type=tf.int16)\r\n        shape = tf.cast(shape, tf.int32)\r\n       \r\n        # dtype information is encoded on 2 bytes at position 70\r\n        f.seek(70)\r\n        dtype = tf.io.decode_raw(f.read(2), out_type=tf.int16)[0]\r\n\r\n        # tensor values populate the  rest of the buffer\r\n        values = tf.io.decode_raw(f.read(-1), out_type=dtype)\r\n        tensor = tf.reshape(values, shape)\r\n\r\n        return tensor\r\n```", "comments": ["This likely is related to #33563", "Inasmuch as this is an issue, it also exists for the methods of the `tf.io.gfile.GFile` class. For example, I cannot pass a `tf.Tensor` as `offset` for `f.seek(offset)`. Supporting tensors here would make it easier to integrate into `tf.data`.", "Actually, now you can simply use tf.io.read_file() with a gs file and it would be able to read it even from a tensor. \ud83d\ude04 ", "@remydubois,\r\nCan you please let us know if [this comment](https://github.com/tensorflow/tensorflow/issues/32620#issuecomment-811044008) helped you to resolve this issue, so that we can close it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32619, "title": "Missing output shape for inverse stft", "body": "## Link\r\nhttps://www.tensorflow.org/api_docs/python/tf/signal/inverse_stft\r\n\r\n## Description of issue:\r\nI am missing what the output dimensions are.\r\n\r\n### Clear description\r\n\r\nI do not know what the output dimensions of inverse_stft are. And I do not know how they come to be.\r\n\r\n### Example Code\r\n\r\n```\r\naudioNp = np.random.random((1,120800)).astype(np.float32)\r\nframe_length = 2048\r\nframe_step = 2048//4\r\nstft = tf.contrib.signal.stft(\r\n    audioNp[0], \r\n    frame_length, \r\n    frame_step)\r\ninvstft = tf.contrib.signal.inverse_stft(\r\n    stft, \r\n    frame_length, \r\n    frame_step, \r\n    window_fn = tf.contrib.signal.inverse_stft_window_fn(frame_step))\r\nsess = tf.Session()\r\nstft, invstft = sess.run((stft, invstft))\r\nprint(invstft.shape)\r\n```\r\nThe output shape of invstft is then (120320,). And I don't know how it got there. In my opinion it should be again (120800).\r\n", "comments": ["I have tried on colab with TF version1.14 , 1.15.0-rc0 and see output shape of invstft is (120320,).Thanks!", "With trial and error I found out that the ouput shape seems to be: ```audioNp[0].shape//frame_step*frame_step```\r\nSo rounded down to a multiple of frame_step. But can anyone confirm this?", "Thanks for the report! This is actually intended behavior. The padding behavior of `tf.signal.stft` can be set via the `pad_end` keyword argument. \r\n\r\nIf you set `pad_end=True`, you'll get 236 frames (and a `(1, 120832)` signal using `inverse_stft`), while the default (`pad_end=False`), will truncate the signal to 235 frames (only computing the STFT over \"full\" frames) and `inverse_stft` will produce a `(1, 120320)` signal. \r\n\r\nThere's no way we can give you the exact size of your input back because the input to `tf.signal.inverse_stft` can only be an integer number of frames. Hope that helps :).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32619\">No</a>\n"]}, {"number": 32618, "title": "Use of fit_generator and evaluate_generator result in \"builtins.TypeError: 'NoneType' object is not subscriptable\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nhttps://colab.research.google.com/gist/isaacgerg/03717f55f020f36aa7b5138fc04cfc43/tf-keras-generator-issue.ipynb\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 7, Python 3.7, tensorflow 1.13.1\r\n\r\n- TensorFlow installed from (source or binary):\r\nvia pip\r\n\r\n- TensorFlow version (use command below): 1.13,1\r\n\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: RTX\r\n\r\n**Describe the current behavior**\r\n```\r\nThe code in the colab, when run on a GPU, results in teh following error:\r\nFile \"c:\\python37\\Lib\\threading.py\", line 885, in _bootstrap\r\n  self._bootstrap_inner()\r\nFile \"c:\\python37\\Lib\\threading.py\", line 917, in _bootstrap_inner\r\n  self.run()\r\nFile \"c:\\python37\\Lib\\threading.py\", line 865, in run\r\n  self._target(*self._args, **self._kwargs)\r\nFile \"c:\\python37\\Lib\\multiprocessing\\pool.py\", line 121, in worker\r\n  result = (True, func(*args, **kwds))\r\nFile \"c:\\python37\\Lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 445, in get_index\r\n  return _SHARED_SEQUENCES[uid][i]\r\n\r\nbuiltins.TypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo error\r\n\r\n**Code to reproduce the issue**\r\nSee colab link in description.  This is the offending code.  It only appears to happen on windows.  I am unsure in colab how to select a windows based environment.\r\n\r\nEDIT 1: Added windows caveat.", "comments": ["I tried on colab it kept running for long time. Please see the colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/a3a781843e4182513151e73121d0d307/tf-keras-generator-issue.ipynb). Thanks!", "@gadagashwini Same happened for me.  Please run it in windows, the error occurs after the first loop.", "@isaacgerg This is a big model with almost ~2 million parameters to optimize, 62 steps per training epoch and 9999 total epochs. I reduced number of epochs and the code is running without any error but loss was `nan`.\r\n\r\nI would suggested run for couple of 1 or 2 iterations on GPU, then save weights and make the model as `trainable = False` , then work on the code. There might be some coding issue which is why it is showing loss as `nan`.\r\n\r\nAnother things to consider here is installing TF1.15 which was running faster than 1.13. There were lots of performance improvements over the time so I would suggest to use `TF1.15.\r\n\r\nI am closing the issue here. Please feel free to open a new issue if you notice any issue with TF. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32618\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32618\">No</a>\n", "@jvishnuvardhan Can you reopen this?  The code producing the error is on colab but it happens in the windows enviroment.  You have to change your colab environment to run under winows (is that even possible?)  \r\n\r\nThe Nan's arent the issue and are expected as I trimmed down my code to the minimal form to reporduce the error.  The problem is when you run in windows, i get the following error:\r\n\r\n```\r\nFile \"c:\\python37\\Lib\\threading.py\", line 885, in _bootstrap\r\n  self._bootstrap_inner()\r\nFile \"c:\\python37\\Lib\\threading.py\", line 917, in _bootstrap_inner\r\n  self.run()\r\nFile \"c:\\python37\\Lib\\threading.py\", line 865, in run\r\n  self._target(*self._args, **self._kwargs)\r\nFile \"c:\\python37\\Lib\\multiprocessing\\pool.py\", line 121, in worker\r\n  result = (True, func(*args, **kwds))\r\nFile \"c:\\python37\\Lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 445, in get_index\r\n  return _SHARED_SEQUENCES[uid][i]\r\n\r\nbuiltins.TypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\nPlease run the colab code on a windows environment and you will see the error.", "@isaacgerg As mentioned in another issue of your's, this code also running in Windows10 without any issue. \r\nhttps://github.com/tensorflow/tensorflow/issues/32697#issuecomment-655193838\r\n\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Adding the argument workers=0 to model.eval() and model.fit() fixed this issue."]}]