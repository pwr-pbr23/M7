[{"number": 44133, "title": "Being able to register custom DataAdapter that inherit from the base class", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently the list of available data adapter are registered here:\r\nhttps://github.com/tensorflow/tensorflow/blob/6f440e215d0b4dfa2952729d8592af28976e48ef/tensorflow/python/keras/engine/data_adapter.py#L946\r\n\r\nTo support new use case such as supporting Py arrow array or Listarray (that will be converted to ragged). It could be nice to be able to register a custom data adapter to call keras directly with a dict of List Array or even with a Pandas Dataframe. \r\n\r\n**Will this change the current api? How?**\r\n\r\nno.\r\n**Who will benefit with this feature?**\r\nUsers with custom data need. \r\n\r\n**Any Other info.**\r\n", "comments": ["The `DataAdapter` class is a private API. If we made it a public API that users can extend, it would increase our maintenance burden (in an area that is not our core competency) and it would limit our ability to do internal refactors.\r\n\r\nIf you need dataset support for things like PyArrow and so on, I suggest instead to seek adding an add-on in https://github.com/tensorflow/io\r\n\r\nThank you"]}, {"number": 44132, "title": "[INTEL MKL] Fix DCHECK problem in mkl_layout_pass.cc", "body": "This PR fixes UT failure in mkl_layout_pass.cc caused by DCHECK", "comments": []}, {"number": 44131, "title": "added small tuches read description", "body": "line 53 added bold\r\nand added line 37", "comments": ["oops kinada new to this"]}, {"number": 44130, "title": "Have installed tensorflow but still unbale to access it in Jupyter Error: ModuleNotFoundError: No module named 'tensorflow'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["![kernel](https://user-images.githubusercontent.com/66782911/96372719-de636f80-1170-11eb-86fa-d8edc508cb02.PNG)\r\n![prompt](https://user-images.githubusercontent.com/66782911/96372722-df949c80-1170-11eb-9ff6-6d0cd27ebe8a.PNG)\r\n", "@qaiserizvi \r\n\r\nPlease install tensorflow in jupyter notebook using `!pip install tensorflow` and then you import tensorflow. If you have created virtual environment then please make sure you have installed  tensorflow in that environment.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/26e3cdbd0ad7bcc26666be5a0dd5e1c0/untitled461.ipynb). Thanks!", "Still getting these errors, after running pip install tensorflow==2.3.0\r\n\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-5-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "@qaiserizvi \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer duplicate issue #39007\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44130\">No</a>\n"]}, {"number": 44129, "title": "Tensorflow Lite with Nvidia GPU on Ubuntu happen coredump when creating delegate.", "body": "Hi Supporters,\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): master branch (2.3.1)\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce GTX 1050 Ti, 4Gb\r\n\r\n**Describe the current behavior**\r\nI tried to inference TF lite model with GPU, everything is normal with TF using CPU, but when create delagate, coredump happens immediately.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#include <iostream>\r\n#include <memory>\r\n\r\n#include <tensorflow/lite/interpreter.h>\r\n#include <tensorflow/lite/kernels/register.h>\r\n#include <tensorflow/lite/string_util.h>\r\n#include <tensorflow/lite/model.h>\r\n#include <tensorflow/lite/delegates/gpu/delegate.h>\r\n#include <tensorflow/lite/delegates/gpu/gl_delegate.h>\r\n\r\nint main()\r\n{\r\n    std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(\"superpoint_640x480.tflite\");\r\n    if (!model)\r\n    {\r\n        std::cerr << \"Failed to mmap tflite model\" << std::endl;\r\n        return -1;\r\n    }\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    if (tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter) != kTfLiteOk)\r\n    {\r\n        std::cerr << \"Failed to interpreter tflite model\" << std::endl;\r\n        return -1;\r\n    }\r\n\r\n    // auto* delegate = TfLiteGpuDelegateCreate(nullptr);\r\n    // if (interpreter->ModifyGraphWithDelegate(delegate) == kTfLiteOk)\r\n    // {\r\n    //     std::cerr << \"Failed to enable GPU.\" << std::endl;\r\n    //     return -1;\r\n    // }\r\n\r\n    //  NEW: Prepare custom options with feature enabled.\r\n    TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();\r\n    options.experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE;\r\n\r\n    auto* delegate = TfLiteGpuDelegateV2Create(&options);\r\n    if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk){\r\n        std::cerr << \"Failed to register delegate\" << std::endl;\r\n        return -1;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\nTF Lite model file : \r\n[superpoint_640x480.zip](https://github.com/tensorflow/tensorflow/files/5397657/superpoint_640x480.zip)\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[backtrace.txt](https://github.com/tensorflow/tensorflow/files/5397656/backtrace.txt)\r\n\r\nAs I know so far, TF Lite on GPU supported PAD ops, but don't know why the coredump happened here.\r\nHave you seen this ? Could you help me overcome this kind of issue ?\r\nMany thanks in advance.", "comments": ["Are you trying TF Lite gpu delegate on desktop machine?\r\nTensorFlow Lite delegate APIs are supported on Android and iOS platform as of now. Thanks!\r\nSee https://www.tensorflow.org/lite/performance/gpu", "Hi @ymodak,\r\n\r\nYes I'm using TF Lite GPU delegate on Desktop machine. So the TF Lite GPU supports only Android and iOS platforms, does it ?\r\nI'm developing a TF Lite model and expecting to inference it on embedded device which included NPU. So could you please give me some hint that I can use the TF Lite GPU delegate to inference the model on the edge with NPU ?\r\n\r\nMany thanks in advance.", "NPU has nothing in common with GPU, and I'm kinda confused why you would start with the GPU.\r\n\r\nHaving said that, I don't see anything in the code that could cause a segfault in the PadOperationParser.  Can you compile with -g to get more info from the debugger?", "Hi @impjdi,\r\n\r\nThank you for your response.\r\n\r\nAs so far I know, creating GPU and NNAPI delegate and register to Interpreter are pretty similar, so I would like to start with GPU first for easy to develop and test on laptop. \r\n\r\nAdded CMAKE_BUILD_TYPE=Debug to cmake. Please see that attach file for back trace\r\n[gdb.txt](https://github.com/tensorflow/tensorflow/files/5413234/gdb.txt)\r\n\r\n", "> No symbol table info available.\r\n\r\ntells me that the delegate isn't built with `-g` =/", "Thanks for you help. I managed to run the Nvidia GPU with TF Lite version 2.3\r\n\r\nUsed this command to build TF Lite gpu delegate :\r\n```\r\nbazel build -s -c opt --config=monolithic --config=noaws --config=nonccl --config=v2 --define=tflite_convert_with_select_tf_ops=true --define=with_select_tf_ops=true --copt=\"-DMESA_EGL_NO_X11_HEADERS\" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\r\n```\r\n\r\nAs well as the use this code to register gpu delegate:\r\n```\r\nconst TfLiteGpuDelegateOptionsV2 options = {\r\n    .is_precision_loss_allowed = 1, // FP16\r\n    .inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,\r\n    .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY,\r\n    .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n    .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,\r\n};\r\n\r\nTfLiteDelegate *delegate = TfLiteGpuDelegateV2Create(&options);\r\nif (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)\r\n{\r\n    std::cerr << \"Failed to modify graph with delegate\" << std::endl;\r\n    exit(0);\r\n}\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44129\">No</a>\n"]}, {"number": 44128, "title": "cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30 + master branch= some errors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 20H1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): build from master branch\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30\r\n- GPU model and memory: nvidia 1070 ti / 8Gb\r\n\r\nI successfully built Tensorflow from master brach with cuda optimization.\r\ncuda_11.1.0_456.43_win10 + cudnn-11.1-windows-x64-v8.0.4.30\r\n\r\nwhen I try to train a model I get errors that I don't understand.\r\ntensorflow 2.3 the model was built without errors.\r\nLog:\r\n\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-10-18 17:32:44.007532: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-18 17:32:44.386490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-18 17:32:44.604977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-18 17:32:44.632185: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-10-18 17:32:44.866696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2020-10-18 17:32:45.041032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-10-18 17:32:45.055357: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-10-18 17:32:45.055986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-18 17:32:45.059177: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nInstructions AVX2\r\n2020-10-18 17:47:53.948993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-18 17:47:55.715569: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-10-18 17:47:55.716176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-10-18 17:47:55.729870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-10-18 17:47:55.729946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-18 17:47:55.735471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-18 17:47:55.738640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-18 17:47:55.739592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-10-18 17:47:55.747533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2020-10-18 17:47:55.750147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-10-18 17:47:55.750569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-10-18 17:47:55.750671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-18 17:47:55.751166: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-18 17:47:55.751629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-10-18 17:47:55.751780: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-10-18 17:47:55.752125: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-18 17:47:55.752364: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-18 17:47:55.752607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-10-18 17:47:55.752854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2020-10-18 17:47:55.753097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-10-18 17:47:55.753343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-10-18 17:47:55.753635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-10-18 17:47:56.173367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-18 17:47:56.173485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2020-10-18 17:47:56.173812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2020-10-18 17:47:56.174571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6696 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-18 17:47:56.175262: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nembedding (Embedding)        (None, 131, 64)           18151104\r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 131, 64)           33024\r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 64)                33024\r\n_________________________________________________________________\r\ndense (Dense)                (None, 5000)              325000\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 5000)              25005000\r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 1)                 5001\r\n_________________________________________________________________\r\nactivation (Activation)      (None, 1)                 0\r\n=================================================================\r\nTotal params: 43,552,153\r\nTrainable params: 43,552,153\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n2020-10-18 17:47:56.892579: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)\r\nEpoch 1/5\r\n2020-10-18 17:47:58.976202: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer\r\n2020-10-18 17:47:59.278004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-10-18 17:47:59.600803: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n199/200 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.50622020-10-18 17:48:18.028039: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer\r\n200/200 [==============================] - 22s 93ms/step - loss: 0.6892 - accuracy: 0.5062 - val_loss: 0.5702 - val_accuracy: 0.5098\r\nEpoch 2/5\r\n122/200 [=================>............] - ETA: 6s - loss: 0.4413 - accuracy: 0.50562020-10-18 17:48:29.581544: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-10-18 17:48:29.581677: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n\r\nErrors:\r\ntensorflow/core/grappler/optimizers/meta_optimizer.cc:596] layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer\r\n\r\ntensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-10-18 17:48:29.581677: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1\r\n\r\n\r\nwhat do these errors mean?\r\n\r\n", "comments": ["@Expert73 \r\n please refer to these issues causes due to mismatch and let us know if it helps resolve.\r\n\r\n#42608 #41169 #33536  [link](https://stackoverflow.com/questions/55195701/tensorflow-why-does-cudnn-fails-to-launch-cuda-error-launch-failed)", "the problem like #41169. There is an assumption that the fault may be nvidia drivers for windows. Now there are the latest ones available on the Nvidia website. How do I report this issue to them? Technical support will redirect me to tensorflow developers. That is, here.\r\nwhat about layout failed: Invalid argument: Size of values 2 does not match size of permutation 4 @ fanin shape insequential/dense/BiasAdd-0-TransposeNHWCToNCHW-LayoutOptimizer ?", "We are building with CUDA 11.0, not 11.1. Adding @sanjoy and @pkanwar23 for future work.", "@Expert73 \r\nIs this still a issue", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44128\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44128\">No</a>\n"]}, {"number": 44127, "title": "Optionally return attention weights in tf.keras.layers.BaseDenseAttention to allow us to visualize them", "body": "**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, when using attention in Keras, the weights are calculated in tf.keras.layers.BaseDenseAttention._apply_scores, and then discarded with only the score returned. It would be nice if the weights could be returned as they do in the \"Neural machine translation with attention\" TensorFlow Core tutorial (you can find the tutorial [here](https://www.tensorflow.org/tutorials/text/nmt_with_attention#write_the_encoder_and_decoder_model), see in the class BahdanauAttention in the tutorial how the attention_weights are calculated and then returned). This would be nice as it would allow us to visualize the scores like they do in the tutorial, which would help us interpret our models.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes. As well as returning the regular output, it should also return the attention weights. \r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone who wants to better understand their model that uses attention. Being able to visualize the attention weights helps understand what the model is paying attention to in the input when predicting the output. \r\n\r\n**Any Other info.**\r\n\r\nI'm surprised this is not already implemented in TensorFlow, so if there's any way I can achieve this without this change, please say, as I'm sure it will help others in the future too.", "comments": ["I just realized as self-attention is parameterless, another option is just to extract from the model the inputs to the attention layer at test time and then recalculate the weights using them. For example, if using Luong-style attention you'd extract the query and key and calculate the weights as follows:\r\n\r\n`scores = tf.matmul(query, key, transpose_b=True)`\r\n`weights = tf.nn.softmax(scores)`\r\n\r\nThen the weights variable is the same that they calculate in the tutorial (although the calculations are slightly different as that's Bahdanau-style attention), so this can just be visualized to better understand what attention is actually doing. Maybe this is actually a better way to approach it instead of changing the API? ", "This feature is available in TF Nightly and will be present in the next release: you can pass `return_attention_scores=True` in the `__call__` method of the dense attention layers. When True, the layer call will return a tuple `result, attention_scores`."]}, {"number": 44125, "title": "No gradients provided for any variable when doing binarization", "body": "I have written a Lambda layer that converts an input variable of range `(0, 1)` to either 0 or 1, i.e. the layer binarizes the inputs. I did this by using `K.random_uniform(shape=K.shape(x)) <= x`. However, when wantint to train the model, I'm getting the error message, that no gradients are provided for any variable. How do I have to change the code such that my idea works? TF version is 2.2.0.\r\n\r\n    import numpy as np\r\n\r\n    from keras import backend as K\r\n    from keras.layers import *\r\n    from keras.models import Model\r\n\r\n\r\n    def binarize(d):\r\n        x1, x2 = d\r\n        return x1 * K.cast(K.random_uniform(shape=K.shape(x2)) <= x2, 'float32')\r\n\r\n\r\n    inp = Input((1,))\r\n    var = Dense(1, activation='sigmoid')(inp)\r\n    out = Lambda(binarize)([inp, var])\r\n\r\n    model = Model(inp, out)\r\n    model.compile(loss='mse', optimizer='sgd')\r\n\r\n    x = np.random.normal(size=(128, 1))\r\n    y = x >= 0\r\n    model.train_on_batch(x, y)", "comments": ["@maxbry \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/676f7c98d228a2a96d938831d6fd9d70/untitled446.ipynb), please share a colab gist if possible with the issue reported.", "[Here](https://colab.research.google.com/drive/1WnDD0CnhBdFLqolEnMuvfCjGZhLhCEt0) is a gist where the reported error is raised.", "@maxbry \r\nCould you please provide access to the colab.", "[Here](https://colab.research.google.com/drive/1WnDD0CnhBdFLqolEnMuvfCjGZhLhCEt0?usp=sharing)", "I am able to replicate this issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/34bd82ee685cacbc789d5ff9ec795de8/untitled450.ipynb).", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44125\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44125\">No</a>\n"]}, {"number": 44124, "title": "Running TFlite on mobile device GPU fails  - Ops not supported", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source):\r\nnightly\r\n\r\n.\r\n.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: Following operations are not supported by GPU delegate:\r\nDELEGATE TfLiteFlexDelegate: Operation is not supported.\r\nEQUAL: Operation is not supported.\r\nSPLIT: Operation is not supported.\r\nWHILE: Operation is not supported.\r\n62 operations will run on the GPU, and the remaining 907 operations will run on the CPU.\r\nERROR: TfLiteGpuDelegate Prepare: No shader implementation for transpose\r\nERROR: Node number 1001 (TfLiteGpuDelegate) failed to prepare.\r\n\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 813 (FlexTensorScatterUpdate) failed to prepare.\r\n.\r\n.\r\nunfortunately can't send the code.\r\n", "comments": ["@shlomi-amitai \r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using?\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "I'm working on tf nightly build.\r\nattached is my model with a python tester.\r\nc++ execution code is more problematic for me to send.\r\n[modelWrapper.zip](https://github.com/tensorflow/tensorflow/files/5400662/modelWrapper.zip)\r\n\r\n", "> DELEGATE TfLiteFlexDelegate: Operation is not supported.\r\n\r\nOf course, GPU cannot handle the flex delegate.  I'm not even sure why this is fed into the GPU delegate's prepare.  @srjoglekar246 \r\n\r\n> EQUAL: Operation is not supported.\r\n\r\nI think there's some plan to bring this in.  No ETAs though.\r\n\r\n> SPLIT: Operation is not supported.\r\n\r\nThis is not difficult to implement, but not prioritized because there was no use case.  Can this be rewritten with `STRIDED_SLICE`?  @renjie-liu Is this applicable for tac?\r\n\r\n> WHILE: Operation is not supported.\r\n\r\nControl flows are not available on the GPU delegate.  This is gonna be quite difficult, and probably can't be easily supported.  It would have to fall back to the CPU when control flow comes into play.", "> ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 813 (FlexTensorScatterUpdate) failed to prepare\r\n\r\nUsing Select TF operators (aka Flex) requires some additional changes to your BUILD, to pull in TF dependencies. See [here](https://www.tensorflow.org/lite/guide/ops_select).\r\n\r\n> ERROR: TfLiteGpuDelegate Prepare: No shader implementation for transpose\r\n\r\n@impjdi Any idea why this would occur?\r\n\r\n@shlomi-amitai In general, if performance is important, see if you can change your model authoring logic (in TensorFlow) to have ops that [TFLite supports](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc). Looks like the only op that requires Flex here is [scatter and update](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update).", "Yes, split can be unrolled into `slice` op.", "Thank you guys for your help.\r\n@impjdi, regarding the split operation. \r\nI'm converting a model from pytorch so these split operations are automatically added when generating the protobuffer (.pb)  from .onnx. I have no way, I assume, to control these additions. if it helps, I can send you the .onnx model.\r\n", "Basically, all I care now is to make this pass and run (even partly) on the device's gpu. \r\nI would like that parts of the NN that can run on GPU would do that. but currently the run failes with the above errors. \r\n\r\nI'm compiling libtesnorflowlite.so this way:\r\n1. in tensorflow/lite/BUILD -> under deps I add a flag  \"//tensorflow/lite/delegates/flex:delegate\"\r\n2. bazel build -c opt --local_ram_resources 2048 --local_cpu_resources 1 --jobs 1 --config=android_arm64 --config=monolithic tensorflow/lite:libtensorflowlite.so\r\n3. for GPU: bazel build -c opt --local_ram_resources 2048 --local_cpu_resources 1 --jobs 1 --config=android_arm64 --config=monolithic //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so", "> in tensorflow/lite/BUILD -> under deps I add a flag \"//tensorflow/lite/delegates/flex:delegate\"\r\n\r\nDoes this remove the error \"Make sure you apply/link the Flex delegate before inference.\"? Not sure if this matters, but I usually link the delegate to my client app (that is the BUILD rule of my binary that uses TFLite), rather than TFLite itself.\r\n\r\nTo debug further, can you check if the Flex error vanish *and* you can run the model successfully without the GPU? That would atleast confirm if the Flex part of this is running as intended, We can then debug the GPU part further.", "I use this flag also with libtensorflowlite.so  and there it runs smoothly on CPU", "I see. The GPU delegate `.so` BUILDs from tensorflow/lite/delegates/gpu/BUILD, so you will have  to add the Flex rule there :-). Also use the latest TF nightly for this, since we only recently added support for Flex w/ GPU", "I compiled #3 with the flex rule and I get libtensorflowlite_gpu_gl.so of size 4GB (50MB without the flag).. I can't even test the solution since I don't have this amount of space free in my device.", "to be on the same page here, I added the flex rule in tensorflow/lite/delegates/gpu/BUILD under libtensorflowlite_gpu_gl.so deps list.", "You can choose only the relevant ops to be added to the binary, since the current solution you implemented adds *all* supported TF ops to the binary. See [this](https://www.tensorflow.org/lite/guide/reduce_binary_size). Adding @abattery who has more context on this.", "Hi @shlomi-amitai\r\n\r\nYou can create your selective build-enabled flex build target in the bazel as the following snippet:\r\n\r\n```\r\nload(\"//tensorflow/lite/delegates/flex:build_def.bzl\", \"tflite_flex_cc_library\")\r\n\r\n...\r\n\r\ntlite_flex_cc_library(\r\n  name = \"my_flex_delegate\",\r\n  models = [\"model.tflite\"],\r\n)\r\n```\r\n\r\nThe above special build target will trim the unused TF operators when linking. You can replace the //tensorflow/lite/delegates/flex:delegate with your own Flex delegate, my_flex_delegate.", "Thanks for the support.\r\nI added the above code to the BUILD file, replaced \"//tensorflow/lite/delegates/flex:delegate\" with  \"my_flex_delegate\" and copied my model.tflite to the BUILD directory.\r\nthe build ended with a size of 1.12GB, which is better but yet very big.\r\nI will try and change the original model (in pytorch) to somehow reduce operator types but my questions are as follows:\r\n1. Did my actions followed your instructions?\r\n2.  Is there something more that can be done build wise to reduce the size of the SO?\r\n\r\n", "Instead of adding the flex delegate dependency to the build targets in the tensorflow/lite/BUILD file, could you follow the document to generate the tensorflow-lite-select-tf-ops.aar based on your model?\r\n\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "Thanks. \r\nI have tried this with nightly version of tensorflow and now getting this error during compilation (working with docker):\r\nERROR: /tensorflow_src/WORKSPACE:53:1: //external:android/sdk depends on @androidsdk//:sdk in repository @androidsdk which failed to fetch. no such package '@androidsdk//': Bazel does not recognize Android build tools version debian: Invalid revision: debian\r\nI will try to debug it.", "\"Selective Build for C API and iOS version is not supported currently.\"\r\n\r\nis this still valid?\r\nI'm working with the C API.", "@thaink could you answer the question at this thread?", "> \"Selective Build for C API and iOS version is not supported currently.\"\r\n> \r\n> is this still valid?\r\n> I'm working with the C API.\r\n\r\nThe selective build for C and iOS is not yet released.\r\nFor the build error when building with docker, What OS version are you using? did you enter \"y\" when the android license showed up?", "@shlomi-amitai Could you please respond to the above [comment](https://github.com/tensorflow/tensorflow/issues/44124#issuecomment-726470856) ? It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.6 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44124\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44124\">No</a>\n"]}, {"number": 44123, "title": "Tensorflow model inference different than loading weights from checkpoint into equivalent model in keras", "body": "Python version 3.8.3\r\nTensorflow 1.14 and 2.0 used\r\nWindows 10\r\nCUDA 10, Cudnn 7.6.5\r\n\r\n\r\nI can send the variable or checkpoint file if this would be helpful.\r\n\r\n\r\nI am trying to convert the following tensorflow model to keras so I can upload the weights directly, but I am not getting the correct predictions after I do this can someone tell me if I am making an obvious mistake? \r\n\r\n\r\n    class LinearModel(object):\r\n      \"\"\" A simple Linear+RELU model \"\"\"\r\n    \r\n      def __init__(self,\r\n                   linear_size,\r\n                   num_layers,\r\n                   residual,\r\n                   batch_norm,\r\n                   max_norm,\r\n                   batch_size,\r\n                   learning_rate,\r\n                   summaries_dir,\r\n                   predict_14=False,\r\n                   dtype=tf.float32):\r\n        \r\n   \r\n        self.HUMAN_2D_SIZE = 16 * 2\r\n    \r\n        self.HUMAN_3D_SIZE = 14 * 3 if predict_14 else 16 * 3\r\n    \r\n        self.input_size  = self.HUMAN_2D_SIZE\r\n        self.output_size = self.HUMAN_3D_SIZE\r\n    \r\n        self.isTraining = tf.placeholder(tf.bool,name=\"isTrainingflag\")\r\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\r\n    \r\n        # Summary writers for train and test runs\r\n        self.train_writer = tf.summary.FileWriter( os.path.join(summaries_dir, 'train' ))\r\n        self.test_writer  = tf.summary.FileWriter( os.path.join(summaries_dir, 'test' ))\r\n    \r\n        self.linear_size   = linear_size\r\n        self.batch_size    = batch_size\r\n        self.learning_rate = tf.Variable( float(learning_rate), trainable=False, dtype=dtype, name=\"learning_rate\")\r\n        self.global_step   = tf.Variable(0, trainable=False, name=\"global_step\")\r\n        decay_steps = 100000  # empirical\r\n        decay_rate = 0.96     # empirical\r\n        self.learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, decay_steps, decay_rate)\r\n    \r\n        # === Transform the inputs ===\r\n        with vs.variable_scope(\"inputs\"):\r\n    \r\n          # in=2d poses, out=3d poses\r\n          enc_in  = tf.placeholder(dtype, shape=[None, self.input_size], name=\"enc_in\")\r\n          dec_out = tf.placeholder(dtype, shape=[None, self.output_size], name=\"dec_out\")\r\n    \r\n          self.encoder_inputs  = enc_in\r\n          self.decoder_outputs = dec_out\r\n    \r\n        # === Create the linear + relu combos ===\r\n        with vs.variable_scope( \"linear_model\" ):\r\n    \r\n          # === First layer, brings dimensionality up to linear_size ===\r\n          w1 = tf.get_variable( name=\"w1\", initializer=kaiming, shape=[self.HUMAN_2D_SIZE, linear_size], dtype=dtype )\r\n          b1 = tf.get_variable( name=\"b1\", initializer=kaiming, shape=[linear_size], dtype=dtype )\r\n          w1 = tf.clip_by_norm(w1,1) if max_norm else w1\r\n          y3 = tf.matmul( enc_in, w1 ) + b1\r\n    \r\n          if batch_norm:\r\n            y3 = tf.layers.batch_normalization(y3,training=self.isTraining, name=\"batch_normalization\")\r\n          y3 = tf.nn.relu( y3 )\r\n          y3 = tf.nn.dropout( y3, self.dropout_keep_prob )\r\n    \r\n          # === Create multiple bi-linear layers ===\r\n          for idx in range( num_layers ):\r\n            y3 = self.two_linear( y3, linear_size, residual, self.dropout_keep_prob, max_norm, batch_norm, dtype, idx )\r\n    \r\n          # === Last linear layer has HUMAN_3D_SIZE in output ===\r\n          w4 = tf.get_variable( name=\"w4\", initializer=kaiming, shape=[linear_size, self.HUMAN_3D_SIZE], dtype=dtype )\r\n          b4 = tf.get_variable( name=\"b4\", initializer=kaiming, shape=[self.HUMAN_3D_SIZE], dtype=dtype )\r\n          w4 = tf.clip_by_norm(w4,1) if max_norm else w4\r\n          y = tf.matmul(y3, w4) + b4\r\n          # === End linear model ===\r\n    \r\n        # Store the outputs here\r\n        self.outputs = y\r\n        self.loss = tf.reduce_mean(tf.square(y - dec_out))\r\n        self.loss_summary = tf.summary.scalar('loss/loss', self.loss)\r\n    \r\n        # To keep track of the loss in mm\r\n        self.err_mm = tf.placeholder( tf.float32, name=\"error_mm\" )\r\n        self.err_mm_summary = tf.summary.scalar( \"loss/error_mm\", self.err_mm )\r\n    \r\n        # Gradients and update operation for training the model.\r\n        opt = tf.train.AdamOptimizer( self.learning_rate )\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    \r\n        with tf.control_dependencies(update_ops):\r\n    \r\n          # Update all the trainable parameters\r\n          gradients = opt.compute_gradients(self.loss)\r\n          self.gradients = [[] if i==None else i for i in gradients]\r\n          self.updates = opt.apply_gradients(gradients, global_step=self.global_step)\r\n    \r\n        # Keep track of the learning rate\r\n        self.learning_rate_summary = tf.summary.scalar('learning_rate/learning_rate', self.learning_rate)\r\n    \r\n        # To save the model\r\n        self.saver = tf.train.Saver( tf.global_variables(), max_to_keep=10 )\r\n    \r\n    \r\n      def two_linear( self, xin, linear_size, residual, dropout_keep_prob, max_norm, batch_norm, dtype, idx ):\r\n        \"\"\"\r\n        Make a bi-linear block with optional residual connection\r\n    \r\n        Returns\r\n          y: the batch after it leaves the block\r\n        \"\"\"\r\n    \r\n        with vs.variable_scope( \"two_linear_\"+str(idx) ) as scope:\r\n    \r\n          input_size = int(xin.get_shape()[1])\r\n    \r\n          # Linear 1\r\n          w2 = tf.get_variable( name=\"w2_\"+str(idx), initializer=kaiming, shape=[input_size, linear_size], dtype=dtype)\r\n          b2 = tf.get_variable( name=\"b2_\"+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)\r\n          w2 = tf.clip_by_norm(w2,1) if max_norm else w2\r\n          y = tf.matmul(xin, w2) + b2\r\n          if  batch_norm:\r\n            y = tf.layers.batch_normalization(y,training=self.isTraining,name=\"batch_normalization1\"+str(idx))\r\n    \r\n          y = tf.nn.relu( y )\r\n          y = tf.nn.dropout( y, dropout_keep_prob )\r\n    \r\n          # Linear 2\r\n          w3 = tf.get_variable( name=\"w3_\"+str(idx), initializer=kaiming, shape=[linear_size, linear_size], dtype=dtype)\r\n          b3 = tf.get_variable( name=\"b3_\"+str(idx), initializer=kaiming, shape=[linear_size], dtype=dtype)\r\n          w3 = tf.clip_by_norm(w3,1) if max_norm else w3\r\n          y = tf.matmul(y, w3) + b3\r\n    \r\n          if  batch_norm:\r\n            y = tf.layers.batch_normalization(y,training=self.isTraining,name=\"batch_normalization2\"+str(idx))\r\n    \r\n          y = tf.nn.relu( y )\r\n          y = tf.nn.dropout( y, dropout_keep_prob )\r\n    \r\n          # Residual every 2 blocks\r\n          y = (xin + y) if residual else y\r\n    \r\n        return y\r\n\r\nThe pretrained weights for this model are stored in a checkpoint file which I extract using the following code:\r\n\r\n    PATH_REL_META= #insert file path to checkpoint\r\n    \r\n    with tf.Session() as sess:\r\n            \r\n        #import graph\r\n        saver=tf.train.import_meta_graph(PATH_REL_META)\r\n        \r\n        #load weights for graph\r\n        saver.restore(sess,PATH_REL_META[:-5])\r\n        \r\n        #get all global variables \r\n        vars_global=tf.global_variables()\r\n        \r\n        #get their name and value and puth them into a dictionary\r\n        sess.as_default()\r\n        model_vars={}\r\n        for var in vars_global:\r\n            try:\r\n                model_vars[var.name]=var.eval()\r\n            except:\r\n                print(\"For var={}, an exception occurred\".format.name)\r\n\r\nThen I define the keras model with the following code and load the weights from the dictionary:\r\n\r\n    def two_linear(xin,linear_size,dropout_keep_prob,idx):\r\n       # Args\r\n       # xin: the batch that enters the block\r\n       # linear_size: integer. The size of the linear units\r\n       # residual: boolean. Whether to add a residual connection\r\n       # dropout_keep_prob: float [0,1]. Probability of dropping something out\r\n       # max_norm: boolean. Whether to clip weights to 1-norm\r\n       # batch_norm: boolean. Whether to do batch normalization\r\n       # Returns\r\n       # y: batch after it leaves the block\r\n       \r\n       # Linear 1\r\n       # he_uniform is the Kainming initilization in Keras\r\n       \r\n       \r\n    \r\n       y=tf.keras.layers.Dense(linear_size,kernel_initializer=\"he_uniform\",\r\n                               bias_initializer=\"he_uniform\",\r\n                               kernel_constraint=max_norm(1),\r\n                               name=\"two_linear/w2_\"+str(idx))(xin)\r\n       y=tf.keras.layers.BatchNormalization(name=\"two_linear/batch_normalization1\"+str(idx))(y,training=False)\r\n       y=tf.keras.layers.Activation('relu')(y)\r\n       y=tf.keras.layers.Dropout(dropout_keep_prob)(y,training=False)\r\n       \r\n       \r\n       y=tf.keras.layers.Dense(linear_size,kernel_initializer=\"he_uniform\",\r\n                               bias_initializer=\"he_uniform\",\r\n                               kernel_constraint=max_norm(1),\r\n                               name=\"two_linear/w3_\"+str(idx))(y)\r\n       y=tf.keras.layers.BatchNormalization(name=\"two_linear/batch_normalization2\"+str(idx))(y,training=False)\r\n       y=tf.keras.layers.Activation('relu')(y)\r\n       y=tf.keras.layers.Dropout(dropout_keep_prob)(y,training=False)  \r\n       \r\n       \r\n       y=tf.keras.layers.Add()([xin,y])\r\n       \r\n       \r\n       return y\r\n      \r\n    def ModelDef(linear_size,dropout_keep_prob,num_layers):\r\n        input_shape=(32,)\r\n        xin=tf.keras.layers.Input(input_shape)\r\n        #y_1=tf.keras.layers.Reshape((32,))(xin)\r\n        \r\n        y_1=tf.keras.layers.Dense(linear_size,kernel_initializer='he_uniform',\r\n                                bias_initializer='he_uniform',\r\n                                kernel_constraint=max_norm(1),\r\n                                name=\"linear_model/w1\")(xin)\r\n        y_1=tf.keras.layers.BatchNormalization(name=\"batch_norm/0\")(y_1,training=False)\r\n        y_1=tf.keras.layers.Activation('relu')(y_1)\r\n        y=tf.keras.layers.Dropout(dropout_keep_prob)(y_1,training=False)\r\n        \r\n        for idx in range(num_layers):\r\n            y=two_linear(y,linear_size,dropout_keep_prob,idx)\r\n            \r\n        \r\n        y=tf.keras.layers.Dense(16*3,kernel_initializer='he_uniform',\r\n                            bias_initializer='he_uniform',\r\n                            kernel_constraint=max_norm(1),\r\n                            name=\"linear_model/w4\")(y)\r\n        \r\n        model=tf.keras.Model(inputs=xin,outputs=y)\r\n        \r\n        return model\r\n    \r\n    #Create model and load weights\r\n    \r\n    weight_path=''\r\n    with open(weight_path,\"rb\") as pf:\r\n        weights=pickle.load(pf)\r\n        \r\n    model=ModelDef(1024,.1,2)\r\n    \r\n    print(model.weights[0])\r\n        model.get_layer(\"linear_model/w1\").set_weights([tf.clip_by_norm(weights[\"linear_model/w1:0\"],1),weights[\"linear_model/b1:0\"]])\r\n    model.get_layer(\"batch_norm/0\").set_weights([weights['linear_model/batch_normalization/gamma:0']\r\n                                                  ,weights['linear_model/batch_normalization/beta:0']\r\n                                                ,weights['linear_model/batch_normalization/moving_mean:0'] \r\n                                                ,weights['linear_model/batch_normalization/moving_variance:0']])\r\n    \r\n    model.get_layer(\"two_linear/w2_0\").set_weights([tf.clip_by_norm(weights[\"linear_model/two_linear_0/w2_0:0\"],1),\r\n                                                    weights[\"linear_model/two_linear_0/b2_0:0\"]])\r\n    model.get_layer(\"two_linear/batch_normalization10\").set_weights([weights[\"linear_model/two_linear_0/batch_normalization10/gamma:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization10/beta:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization10/moving_mean:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization10/moving_variance:0\"]])\r\n    \r\n    model.get_layer(\"two_linear/w3_0\").set_weights([tf.clip_by_norm(weights[\"linear_model/two_linear_0/w3_0:0\"],1),\r\n                                                   weights[\"linear_model/two_linear_0/b3_0:0\"]])\r\n    \r\n    model.get_layer(\"two_linear/batch_normalization20\").set_weights([weights[\"linear_model/two_linear_0/batch_normalization20/gamma:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization20/beta:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization20/moving_mean:0\"],\r\n                                                            weights[\"linear_model/two_linear_0/batch_normalization20/moving_variance:0\"]])\r\n    \r\n    \r\n    model.get_layer(\"two_linear/w2_1\").set_weights([tf.clip_by_norm(weights[\"linear_model/two_linear_1/w2_1:0\"],1),\r\n                                                    weights[\"linear_model/two_linear_1/b2_1:0\"]])\r\n    model.get_layer(\"two_linear/batch_normalization11\").set_weights([weights[\"linear_model/two_linear_1/batch_normalization11/gamma:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization11/beta:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization11/moving_mean:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization11/moving_variance:0\"]])\r\n    \r\n    model.get_layer(\"two_linear/w3_1\").set_weights([tf.clip_by_norm(weights[\"linear_model/two_linear_1/w3_1:0\"],1),\r\n                                                   weights[\"linear_model/two_linear_1/b3_1:0\"]])\r\n    \r\n    model.get_layer(\"two_linear/batch_normalization21\").set_weights([weights[\"linear_model/two_linear_1/batch_normalization21/gamma:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization21/beta:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization21/moving_mean:0\"],\r\n                                                            weights[\"linear_model/two_linear_1/batch_normalization21/moving_variance:0\"]])    \r\n    \r\n    model.get_layer(\"linear_model/w4\").set_weights([tf.clip_by_norm(weights[\"linear_model/w4:0\"],1)\r\n                                                    ,weights[\"linear_model/b4:0\"]])    \r\n\r\n \r\n\r\n\r\nweights contains the dictionary created from the middle block of code. Inside the dictionary I have the following keys: \r\n\r\n    learning_rate:0\r\n    global_step:0\r\n    linear_model/w1:0\r\n    linear_model/b1:0\r\n    linear_model/batch_normalization/beta:0\r\n    linear_model/batch_normalization/gamma:0\r\n    linear_model/batch_normalization/moving_mean:0\r\n    linear_model/batch_normalization/moving_variance:0\r\n    linear_model/two_linear_0/w2_0:0\r\n    linear_model/two_linear_0/b2_0:0\r\n    linear_model/two_linear_0/batch_normalization10/beta:0\r\n    linear_model/two_linear_0/batch_normalization10/gamma:0\r\n    linear_model/two_linear_0/batch_normalization10/moving_mean:0\r\n    linear_model/two_linear_0/batch_normalization10/moving_variance:0\r\n    linear_model/two_linear_0/w3_0:0\r\n    linear_model/two_linear_0/b3_0:0\r\n    linear_model/two_linear_0/batch_normalization20/beta:0\r\n    linear_model/two_linear_0/batch_normalization20/gamma:0\r\n    linear_model/two_linear_0/batch_normalization20/moving_mean:0\r\n    linear_model/two_linear_0/batch_normalization20/moving_variance:0\r\n    linear_model/two_linear_1/w2_1:0\r\n    linear_model/two_linear_1/b2_1:0\r\n    linear_model/two_linear_1/batch_normalization11/beta:0\r\n    linear_model/two_linear_1/batch_normalization11/gamma:0\r\n    linear_model/two_linear_1/batch_normalization11/moving_mean:0\r\n    linear_model/two_linear_1/batch_normalization11/moving_variance:0\r\n    linear_model/two_linear_1/w3_1:0\r\n    linear_model/two_linear_1/b3_1:0\r\n    linear_model/two_linear_1/batch_normalization21/beta:0\r\n    linear_model/two_linear_1/batch_normalization21/gamma:0\r\n    linear_model/two_linear_1/batch_normalization21/moving_mean:0\r\n    linear_model/two_linear_1/batch_normalization21/moving_variance:0\r\n    linear_model/w4:0\r\n    linear_model/b4:0\r\n    beta1_power:0\r\n    beta2_power:0\r\n    linear_model/w1/Adam:0\r\n    linear_model/w1/Adam_1:0\r\n    linear_model/b1/Adam:0\r\n    linear_model/b1/Adam_1:0\r\n    linear_model/batch_normalization/beta/Adam:0\r\n    linear_model/batch_normalization/beta/Adam_1:0\r\n    linear_model/batch_normalization/gamma/Adam:0\r\n    linear_model/batch_normalization/gamma/Adam_1:0\r\n    linear_model/two_linear_0/w2_0/Adam:0\r\n    linear_model/two_linear_0/w2_0/Adam_1:0\r\n    linear_model/two_linear_0/b2_0/Adam:0\r\n    linear_model/two_linear_0/b2_0/Adam_1:0\r\n    linear_model/two_linear_0/batch_normalization10/beta/Adam:0\r\n    linear_model/two_linear_0/batch_normalization10/beta/Adam_1:0\r\n    linear_model/two_linear_0/batch_normalization10/gamma/Adam:0\r\n    linear_model/two_linear_0/batch_normalization10/gamma/Adam_1:0\r\n    linear_model/two_linear_0/w3_0/Adam:0\r\n    linear_model/two_linear_0/w3_0/Adam_1:0\r\n    linear_model/two_linear_0/b3_0/Adam:0\r\n    linear_model/two_linear_0/b3_0/Adam_1:0\r\n    linear_model/two_linear_0/batch_normalization20/beta/Adam:0\r\n    linear_model/two_linear_0/batch_normalization20/beta/Adam_1:0\r\n    linear_model/two_linear_0/batch_normalization20/gamma/Adam:0\r\n    linear_model/two_linear_0/batch_normalization20/gamma/Adam_1:0\r\n    linear_model/two_linear_1/w2_1/Adam:0\r\n    linear_model/two_linear_1/w2_1/Adam_1:0\r\n    linear_model/two_linear_1/b2_1/Adam:0\r\n    linear_model/two_linear_1/b2_1/Adam_1:0\r\n    linear_model/two_linear_1/batch_normalization11/beta/Adam:0\r\n    linear_model/two_linear_1/batch_normalization11/beta/Adam_1:0\r\n    linear_model/two_linear_1/batch_normalization11/gamma/Adam:0\r\n    linear_model/two_linear_1/batch_normalization11/gamma/Adam_1:0\r\n    linear_model/two_linear_1/w3_1/Adam:0\r\n    linear_model/two_linear_1/w3_1/Adam_1:0\r\n    linear_model/two_linear_1/b3_1/Adam:0\r\n    linear_model/two_linear_1/b3_1/Adam_1:0\r\n    linear_model/two_linear_1/batch_normalization21/beta/Adam:0\r\n    linear_model/two_linear_1/batch_normalization21/beta/Adam_1:0\r\n    linear_model/two_linear_1/batch_normalization21/gamma/Adam:0\r\n    linear_model/two_linear_1/batch_normalization21/gamma/Adam_1:0\r\n    linear_model/w4/Adam:0\r\n    linear_model/w4/Adam_1:0\r\n    linear_model/b4/Adam:0\r\n    linear_model/b4/Adam_1:0\r\n\r\nThe predictions I am getting from the Keras model are way off from directly loading the tensorflow checkpoint data and making predictions. Is there something obvious I am missing that someone can point me in the right direction to correct?\r\n\r\nThanks in advance and sorry for the long post!\r\n", "comments": ["@alecda573,\r\nThe code provided is fairly complex, and it is hard for us to pinpoint the issue. Can you please get the example down to the simplest possible repro? That will allow easily determine the source of the issue. \r\n\r\nAlso, could you please update TensorFlow to v2.3 and check if you are facing the same issue. Thanks!", "@amahendrakar I understand it is a lot of code, but the error comes from the predictions, so one needs both models with their corresponding weights to see the difference. I can share the input and the expected output from the checkpoint file, but the error is either in the way the layers are defined in tf 1.14 vs the layers in tf2.0 or something with loading the weights. \r\n", "@amahendrakar \r\n\r\nHere is a test input\r\n\r\ninputs=[ 1.20055843 , 9.5001441 , 1.28896216, 9.20799957, 1.02979768 ,14.61186754,\r\n1.34260403 ,15.19059108 , 1.06410921 ,9.80336169 , 0.21302668, 16.85347291,\r\n0.66403191 ,19.87040819 ,-0.91137277 , 5.88071359 ,-0.159872 , 5.91967813,\r\n-2.33728939 , 5.72899468 ,-1.01297238, 6.04449154 , 0.23375924, 7.69306223,\r\n-0.58144358, 7.37566207 ,-0.80297561, 5.55409911 , 0.0400928 , 7.04556159,\r\n-0.24246983 , 6.69960958]\r\n\r\nand expected output\r\n\r\n-1.4627209 0.06322208 -1.5446033 -0.8570251 0.10563412 -1.7503448\r\n-1.0881038 -0.21964559 -1.710271 1.4627217 -0.06322674 1.5446105\r\n1.5793833 -0.23196149 -0.19457686 -0.09048957 -0.8984171 0.21485351\r\n1.5640202 3.0261412 -2.4134905 2.2451577 4.6926637 -2.7548583\r\n1.7437289 4.536124 -2.8321414 1.8803942 4.6508107 -2.988177\r\n2.3924298 4.6636 -2.0411975 1.4061872 4.8059707 -1.3145562\r\n0.7168709 4.5796676 -1.5989876 0.23200849 4.1992345 -2.8457897\r\n-0.0461012 3.92069 -2.0317354 0.27443174 3.9237206 -2.2436898\r\n\r\nhttps://www.dropbox.com/sh/1tmttjfcmebluil/AABAjy2ItvvZpUzdTYHWECy_a?dl=0 -- checkpoint that gives correct predictions\r\nweight dictionary also in this folder\r\nhttps://github.com/alecda573/Frozen_Graph-and-HDF5 -- frozen graph", "Don't know it is the same problem or not but, it is also occur to me and I create a repo that is reproducible on both my window and mac machine\r\n\r\n@amahendrakar \r\n\r\n[This](https://github.com/sainttail/tensorflow-load-weights-problem) is the full standalone reproducible problem when `load_weights` each time get you a random evaluation loss and accuracy", "@sainttail did you ever find a solution to the problem? ", "@alecda573 No I try both `load_weights()` and `tf.keras.models.load_model()` same problem. \r\n\r\nSince my model is fairly simple, if I can't find solution soon I will have to use other framework", "@sainttail it seems there are issues between weights generated in tensorflow and passing to keras and even reloading weights just within keras. I would love some explanation of this or suggested fixes from @amahendrakar ", "@alecda573,\r\nSorry for the delayed response. I am facing an error stating `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 45: invalid start byte` on running the code. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/15607eccd17d1fc1bf540ecb0e0c3892/44123.ipynb#scrollTo=8qtzsBcF8fQe). \r\n\r\nCould you please provide a repo link as @sainttail has provided or a Colab notebook of the code, so that we can reproduce the issue on our end. Thanks!", "> @amahendrakar\r\n> \r\n> [This](https://github.com/sainttail/tensorflow-load-weights-problem) is the full standalone reproducible problem when `load_weights` each time get you a random evaluation loss and accuracy\r\n\r\n@sainttail,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44122, "title": "Hello World Ubuntu 16.04 with nvidia 1660 super", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:0.4.0\r\n- Python version:2.7.12\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12)\r\n- CUDA/cuDNN version:V7.5.17\r\n- GPU model and memory: NVIDIA 1660 SUPER GIGABYTE 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI do the command make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test to run the hello_world_test and print me this :\r\n\r\ntensorflow/lite/micro/tools/make/Makefile:413: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:413: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ng++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter  -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/kissfft -c tensorflow/lite/micro/kernels/fully_connected.cc -o tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o\r\ntensorflow/lite/micro/kernels/fully_connected.cc: In function \u2018TfLiteStatus tflite::{anonymous}::EvalQuantized(TfLiteContext*, TfLiteNode*, const tflite::{anonymous}::OpData&, const TfLiteEvalTensor*, const TfLiteEvalTensor*, const TfLiteEvalTensor*, TfLiteEvalTensor*)\u2019:\r\ntensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n TfLiteStatus EvalQuantized(TfLiteContext* context, TfLiteNode* node,\r\n              ^\r\ntensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\ntensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\ntensorflow/lite/micro/kernels/fully_connected.cc:135:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:29:0,\r\n                 from tensorflow/lite/micro/kernels/fully_connected.cc:20:\r\n./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n     TFLITE_DCHECK_LT(i, size_);\r\n                               ^\r\n./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n     TFLITE_DCHECK_LT(i, size_);\r\n                               ^\r\ntensorflow/lite/micro/kernels/fully_connected.cc: In function \u2018TfLiteStatus tflite::{anonymous}::Eval(TfLiteContext*, TfLiteNode*)\u2019:\r\ntensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n              ^\r\ntensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\ntensorflow/lite/micro/kernels/fully_connected.cc:203:14: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:29:0,\r\n                 from tensorflow/lite/micro/kernels/fully_connected.cc:20:\r\n./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n     TFLITE_DCHECK_LT(i, size_);\r\n                               ^\r\n./tensorflow/lite/kernels/internal/types.h:197:31: error: assuming signed overflow does not occur when assuming that (X - c) > X is always false [-Werror=strict-overflow]\r\n     TFLITE_DCHECK_LT(i, size_);\r\n                               ^\r\ncc1plus: all warnings being treated as errors\r\ntensorflow/lite/micro/tools/make/Makefile:424: recipe for target 'tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/linux_x86_64/obj/tensorflow/lite/micro/kernels/fully_connected.o] Error 1\r\n \r\nHow can i fix it and run ?\r\n\r\n\r\n\r\n**Any other info / logs**\r\nI didn't change anything i run it like i get the source code from github.\r\n", "comments": ["@aloizo03 \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/29524) for the location of the makefile and let us know.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44122\">No</a>\n"]}, {"number": 44121, "title": "trt_convert error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2, 8.0\r\n- GPU model and memory: Nvidia Xavier\r\n\r\n\r\nI have keras model that I'm trying to convert to tensorrt using TrtGraphConverterV2. The model takes image and classifies it. \r\n\r\n    def my_input_fn():\r\n\t\timg = cv2.imread('messi5.jpg',0)\r\n\t\timg = np.expand_dims(img, axis=0)\r\n\t\treturn img\r\n\r\n    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\r\n    conversion_params = conversion_params._replace(\r\n        max_workspace_size_bytes=(1<<32))\r\n    conversion_params = conversion_params._replace(precision_mode=\"FP16\")\r\n    conversion_params = conversion_params._replace(\r\n        maximum_cached_engines=100)\r\n\r\n    converter = trt.TrtGraphConverterV2(\r\n        input_saved_model_dir=\"input_model/\", conversion_params=conversion_params)\r\n    converter.convert()\r\n    converter.build(input_fn=my_input_fn)\r\n    converter.save(\"tensorrt_model\")\r\n\r\n\r\nWith this code I'm getting:\r\n2020-10-17 21:47:52.620064: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:588] Running native segment forTRTEngineOp_0_0 due to failure in verifying input shapes: Input shapes do not match input partial shapes stored in graph, for TRTEngineOp_0_0: [[480,768,3]] != [[1,480,768,3]]\r\n2020-10-17 21:47:52.677691: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at transpose_op.cc:157 : Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\r\n2020-10-17 21:47:52.677916: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at trt_engine_op.cc:401 : Invalid argument: transpose expects a vector of size 3. But input(1) is a vector of size 4\r\n         [[{{node StatefulPartitionedCall/model_1/conv2d_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\nTraceback (most recent call last):\r\n  File \"convert_to_tensorrt.py\", line 118, in <module>\r\n    converter.build(input_fn=my_input_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1174, in build\r\n    func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1657, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1675, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1724, in _call_with_flat_signature\r\n    return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1926, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  transpose expects a vector of size 3. But input(1) is a vector of size 4\r\n         [[{{node StatefulPartitionedCall/model_1/conv2d_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\n         [[TRTEngineOp_0_0]] [Op:__inference_pruned_16623]\r\n  <br/>\r\n<br/>\r\nIf I remove expand_dims I'm getting:\r\nTraceback (most recent call last):\r\n  File \"convert_to_tensorrt.py\", line 118, in <module>\r\n    converter.build(input_fn=my_input_fn)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py\", line 1174, in build\r\n    func(*map(ops.convert_to_tensor, inp))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1657, in __call__\r\n    return self._call_impl(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py\", line 247, in _call_impl\r\n    args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1675, in _call_impl\r\n    return self._call_with_flat_signature(args, kwargs, cancellation_manager)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1697, in _call_with_flat_signature\r\n    len(args)))\r\nTypeError: pruned(x) takes 1 positional arguments but 480 were given", "comments": ["@PawelFaron \r\n\r\nCan you please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44121\">No</a>\n"]}, {"number": 44120, "title": "Disable internal mkldnn_sgemm calls", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version:latest \r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):3.3.0\r\n- GCC/Compiler version (if compiling from source): gcc 8.3\r\n- I did **NOT** use the --config=mkl flag while building\r\n- The image is a call graph of 1024X1024 Matmul done 100 times, if you look carefully mkldnn_sgemm is called internally by Eigen, this is what I want to disable.\r\n \r\n![internal_eigen](https://user-images.githubusercontent.com/44555985/96214609-7444a200-0f99-11eb-988b-c4f9b32c6d2d.jpg)\r\n\r\n\r\nAfter some amount of reading I found out that MKL/mkl_dnn can be called internally by Eigen and after reading the Bazel documentation and seeing how TensorFlow is strucured.I want to disable mkl/mkl_dnn completely if eigen is to be used.\r\nThe Eigen[ build files loads the ifmkl symbol](https://github.com/tensorflow/tensorflow/blob/bd35e19edbf55bf3f6b5d10545725d41a99aa9ba/third_party/eigen3/BUILD#L5) from the mkl directory.\r\nMy next step was to look at the [if_mkl function inside the build_defs.bzl](https://github.com/tensorflow/tensorflow/blob/c63b59ce648b525c4bbb2d85f04f5b71f1849ff2/third_party/mkl/build_defs.bzl#L17)   \r\n\r\nI changed the [includes attribute in the cclibrary rule in the BUILD file of eigen](https://github.com/tensorflow/tensorflow/blob/44aace846df438bf55e88dad1e047b7c81f9efb9/third_party/eigen3/BUILD#L36) to [\"//conditions:default\"]\r\n\r\nwhich game me an error saying `ModuleNotFoundError: No module named 'portpicker' `\r\n\r\nSo pip installed portpicker `pip install portpicker`\r\nAnd the build completed sucessfully, but the profile literally shows no difference mkldnn_sgemm is still bieng called the same number of time by eigen internally\r\n\r\nNOTE: **The Ultimate aim is to disable mkldnn which is bieng called from inside eigen**", "comments": ["Hi @electricSamarth,\r\n\r\nThis `mkldnn_sgemm` call in vanilla TensorFlow (non-MKL) build through Eigen is intentional and not a bug. To disable it, you can just add `--define tensorflow_mkldnn_contraction_kernel=0` to your build command. No code modifications necessary.\r\nhttps://github.com/tensorflow/tensorflow/blob/20ca86ca6622ba66b6c970b221729ff9b6f931c6/tensorflow/core/kernels/BUILD#L122-L132\r\n\r\nMay I ask why you would like to disable `mkldnn_sgemm`? Based on our experiments, it performs at least as well as Eigen contractions for AVX and AVX2, and is better than Eigen for AVX-512. It also has dynamic dispatch, which means it can utilize up to AVX-512 instructions even when TensorFlow is compiled with older flags (e.g., AVX) -- Eigen can only use the vector instructions TensorFlow is compiled with.\r\n\r\ncc: @TensorFlow-MKL @ezhulenev ", "Thanks a lot @penpornk \r\nI am a 3rd year undergraduate student working on a research project at [CCBD](http://research.pes.edu/cloud-computing-big-data/research-team/) under [Dr. Reetinder Sidhu](https://scholar.google.com/citations?user=CzPQAekAAAAJ&hl=en) and we are trying to accelerate TF as a framework on FPGAs. I needed to conduct some benchmarks and wanted to compare the difference b/w performance of MKL and the vanilla install and got a little confused with all the mkl-dnn calls. \r\n\r\nFurthermore, I have a few queries and I would be very very grateful if you can answer:\r\n\r\n1) These MKL-DNN calls are not calling sgemm from MKL, the closed source library as I did not specify the `--config=mkl` flag, right?\r\n\r\n2) What would be the best way to find out how much percentage of resources are being spent on doing sgemm (resources could be IR reads as in case of profilers like Callgrind or simply time as in case of the Tensor-board profiler that comes with TF)\r\n\r\n3) I have in fact tried both the methods but the problem with callgrind is [cycles](https://valgrind.org/docs/manual/cl-manual.html#cl-manual.cycles) which makes inclusive costs senseless and in the case of Tensor-board profiler, I am getting confused as to which metric I should be looking at? `The Cumulative total-self time on Host (%)` **OR**  `Total self-time on Host (%)`  \r\n\r\n", "@electricSamarth Thank you for the explanation!\r\n\r\n1. Both vanilla TF and TF-MKL (`--config=mkl`) only depend on the open-source [oneDNN](https://github.com/oneapi-src/oneDNN) library (formerly-known as MKL-DNN) and not the closed-source MKL. Here is the source of [dnnl_sgemm](https://github.com/oneapi-src/oneDNN/blob/f57921d3b7f18c07d2c07168fccad13749cb647f/src/cpu/gemm/gemm.cpp#L262-L267) (new name of `mkldnn_sgemm`).\r\n\r\n2. I'd recommend [Google Perf Tools + pprof](https://riptutorial.com/cplusplus/example/19179/profiling-cpu-usage-with-gcc-and-google-perf-tools). It can generate a call graph annotated with running time ([like this](https://gperftools.github.io/gperftools/pprof-test.gif)) and highlight the critical path. Or [perf](https://dev.to/etcwilde/perf---perfect-profiling-of-cc-on-linux-of) tool. If you're fine with op-level profiling (as opposed to c++ function call-level), TensorBoard/TensorFlow profiler is great too.\r\n\r\n> in the case of Tensor-board profiler, I am getting confused as to which metric I should be looking at? The Cumulative total-self time on Host (%) OR Total self-time on Host (%)\r\n\r\n3. I assume you're on the TensorFlow Stats page. According to @qiuminxu , `Cumulative total-self time on Host (%)` is just the sum of `Total self-time on Host (%)` from the rank 1 op (in row 1 of the table) to the op you're looking at. So the numbers in that column are monotonically increasing. I think you'd be more interested in `Total self-time on Host (%)`.", "@penpornk Thanks for all the information, this is by far the most informative interaction I've had on GitHub.\r\n\r\nBut I tried installing TF with `--config=mkl` and the build failed, to resolve this I installed mkl from [Intel's official website](https://software.intel.com/content/www/us/en/develop/tools/math-kernel-library/choose-download/linux.html) and it built successfully. What could be the reason for this and this happened on not one but two machines\r\n1) My own machine: DELL G7-7588 running Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz\r\n2) Server at the Lab I work at running Intel(R) Xeon(R) CPU E3-1220 V2 @ 3.10GHz\r\n\r\nAlso the following returns true if MKL(the closed source library is installed & TF is built with --config=mkl flag) and returns false in 'Vanilla' Install we have been talking about\r\n`import tensorflow`\r\n`print(\"Is MKL enabled?{}\".format(tensorflow.pywrap_tensorflow.IsMklEnabled()))`\r\n", "@agramesh1 ^this^", "@claynerobison Hi!\r\nI am a little confused :)\r\nI was about to close this as no response, but I shall keep it open for further discussion from you and @agramesh1 ", "Hi @electricSamarth,\r\n\r\n> But I tried installing TF with --config=mkl and the build failed, to resolve this I installed mkl from Intel's official website and it built successfully. What could be the reason for this and this happened on not one but two machines\r\n\r\nWhat was the error message? We'll need more information. Could you please start a new Build/Installation issue and fill all necessary details? Here a [quick link](https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Abuild%2Finstall&template=10-build-installation-issue.md) to the form. Please state in the topic that this is TF-MKL build and tag me and the @TensorFlow-MKL team.\r\n\r\n> Also the following returns true if MKL(the closed source library is installed & TF is built with --config=mkl flag) and returns false in 'Vanilla' Install we have been talking about\r\nimport tensorflow\r\nprint(\"Is MKL enabled?{}\".format(tensorflow.pywrap_tensorflow.IsMklEnabled()))\r\n\r\n`IsMKLEnabled` checks whether the build is TF-MKL build or not (returns true if the build is TF-MKL, and returns false with vanilla TF). The function name is a bit misleading now that both builds use oneDNN (a.k.a. MKL-DNN). I'll discuss with the @TensorFlow-MKL team on whether we should rename it.\r\n\r\nThe main difference between vanilla TF and TF-MKL is:\r\n* Vanilla TF only uses `mkldnn_sgemm` routine as a basic building block (through Eigen) for all matmul/convolution ops in TF.\r\n* TF-MKL replaces many TF ops with its custom ops that call DNN primitives from the oneDNN library. (See op list [here](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/common_runtime/mkl_layout_pass.cc;drc=b348ad40bcee666e11cefd0388fa78577f76df7b;l=243-375).) \r\n\r\n\r\n", "Thanks @penpornk, we will look into it.\r\n@electricSamarth if you are using the latest master or 2.4 branch, the build no longer uses the binary MKL. We made changes over the last 1 or 2 months to replace cblas sgemms with oneDNN sgemms. (oneDNN is newer name for MKL DNN). Please try one of these branches. ", "Thanks a ton! @penpornk & @agramesh1 \r\nAnd yes, I shall open a new issue with the install/build template. I will also take a look/try the new branches. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44120\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44120\">No</a>\n"]}, {"number": 44119, "title": "Latency Tensorflow serving with docker for video analysis", "body": "I would like to ask a question regarding the response time of a tensorflow model in docker. I transformed a Yolo model from .h5 into a model API with the function.\r\n\r\nI use a Ubuntu machine with an i9 CPU and an RTX 2080 TI graphics card\r\n\r\n```\r\nModel.save(\r\n    filepath,\r\n    overwrite=True,\r\n    include_optimizer=True,\r\n    save_format=None,\r\n    signatures=None,\r\n    options=None,\r\n)\r\n```\r\nI took a docker image from tensorflow/serving:latest-gpu which I copied my models as follows:\r\n\r\n```\r\nFROM tensorflow/serving:latest-gpu\r\nCOPY models /apps/\r\nEXPOSE 8500\r\nEXPOSE 8501\r\n```\r\nI wrote the following script to analyze a video\r\n\r\n```\r\nimg = img.astype('float32')\r\n    img = img / 255.0\r\n\r\n    data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": img.tolist()})\r\n\r\n\r\n        }\r\n\r\n    headers = {\"content-type\": \"application/json\"}\r\n    start_time = time.time()\r\n    json_response = requests.post('http://172.27.240.5:8501/v1/models/vetements:predict', data=dat   a, headers=headers)\r\n    print(\"--- %s seconds ---\" % (time.time() - start_time))\r\n\r\n    predictions = json.loads(json_response.text)['predictions']\r\n```\r\nmy question is as follows the rest time to analyze a single image is 0.5 seconds, please does it have a way to speed up the execution of analysis\r\n\r\nthank you", "comments": ["@userEnsb \r\nPlease create this issue in serving repo and move this to closed status a sit is tracked at the serving repo.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44117, "title": "[ROCm] Fix for ROCm CSB Breakage - 201017", "body": "The following commit introduces a new subtest that fails on the ROCM platform.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/e31505fdee68ca72885cc60b170939bb8efcce0d\r\n\r\nThe subtest is for the `tf.math.efrcinv` op uses the `ndtri` op. The (Eigen) implementation for the `ndtri` op on the ROCM platform has a known bug in it. That bug has been fixed (for reference - internal JIRA ticket 236756), and will be available in a forthcoming ROCm release.\r\n\r\nFor the time being (i.e. until that ROCm release is out) we need to skip this subtest on the ROCm platform, and hence this commit.\r\n\r\n\r\n--------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": []}, {"number": 44116, "title": "3080 & 3090 coumpute capability 86 degraded performance after some updates", "body": "This issue is apparent from the difference in performance in NGC containers https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow . For 20.08 se-resnext101 example training performance\r\n```\r\npython nvidia-examples/resnet50v1.5/main.py --arch=se-resnext101-32x4d --batch_size=64  --warmup_steps 200 --data_dir=/hdd/datasets/imagenet/tf/train/ --gpu_memory_fraction 0.95  --precision fp32  --results_dir=/toy/tmp/results_dir/   --mode=training_benchmark   --use_tf_amp --use_xla\r\n```\r\n(have to adapt directories)\r\non 3080 is around 370-400 img/sec. While on 20.09 container it's more like 115 img/sec. This is also similar for resnet-50 and most likely all other CNN benchmarks. This is not an issue with my setup, it's the same for other folks - you can view discussion at https://www.pugetsystems.com/labs/hpc/RTX3090-TensorFlow-NAMD-and-HPCG-Performance-on-Linux-Preliminary-1902/", "comments": ["Can you please report this on the [NVIDIA developer forum](https://forums.developer.nvidia.com/c/ai-deep-learning/deep-learning-framework/tensorflow/101)?\r\n\r\nCC @nluehr \r\n\r\nWe can circle back here if/when this is triaged down to an issue with the TF nightly and/or TF release builds.", "3090's performance on 20.10 tf1 ngc container is even 15-20% better than on 20.08, so I guess we can just agree that we should never use 20.08 container because it sucks and let go of this issue. I'll later also try to run some tests on 20.10 tf2 and report back.\r\n\r\nEdit: seems like might be very different for different cases, though. Got to test more. Will report later.", "Ran extensive benchmarks https://fsymbols.com/3080-3090-benchmarks/", "(And the performance was pretty inconsistent, you better take a look.)", "Retested on 20.11 container. 3080 performance still effed up https://fsymbols.com/3080-3090-benchmarks/ It still has Cudnn 8.04 and same CUDA version as 20.10 container, though.", "Could you please test in the latest  `22.01-tf1` https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow container and let us know if you are getting good performance. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44115, "title": "Fix nasnet.py default weights", "body": "Default `weights` of `nasnet.py` should be `'imagenet'` instead of `None`", "comments": []}, {"number": 44114, "title": "TFlite RuntimeError: Model resulted in Nan value during calibration. Please make sure model results in all real-values during inference with provided dataset.Node number 37 (CONV_2D) failed to invoke.", "body": "I use tf 2.3 and this error occurs when doing full integer quantization.\r\n`RuntimeError: Model resulted in Nan value during calibration. Please make sure model results in all real-values during inference with provided dataset.Node number 37 (CONV_2D) failed to invoke.`\r\n\r\n**Code (just a demo how I do quantize and it can't reproduce error)**\r\n```\r\ndef representative_dataset_gen():\r\n    for x in validation_fingerprints:\r\n      x = x[np.newaxis,:]\r\n      yield [x]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(flags.train_dir + '/last_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_dataset_gen\r\nlast_quant_model = converter.convert()\r\nwith open(flags.train_dir + '/quant_last_model.tflite', 'wb') as w:\r\n  w.write(last_quant_model)\r\n```\r\n**Some config**\r\ntype(validation_fingerprints): <class 'numpy.ndarray'>\r\nshape(validation_fingerprints): (3093, 16384)\r\ntype(x): <class 'numpy.ndarray'>\r\nshape(x): (1,16384)\r\nThe model_summary\r\n[model_summary.txt](https://github.com/tensorflow/tensorflow/files/5395787/model_summary.txt)\r\n\r\nCan anyone help? Thanks.", "comments": ["@Saduf2019 Do you know how to solve it?", "@tu1258 \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8dbdd46c8119ff67d2f7dd0e878a45b8/untitled446.ipynb).", "@Saduf2019 I check the gist and don't find it relative to my issue. Could you explain it a bit? I guess you reply to a wrong thread.\r\n\r\nEdit: yes your reply is for #44111 ", "@tu1258 \r\nApologies for the incorrect gist, please find the correct [gist here](https://colab.research.google.com/gist/Saduf2019/5fa92c1e15e768f29a8851a5e2279ef4/untitled446.ipynb).", "@Saduf2019 bruh \r\n\r\n> just a demo how I do quantize and it can't reproduce error\r\n\r\nI completely understand your workflow(reproduce the error and find another expert to solve it I guess?) and I appreciate that. But reproducing the error may be complicated and the code would be a little hard to trace. Could you just route this issue to that expert since the information above is enough imo.", "@tu1258 \r\nWe will not be able to replicate the issue without stand alone or if possible share a colab gist with the  error reported.\r\n", "@Saduf2019 \r\nAlright, I finally minimize the code and please take a look! You have to read model and data I attach below tho.\r\n\r\n### **Code**\r\n```\r\nimport os\r\nimport sys\r\nimport absl.logging as logging\r\nimport numpy as np\r\nimport tensorflow.compat.v1 as tf\r\n\r\nnp.set_printoptions(threshold=sys.maxsize)\r\ntf.reset_default_graph()\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\ntf.keras.backend.set_session(sess)\r\n\r\nread_fingerprints = np.loadtxt('./testbench/best/testset_raw.csv.gz', delimiter=',')\r\n# logging.info(type(read_fingerprints))\r\n# logging.info(read_fingerprints.shape)\r\ntest_fingerprints = read_fingerprints.reshape(-1,16384).astype(np.float32)\r\nprint('type(test_fingerprints):', type(test_fingerprints))\r\nprint('shape(test_fingerprints):', test_fingerprints.shape)\r\n# logging.info(type(test_fingerprints))\r\n# logging.info(test_fingerprints.shape)\r\ndef representative_dataset_gen():\r\n  for x in test_fingerprints:\r\n    x = x[np.newaxis,:]\r\n    yield [x]\r\n\r\ntrain_dir = 'tcn/kws_7x36_1'\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(train_dir + '/best_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.allow_custom_ops = True\r\n# converter.inference_type = tf.lite.constants.FLOAT\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_dataset_gen\r\nquant_best_model = converter.convert()\r\nwith open(train_dir + '/quant_best_model.tflite', 'wb') as w:\r\n  w.write(quant_best_model)\r\n \r\n```\r\nThe data (correspond to `np.loadtxt('./testbench/best/testset_raw.csv.gz', delimiter=',')`)\r\nhttps://drive.google.com/file/d/1Fc-j_oqhD43kZO6OTn00C8p0PSF08prX/view?usp=sharing\r\nThe model (correspond to `tf.lite.TFLiteConverter.from_saved_model(train_dir + '/best_model'`)\r\n[best_model.zip](https://github.com/tensorflow/tensorflow/files/5413570/best_model.zip)\r\n\r\nIt should be able to reproduce now, please take a look ASAP.", "@Saduf2019 Please let me know if there is any question?", "I think I should quit and close issue and move on due to the late response since this is not my first priority and I wasted too much time on it. Thanks for your reply tho.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44114\">No</a>\n", "@tu1258 \r\nI ran the code shared and faced a different error which was been analysed, hence the delay, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/4041c638b0bf4484f2aed4377e30c843/untitled450.ipynb)."]}, {"number": 44113, "title": "Loading checkpoint.data weights into equivalent keras model producing horrible inference results", "body": "Python V 3.8.3\r\nTF 1.14 -- class model built here\r\nTF 2.0 -- functional model built here \r\nRTX 2060 \r\nCUDA 10.0.13\r\nCudnn 7.6.5\r\nWindows 10\r\n**Describe the current behavior**\r\n\r\n\r\n\r\n", "comments": []}, {"number": 44112, "title": "Unable to fit against and batch ragged output in Keras.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Reproduced on Arch Linux & macOS Catalina.\r\n- TensorFlow installed from (source or binary): Binary(?) with pip.\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087\r\n- Python version: 3.8.6\r\n\r\n**Describe the current behavior**\r\n\r\nI'm working on a text autoencoder in tensorflow to reduce the dimensionality of large text files (> 50000 chars, > 100000 vocab). I feel like I've got what should be a working solution, but run into `ValueError: TypeError: object of type 'RaggedTensor' has no len()`. My program also can't function when using a batch size of greater than 1, instead producing `tensorflow.python.framework.errors_impl.InvalidArgumentError: PartialTensorShape: Incompatible shapes during merge`. It seems that this kind of use case has been supported in Tensorflow for a long time, and I'm rather confused as to why I've been running into so much difficulty with ragged tensors. My code is linked in Google Colab below. Am I doing something wrong, is this a bug in Tensorflow, or unimplemented functionality? Thanks in advance.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorflow fits happily against ragged truths and is able to batch ragged outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/13yYGVDAsf8sfoPrbYPtEsrJxZz8OSoVU?usp=sharing\r\n\r\n**Exception logs**\r\n\r\n[batched-log.txt](https://github.com/tensorflow/tensorflow/files/5395543/batched-log.txt)\r\n[unbatched-log.txt](https://github.com/tensorflow/tensorflow/files/5395544/unbatched-log.txt)\r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/8bd24ebef91577830bda50cd2178b1d7/44112.ipynb). \r\n\r\nOn running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/9fc55134d980288283df0c4e163b216e/44112-tf-nightly.ipynb#scrollTo=1FkK0JPRS2cd), the error changes to `AttributeError: module 'tensorflow' has no attribute 'python'`. Please find the attached gist. Thanks!", "@amahendrakar In the gist that you attached, the issue is with the user defined code in RepeatNVectors.call. It should just use ```tf.shape(x)``` instead of ```tf.python.ops.array_ops.shape```. The original issue was trigged by attempting to convert a ragged tensor to a standard tensor in the loss function utils. I believe it should be addressed by PR #45015 ", "> @amahendrakar In the gist that you attached, the issue is with the user defined code in RepeatNVectors.call. It should just use `tf.shape(x)` instead of `tf.python.ops.array_ops.shape`. The original issue was trigged by attempting to convert a ragged tensor to a standard tensor in the loss function utils. I believe it should be addressed by PR #45015\r\n\r\nNice!  Thanks for getting back to me on this, and thanks for the fix-in-progress :)", "Note that supporting RaggedTensors as targets (e.g., in `model.fit`) is a frequent request, see #44988, #44112, #43591, #43093, #42320, #41810.\r\n\r\nSome partial progress is in pull requests #45060 and #45015, which will allow using custom losses in Keras model (but existing losses like MSE or (S)CE will still not work).", "Note that I created a new feature request #45403 to support RaggedTensors in standard Keras loss functions.", "I think there's also an issue of documentation, it seems to suggest that ragged tensors should mostly just work out-of-the-box in Keras.  This is probably why these are incoming as bugs rather than feature request.\r\n\r\nSee https://www.tensorflow.org/guide/ragged_tensor#what_you_can_do_with_a_ragged_tensor\r\n> Ragged tensors are supported by many TensorFlow APIs, including Keras, Datasets, tf.function, SavedModels, and tf.Example. For more information, see the section on TensorFlow APIs below.\r\n\r\nand https://www.tensorflow.org/guide/ragged_tensor#tensorflow_apis\r\n>  Ragged tensors may be passed as inputs to a Keras model by setting ragged=True on tf.keras.Input or tf.keras.layers.InputLayer. Ragged tensors may also be passed between Keras layers, and returned by Keras models.\r\n\r\nWhile it only says \"returned by Keras models\" (which seems to be true), it doesn't mention the inability use keras APIs to train a model returning ragged tensors (with or without a custom loss function).", "I ran the code shared and issue exists on tf-nightly [2.5.0-dev20210114], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/9b48a4c108534eb00bf655e26f7fdfdf/untitled499.ipynb#scrollTo=_IpQoLNDHgyt)", "@Saduf2019 you should note that my original code made a mistake (leveraging an internal API) causing the apparent error in TF >= 2.4.  Quoting @pedro-r-marques:\r\n\r\n>  It should just use tf.shape(x) instead of tf.python.ops.array_ops.shape\r\n\r\nTo actually retest against the nightly build, the gist needs to be updated to use `tf.shape` instead of `tf.python.ops.array_ops.shape`.  However, the original \"real\" error experienced in TF 2.3.0rc2 is not expected to be fixed until at least https://github.com/tensorflow/tensorflow/issues/45403 is merged.", "The issue still exists in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/f4a959ac6be19f5a30cd23c5dfa104ce/44112.ipynb). Thanks!", "@sachinprasadhs The gist that you point to has the following issues:\r\n- ```tf.python.ops.array_ops``` is not allowed; the symbols should be ```tf.shape()```\r\n- In order for a loss to be able to operate on RaggedTensors both values must be RaggedTensors... in this gist, the model output is a dense tensor.\r\n\r\nAfter making this [changes](https://colab.research.google.com/gist/sachinprasadhs/f4a959ac6be19f5a30cd23c5dfa104ce/44112.ipynb) the model executes... however the model itself seems to have design issues. For instance the output is a single value per timestep and that is being compared with a character... that doesn't seem correct to me.", "> @sachinprasadhs The gist that you point to has the following issues:\r\n> \r\n> * `tf.python.ops.array_ops` is not allowed; the symbols should be `tf.shape()`\r\n> * In order for a loss to be able to operate on RaggedTensors both values must be RaggedTensors... in this gist, the model output is a dense tensor.\r\n> \r\n> After making this [changes](https://colab.research.google.com/gist/sachinprasadhs/f4a959ac6be19f5a30cd23c5dfa104ce/44112.ipynb) the model executes... however the model itself seems to have design issues. For instance the output is a single value per timestep and that is being compared with a character... that doesn't seem correct to me.\r\n\r\nI'm absolutely sure it has some design issues.  If the model executes, IMHO the issue is fixed (and I am no longer using this approach, personally, I just bought a GPU with more memory and did a bunch of padding).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44112\">No</a>\n"]}, {"number": 44111, "title": "ValueError: Shapes (None, 3, 2) and (None, 2) are incompatible", "body": "In the following code, I save the label to tfrecord and read it again.\r\n(In reality, I save both images and labels to tfrecord, here is a simple example for illustration purpose).\r\n\r\nI got an error `ValueError: Shapes (None, 3, 2) and (None, 2) are incompatible`, it looks like a bug? I am using Tensorflow 2.3. \r\n\r\n\r\n    import contextlib2\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.keras import Model\r\n    from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\r\n    \r\n    \r\n    def process_image():\r\n    \r\n        dic={\r\n                \"image/label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[0,1]))\r\n        }\r\n        return tf.train.Example(features=tf.train.Features(feature=dic))\r\n    \r\n    \r\n    with contextlib2.ExitStack() as tf_record_close_stack:\r\n        output_tfrecords = [tf_record_close_stack.enter_context(tf.io.TFRecordWriter(file_name)) for file_name in\r\n                            [f\"data_train.tfrecord\"]]\r\n        output_tfrecords[0].write(process_image().SerializeToString())\r\n    \r\n    def parse_examples(examples):\r\n        parsed_examples = tf.io.parse_example(examples, features={\r\n            \"image/label\": tf.io.FixedLenFeature(shape=[2], dtype=tf.int64),\r\n        })\r\n        res = np.random.randint(2, size=3072).reshape(32, 32, 3)\r\n        return (res, [parsed_examples[\"image/label\"],parsed_examples[\"image/label\"],parsed_examples[\"image/label\"]])\r\n    \r\n    \r\n    def process_dataset(dataset):\r\n        dataset = dataset.map(parse_examples, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.batch(1)\r\n        return dataset\r\n    \r\n    train_data = tf.data.TFRecordDataset(filenames=\"data_train.tfrecord\")\r\n    train_data = process_dataset(train_data)\r\n    \r\n    base_model = tf.keras.applications.EfficientNetB7(input_shape=(32,32, 3), weights='imagenet',\r\n                                                      include_top=False)  # or weights='noisy-student'\r\n    \r\n    for layer in base_model.layers[:]:\r\n        layer.trainable = False\r\n    \r\n    x = GlobalAveragePooling2D()(base_model.output)\r\n    dropout_rate = 0.3\r\n    \r\n    \r\n    x = Dense(256, activation='relu')(x)\r\n    x = Dropout(dropout_rate)(x)\r\n    x = Dense(256, activation='relu')(x)\r\n    x = Dropout(dropout_rate)(x)\r\n    \r\n    \r\n    all_target = []\r\n    loss_list = []\r\n    test_metrics = {}\r\n    for name, node in  [(\"task1\", 2), (\"task2\", 2), (\"task3\", 2)]:\r\n        y1 = Dense(128, activation='relu')(x)\r\n        y1 = Dropout(dropout_rate)(y1)\r\n        y1 = Dense(64, activation='relu')(y1)\r\n        y1 = Dropout(dropout_rate)(y1)\r\n        y1 = Dense(node, activation='softmax', name=name)(y1)\r\n        all_target.append(y1)\r\n        loss_list.append('categorical_crossentropy')\r\n        test_metrics[name] = \"accuracy\"\r\n    \r\n    #    model = Model(inputs=model_input, outputs=[y1, y2, y3])\r\n    model = Model(inputs=base_model.input, outputs=all_target)\r\n    \r\n    model.compile(loss=loss_list, optimizer='adam', metrics=test_metrics)\r\n    \r\n    history = model.fit(train_data, epochs=1, verbose=1)\r\n\r\n", "comments": ["@tianhuat \r\nI ran the code shared and[ face this error](https://colab.research.google.com/gist/Saduf2019/d7fbee0c9f7d1fc06078d2ef529bb17f/untitled446.ipynb), if possible share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44111\">No</a>\n"]}, {"number": 44110, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.6\r\n- Installed using: pip\r\n- CUDA/cuDNN version: 10.1/ 7.6.5\r\n- GPU model and memory: Nvidia Geforce GTX 1050i TI\r\n\r\n\r\n**Describe the problem**\r\nWhen i import tensorflow like normal it throw an error, i have done the same thing on my laptop and there it all works fine, ik have the exact same code and followed the exact same steps, but on my computer it just fails. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- open a python file;\r\n- import tensorflow as tf\r\n- error occurs\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 31, in <module>\r\n    import tensorflow.compat.v2 as tf\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "I have seen this auto response on other issues, I think it has to do with the CPU bot being compatible, why is that? Because I have a GPU shouldn't it just run on my GPU and no my CPU?", "@aradox \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer duplicate issue #43130\r\nThanks!", "Vendor ID         : GenuineIntel\r\nCPU name          : Intel(R) Core(TM) i5 CPU         660  @ 3.33GHz\r\nMicroarchitecture : westmere\r\nVector instructions supported:\r\nSSE       : Yes\r\nSSE2      : Yes\r\nSSE3      : Yes\r\nSSSE3     : Yes\r\nSSE4.1    : Yes\r\nSSE4.2    : Yes\r\nSSE4a     : --\r\nAVX       : --\r\nAVX2      : --\r\nBMI1      : --\r\nBMI2      : --\r\n\r\nThe system is a 64bit system. \r\nI see that my CPU does not allow for AVX instructions. so what can i do now? I already tried to compile from source but this failed after 67 minutes of compiling.", "@aradox \r\n\r\nTensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\nTry Google Colab to use TensorFlow.\r\nThe easiest way to use TF will be to switch to google colab. You get pre-installed latest stable TF version. Also you can use pip install to install any other preferred TF version.\r\nIt has an added advantage since you can you easily switch to different hardware accelerators\r\n(cpu, gpu, tpu) as per the task.\r\nAll you need is a good internet connection and you are all set.\r\nor else try to build TF from sources by changing CPU optimization flags.\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44110\">No</a>\n", "> **System information**\r\n> \r\n> * OS Platform and Distribution: Windows 10\r\n> * TensorFlow installed from: binary\r\n> * TensorFlow version: 2.2.0\r\n> * Python version: 3.8.6\r\n> * Installed using: pip\r\n> * CUDA/cuDNN version: 10.1/ 7.6.5\r\n> * GPU model and memory: Nvidia Geforce GTX 1050i TI\r\n> \r\n> **Describe the problem** When i import tensorflow like normal it throw an error, i have done the same thing on my laptop and there it all works fine, ik have the exact same code and followed the exact same steps, but on my computer it just fails.\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> \r\n> * open a python file;\r\n> * import tensorflow as tf\r\n> * error occurs\r\n> \r\n> Traceback (most recent call last): File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic return _load(spec) ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last): File \"model_main_tf2.py\", line 31, in import tensorflow.compat.v2 as tf File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow__init__.py\", line 41, in from tensorflow.python.tools import module_util as _module_util File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python__init__.py\", line 52, in from tensorflow.python import pywrap_tensorflow File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic return _load(spec) ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\nonly need for !pip install tensorflow==2.7.0  tensorflow-gpu==2.7.0 --upgrade\r\nto solve hole problem", "> **System information**\r\n> \r\n> * OS Platform and Distribution: Windows 10\r\n> * TensorFlow installed from: binary\r\n> * TensorFlow version: 2.2.0\r\n> * Python version: 3.8.6\r\n> * Installed using: pip\r\n> * CUDA/cuDNN version: 10.1/ 7.6.5\r\n> * GPU model and memory: Nvidia Geforce GTX 1050i TI\r\n> \r\n> **Describe the problem** When i import tensorflow like normal it throw an error, i have done the same thing on my laptop and there it all works fine, ik have the exact same code and followed the exact same steps, but on my computer it just fails.\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> \r\n> * open a python file;\r\n> * import tensorflow as tf\r\n> * error occurs\r\n> \r\n> Traceback (most recent call last): File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic return _load(spec) ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last): File \"model_main_tf2.py\", line 31, in import tensorflow.compat.v2 as tf File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow__init__.py\", line 41, in from tensorflow.python.tools import module_util as _module_util File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python__init__.py\", line 52, in from tensorflow.python import pywrap_tensorflow File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 29, in _pywrap_tensorflow_internal = swig_import_helper() File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 25, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"C:\\Users\\TastDesktop\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic return _load(spec) ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n> \r\n> Failed to load the native TensorFlow runtime.\r\n\r\nHey, did you finally get your TF up and running?\r\nI have a similar architecture as you, and running into the same problem."]}, {"number": 44109, "title": "Support LeakyReluGrad for 5D tensors in Layout optimizer", "body": "This PR is to allow the grappler layout optimizer to support unary grad ops (e.g., LeakyReluGrad) for 5D tensors.\r\n\r\nfyi. @nluehr ", "comments": ["@kaixih  Can you please check @andyly's comments and keep us posted ? Thanks!", "@andyly PTAL.", "> @andyly PTAL.\r\n\r\nCan you also address the other comment for explicitly checking the rank? The transposer originally checked for if the output is rank 4 but now there is no check or restriction of rank. You should be able to replace getting and checking the rank with something like https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc#L736-L739.", "Sure. I guess I missed that comments. Changes are made. PTAL."]}, {"number": 44108, "title": "TFLite Interpreter returns different results when in Python on desktop and when in Java on Android mobile device", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04, Android 9 PKQ1.190118.0 01\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Xiaomi MIX 2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nLoading the same TFLite model on my desktop using the following command:\r\n\r\n```interpreter = tf.lite.Interpreter(model_path='./model.tflite') \r\ninterpreter.allocate_tensors() \r\ninput_details = interpreter.get_input_details() \r\noutput_details = interpreter.get_output_details() \r\ninterpreter.set_tensor(input_details[0]['index'], img_data.reshape(1, 28, 28, 1).astype(np.float32)) \r\ninterpreter.invoke() \r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nReturns the correct results in Float32 in output_data, something like [0.345, 0.655]. However, if I load the same model on my device using:\r\n\r\n```MappedByteBuffer tfliteModel = FileUtil.loadMappedFile(this, \"model.tflite\");\r\nInterpreter tflite = new Interpreter(tfliteModel);\r\nTensorBuffer probabilityBuffer = TensorBuffer.createFixedSize(new int[]{1, 2}, DataType.FLOAT32);\r\ntflite.run(img_data, probabilityBuffer.getBuffer());\r\nfloat[] results = probabilityBuffer.getFloatArray();\r\n```\r\n\r\nAlways returns a very strange result of [1.0, 0.0]\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@peidaqi,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44107, "title": "WARNING:tensorflow:Can save best model only with val_loss available, skipping.", "body": "**System information**\r\nCurrent Google Colab Pro notebook, Tensorflow version 2.3.0\r\n\r\n\r\n**Describe the current behavior**\r\nwhen running .fit() with callback.ModelCheckpoint monitoring both`val_loss` and `val_sparse_categorical_accuracy` the \"WARNING:tensorflow:Can save best model only with val_loss available, skipping.\" pops up even if both terms are in hist.history returned from fit\r\n\r\n**Describe the expected behavior**\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.05),\r\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\r\n\r\nhist = model.fit(X_train,\r\n                 y_train,\r\n                 batch_size = BATCH_SIZE,\r\n                 epochs=EPOCH,\r\n                 validation_data=(X_val, y_val),\r\n                 shuffle = True,\r\n                 callbacks = [ModelCheckpoint(\"model.hdf5\",\r\n                              monitor = 'val_loss',\r\n                              save_best_only = True,\r\n                              save_weights_only = False,\r\n                              save_freq= 1,\r\n                              verbose = 0)],\r\n                 verbose = 0)\r\n```\r\n\r\nwhere data are np.ndarray, and \r\n\r\n```\r\nfor key in hist.history:\r\n  print(key)\r\n```\r\nreturns the following:\r\n\r\nloss\r\nsparse_categorical_accuracy\r\nval_loss\r\nval_sparse_categorical_accuracy\r\nlr\r\n", "comments": ["As stated [here](https://github.com/tensorflow/tensorflow/issues/33163#issuecomment-547887456), monitor argument has to match one of the metric names passed at the metrics argument of model.compile(). So, you need to add tf.keras.metrics.SparseCategoricalCrossentropy to your metrics list (but this will print loss twice at each epoch) and \"sparse_categorical_crossentropy\" to monitor. But, this is not a solution, since this monitors the train set loss. I tried prepending val_ (\"val_sparse_categorical_crossentropy\"), which is a valid monitor being printed at each epoch, but got the same error.", "@alexliyihao \r\nPlease share complete code, i ran the code shared and [face this error](https://colab.research.google.com/gist/Saduf2019/c4352e72a8f5ddaa3836b8265f5fc350/untitled446.ipynb), if possible share a colab gist with issue faced.", "Ah sorry, the complete code is a part of wrapped code and it took me a while to extract them out, a complete reproducible code is like following: \r\n\r\nyou can run it in https://colab.research.google.com/drive/1glZ4Mm5mo-Ev3YI9IlucfNH43AVZLFm0?usp=sharing ,I just have it run on normal Colab Notebook, the warning popped up as well. Not sure about local because my old macbook cannot handle much training= =\r\n\r\n```\r\n# init some fast hand data\r\nimport numpy as np\r\nX_train = np.random.randn(5,200,200,3)\r\ny_train = np.array([1,2,3,4,5])\r\nX_val = np.random.randn(5,200,200,3)\r\ny_val = np.array([1,2,3,4,5])\r\n#-------------------------------------------------------\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.layers import Flatten, Dense, BatchNormalization\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\n\r\n# init a normal model efn+dense+dense\r\nefn = tf.keras.applications.EfficientNetB2(weights='imagenet', include_top = False)\r\ninput = Input(shape= (200,200,3))\r\nx = efn(input)\r\nx = Flatten()(x)\r\nx = Dense(64, activation='relu')(x)\r\nx = BatchNormalization()(x)\r\noutput = Dense(30, activation='softmax')(x) \r\nmodel = Model(input,output)\r\n\r\n# compile the model\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.05),\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\r\n#-------------------------------------------------------\r\nMONITOR = \"val_loss\" #try MONITOR = \"val_sparse_categorical_accuracy\" as well\r\n\r\nhist = model.fit(X_train,\r\n                 y_train,\r\n                 batch_size = 64,\r\n                 epochs=5,\r\n                 validation_data=(X_val, y_val),\r\n                 shuffle = True,\r\n                 callbacks = [ModelCheckpoint(\"model.hdf5\",\r\n                              monitor = MONITOR,\r\n                              save_best_only = True,\r\n                              save_weights_only = False,\r\n                              save_freq= 1,\r\n                              verbose = 0)],\r\n                 verbose = 0)\r\nprint(f\"{MONITOR} in keys: {MONITOR in hist.history.keys()}\", )\r\n```\r\n```", "@alexalemi \r\nCould you please try with the actual data [and verbose 1] and let us know if the issue exists instead of trying with  np.random dummy data.", "@Saduf2019 \r\n\r\nThe data is some project-specific data so I'm afraid that I cannot provide the actual one, but I can confirm that the issue still exist.\r\n\r\nVerbose = 1 will create this multi-line format in jupyter notebook, therefore I'm using tqdm.keras.TqdmCallback for most of the time. This is the output for 2 epochs when not using tqdm.keras.TqdmCallback, but using verbose = 1 for both fit() and ModelCheckpoint.\r\n\r\nWhat can be also stated is that I tried tf.data.Dataset in `<BatchDataset shapes: ((None, 200, 200, 3), (None,)), types: (tf.float64, tf.int64)>` format, the error still exists. I personally think it's not related much to the input data.\r\n\r\n```\r\nEpoch 1/100\r\nWARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 1/28 [>.............................] - ETA: 0s - loss: 3.9611 - sparse_categorical_accuracy: 0.0156WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 2/28 [=>............................] - ETA: 4s - loss: 4.0169 - sparse_categorical_accuracy: 0.0547WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0958s vs `on_train_batch_end` time: 0.2455s). Check your callbacks.\r\nWARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 3/28 [==>...........................] - ETA: 5s - loss: 3.9681 - sparse_categorical_accuracy: 0.0573WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 4/28 [===>..........................] - ETA: 6s - loss: 3.9270 - sparse_categorical_accuracy: 0.0430WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 5/28 [====>.........................] - ETA: 6s - loss: 3.8824 - sparse_categorical_accuracy: 0.0437WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 6/28 [=====>........................] - ETA: 6s - loss: 3.8315 - sparse_categorical_accuracy: 0.0521WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 7/28 [======>.......................] - ETA: 5s - loss: 3.8645 - sparse_categorical_accuracy: 0.0536WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 8/28 [=======>......................] - ETA: 5s - loss: 3.8801 - sparse_categorical_accuracy: 0.0488WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 9/28 [========>.....................] - ETA: 5s - loss: 3.8523 - sparse_categorical_accuracy: 0.0486WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n10/28 [=========>....................] - ETA: 5s - loss: 3.8176 - sparse_categorical_accuracy: 0.0453WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n11/28 [==========>...................] - ETA: 5s - loss: 3.8034 - sparse_categorical_accuracy: 0.0455WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n12/28 [===========>..................] - ETA: 4s - loss: 3.7804 - sparse_categorical_accuracy: 0.0443WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n13/28 [============>.................] - ETA: 4s - loss: 3.7688 - sparse_categorical_accuracy: 0.0421WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n14/28 [==============>...............] - ETA: 4s - loss: 3.7455 - sparse_categorical_accuracy: 0.0435WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n15/28 [===============>..............] - ETA: 4s - loss: 3.7266 - sparse_categorical_accuracy: 0.0458WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n16/28 [================>.............] - ETA: 3s - loss: 3.7036 - sparse_categorical_accuracy: 0.0488WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n17/28 [=================>............] - ETA: 3s - loss: 3.7000 - sparse_categorical_accuracy: 0.0478WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n18/28 [==================>...........] - ETA: 3s - loss: 3.7036 - sparse_categorical_accuracy: 0.0469WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n19/28 [===================>..........] - ETA: 2s - loss: 3.6926 - sparse_categorical_accuracy: 0.0461WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n20/28 [====================>.........] - ETA: 2s - loss: 3.6940 - sparse_categorical_accuracy: 0.0469WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n21/28 [=====================>........] - ETA: 2s - loss: 3.6725 - sparse_categorical_accuracy: 0.0469WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n22/28 [======================>.......] - ETA: 1s - loss: 3.6490 - sparse_categorical_accuracy: 0.0490WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n23/28 [=======================>......] - ETA: 1s - loss: 3.6498 - sparse_categorical_accuracy: 0.0496WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n24/28 [========================>.....] - ETA: 1s - loss: 3.6471 - sparse_categorical_accuracy: 0.0482WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n25/28 [=========================>....] - ETA: 0s - loss: 3.6624 - sparse_categorical_accuracy: 0.0475WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n26/28 [==========================>...] - ETA: 0s - loss: 3.6560 - sparse_categorical_accuracy: 0.0475WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n27/28 [===========================>..] - ETA: 0s - loss: 3.6472 - sparse_categorical_accuracy: 0.0475WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n28/28 [==============================] - 12s 419ms/step - loss: 3.6460 - sparse_categorical_accuracy: 0.0470 - val_loss: 382778278225658249216.0000 - val_sparse_categorical_accuracy: 0.0317\r\nEpoch 2/100\r\nWARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 1/28 [>.............................] - ETA: 0s - loss: 3.3321 - sparse_categorical_accuracy: 0.0156WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 2/28 [=>............................] - ETA: 4s - loss: 3.3042 - sparse_categorical_accuracy: 0.0469WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 3/28 [==>...........................] - ETA: 5s - loss: 3.2632 - sparse_categorical_accuracy: 0.0521WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 4/28 [===>..........................] - ETA: 5s - loss: 3.2630 - sparse_categorical_accuracy: 0.0586WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 5/28 [====>.........................] - ETA: 5s - loss: 3.2139 - sparse_categorical_accuracy: 0.0594WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 6/28 [=====>........................] - ETA: 6s - loss: 3.2333 - sparse_categorical_accuracy: 0.0573WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 7/28 [======>.......................] - ETA: 5s - loss: 3.1948 - sparse_categorical_accuracy: 0.0647WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 8/28 [=======>......................] - ETA: 5s - loss: 3.2231 - sparse_categorical_accuracy: 0.0586WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n 9/28 [========>.....................] - ETA: 5s - loss: 3.1942 - sparse_categorical_accuracy: 0.0677WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n10/28 [=========>....................] - ETA: 5s - loss: 3.1784 - sparse_categorical_accuracy: 0.0688WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n11/28 [==========>...................] - ETA: 5s - loss: 3.1815 - sparse_categorical_accuracy: 0.0696WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n12/28 [===========>..................] - ETA: 4s - loss: 3.1655 - sparse_categorical_accuracy: 0.0703WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n13/28 [============>.................] - ETA: 4s - loss: 3.1538 - sparse_categorical_accuracy: 0.0709WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n14/28 [==============>...............] - ETA: 4s - loss: 3.1474 - sparse_categorical_accuracy: 0.0703WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n15/28 [===============>..............] - ETA: 4s - loss: 3.1502 - sparse_categorical_accuracy: 0.0698WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n16/28 [================>.............] - ETA: 3s - loss: 3.1374 - sparse_categorical_accuracy: 0.0732WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n17/28 [=================>............] - ETA: 3s - loss: 3.1316 - sparse_categorical_accuracy: 0.0744WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n18/28 [==================>...........] - ETA: 3s - loss: 3.1259 - sparse_categorical_accuracy: 0.0764WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n19/28 [===================>..........] - ETA: 2s - loss: 3.1073 - sparse_categorical_accuracy: 0.0806WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n20/28 [====================>.........] - ETA: 2s - loss: 3.0857 - sparse_categorical_accuracy: 0.0859WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n21/28 [=====================>........] - ETA: 2s - loss: 3.0644 - sparse_categorical_accuracy: 0.0893WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n22/28 [======================>.......] - ETA: 1s - loss: 3.0625 - sparse_categorical_accuracy: 0.0888WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n23/28 [=======================>......] - ETA: 1s - loss: 3.0482 - sparse_categorical_accuracy: 0.0890WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n24/28 [========================>.....] - ETA: 1s - loss: 3.0320 - sparse_categorical_accuracy: 0.0911WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n25/28 [=========================>....] - ETA: 0s - loss: 3.0124 - sparse_categorical_accuracy: 0.0950WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n26/28 [==========================>...] - ETA: 0s - loss: 2.9966 - sparse_categorical_accuracy: 0.0974WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n27/28 [===========================>..] - ETA: 0s - loss: 2.9805 - sparse_categorical_accuracy: 0.1001WARNING:tensorflow:Can save best model only with val_sparse_categorical_crossentropy available, skipping.\r\n28/28 [==============================] - 9s 332ms/step - loss: 2.9732 - sparse_categorical_accuracy: 0.1002 - val_loss: 9081760776192.0000 - val_sparse_categorical_accuracy: 0.0362\r\n```", "Hello I am having exactly the same error.\r\n**Environment**:\r\n```\r\nDISTRIB_ID=Ubuntu\r\nDISTRIB_RELEASE=20.04\r\nDISTRIB_CODENAME=focal\r\nDISTRIB_DESCRIPTION=\"Ubuntu 20.04.1 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION=\"20.04.1 LTS (Focal Fossa)\"\r\nID=ubuntu\r\nID_LIKE=debian\r\nPRETTY_NAME=\"Ubuntu 20.04.1 LTS\"\r\nVERSION_ID=\"20.04\"\r\nHOME_URL=\"https://www.ubuntu.com/\"\r\nSUPPORT_URL=\"https://help.ubuntu.com/\"\r\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\r\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\r\nVERSION_CODENAME=focal\r\nUBUNTU_CODENAME=focal\r\n```\r\n**nvidia-smi**\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.38       Driver Version: 455.38       CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:05:00.0 Off |                  N/A |\r\n| 95%   84C    P2   242W / 260W |  10776MiB / 11016MiB |     95%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  Off  | 00000000:06:00.0 Off |                  N/A |\r\n| 95%   84C    P2   240W / 260W |  10778MiB / 11019MiB |     93%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 108...  Off  | 00000000:09:00.0 Off |                  N/A |\r\n| 28%   45C    P0    55W / 250W |      0MiB / 11178MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\r\n|  0%   39C    P5    16W / 275W |      0MiB / 11178MiB |      3%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n```\r\n**Running in nvidia docker**\r\n`tensorflow/tensorflow:latest-gpu   \"/bin/bash\"         6 days ago          Up 6 days           0.0.0.0:5001->6006/tcp, 0.0.0.0:5000->8888/tcp   tf`\r\n**Installed with the following command**\r\n`docker run --gpus all -d --name tf -it -p 5000:8888 -p 5001:6006 -v /home:/home tensorflow/tensorflow:latest-gpu`\r\n**Python 3.6.9**\r\n**tensorflow.__version__\r\n'2.3.1'**\r\n\r\n**THE CODE:**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom itertools import cycle\r\nfrom tensorflow import keras\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import precision_recall_curve\r\nfrom sklearn.metrics import average_precision_score\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input\r\nimport utils.tf_losses as tfl\r\nimport utils.tf_utils as tf_utils\r\nimport utils.data as data\r\nimport argparse\r\nimport numpy as np\r\nimport pandas as pd\r\nimport io\r\nfrom tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\r\nfrom tensorflow.keras.metrics import Mean, MeanIoU, PrecisionAtRecall, Precision, Recall, Accuracy, AUC\r\nimport json\r\nfrom glob import glob\r\nfrom datetime import datetime\r\n\r\nrandom_seed = 42 # Fixing the seed for PRNGs, to help reproducibility\r\nnp.random.seed(random_seed)\r\ntf.random.set_seed(random_seed)\r\n\r\n# # def gaussian_blur(img, kernel_size=11, sigma=5):\r\n# # \tdef gauss_kernel(channels, kernel_size, sigma):\r\n# # \t\tax = tf.range(-kernel_size // 2 + 1.0, kernel_size // 2 + 1.0)\r\n# # \t\txx, yy = tf.meshgrid(ax, ax)\r\n# # \t\tkernel = tf.exp(-(xx ** 2 + yy ** 2) / (2.0 * sigma ** 2))\r\n# # \t\tkernel = kernel / tf.reduce_sum(kernel)\r\n# # \t\tkernel = tf.tile(kernel[..., tf.newaxis], [1, 1, channels])\r\n# # \t\treturn kernel\r\n# #\r\n# # \tgaussian_kernel = gauss_kernel(tf.shape(img)[-1], kernel_size, sigma)\r\n# # \tgaussian_kernel = gaussian_kernel[..., tf.newaxis]\r\n# #\r\n# # \treturn tf.nn.depthwise_conv2d(img, gaussian_kernel, [1, 1, 1, 1], padding='SAME', data_format='NHWC')\r\n#\r\ndef _morph_dilation(input, k_size=3):\r\n\tif len(input.shape)!=4:\r\n\t\traise ValueError(\"Inputs shape be 4 channels NWHC type\")\r\n\tinput = tf.cast(input,tf.float32)\r\n\tkernel = tf.zeros((k_size, k_size, 1))\r\n\tdilated_label = tf.nn.dilation2d(\r\n\t\tinput,\r\n\t\tfilters=kernel,\r\n\t\tstrides=(1, 1, 1, 1),\r\n\t\tdilations=(1, 1, 1, 1),\r\n\t\tpadding=\"SAME\",\r\n\t\tdata_format='NHWC'\r\n\t)\r\n\treturn dilated_label\r\n\r\n# # def _smooth_labels(onehot_tensor,label_smoothing):\r\n# # \tnum_classes = tf.cast(onehot_tensor.shape[-1],tf.float32)\r\n# # \treturn onehot_tensor * (1.0 - label_smoothing) + (label_smoothing / num_classes)\r\n\r\nclass CustomModel(Model):\r\n\r\n\tdef train_step(self, data):\r\n\t\t# Unpack the data. Its structure depends on your model and\r\n\t\t# on what you pass to `fit()`.\r\n\t\tx, y = data\r\n\t\twith tf.GradientTape() as tape:\r\n\t\t\ty_pred = self(x, training=True)  # Forward pass\r\n\t\t\t# Compute the loss value\r\n\t\t\t# (the loss function is configured in `compile()`)\r\n\t\t\tloss = self.compiled_loss(y, y_pred)\r\n\r\n\t\t# Compute gradients\r\n\t\ttrainable_vars = self.trainable_variables\r\n\t\tgradients = tape.gradient(loss, trainable_vars)\r\n\t\t# Update weights\r\n\t\tself.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n\t\t# Update metrics (includes the metric that tracks the loss)\r\n\t\tself.compiled_metrics.update_state(tf.squeeze(y,axis=-1), tf.argmax(tf.nn.softmax(y_pred,axis=-1),axis=-1))\r\n\t\t# Return a dict mapping metric names to current value\r\n\t\treturn {m.name: m.result() for m in self.metrics}\r\n\r\n\tdef test_step(self, data):\r\n\t\t# Unpack the data\r\n\t\tx, y = data\r\n\t\tif args['morph_dilation']==True:\r\n\t\t\ty = _morph_dilation(y,3)\r\n\t\t# Compute predictions\r\n\t\ty_pred = self(x, training=False)\r\n\t\t# Updates the metrics tracking the loss\r\n\t\tself.compiled_loss(y, y_pred)\r\n\t\t# Update the metrics.\r\n\t\tself.compiled_metrics.update_state(tf.squeeze(y,axis=-1), tf.argmax(tf.nn.softmax(y_pred,axis=-1),axis=-1))\r\n\t\twith file_writer.as_default():\r\n\t\t\ttf.summary.image(\"input\", x, step=self.optimizer.iterations, max_outputs=params['batch_size'])\r\n\t\t\ttf.summary.image(\"ground_truth\", tf.one_hot(tf.squeeze(y, axis=-1), depth=2, axis=-1), step=self.optimizer.iterations, max_outputs=params['batch_size'])\r\n\t\t\ttf.summary.image(\"prediction_output\", y_pred, step=self.optimizer.iterations, max_outputs=params['batch_size'])\r\n\t\t# Return a dict mapping metric names to current value.\r\n\t\t# Note that it will include the loss (tracked in self.metrics).\r\n\t\treturn {m.name: m.result() for m in self.metrics}\r\n\r\ndef train():\r\n\ttrain_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch = data.dataset(args[\"data_dir\"], batch_size=params['batch_size'], image_size = params['image_size'])\r\n\tinputs = Input(shape=(*params['image_size'], params['num_channels']), name='input')\r\n\r\n\toutputs = tf_utils.get_model(args, params, inputs)\r\n\r\n\tmodel = CustomModel(inputs, outputs)\r\n#\ttf_utils.print_summary(model=model,logdir=args['log_dir'],params=params,args=args)\r\n\twith file_writer.as_default():\r\n\t\ttf.summary.text('config', tf_utils.tb_config(logdir=args['log_dir'],params=params,args=args), step=0)\r\n\r\n\tcallbacks = [\r\n\t\t# Callback to reduce the learning rate once the plateau has been reached:\r\n\t\ttf.keras.callbacks.ReduceLROnPlateau(\r\n\t\t\tmonitor='val_loss',\r\n\t\t\tfactor=0.1,\r\n\t\t\tpatience=8,\r\n\t\t\tmode='auto',\r\n\t\t\tmin_delta=0.001,\r\n\t\t\tcooldown=0,\r\n\t\t\tmin_lr=1e-8\r\n\t\t),\r\n\t\t# Callback to stop the training once no more improvements are recorded:\r\n\t\ttf.keras.callbacks.EarlyStopping(\r\n\t\t\tmonitor='val_loss',\r\n\t\t\tmin_delta=0.001,\r\n\t\t\tpatience=16,\r\n\t\t\tmode='auto',\r\n\t\t\trestore_best_weights=True\r\n\t\t),\r\n\t\t# Callback to log the graph, losses and metrics into TensorBoard:\r\n\t\ttf.keras.callbacks.TensorBoard(\r\n\t\t\tlog_dir=args['log_dir'],\r\n\t\t\thistogram_freq=0,\r\n\t\t\tupdate_freq='epoch',\r\n\t\t\twrite_graph=True,\r\n\t\t\twrite_images=False\r\n\t\t),\r\n\t\t# Callback to save the model  specifying the epoch and val-loss in the filename:\r\n\t\ttf.keras.callbacks.ModelCheckpoint(\r\n\t\t\tfilepath=save_path,),\r\n\t\t\tsave_freq=5,\r\n\t\t\tmonitor='val_loss',\r\n\t\t\tverbose=0,\r\n\t\t\tsave_best_only=True,\r\n\t\t\tsave_weights_only=False\r\n\t\t)\r\n\t]\r\n\r\n\t## Train Metrics ##\r\n\ttrain_metrics = [tf.keras.metrics.Mean() for _ in range(5)]\r\n\ttrain_metrics[0] = tf.keras.metrics.MeanIoU(num_classes=2, name='mIoU')\r\n\ttrain_metrics[1] = tf.keras.metrics.Precision(name='Precision')\r\n\ttrain_metrics[2] = tf.keras.metrics.Recall(name='Recall')\r\n\ttrain_metrics[3] = tf.keras.metrics.Accuracy(name='Accuracy')\r\n\ttrain_metrics[4] = tf.keras.metrics.AUC(curve='PR', name='AUC_PR')\r\n\toptimizer = tf.keras.optimizers.Adam(learning_rate=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-07,\r\n\t\t\t\t\t\t\t\t\t\t amsgrad=False, name='Adam')\r\n\tmodel.compile(\r\n\t\toptimizer=optimizer,\r\n\t\tloss = tfl.DiceLoss(from_logits=True),\r\n\t\tmetrics=[train_metrics]\r\n\t)\r\n\thist=model.fit(\r\n\t\tx=train_dataset,\r\n\t\tbatch_size=params['batch_size'],\r\n\t\tepochs=params['Epochs'],\r\n\t\tverbose=1,\r\n\t\tcallbacks=callbacks,\r\n\t\tvalidation_data=val_dataset,\r\n\t\tshuffle=True,\r\n\t\tclass_weight=None,\r\n\t\tsample_weight=None,\r\n\t\tinitial_epoch=0,\r\n\t\tsteps_per_epoch=train_steps_per_epoch,\r\n\t\tvalidation_steps=val_steps_per_epoch,\r\n\t\tvalidation_freq=1,\r\n\t\tmax_queue_size=10,\r\n\t\tworkers=1,\r\n\t\tuse_multiprocessing=False,\r\n\t)\r\n\tfor key in hist.history:\r\n\t\tprint(key)\r\n\r\nif __name__ == '__main__':\r\n\t# -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # --\r\n\t# -- # -- Retrieve the config files and parse the arguments # -- # --\r\n\t# -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # -- # --\r\n\twith open('config_files/model_config.json','r') as f:\r\n\t\tconfigs = json.load(f)\r\n\targs = configs['args']\r\n\tparams= configs['params']\r\n\r\n\t# -- # -- Add extra parameters\r\n\tparams['final_activation'] = None\r\n\tparams['pooling'] = True\r\n\tparams['skipping'] = False\r\n\targs['morph_dilation'] = False\r\n\r\n\t# -- # -- Change the log_dir name coherently with the pooling parameter\r\n\tif params['pooling']==False:\r\n\t#\tparams['dilation_rate']=2 #uncomment to use dilated convolution\r\n\t#\tparams['strides']=1 #uncomment to use dilated convolution\r\n\t\targs['log_dir'] = os.path.join(args['log_dir'],'Un_'+args['model']+'_'+str(int(datetime.now().strftime(\"%Y%m%d%H%M%S\"))))\r\n\telse:\r\n\t\targs['log_dir'] = os.path.join(args['log_dir'],args['model']+'_'+str(int(datetime.now().strftime(\"%Y%m%d%H%M%S\"))))\r\n\r\n        # -- # -- If log_dir does not exist, create it\r\n\tif not os.path.isdir(args['log_dir']):\r\n\t\tos.makedirs(args['log_dir'])\r\n\r\n\t# -- # -- If save_path does not exist, create it (it is the checkpoint saving dir)\r\n\tsave_path=os.path.join(args['log_dir'],'checkpoint')\r\n\tif not os.path.isdir(save_path):\r\n\t\tos.makedirs(save_path)\r\n\r\n\t#args['dropout']= None\r\n\tfile_writer = tf.summary.create_file_writer(args['log_dir'] + '/images')\r\n\ttrain()\r\n\tconfigs_savepath=args['log_dir']\r\n\tparams = json.dumps(params, indent=4)\r\n\twith open(os.path.join(configs_savepath, 'model_params.json'), 'w') as params_file:\r\n\t\tparams_file.write(params)\r\n\targs = json.dumps(args, indent=4)\r\n\twith open(os.path.join(configs_savepath, 'model_args.json'), 'w') as args_file:\r\n\t\targs_file.write(args)\r\n```\r\n**Keys of hist.history**\r\n```\r\nloss\r\nmIoU\r\nPrecision\r\nRecall\r\nAccuracy\r\nAUC_PR\r\nval_loss\r\nval_mIoU\r\nval_Precision\r\nval_Recall\r\nval_Accuracy\r\nval_AUC_PR\r\nlr\r\n```\r\n**When running I receive the following warning:**\r\n```\r\nEpoch 20/300\r\n15795/15795 [==============================] - 3008s 190ms/step - loss: 0.1272 - mIoU: 0.7991 - Precision: 0.7277 - Recall: 0.7746 - Accuracy: 0.9977 - AUC_PR: 0.5681 - val_loss: 0.1471 - val_mIoU: 0.7835 - val_Precision: 0.7082 - val_Recall: 0.7437 - val_Accuracy: 0.9977 - val_AUC_PR: 0.5313\r\nEpoch 21/300 [=> . . . . . . . . . . . . . . . . . . . . . . . . . . . . .] - ETA: 41:43 - loss: 0.1243 - mIoU: 0.8025 - Precision: 0.7320 - Recall - 0.7808 - Accuracy: 0.9978 - AUC_PR: 0.5755WARNING: tensorflow: Can save best model only with val_loss available, skipping.\r\n```\r\n**I tried to change** \r\n```\r\ntf.keras.callbacks.ModelCheckpoint(\r\n\t\t\tfilepath=save_path,),\r\n\t\t\tsave_freq=5,\r\n\t\t\tmonitor='val_AUC_PR', #'val_loss'\r\n\t\t\tverbose=0,\r\n\t\t\tsave_best_only=True,\r\n\t\t\tsave_weights_only=False\r\n\t\t)\r\n```\r\n**But I receive the same warning:**\r\n`WARNING: tensorflow:Can save best model only with val_AUC_PR available, skipping.`", "It just found the solution to my problem. \"save_freq\" was set to 5 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to `save_freq='epoch'` solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search.\r\nRefer to [https://github.com/tensorflow/tensorflow/issues/33163](url) and look for the comment by **MichaelSoegaard**", "I changed `save_freq=1` to `save_freq='epoch'` solved the problem", "Closing this issue since the following setting helps resolves the problem for many users. Thank you.\r\n\r\n> `save_freq='epoch'` solved my problem. \r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44107\">No</a>\n", "> It just found the solution to my problem. \"save_freq\" was set to 5 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to `save_freq='epoch'` solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search.\r\n> Refer to [https://github.com/tensorflow/tensorflow/issues/33163](url) and look for the comment by **MichaelSoegaard**\r\n\r\nThis saved my life.\r\nInternet has so many confusing sources, with different version and release dates,\r\nthe solution of Tensorflow also differs.", "> It just found the solution to my problem. \"save_freq\" was set to 5 meaning it would save the model at each batch if val_acc improved. But as val_acc is computed after each epoch it didn't get any info on how each batch did. Changing to `save_freq='epoch'` solved my problem. Please note that 'save_freq' in a previous version was called something else. Don't remember it now, and I couldn't find the info by a quick Google search.\r\n> Refer to [https://github.com/tensorflow/tensorflow/issues/33163](url) and look for the comment by **MichaelSoegaard**\r\n\r\nsave_freq was called period in the previous version"]}, {"number": 44106, "title": "cublasComputeType_t has not been declared", "body": "**System information**\r\n- Ubuntu 20.04\r\n- Master Branch and Nightly Branch\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC 8\r\n- CUDA/cuDNN version: CUDA 11.1, cuDNN 8\r\n- GPU model and memory: 3090\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nConfigure successful, with cuda11, cudnn8, tensorrt7\r\n\r\ncommand:\r\n`bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n**Any other info / logs**\r\n\r\nINFO: Found applicable config definition build:linux in file /home/frank/tffun/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/frank/tffun/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/51ff04567b2f8d06b2062bd3ed72eab2e93e4466.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (261 packages loaded, 29454 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/frank/tffun/tensorflow/tensorflow/stream_executor/cuda/BUILD:254:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cublas_lt_stub' failed (Exit 1)\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:576:5: error: 'cublasComputeType_t' has not been declared\r\n     cublasComputeType_t computeType,\r\n     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:586:5: error: 'cublasComputeType_t' has not been declared\r\n     cublasComputeType_t computeType,\r\n     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:597:60: error: 'cublasComputeType_t' has not been declared\r\n cublasLtMatmulDescCreate(cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType, cudaDataType_t scaleType);\r\n                                                            ^~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1069:5: error: 'cublasComputeType_t' has not been declared\r\n     cublasComputeType_t computeType,\r\n     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1087:26: error: 'cublasComputeType_t' has not been declared\r\n                          cublasComputeType_t computeType,\r\n                          ^~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:58:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:135:5: error: 'cublasComputeType_t' has not been declared\r\n     cublasComputeType_t computeType, cudaDataType_t scaleType) {\r\n     ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulDescInit_internal(cublasLtMatmulDesc_t, size_t, int, cudaDataType_t)':\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:137:37: error: 'cublasComputeType_t' has not been declared\r\n       cublasLtMatmulDesc_t, size_t, cublasComputeType_t, cudaDataType_t);\r\n                                     ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:144:39: error: 'cublasComputeType_t' has not been declared\r\n     cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType,\r\n                                       ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulDescCreate(cublasLtMatmulDescOpaque_t**, int, cudaDataType_t)':\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:147:31: error: 'cublasComputeType_t' has not been declared\r\n       cublasLtMatmulDesc_t *, cublasComputeType_t, cudaDataType_t);\r\n                               ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:309:35: error: 'cublasComputeType_t' has not been declared\r\n     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,\r\n                                   ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulAlgoGetIds(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, int*, int*)':\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:314:25: error: 'cublasComputeType_t' has not been declared\r\n       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,\r\n                         ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:323:35: error: 'cublasComputeType_t' has not been declared\r\n     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,\r\n                                   ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function 'cublasStatus_t cublasLtMatmulAlgoInit(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, cublasLtMatmulAlgo_t*)':\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:328:25: error: 'cublasComputeType_t' has not been declared\r\n       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,\r\n                         ^~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 492.578s, Critical Path: 86.39s\r\nINFO: 4024 processes: 4024 local.\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["configuration:\r\n\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=175\r\nINFO: Reading rc options for 'build' from /home/frank/tffun/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/frank/tffun/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/frank/tffun/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3 --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION= --action_env TF_CUDA_PATHS=/lib,/lib/i386-linux-gnu,/lib/x86_64-linux-gnu,/usr,/usr/lib,/usr/lib/i386-linux-gnu,/usr/lib/x86_64-linux-gnu,/usr/lib/x86_64-linux-gnu/libfakeroot,/usr/local/cuda,/usr/local/tensorrt,/usr/local/tensorrt/lib --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:/usr/local/TensorRT-7.2.0.14/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.0/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/frank/tffun/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/frank/tffun/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/frank/tffun/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /home/frank/tffun/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /home/frank/tffun/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/frank/tffun/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:cuda in file /home/frank/tffun/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/frank/tffun/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /home/frank/tffun/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/frank/tffun/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (254 packages loaded, 27644 targets configured).\r\nINFO: Found 1 target..", "'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3 --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION= --action_env TF_CUDA_PATHS=/lib,/lib/i386-linux-gnu,/lib/x86_64-linux-gnu,/usr,/usr/lib,/usr/lib/i386-linux-gnu,/usr/lib/x86_64-linux-gnu,/usr/lib/x86_64-linux-gnu/libfakeroot,/usr/local/cuda,/usr/local/tensorrt,/usr/local/tensorrt/lib --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --action_env LD_LIBRARY_PATH=/usr/local/cuda-11.1/lib64:/usr/local/TensorRT-7.2.0.14/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.0/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-8 --config=cuda --action_env TF_CONFIGURE_IOS=0", "This issue can be fixed by checking the path for `./configuration`. It seems that online tutorials from google search have a few mistakes. The correct path is `/local/usr/cuda-11.0/` and wherever your tensorrt path is", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44106\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44106\">No</a>\n"]}, {"number": 44105, "title": "Make fname optional in get_file", "body": "Make `fname` parameter optional in function `keras.utils.get_file`:\r\n- make `fname` and `origin` keyword arguments\r\n- raise exception if no `origin`\r\n- if no `fname`, set it to `origin` path basename\r\n- implement tests", "comments": ["Thank you @fchollet for your review. I have addressed your requested changes.", "Seems there are some tests that are failing. Can you address this?", "@fchollet I only needed to merge upstream...", "It seems there are still pending test failures. Could you take a look?", "@fchollet I only needed to merge upstream... All checks passed!", "Closing this PR since it has pushed to keras-team/kera repo. Thanks!"]}, {"number": 44104, "title": "Avoid doing reset when position is still in buffer in BufferedInputStream.", "body": "See https://github.com/tensorflow/tensorflow/issues/34510.\r\n\r\nThis is a duplicated PR of https://github.com/tensorflow/tensorflow/pull/34515, but it cannot be reopened, thus creating a new one.", "comments": ["@rohan100jain can you help review? thanks.", "i drew a diagram for better illustration:\r\nhttps://www.dropbox.com/s/0z4fv7u7atroixh/Photo%20Oct%2012%2C%203%2025%2056%20PM.jpg?dl=0\r\n\r\n", "Could you add a test for buffered_inputstream_test.cc that demonstrates the failure? I believe you can test it via \r\n\r\nbazel test -c opt tensorflow/core:__tensorflow_core_lib_io_legacy_lib_io_all_tests", "@burgerkingeater  Can you please check @rohan100jain's comments and keep us posted ? Thanks!", "this is still being worked on.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "This is still being worked on, thanks.\n\nOn Fri, Dec 11, 2020 at 5:04 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this PR still valid? Assigning the stalled label. Please\n> comment to reassure me that this is still being worked on.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/44104#issuecomment-743558536>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAK4WZ3QZ5CXT2RFKNS3YHTSUK6S5ANCNFSM4ST2U2EQ>\n> .\n>\n", "@rohan100jain Unit test added. Please take a look and let me know if anything else is needed. Thanks.", "@rohan100jain kindly ping", "@rohan100jain pinging again..", "Can you look at the failure, please?\r\n\r\n`//tensorflow/python/lib/io:file_io_test` is failing and the failure looks related to the PR\r\n\r\n```\r\n[       OK ] FileIoTest.test_session\r\n======================================================================\r\nERROR: testFileSeekableWithZip (__main__.FileIoTest)\r\ntestFileSeekableWithZip (__main__.FileIoTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/lib/io/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io_test.py\", line 674, in testFileSeekableWithZip\r\n    _ = [i for i in info.items()]\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 249, in items\r\n    return [(f, self[f]) for f in self.files]\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 249, in <listcomp>\r\n    return [(f, self[f]) for f in self.files]\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 228, in __getitem__\r\n    bytes = self.zip.open(key)\r\n  File \"/usr/lib/python3.6/zipfile.py\", line 1419, in open\r\n    % (zinfo.orig_filename, fname))\r\nzipfile.BadZipFile: File name in directory 'arr_0.npy' and header b'\\xbcx\\xf3`\\xb0\\x93\\x82P\\x0f' differ.\r\n\r\n======================================================================\r\nFAIL: testSeek (__main__.FileIoTest)\r\ntestSeek (__main__.FileIoTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/lib/io/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io_test.py\", line 462, in testSeek\r\n    self.assertEqual(18, f.tell())\r\nAssertionError: 18 != 27\r\n\r\n======================================================================\r\nFAIL: testSeekFromWhat (__main__.FileIoTest)\r\ntestSeekFromWhat (__main__.FileIoTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/lib/io/file_io_test.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io_test.py\", line 496, in testSeekFromWhat\r\n    self.assertEqual(18, f.tell())\r\nAssertionError: 18 != 27\r\n\r\n----------------------------------------------------------------------\r\nRan 65 tests in 0.947s\r\n\r\nFAILED (failures=2, errors=1)\r\n================================================================================\r\n```", "@mihaimaruseac fixed, please take a look, thanks.", "@mihaimaruseac thanks for the review. All check passed, can this PR be merged?", "It should get merged automatically once internal review and internal CI pass.", "@mihaimaruseac how is it going with the internal review and CI? It's been 3 days.", "Waiting for owner approval, it gets delayed as people are returning from holidays. Apologies for the delay", "@mihaimaruseac thanks. just curious about the process. Does every PR need to go through the internal review/CI process?", "@burgerkingeater yes, every PR is first approved on GitHub, there is a CI run that only tests the open source part of TF and there is a process (Copybara) to integrate the change into the internal version of TF. The result of this integration needs to be reviewed again and tested on CI again (both internally and re-exported to open source). If all is green, the change is merged internally and copybara then merges the PR.\r\n\r\nFor some PRs there is an additional review round: if APIs are changed, API owners need to approve too. This happens twice a week, all the API-owners tagged PRs and issues and internal changelists are reviewed during these meetings. Once approved, the process from above continues", "Thanks for the explanation! @mihaimaruseac "]}, {"number": 44103, "title": "Dealing with `from tensorflow.contrib import ...` in  tf_upgrade_v2 ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.3.0\r\n- Are you willing to contribute it (Yes/No):No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI ran into cases not covered by the `tf_upgrade_v2` tool yet, namely certain TF1 models in the zoo (https://github.com/tensorflow/models/tree/master/research)  that import from `tensorflow.contrib` - which was deprecated in TF2\r\n\r\nI created a small example to illustrate the current behaviour of  the `tf_upgrade_v2` tool on these:\r\n\r\n```\r\n%%bash\r\n\r\ncat > /tmp/example.py << EOL\r\ntry:\r\n    from tensorflow.contrib import quantize as contrib_quantize\r\n    from tensorflow.contrib import framework as contrib_framework\r\n    from tensorflow.contrib import slim as contrib_slim\r\n    from tensorflow.contrib.slim.nets import resnet_utils\r\n    from tensorflow.contrib import layers as contrib_layers\r\n    from tensorflow.contrib import tfprof as contrib_tfprof\r\n    from tensorflow.contrib import training as contrib_training\r\n    from tensorflow.contrib import metrics as contrib_metrics\r\nexcept Exception as e:\r\n    print(type(e).__name__, \":\", e)\r\n\r\nfrom tensorflow_addons import metrics as addons_metrics\r\n\r\ndef demo_deprecated():\r\n    print(dir(contrib_metrics))\r\n\r\ndef demo_ok():\r\n    print(dir(addons_metrics))\r\n\r\n\r\nif __name__ == '__main__':\r\n    demo_ok()\r\n    print(\"===========================\")\r\n    demo_deprecated()\r\nEOL\r\n```\r\n\r\nThen, according to https://www.tensorflow.org/guide/upgrade, we do:\r\n\r\n```\r\n!tf_upgrade_v2 \\\r\n  --infile /tmp/example.py \\\r\n  --outfile /tmp/example_v2.py\r\n```\r\n\r\nThe report file doesn't contain any warning about the deprecated modules:\r\n\r\n```!cat report.txt```\r\n\r\n\r\n```\r\nTensorFlow 2.0 Upgrade Script\r\n-----------------------------\r\nConverted 1 files\r\nDetected 0 issues that require attention\r\n--------------------------------------------------------------------------------\r\n================================================================================\r\nDetailed log follows:\r\n\r\n================================================================================\r\n--------------------------------------------------------------------------------\r\nProcessing file '/tmp/example.py'\r\n outputting to '/tmp/example_v2.py'\r\n--------------------------------------------------------------------------------\r\n\r\n\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\nThe two files are identical:\r\n\r\n```!diff -u /tmp/example.py /tmp/example_v2.py```\r\n\r\nHowever, unsurprisingly, the ModuleNotFoundError is thrown when we execute:\r\n\r\n```!python /tmp/example_v2.py```\r\n\r\n\r\n```\r\n2020-10-16 19:52:37.022043: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nModuleNotFoundError : No module named 'tensorflow.contrib'\r\n['CohenKappa', 'F1Score', 'FBetaScore', 'HammingLoss', 'MatthewsCorrelationCoefficient',  ... , 'utils']\r\n===========================\r\nTraceback (most recent call last):\r\n  File \"/tmp/example_v2.py\", line 25, in <module>\r\n    demo_deprecated()\r\n  File \"/tmp/example_v2.py\", line 16, in demo_deprecated\r\n    print(dir(contrib_metrics))\r\nNameError: name 'contrib_metrics' is not defined\r\n```\r\n\r\n\r\nIt would be nice to see the `tf_upgrade_v2` tool (more precisely: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/compatibility/tf_upgrade_v2_main.py) updated to treat these cases `from tensorflow.contrib import ...` .  \r\n\r\nMoreover, it would be great to have the `tf_upgrade_v2` tool extended so that it incorporates in the migration process (or at least as guidance in the report.txt) the successors of the modules in TF1 `tensorflow.contrib` , such as the `tensorflow_addons` (https://github.com/tensorflow/addons), `tf_slim` (https://github.com/tensorflow/models/tree/master/research/slim), etc.  \r\n\r\n**Will this change the current api? How?**\r\nNo API changes are necessary.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who tries to migrate TF1 models to TF2, when the files contain imports from `tensorflow.contrib`\r\n\r\n**Any Other info.**\r\n-\r\n\r\nMany thanks in advance,\r\nCamelia\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ddd1f9f2ccafdedac21965c7ca314be5/44103.ipynb). Thanks!", "Thank you for filing the issue and sorry for delay!\r\nWe should print an error/warning on `from tensorflow.contrib ...` import is used.\r\nNote that the upgrade documentation (https://www.tensorflow.org/guide/upgrade) has a note saying that `tf.contrib` references need to be updated manually and proposes to check https://github.com/tensorflow/addons.\r\n\r\n", "I think this is resolved.  As mentioned above, we need to update `contrib` manually.\r\n\r\n> Because of TensorFlow 2.x module deprecations (for example, tf.flags and tf.contrib), some changes can not be worked around by switching to compat.v1. Upgrading this code may require using an additional library (for example, absl.flags) or switching to a package in tensorflow/addons.\r\n\r\nPlease check the updated guide here https://www.tensorflow.org/guide/upgrade\r\n\r\nI am closing this issue as this was resolved. Thanks!\r\n", "> I think this is resolved. As mentioned above, we need to update `contrib` manually.\r\n> \r\n> > Because of TensorFlow 2.x module deprecations (for example, tf.flags and tf.contrib), some changes can not be worked around by switching to compat.v1. Upgrading this code may require using an additional library (for example, absl.flags) or switching to a package in tensorflow/addons.\r\n> \r\n> Please check the updated guide here https://www.tensorflow.org/guide/upgrade\r\n> \r\n> I am closing this issue as this was resolved. Thanks!\r\n\r\ni don't think this sloved!\r\ncan you tell me how to solve tf.contrib,the update guide can't explain how to solve it", "i don't think this sloved!\r\ncan you tell me how to solve tf.contrib,the update guide can't explain how to solve it"]}, {"number": 44102, "title": "Rename Raspbian to Raspberry Pi OS", "body": "This May, Raspbian was renamed to Raspberry Pi OS, as visible on the [official Raspberry Pi downloads page](https://www.raspberrypi.org/downloads/raspberry-pi-os/). In order to maintain consistency and reduce confusion for new users, we should replace `Raspbian` with `Raspberry Pi OS` in our docs wherever it makes sense.", "comments": []}]