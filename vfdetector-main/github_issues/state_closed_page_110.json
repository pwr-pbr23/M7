[{"number": 51754, "title": "[INTEL MKL] Skip CUDA specific remapper tests", "body": "FuseBatchNormGradWithReluGrad and FuseBatchNormGradWithAddAndReluGrad\r\nfusions are not available with MKL", "comments": []}, {"number": 51753, "title": "[tf.data] graduate choose_from_datasets API from experimental to tf.data.Dataset", "body": "This PR graduates the `tf.data.experimental.choose_from_datasets` API into `tf.data.Dataset.choose_from_datasets` by making the following changes:\r\n\r\n- [x] Adds the deprecation decorator for the experimental API.\r\n- [x] Add the `choose_from_datasets()` method to DatasetV2 class.\r\n- [x]  Updates example in documentation with new API.\r\n- [x]  Regenerate golden API's.\r\n- [x]  Updated the sample_from_datasets usage in tests.\r\n- [x]  Updated the RELEASE.md file\r\n\r\ncc: @aaudiber ", "comments": ["@aaudiber can you please approve the pull request again? ", "@aaudiber looks like there is an internal check failure. Anything that I can help with? Just wanted to get this merged before the dataset_ops refactor"]}, {"number": 51750, "title": "[AArch64] error: inlining failed in call to 'always_inline' 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch", "body": "I did a build today and got hit by same issue as in #41409 one. I use manylinux2014 image for builds.\r\n\r\ntop commit:\r\n\r\n```\r\ncommit 4e8bba06da6c3212c1d4e19ef86c42d905d47c7a (grafted, HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: TensorFlower Gardener <gardener@tensorflow.org>\r\nDate:   Mon Aug 30 03:37:48 2021 -0700\r\n\r\n    Merge pull request #50961 from nouiz:upstream-cudaMallocAsync-no_stats_error\r\n    \r\n    PiperOrigin-RevId: 393736407\r\n    Change-Id: I31f5ba2b885683cc587a30cdce70e0f20ffefef9\r\n```\r\n\r\nError:\r\n\r\n```\r\n      /opt/rh/devtoolset-10/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-o\r\npt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o' -fPIC -iquoteexternal/com_google_absl -iquotebazel-\r\nout/aarch64-opt/bin/external/com_google_absl -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwr\r\nite-strings -DNOMINMAX -Wno-pass-failed -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/com_google_absl/absl/random/internal/randen_hwaes.cc -o bazel-out/aarch64-opt/bin/external\r\n/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o)                                                                                                                                                                                                     \r\n    Execution platform: @local_execution_config_platform//:platform                                                                        \r\n    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:\r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h: In function 'Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)':\r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12332:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch\r\n    12332 | vaesmcq_u8 (uint8x16_t data)                                                                                                                                                                                                                                              \r\n          | ^~~~~~~~~~                                                                                                                                                                                                                                                                \r\n    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      \r\n      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          \r\n          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       \r\n    In file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:                                                                                                                                                                                          \r\n    /opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/include/arm_neon.h:12318:1: error: inlining failed in call to 'always_inline' 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch                                                     \r\n    12318 | vaeseq_u8 (uint8x16_t data, uint8x16_t key)                                                                                                                                                                                                                               \r\n          | ^~~~~~~~~                                                                                                                                                                                                                                                                 \r\n    external/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here                                                                                                                                                                                      \r\n      255 |   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;                                                                                                                                                                                                          \r\n          |          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                                                                                                                       \r\n    At global scope:                                                                                                                                                                                                                                                                  \r\n    cc1plus: note: unrecognized command-line option '-Wno-pass-failed' may have been intended to silence earlier diagnostics                                                                                                                                                          \r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build                                                                                                                                                                                                           \r\n    INFO: Elapsed time: 5671.673s, Critical Path: 500.29s                                                                                                                                                                                                                             \r\n    INFO: 9206 processes: 996 internal, 8210 local.                                                                                                                                                                                                                                   \r\n\r\n```\r\n\r\n_Originally posted by @hrw in https://github.com/tensorflow/tensorflow/issues/41409#issuecomment-908364569_", "comments": ["Hi @hrw, \r\nCould you please  fill the template  as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "**System information**\r\n- OS Platform and Distribution: manylinux2014 container (so CentOS 7)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: git HEAD\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: not installed\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: gcc (GCC) 10.2.1 20210130 (Red Hat 10.2.1-11)\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ git clone --depth 1 https://git.linaro.org/ci/job/configs.git\r\n$ cp configs/ldcg-python-manylinux-tensorflow/build-manylinux2014-wheels.sh .\r\n$ chmod 755 build-manylinux2014-wheels.sh\r\n$ echo '  - \"git\"' >> configs/ldcg-python-manylinux-tensorflow/ansible/vars/vars.yml\r\n$ docker run --rm -u root --security-opt seccomp=unconfined -v $PWD:/tmp/workspace quay.io/pypa/manylinux2014_aarch64 /tmp/workspace/build-manylinux2014-wheels.sh\r\n```\r\n\r\nThis then installs Ansible and runs set of playbooks to build Tensorflow. Which means:\r\n1. installation of proper Bazel version (3.7.2 from github release in this case)\r\n2. installation of required system packages\r\n2. creation of virtualenv\r\n3. installation of numpy, h5py and other Python packages\r\n4. clone of HEAD of tensorflow repo (depth=1)\r\n5. configure build\r\n6. run build to get Python wheel\r\n\r\nSteps 3-7 are done in a loop over requested Python versions. This time I used only 3.9 as I needed build of git version to build Tensorflow/io package.", "https://ci.linaro.org/job/ldcg-python-manylinux-tensorflow-nightly/80/consoleText for example build log", "Hi @Saduf2019 ,Could you please look into this issue?", "Having the same issue. Building Tensorflow Lite pip package on Ubuntu 20.04 with\r\n```sudo CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3.7 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\"   tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37   tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64```\r\n\r\nHere is the output:\r\n```\r\nSUBCOMMAND: # @com_google_protobuf//:protobuf_lite [action 'Compiling com_google_protobuf/src/google/protobuf/any_lite.cc', configuration: b6fcde5caf516b18393a5aaba5217f71c7f0fce6771445fc2ee2d8aad33523ea, execution platform: @local_execution_config_platform//:platform]\r\n(cd /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include-fixed -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/include/c++/8.3.0/ -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.7m -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.pic.o' -fPIC -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -w -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-std=c++14' -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_protobuf/src/google/protobuf/any_lite.cc -o bazel-out/aarch64-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/any_lite.pic.o)\r\nERROR: /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/com_google_absl/absl/random/internal/BUILD.bazel:323:11: C++ compilation of rule '@com_google_absl//absl/random/internal:randen_hwaes_impl' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command \r\n  (cd /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include-fixed -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/include/c++/8.3.0/ -isystem /root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.7m -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -w -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-std=c++14' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -Wno-pass-failed -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/com_google_absl/absl/random/internal/randen_hwaes.cc -o bazel-out/aarch64-opt/bin/external/com_google_absl/absl/random/internal/_objs/randen_hwaes_impl/randen_hwaes.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:\r\n/root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include/arm_neon.h: In function 'Vector128 {anonymous}::AesRound(const Vector128&, const Vector128&)':\r\n/root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include/arm_neon.h:12440:1: error: inlining failed in call to always_inline 'uint8x16_t vaesmcq_u8(uint8x16_t)': target specific option mismatch\r\n vaesmcq_u8 (uint8x16_t data)\r\n ^~~~~~~~~~\r\nexternal/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here\r\n   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;\r\n          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from external/com_google_absl/absl/random/internal/randen_hwaes.cc:229:\r\n/root/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include/arm_neon.h:12426:1: error: inlining failed in call to always_inline 'uint8x16_t vaeseq_u8(uint8x16_t, uint8x16_t)': target specific option mismatch\r\n vaeseq_u8 (uint8x16_t data, uint8x16_t key)\r\n ^~~~~~~~~\r\nexternal/com_google_absl/absl/random/internal/randen_hwaes.cc:255:20: note: called from here\r\n   return vaesmcq_u8(vaeseq_u8(state, uint8x16_t{})) ^ round_key;\r\n          ~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nINFO: Elapsed time: 547.661s, Critical Path: 65.94s\r\nINFO: 4323 processes: 1428 internal, 2895 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\nCommits tried:\r\n18794fe8889572b91cee62184ad3e71ef6464d51\r\n2509a6e82e6eb95888de697539845089923c23d5\r\ncurrent master\r\n\r\n", "Related issue https://github.com/abseil/abseil-cpp/issues/662.", "@hwrod,\r\n@cfRod \r\n\r\n* Can you try [tensorflow-aarch64.patch.gz](https://github.com/tensorflow/tensorflow/files/7093182/tensorflow-aarch64.patch.gz) patch ?\r\n* It is a backport from [absl upstream](https://github.com/abseil/abseil-cpp/commit/2e94e5b6e152df9fa9c2fe8c1b96e1393973d32c) that address exactly this issue.\r\n\r\nIf works we can PR it into tensorflow.\r\n", "will check tomorrow", "```\r\nINFO: Elapsed time: 5664.480s, Critical Path: 543.17s\r\nINFO: 5889 processes: 223 internal, 5666 local.\r\nINFO: Build completed successfully, 5889 total actions\r\n```\r\n\r\n```\r\ncommit 8eaf306cdb5a7db33a72e8c2be96591b0defa6b5 (HEAD -> master, origin/master, origin/HEAD)\r\nAuthor: Mehdi Amini <aminim@google.com>\r\nDate:   Wed Sep 1 11:25:16 2021 -0700\r\n\r\n    Fix tensorflow/core/ir/... build: use tf_cc_binary instead of tf_native_binary\r\n    \r\n    This is fixing linkage issue, likely discrepancies between internal / external builds.\r\n    \r\n    PiperOrigin-RevId: 394275436\r\n    Change-Id: I16d2d70dc29b455557cb6779c0ebb04da8625be8\r\n```", "Build successful with the patch (@cbalint13 )\r\n```\r\nINFO: Elapsed time: 1090.835s, Critical Path: 255.11s\r\nINFO: 4664 processes: 204 internal, 4460 local.\r\nINFO: Build completed successfully, 4664 total actions\r\n\r\n```\r\nIssues when installing from wheel. Seems to be related to https://github.com/tensorflow/io/issues/1441 (@hrw?)\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-io-gcs-filesystem>=0.20.0 (from tensorflow) (from versions: none)\r\nERROR: No matching distribution found for tensorflow-io-gcs-filesystem>=0.20.0\r\n\r\n```", "It is kind of chicken and egg.\r\n\r\nTo install Tensorflow 2.7 you need tensorflow-io-gcs-filesystem.\r\n\r\nTo build tensorflow-io-gcs-filesystem you need Tensorflow package. Good that 0.20 needs TF 2.6.0+.\r\n\r\nI have something built but need to clean my hacks and submit them to tensorflow/io (several already got merged).", "So those changes will be in tensorflow/io?... and do we need TensorFlow then to pick the right version of tensorflow-io-gcs-filesystem?", "@cfRod no - this issue is about Tensorflow. Once it is fixed we can build Tensorflow 2.7-git.\r\n\r\nInstalling it would require tensorflow-io-gcs-filesystem which is from tensorflow/io which is other project. And I sent several aarch64 related changes/issues there already.", "> Build successful with the patch (@cbalint13 )\r\n> \r\n> ```\r\n> INFO: Elapsed time: 1090.835s, Critical Path: 255.11s\r\n> INFO: 4664 processes: 204 internal, 4460 local.\r\n> INFO: Build completed successfully, 4664 total actions\r\n> ```\r\n\r\nThanks for the confirmation @cfRod ! \r\n* It seems to pass on my [side](https://download.copr.fedorainfracloud.org/results/rezso/ML/fedora-rawhide-aarch64/02679631-tensorflow/builder-live.log) too.\r\n* I proposed it upstream via a new [PR#51806](https://github.com/tensorflow/tensorflow/pull/51806) .\r\n\r\n\r\n", "@hrw > no - this issue is about Tensorflow. \r\nI was referring to the issue on tensorflow-io-gcs-filesystem actually but thanks for the update. Is it just tensorflow/io#1441 that is remaining? \r\n\r\n@cbalint13 thanks for the PR.", "Depends on build environment. I do builds in Debian 'bullseye' so https://github.com/tensorflow/io/issues/1510 is a problem too (due to gcc 10).\r\n\r\nSorry, manylinux2014 is too old ;(", "https://snapshots.linaro.org/ldcg/python-cache/ has aarch64 tensorflow-io packages if someone needs.\r\n\r\nversioned as \"0.20.0\"  but they are git HEAD ones.", "Thanks for the package @hrw ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51750\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51750\">No</a>\n"]}, {"number": 51747, "title": "\"ZeroDivisionError: integer division or modulo by zero\" while backpropagating #51653", "body": "\"ZeroDivisionError: integer division or modulo by zero\" while backpropagating #51653\r\nlogic reference:\r\nhttps://github.com/tensorflow/tensorflow/issues/51653#issuecomment-904756234", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51747) for more info**.\n\n<!-- need_sender_cla -->", "@sannamir  Can you please sign CLA. Thanks!", "@googlebot I signed it!", "@sannamir  Can you please check @allenlavoie's comments and resolve conflicts?. Thanks!", "@sannamir Any update on this PR? Please. Thanks!\r\n", "@sannamir Any update on this PR? Please. Thanks!", "It has passed all checks.", "It has a conflict", "@sannamir  Can you please resolve conflicts? Thanks!", "@sannamir Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 51746, "title": "Error loading saved model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: NVIDIA GeForce RTX 2060, 6144 MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter saving a Quant Aware model to saved_model using model.save(), I get this error while loading it with tf.keras.models.load_model() - KeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'. This happens only when the model has SeparableConv2D layer.\r\n\r\n**Describe the expected behavior**\r\nThe model should load without any error. It does if the model has only Conv2D layers, but doesn't if it has SeparableConv2D layers\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1_afuGL5IUrquuXEhAJkOExN5HXJUbtOt?usp=sharing \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nKeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-23-39c24960e684> in <module>\r\n      1 with tfmot.quantization.keras.quantize_scope():\r\n----> 2     q_model = tf.keras.models.load_model(saved_model_dir.format(epoch=epoch))\r\n      3 \r\n      4 PATH = '/media/dash/027A71F47A71E537/Dash_ext/sonyliv/exp/vids/input/KBC_1_1080p_640x360_664_kbps/input_2.png'\r\n      5 im = cv2.imread(PATH)\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    210       if isinstance(filepath, six.string_types):\r\n    211         loader_impl.parse_saved_model(filepath)\r\n--> 212         return saved_model_load.load(filepath, compile, options)\r\n    213 \r\n    214   raise IOError(\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)\r\n    142   for node_id, loaded_node in keras_loader.loaded_nodes.items():\r\n    143     nodes_to_load[keras_loader.get_path(node_id)] = loaded_node\r\n--> 144   loaded = tf_load.load_partial(path, nodes_to_load, options=options)\r\n    145 \r\n    146   # Finalize the loaded layers and remove the extra tracked dependencies.\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_partial(export_dir, filters, tags, options)\r\n    763     A dictionary mapping node paths from the filter to loaded objects.\r\n    764   \"\"\"\r\n--> 765   return load_internal(export_dir, tags, options, filters=filters)\r\n    766 \r\n    767 \r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)\r\n    888       try:\r\n    889         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n--> 890                             ckpt_options, filters)\r\n    891       except errors.NotFoundError as err:\r\n    892         raise FileNotFoundError(\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in __init__(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, filters)\r\n    158       self._concrete_functions[name] = _WrapperFunction(concrete_function)\r\n    159 \r\n--> 160     self._load_all()\r\n    161     self._restore_checkpoint()\r\n    162 \r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _load_all(self)\r\n    254   def _load_all(self):\r\n    255     \"\"\"Loads all nodes and functions from the SavedModel and their edges.\"\"\"\r\n--> 256     self._load_nodes()\r\n    257     self._load_edges()\r\n    258     # TODO(b/124045874): There are limitations with functions whose captures\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _load_nodes(self)\r\n    432         # interface.\r\n    433         continue\r\n--> 434       node, setter = self._recreate(proto, node_id)\r\n    435       nodes[node_id] = node\r\n    436       node_setters[node_id] = setter\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _recreate(self, proto, node_id)\r\n    550     if kind not in factory:\r\n    551       raise ValueError(\"Unknown SavedObject type: %r\" % kind)\r\n--> 552     return factory[kind]()\r\n    553 \r\n    554   def _recreate_user_object(self, proto, node_id):\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in <lambda>()\r\n    539             lambda: self._recreate_user_object(proto.user_object, node_id)),\r\n    540         \"asset\": lambda: self._recreate_asset(proto.asset),\r\n--> 541         \"function\": lambda: self._recreate_function(proto.function),\r\n    542         \"bare_concrete_function\": functools.partial(\r\n    543             self._recreate_bare_concrete_function,\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in _recreate_function(self, proto)\r\n    578   def _recreate_function(self, proto):\r\n    579     return function_deserialization.recreate_function(\r\n--> 580         proto, self._concrete_functions), setattr\r\n    581 \r\n    582   def _recreate_bare_concrete_function(self, proto):\r\n\r\n~/Dash/virtual_envs/tf2_gpu_conda/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py in recreate_function(saved_function, concrete_functions)\r\n    275   concrete_function_objects = []\r\n    276   for concrete_function_name in saved_function.concrete_functions:\r\n--> 277     concrete_function_objects.append(concrete_functions[concrete_function_name])\r\n    278 \r\n    279   for cf in concrete_function_objects:\r\n\r\nKeyError: '__inference_depthwise_conv2d_layer_call_and_return_conditional_losses_8913'\r\n", "comments": ["Is there any way to save the Quant aware model to saved model or pb format?", "Hi @dash-myelin , It seems you have missed two steps  - model.compile() and model.fit() before saving the model , you can use the setup_pretrained_weights() function from this reference [colab link](https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/quantization/training_comprehensive_guide.ipynb#scrollTo=lvpH1Hg7ULFz) to train your model.", "Hi @mohantym , actually I am training the model in a VM, with a custom training loop (not the default fit method). In my local, I get the relevant .ckpt files, load it to a new model, and save the model to a saved_model. \r\n\r\nAny suggestions for this?", "@mohantym The model I am trying to save is already trained for some epochs with quantization aware training. The issue seems to be saving a QAT model with QuantWrapper(SeparableConv2D)", "Ok! Could you please provide the  same complete code in a Colab gist to expedite the issue  then?", "@mohantym - https://colab.research.google.com/drive/1_afuGL5IUrquuXEhAJkOExN5HXJUbtOt?usp=sharing \r\n\r\nThere you go. I have shown clearly in the code the issue with Quantize Aware SeparableConv2D layer", "@mohantym It's a critical feature. I use SeparableConvs in all my models for speed. And tf2.x allows only model.save() to create pb files. It's a serious bug.\r\n\r\nPlease tell me if there is another way to achieve saving and loading Quant aware models with SeparableConvs. ", "Hi @sanatmpa1 ,Could you please look into this issue?", "@mohantym @sanatmpa1 Is anyone going to help me with this issue? It's a critical bug that is not supposed to be there in the first place and still, no one is responding for the past 2 days. \r\n\r\nIt's really frustrating that after all the training and evaluation when you are finally going to deploy a model, you can't even save it! \r\n\r\nAlso, it's a **Quantization Aware Model** that is being saved. \r\n\r\nPlease help me..", "@dash-myelin,\r\n\r\nI am able to reproduce the issue with `TF 2.6.0` whereas it works fine when using `tf-nightly`. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/b4d16c99e24f16c7fbbd2af1832c6de9/51746.ipynb) and let me know if it helps. Thanks!", "@sanatmpa1,\r\n\r\nThanks for the suggestion to use ```tf-nightly```. However, I ran into this issue with SeparableConvs. I have added my actual model code - [gist here](https://colab.research.google.com/drive/1_afuGL5IUrquuXEhAJkOExN5HXJUbtOt?usp=sharing) \r\n\r\nPlease let me know what can be done.", "@sanatmpa1 \r\n\r\nCan you please help me solve this issue. I am really stuck with this QAt issue with tf-nightly", "@dash-myelin,\r\n\r\nI can see that this issue with your custom model is not just with `tf-nightly`, it happens even in `2.6.0`. Take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/f2e4ca93d3110ae0da974bcc590d5aa9/issue.ipynb).", "@sanatmpa1 \r\n\r\nOk. But with tf-nightly, I can save the QAT model with Separable Convs. How Do I get rid of this error?", "@dash-myelin,\r\n\r\nI feel the issue is probably related to model optimization. Can you open an issue with [model optimization repo](https://github.com/tensorflow/model-optimization/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51746\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51746\">No</a>\n"]}, {"number": 51745, "title": "NameError:", "body": "![4](https://user-images.githubusercontent.com/26819449/131285254-1698b895-d3d7-4c10-9251-8c169d7d5dd8.JPG)\r\n\r\n![5](https://user-images.githubusercontent.com/26819449/131285259-61b09421-68ae-490e-9a1d-30aeefa364a5.JPG)\r\n\r\nTensorFlow version : 2.2\r\nKeras version: 2.4.3\r\nEven after defining it. It's telling to define it.\r\nPlease assign someone who can reply fast.\r\nThank You.\r\n", "comments": ["@starboyvarun We see that you have attached the snapshot of the issue .In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Could you please try on the latest version of TF 2.6 and let us know if it helps?Thanks!", "![i](https://user-images.githubusercontent.com/26819449/131324477-5c1bd7b3-67a2-44de-8957-9c1a59bd7809.JPG)\r\n\r\n![j](https://user-images.githubusercontent.com/26819449/131324500-18585d5d-9648-4a7e-85a9-82ae6f309a02.JPG)\r\n\r\nActually, the framework [janggu], I am using that supports only this version. and a member of that repo advised me to use these versions. Even it's mentioned in read me.\r\n", "@starboyvarun Could you please provide us the complete code of colab gist, notebook or github link to reproduce the issue reported here.It is difficult for us to replicate the issue with the snapshot .Thanks!", "@sushreebarsa sure.\r\nlink: https://colab.research.google.com/drive/1h8ECIVvp3sezj2q_hRHC3D3pacT9oooO?usp=sharing", "@starboyvarun Thanks for the quick update! I tried to run your code on Colab and didn't face the error reported as `NameError: name GlobalAveragePooling2D is not defined`. I faced `NotImplementedError: \"intersectBed\" does not appear to be installed or on the path, so this method is disabled.  Please install a more recent version of BEDTools and re-import to use this method.`error while running your code, Please find the [gist](https://colab.research.google.com/gist/sushreebarsa/3b3699df13663c538aea37eca89b717c/30augsolved.ipynb#scrollTo=RU9QSOaW8nL_) for reference. Can you please try to use **`from keras.layers import GlobalAveragePooling2D`** to resolve the NameError, we hope it would work if you're facing this issue. We can see the issue is not related to TensorFlow and more specific to Janggu so can you please post this issue on Janggu repo .Please close this issue here and open a new ticket on Janggu repo if you have further concerns .We hope you'll get the right help there! Thank you!", "I just created a copy of my google collab.\r\nAnd sent you the link, its showing NameError: name GlobalAveragePooling2D is not defined "]}, {"number": 51744, "title": "NameError:", "body": "![4](https://user-images.githubusercontent.com/26819449/131285254-1698b895-d3d7-4c10-9251-8c169d7d5dd8.JPG)\r\n\r\n![5](https://user-images.githubusercontent.com/26819449/131285259-61b09421-68ae-490e-9a1d-30aeefa364a5.JPG)\r\n\r\nTensorFlow version : 2.2\r\n Keras version : 2.4.3\r\nEven after defining it. It's telling to define it.\r\n\r\n", "comments": ["@starboyvarun,\r\n\r\nCan you share a reproducible code to expedite the trouble-shooting process? Thanks!", "Duplicate of #51745", "@sanatmpa1  you replied very late that's why.\r\n\r\nI mean tilakrayal", "@sanatmpa1  Here is code.\r\n![1](https://user-images.githubusercontent.com/26819449/131329177-db11ab2b-5d8c-438c-8686-1c1517c49ec4.JPG)\r\n![2](https://user-images.githubusercontent.com/26819449/131329187-25c36f3b-adba-4cd6-ae65-e18ec7af7466.JPG)\r\n![3](https://user-images.githubusercontent.com/26819449/131329190-9f00478e-37ed-49de-9289-200397a4b973.JPG)\r\nDo you need google Collab link?\r\n", "@starboyvarun,\r\n\r\nAs this is a duplicate, I would recommend you to provide the colab gist on #51745 and confirm if I can close this issue. Thanks!", "you solve it. I  will close the other one. @sanatmpa1 ", "@starboyvarun,\r\n\r\nAs I see that @sushreebarsa is already working on the issue #51745. To avoid duplication of efforts, Can you kindly confirm if we can close the issue here? Thanks!", "@sanatmpa1 solve the issue I have closed that issue.\r\nIf you cant do it .assign someone else.", "@sanatmpa1  solve the issue or assign someone else", "@starboyvarun,\r\n\r\nCan you share your code as colab gist for us to take a look? I am not able to access the colab link that you posted in #51745.", "@sanatmpa1  can you just tell me how to add the path to a particular library in google collab?", "@starboyvarun,\r\n\r\nCan you take a look at this [blog](https://zerowithdot.com/colab-workspace/) and this SO [thread](https://stackoverflow.com/questions/55253498/how-do-i-install-a-library-permanently-in-colab) and let me know if this is what you are looking for?", "@sanatmpa1  I have solved this particular error. Thank you for your blog and thread link really appreciate it.\r\n\r\nThanks!"]}, {"number": 51743, "title": "typing-extensions package is conflicted", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS 11.5.2`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: `2.6.0`\r\n- Python version: `3.8.10`\r\n- Installed using virtualenv? pip? conda?: `virtualenv` and `pip`\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the problem**\r\nI installed `tensorflow` using `pip` on the virtualenv. The `typing-extensions` package is conflicted with `black` package dependency. `black==21.8b0` is required `typing-extensions>=3.10.0.0`. But, `tensorflow` has `'typing_extensions ~= 3.7.4` dependency. Are there any plans to upgrade the `typing-extensions` dependency package version?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\n$ python3 -m venv venv\r\n$ . venv/bin/activate\r\n$ pip install -U pip\r\n$ pip install black\r\n$ pip install tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```bash\r\n$ pip install tensorflow\r\nRequirement already satisfied: tensorflow in ./venv/lib/python3.8/site-packages (2.6.0)\r\nRequirement already satisfied: keras-preprocessing~=1.1.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.2)\r\nRequirement already satisfied: astunparse~=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.6.3)\r\nRequirement already satisfied: opt-einsum~=3.3.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.3.0)\r\nCollecting typing-extensions~=3.7.4\r\n  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\nRequirement already satisfied: numpy~=1.19.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.19.5)\r\nRequirement already satisfied: wrapt~=1.12.1 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12.1)\r\nRequirement already satisfied: tensorflow-estimator~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)\r\nRequirement already satisfied: six~=1.15.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.15.0)\r\nRequirement already satisfied: wheel~=0.35 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.37.0)\r\nRequirement already satisfied: google-pasta~=0.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.2.0)\r\nRequirement already satisfied: flatbuffers~=1.12.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.12)\r\nRequirement already satisfied: tensorboard~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)\r\nRequirement already satisfied: keras~=2.6 in ./venv/lib/python3.8/site-packages (from tensorflow) (2.6.0)\r\nRequirement already satisfied: protobuf>=3.9.2 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.17.3)\r\nRequirement already satisfied: gast==0.4.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.4.0)\r\nRequirement already satisfied: termcolor~=1.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.1.0)\r\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (1.39.0)\r\nRequirement already satisfied: clang~=5.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (5.0)\r\nRequirement already satisfied: absl-py~=0.10 in ./venv/lib/python3.8/site-packages (from tensorflow) (0.13.0)\r\nRequirement already satisfied: h5py~=3.1.0 in ./venv/lib/python3.8/site-packages (from tensorflow) (3.1.0)\r\nRequirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\r\nRequirement already satisfied: werkzeug>=0.11.15 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\r\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\r\nRequirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (56.0.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\r\nRequirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in ./venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.5)\r\nRequirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\r\nRequirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\r\nRequirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\r\nInstalling collected packages: typing-extensions\r\n  Attempting uninstall: typing-extensions\r\n    Found existing installation: typing-extensions 3.10.0.1\r\n    Uninstalling typing-extensions-3.10.0.1:\r\n      Successfully uninstalled typing-extensions-3.10.0.1\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\nblack 21.8b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.\r\nSuccessfully installed typing-extensions-3.7.4.3\r\n```\r\n", "comments": ["Hi @sanatmpa1 ,Could you please look into this issue?", "If just the two are specified then `pip` is able to figure out the dependency mismatch and find a version of `black` (`v21.7.b0`) that will work\r\n\r\n<details>\r\n<summary>Full Example in a python:3.9 Docker image:</summary>\r\n\r\n```console\r\n$ docker run --rm -ti python:3.9 /bin/bash\r\nroot@12adf30cd4d4:/# python -m venv /usr/local/venv && . /usr/local/venv/bin/activate\r\n(venv) root@12adf30cd4d4:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@12adf30cd4d4:/# python -m pip install black tensorflow\r\nCollecting black\r\n  Downloading black-21.8b0-py3-none-any.whl (148 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148 kB 6.3 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 458.4 MB 40.8 MB/s \r\nCollecting pathspec<1,>=0.9.0\r\n  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\r\nCollecting tomli<2.0.0,>=0.2.6\r\n  Downloading tomli-1.2.1-py3-none-any.whl (11 kB)\r\nCollecting click>=7.1.2\r\n  Downloading click-8.0.1-py3-none-any.whl (97 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 9.5 MB/s \r\nCollecting mypy-extensions>=0.4.3\r\n  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\r\nCollecting platformdirs>=2\r\n  Downloading platformdirs-2.3.0-py3-none-any.whl (13 kB)\r\nCollecting typing-extensions>=3.10.0.0\r\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\r\nCollecting regex>=2020.1.8\r\n  Downloading regex-2021.8.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 759 kB 42.1 MB/s \r\nCollecting keras~=2.6\r\n  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 51.1 MB/s \r\nCollecting clang~=5.0\r\n  Downloading clang-5.0.tar.gz (30 kB)\r\nCollecting h5py~=3.1.0\r\n  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4 MB 59.1 MB/s \r\nCollecting keras-preprocessing~=1.1.2\r\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 2.4 MB/s \r\nCollecting opt-einsum~=3.3.0\r\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65 kB 5.3 MB/s \r\nCollecting grpcio<2.0,>=1.37.0\r\n  Downloading grpcio-1.39.0-cp39-cp39-manylinux2014_x86_64.whl (4.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 31.8 MB/s \r\nCollecting flatbuffers~=1.12.0\r\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\nCollecting six~=1.15.0\r\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\r\nCollecting gast==0.4.0\r\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\nCollecting tensorboard~=2.6\r\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6 MB 29.8 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.5.1-cp39-cp39-manylinux2010_x86_64.whl (454.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454.5 MB 23.6 MB/s \r\nCollecting grpcio~=1.34.0\r\n  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 38.4 MB/s \r\nCollecting keras-nightly~=2.5.0.dev\r\n  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 72.1 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454.4 MB 73 kB/s \r\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of black to determine which version is compatible with other requirements. This could take a while.\r\nCollecting black\r\n  Downloading black-21.7b0-py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 52.8 MB/s \r\nCollecting appdirs\r\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\nCollecting typing-extensions~=3.7.4\r\n  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\nCollecting termcolor~=1.1.0\r\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\r\nCollecting astunparse~=1.6.3\r\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting numpy~=1.19.2\r\n  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.9 MB 41.4 MB/s \r\nCollecting absl-py~=0.10\r\n  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132 kB 28.9 MB/s \r\nCollecting tensorflow-estimator~=2.6\r\n  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 462 kB 29.1 MB/s \r\nRequirement already satisfied: wheel~=0.35 in /usr/local/venv/lib/python3.9/site-packages (from tensorflow) (0.37.0)\r\nCollecting wrapt~=1.12.1\r\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\r\nCollecting protobuf>=3.9.2\r\n  Downloading protobuf-3.17.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0 MB 34.8 MB/s \r\nCollecting google-pasta~=0.2\r\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57 kB 8.0 MB/s \r\nCollecting werkzeug>=0.11.15\r\n  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 288 kB 40.0 MB/s \r\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\r\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.9 MB 32.5 MB/s \r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (58.0.2)\r\nCollecting google-auth<2,>=1.6.3\r\n  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 152 kB 32.8 MB/s \r\nCollecting requests<3,>=2.21.0\r\n  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62 kB 1.2 MB/s \r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 781 kB 29.1 MB/s \r\nCollecting markdown>=2.6.8\r\n  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 10.2 MB/s \r\nCollecting pyasn1-modules>=0.2.1\r\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 155 kB 32.7 MB/s \r\nCollecting rsa<5,>=3.1.4\r\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting pyasn1<0.5.0,>=0.4.6\r\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 8.6 MB/s \r\nCollecting urllib3<1.27,>=1.21.1\r\n  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 138 kB 37.6 MB/s \r\nCollecting charset-normalizer~=2.0.0\r\n  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\r\nCollecting idna<4,>=2.5\r\n  Downloading idna-3.2-py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 10.0 MB/s \r\nCollecting certifi>=2017.4.17\r\n  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 145 kB 28.4 MB/s \r\nCollecting oauthlib>=3.0.0\r\n  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146 kB 26.5 MB/s \r\nBuilding wheels for collected packages: clang, termcolor, wrapt\r\n  Building wheel for clang (setup.py) ... done\r\n  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=26573dc9851cb201eebc9f3acd403834783398b5f7140b7fd86d1942a44c38aa\r\n  Stored in directory: /root/.cache/pip/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\r\n  Building wheel for termcolor (setup.py) ... done\r\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=e374182ff99d7aa8fff47b8b498e27a1a475777ed7c2ecad2abc722f62b221bf\r\n  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\r\n  Building wheel for wrapt (setup.py) ... done\r\n  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=73034 sha256=92b09a8a10deab562a852acbbfe5c7cda670f0e4c1c47103f2a0591854ec8fa7\r\n  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\r\nSuccessfully built clang termcolor wrapt\r\nInstalling collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tomli, termcolor, tensorflow-estimator, tensorboard, regex, pathspec, opt-einsum, mypy-extensions, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, click, clang, astunparse, appdirs, tensorflow, black\r\nSuccessfully installed absl-py-0.13.0 appdirs-1.4.4 astunparse-1.6.3 black-21.7b0 cachetools-4.2.2 certifi-2021.5.30 charset-normalizer-2.0.4 clang-5.0 click-8.0.1 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.39.0 h5py-3.1.0 idna-3.2 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 mypy-extensions-0.4.3 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 pathspec-0.9.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 regex-2021.8.28 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 termcolor-1.1.0 tomli-1.2.1 typing-extensions-3.7.4.3 urllib3-1.26.6 werkzeug-2.0.1 wrapt-1.12.1\r\n(venv) root@12adf30cd4d4:/# pip list | grep \"black\\|tensorflow\"\r\nblack                   21.7b0\r\ntensorflow              2.6.0\r\ntensorflow-estimator    2.6.0\r\n```\r\n\r\n</details>\r\n\r\nbut that `tensorflow` is pinning down to the patch release here (`typing-extensions~=3.7.4`) is pretty restrictive (and making it so that everyone using TensorFlow needs to treat it like an application and not a library. c.f.  PR #40789 for another example of problems coming from being overly restrictive, though this is just a historical note as the SciPy dependency has been removed and so no longer affects TensorFlow). \r\n\r\nIn more complicated environments though I can get `pip` to fail on this problem.\r\n\r\n**Edit:** Following @alexander-held example below I can get things to install on Python 3.9 but not on Python 3.8. c.f. example in details for `pip install black tensorflow tensorflow-probability` on Python 3.9\r\n\r\n<details>\r\n<summary>Example with TensorFlow Probability in a python:3.9 Docker image:</summary>\r\n\r\n```console\r\n$ docker run --rm -ti python:3.9 /bin/bash\r\nroot@320d8517e990:/# python -m venv /usr/local/venv && . /usr/local/venv/bin/activate\r\n(venv) root@320d8517e990:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@320d8517e990:/# python -m pip install black tensorflow tensorflow-probability\r\nCollecting black\r\n  Downloading black-21.8b0-py3-none-any.whl (148 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148 kB 5.6 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 458.4 MB 28 kB/s \r\nCollecting tensorflow-probability\r\n  Downloading tensorflow_probability-0.13.0-py2.py3-none-any.whl (5.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.4 MB 46.2 MB/s \r\nCollecting mypy-extensions>=0.4.3\r\n  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\r\nCollecting tomli<2.0.0,>=0.2.6\r\n  Downloading tomli-1.2.1-py3-none-any.whl (11 kB)\r\nCollecting platformdirs>=2\r\n  Downloading platformdirs-2.3.0-py3-none-any.whl (13 kB)\r\nCollecting typing-extensions>=3.10.0.0\r\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\r\nCollecting regex>=2020.1.8\r\n  Downloading regex-2021.8.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 759 kB 33.1 MB/s \r\nCollecting click>=7.1.2\r\n  Downloading click-8.0.1-py3-none-any.whl (97 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 9.6 MB/s \r\nCollecting pathspec<1,>=0.9.0\r\n  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\r\nCollecting clang~=5.0\r\n  Downloading clang-5.0.tar.gz (30 kB)\r\nCollecting astunparse~=1.6.3\r\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\nCollecting wrapt~=1.12.1\r\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\r\nCollecting keras-preprocessing~=1.1.2\r\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 1.9 MB/s \r\nCollecting grpcio<2.0,>=1.37.0\r\n  Downloading grpcio-1.39.0-cp39-cp39-manylinux2014_x86_64.whl (4.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.3 MB 41.9 MB/s \r\nCollecting protobuf>=3.9.2\r\n  Downloading protobuf-3.17.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0 MB 47.2 MB/s \r\nCollecting google-pasta~=0.2\r\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57 kB 7.9 MB/s \r\nCollecting tensorflow-estimator~=2.6\r\n  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 462 kB 56.3 MB/s \r\nRequirement already satisfied: wheel~=0.35 in /usr/local/venv/lib/python3.9/site-packages (from tensorflow) (0.37.0)\r\nCollecting tensorboard~=2.6\r\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6 MB 58.4 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.5.1-cp39-cp39-manylinux2010_x86_64.whl (454.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454.5 MB 90 kB/s \r\nCollecting tensorflow-estimator<2.6.0,>=2.5.0\r\n  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 462 kB 54.2 MB/s \r\nCollecting tensorflow\r\n  Downloading tensorflow-2.5.0-cp39-cp39-manylinux2010_x86_64.whl (454.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454.4 MB 65 kB/s \r\nINFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\r\nINFO: pip is looking at multiple versions of black to determine which version is compatible with other requirements. This could take a while.\r\nCollecting black\r\n  Downloading black-21.7b0-py3-none-any.whl (141 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 141 kB 45.6 MB/s \r\nCollecting appdirs\r\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\nCollecting typing-extensions~=3.7.4\r\n  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\r\nCollecting numpy~=1.19.2\r\n  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.9 MB 40.7 MB/s \r\nCollecting flatbuffers~=1.12.0\r\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\r\nCollecting h5py~=3.1.0\r\n  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.4 MB 37.6 MB/s \r\nCollecting six~=1.15.0\r\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\r\nCollecting termcolor~=1.1.0\r\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\r\nCollecting opt-einsum~=3.3.0\r\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65 kB 6.1 MB/s \r\nCollecting keras~=2.6\r\n  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3 MB 40.4 MB/s \r\nCollecting gast==0.4.0\r\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\r\nCollecting absl-py~=0.10\r\n  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132 kB 26.6 MB/s \r\nCollecting decorator\r\n  Downloading decorator-5.0.9-py3-none-any.whl (8.9 kB)\r\nCollecting cloudpickle>=1.3\r\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\r\nCollecting dm-tree\r\n  Downloading dm_tree-0.1.6-cp39-cp39-manylinux_2_24_x86_64.whl (94 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 94 kB 4.5 MB/s \r\nCollecting google-auth<2,>=1.6.3\r\n  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 152 kB 45.8 MB/s \r\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\r\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.9 MB 36.4 MB/s \r\nCollecting requests<3,>=2.21.0\r\n  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 62 kB 855 kB/s \r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/venv/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (58.0.2)\r\nCollecting werkzeug>=0.11.15\r\n  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 288 kB 54.4 MB/s \r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 781 kB 55.7 MB/s \r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\nCollecting markdown>=2.6.8\r\n  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 9.9 MB/s \r\nCollecting rsa<5,>=3.1.4\r\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 155 kB 52.0 MB/s \r\nCollecting cachetools<5.0,>=2.0.0\r\n  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nCollecting pyasn1<0.5.0,>=0.4.6\r\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 6.6 MB/s \r\nCollecting charset-normalizer~=2.0.0\r\n  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\r\nCollecting idna<4,>=2.5\r\n  Downloading idna-3.2-py3-none-any.whl (59 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 59 kB 12.8 MB/s \r\nCollecting urllib3<1.27,>=1.21.1\r\n  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 138 kB 54.1 MB/s \r\nCollecting certifi>=2017.4.17\r\n  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 145 kB 38.8 MB/s \r\nCollecting oauthlib>=3.0.0\r\n  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146 kB 50.2 MB/s \r\nBuilding wheels for collected packages: clang, termcolor, wrapt\r\n  Building wheel for clang (setup.py) ... done\r\n  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=90a2386782700fa4c0e5b920347a2421b9ae6ba24b65aeb409d798a9fd084506\r\n  Stored in directory: /root/.cache/pip/wheels/3a/ce/7a/27094f689461801c934296d07078773603663dfcaca63bb064\r\n  Building wheel for termcolor (setup.py) ... done\r\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=800d922b6de838ad1211e5a07fa912cffbc8783c2564bb0f55a31068cffa99c5\r\n  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\r\n  Building wheel for wrapt (setup.py) ... done\r\n  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=73028 sha256=f484ce3455daa6aeb299cda0a61d540b6741d8088e99edc9cf7fb5d49ac8c8cc\r\n  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\r\nSuccessfully built clang termcolor wrapt\r\nInstalling collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, tomli, termcolor, tensorflow-estimator, tensorboard, regex, pathspec, opt-einsum, mypy-extensions, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, dm-tree, decorator, cloudpickle, click, clang, astunparse, appdirs, tensorflow-probability, tensorflow, black\r\nSuccessfully installed absl-py-0.13.0 appdirs-1.4.4 astunparse-1.6.3 black-21.7b0 cachetools-4.2.2 certifi-2021.5.30 charset-normalizer-2.0.4 clang-5.0 click-8.0.1 cloudpickle-1.6.0 decorator-5.0.9 dm-tree-0.1.6 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.39.0 h5py-3.1.0 idna-3.2 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 mypy-extensions-0.4.3 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 pathspec-0.9.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 regex-2021.8.28 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 tensorflow-probability-0.13.0 termcolor-1.1.0 tomli-1.2.1 typing-extensions-3.7.4.3 urllib3-1.26.6 werkzeug-2.0.1 wrapt-1.12.1\r\n(venv) root@320d8517e990:/# pip list | grep \"black\\|tensorflow\"\r\nblack                   21.7b0\r\ntensorflow              2.6.0\r\ntensorflow-estimator    2.6.0\r\ntensorflow-probability  0.13.0\r\n```\r\n\r\n</details>", "An example for a setup where the dependency resolution fails is the combination of `black`, `tensorflow` and `tensorflow-probability`. Using a `python:3.8` Docker environment (otherwise equivalent setup to @matthewfeickert's comment above),\r\n```bash\r\npython -m pip install black tensorflow tensorflow-probability\r\n```\r\ninstalls the following (output of `pip list | grep \"black\\|tensorflow\"`):\r\n```bash\r\nblack                   21.8b0\r\ntensorflow              2.3.4\r\ntensorflow-estimator    2.3.0\r\ntensorflow-probability  0.13.0\r\n```\r\nThe import of `tensorflow-probability` after `tensorflow` then fails:\r\n```bash\r\npython -c \"import tensorflow, tensorflow_probability\"\r\n```\r\nresults in\r\n```\r\nImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.5; Detected an installation of version 2.3.4. Please upgrade TensorFlow to proceed.\r\n```", "\r\n> but that `tensorflow` is pinning down to the patch release here (`typing-extensions~=3.7.4`)\r\n\r\nDoesn't this mean any version at `3.7.4` or after but not after `3.8` or `4.0`? That is, `3.7.5` could still be installed.", "> Doesn't this mean any version at `3.7.4` or after but not after `3.8` or `4.0`? That is, `3.7.5` could still be installed.\r\n\r\nYup, and indeed the compatible release syntax does that (get the latest in the `v3.7.X` series for `X>=4` or more concisely `\"<3.8,>=3.7.4\"`, which for [`typing-extensions` is `3.7.4.3`](https://pypi.org/project/typing-extensions/3.7.4.3/#history))\r\n\r\n```console\r\n$ docker run --rm -ti python:3.9 /bin/bash\r\nroot@b4bbf0636f94:/# python -m venv /usr/local/venv && . /usr/local/venv/bin/activate\r\n(venv) root@b4bbf0636f94:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@b4bbf0636f94:/# python -m pip --quiet install black tensorflow tensorflow-probability\r\n(venv) root@b4bbf0636f94:/# python -m pip list | grep \"pip\\|black\\|tensorflow\\|typing\"\r\nblack                   21.7b0\r\npip                     21.2.4\r\ntensorflow              2.6.0\r\ntensorflow-estimator    2.6.0\r\ntensorflow-probability  0.13.0\r\ntyping-extensions       3.7.4.3\r\n```\r\n\r\nPython 3.8 struggles though\r\n\r\n```console\r\n$ docker run --rm -ti python:3.8 /bin/bash\r\nroot@521504a6d9b2:/# python -m venv /usr/local/venv && . /usr/local/venv/bin/activate\r\n(venv) root@521504a6d9b2:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@521504a6d9b2:/# python -m pip --quiet install black tensorflow tensorflow-probability\r\n(venv) root@521504a6d9b2:/# python -m pip list | grep \"pip\\|black\\|tensorflow\\|typing\"\r\nblack                   21.8b0\r\npip                     21.2.4\r\ntensorflow              2.3.4\r\ntensorflow-estimator    2.3.0\r\ntensorflow-probability  0.13.0\r\ntyping-extensions       3.10.0.2\r\n(venv) root@521504a6d9b2:/# python -c \"import tensorflow as tf; import tensorflow_probability as tfp\"\r\n2021-09-07 22:21:56.457553: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2021-09-07 22:21:56.457576: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/__init__.py\", line 20, in <module>\r\n    from tensorflow_probability import substrates\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/substrates/__init__.py\", line 21, in <module>\r\n    from tensorflow_probability.python.internal import all_util\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py\", line 142, in <module>\r\n    dir(globals()[pkg_name])  # Forces loading the package from its lazy loader.\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 61, in __dir__\r\n    module = self._load()\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 41, in _load\r\n    self._on_first_access()\r\n  File \"/usr/local/venv/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py\", line 63, in _validate_tf_environment\r\n    raise ImportError(\r\nImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.5; Detected an installation of version 2.3.4. Please upgrade TensorFlow to proceed.\r\n```\r\n\r\nand same story for Python 3.7 (which isn't surprising given that it hits Python 3.8)\r\n\r\n```console\r\n$ docker run --rm -ti python:3.7 /bin/bash\r\nroot@94133784a35f:/# python -m venv /usr/local/venv && . /usr/local/venv/bin/activate\r\n(venv) root@94133784a35f:/# python --version --version\r\nPython 3.7.11 (default, Sep  3 2021, 20:46:09) \r\n[GCC 10.2.1 20210110]\r\n(venv) root@94133784a35f:/# python -m pip --quiet install --upgrade pip setuptools wheel\r\n(venv) root@94133784a35f:/# python -m pip --quiet install black tensorflow tensorflow-probability\r\n(venv) root@94133784a35f:/# python -m pip list | grep \"pip\\|black\\|tensorflow\\|typing\"\r\nblack                   21.8b0\r\npip                     21.2.4\r\ntensorflow              2.3.4\r\ntensorflow-estimator    2.3.0\r\ntensorflow-probability  0.13.0\r\ntyping-extensions       3.10.0.2\r\n(venv) root@94133784a35f:/# python -c \"import tensorflow as tf; import tensorflow_probability as tfp\"\r\n2021-09-07 22:53:52.689934: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2021-09-07 22:53:52.690053: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/__init__.py\", line 20, in <module>\r\n    from tensorflow_probability import substrates\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/substrates/__init__.py\", line 21, in <module>\r\n    from tensorflow_probability.python.internal import all_util\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/python/__init__.py\", line 142, in <module>\r\n    dir(globals()[pkg_name])  # Forces loading the package from its lazy loader.\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 61, in __dir__\r\n    module = self._load()\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/python/internal/lazy_loader.py\", line 41, in _load\r\n    self._on_first_access()\r\n  File \"/usr/local/venv/lib/python3.7/site-packages/tensorflow_probability/python/__init__.py\", line 68, in _validate_tf_environment\r\n    present=tf.__version__))\r\nImportError: This version of TensorFlow Probability requires TensorFlow version >= 2.5; Detected an installation of version 2.3.4. Please upgrade TensorFlow to proceed.\r\n```", "This issue makes `pip` unable to install `tensorflow==2.6.0` alongside `astroid==2.8.0`, compare\r\nhttps://github.com/PyCQA/astroid/commit/40ea1a3b8e52bbfed43deb1725cde461f4bd8a96#diff-fa602a8a75dc9dcc92261bac5f533c2a85e34fcceaff63b3a3a81d9acde2fc52,\r\nor `black==21.8b0`, see https://github.com/psf/black/issues/2465.", "I had to temporarily pin an older version of `black` in my `dev-requirements.in` / `dev-requirements.txt` to work around this issue:\r\n\r\n```\r\n# TODO: Unpin black once upstream tensorflow conflicting dependency is resolved:\r\n#    https://github.com/tensorflow/tensorflow/issues/51743\r\nblack == 21.7b0\r\n```", "Hi @mihaimaruseac, I ran into this issue myself so I just submitted a very small PR that I believe should fix it (#52136). Let me know if there are any changes you would like to it! \ud83d\ude42 ", "This seems to be fixed on master by https://github.com/tensorflow/tensorflow/commit/41e66148999080da5b2b80a59740a8f2a59f17da. The `install_requires` is now `typing_extensions >= 3.6.6`.", "Closing as this has been fixed while I was OOO.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51743\">No</a>\n", "For anyone wondering, this did not make it into `tensorflow==2.7.0rc0`.\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.7.0-rc0/tensorflow/tools/pip_package/setup.py#L94", "> For anyone wondering, this did not make it into `tensorflow==2.7.0rc0`.\r\n\r\nThis isn't too surprising to me as I would imagine that freezing the list of things that go into a release would happen several days if not week+ in advance. Releases are hard, especially for projects of this size.\r\n\r\nThis is good to know in advance though, so thank you for noting that!\r\n\r\n", "We need a cherry-pick of 41e6614 into `r2.7` branch. We didn't do it while working on the RC0 release, but I think we should get it for the next RC.", "The commit has been cherry-picked, so the next release candidate should fully resolve this", "RC1 of TF 2.7 should already have this.", "Hi @mihaimaruseac, do you know if this is likely to be picked back into 2.6 at some point? I've just run into this there, and we hadn't planned on moving up to 2.7 just yet.", "Hi. Unfortunately we cannot do a cherry pick of this into TF 2.6 given the magnitude of the change and potential for breakages. Patch releases don't go through the same extensive QA process as final releases, so we're conservative in what changes get included there.", "Ok, yeah that full change is a lot more than I realised initially. Would a PR for just the specific loosening of the `typing_extensions` compatibility against 2.6 be something that would be entertained? ", "That would potentially work", "@mihaimaruseac @woodyza Since there's consideration of including this change in a 2.6 release, I went ahead and rebased my changes on the `r2.6` branch and opened a new PR here:\r\nhttps://github.com/tensorflow/tensorflow/pull/53250\r\n\r\nI would have just reopened the old PR, but I had deleted the fork, so that wasn't possible \ud83d\ude2c\r\n\r\nIf there's a preference to more exactly match [the changes cherry-picked into 2.7](https://github.com/tensorflow/tensorflow/commit/41e66148999080da5b2b80a59740a8f2a59f17da), I can change the version constraint to `typing-extensions >= 3.6.6` \ud83d\ude42 ", "Awesome. Will review and approve and merge a while later when we do patch releases."]}, {"number": 51742, "title": "TransformGraphWithStringInputs is available in tf2.0 however I am planning to jump to tf2.2 or tf2.3 but I couldn't find \"TransformGraphWithStringInputs\" in these 2 versions. Any suggestions ?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@py2ai Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Please refer to the [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/__init__.py) as well and let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51741, "title": "CAST: Not supported Cast case.", "body": "Version: TFLite 2.6.0\r\nDevice: Huawei Mate30\r\n\r\n\r\n08-30 11:00:38.255 28325 28842 I tflite  : Initialized TensorFlow Lite runtime.\r\n08-30 11:00:38.257 28325 28842 E tflite  : Following operations are not supported by GPU delegate:\r\n08-30 11:00:38.257 28325 28842 E tflite  : CAST: Not supported Cast case.\r\n08-30 11:00:38.257 28325 28842 E tflite  : DEQUANTIZE: \r\n08-30 11:00:38.257 28325 28842 E tflite  : 6 operations will run on the GPU, and the remaining 8 operations will run on the CPU.", "comments": ["@hongye007,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) for us to expedite the trouble-shooting process. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51740, "title": "Reshape output tensor in Tensorflow lite.", "body": "\r\n\r\n**System information**\r\n- TensorFlow version (you are using): master branch\r\n- Are you willing to contribute it (Yes/No): YEs\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI'm currently working on this [pull request](https://github.com/tensorflow/tflite-support/pull/612). I want to resize both the input and output tensor of an interpreter. The current API supports only `ResizeInputTensor()` for some reason. Any leads?\r\n\r\n**Will this change the current api? How?**\r\n   This is an internal API, there won't be any visible change.\r\n\r\n**Who will benefit with this feature?**\r\n   \r\n  Those working with core TFLite C++ API.\r\n\r\n**Any Other info.**\r\n", "comments": ["The output tensors will be determined by the input tensors' shapes and the model graph structure by programmatically.\r\n\r\nCould you share why this feature is needed to understand the context of your problem?", "@abattery For image-to-image task, it's desirable to have the output shape being determined by the input image shape. \r\n\r\nFor ex: If input is input dims: `[1, 1, 1, 3]` and output dims: `[1, 1, 1, 16]`. But height and width have `dim_signature` as -1. So, based on the input `(height, width)`, input dim should be `[1, height, width, 3]` and output dim then would be `[1, height, width, 16]`. You can find more information in the PR I've linked.\r\n\r\nCould you please guide me on how to resize output tensor? Thanks.", "Hi @ymodak ,Could you please look into this feature request .", "After invoking with the inputs, you can get the correct output shapes automatically.\r\n\r\nEven after the output shapes are set, the TensorFlow Lite runtime will replace the resize tensor shape with the calculated tensor shape through the model evaluation. In my opinion, this feature request isn't well aligned with the TensorFlow Lite runtime's behavior.", "@abattery Thanks a lot, Invoking() worked! "]}, {"number": 51739, "title": "Add calls to `reserve()` before populating vectors", "body": "\u2026and am at capacity\r\n\r\nPS: WiP. Will finish going through you codebase adding capacity hints to all vectors with obvious opportunity for this optimisation.", "comments": ["Following the huge amount of interest from @sanjoy @kkimdev @sherhut @qqfish and @joker-eph\u2014over the past 5 weeks\u2014I merged in the latest master and reserved vector allocation to another 82 files in the TensorFlow codebase.", "Nice! I had missed this pull-request originally.\r\n\r\nIt is likely that I read the title as some spam somehow though, can you title this explicitly: ```Add calls to `reserve()` before populating vectors``` or something like that.\r\n", "> 123 CAN HAZ REZERVATIONS\r\n\r\nSure things @joker-eph ; and whilst I was at it I finished the cc files in the compiler dir", "@SamuelMarks Can you please fix build failures ? Thanks!", "@gbaned Ahhh it wasn't erring on my end, ok going through the CI logs now", "```\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc: In function 'tensorflow::Status tensorflow::tensorrt::convert::ConvertAddN(tensorflow::tensorrt::convert::OpConverterParams*)':\r\ntensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:6570:31: error: no matching function for call to 'std::vector<tensorflow::tensorrt::ITensorProxyPtr>::reserve(const std::vector<tensorflow::tensorrt::convert::TRT_TensorOrWeights>&)'\r\n   tensor_inputs.reserve(inputs);\r\n                               ^\r\n.....\r\n  no known conversion for argument 1 from 'const std::vector<tensorflow::tensorrt::convert::TRT_TensorOrWeights>' to 'std::vector<tensorflow::tensorrt::ITensorProxyPtr>::size_type {aka long unsigned int}'\r\n```\r\n", "```\r\ntensorflow/compiler/xla/tests/array_elementwise_ops_test.cc: In member function 'virtual void xla::{anonymous}::ArrayElementwiseOpTest_Atan2F32s_Test::TestBody()':\r\ntensorflow/compiler/xla/tests/array_elementwise_ops_test.cc:2361:9: error: 'ys' does not name a type\r\n   const ys = {+0.0f, -0.0f, inf, -inf, 5.0f, -3.0f, 2.0f, -8.0f, 1.0f};\r\n         ^~\r\n```", "@SamuelMarks Can you please resolve conflicts? Thanks!", "@gbaned Not sure if the commit notified you, but I modified within 30 minutes of your message", "Also started to look more at the size types, any .cc file I opened I was able to find a multitude of size inconsistencies (size type in `for` loop not matching size type being compared against). Maybe someone wants to work with me on a LibClang/LibTooling tool to automatically correct these errors across the entire codebase?", "@joker-eph The tests didn't pass so I merged master and built it locally without error. Probably best to merge this quickly, before master deviates too far", "Also this is not clearly unrelated:\r\n```\r\n[ RUN      ] DepthwiseConv2DTest.testDepthwiseConv2DInputGradCompare\r\nINFO:tensorflow:Start test case: DepthwiseConv2DTest.testDepthwiseConv2DInputGradCompare\r\nI1010 21:05:46.342214 139639331483776 xla_test.py:230] Start test case: DepthwiseConv2DTest.testDepthwiseConv2DInputGradCompare\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nFatal Python error: Aborted\r\n```", "Fixing now @joker-eph ", "You still have a lot of `_test.cc` files changed, I'm not sure why is this a good thing though.", "@joker-eph Tests may run faster with this optimisation.\r\n\r\nAnyway the idea with all of this is to cover the entire TensorFlow codebase, using [`.reserve`](https://en.cppreference.com/w/cpp/container/vector/reserve) on [at least] the most obvious of cases. There's no need to skip any C++ file.\r\n\r\nThe relevant tests, are they passing now, or did I miss one [e.g., another typo]?", "> @joker-eph Tests may run faster with this optimisation.\r\n\r\nI am very doubtful on this: do you have any evidence of this?\r\nThis looks to me like a micro optimization in non-hot code.\r\n\r\n\r\n> \r\n> Anyway the idea with all of this is to cover the entire TensorFlow codebase, using [`.reserve`](https://en.cppreference.com/w/cpp/container/vector/reserve) on [at least] the most obvious of cases. There's no need to skip any C++ file.\r\n\r\nThis is not that one sided though: there is a cost in readability / maintenance, when it may not always be an actual win.\r\nAlso for many temporary small vectors there would be much more to gain to use Absl::InlineVector or llvm:SmallVector for example.\r\n\r\n\r\n\r\n", "Also the build looks still broken", "> @joker-eph The tests didn't pass so I merged master and built it locally without error. Probably best to merge this quickly, before master deviates too far\r\n\r\nOne problem here is, that we cannot merge this quickly. Since you cover the whole codebase, it needs approval by several people. If you would split this change into several PRs, where each only covers certain directories, it would make things a lot easier. First, one problem (e.g. merge conflicts or broken tests) wouldn't block other changes which are ok from landing. Second, fewer approvals (hopefully just one) needed to land one PR.", "@joker-eph Yes there is a maintenance cost, but IMHO reserving vector sizes is something everything should do all the time (when they know the size in advance). Similarly in C you should `malloc` the entire size of what you are going to be working on, rather than\u2014`realloc` and\u2014`malloc` continuously.\r\n\r\n(with very few exceptions, e.g., if you're spinning up threads, processes, memory pooling, or some other such scheme to conserve total memory consumption)\r\n\r\n@akuegel Okay I'll do that in my next commit. But this one is basically ready to land (I'm just double-checking the logs and merging master to confirm they're behaving as expected). So would be good to merge ASAP rather than wait another 43+ days to merge my next PR.\r\n\r\nFor reference, my previous PRs on this repo, ordered newest first:\r\n\r\n| Days  | PR         |\r\n| ----- | -------- |\r\n|  43+  | this one \u2020 |\r\n|  56    | #50018 \u2020 |\r\n|  97    | #49974   |\r\n|  51     | #45427  |\r\n|  13     | #45420  |\r\n|  10     | #44679  |\r\n\r\n\u2020 not merged", "The problem is, each time you change something, another round of approvals is necessary. Not all reviewers are in the same timezone. So it takes probably too long to get approvals. By the time the approvals are complete, there is another merge conflict.", "> @joker-eph Yes there is a maintenance cost, but IMHO reserving vector sizes is something everything should do all the time (when they know the size in advance). \r\n\r\nWe'll have to disagree on this: I don't see the point in clobbering unit-tests.\r\n\r\n> Similarly in C you should malloc the entire size of what you are going to be working on, rather than\u2014realloc and\u2014malloc continuously.\r\n\r\nLuckily we don't write C and we use reasonable abstractions which take care of anything like this for us transparently :)\r\n\r\n\r\nAnyway a lot of the wasted time was that many iterations of the code just didn't build in the first place! I triggered a CI run and we'll see if we're there yet.\r\n\r\n\r\n", "> > @joker-eph Yes there is a maintenance cost, but IMHO reserving vector sizes is something everything should do all the time (when they know the size in advance).\r\n> \r\n> We'll have to disagree on this: I don't see the point in clobbering unit-tests.\r\n> \r\n> > Similarly in C you should malloc the entire size of what you are going to be working on, rather than\u2014realloc and\u2014malloc continuously.\r\n> \r\n> Luckily we don't write C and we use reasonable abstractions which take care of anything like this for us transparently :)\r\n\r\nI'll both add my agreement here and suggest that in future PRs tests are left alone. There's no real benefit in modifying them, and new tests are invariably going to be written with \"unoptimized vectors\" - because it is convenient and doesn't hurt.\r\n\r\n", "CI is still failing, you have the links at the bottom, click on \"details\" for the Ubuntu CPU failure:\r\n<img width=\"950\" alt=\"Screen Shot 2021-10-11 at 12 11 43 PM\" src=\"https://user-images.githubusercontent.com/3372300/136844003-46bc0115-5d7a-488c-b87c-872538531c9c.png\">\r\n\r\n", "I really think that some micro-optimizations could be bench-marked to weight their impact on the readability. \r\n\r\nFrom the famous [ISO CPP FAQ](https://isocpp.org/wiki/faq/containers):\r\n> People sometimes worry about the cost of std::vector growing incrementally. Many C++ programmers used to worry about that and used reserve() to optimize the growth. After measuring their code and repeatedly having trouble finding the performance benefits of reserve() in real programs, they stopped using it except where it is needed to avoid iterator invalidation (a rare case in most code). Again: measure before you optimize.\r\n\r\n", "Hmm, could it be my use of [`leaf_count`](https://fossies.org/dox/tensorflow-2.6.0/classxla_1_1ShapeTree.html#ae03e02346b8280c6c9311fb2474db426)?\r\n\r\n---\r\n\r\nC++ is\u2014or can be\u2014considered a low-level language. It does allow for granular memory management. Just because people aren't calling `reserve` on their `std::vector`s doesn't mean they shouldn't.\r\n\r\nTo point out another issue that I've seen: just because using `int` and `size_t` interchangeably doesn't seem to have any measurable difference, doesn't mean you should continue using wrong types [not even to mention the UB!]. And just because incrementally growing vectors versus AOT allocations rarely has a measurable improvement on performance, does not mean that this shouldn't be optimised\u2026 especially when it isn't a one line change but adjusting this across the entire codebase. Sure I could profile, and only apply this change to the hot-paths where it's clearly advantageous.  Imagine if I opened 127 pull-requests for this one trivial change? And make one line commits, that take 50+ days to merge, and 40 days in I get a message saying the file has been changed, update your branch with latest master, and resolve the merge conflicts.\r\n\r\n---\r\n\r\nNow yes, your critique of my typos is completely founded and indefensible. I'm working on making my future commits from a compiler-level (`ast` and `inspect` in Python; LLVM's LibClang and LibTooling in C++). Which will make the commits more mechanical, less error-prone, and impact [even] more of your codebase. Another advantage is in these cases it wouldn't matter if it takes 90+ days for you to merge, because then I can just `git reset --hard HEAD~1 && git pull && <tool apply> && git commit && git push --force` everytime you complain (of a merge conflict).", "> C++ is\u2014or can be\u2014considered a low-level language. It does allow for granular memory management. Just because people aren't calling reserve on their std::vectors doesn't mean they shouldn't.\r\n\r\nSure, I'm supportive of reserving in general. That said I apply this treatment where it makes sense (i.e. in library code and not in unit-tests).\r\n\r\nAs I said before, if you really care about \"low level granular memory management\" we would better be using `llvm::SmallVector` in most places.\r\n\r\n\r\n> To point out another issue that I've seen: just because using int and size_t interchangeably doesn't seem to have any measurable difference, doesn't mean you should continue using wrong types [not even to mention the UB!].\r\n\r\nRight, we should use `int` instead (or ssize_t) most of the time.\r\n\r\n>  Imagine if I opened 127 pull-requests for this one trivial change? And make one line commits, that take 50+ days to merge, \r\n\r\nActually no, it most likely that 127 pull-requests get merges faster. We have a latency issue in the process more than a throughput one and PR merging happens concurrently.", "> Hmm, could it be my use of leaf_count?\r\n\r\nIt is purely syntactic. Compiler says:\r\n```\r\ntensorflow/compiler/xla/shape_tree_test.cc:454:3: error: expected ';' before 'for'\r\n   for (auto index_to_data : t) {\r\n   ^~~\r\n```\r\n\r\nIndeed:\r\n\r\n ```\r\n   v.reserve(t.leaf_count())\r\n   for (auto index_to_data : t) {\r\n```\r\n\r\nThe first line here does not have a `;`.\r\n\r\nThen you have issues with how you get `size()`.\r\n\r\n```\r\ntensorflow/compiler/xla/shape_tree_test.cc:518:20: error: 'class tensorflow::gtl::iterator_range<xla::ShapeTreeLeafIterator<std::vector<xla::internal::ShapeTreeNode<int>, std::allocator<xla::internal::ShapeTreeNode<int> > >, __gnu_cxx::__normal_iterator<xla::internal::ShapeTreeNode<int>*, std::vector<xla::internal::ShapeTreeNode<int>, std::allocator<xla::internal::ShapeTreeNode<int> > > >, std::pair<xla::ShapeIndex, int> > >' has no member named 'size'\r\n   v.reserve(leaves.size())\r\n                    ^~~~\r\ntensorflow/compiler/xla/shape_tree_test.cc: In member function 'virtual void xla::{anonymous}::ShapeTreeTest_ReverseIterateOrderLeaves_Test::TestBody()':\r\ntensorflow/compiler/xla/shape_tree_test.cc:529:15: error: 'class xla::ShapeTree<int>' has no member named 'size'\r\n   v.reserve(t.size());\r\n               ^~~~\r\n```\r\n\r\n\r\nNow again this is test code and I feel we're just wasting everyone time here really.", "I'm sure it's just my n00bness on the bazel side (each build I do takes many many hours). And also I'm not testing on every platform your CI runs on. And the IDE integration with bazel I could get to work (TODO follow-up on my: https://github.com/bazelbuild/intellij/issues/2895). And I'm not filtering the log files from CI effectively. All in all, a whole bunch of excuses that are wholly unwarranted and an understandable annoyance to your team.", "> I'm sure it's just my n00bness on the bazel side (each build I do takes many many hours).\r\n\r\nThis is true just when it is your first time that you compile TF or when your last build was very old (e.g. sparse development) cause currently we don't have a [public read-only pre-filled bazel GCS cache to consume](https://github.com/tensorflow/build/issues/5).\r\n\r\nBut if your problem is about multiple commits->rebases-merges->builds on the same PR  you need to use `disk-cache` arg for a fast build:\r\nhttps://docs.bazel.build/versions/main/remote-caching.html#disk-cache\r\n\r\nEdit:\r\nPlease remember for your host sanity on the long run that `disk-cache` has no limit flag and no automatic garbage collection. See more at:\r\nhttps://github.com/bazelbuild/bazel/issues/5139\r\n\r\n>  And also I'm not testing on every platform your CI runs on. And the IDE integration with bazel I could get to work (TODO follow-up on my: [bazelbuild/intellij#2895](https://github.com/bazelbuild/intellij/issues/2895)). And I'm not filtering the log files from CI effectively. All in all, a whole bunch of excuses that are wholly unwarranted and an understandable annoyance to your team.\r\n\r\nYou can run the tests with command-line just for your host. We have example strings for running tests on a single target or single test so that you could only rerun the specific fix that was failing:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#running-unit-tests\r\n", "There are still compile errors:\r\ntensorflow/compiler/xla/python/tpu_driver:pod_tpu_driver\r\ntensorflow/compiler/xla/service:algebraic_simplifier_test\r\ntensorflow/compiler/xla/service:hlo_module_group_test\r\ntensorflow/compiler/xrt/tests:raw_api_test_lib\r\n\r\nI would suggest that you just revert your changes to those files, and create a new PR that contains these changes but with the compile errors fixed. This would potentially make it possible to land all your other changes (although as I said before, with such a big change, there is a risk of merge conflicts, and latency due to needing again a round of approvals of several people).\r\nIn general, we expect contributors to test that their changes at least compile. Even if it takes a few hours to compile, that doesn't mean you can just try to do changes without testing them and hope they will work. The more changes you do, the more likely there will be *some* typos somewhere.", "> There are still compile errors: tensorflow/compiler/xla/python/tpu_driver:pod_tpu_driver tensorflow/compiler/xla/service:algebraic_simplifier_test tensorflow/compiler/xla/service:hlo_module_group_test tensorflow/compiler/xrt/tests:raw_api_test_lib\r\n> \r\n> I would suggest that you just revert your changes to those files, and create a new PR that contains these changes but with the compile errors fixed. This would potentially make it possible to land all your other changes (although as I said before, with such a big change, there is a risk of merge conflicts, and latency due to needing again a round of approvals of several people). In general, we expect contributors to test that their changes at least compile. Even if it takes a few hours to compile, that doesn't mean you can just try to do changes without testing them and hope they will work. The more changes you do, the more likely there will be _some_ typos somewhere.\r\n\r\nI was trying to give you a good advice to get your working changes landed. You tried instead to fix the non-working changes and land it in the same PR. There are still compile errors, e.g.\r\n\r\ntensorflow/compiler/xla/python/tpu_driver/pod_tpu_driver.cc:418:33: error: variable 'children_ids_size' cannot be implicitly captured in a lambda with no capture-default specified\r\n          child_buffers.reserve(children_ids_size);\r\n\r\n_Comment removed._", "@akuegel I understand your position but we also know that the required build resources are quite unfriendly for the AVG TF contributor as this I well documented as one of the principal causes of the frequent CI abuse roundtrips:\r\nhttps://github.com/tensorflow/tensorflow/pull/48421\r\nhttps://github.com/tensorflow/build/issues/5\r\nhttps://discuss.tensorflow.org/t/llvm-updates-and-bazel-cache/2060\r\n\r\nEven with all the good will and patience the mentioned bazel issue https://github.com/bazelbuild/bazel/issues/5139 doesn't help as you cannot let grow the `disk_cache` forever.", "@bhack Thanks for the suggestions. I'll also see if my aforementioned IDE issue is now resolvable https://github.com/bazelbuild/intellij/issues/2895 so that I'll get nice errors/warnings before sending it off to your build server (and/or waiting a very long time for it to build locally)", "> @bhack Thanks for the suggestions. I'll also see if my aforementioned IDE issue is now resolvable [bazelbuild/intellij#2895](https://github.com/bazelbuild/intellij/issues/2895) so that I'll get nice errors/warnings before sending it off to your build server (and/or waiting a very long time for it to build locally)\r\n\r\n@james-martens I recommend you follow @akuegel and @joker-eph's request to break this PR up, at least \r\nby narrowing down to your working changes. \r\n\r\nOur [contributor guidelines](https://www.tensorflow.org/community/contribute/code) state: \"Maintainers and other contributors will review your PR. Please participate in the conversation, and try to make any requested changes.\" \r\n\r\nI will be closing this PR for now - please reopen when you've reduced the PR scope as requested. \r\n\r\n\r\n", "Thanks for splitting, but I'm not sure where you got the idea that one PR-per-file is the right granularity?\r\n\r\nI handled all of tensorflow/compiler/mlir/... changes though, let me know if I missed any.", "Also, I don't think we should spend time on any test.cc pull-request:\r\n\r\n1) We don't have general agreement that this is a good thing to do in general\r\n2) Going through all this seems like a waste of everyone's time.", "I'm still more of a fan of one big PR\u2014i.e., this one\u2014filled with excruciatingly minor changes, rather than hundreds of one-file changing PRs. But it was commented for above, and this issue was closed, so I didn't have much option left \ud83d\ude15", "> so I didn't have much option left \ud83d\ude15\r\n\r\nThere is a wide spectrum between \"everything in one PR\" and \"one PR per file\": you went from one extreme to the other ; I'm saying there is also the possibility to exercise some reasonable judgement.\r\nFor example it is quite common that a large software has many components, and different people working on these various component. Sharding across these lines helps having the right people reviewing the right PR, without exploding the overhead of so many PR to click through individually.\r\n\r\n(I'm puzzled how you still like the one big PR after suffering through rebase / merge conflicts... the larger the code change the more likely it is to suffer through these).\r\n", "> For example it is quite common that a large software has many components, and different people working on these various component.\n\nThis is true but if we give more care in the maintainership of our codeowners file it could be easier to identifiy 1 PR x component logic:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/CODEOWNERS\n\nCurrently It seems to me quite partial.", "Hi,\n\nI'm not sure why I was added to this thread. Perhaps someone typed the\nwrong name?\n\nJames\n\nOn Fri, Oct 15, 2021 at 12:33 PM bhack ***@***.***> wrote:\n\n> For example it is quite common that a large software has many components,\n> and different people working on these various component.\n>\n> This is true but if we give more care in the maintainership of our\n> codeowners file it could be easier to identifiy 1 PR x component logic:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/CODEOWNERS\n>\n> Currently It seems to me quite partial.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-944226129>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACAXP7MLCO24S7LM2JTGJXLUHAGQDANCNFSM5DAYH53A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "@james-martens I don't see your user name in any comment.", "It was in Thea Lamkin's message from yesterday:\n\n***@***.*** <https://github.com/james-martens> I recommend you follow\n@akuegel <https://github.com/akuegel> and @joker-eph\n<https://github.com/joker-eph>'s request to break this PR up, at least\nby narrowing down to your working changes.\"\n\nOn Fri, Oct 15, 2021 at 1:52 PM bhack ***@***.***> wrote:\n\n> @james-martens <https://github.com/james-martens> I don't see your user\n> name in any comment.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-944272602>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACAXP7NSBUPKA34KQYU62I3UHAPZRANCNFSM5DAYH53A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n", "OK I think it was a typo searching for the alias auto-completion `mark`->`mart` ", "> > For example it is quite common that a large software has many components, and different people working on these various component.\r\n> \r\n> This is true but if we give more care in the maintainership of our codeowners file it could be easier to identifiy 1 PR x component logic:\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/master/CODEOWNERS\r\n> \r\n> Currently It seems to me quite partial.\r\n\r\nThanks @bhack, I wasn't aware of this file. There is another tool we have that auto-assign based on path (for example I get all reviews in tensorflow/compiler/mlir auto-assigned. Seems like we could use this CODEOWNERS files instead!\r\n", "The auto-assignment is via a github bot configured by the GitHub team (cc @gbaned ) https://github.com/tensorflow/tensorflow/blob/master/.github/bot_config.yml", "> Thanks @bhack, I wasn't aware of this file. There is another tool we have that auto-assign based on path (for example I get all reviews in tensorflow/compiler/mlir auto-assigned. Seems like we could use this CODEOWNERS files instead!\r\n\r\nI think that could be useful to maintain a reference github account for every folder. If not in CODEOWNERS as we don't want to notify directly team members in another reference file. \r\n\r\nThis could help up to write in the contribution guide how to segment a code contribution in multiple PR, like this one, for the single team review unit if and when this is possible.", "This was transformed in [116 open PRs](https://github.com/tensorflow/tensorflow/search?q=%22add+calls+to+reserve%22&state=open&type=issues).\r\nI suggest that they could be aggregated by folder as this approach it is also going to spam CI resources.", "Just to make an example:\r\n```\r\ngit fetch origin pull/51739/head:reserve\r\ngit diff --dirstat=files,0 reserve\r\n```\r\n\r\n```\r\n   0.3% tensorflow/c/eager/\r\n   0.3% tensorflow/c/experimental/filesystem/plugins/gcs/\r\n   1.9% tensorflow/c/\r\n   0.7% tensorflow/cc/gradients/\r\n   0.3% tensorflow/cc/ops/\r\n   0.7% tensorflow/cc/saved_model/\r\n   0.3% tensorflow/compiler/aot/\r\n   2.2% tensorflow/compiler/jit/\r\n   1.1% tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/\r\n   0.3% tensorflow/compiler/mlir/hlo/lib/Analysis/\r\n   0.3% tensorflow/compiler/mlir/hlo/\r\n   0.3% tensorflow/compiler/mlir/lite/ir/\r\n   0.3% tensorflow/compiler/mlir/lite/transforms/\r\n   0.3% tensorflow/compiler/mlir/python/\r\n   1.1% tensorflow/compiler/mlir/tensorflow/ir/\r\n   2.2% tensorflow/compiler/mlir/tensorflow/tests/\r\n  13.3% tensorflow/compiler/mlir/tensorflow/transforms/\r\n   0.3% tensorflow/compiler/mlir/tensorflow/utils/\r\n   0.3% tensorflow/compiler/mlir/tensorflow/\r\n   0.3% tensorflow/compiler/mlir/tfr/examples/mnist/\r\n   0.3% tensorflow/compiler/mlir/tfr/integration/\r\n   0.3% tensorflow/compiler/mlir/tfrt/jit/\r\n   0.3% tensorflow/compiler/mlir/tfrt/python_tests/regression_tests/\r\n   0.3% tensorflow/compiler/mlir/tfrt/python_tests/\r\n   0.3% tensorflow/compiler/mlir/tfrt/tests/tf_to_corert/\r\n   0.7% tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data/\r\n   0.3% tensorflow/compiler/mlir/tfrt/transforms/lmhlo_to_gpu/\r\n   0.3% tensorflow/compiler/mlir/tfrt/\r\n   1.5% tensorflow/compiler/mlir/tools/kernel_gen/transforms/\r\n   0.3% tensorflow/compiler/mlir/tools/kernel_gen/\r\n   0.3% tensorflow/compiler/mlir/xla/experimental/conv_emitter/\r\n   0.3% tensorflow/compiler/mlir/xla/ir/\r\n   0.3% tensorflow/compiler/mlir/xla/tests/\r\n   0.7% tensorflow/compiler/mlir/xla/transforms/\r\n   0.3% tensorflow/compiler/mlir/\r\n   0.3% tensorflow/compiler/tests/\r\n   1.1% tensorflow/compiler/tf2tensorrt/convert/\r\n   0.3% tensorflow/compiler/tf2tensorrt/kernels/\r\n   4.1% tensorflow/compiler/tf2xla/kernels/\r\n   0.3% tensorflow/compiler/tf2xla/lib/\r\n   2.2% tensorflow/compiler/tf2xla/\r\n   2.2% tensorflow/compiler/xla/client/lib/\r\n   1.5% tensorflow/compiler/xla/client/\r\n   0.3% tensorflow/compiler/xla/pjrt/\r\n   0.7% tensorflow/compiler/xla/python/tpu_driver/\r\n   1.5% tensorflow/compiler/xla/python/xla_extension/\r\n   1.5% tensorflow/compiler/xla/python/\r\n   1.9% tensorflow/compiler/xla/service/cpu/\r\n   0.7% tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/\r\n   1.1% tensorflow/compiler/xla/service/gpu/tests/\r\n   2.2% tensorflow/compiler/xla/service/gpu/\r\n   0.3% tensorflow/compiler/xla/service/interpreter/\r\n   1.9% tensorflow/compiler/xla/service/spmd/\r\n  12.9% tensorflow/compiler/xla/service/\r\n   4.1% tensorflow/compiler/xla/tests/\r\n   1.9% tensorflow/compiler/xla/\r\n   0.3% tensorflow/compiler/xrt/kernels/\r\n   0.3% tensorflow/compiler/xrt/tests/\r\n   0.3% tensorflow/core/common_runtime/eager/\r\n   0.3% tensorflow/core/common_runtime/\r\n   0.3% tensorflow/core/distributed_runtime/eager/\r\n   0.3% tensorflow/core/framework/\r\n   0.3% tensorflow/core/kernels/\r\n   0.3% tensorflow/core/ops/\r\n   0.7% tensorflow/core/profiler/utils/\r\n   1.1% tensorflow/core/protobuf/\r\n   0.3% tensorflow/core/public/\r\n   1.1% tensorflow/core/runtime_fallback/kernel/\r\n   0.3% tensorflow/core/runtime_fallback/runtime/\r\n   0.3% tensorflow/core/runtime_fallback/\r\n   0.3% tensorflow/go/op/\r\n   0.3% tensorflow/lite/kernels/\r\n   0.3% tensorflow/lite/toco/graph_transformations/\r\n   0.3% tensorflow/python/autograph/impl/\r\n   0.3% tensorflow/python/compat/\r\n   0.3% tensorflow/python/data/ops/\r\n   0.3% tensorflow/python/distribute/integration_test/\r\n   1.1% tensorflow/python/distribute/\r\n   1.5% tensorflow/python/eager/\r\n   1.1% tensorflow/python/framework/\r\n   0.7% tensorflow/python/keras/estimator/\r\n   0.3% tensorflow/python/keras/\r\n   0.3% tensorflow/python/training/\r\n   0.3% tensorflow/python/types/\r\n   0.7% tensorflow/tools/api/golden/v1/\r\n   0.7% tensorflow/tools/api/golden/v2/\r\n   0.3% tensorflow/tools/api/lib/\r\n   0.7% tensorflow/tools/ci_build/\r\n   0.7% tensorflow/\r\n   0.3% third_party/llvm/\r\n   0.3% third_party/tf_runtime/\r\n```\r\n\r\nSo what kind of PR clustering on folders do you suggest?:\r\n\r\n1. `tensorflow/c`\r\n2. `tensorflow/cc`\r\n3. `tensorflow/compiler/jit/`\r\n4. `tensorflow/compiler/mlir`\r\n5. `tensorflow/compiler/xla`\r\n5. ....ETC...", "P.s. now we have 118 PRs", "5 PRs (`compiler/mlir`, `compiler/xla`, `python`, `core`, everything else) would be a good compromise I think", "@mihaimaruseac So I made #52532 through:\r\n```sh\r\ngit checkout master\r\ngit checkout -b 'tensorflow.compiler.xla'\r\ngit branch -a | grep -F ' tensorflow.compiler.xla' | xargs -n 1 git merge\r\ngit push --set-upstream offscale\r\ngh pr create --title '[tensorflow/compiler/xla/**/*.cc] Add calls to `reserve()` before populating vectors' \\\r\n             --body '#51739#issuecomment-945027209 told me to merge into one PR per \"large module/namespace\"'\r\n```\r\n\r\nIs that correct? - If so, I'll do the same for the others.\r\n\r\nPS: I purposefully didn't squash\u2026 do you want me to, or are you happy to just use the GitHub button shortcut?", "Now it think that your changes are better aggregated.", "So what do you want me to do?", "> So what do you want me to do?\n\nThat you run tests for your PR locally as I've already commented at https://github.com/tensorflow/tensorflow/pull/52532#issuecomment-945849096", "Let's keep it now to one PR per file as those have been reviewed already and are in the pipeline. Assuming they build things should progress from here.\r\n\r\nIn the future though, please split per directories, #52532 is still quite large. Also, please run at least a `bazel build` locally with the PR to make sure it builds.", "If we suppose a split per directories as general advice we are going to still generate 91 PRs in cases like this on. No too much different that the 118 PR generated by 1 file x PR ", "I don't think there is an automated way to tell where to split: this is a semantic kind of thing, for example under `tensorflow/compiler/` every single directory could be a separate component, similar under `tensorflow/core`, but not under `tensorflow/c`.\r\nIn general with minimal judgement it isn't too hard to figure out a reasonable grouping.\r\nIt is also likely not the common case to have contributions from people who don't really understand the software's high-level components organization.", "> I don't think there is an automated way to tell where to split: this is a semantic kind of thing, for example under tensorflow/compiler/ every single directory could be a separate component, similar under tensorflow/core, but not under tensorflow/c.\r\n\r\nThat's why in my previous comment I preferred to have a reference fie on github with a github reference team or user account for each component. You could also use a file different from `CODEOWNERS.md` if don't want to be notified with that logic.  \r\n\r\nBut in that way ware are almost aware how the code is organized on your side at a semantic level.", "I think CODEOWNERS is orthogonal. One is for automatic assignment to reviewers, the other is using judgement to split large PRs.", "But also now we partially use the `CODEOWNERS` default github assigment/notification logic but in other cases we use a more complex triaging/notification/assigmnet logic as you have mentioned in https://github.com/tensorflow/tensorflow/pull/51739#issuecomment-944427142.\r\n\r\nWhat I meant here is that as we don't have an unique traditional assignment file, it would be nice to have a file in the repository where we maintain the proxy ownership and segmentation of folders/components with a so large project like Tensorflow as this is also not documented in the official website so we don't have any source on this topic.\r\n\r\nIf you think that then the community will abuse  notification on these Github alias I think that  you could also omit them and just push, in that file, an overview of the folders components semantic.", "So most PRs have been merged. Can you go through the comments on the remaining ones and try to fix them? There are a few with no comments that are going through the pipeline right now but given we've had 100 PRs this resulted in somewhat of a denial of service on CI runners so it will take a while.", "Happy to field test your CI runners =)\r\n\r\n@mihaimaruseac Just double-checked and did a bunch of comments & commits. That should cover all open\u2014and a couple of closed\u2014PRs."]}, {"number": 51738, "title": "Importing layers from keras instead of tensorflow.keras causes erroneous model.summary() behavior", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 21H1\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9.6\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nfrom tensorflow import keras \r\nfrom keras import layers\r\nmodel = keras.Sequential()\r\nmodel.add(layers.Dense(1,input_shape=(10, 1)))\r\nmodel.summary()\r\n```\r\n\r\nThis code results in a \"model has not yet been built\" error, even though `input_shape` is specified in the first layer. I noticed that changing the `from keras import layers` to `from tensorflow.keras import layers` solves the problem. But this seems like a trivial change so I don't see why it solves the problem, the error shouldn't be present even if the import is directly from `keras`", "comments": ["@hdavis472 I tried to run your code on Colab using TF **`v2.6.0`** , **`tf-nightly`** and didn't face the error reported here.Please find the [gist ](https://colab.research.google.com/gist/sushreebarsa/65359c429a4860297b819d79dc1e5638/51738.ipynb)for your reference and let us know if it helps ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51738\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51738\">No</a>\n"]}, {"number": 51737, "title": "Failed to build minimal example of Tensorflow Lite on Qualcomm robotic kit rb5", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Device: Qualcomm Robotic kit rb5 with 16Gb RAM (I connect the board directly to a monitor using HDMI) \r\n- TensorFlow installed from (source or binary): 2.6 build from source using Cmake (Also I tried 2.5)\r\n- TensorFlow version: - \r\n- Python version:  2.7\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source): Cmake 3.20.5\r\n- CUDA/cuDNN version: - \r\n- GPU model and memory: GPU: Qualcomm Adreno 650         DSP: Qualcomm Hexagon 698\r\n\r\nDear All,\r\nI\u2019v followed the instruction in this [link](https://www.tensorflow.org/lite/guide/build_cmake) and successfully build Tensorflow Lite using Cmake. I tried to build the minimal example in this [link](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/minimal), but I get a black screen after the build process is 98% completed. Then the black screen remains permanently. \r\nAnyone has an idea why this happens?\r\n\r\nMy ultimate goal is to link Tensorflow lite as a static library to a ROS(melodic) package and run an inference for Tensorflow Lite models using Hexagon delegate on the Qualcomm rb5 DSP in order to do an object detection task. Anyone has an idea what is the best way to do it?\r\n\r\nThanks\r\nHamoun\r\n", "comments": ["Black screen .Like the whole system freezes and it needs a reset? If this is the case I don't think the problem is from the tensorflow. Have you tried to compile something other than tensorflow with cmake ?\r\n", "Finally, I was able to solve the problem. It wasn't the Cmake problem. The problem was coming from setting the default parallelization values. Before that, I used: `cmake --build . -j` in order to build the TFLite. I changed it to `cmake --build . -j4` and it worked successfully.\r\nBut I still don't know how and why it solved the problem?", "I don't know why the build on your machine was freezing and limiting threads worked. \r\nBut looks like you don't have an issue now. \r\nClosing the issue, feel free to reopen if needed.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51737\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51737\">No</a>\n"]}, {"number": 51736, "title": "Image transformations require SciPy. Install SciPy.", "body": "I am running tensorflow with the help of miniforge3 on MacBook Pro M1.\r\n\r\nI have followed the steps to install tensorflow on M1 from here: https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jul-2021.ipynb\r\n\r\n- TensorFlow version (use command below): tensorflow-mac 2.5.0\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source):\r\n- GPU model and memory: M1 8gb\r\n\r\nImportError                               Traceback (most recent call last)\r\n/var/folders/k9/t0sd91511s16j207y0y2wps40000gn/T/ipykernel_91808/2572958566.py in <module>\r\n      9 import scipy\r\n     10 \r\n---> 11 history = model.fit(\r\n     12     train_generator,\r\n     13     steps_per_epoch = 15,\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1131          training_utils.RespectCompiledTrainableState(self):\r\n   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.\r\n-> 1133       data_handler = data_adapter.get_data_handler(\r\n   1134           x=x,\r\n   1135           y=y,\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)\r\n   1362   if getattr(kwargs[\"model\"], \"_cluster_coordinator\", None):\r\n   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)\r\n-> 1364   return DataHandler(*args, **kwargs)\r\n   1365 \r\n   1366 \r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\r\n   1152     adapter_cls = select_data_adapter(x, y)\r\n   1153     self._verify_data_adapter_compatibility(adapter_cls)\r\n-> 1154     self._adapter = adapter_cls(\r\n   1155         x,\r\n   1156         y,\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    930     self._keras_sequence = x\r\n    931     self._enqueuer = None\r\n--> 932     super(KerasSequenceAdapter, self).__init__(\r\n    933         x,\r\n    934         shuffle=False,  # Shuffle is handed in the _make_callable override.\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\r\n    807     # Since we have to know the dtype of the python generator when we build the\r\n    808     # dataset, we have to look at a batch to infer the structure.\r\n--> 809     peek, x = self._peek_and_restore(x)\r\n    810     peek = self._standardize_batch(peek)\r\n    811     peek = _process_tensorlike(peek)\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py in _peek_and_restore(x)\r\n    941   @staticmethod\r\n    942   def _peek_and_restore(x):\r\n--> 943     return x[0], x\r\n    944 \r\n    945   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py in __getitem__(self, idx)\r\n     63         index_array = self.index_array[self.batch_size * idx:\r\n     64                                        self.batch_size * (idx + 1)]\r\n---> 65         return self._get_batches_of_transformed_samples(index_array)\r\n     66 \r\n     67     def __len__(self):\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)\r\n    236             if self.image_data_generator:\r\n    237                 params = self.image_data_generator.get_random_transform(x.shape)\r\n--> 238                 x = self.image_data_generator.apply_transform(x, params)\r\n    239                 x = self.image_data_generator.standardize(x)\r\n    240             batch_x[i] = x\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/image_data_generator.py in apply_transform(self, x, transform_parameters)\r\n    861         img_channel_axis = self.channel_axis - 1\r\n    862 \r\n--> 863         x = apply_affine_transform(x, transform_parameters.get('theta', 0),\r\n    864                                    transform_parameters.get('tx', 0),\r\n    865                                    transform_parameters.get('ty', 0),\r\n\r\n/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\r\n    279     \"\"\"\r\n    280     if scipy is None:\r\n--> 281         raise ImportError('Image transformations require SciPy. '\r\n    282                           'Install SciPy.')\r\n    283     transform_matrix = None\r\n\r\nImportError: Image transformations require SciPy. Install SciPy.", "comments": ["Hi ! @jackfrost1411 , That is issue related to installation of Scipy.  Please uninstall and install Scipy and check the issue again. Thank you.", "This kind of error on m1 macbooks usually are only solved by reinstalling the tensorflow. Just remove the tensorflow that you have installed and follow the instruction on the apple site ->\r\n\r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nAfter doing all the steps, you also need to install tensorboard also using pip in the same environment.", "Thanks  @jackfrost1411 ! Could you close  the issue then!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "This should not be closed.\r\nIn the beginning of the code there is a try block to ignore the scipy import\r\n\r\n-- keras_preprocessing > image > affine_transformations.py\r\n```\r\ntry:\r\n    import scipy\r\n    # scipy.ndimage cannot be accessed until explicitly imported\r\n    from scipy import ndimage\r\nexcept ImportError:\r\n    scipy = None\r\n```\r\n\r\nBut later in the code, if scipy is not imported, it raises an exception\r\n```\r\n    if scipy is None:\r\n        raise ImportError('Image transformations require SciPy. '\r\n                          'Install SciPy.')\r\n```\r\n\r\nSo, the fact the an exception is raised it means that the scipy is needed, thus it should be a requirement and installed with \"keras_preprocessing\".\r\nOr, if it is not needed, remove the raise exception from the code.\r\n\r\n", "[This link helped me](https://github.com/jasmcaus/opencv-course/pull/26#issuecomment-981225561)\r\n\r\n1. reinstall scipy  \r\n2. shutdown and reopen jupyter server. \r\n\r\nIt worked for me. Thank you"]}, {"number": 51735, "title": "Add missing deps for Legalize to Linalg", "body": "When compiling IREE baremetal I ran into mhlo_passes.h.inc file not found\r\n[ 69%] Built target MhloTypeConversion\r\n[ 69%] Building CXX object third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/legalize_to_linalg.cc.o\r\nIn file included from /home/foo/github/iree-bare-metal-arm/third_party/iree/third_party/mlir-hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc:24:\r\n/home/foo/github/iree-bare-metal-arm/third_party/iree/third_party/mlir-hlo/include/mlir-hlo/Dialect/mhlo/transforms/PassDetail.h:32:10: fatal error: 'mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc' file not found\r\n#include \"mlir-hlo/Dialect/mhlo/transforms/mhlo_passes.h.inc\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\nmake[2]: *** [third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/build.make:63: third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/legalize_to_linalg.cc.o] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:52089: third_party/llvm-project/llvm/tools/mlir-hlo/lib/Dialect/mhlo/transforms/CMakeFiles/obj.MhloLhloToLinalg.dir/all] Error 2\r\nmake: *** [Makefile:152: all] Error 2\r\n\r\n\r\n\r\nFix is similar to https://github.com/tensorflow/tensorflow/pull/51085/\r\n\r\nTEST: Builds after the fix", "comments": []}, {"number": 51734, "title": "Upgrade curl from 7.77 to 7.78", "body": "This PR upgrades curl from 7.77 to 7.78.\r\nThe following are CVEs that might be related to curl 7.77 (old version):\r\n\r\nCVE-2021-22926: CURLOPT_SSLCERT mixup with Secure Transport\r\nCVE-2021-22925: TELNET stack contents disclosure again\r\nCVE-2021-22924: Bad connection reuse due to flawed path name checks\r\nCVE-2021-22923: Metalink download sends credentials\r\nCVE-2021-22922: Wrong content via metalink not discarded\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["#51661 does the same", "Thanks @mihaimaruseac Didn't see the other PR. Let me close this one."]}, {"number": 51733, "title": "Fix tf.math.segment_max/min/mean/sun/prod crashes(aborts) when segment_ids is large", "body": "This PR fixes the issue raised in #46888 where tf.math.segment_max/min/mean/sun/prod crashes(aborts) when segment_ids is large.\r\n    \r\nThis PR fixes #46888.\r\n    \r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 51732, "title": "Fix crash of tf.image.crop_and_resize when input is large number", "body": "This PR is part of the effort in #46890 where\r\ntf.image.crop_and_resize will crash if shape consists of large number.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR.", "The test provided in this PR breaks with the sanitizers:\r\n\r\n```\r\ntensorflow/core/grappler/costs/op_level_cost_estimator.cc:1547:17: runtime error: signed integer overflow: 22719123801 * 1145309325 cannot be represented in type 'long'\r\n    #0 0x562c0294ff1f in tensorflow::grappler::OpLevelCostEstimator::CalculateTensorElementCount(tensorflow::OpInfo_TensorProperties const&, bool*) tensorflow/core/grappler/costs/op_level_cost_estimator.cc:1547:17\r\n    #1 0x562c0294a581 in tensorflow::grappler::OpLevelCostEstimator::PredictCropAndResize(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const tensorflow/core/grappler/costs/op_level_cost_estimator.cc:2643:35\r\n    #2 0x562c0296349e in operator() tensorflow/core/grappler/costs/op_level_cost_estimator.cc:365:14\r\n    #3 0x562c0296349e in __invoke<(lambda at tensorflow/core/grappler/costs/op_level_cost_estimator.cc:364:12) &, const tensorflow::grappler::OpContext &, tensorflow::grappler::NodeCosts *> include/c++/v1/type_traits:3918:1\r\n    #4 0x562c0296349e in __call<(lambda at tensorflow/core/grappler/costs/op_level_cost_estimator.cc:364:12) &, const tensorflow::grappler::OpContext &, tensorflow::grappler::NodeCosts *> include/c++/v1/__functional/invoke.h:30:16\r\n    #5 0x562c0296349e in operator() include/c++/v1/__functional/function.h:221:12\r\n    #6 0x562c0296349e in tensorflow::Status std::__u::__function::__policy_invoker<tensorflow::Status (tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*)>::__call_impl<std::__u::__function::__default_alloc_func<tensorflow::grappler::OpLevelCostEstimator::OpLevelCostEstimator()::$_2::operator()(tensorflow::Status (tensorflow::grappler::OpLevelCostEstimator::*)(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const) const::'lambda'(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*), tensorflow::Status (tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*)> >(std::__u::__function::__policy_storage const*, tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) include/c++/v1/__functional/function.h:702:16\r\n    #7 0x562c0294cd22 in operator() include/c++/v1/__functional/function.h:834:16\r\n    #8 0x562c0294cd22 in operator() include/c++/v1/__functional/function.h:1175:12\r\n    #9 0x562c0294cd22 in tensorflow::grappler::OpLevelCostEstimator::PredictNodeCosts(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const tensorflow/core/grappler/costs/op_level_cost_estimator.cc:702:12\r\n    #10 0x562c0294b58d in tensorflow::grappler::OpLevelCostEstimator::PredictCosts(tensorflow::grappler::OpContext const&) const tensorflow/core/grappler/costs/op_level_cost_estimator.cc:653:7\r\n    #11 0x562c0292c112 in tensorflow::grappler::AnalyticalCostEstimator::PredictCosts(tensorflow::GraphDef const&, tensorflow::RunMetadata*, tensorflow::grappler::Costs*) const tensorflow/core/grappler/costs/analytical_cost_estimator.cc:201:35\r\n    #12 0x562c029277c7 in tensorflow::grappler::VirtualCluster::Run(tensorflow::grappler::GrapplerItem const&, tensorflow::RunMetadata*) tensorflow/core/grappler/clusters/virtual_cluster.cc:88:3\r\n    #13 0x562c027cbdfc in tensorflow::grappler::GraphMemory::InferStatically(std::__u::unordered_map<std::__u::basic_string<char, std::__u::char_traits<char>, std::__u::allocator<char> >, tensorflow::DeviceProperties, std::__u::hash<std::__u::basic_string<char, std::__u::char_traits<char>, std::__u::allocator<char> > >, std::__u::equal_to<std::__u::basic_string<char, std::__u::char_traits<char>, std::__u::allocator<char> > >, std::__u::allocator<std::__u::pair<std::__u::basic_string<char, std::__u::char_traits<char>, std::__u::allocator<char> > const, tensorflow::DeviceProperties> > > const&) tensorflow/core/grappler/costs/graph_memory.cc:40:22\r\n    #14 0x562c0279dcc3 in IdentifySwappingCandidates tensorflow/core/grappler/optimizers/memory_optimizer.cc:987:31\r\n```\r\n\r\ncan you take a look?", "Thanks @joker-eph for the comment. In the test, the large number was used on purpose so that tensorflow can trigger an error gracefully (as was shown in the test setup):\r\nhttps://github.com/tensorflow/tensorflow/blob/c8d87055a56d8740d27ad8bdc74a7459ede6900e/tensorflow/python/ops/image_ops_test.py#L6080\r\n\r\nOriginally, the above test will cause tensorflow to crash with undefined behavior, which may potentially be a security issue.\r\n\r\nSo the test itself is expected.\r\n\r\nFrom the sanitizer error in logs:\r\n```\r\ntensorflow/core/grappler/costs/op_level_cost_estimator.cc:1547:17: runtime error: signed integer overflow: 22719123801 * 1145309325 cannot be represented in type 'long'\r\n    #0 0x562c0294ff1f in tensorflow::grappler::OpLevelCostEstimator::CalculateTensorElementCount(tensorflow::OpInfo_TensorProperties const&, bool*) tensorflow/core/grappler/costs/op_level_cost_estimator.cc:1547:17\r\n    #1 0x562c0294a581 in tensorflow::grappler::OpLevelCostEstimator::PredictCropAndResize(tensorflow::grappler::OpContext const&, tensorflow::grappler::NodeCosts*) const tensorflow/core/grappler/costs/op_level_cost_estimator.cc:2643:35\r\n    #2 0x562c0296349e in operator() tensorflow/core/grappler/costs/op_level_cost_estimator.cc:365:14\r\n```\r\n\r\nI think the issue is actually coming from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/op_level_cost_estimator.cc#L1547\r\n\r\nwhich should result in an status error (when multiply overflows) and be populated to `Status OpLevelCostEstimator::PredictCropAndResize` instead:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/op_level_cost_estimator.cc#L2621\r\n\r\nLet me see if I can create a PR to address the issue.", "I think there is a more fundamental question: should we allow this in the first place in the graph? What is the use case?\r\nI'm worried of chasing this everywhere in the codebase instead of forbidding such shape from being created entirely for example.", "For example, the fix implemented in this PR in crop_and_resize_op.cc using AddDimWithStatus seems in line with this: `AddDimWithStatus` exists because it seems invalid to create a shape when we can't compute the number of elements.", "@joker-eph Since `crop_size` is passed as input (not attr) of the node in the graph, it may only be checked when graph runs in case `crop_size` itself is a non-constant tensor I think.\r\n\r\nOn the other hand, it might be possible to do some preliminary check so that in case crop_size is a constant, then the graph can return an error before it runs. \r\n\r\nIn tensorflow 1.x, the shape is pre-checked in the following (before graph runs):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/image_ops.cc#L879-L908\r\nso it will be possible to do some preliminary check in case `crop_size` itself is a constant tensor.\r\n\r\nHowever, in tensorflow 2.x's eager mode, the path to do the shape check (and resolve and constant tensor) before graph runs does not go through the above path any more. As a result the above function will not be triggered in eager mode anymore. As such a potential shape check is not possible in 2.x.", "I meant that we could forbid at the NodeDef creation to create such shapes, so that it would never appear in the Graph. I'm not sure about all the implication, I just doubt the stack is robust on this."]}, {"number": 51731, "title": "TF RNN subclass model called twice although sequence length is one", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 / 8\r\n- GPU model and memory: RTX 2070 8 GB\r\n\r\n**Describe the current behavior**\r\nI am building a custom RNN layer using tensorflow's model subclass documentation. My input is a 1D vector while the RNN states are also fed into the model at each timestep i.e. my input is generated at each timestep, the RNN's state is obtained from the previous timestep and I need the model output at each timestep. The issue I am facing is that when a single input is given each timestep, the RNN's call function is called twice instead of just once i.e. print('--------- Loop --------') is displayed twice instead of just once when the input is only given once. This becomes a bigger issue as runtime is doubled when I am running a bigger network for a reinforcement learning problem where weights are updated at each timestep instead of the Monte Carlo method.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour would be that the call function is only called once at each timestep so that print('--------- Loop --------') runs once at each timestep.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`\r\n\r\n    class MinimalRNNCell(tf.keras.layers.Layer):\r\n        def __init__(self, units=100, **kwargs):\r\n            self.units = units\r\n            self.state_size = units\r\n            super(MinimalRNNCell, self).__init__(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\r\n                                      initializer='uniform',\r\n                                      name='kernel')\r\n            self.recurrent_kernel = self.add_weight(\r\n            shape=(self.units, self.units),\r\n            initializer='uniform',\r\n            name='recurrent_kernel')\r\n            self.built = True\r\n\r\n        def call(self, inputs, states):\r\n            prev_output = states[0]\r\n            h = tf.matmul(inputs, self.kernel)\r\n            output = h + tf.matmul(prev_output, self.recurrent_kernel)\r\n            print('--------- Loop --------')\r\n            return output, [output]\r\n\r\n    rnncell = MinimalRNNCell()\r\n    rnn = tf.keras.layers.RNN(rnncell, return_state=True, return_sequences=False, time_major=False, stateful=False)\r\n\r\n    x = tf.random.normal(shape=[1,67],stddev=1)\r\n    state = tf.random.normal(shape=[1,100],stddev=1)\r\n    for t in range(3):\r\n        r, state = rnn(x[None,:], state)\r\n        print('{} timestep done'.format(t+1))\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRun log: \r\n--------- Loop --------\r\n--------- Loop --------\r\n1 timestep done\r\n--------- Loop --------\r\n--------- Loop --------\r\n2 timestep done\r\n--------- Loop --------\r\n--------- Loop --------\r\n3 timestep done\r\n", "comments": ["@mgkumar138 Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "Will do thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51731\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51731\">No</a>\n"]}, {"number": 51730, "title": "Failed to get compute capability major for device: UNKNOWN ERROR (1); 0", "body": "Dear All,\r\n\r\nI have installed tensorflow-cpu 2.6.0 on my anaconda. Both Tensorflow and Keras will be imported correctly, but running a cell containing them raises error:\r\n\r\n> RuntimeError Traceback (most recent call last) in 1 from tensorflow.python.client import device_lib ----> 2 print(device_lib.list_local_devices())\r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\device_lib.py in list_local_devices(session_config) 43 serialized_config = session_config.SerializeToString() 44 return [ ---> 45 _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config) 46 ]\r\n> \r\n> RuntimeError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0\r\n\r\n**System information**\r\n- Python 3.8.5 (Anaconda)\r\n- Tensorflow 2.6.0\r\n- keras 2.6.0\r\n- Windows 7 64\r\n\r\nThank alot\r\nSilvio", "comments": ["@Silvio-Ma,\r\n\r\n> Both Tensorflow and Keras will be imported correctly, but running a cell containing them raises error:\r\n\r\nDo you mean the import lines are throwing you this error, or after importing when you try running some other code, its throwing you this error?\r\n\r\nCan you share a standalone code snippet to reproduce this issue, so that we can expedite the trouble shooting process?Thanks!\r\n", "Thanks for your consideration Sanatmpa, I'm really stuck.\r\nNo, the import lines return no error, they work fine:\r\n\r\n```\r\nimport numpy as np\r\nimport keras\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense,  Activation\r\nfrom keras.utils import np_utils\r\n```\r\n\r\nThis will run properly. And also when I run this cell:\r\n\r\n`!pip show tensorflow\r\n`\r\nThe result is:\r\n\r\n> Name: tensorflow\r\n> Version: 2.6.0\r\n> Summary: TensorFlow is an open source machine learning framework for everyone.\r\n> Home-page: https://www.tensorflow.org/\r\n> Author: Google Inc.\r\n> Author-email: packages@tensorflow.org\r\n> License: Apache 2.0\r\n> Location: d:\\programs\\anaconda\\lib\\site-packages\r\n> Requires: absl-py, flatbuffers, keras, termcolor, keras-preprocessing, h5py, wrapt, google-pasta, gast, protobuf, astunparse, typing-extensions, six, tensorboard, opt-einsum, wheel, numpy, tensorflow-estimator, grpcio, clang\r\n> Required-by: \r\n\r\nNow when I run this cell:\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\nThis error will raise:\r\n\r\n> ---------------------------------------------------------------------------\r\n> RuntimeError                              Traceback (most recent call last)\r\n> <ipython-input-25-0ca82b29604d> in <module>\r\n>       1 from tensorflow.python.client import device_lib\r\n> ----> 2 print(device_lib.list_local_devices())\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\device_lib.py in list_local_devices(session_config)\r\n>      43     serialized_config = session_config.SerializeToString()\r\n>      44   return [\r\n> ---> 45       _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\n>      46   ]\r\n> \r\n> RuntimeError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0\r\n\r\n\r\nOr when I run this cell:\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(Dense(64, activation='relu', input_dim=25))\r\nmodel.add(Dense(10, activation='softmax'))\r\n```\r\n\r\nThis will be the result, sorry, it's too long:\r\n\r\n> InternalError                             Traceback (most recent call last)\r\n> <ipython-input-3-e1250b93a859> in <module>\r\n> ----> 1 model = Sequential()\r\n>       2 model.add(Dense(64, activation='relu', input_dim=25))\r\n>       3 model.add(Dense(10, activation='softmax'))\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n>     528     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n>     529     try:\r\n> --> 530       result = method(self, *args, **kwargs)\r\n>     531     finally:\r\n>     532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\sequential.py in __init__(self, layers, name)\r\n>     105     \"\"\"\r\n>     106     # Skip the init in FunctionalModel since model doesn't have input/output yet\r\n> --> 107     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n>     108         name=name, autocast=False)\r\n>     109     base_layer.keras_api_gauge.get_cell('Sequential').set(True)\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n>     528     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n>     529     try:\r\n> --> 530       result = method(self, *args, **kwargs)\r\n>     531     finally:\r\n>     532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py in __init__(self, *args, **kwargs)\r\n>     287     self._steps_per_execution = None\r\n>     288 \r\n> --> 289     self._init_batch_counters()\r\n>     290     self._base_model_initialized = True\r\n>     291 \r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py in _method_wrapper(self, *args, **kwargs)\r\n>     528     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n>     529     try:\r\n> --> 530       result = method(self, *args, **kwargs)\r\n>     531     finally:\r\n>     532       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py in _init_batch_counters(self)\r\n>     295     # `evaluate`, and `predict`.\r\n>     296     agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\r\n> --> 297     self._train_counter = tf.Variable(0, dtype='int64', aggregation=agg)\r\n>     298     self._test_counter = tf.Variable(0, dtype='int64', aggregation=agg)\r\n>     299     self._predict_counter = tf.Variable(\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n>     266       return cls._variable_v1_call(*args, **kwargs)\r\n>     267     elif cls is Variable:\r\n> --> 268       return cls._variable_v2_call(*args, **kwargs)\r\n>     269     else:\r\n>     270       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\r\n>     248     if aggregation is None:\r\n>     249       aggregation = VariableAggregation.NONE\r\n> --> 250     return previous_getter(\r\n>     251         initial_value=initial_value,\r\n>     252         trainable=trainable,\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in <lambda>(**kws)\r\n>     241                         shape=None):\r\n>     242     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n> --> 243     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n>     244     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n>     245       previous_getter = _make_getter(getter, previous_getter)\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)\r\n>    2660   shape = kwargs.get(\"shape\", None)\r\n>    2661 \r\n> -> 2662   return resource_variable_ops.ResourceVariable(\r\n>    2663       initial_value=initial_value,\r\n>    2664       trainable=trainable,\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py in __call__(cls, *args, **kwargs)\r\n>     268       return cls._variable_v2_call(*args, **kwargs)\r\n>     269     else:\r\n> --> 270       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n>     271 \r\n>     272 \r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n>    1600       self._init_from_proto(variable_def, import_scope=import_scope)\r\n>    1601     else:\r\n> -> 1602       self._init_from_args(\r\n>    1603           initial_value=initial_value,\r\n>    1604           trainable=trainable,\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n>    1743               self._update_uid = initial_value.checkpoint_position.restore_uid\r\n>    1744               initial_value = initial_value.wrapped_value\r\n> -> 1745             initial_value = ops.convert_to_tensor(initial_value,\r\n>    1746                                                   name=\"initial_value\",\r\n>    1747                                                   dtype=dtype)\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py in wrapped(*args, **kwargs)\r\n>     161         with Trace(trace_name, **trace_kwargs):\r\n>     162           return func(*args, **kwargs)\r\n> --> 163       return func(*args, **kwargs)\r\n>     164 \r\n>     165     return wrapped\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n>    1564 \r\n>    1565     if ret is None:\r\n> -> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n>    1567 \r\n>    1568     if ret is NotImplemented:\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)\r\n>      50 def _default_conversion_function(value, dtype, name, as_ref):\r\n>      51   del as_ref  # Unused.\r\n> ---> 52   return constant_op.constant(value, dtype, name=name)\r\n>      53 \r\n>      54 \r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n>     269     ValueError: if called on a symbolic tensor.\r\n>     270   \"\"\"\r\n> --> 271   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n>     272                         allow_broadcast=True)\r\n>     273 \r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n>     281       with trace.Trace(\"tf.constant\"):\r\n>     282         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n> --> 283     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n>     284 \r\n>     285   g = ops.get_default_graph()\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n>     306 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):\r\n>     307   \"\"\"Creates a constant on the current device.\"\"\"\r\n> --> 308   t = convert_to_eager_tensor(value, ctx, dtype)\r\n>     309   if shape is None:\r\n>     310     return t\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n>     103     except AttributeError:\r\n>     104       dtype = dtypes.as_dtype(dtype).as_datatype_enum\r\n> --> 105   ctx.ensure_initialized()\r\n>     106   return ops.EagerTensor(value, ctx.device_name, dtype)\r\n>     107 \r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in ensure_initialized(self)\r\n>     534       opts = pywrap_tfe.TFE_NewContextOptions()\r\n>     535       try:\r\n> --> 536         config_str = self.config.SerializeToString()\r\n>     537         pywrap_tfe.TFE_ContextOptionsSetConfig(opts, config_str)\r\n>     538         if self._device_policy is not None:\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in config(self)\r\n>     962     \"\"\"Return the ConfigProto with all runtime deltas applied.\"\"\"\r\n>     963     # Ensure physical devices have been discovered and config has been imported\r\n> --> 964     self._initialize_physical_devices()\r\n>     965 \r\n>     966     config = config_pb2.ConfigProto()\r\n> \r\n> D:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in _initialize_physical_devices(self, reinitialize)\r\n>    1291         return\r\n>    1292 \r\n> -> 1293       devs = pywrap_tfe.TF_ListPhysicalDevices()\r\n>    1294       self._physical_devices = [\r\n>    1295           PhysicalDevice(name=d.decode(), device_type=d.decode().split(\":\")[1])\r\n> \r\n> InternalError: failed to get compute capability major for device: UNKNOWN ERROR (1); 0\r\n", "@Silvio-Ma,\r\n\r\nCan you let us know how you have installed tensorflow 2.6.0? Have you tried installation with PIP or built it from source?\r\n\r\nAlso do you have MSVC2019 installed as per the tested build configuration [link](https://www.tensorflow.org/install/source_windows#cpu)?", "I tried conda install, it did not work. pip install just worked for me.\r\nI'm quite a novice in this field, and in installing instructions I've seen nothing about Bazel or MSVC2019 as noted in your link. But if you mean microsoft visual c++ 2019, yes I have installed 2015-2019 redistributable.", "@Silvio-Ma,\r\n\r\nHave you tried following this [guide](https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow_cpu-2.6.0-cp38-cp38-win_amd64.whl) to install the tensorflow for windows? If not, Can you try creating a new venv and install as per the guide and let us know if it helps? Thanks!", "@sanatmpa1 \r\n\r\nMay you send the guide again, the link you have sent above is pointing to a wheel file.\r\nThanks", "@Silvio-Ma,\r\n\r\nCan you checkout this [guide](https://www.tensorflow.org/install/source_windows) for TF installation in windows. Thanks!", "Solved\r\nI tried too many ways for days, this is what finally worked for me:\r\n\r\n1- Run cmd as administrator\r\n\r\n2- Uninstall tensorflow package\r\n\r\n3- Delete unnecessary files, this will save lots of space:\r\n`conda clean --tarballs -packages`\r\n \r\n4- Create a new Virtual Environment: \r\n`conda create --name env-tf python=3.8.5 `\r\n\r\n5- Download tensorflow whl file, close ALL apps (including browsers), activate the new environment in cmd:\r\n`conda activate env-tf`\r\n\r\n6- Install the downloaded tensorflow wheel file:\r\n`pip install f:/ tensorflow_cpu-2.6.0-cp38-cp38-win_amd64.whl`\r\n\r\n7- Check if it has been installed correctly:\r\n`python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\n8- Install jupyter in the new environment:\r\n```\r\npip install jupyter\r\npip install jupyter lab\r\n```\r\n\r\n9- Install python kernel:\r\n`conda install ipykernel`\r\n\r\nIt's done!", "@Silvio-Ma,\r\n\r\nThanks for the detailed steps that worked for you, as it may help someone else in the future to try out. As your problem is resolved, can you move this issue to closed status? Thanks!", "@sanatmpa1 \r\nThanks for your help Sanat, and sure, I will do it now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51730\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51730\">No</a>\n"]}, {"number": 51729, "title": "Unable to generate train.record, but for text.record it works fine", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): nope\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): I used the guide at https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: Python 3.8.8\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): nope \r\n- CUDA/cuDNN version: nope\r\n- GPU model and memory: nope\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior** When following the guide command to change format from xml to record, it doesn't work for train but works for text\r\n\r\n**Describe the expected behavior**\r\n\r\nit should output something like: Successfully created the TFRecord file: /home/jrhin/Tensorflow/workspace/training_demo/annotations/train.record\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\npython generate_tfrecord.py -x ~/Tensorflow/workspace/training_demo/images/train -l ~/Tensorflow/workspace/training_demo/annotations/label_map.pbtxt -o ~/Tensorflow/workspace/training_demo/annotations/train.record\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"generate_tfrecord.py\", line 172, in <module>\r\n    tf.app.run()\r\n  File \"/home/jrhin/anaconda3/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/home/jrhin/anaconda3/lib/python3.8/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"generate_tfrecord.py\", line 162, in main\r\n    tf_example = create_tf_example(group, path)\r\n  File \"generate_tfrecord.py\", line 136, in create_tf_example\r\n    classes.append(class_text_to_int(row['class']))\r\n  File \"generate_tfrecord.py\", line 105, in class_text_to_int\r\n    return label_map_dict[row_label]\r\nKeyError: 'w'\r\n", "comments": ["Ok, I've find out that you have to first divide the images into train and test folder. Then you can add labeling to them. After that TF records are generated successfully!\r\nBut the guide tells you to first add label and then divide them into train and test folders!!! https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51729\">No</a>\n"]}, {"number": 51727, "title": "Unable to save custom model using save_model, even though documentation states it is possible", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThrows error\r\n\r\nUnable to save function b'__inference_paraphrase_detector_1_layer_call_fn_2757942' because it captures graph tensor Tensor(\"dense_83/BiasAdd_3:0\", shape=(16, 32), dtype=float32) from a parent function which cannot be converted to a constant with `tf.get_static_value`.\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nColab: https://colab.research.google.com/drive/11VGeylggSZ-0yb9CJmg3fE79CiRAvERX?usp=sharing\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nNo need for the database, even if you simply build the model with no training and try to save it, it throws the same error. ", "comments": ["@microcoder-py,\r\n\r\nCan you provide the sample data if possible, to reproduce the issue from our end? Thanks!", "HI, unable to attach file because GitHub doesn't support tsv files. However, I tried to save the weights and that happened just fine. I think the error is coming in because i have declared python lists inside my layers, and those python lists are inaccessible to the tf computation graph", "@microcoder-py,\r\n\r\nIf you feel that the issue is already fixed, please feel free to move the issue to closed status. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51727\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51727\">No</a>\n", "Had a similar error (`ValueError: Unable to save function b'__inference_my_model_layer_call_fn_85894' because it captures graph tensor Tensor(\"add:0\", shape=(), dtype=int32) from a parent function which cannot be converted to a constant with 'tf.get_static_value'.`) with tf=2.7. \r\n\r\nIn my case it could be solved by using `enable_eager_execution()` instead of `disable_eager_execution()` in the export script"]}, {"number": 51726, "title": "Several links to the github code are broken in https://www.tensorflow.org/", "body": "As the title says, some links from https://www.tensorflow.org/ to the corresponding code on github are broken.\r\n[here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation) an example. \r\nI believe this is due to the removal of the `python` module.\r\n\r\nAlso, some other links have a line offset, such as [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D).\r\nThis is probably because some imports were removed. ", "comments": ["Hi @Saduf2019 ,could you please look into this  issue?", "@NEGU93 \r\nI have opened the links shared and all of them work as expected, can you please share a screen shot of the issue reported.", "Steps to reproduce:\r\n1. Go [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)\r\n2. Click on the `View source on GitHub` button.\r\n3. The button will take you to [this](https://github.com/keras-team/keras/tree/master/keras/layers/core.py#L389-L430) broken link.", "@NEGU93 \r\nA CL is created to resolve this, thank you for reporting.", "Your welcome! \r\nIt was not the only example I found, there are a few and for sure many more I did not found yet.\r\n\r\nI think since version 2.6 or 2.5 the `tf.keras.python` module was removed (and items moved to another place) that is my theory and it may help to track the broken links.", "@NEGU93 \r\nPlease move this to closed status as the issue has been resolved,and feel free to share any further issues [broken links]"]}, {"number": 51725, "title": "How can I increase  throughput for multi sessions/models on single GPU of online inference\uff1f", "body": "Environment:\r\nTF Version: 1.14.0\r\nCuda Version: 10.0\r\nGPU Device: NVIDIA-T4\r\n\r\nDear all,\r\n     When I got multi models (4 models for example) online inference on single GPU, It turned out that the overall throughput can be increased (despite of batching) and got very low GPU utilization. I invested on this issue and found it occurs because tensorflow use only one compute stream for a physical gpu device according to https://stackoverflow.com/questions/55907275/can-multiple-tensorflow-inferences-run-on-one-gpu-in-parallel. Do you have any solutions for this issue? \r\n\r\nThanks a lot,\r\nAsher\r\n\r\n", "comments": ["@AsherXian903 We see that you are using older version of tensorflow **`1.14`** which is not actively supported, We recommend that you upgrade to latest stable of TF  version **`2.6.0`** and let us know if the issue still persists in newer versions.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51724, "title": "GPU not found when using TF 2.5.0 in Google Colab", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.11\r\n- CUDA/cuDNN version: V11.0.221\r\n- GPU model and memory: Tesla K80\r\n<br>\r\n\r\n**Describe the current behavior**\r\n- I installed `tensorflow 2.5.0` in colab\r\n- After installation, I imported `tensorflow` and checked available GPU device\r\n- However, `tf.config.experimental.list_physical_devices('GPU')` returns an empty list.\r\n- I already checked GPU deivce with `nvidia-smi`\r\n<br>\r\n\r\n<ins>Is there any problem the way I ran below code?</ins>\r\n<br>\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n!pip install tensorflow==2.5.0\r\nimport tensorflow as tf\r\n!nvidia-smi\r\nprint(f\"tf ({tf.__version__})\")\r\nprint(f\"gpus : {tf.config.experimental.list_physical_devices('GPU')}\")\r\n```", "comments": ["@sam351 \r\nIt is working as expected now, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6e1e708ca606ef8753522e65f9d80567/2_5.ipynb),and [2.5](https://colab.research.google.com/gist/Saduf2019/f63a8fc68e6499862e4cac97bf2a052b/untitled632.ipynb). Please move this to closed status as this is resolved and this repo is only for performance related issues and bugs for tensorflow.", "@Saduf2019 Still not working here... I am able to see a gpu when I put nvidia-smi but when I try to list GPU's using tf.config it returns an empty list... This update to tensorflow 2.6 broke so many things in colab, like saving models using tf.save it's a headache now", "@Saduf2019 I moved to the link(gist) that you shared, and ran it in colab. However, it's not working as expected, unlike the saved outputs of the notebook that you've shared. Even in your notebook, the first cell doesn't show any available gpu (that is, it shows an empty list)\r\nBy the way, if this repo is not for asking this kind of question, where should I ask? I'm not familiar with the conventions about repos, so hope your help to know how to ask appropriate questions in appropriate place.\r\n\r\nThank you in advance :)", "@sam351 @DenysMenfredy \r\nI ran the code again and it working as expected.\r\nCan you please create an issue in the colab repo in case it still persist. @sam351 if an issue is not related performance or a tf bug/feature request, you may open it in the tf [discussion forum](https://discuss.tensorflow.org/) as it has a large community to support and respond.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51724\">No</a>\n"]}, {"number": 51723, "title": "Build stuck at Linking _pywrap_tensorflow_internal.so step", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): From source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.9.2\r\n- Installed using virtualenv? pip? conda?: Bazel\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): VS 2019\r\n\r\n**Describe the problem**\r\n\r\nThe build gets stuck at \"Linking tensorflow/python/ _pywrap_tensorflow_internal.so\" step. It takes more than 5000s and never completes the build\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r2.6\r\npython ./configure.py\r\nbazel --output_user_root=C:\\tfb build --jobs=3 --config=opt --define=no_tensorflow_py_deps=true /tensorflow/tools/pip_package:build_pip_package\r\n```\r\nIn the configuration , I have chosen all the default options\r\n\r\n**Any other info / logs**\r\n![2021-08-27 21_22_53-Window](https://user-images.githubusercontent.com/64022998/131206786-a30fa5fc-213c-4b0c-b9b4-1fa40ccfcdb2.png)\r\n\r\n", "comments": ["Hi @akkshay0107! you have mentioned OS you are using Windows 10 , but installation procedure is different. Could you confirm you are following below procedure for [windows ](https://www.tensorflow.org/install/source_windows) , for Ubuntu the  steps can be found [here](https://www.tensorflow.org/install/source),", "I have followed all the steps given in https://www.tensorflow.org/install/source_windows . Due to RAM being less in my system I had to reduce the number of jobs and because my user folder contains a space I had to set the output root to a folder in C:\\ drive.", "Hi @sanatmpa1 ,Could you please look into this issue!", "@akkshay0107,\r\n\r\nYou can take a look at this issue #27471 for reference, though in your case it haven't failed and just took longer time.Can you try modifying the configuration to disable `XLA JIT Support` if its enabled, and build again to see if its working fine?\r\n\r\nAlso check if the graphics card in your machine supports the corresponding cuda version as per this [link](https://www.tensorflow.org/install/source_windows#gpu)\r\n", "In ``configure.py`` there is no option to disable XLA JIT, do i have to disable it in the .bazelrc file manually? Also I dont have a GPU in my computer and have inputted \"N\" to this line in ``configure.py`` and so i dont use cuda / cudNN\r\n```\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\n```\r\n", "@sanatmpa1 the build has worked after disabling XLA JIT but after that there is an installation error \r\n```\r\nC:\\tensorflow>bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package C:/tmp/tensorflow_pkg\r\nWed Sep 1 20:40:33 IST 2021 : === Preparing sources in dir: /tmp/tmp.Y95WQ31j0L\r\nUnzipping simple_console_for_windows.zip to create runfiles tree...\r\nc:\\tensorflow\\bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package: line 132: unzip: command not found\r\n```\r\nis there any way to fix this ?", "@akkshay0107,\r\n\r\nCan you update on what's the output you're getting for the commands `which unzip` and `unzip`?", "@sanatmpa1 \r\n\r\ni get that unzip is not recognized as an internal or external command\r\n![2021-09-08 22_02_49-](https://user-images.githubusercontent.com/64022998/132549146-e4fb4cb1-1107-4714-897d-55f1e3633cea.png)\r\n", "@akkshay0107,\r\n\r\n`unzip` is not a normal program in Windows. It won't work directly when using CMD as your shell. So you should install command line archiving programs like [unzip](http://gnuwin32.sourceforge.net/packages/unzip.htm) and add corresponding path to your environments. This is not an issue with TF and its a common windows error. Please try the same and check if its working. Thanks!", "It has started working. Thanks a lot !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51723\">No</a>\n"]}, {"number": 51722, "title": "[Intel MKL] Fuse BiasAdd and Gelu with MatMul", "body": "This PR is dependent on #51808.\r\n\r\nThis PR is a fusion of MatMul + BiasAdd + {set of ops from `tf.nn.gelu` python api}. It has been enabled with oneDNN CPU backend only with grappler remapper graph optimizer.\r\n\r\nSince the set of actual ops for `tf.nn.gelu` python api is heavily dependent on various graph optimizers in the grappler that runs before remapper, a python unit test (`tensorflow/python/grappler/remapper_test.py`) has been added.\r\n", "comments": ["@ezhulenev I have fixed some build issue. Could you please re-approve it."]}, {"number": 51721, "title": "NameError: name 'trainer' is not defined", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2+\r\n- Python version: 3.9.2\r\n- Installed using virtualenv? pip? conda?: python\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to get python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config to build but it gives me the error \"NameError: name 'trainer' is not defined.\"\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI followed this tutorial for object detection: https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10 and this is step 6 of 7, so I'm almost done.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nThis is the code I'm working with, and the problem is in line 168 in main, trainer.train(.\r\n\r\nr\"\"\"Training executable for detection models.\r\n\r\nThis executable is used to train DetectionModels. There are two ways of\r\nconfiguring the training job:\r\n\r\n1) A single pipeline_pb2.TrainEvalPipelineConfig configuration file\r\ncan be specified by --pipeline_config_path.\r\n\r\nExample usage:\r\n    ./train \\\r\n        --logtostderr \\\r\n        --train_dir=path/to/train_dir \\\r\n        --pipeline_config_path=pipeline_config.pbtxt\r\n\r\n2) Three configuration files can be provided: a model_pb2.DetectionModel\r\nconfiguration file to define what type of DetectionModel is being trained, an\r\ninput_reader_pb2.InputReader file to specify what training data will be used and\r\na train_pb2.TrainConfig file to configure training parameters.\r\n\r\nExample usage:\r\n    ./train \\\r\n        --logtostderr \\\r\n        --train_dir=path/to/train_dir \\\r\n        --model_config_path=model_config.pbtxt \\\r\n        --train_config_path=train_config.pbtxt \\\r\n        --input_config_path=train_input_config.pbtxt\r\n\"\"\"\r\n\r\nimport functools\r\nimport json\r\nimport os\r\nimport tensorflow.compat.v1 as tf\r\nfrom tensorflow.python.util.deprecation import deprecated\r\n\r\n\r\nfrom object_detection.builders import dataset_builder\r\nfrom object_detection.builders import graph_rewriter_builder\r\nfrom object_detection.builders import model_builder\r\nfrom object_detection.utils import config_util\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')\r\nflags.DEFINE_integer('task', 0, 'task id')\r\nflags.DEFINE_integer('num_clones', 1, 'Number of clones to deploy per worker.')\r\nflags.DEFINE_boolean('clone_on_cpu', False,\r\n                     'Force clones to be deployed on CPU.  Note that even if '\r\n                     'set to False (allowing ops to run on gpu), some ops may '\r\n                     'still be run on the CPU if they have no GPU kernel.')\r\nflags.DEFINE_integer('worker_replicas', 1, 'Number of worker+trainer '\r\n                     'replicas.')\r\nflags.DEFINE_integer('ps_tasks', 0,\r\n                     'Number of parameter server tasks. If None, does not use '\r\n                     'a parameter server.')\r\nflags.DEFINE_string('train_dir', '',\r\n                    'Directory to save the checkpoints and training summaries.')\r\n\r\nflags.DEFINE_string('pipeline_config_path', '',\r\n                    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '\r\n                    'file. If provided, other configs are ignored')\r\n\r\nflags.DEFINE_string('train_config_path', '',\r\n                    'Path to a train_pb2.TrainConfig config file.')\r\nflags.DEFINE_string('input_config_path', '',\r\n                    'Path to an input_reader_pb2.InputReader config file.')\r\nflags.DEFINE_string('model_config_path', '',\r\n                    'Path to a model_pb2.DetectionModel config file.')\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\n@deprecated(None, 'Use object_detection/model_main.py.')\r\ndef main(_):\r\n  assert FLAGS.train_dir, '`train_dir` is missing.'\r\n  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)\r\n  if FLAGS.pipeline_config_path:\r\n    configs = config_util.get_configs_from_pipeline_file(\r\n        FLAGS.pipeline_config_path)\r\n    if FLAGS.task == 0:\r\n      tf.gfile.Copy(FLAGS.pipeline_config_path,\r\n                    os.path.join(FLAGS.train_dir, 'pipeline.config'),\r\n                    overwrite=True)\r\n  else:\r\n    configs = config_util.get_configs_from_multiple_files(\r\n        model_config_path=FLAGS.model_config_path,\r\n        train_config_path=FLAGS.train_config_path,\r\n        train_input_config_path=FLAGS.input_config_path)\r\n    if FLAGS.task == 0:\r\n      for name, config in [('model.config', FLAGS.model_config_path),\r\n                           ('train.config', FLAGS.train_config_path),\r\n                           ('input.config', FLAGS.input_config_path)]:\r\n        tf.gfile.Copy(config, os.path.join(FLAGS.train_dir, name),\r\n                      overwrite=True)\r\n\r\n  model_config = configs['model']\r\n  train_config = configs['train_config']\r\n  input_config = configs['train_input_config']\r\n\r\n  model_fn = functools.partial(\r\n      model_builder.build,\r\n      model_config=model_config,\r\n      is_training=True)\r\n\r\n  def get_next(config):\r\n    return dataset_builder.make_initializable_iterator(\r\n        dataset_builder.build(config)).get_next()\r\n\r\n  create_input_dict_fn = functools.partial(get_next, input_config)\r\n\r\n  env = json.loads(os.environ.get('TF_CONFIG', '{}'))\r\n  cluster_data = env.get('cluster', None)\r\n  cluster = tf.train.ClusterSpec(cluster_data) if cluster_data else None\r\n  task_data = env.get('task', None) or {'type': 'master', 'index': 0}\r\n  task_info = type('TaskSpec', (object,), task_data)\r\n\r\n  # Parameters for a single worker.\r\n  ps_tasks = 0\r\n  worker_replicas = 1\r\n  worker_job_name = 'lonely_worker'\r\n  task = 0\r\n  is_chief = True\r\n  master = ''\r\n\r\n  if cluster_data and 'worker' in cluster_data:\r\n    # Number of total worker replicas include \"worker\"s and the \"master\".\r\n    worker_replicas = len(cluster_data['worker']) + 1\r\n  if cluster_data and 'ps' in cluster_data:\r\n    ps_tasks = len(cluster_data['ps'])\r\n\r\n  if worker_replicas > 1 and ps_tasks < 1:\r\n    raise ValueError('At least 1 ps task is needed for distributed training.')\r\n\r\n  if worker_replicas >= 1 and ps_tasks > 0:\r\n    # Set up distributed training.\r\n    server = tf.train.Server(tf.train.ClusterSpec(cluster), protocol='grpc',\r\n                             job_name=task_info.type,\r\n                             task_index=task_info.index)\r\n    if task_info.type == 'ps':\r\n      server.join()\r\n      return\r\n\r\n    worker_job_name = '%s/task:%d' % (task_info.type, task_info.index)\r\n    task = task_info.index\r\n    is_chief = (task_info.type == 'master')\r\n    master = server.target\r\n\r\n  graph_rewriter_fn = None\r\n  if 'graph_rewriter_config' in configs:\r\n    graph_rewriter_fn = graph_rewriter_builder.build(\r\n        configs['graph_rewriter_config'], is_training=True)\r\n\r\n  trainer.train(\r\n      create_input_dict_fn,\r\n      model_fn,\r\n      train_config,\r\n      master,\r\n      task,\r\n      FLAGS.num_clones,\r\n      worker_replicas,\r\n      FLAGS.clone_on_cpu,\r\n      ps_tasks,\r\n      worker_job_name,\r\n      is_chief,\r\n      FLAGS.train_dir,\r\n      graph_hook_fn=graph_rewriter_fn)\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n\r\n\r\n# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================", "comments": ["Please format your code.Also I don't see were you define your trainer ", "it looks like you forgot to import it \r\nfrom object_detection.legacy import trainer\r\n", "Ah, thanks this helped.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51721\">No</a>\n"]}, {"number": 51720, "title": "Inconsistency In CategoricalCrossentropy Calculation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below):  **2.6.0**\r\n- Python version:  **3.6.13**\r\n- GPU model and memory:  **CPU-only**\r\n\r\n**Describe the current behavior**\r\nThe `categorical_crossentropy` loss values calculated with different ways is inconsistent in a model built by myself. \r\n\r\n1. In the first way, I calculate the loss value using `model.predict` and `CategoricalCrossentropy`\r\n2. In tne second way, I calculate the loss value using `model.evaluate`\r\n3. In the third way, I show the loss value using `model.fit`\r\n\r\nThe small model, input data `x`, target data `y_true` and the testing code are here:  [cce_test.zip](https://github.com/tensorflow/tensorflow/files/7068915/cce_test.zip)\r\n\r\nThe three results are: \r\n```\r\n14.506287     287.1078     287.1078\r\n```\r\nBesides, I calculate the loss with the same model and data using `CNTK2.7` and Theano`1.0.4`\uff0cand their result are both `14.506287`. This makes me more suspicious to the calculation result of tensorflow.\r\n\r\n**Describe the expected behavior**\r\n```\r\n14.506287    14.506287     14.506287\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.load_model(\"model.h5\")\r\nx = np.load(\"x.npy\")\r\ny_true = np.load(\"y_true.npy\")\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\n\r\n# calculate loss using model.predict and categorical_crossentropy\r\ncce = tf.keras.losses.CategoricalCrossentropy()\r\ny_pred = model.predict(x=x)\r\nloss1 = cce(y_true, y_pred)\r\nprint(\"loss1 = \", loss1.numpy())\r\n\r\n# calculate loss using model.evaluate\r\nloss2 = model.evaluate(x=x, y=y_true)\r\nprint(\"loss2 = \", loss2)\r\n\r\n# show loss using model.fit\r\nmodel.fit(x=x, y=y_true)\r\n```\r\nThe result is:\r\n```\r\nloss1 =  14.506287\r\n1/1 [==============================] - 0s 461ms/step - loss: 287.1078\r\nloss2 =  287.10784912109375\r\n1/1 [==============================] - 2s 2s/step - loss: 287.1078\r\n```\r\n\r\n**Other info / logs** \r\nHere is the code do the same thing using CNTK2.7:\r\n```python\r\nimport numpy as np\r\nimport os\r\nos.environ['KERAS_BACKEND'] = 'cntk'\r\nimport keras\r\n\r\nmodel = keras.models.load_model(\"model.h5\")\r\nx = np.load(\"x.npy\")\r\ny_true = np.load(\"y_true.npy\")\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\n\r\n# calculate loss using model.predict and categorical_crossentropy\r\ncce = keras.losses.CategoricalCrossentropy()\r\ny_pred = model.predict(x=x)\r\nloss1 = cce([y_true], [y_pred])\r\nprint(\"loss1 = \", loss1.eval())\r\n\r\n# calculate loss using model.evaluate\r\nloss2 = model.evaluate(x=x, y=y_true)\r\nprint(\"loss2 = \", loss2)\r\n\r\n# show loss using model.fit\r\nmodel.fit(x=x, y=y_true)\r\n```\r\nThe result is:\r\n```\r\nloss1 =  14.506287\r\nconvolution engine.\r\n10/10 [==============================] - 0s 5ms/step\r\nloss2 =  14.50628662109375\r\nEpoch 1/1\r\n10/10 [==============================] - 0s 21ms/step - loss: 14.5063\r\n```\r\n\r\nAny repiies will be appreciated.\r\nThanks.", "comments": ["I think this is because you load the model and funny thinks happens.Latter I will test it ", "@River861,\r\n\r\nI am able to reproduce the issue in colab with TF 2.6 and here is the [gist](https://colab.research.google.com/gist/sanatmpa1/58d9505fc6f1db5942a44a34b1eb7c09/51720.ipynb)\r\n\r\nI can see that this issue is mostly related to Keras component and so can you please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues),To know more see;[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)", "@sanatmpa1 ,\r\nThanks for your response.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51720\">No</a>\n"]}]