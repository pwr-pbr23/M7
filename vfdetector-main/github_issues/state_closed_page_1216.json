[{"number": 16685, "title": "Fix a broken link in regression_examples.md", "body": "This fix fixes a broken link in regression_examples.md", "comments": []}, {"number": 16684, "title": "The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error", "body": "Go tot the page \r\nhttps://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/\r\n\r\nand click the \"tutorial\" link. It gets a 404 error.\r\n\r\nThe target of the link is\r\nhttps://github.com/tensorflow/models/tree/master/syntaxnet#installation\r\n", "comments": ["That's a really old version. I don't think we update the old docs.\r\n\r\n@MarkDaoust can confirm.", "Is there someplace newer I should be looking for a tutorial on SyntaxNet?\n\n\nOn Fri, Feb 2, 2018 at 1:47 PM, drpngx <notifications@github.com> wrote:\n\n> That's a really old version. I don't think we update the old docs.\n>\n> @MarkDaoust <https://github.com/markdaoust> can confirm.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16684#issuecomment-362670329>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAAHRjdj-uSZhl8i_LNOeICXrVC0nyXsks5tQ1g5gaJpZM4R2pfz>\n> .\n>\n\n\n\n-- \nHenry Minsky\n", "That is correct, old docs not worth updating.\r\n\r\nThe tutorial is here: \r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/syntaxnet-tutorial.md", "Someone should update the live link on the web page ...\n\n\nOn Fri, Feb 2, 2018 at 11:51 PM, Mark Daoust <notifications@github.com>\nwrote:\n\n> That is correct, old docs not worth updating.\n>\n> The tutorial is here:\n>\n> https://github.com/tensorflow/models/blob/master/research/\n> syntaxnet/g3doc/syntaxnet-tutorial.md\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16684#issuecomment-362779876>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAAHRhs_O8iS3oEHF8wMwdc7y-CG6m51ks5tQ-XKgaJpZM4R2pfz>\n> .\n>\n\n\n\n-- \nHenry Minsky\n"]}, {"number": 16683, "title": "Tensorflow 1.5: failed to use tf.keras.applications.MobileNet()", "body": "I updated my Tensorflow to 1.5, and I tried to run the codes as below:\r\n`import tensorflow as tf`\r\n`model = tf.keras.applications.MobileNet()`\r\n\r\nBut it raised an error  as below:\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-14-2182e918e983> in <module>()\r\n----> 1 model = tf.keras.applications.MobileNet()\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)\r\n    538     K.set_image_data_format(old_data_format)\r\n    539   elif weights is not None:\r\n--> 540     model.load_weights(weights)\r\n    541   return model\r\n    542 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)\r\n   1099     if h5py is None:\r\n   1100       raise ImportError('`load_weights` requires h5py.')\r\n-> 1101     f = h5py.File(filepath, mode='r')\r\n   1102     if 'layer_names' not in f.attrs and 'model_weights' in f:\r\n   1103       f = f['model_weights']\r\n\r\n/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\r\n    267             with phil:\r\n    268                 fapl = make_fapl(driver, libver, **kwds)\r\n--> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\r\n    270 \r\n    271                 if swmr_support:\r\n\r\n/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\r\n     97         if swmr and swmr_support:\r\n     98             flags |= h5f.ACC_SWMR_READ\r\n---> 99         fid = h5f.open(name, flags, fapl=fapl)\r\n    100     elif mode == 'r+':\r\n    101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5f.pyx in h5py.h5f.open()\r\n\r\nOSError: Unable to open file (unable to open file: name = 'imagenet', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\r\n\r\nIt seems like that something wrong with h5py\r\nI try to upgrade h5py but it's still invalid\r\nThis works well in Tensorflow 1.4\r\nHow should I resolve it with Tensorflow 1.5, thanks\r\n", "comments": ["/CC @fchollet, do you know what the issue is?", "> No such file or directory\r\n\r\nThis is a simple error: weight loading can't find the cached file for this model.\r\n\r\nYour code snippet does work for me, so nothing wrong with the file location in tf.keras. I assume there was an issue when keras downloaded the file locally for you.\r\n\r\nTry deleting `~/.keras/models/`. Maybe that will fix it.", "I tried to delete ~/.keras/models and rerun the codes\r\nThe process would download the model, but it raises the same error\r\nNot only me but my colleagues have the same issues with Tensorflow 1.5 but it does work with Tensorflow 1.4\r\nfchollet, do you try with Tensorflow 1.5 as well?\r\n\r\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\r\n17227776/17225924 [==============================]17227776/17225924 [==============================] - 8s 0us/step\r\n\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-3-2182e918e983> in <module>()\r\n----> 1 model = tf.keras.applications.MobileNet()\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)\r\n    538     K.set_image_data_format(old_data_format)\r\n    539   elif weights is not None:\r\n--> 540     model.load_weights(weights)\r\n    541   return model\r\n    542 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)\r\n   1099     if h5py is None:\r\n   1100       raise ImportError('`load_weights` requires h5py.')\r\n-> 1101     f = h5py.File(filepath, mode='r')\r\n   1102     if 'layer_names' not in f.attrs and 'model_weights' in f:\r\n   1103       f = f['model_weights']\r\n\r\n/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\r\n    267             with phil:\r\n    268                 fapl = make_fapl(driver, libver, **kwds)\r\n--> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\r\n    270 \r\n    271                 if swmr_support:\r\n\r\n/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\r\n     97         if swmr and swmr_support:\r\n     98             flags |= h5f.ACC_SWMR_READ\r\n---> 99         fid = h5f.open(name, flags, fapl=fapl)\r\n    100     elif mode == 'r+':\r\n    101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/_objects.pyx in h5py._objects.with_phil.wrapper()\r\n\r\nh5py/h5f.pyx in h5py.h5f.open()\r\n\r\nOSError: Unable to open file (unable to open file: name = 'imagenet', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)", "Hi fchollet, \r\nI add one line in /usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py to print the name of file the program expect to load\r\n\r\n268                 fapl = make_fapl(driver, libver, **kwds)\r\n269                 print(name, 'aaaaaaaaaabbbbbbbbbbccccccccccccc')  # the code I added to show the name\r\n270                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)\r\n\r\nI rerun the codes and it showed as below:\r\n\r\nb'/home/wilson/.keras/models/mobilenet_1_0_224_tf.h5' aaaaaaaaaabbbbbbbbbbccccccccccccc\r\nb'imagenet' aaaaaaaaaabbbbbbbbbbccccccccccccc\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-5-2182e918e983> in <module>()\r\n----> 1 model = tf.keras.applications.MobileNet()\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)\r\n    538     K.set_image_data_format(old_data_format)\r\n    539   elif weights is not None:\r\n--> 540     model.load_weights(weights)\r\n    541   return model\r\n... skip...\r\n\r\nFrom the results, the program would run this snippet twice and with different names\r\nMay it be a clue to make my program can't work ?\r\nHow should resolve it? thanks\r\n\r\nI also tried the codes as below\r\n\r\nfrom keras import applications\r\nmodel = applications.MobileNet()\r\n\r\nIt only show \"b'/home/wilson/.keras/models/mobilenet_1_0_224_tf.h5' aaaaaaaaaabbbbbbbbbbccccccccccccc\"  once", "I confirm this is a real issue. It was introduced in PR https://github.com/tensorflow/tensorflow/pull/15146 and it applies to both Xception and MobileNet.\r\n\r\nHowever, it has been fixed since. If you upgrade to TF 1.6, you will no longer see this issue. For instance, you can `pip install` the TF 1.6 release candidate `1.6.0rc0` as of now."]}, {"number": 16682, "title": "[Recommendation] Expose tensorflow/core/kernels/dataset.h in wheel file", "body": "Hi,\r\n\r\nRight now, to build a new dataset op, you need to access to the header file `tensorflow/core/kernels/dataset.h`, but the tensorflow wheel does not expose this header.\r\n\r\nUse case: I built new Dataset Ops to read Kaldi's \"Table\" I/O format to enable others to be able to move from Kaldi-based automatic speech recognition recipes to tensorflow-based ones without having to do a bunch of extra data-munging. Right now, I require users to build tensorflow from source code and point my build to the tensorflow source code path, so I can guarantee that I have access to a header file compatible with their binary. I'd prefer to be able to build my package by depending only on pip-installed tensorflow to make things easier on users.\r\n\r\nI manually verified that adding \r\n\r\n```\r\n\"//tensorflow/core/kernels:dataset\",\r\n```\r\n\r\nto the deps of\r\n\r\n```\r\ntransitive_hdrs(\r\n    name = \"included_headers\",\r\n    deps = [\r\n        \"//tensorflow/core:core_cpu\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:lib\",\r\n        \"//tensorflow/core:protos_all_cc\",  \r\n        \"//tensorflow/core:stream_executor\",\r\n        \"//third_party/eigen3\",\r\n    ],\r\n)\r\n```\r\n\r\nin `tensorflow/tools/pip_package/BUILD` will include the right header file.\r\n\r\nThis is a pretty small change. Is there a particular reason why tensorflow does not already expose the header file? Is this an oversight or because you aren't ready to expose this interface publically?", "comments": []}, {"number": 16681, "title": "Sync r1.6 to master HEAD", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 16680, "title": "Branch 184220615", "body": "", "comments": ["@gunan looks like it also grabbed the commits in #16674 from master. PTAL."]}, {"number": 16679, "title": "fix typo", "body": "fix typo", "comments": []}, {"number": 16678, "title": "Remove invalid exception in linear operator", "body": "Remove unreachable `NotImplementedError` exception from _assert_non_singular() in LinearOperator.", "comments": ["Thank you @jhseu for reviewing and merging!"]}, {"number": 16677, "title": "Dear frinds", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 16676, "title": "Convert unicode to six.string_types for python 3", "body": "In Python 3, there is no unicode type. This fix converts\r\nunicode to use six.string_types instead, while maintaining\r\npython 2/3 compatibility.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16675, "title": "Fix linter error in losses_impl.py", "body": "", "comments": []}, {"number": 16674, "title": "Fix sanity build", "body": "- [x] Fix build error\r\n- [x] Update test\r\n", "comments": ["bazel nobuild issue looks new?", "@gunan PTAL? Thanks!"]}, {"number": 16673, "title": "Compile libtensorflow on windows with AVX.", "body": "", "comments": []}, {"number": 16672, "title": "Updating the version to 1.6.0-rc0.", "body": "", "comments": ["LGTM for profiler version upgrades."]}, {"number": 16671, "title": "Release Notes for r1.6", "body": "", "comments": []}, {"number": 16670, "title": "Tensorflow 1.5.0 import error under CUDA 8.0", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\npip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n3.5.4\r\n- **CUDA/cuDNN version**:\r\n8.0/6.0\r\n- **GPU model and memory**:\r\nGTX1080ti\r\n- **Exact command to reproduce**:\r\npip install tensorflow-gpu\r\npython\r\nimport tensorflow as tf\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nTensorflow 1.4 works fine with the same setup as described above. However, after upgrade tensorflow to 1.5 using pip install tensorflow-gpu, it fails to import tensorflow package in python.\r\n\r\n### Source code / logs\r\n(tensorflow_1_5) C:\\WINDOWS\\system32>python\r\nPython 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 75, in preload_check\r\n    ctypes.WinDLL(build_info.cudart_dll_name)\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\ctypes\\__init__.py\", line 351, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 30, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Program Files\\Anaconda3\\envs\\tensorflow_1_5\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 82, in preload_check\r\n    % (build_info.cudart_dll_name, build_info.cuda_version_number))\r\nImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit", "comments": ["Did you try following these instructions in the error message?\r\n\r\n> ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\n", "I have the same error following the instructions at https://www.tensorflow.org/install/install_windows. After reverting back to: \r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-win_amd64.whl the issue is resolved. ", "@dkhusial Did you try upgrading to CUDA 9.0, as suggested in the error message?", "Just tried tf 1.4 and works with my cuda 8 also. I am doing some testing on some tf from an old Windows machine to a new machine once I get through that I can try upgrading to cuda 9 and see if that will work with the latest tf 1.5. But that is likely tomorrow.", "@mrry Just found another machine todo the Cuda 9.0 upgrade with the matching cudnn. After upgrade, the tensorflow hello world validation instructions works on a GTX 960M. Can you update the install_windows instructions at tensorflow.org?", "@av8ramit looks like we may have forgotten tf install instructions with cuda version information?", "Yes, as of this moment the install instruction webpages (https://www.tensorflow.org/install/install_linux  and  https://www.tensorflow.org/install/install_windows) still show CUDA 8.0 and cuDNN 6.0, as noted in https://github.com/tensorflow/tensorflow/issues/16477", ">Yes, as of this moment the install instruction webpages (https://www.tensorflow.org/install/install_linux and https://www.tensorflow.org/install/install_windows) still show CUDA 8.0 and cuDNN 6.0, as noted in #16477\r\n@gunan @tylerlekang \r\n\r\nYes I followed the instruction and didn't update the CUDA when the exception happened.\r\n \r\nI upgraded CUDA and cudnn in my laptop (GTX 1050ti) and it worked. \r\nAfter updating to CUDA 9.0, cudnn 7 and Tensorflow 1.5, some model that I am now working with trains 25% faster than before in my laptop.\r\n\r\nInterestingly, The old tensorflow 1.4 still works under CUDA 9.0.\r\n\r\nIt seems to be safe to upgrade tensorflow 1.5 for my legacy code. So I will do the same thing for the desktop.\r\n\r\nThanks a lot!\r\n\r\n\r\n", "I tried tensorflow1.5 with cuda9.1 and cuda8.0, I got the same question. I see the install instruction say cuda8.0 is enough?", "Having just been through this myself, I found that if you are using TF 1.5 you need Cuda 9.0. The nightly tensorflow ought to support 9.1, but I don't think that update has been pushed to the pip3 repo. ", "It might be nice to have CUDA/cuDNN version support information in one location, and have everything else point to that webpage. Then you don't have to remember to update the version numbers in multiple places (and potentially forget a few).\r\n\r\nIn that case, it might also be nice to mention a few more specifics of the version numbers that a particular tf-gpu release was tested with.\r\n\r\nBecause there are a lot of different versions of CUDA and cuDNN, beyond simply saying 8.0 or 9.0. CUDA has 8.0GA1, 8.0GA2, and 9.0, the latter two of which also have an option for a separately installed update. cuDNN has 6.0 for CUDA 8.0, but also 7.0.4 and 7.0.5 for CUDA 9.0\r\n\r\nFurthermore, it could be mentioned what version of the nVidia driver was used ... but only if that might actually make a difference (not sure).\r\n\r\n\r\n@ha-ha-ha-han I think that when you install CUDA 9.0, it automatically installs side-by-side with CUDA 8.0 (not replaced it). Therefore, if you revert back to tf-gpu 1.4.0, it will still recognize the install of CUDA 8.0 that remains in your system.", "@dkhusial I was debugging this problem for HOURS! Thanks so much. Reverting solved my issue!!", "1.5, 1.6 and nightlies are all built for CUDA 9.0\r\nWe discussed with NVIDIA, and after their recommendation we decided to stay with cuda 9.0 rather than 9.1, as 9.1 requires drivers that are not available in all distros.", "@ha-ha-ha-han I have similar problem with upgrading to tf-1.5. However, I was unable to use @mrry advice and upgrade to cuda-9.0 due to the fact that U am using Ubuntu-14.04, for which NVIDIA does not provide an installer. \r\nThe call to `pip3 install --upgrade tensorflow-gpu` does provide installation for ubuntu 14.04.\r\nMaybe the configuration of ubuntu-14.04 and tf-1.5 can be achieved when building from source, however it was a real struggle in older tf versions so it would be highly appreciated if TF developers could provide prebuilt package of tf-1.5 for cuda-8.0. Or at least exclude ubuntu-14.04 out of supported OS for gpu versions.\r\n\r\n*update*\r\nI also tried to simply create symlinks to the older cuda RT library versions, but that didn't work as libcudnn.so.7 has some unique functions that can not be found by tensorflow runtime librarties.", "The runfile installer for CUDA 9.0 should work for ubuntu 14, too.\r\nOtherwise, you could also try using docker.", "problem exists on Windows with CUDA 9.0 and CUDNN 7.0.5 for CUDA 9", "Please file a new issue with all the debugging information requested by the template.\r\nThat way we can reproduce and pinpoint the exact problem.", "@gunan it seems like part of the issue is documentation.  It's currently impossible to install tensorflow successfully by following [the instructions](http://archive.is/S1P6z)", "I see. I think the documentation is fixed in the source, but we still need to push them to the website.\r\n@MarkDaoust can we update the documentation in 1.5 and 1.6 branches?", "Following instructions to install Cuda 9.0 on Ubuntu (apt-get) doesn't work, it will still install 9.1.", "You don't  install cuda correctly", "@mrry The problem is not that people can't figure out what the error means. If the docs (https://www.tensorflow.org/versions/r1.5/install/install_linux) specify that TF 1.5 supports CUDA 8, then either the docs are wrong or the code is, not people's expectations. \r\n\r\n@gunan this hasn't been pushed to the website for 1.5 (its fixed for 1.6). I just ran into the same error for 1.5 with Ubuntu 16.04/cuda 8/cudnn 6 .", "@MarkDaoust Can you push a fix to the 1.5 and 1.6 docs, please?"]}, {"number": 16669, "title": "grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**:  v1.5.0-0-g37aa430d84\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.10.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.1 / 7\r\n- **GPU model and memory**: TITAN Xp, 12196MiB\r\n- **Exact command to reproduce**: -\r\n\r\n### Describe the problem\r\nWhen I enable the memory optimizer in grappler, it fails with the following errors:\r\n```\r\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\r\nE tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.\r\n```\r\n\r\nMy network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.\r\n\r\nIs this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?", "comments": ["/CC @zhangyaobit, any ideas?", "Adding @benoitsteiner and @allenlavoie ", "@olesalscheider Could you add a bit about how you're using the memory optimizer, i.e. your RewriterConfig options?", "Sure. I do it like this:\r\n```\r\nrewrite_options = rewriter_config_pb2.RewriterConfig()\r\nrewrite_options.memory_optimization = rewriter_config_pb2.RewriterConfig.HEURISTICS\r\ngraph_options = tf.GraphOptions(rewrite_options=rewrite_options, infer_shapes=True)\r\nconfig = tf.ConfigProto(graph_options=graph_options)\r\n```", "Thanks! It looks like the swapping and scheduling heuristics depend on shapes being available (this is @benoitsteiner 's department). The recomputation/rematerialization heuristic is dumb enough to not care yet, so it should still be running.\r\n\r\nTo confirm, the graph does run, correct? It's just that errors get printed for swapping and scheduling passes.", "olesalscheider@ It would be great if you could share an example, since this error message should not be triggered. We haven't seen it internally, so without a way to reproduce the problem it will be very difficult to get to the bottom of this.", "@benoitsteiner I have extracted the relevant code that triggers the bug.\r\nYou can find it here: https://gist.github.com/olesalscheider/6d37c9716671dc607cab9ab281ae1cd1\r\nOf course it does not do anything useful now, but it runs fine without the memory optimizer.\r\n\r\n@allenlavoie With the memory optimizer, the code finally fails with \r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Creating a partition for /device::0 which doesn't exist in the list of available devices. Available devices: /device:CPU:0,/device:XLA_CPU:0,/device:XLA_GPU:0,/device:GPU:0\r\n\r\nWithout the optimizer it runs fine.", "Can you reproduce the problem with my script?", "@olesalscheider I reproduced the problem, and fixes are on their way.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Fixes should be available in the nightly builds. "]}, {"number": 16668, "title": "Fix undefined name: import as_str_any for line 35", "body": "flake8 testing of https://github.com/tensorflow/tensorflow on Python 2.7.14\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./tensorflow/python/util/compat_internal.py:33:12: F821 undefined name 'as_str_any'\r\n    path = as_str_any(path.__fspath__())\r\n           ^\r\n```", "comments": []}, {"number": 16667, "title": "Adds parameter 'msg' to tf.TensorFlowTestCase.", "body": "This commit adds a msg parameter that defaults to None to the following\r\nfunctions:\r\n- assertProtoEquals\r\n- assertArrayNear\r\n- assertNDArrayNear\r\n- assertAllClose\r\n- assertAllEqual\r\n- assertShapeEqual\r\n- assertDeviceEqual\r\n\r\nCloses #15729.", "comments": ["Thanks for reviewing :)\r\nDid the `matrix_triangular_solve_op_test` time out? It took 177.3 seconds and I don't spot a failure log. "]}, {"number": 16666, "title": "Typo in variable name: BETA --> self.BETA", "body": "__BETA__ is defined on line 118 as a class member so it can only be accessed via __self__ or via the class name, __ElasticAverageOptimizer__.\r\n\r\nflake8 testing of https://github.com/tensorflow/tensorflow\r\n\r\n$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__\r\n```\r\n./tensorflow/contrib/opt/python/training/elastic_average_optimizer.py:153:27: F821 undefined name 'BETA'\r\n      self._moving_rate = BETA / communication_period / num_worker\r\n                          ^\r\n```", "comments": []}, {"number": 16665, "title": "Android: No OpKernel was registered to support Op 'Min' with these attrs when using custom TensorFlow library built with SELECTIVE_REGISTRATION", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.7.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n\r\n### Describe the problem\r\n\r\nWhen running a **custom TensorFlow library** built with `SELECTIVE_REGISTRATION`and running our **quantized model** on Android we see this crash log:\r\n\r\n```Ruby\r\n java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Min' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n <no registered kernels>\r\n\r\n [[Node: mul_2_eightbit/mul_2/y/min = Min[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](mul_2_eightbit/mul_2/y/reshape, mul_2_eightbit/mul_2/y/reduction_dims)]]\r\n at org.tensorflow.Session.run(Native Method)\r\n at org.tensorflow.Session.access$100(Session.java:48)\r\n at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n at org.tensorflow.Session$Runner.run(Session.java:248)\r\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)\r\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n at io.cometapp.tensortest.models.yolo.YoloClassifier.predict(YoloClassifier.java:81)\r\n at io.cometapp.tensortest.ClassifierActivity$4.run(ClassifierActivity.java:713)\r\n at java.lang.Thread.run(Thread.java:764)\r\n```\r\n\r\nHere is how we build the custom TensorFlow Library\r\n\r\n`\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\"  --copt=\"-DTENSORFLOW_DISABLE_META\" --copt=\"-D__ANDROID_TYPES_FULL__\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n`\r\n\r\nIf we do not use `--copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" `, we are able to run the model successfully.\r\n\r\n\r\nHere is the ops_to_register.h that we use\r\n\r\n``` C\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"ConcatV2Op<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"DequantizeOp<CPUDevice, quint8>\",\r\n\"IdentityOp\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>\",\r\n\"BinaryOp< CPUDevice, functor::maximum<float>>\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::MinReducer<float>>\",\r\n\"NoOp\",\r\n\"PadOp<CPUDevice, float>\",\r\n\"PlaceholderOp\",\r\n\"QuantizeV2Op<CPUDevice, quint8>\",\r\n\"QuantizedBiasAddOp<quint8, quint8, qint32>\",\r\n\"QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>\",\r\n\"QuantizedMaxPoolingOp<CPUDevice, quint8>\",\r\n\"QuantizedMulOp<quint8, qint32>\",\r\n\"BinaryOp< CPUDevice, functor::div<float>>\",\r\n\"RequantizationRangeOp\",\r\n\"RequantizeOp<qint32, quint8>\",\r\n\"ReshapeOp\",\r\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"ConcatV2\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Dequantize\")\r\n     || isequal(op, \"Identity\")\r\n     || isequal(op, \"Max\")\r\n     || isequal(op, \"Maximum\")\r\n     || isequal(op, \"Min\")\r\n     || isequal(op, \"NoOp\")\r\n     || isequal(op, \"Pad\")\r\n     || isequal(op, \"Placeholder\")\r\n     || isequal(op, \"QuantizeV2\")\r\n     || isequal(op, \"QuantizedBiasAdd\")\r\n     || isequal(op, \"QuantizedConv2D\")\r\n     || isequal(op, \"QuantizedMaxPool\")\r\n     || isequal(op, \"QuantizedMul\")\r\n     || isequal(op, \"RealDiv\")\r\n     || isequal(op, \"RequantizationRange\")\r\n     || isequal(op, \"Requantize\")\r\n     || isequal(op, \"Reshape\")\r\n     || isequal(op, \"Sub\")\r\n     || isequal(op, \"_Recv\")\r\n     || isequal(op, \"_Send\")\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n\r\n```\r\n", "comments": ["/CC @petewarden ", "@petewarden can you please take a look?", "Your ops_to_register.h should have a line like this:\r\n\"ReductionOp<CPUDevice, float, int32, Eigen::internal::MinReducer<float>>\",\r\nYou can try adding this manually to see if you move past this error. How did you generate ops_to_register.h? I noticed that when I created the file using existing python script directly, I also got similar errors:\r\npython tensorflow/python/tools/print_selective_registration_header.py --graphs=\"path to graph\" > ops_to_register.h\r\n\r\nBut when I built the script using:\r\nbazel build tensorflow/python/tools/print_selective_registration_header\r\nand then:\r\n./bazel-bin/tensorflow/python/tools/print_selective_registration_header --graphs=\"path to graph\" > ops_to_register.h\r\n\r\nI didn't get those errors.\r\n\r\nIrrespective of above steps, I had to modify BUILD file to actually include those ops in the library.\r\n\r\nHope this helps!", "Closing this issue due to staleness. Please test with latest version of TF and reopen this issue if necessary. Thanks!"]}, {"number": 16664, "title": "Improve shape function of NonMaxSuppression", "body": "This fix tries to improve shape function of NonMaxSuppression.\r\n\r\nAs was specified in the docs, the shapes of parameters of `tf.image.non_max_suppression` are clearly defined with:\r\n```\r\nboxes: 2-D with shape [num_boxes, 4]\r\nscores: 1-D with shape [num_boxes]\r\nmax_output_size: 0-D scalar\r\niou_threshold: 0-D scalar\r\n```\r\n\r\nHowever, there is no shape check in the shape function of NonMaxSuppression.\r\n\r\nThis fix adds the shape check for NonMaxSuppression, and adds additinal test cases for it.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 16663, "title": "extract_glimpse padding with fixed value", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit\r\n- **TensorFlow installed from (source or binary)**: source (anaconda)\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: not relevant\r\n- **Exact command to reproduce**: not relevant\r\n\r\n\r\n### Describe the problem\r\n[tf documentation](https://www.tensorflow.org/api_docs/python/tf/image/extract_glimpse)\r\n> Returns a set of windows called glimpses extracted at location offsets from the input tensor. If the windows only partially overlaps the inputs, the non overlapping areas will be filled with random noise.\r\n\r\nthe function `tf.image.extract_glimpse ` padds windows that reach outside of the input tensor with random values from either a Gaussian or a normal distribution.\r\nHowever this prevents consistent classification in some cases.\r\n\r\nWould it be possible to add the option of specifying a fixed padding value (i.e. zero padding)?\r\n\r\n\r\n", "comments": ["/CC @benoitsteiner, what do you think of this proposal?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Added a PR #17690 and an attempt to address the issue. "]}, {"number": 16662, "title": "Current Bazel version is 0.10.0, expected at least 0.5.4", "body": "I get this error message when trying to build from source (r1.5) with the new bazel version published today.\r\n\r\nCurrent Bazel version is 0.10.0, expected at least 0.5.4\r\n\r\nI guess the version check is wrong. ", "comments": ["@jitoledo Seems cannot repro this issue in Ubuntu from my side", "I downloaded everything a couple of hours ago. Bazel 0.10.0 was published today. I am using ubuntu 14.04\r\n\r\n`(venv) jitoledo@CVC203:~/install_tensorflow_sources/tensorflow$ bazel build\r\nERROR: /home/jitoledo/install_tensorflow_sources/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):\r\n        File \"/home/jitoledo/install_tensorflow_sources/tensorflow/WORKSPACE\", line 15\r\n                closure_repositories()\r\n        File \"/home/jitoledo/.cache/bazel/_bazel_jitoledo/29f20a808e018d60d9e350b4366f4703/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n                _check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n        File \"/home/jitoledo/.cache/bazel/_bazel_jitoledo/29f20a808e018d60d9e350b4366f4703/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n                fail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.0\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: /home/jitoledo/install_tensorflow_sources/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\n        File \"/home/jitoledo/install_tensorflow_sources/tensorflow/WORKSPACE\", line 41\r\n                tf_workspace()\r\n        File \"/home/jitoledo/install_tensorflow_sources/tensorflow/tensorflow/workspace.bzl\", line 48, in tf_workspace\r\n                check_version(\"0.5.4\")\r\n        File \"/home/jitoledo/install_tensorflow_sources/tensorflow/tensorflow/workspace.bzl\", line 38, in check_version\r\n                fail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.10.0, expected at least 0.5.4\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nINFO: Elapsed time: 0.127s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n(venv) jitoledo@CVC203:~/install_tensorflow_sources/tensorflow$\r\n`", "@jitoledo It seems you're not using the latest master source branch right?", "According to instructions on  https://www.tensorflow.org/install/install_sources\r\nI did:\r\n`git checkout Branch # where Branch is the desired branch`\r\nFor branch r1.5 (which I thought is the current release version). Isn't it?", "I just switched to master branch and it  seems to work (still building). I just wanted to install an updated version of tensorflow.\r\nI tried with pip and it downloaded a version 1.5 but required cuda 9 and  I didn't to update to cuda9.1 (which would require updating ubuntu version too). ", "It works.", "@jitoledo  just:\r\ngit checkout r1.5 \r\nis that ok?", "I just switched to v1.5.0 but it doesn't work .Same bazel mistake as before.", "You just need to skip the checkout step completely. ", "@jitoledo  That works, thx.", "So by \"skip the checkout step\" I infer that we not checkout TF1.5 and instead work at HEAD?  I think that is TF1.6rc1.  We would prefer not to run on a release-candidate.  Or is that our only option so far."]}, {"number": 16661, "title": "Fix \"Define the model\" link.", "body": "The link syntax was inverted, that is, round brackets were coming before square brackets, but Markdown doesn't like it.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the fix, @kenjitoyama "]}, {"number": 16660, "title": "GPU support for Java in windows", "body": "I am working with Tensorflow in Java using the Maven dependency.\r\n\r\nI would like to use the GPU version in my Java application but I notice there is only a supported Maven repository for Linux.\r\n\r\nWill there be support for Tensorflow with Java in windows?\r\nIf so then what is the timeline for this?\r\n\r\nOtherwise is there another way round it?\r\n\r\nThanks.", "comments": ["@mrry @asimshankar can comment. I think it's safer to assume that it won't be there in the next few months.", "We don't build GPU-enabled binaries for the C or JNI libraries for Windows as part of the release process yet.\r\nHowever, it should happen within a release or two (probably 1.7).\r\n\r\nWe just started setting up the nightly build for GPU - https://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow-windows-gpu/\r\n(which hasn't had a successful build yet :)", "Ok thanks, I'm glad it's in your plans :)\r\n\r\nDo you have an idea how long it will be until 1.7?\r\n\r\nThanks,", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, \r\nYou mentioned that there should be windows gpu java support with 1.7, I see the 1.7 release, is there support for gpu in java with windows coming soon?", "@jedakiah13 : Unfortunately, I was not able to get this in for 1.7.0-rc0 . ", "so are we likely to have to wait until 1.8?", "I was curious to know the same, and appreciate the updates on the status of this @asimshankar !", "I too was wondering what the status on this is.", "Unfortunately, nope, we didn't get this in time for 1.7.\r\nHowever @case540 has been looking into the build issues,  so hopefully we'll be able to make progress in the next release or two.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any news here? ETA?", "Still waiting for an update... We would love to have this up and running. ", "Windows libtensorflow GPU build is broken and requires a fix from the protobuf team. Not entirely sure on eta but it is something we are working on.", "@case540 What exactly needs to be done? ", "Nagging Assignee @case540: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Still waiting for an update....", "Same here... @case540 are you working on this?", "I'm working on this. It should be available soon.", "@jedakiah13 @tlf30 - could you try with 1.10.0? ", "@av8ramit thanks a lot for your work!\r\nI can see the windows-x86_64\\tensorflow_jni.dll in libtensorflow_jni-1.10.0-rc1.jar but not in libtensorflow_jni_gpu-1.10.0-rc1.jar.\r\nShould the lib jni_gpu include a native win lib as well?\r\nCurrently gpu for win does not work for me referencing both libs via Maven.", "@mstritt - Please try version [1.10.0](https://search.maven.org/artifact/org.tensorflow/libtensorflow_jni_gpu/1.10.0/jar), not 1.10.0-rc1 :)", "@asimshankar , thanks for the hint, indeet it's included there.\r\nUnfortunately I get a dependency missing error now (see log).\r\nmsvcp140.dll is available, but I have cudbb64_6.dll. Do I have to upgrade to cudnn 7 ?\r\n\r\n```\r\norg.tensorflow.NativeLibrary: tryLoadLibraryFailed: no tensorflow_jni in java.library.path\r\norg.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/windows-x86_64/tensorflow_jni.dll\r\norg.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/windows-x86_64/tensorflow_framework.dll\r\norg.tensorflow.NativeLibrary: org/tensorflow/native/windows-x86_64/tensorflow_framework.dll not found. This is fine assuming org/tensorflow/native/windows-x86_64/tensorflow_jni.dll is not built to depend on it.\r\norg.tensorflow.NativeLibrary: extracting native library to: C:\\Users\\[...]\\AppData\\Local\\Temp\\tensorflow_native_libraries-1534235673406-0\\tensorflow_jni.dll\r\norg.tensorflow.NativeLibrary: copied 158330368 bytes to C:\\Users\\[...]\\AppData\\Local\\Temp\\tensorflow_native_libraries-1534235673406-0\\tensorflow_jni.dll\r\nException in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\[...]\\AppData\\Local\\Temp\\tensorflow_native_libraries-1534235673406-0\\tensorflow_jni.dll: Can't find dependent libraries\r\n\r\n```", "I think so, see https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support", "ok, works like a charm with cuda9+cudnn7. Thanks a lot for this!", "No problem @mstritt !", "@asimshankar my team and I will check it out, thank you.", "@mstritt Could you please tell the version of tensorflow too?\r\n", "@carlosuc3m : it is working since TF 1.10", "@mstritt it is not working for me in tf 1.12, with cuda 9 and cudnn 7. \r\nIt says that java.lang.UnsatisfiedLinkError: tensorflow_jni.dll: Can't find dependent libraries.\r\nThanks for your answer", "I have also tried with TF 1.15 and it loads well but it seems that is still using the CPU, how can I check this? Or should I call any different method? Regards", "Just in case someone finds it useful, the problem that was causing this error was that I was not using the correct CUDA version. I tried for tf 1.12 and tf 1.15. For tf 1.12 I needed CUDA 9.0 and for tf 1.15 I needed CUDA 10.0", "Hi all. \r\nMy config is Win 10 + CUDA 10.1 + cuDNN 7.6 + (may this have an interest, Python 3.7.7 + tensorflow 2.1 installed with pip in a Python virtual env; TF works well in Python). \r\nIn Java, which I hope having no dependency with TF in Python, I use without problem TF 1.4 and TF 1.14 for cpu with ant. I simply put tensorflow_jni.dll in the project subdirectory `myjni` and mention `-Djava.library.path=myjni` and it runs smoothly.\r\nWhen with ant, I use the same java sample program with TF 1.14 for gpu, with the same `-Djava.library.path=myjni` and tensorflow_jni.dll (gpu version, bigger) in the `myjni `directory, I receive the following error: \r\n`Exception in thread \"main\" java.lang.UnsatisfiedLinkError: C:\\Users\\<me>\\AppData\\Local\\Temp\\tensorflow_native_libraries-1584890522116-0\\tensorflow_jni.dll: Can't find dependent libraries`\r\nWhat is strange is that each run generates a new directory `C:\\Users\\<me>\\AppData\\Local\\Temp\\tensorflow_native_libraries-<NUMBER>\\` where `<NUMBER>` is each time different. The directory is removed automatically after about 2 sec (for the rare case it run fine, the directory is not removed). I would like to understand why. \r\nHence, tensorflow_jni.dll in myjni directory is ignored.\r\nSo many days spent here. Any clue?\r\nNB: I also have:\r\n-  (8!) Microsoft Visual C++ 20nn Redistribuable (x64,x86) with nn in [05, 12, 13, 15-19]. I don't know if I should remove some of them...\r\n- Microsoft Visual Studio Community 2015 with Updates"]}, {"number": 16659, "title": "Windows: Enable tensorflow/contrib in Bazel build", "body": "This change requires upgrading Bazel to 0.10.0 because we need `cc_import` rule.\r\n\r\nI disabled all failing tests on Windows, I will send issues to their owner later and write some tips about how to reproduce and test them.\r\n\r\n@gunan @mrry @martinwicke \r\n\r\nFYI @dslomov @laszlocsomor \r\n", "comments": ["What's the status of this? Still WIP for the infra?", "@martinwicke , @gunan : FYI @meteorcloudy is on vacation this week and the next, so he's unlikely to give an update.", "We are working on upgrading the test infra.\r\nI will try to resolve conflicts and rerun tests.", "I'm assuming you're still working on this.", "@martinwicke, I just came back from vacation, will continue to work on this today!", "Hmm, I'm getting this error?\r\n```\r\nERROR: T:/src/github/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):\r\n\tFile \"T:/src/github/tensorflow/WORKSPACE\", line 41\r\n\t\ttf_workspace()\r\n\tFile \"T:/src/github/tensorflow/tensorflow/workspace.bzl\", line 72, in tf_workspace\r\n\t\tcheck_bazel_version_at_least(\"0.10.0\")\r\n\tFile \"T:/src/github/tensorflow/tensorflow/workspace.bzl\", line 62, in check_bazel_version_at_least\r\n\t\tfail(\"\\nCurrent Bazel version is {}, ...))\r\n\r\nCurrent Bazel version is 0.9.0, expected at least 0.10.0\r\n```\r\nSeems like we forget to upgrade Bazel version on Kokoro Windows machines? @yifeif ", "bazel should be upgraded on windows now", "Nice, all tests passed, We can merge this now!", "This is breaking everything internally.\r\n@meteorcloudy I will roll this back now. Please resend it internally.", "+1 This broke bazel rules. ", "@gunan Can you point me to the internal error this change caused? I'll help fixing it.", "@miaout17 What do you mean it broke bazel rules? Can you elaborate?", "@meteorcloudy The issue was during merge. It caused multiple issues during merge, so we had to roll back."]}, {"number": 16658, "title": "Runtime Error with Qt GUI Application", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version use master**:\r\n- **Python version 2.7**: \r\n- **Bazel version 0.9.0**:\r\n- **GCC/Compiler version 5.4.0**:\r\n- **Without CUDA/cuDNN**:\r\n- **Without GPU**:\r\n\r\n### Describe the problem\r\nWhen I used QtCreator to build GUI Application, if include \"tensorflow/core/lib/core/refcount.h\", it will throw The program has unexpectedly finished.\r\n\r\n.pro like\r\n####\r\n    SOURCES += \\\r\n        main.cpp \\\r\n        mainwindow.cpp\r\n    HEADERS += \\\r\n         mainwindow.h\r\n    FORMS += \\\r\n         mainwindow.ui\r\n    \r\n    #tensorflow\r\n    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`\r\n    INCLUDEPATH += /home/face/Desktop/tensorflow`\r\n    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`\r\n    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`\r\n    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`\r\n    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework\r\n\r\nmain.cpp\r\n####\r\n    #include \"mainwindow.h\"\r\n    #include <QApplication>\r\n    #include <tensorflow/core/platform/env.h>\r\n    #include <tensorflow/core/public/session.h>\r\n\r\n    int main(int argc, char *argv[])\r\n    {\r\n        QApplication a(argc, argv);\r\n        MainWindow w;\r\n        w.show();\r\n        return a.exec();\r\n    }\r\n\r\nthen if \"tensorflow/core/lib/core/refcount.h\" line 79\r\n####\r\n    inline RefCounted::~RefCounted() {\r\n        DCHECK_EQ(ref_.load(), 0); \r\n    }\r\nto\r\n####\r\n    inline RefCounted::~RefCounted() {\r\n        //DCHECK_EQ(ref_.load(), 0); \r\n    }\r\nit will work.\r\n\r\n### Source code / logs\r\ndebug log like:\r\n####\r\n    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 \r\n    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 \r\n    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba \r\n    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb \r\n    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb \r\n    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 \r\n    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 \r\n    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 \r\n    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 \r\n    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 \r\n    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 \r\n    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 \r\n    13 ??                                                                           0x7ffff33100e5 \r\n    14 ??                                                                           0x7ffff3309975 \r\n    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e \r\n    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 \r\n    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 \r\n    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd \r\n    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b \r\n    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b \r\n    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 \r\n    22 main                                                          main.cpp   103 0x402e3e  \r\n\r\n", "comments": ["I'm thinking that it's the logging system that gets uninitialized before protobuf. Can you see if that's the case?\r\n\r\nIt looks like you're building the debug version, which is good. Could you try with asan?", "@drpngx asan like:\r\n#### \r\n    ASAN:SIGSEGV\r\n    =================================================================\r\n    ==13627== ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x7f2ea692e516 bp 0x000000000001 sp 0x7ffd3129fab0 T0)\r\n    #0 0x7f2ea692e515 in google::protobuf::internal::Mutex::Lock() (/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.9+0xe515)\r\n    #1 0x7f2ea692e832 in google::protobuf::internal::OnShutdown(void (*)()) (/usr/lib/x86_64-linux-gnu/libprotobuf-lite.so.9+0xe832)\r\n    #2 0x7f2ec17896b9  (/lib64/ld-linux-x86-64.so.2+0x106b9)\r\n    #3 0x7f2ec17897ca  (/lib64/ld-linux-x86-64.so.2+0x107ca)\r\n    #4 0x7f2ec178e8e1  (/lib64/ld-linux-x86-64.so.2+0x158e1)\r\n    #5 0x7f2ec1789563  (/lib64/ld-linux-x86-64.so.2+0x10563)\r\n    #6 0x7f2ec178dda8  (/lib64/ld-linux-x86-64.so.2+0x14da8)\r\n    #7 0x7f2eb935af08  (/lib/x86_64-linux-gnu/libdl.so.2+0xf08)\r\n    #8 0x7f2ec1789563  (/lib64/ld-linux-x86-64.so.2+0x10563)\r\n    #9 0x7f2eb935b570  (/lib/x86_64-linux-gnu/libdl.so.2+0x1570)\r\n    #10 0x7f2eb935afa0 in dlopen (/lib/x86_64-linux-gnu/libdl.so.2+0xfa0)\r\n    #11 0x7f2ec084320f in dlopen (/usr/lib/x86_64-linux-gnu/libasan.so.2+0x3720f)\r\n    #12 0x7f2eba660ea4  (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Core.so.5+0x27aea4)\r\n    #13 0x7f2eba65a714  (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Core.so.5+0x274714)\r\n    #14 0x7f2eba650cad in QFactoryLoader::instance(int) const (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Core.so.5+0x26acad)\r\n    #15 0x7f2eb8872028 in QPlatformThemeFactory::create(QString const&, QString const&) (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Gui.so.5+0x120028)\r\n    #16 0x7f2eb887cd47 in QGuiApplicationPrivate::createPlatformIntegration() (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Gui.so.5+0x12ad47)\r\n    #17 0x7f2eb887d70c in QGuiApplicationPrivate::createEventDispatcher() (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Gui.so.5+0x12b70c)\r\n    #18 0x7f2eba66bba0 in QCoreApplicationPrivate::init() (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Core.so.5+0x285ba0)\r\n    #19 0x7f2eb887f1aa in QGuiApplicationPrivate::init() (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Gui.so.5+0x12d1aa)\r\n    #20 0x7f2ebac92368 in QApplicationPrivate::init() (/home/face/Qt5.10.0/5.10.0/gcc_64/lib/libQt5Widgets.so.5+0x15e368)\r\n    #21 0x403ec8 in main ../Tensorflow_3_0/main.cpp:8\r\n    #22 0x7f2eb979b82f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x2082f)\r\n    #23 0x403d58 in _start (/home/face/build-Tensorflow_3_0-Desktop_Qt_5_10_0_GCC_64bit-Debug/Tensorflow_3_0+0x403d58)\r\n    AddressSanitizer can not provide additional info.\r\n    SUMMARY: AddressSanitizer: SEGV ??:0 google::protobuf::internal::Mutex::Lock()\r\n    ==13627==ABORTING\r\nAnd there is a problem from protobuf issues(I think they are the same):\r\nhttps://github.com/google/protobuf/issues/4169", "It's because QtApplication use protobuf-lite.so.9 and tensorflow use protobuf.so.14.\r\nIf you have the same problem.You can solve this problem by delete like: \r\n\r\n> /home/USER/Qt5.10.0/5.10.0/gcc_64/plugins/platformthemes/libqgtk3.so\r\n\r\nTO: [https://stackoverflow.com/questions/45703619/protobuf-version-conflicts-with-qt](url)"]}, {"number": 16657, "title": "Tensorflow on banana-pi m64", "body": "How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel \r\n\r\nWhen i trying install i have an error\r\n\r\n> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16656, "title": "Upgrade protobuf pip package version on Windows machine", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/2410/console\r\nThe tests in the Bazel Windows build started to fail after https://github.com/tensorflow/tensorflow/commit/e818d10f84bad3faad398f4b55831064666af5df\r\n\r\nAfter upgrade protobuf, you might also need to update protoc version in the CMake build.\r\n@gunan ", "comments": ["    W_0 = utils.weight_variable([FLAGS.z_dim, 64 * GEN_DIMENSION / 2 * IMAGE_SIZE / 16 * IMAGE_SIZE / 16],\r\nNameError: name 'utils' is not defined\r\n", "@meteorcloudy we did an upgrade in cmake build, but are you saying we missed the breakage because they were done separately?\r\n\r\nI think the latest bazel windows build has a separate issue, so this one may be resolved?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gunan: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Outdated."]}]