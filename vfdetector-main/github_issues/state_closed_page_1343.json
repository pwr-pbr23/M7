[{"number": 12795, "title": "computed output size would be negative      [[Node: pool_3 = AvgPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 8, 8, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mixed_10/join)]]  ", "body": "python train model and run bazel-bin/tensorflow/python/tools/optimize_for_inference  --input=/Users/andylin/Desktop/tess/retrained_graph.pb  --output=/Users/andylin/Desktop/tess/retrained_graph_optimized.pb  --input_names=Mul --output_names=final_result.\r\nerror:\r\ncomputed output size would be negative      [[Node: pool_3 = AvgPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 8, 8, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](mixed_10/join)]]  ", "comments": ["Facing the same error.\r\nHow did you solve this?", "@SragAR @AndyLin0128 I got the same error as well. Can you share your solution?", "@AndyLin0128 @SragAR  @xiaohk  Facing the same error.  Any suggestions", "image_shape=(224, 224, 3)  check the input size of your image...  also if you are pretrained weights, make sure you have the shapes mentioned correctly"]}, {"number": 12794, "title": "protobuf crashes at runtime when loading tensor lib.", "body": "hardware: Huawei P7 Android 4.4.2\r\n\r\ni tried ndk r12b , r10e , and api 9, api 14\r\nall run into this error:\r\n\r\n09-04 19:10:47.640 21660-21660/com.zhuxin.ecg.jijian A/libc: Fatal signal 6 (SIGABRT) at 0x0000549c (code=-6), thread 21660 (uxin.ecg.jijian)\r\n09-04 19:10:47.740 162-162/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n09-04 19:10:47.740 162-162/? I/DEBUG: Build fingerprint: 'Huawei/P7-L00/hwp7:4.4.2/HuaweiP7-L00/C17B620:user/ota-rel-keys,release-keys'\r\n09-04 19:10:47.740 162-162/? I/DEBUG: Revision: '0'\r\n09-04 19:10:47.740 162-162/? I/DEBUG: pid: 21660, tid: 21660, name: uxin.ecg.jijian  >>> com.zhuxin.ecg.jijian <<<\r\n09-04 19:10:47.740 162-162/? I/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n09-04 19:10:47.880 754-754/? W/View: requestLayout() improperly called by com.android.systemui.statusbar.phone.IconMerger{434090b0 V.E..... ......I. 0,0-270,72 #7f0a0069 app:id/notificationIcons} during layout: running second layout pass\r\n...\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #06  pc 0000274d  /system/bin/linker\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #07  pc 00002823  /system/bin/linker\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #08  pc 00002975  /system/bin/linker\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #09  pc 000029e9  /system/bin/linker\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #10  pc 00000f43  /system/bin/linker\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #11  pc 00050ee3  /system/lib/libdvm.so (dvmLoadNativeCode(char const*, Object*, char**)+182)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #12  pc 00068885  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #13  pc 00027ea0  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #14  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #15  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #16  pc 00061595  /system/lib/libdvm.so (dvmCallMethodV(Thread*, Method const*, Object*, bool, JValue*, std::__va_list)+336)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #17  pc 000615b9  /system/lib/libdvm.so (dvmCallMethod(Thread*, Method const*, Object*, JValue*, ...)+20)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #18  pc 0006cd7d  /system/lib/libdvm.so (dvmInitClass+1020)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #19  pc 0006da87  /system/lib/libdvm.so (dvmResolveMethod+198)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #20  pc 000234f4  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #21  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #22  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #23  pc 00061879  /system/lib/libdvm.so (dvmInvokeMethod(Object*, Method const*, ArrayObject*, ArrayObject*, ClassObject*, bool)+392)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #24  pc 00069963  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #25  pc 00027ea0  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #26  pc 0002eef0  /system/lib/libdvm.so (dvmMterpStd(Thread*)+76)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #27  pc 0002c588  /system/lib/libdvm.so (dvmInterpret(Thread*, Method const*, JValue*)+184)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #28  pc 00061595  /system/lib/libdvm.so (dvmCallMethodV(Thread*, Method const*, Object*, bool, JValue*, std::__va_list)+336)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #29  pc 0004ac6b  /system/lib/libdvm.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #30  pc 0004ed47  /system/lib/libandroid_runtime.so\r\n09-04 19:10:48.360 162-162/? I/DEBUG:     #31  pc 0004faef  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, char const*)+354)\r\n09-04 19:10:48.360 162-162/? I/DEBUG: stack:\r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e0  0006be74  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e4  81cfa290  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50e8  beed5104  [stack]\r\n# 09-04 19:10:48.360 162-162/? I/DEBUG:          beed50ec  812a0da8  /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so (std::unordered_map<std::string, google::protobuf::FieldDescriptorProto_Type, google::protobuf::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> > >::operator[](std::string&&)+48)\r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f0  beed51a0  [stack]\r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f4  81cfa290  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50f8  81cfa290  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed50fc  20492111  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed5100  81cfa290  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed5104  00000001  \r\n09-04 19:10:48.360 162-162/? I/DEBUG:          beed5108  00000015  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed510c  71a0b990  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5110  00000001  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5114  4007d9b5  /system/lib/libc.so (write+12)\r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5118  4008e1d8  /system/lib/libc.so\r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed511c  71a0b990  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:     #00  beed5120  00000006  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5124  00000016  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5128  0000549c  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed512c  400b1f0f  /system/bin/linker\r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5130  400b1f0f  /system/bin/linker\r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5134  4005628d  /system/lib/libc.so (pthread_kill+52)\r\n09-04 19:10:48.370 162-162/? I/DEBUG:     #01  beed5138  00000006  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed513c  00000000  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5140  74a2f24c  \r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed5144  400564a1  /system/lib/libc.so (raise+14)\r\n09-04 19:10:48.370 162-162/? I/DEBUG:     #02  beed5148  beed5154  [stack]\r\n09-04 19:10:48.370 162-162/? I/DEBUG:          beed514c  400551d7  /system/lib/libc.so\r\n09-04 19:10:48.390 162-162/? I/DEBUG: memory near r1:\r\n.....\r\n09-04 19:10:49.350 754-754/? W/View: requestLayout() improperly called by com.android.systemui.statusbar.phone.IconMerger{434090b0 V.E..... ........ 0,0-270,72 #7f0a0069 app:id/notificationIcons} during second layout pass: posting in next frame\r\n09-04 19:10:49.600 658-694/? W/InputDispatcher: channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)' ~ Consumer closed input channel or an error occurred.  events=0x9\r\n09-04 19:10:49.600 658-694/? E/InputDispatcher: channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)' ~ Channel is unrecoverably broken and will be disposed!\r\n09-04 19:10:49.710 362-466/? I/logserver: Object Path:/data/system/dropbox/, mask=0x00000080\r\n09-04 19:10:49.710 362-466/? I/logserver: event->len=48, name=data_app_native_crash@1504523449716.txt.gz\r\n09-04 19:10:49.710 362-466/? I/logserver: process_one_event, can not find this event(data_app_native_crash@1504523449716.txt.gz)\r\n09-04 19:10:49.710 362-466/? I/logserver: clean_cur_cache:962, system(rm -r /data/log/logcache/3577632/* > /dev/null 2>&1)\r\n09-04 19:10:49.710 658-1213/? W/InputDispatcher: Attempted to unregister already unregistered input channel '43db7980 com.zhuxin.ecg.jijian/com.ikinloop.ecgapplication.ui.activity.MainActivity (server)'\r\n09-04 19:10:49.720 1095-1095/? I/HwLauncher: DynamicIcon onWindowVisibilityChanged 4 - com.android.calendar\r\n", "comments": ["don't know why , somebody pls help\r\n\r\n\r\nbeed50ec 812a0da8 /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so (std::unordered_map<std::string, google::protobuf::FieldDescriptorProto_Type, google::protobuf::hashstd::string, std::equal_tostd::string, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> > >::operator+48)", "I don't quite understand how you triggered this failure--can you please tell us more about what you built, and what command you issued to get this stack trace?\r\n\r\nPlease provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@cy89 \r\nthks for replying this,  Mr. Young.\r\ni'll answer your questions one by one in a few days , thank you again!", "@cy89 @aselle \r\n \r\nhi , sorry for taking so long time to reply .\r\n\r\nphone:\r\nHuawei/P7-L00/hwp7:4.4.2/HuaweiP7-L00/C17B620\r\n\r\nprotobuf commit version:\r\n0b059a3\r\n\r\ncompiler:\r\nNDK r12b\r\napi level set to 14 (i checked 4.4.2 supports all levels under 19)\r\n\r\ncrash information:\r\n(std::unordered_map<std::string, google::protobuf::FieldDescriptorProto_Type, google::protobuf::hashstd::string, std::equal_tostd::string, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> > >::operator+48)\r\n\r\ni think it maybe a protobuf issue, not a tensor issue, so i posted this issue to protobuf on git too:\r\n\r\nhttps://github.com/google/protobuf/issues/3922\r\n\r\nany suggestions are highly appreciated~\r\n\r\n \r\n\r\n", "the news now is that i tried don't trigger any interface and :\r\n1. only load protobuf lib ,  it works well, \r\n2. load tensor and protobuf lib, crashes.\r\n\r\ni assume that it's a runtime error of protobuf , \r\nsay , protobuf can be loaded normally on the test phone, \r\nbut when u try to call interface of protobuf, it crashes.\r\n\r\nam i right ?   on loading time of tensorflow lib , classes call construct function, \r\ndid tensorflow called any interfaces of protobuf in construct functions ?\r\n", "i also tried the compiler :\r\nAndroid NDK r10e / API 14 / armeabi-v7a / 32-bit ARM\r\n\r\nthe crash remains\r\n", "i down graded the version of protobuf to :\r\nhttps://github.com/google/protobuf/archive/a428e42072765993ff674fda72863c9f1aa2d268.tar.gz\r\n\r\nthe crash remains.\r\n\r\nthe crash information changed to :\r\n11-20 17:54:28.790 182-182/? I/DEBUG:     #31  pc 0004faef  /system/lib/libandroid_runtime.so (android::AndroidRuntime::start(char const*, char const*)+354)\r\n11-20 17:54:28.790 182-182/? I/DEBUG: stack:\r\n11-20 17:54:28.790 182-182/? I/DEBUG:          beec00e0  751be770  \r\n11-20 17:54:28.790 182-182/? I/DEBUG:          beec00e4  829f9e0c  \r\n11-20 17:54:28.790 182-182/? I/DEBUG:          beec00e8  821b4c64  /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so\r\n\r\nit seems nothing to do with protobuf, but after i tested :\r\n1.only load protobuf lib , it works well,\r\n2.load tensor and protobuf lib, crashes.\r\n\r\nafter deeply thinking, there are 2 possible reasons:\r\n1. when loading tensorflow lib , some construction funcs of tensor called protobuf and result in crashes. \r\n2. tensor lib crashes at loading time , back trace of protobuf is only the dead words of victim.", "and i tried protobuf-lite , crashes and reported:\r\njava.lang.UnsatisfiedLinkError: dlopen failed: \r\n\r\nwhich is the same symptom with only load tensor and don't load protobuf", "then i tried a very old compiler:\r\nndk r9  api level 9\r\n\r\nand still crashes:\r\n\r\n11-21 15:17:44.940 2970-2970/com.zhuxin.ecg.jijian A/libc: Fatal signal 6 (SIGABRT) at 0x00000b9a (code=-6), thread 2970 (uxin.ecg.jijian)\r\n11-21 15:17:45.050 182-182/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n11-21 15:17:45.050 182-182/? I/DEBUG: Build fingerprint: 'Huawei/P7-L00/hwp7:4.4.2/HuaweiP7-L00/C17B620:user/ota-rel-keys,release-keys'\r\n11-21 15:17:45.050 182-182/? I/DEBUG: Revision: '0'\r\n11-21 15:17:45.050 182-182/? I/DEBUG: pid: 2970, tid: 2970, name: uxin.ecg.jijian  >>> com.zhuxin.ecg.jijian <<<\r\n11-21 15:17:45.050 182-182/? I/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     r0 00000000  r1 00000b9a  r2 00000006  r3 00000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     r4 00000006  r5 00000016  r6 00000b9a  r7 0000010c\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     r8 0000010b  r9 4005c678  sl 00000001  fp 82040428\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     ip 40057f0f  sp beec0120  lr 4016528d  pc 40174238  cpsr 000f0010\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d0  3436646578696673  d1  0000000000000069\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d2  000000000000006e  d3  0000000000000074\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d4  4058400000000000  d5  4058400000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d6  4058400000000000  d7  0000000000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d8  0000000044008000  d9  0000000000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d10 0000000000000000  d11 0000000000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d12 0000000000000000  d13 0000000000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d14 0000000000000000  d15 0000000000000000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d16 0000000000000000  d17 00000006ffffffff\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d18 3fe47ae147ae147b  d19 3fd51eb851eb851f\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d20 3fd3333333333333  d21 3fe3333333333333\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d22 3fc3333333333333  d23 3faeb851eb851eb8\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d24 40de898000000000  d25 40de89a0058c0000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d26 3fe0000000000000  d27 40e0108ffce00000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d28 40ef400ff4480000  d29 40e01d100abe0000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     d30 40dd4c2013880000  d31 40ed4c1013880000\r\n11-21 15:17:45.450 182-182/? I/DEBUG:     scr 60000017\r\n11-21 15:17:45.460 182-182/? I/DEBUG: backtrace:\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #00  pc 00022238  /system/lib/libc.so (tgkill+12)\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #01  pc 00013289  /system/lib/libc.so (pthread_kill+48)\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #02  pc 0001349d  /system/lib/libc.so (raise+10)\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #03  pc 000121d3  /system/lib/libc.so\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #04  pc 00021aec  /system/lib/libc.so (abort+4)\r\n11-21 15:17:45.460 182-182/? I/DEBUG:     #05  pc 00efba04  /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so (__check_for_sync8_kernelhelper+68)\r\n11-21 15:17:45.460 182-182/? I/DEBUG: stack:\r\n11-21 15:17:45.460 182-182/? I/DEBUG:          beec00e0  751bd678  \r\n11-21 15:17:45.460 182-182/? I/DEBUG:          beec00e4  82863c64  \r\n11-21 15:17:45.460 182-182/? I/DEBUG:          beec00e8  8201f294  /data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so", "ndk r9 's highest supporting gcc version is 4.8, \r\nso i changed all \"4.9\" in contrib/makefile/ to \"4.8\"", "by changing the protobuf version, crash information various:\r\n\r\n/data/app-lib/com.zhuxin.ecg.jijian-1/libecg_sdk.so (std::unordered_map<std::string, google::protobuf::FieldDescriptorProto_Type, google::protobuf::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, google::protobuf::FieldDescriptorProto_Type> > >::operator[](std::string&&)+48)\r\n\r\n\r\ni started to think about this may tensor lib's issue, is there any possibility that tensor lib is not compatible to various phone cpus ? course tensor incline to use gpu or other hardware..?", "i used:\r\n  ndk r9d gcc 4.8 compiled tensorflow 4.0, and still crashed", "could any one tell me :\r\n     how to open the log trace of tensorflow and protobuf ?", "i studied the compile procedure of tf, and find that there are three parts of source file :\r\n1.pb source files\r\n2.pb_text source files\r\n3.tensor core source files\r\n\r\ni collected all these source files to one IDE and study the source . \r\nany one can give any suggestion of learning the source core code of tensor is appreciated.", "i found a path for this issue, first i studied tensor lib consists of:\r\n\r\n1.pb source files\r\n2.pb_text source files\r\n3.tensor core source files\r\n4.protobuf lib\r\n\r\n1) i let the compile procedure pass normally,\r\n2) then before AR all these obj files, i only include part of the obj files\r\n3) then see if the System.loadlibrary() would not crash.\r\n4) in app i comment out all callings to tensorflow lib.\r\n5) as i continuously add obj files to tensorflow lib, and i may could positioning the obj who causes crash.\r\n\r\njust wish me luck , guys~\r\n\r\n", "my project is a shared lib, and in this lib i called tensorflow lib . and linked the shared lib to apk.\r\n\r\ni found that even if i cut tensor to hollow, it still crashes.\r\nthe problem is: i did:\r\ntarget_link_libraries(${PROJECT_NAME} -Wl,--allow-multiple-definition -Wl,--whole-archive tensorflow-core)\r\nwhich means pack tensor lib file into my so lib, phone of huawei p7 seems don't support this sport . \r\n\r\non contrast , my protobuf linked as:\r\ntarget_link_libraries(${PROJECT_NAME} protobuf)\r\nit's a big surprise that protobuf is not packet to my shared lib, \r\nmy shared lib can be loaded normally on android app, that's not surprise, \r\nthe real surprise is : why my app runs fine on many android phones when using tensor interfaces ? \r\nis protobuf a default lib in android kernel ? and every phone just use the local protobuf lib ?\r\n\r\nso the question turn out to be : \r\n    how to pack a static lib into a shared lib ,  and link the shared lib to apk ?\r\n ", "good news:\r\n     through very hard work of these days , this issue finally solved . \r\n\r\nthe solution is : \r\n    target_link_libraries(${PROJECT_NAME} \"-Wl,--whole-archive\" tensorflow-core \"-Wl,--no-whole-archive\")\r\n    target_link_libraries(${PROJECT_NAME} \"-Wl,--whole-archive\" protobuf \"-Wl,--no-whole-archive\")\r\n\r\nnext i will learn what whole archive and no whole archive means.\r\n\r\nalthough no one replied , i still thank this tensor discuss forum , for giving me the stress to hold on to what i am looking for. \r\n\r\n**# happy Thanksgiving**\r\n"]}, {"number": 12793, "title": "feature request: could tfdbg dump out debug data by specified step range?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.4(BuildVersion: 16E195) \r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc1-1702-g512d3d086', '1.3.0')\r\n- **Python version**: Python 2.7.13 (default, Apr  4 2017, 08:47:57)\r\n- **Bazel version (if compiling from source)**: 0.5.4-homebrew(Build time: Fri Aug 25 16:55:29 2017 (1503680129))\r\n- **CUDA/cuDNN version**: null\r\n- **GPU model and memory**: null\r\n- **Exact command to reproduce**: sess = dumping_wrapper.DumpingDebugWrapperSession(sess, session_root=\"<dump_path>\", log_usage=False)\r\n\r\n### feature request description\r\nI find that DumpingDebugWrapperSession in [dumping_wrapper.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/dumping_wrapper.py#L31) dumps debug data for each step, which makes the training very slow with hundreds (thousands?) of small files being generated. In fact, sometimes it is not necessary for us to need the information of all steps to debug.\r\nSo, could tfdbg dump out debug data by specified step range? Thanks.\r\n", "comments": ["@luchensk \r\n\r\nThe constructor of `tf_debug.DumpingDebugWrapper` provides a keyword argument called `watch_fn`, which can be used to control what tensors get watched on each run. `watch_fn` is a callable that takes fetch and feed names as input arguments and out puts an instance of [`tf_debug.WatchOptions`](https://www.tensorflow.org/api_docs/python/tfdbg/WatchOptions). The following example shows how to let the DumpingDebugWrapper dump to disk only for the 100th to 200th `Session.run()` calls:\r\n\r\n``` python\r\ncounter = 0\r\ndef my_watch_fn(fetches, feeds):\r\n  del fetches, feeds  # Not used. But you could use them to dump runs of specific fetches/feeds.\r\n  watch_opt = tf_debug.WatchOptions(debug_ops=\"DebugIdentity\", node_name_regex_whitelist=r\".*\" if (counter >= 100 and counter < 200) else r\"$^\") \r\n  counter += 1\r\n  return watch_opt\r\n\r\nsess = tf_debug.DumpingDebugWrapperSession(sess, watch_fn=my_watch_fn)\r\n# ...\r\n```", "@caisq Thanks, I will try it later."]}, {"number": 12792, "title": "Feature request: let transform_graph use summarize_graph inputs/outputs guess as default flags", "body": "`tensorflow/tools/graph_transforms/summarize_graph` is very useful for listing names of input and output nodes, however it seems like you still have to set `--inputs` and `--outputs` with `tensorflow/tools/graph_transforms/transform_graph` explicitly. Would it be possible to have `transform_graph` assume the same nodes from `summarize_graph` by default?", "comments": ["@petewarden, it looks like you've worked on both of these; can you please comment about whether this is feasible?", "Nagging Assignee @petewarden: It has been 209 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to stale."]}, {"number": 12791, "title": "Add a missing template explicit instantiation of SetZeroFunctor", "body": "To fix a build error on Windows:\r\n```\r\nlibconstant_op.lo(constant_op.o) : error LNK2019: unresolved external symbol \"public: void __cdecl tensorflow::functor::SetZeroFunctor<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::operator()(struct Eigen::ThreadPoolDevice const &,class Eigen::TensorMap<class Eigen::Tensor<class tensorflow::Variant,1,1,__int64>,16,struct Eigen::MakePointer>)\" (??R?$SetZeroFunctor@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@functor@tensorflow@@QEAAXAEBUThreadPoolDevice@Eigen@@V?$TensorMap@V?$Tensor@VVariant@tensorflow@@$00$00_J@Eigen@@$0BA@UMakePointer@2@@4@@Z) referenced in function \"public: virtual void __cdecl tensorflow::ZerosLikeOp<struct Eigen::ThreadPoolDevice,class tensorflow::Variant>::Compute(class tensorflow::OpKernelContext *)\" (?Compute@?$ZerosLikeOp@UThreadPoolDevice@Eigen@@VVariant@tensorflow@@@tensorflow@@UEAAXPEAVOpKernelContext@2@@Z)\r\n```", "comments": ["@snnn, thanks for your PR! By analyzing the history of the files in this pull request, we identified @martinwicke, @lukeiwanski and @andrewharp to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@snnn sorry for the delay and thanks for the fix.", "@tensorflow-jenkins test this please", "The cmake build error is puzzling to me and I don't see any clear error message. @drpngx is this a known error?", "We seem to have reduced the verbosity of the errors.\r\n\r\n```\r\n14:36:46 C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\cmake_build\\gemmlowp.vcxproj]\r\n```\r\nBut it's not clear why.\r\n\r\n@gunan do you know about the error verbosity? Should we switch it back on (and how)?", "It may due to all github regenerated all the archives\r\n\r\n#12986\r\n#12984\r\n#12979\r\n", "@snnn you are correct, the checksum is not matching, and that is the root cause of the issue.\r\nI have a change to increase the verbosity back, will send it internally.", "@gunan so this should be OK to merge then, right?", "Yes, merged."]}, {"number": 12790, "title": "tensorflow is moving too fast, could you release it a long term support version?", "body": "Hi\r\n    Tensorflow  is moving too fast to use my future project , could you release it a long term support \u3001stable  and full version?\r\n", "comments": ["@biyaa TF version 1 is supposed to be stable", "thank you!!!"]}, {"number": 12789, "title": "Zero accuracy if shuffle is False in TF Keras ImageDataGenerator", "body": "If I use the TF Keras reimplementation (tensorflow.contrib.keras) and set the ImageDataGenerator's shuffle param to False, I get zero accuracy every time. Also this:\r\n\r\nI've just used for the first time the ModelCheckpoint from function to save the best model (best_model = True) and wanted to test its performance. When the model was saved it said that the \"val_acc\" was at 83.3% before saving. I loaded the model and used the evaluate_generator on validation_generator but the result for \"val_acc\" was 0.639. I got confused and used it again and got 0.654 and then 0.647, 0.744 and so on. Questions are:\r\n\r\n1. Am I loading the model correctly, and if not what did I miss? Why is the result so much different?\r\n2. Why are the results between different evaluate_generator executions different (no retraining is happening, just shear execution of predict_generator multiple times in a row)?\r\nimportant part of the code (ResNet50 fine-tuning):\r\n\r\n```\r\nmodel.compile(loss='categorical_crossentropy',\r\n              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\r\n              metrics=['accuracy'])\r\ncheckpointer = ModelCheckpoint(filepath='/tmp/weights.hdf5', monitor = \"val_acc\", verbose=1, save_best_only=True)\r\n# prepare data augmentation configuration\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale = 1./ 255,\r\n    shear_range = 0.2,\r\n    zoom_range = 0.2,\r\n    horizontal_flip = True)\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size = (img_height, img_width),\r\n    batch_size = batch_size)\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size = (img_height, img_width),\r\n    batch_size = batch_size)\r\n# fine-tune the model\r\nmodel.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch = math.ceil(train_samples/batch_size),\r\n    epochs=100,\r\n    workers = 120,\r\n    validation_data=validation_generator,\r\n    validation_steps=math.ceil(val_samples/batch_size),\r\n    callbacks=[checkpointer])\r\nmodel.load_weights(filepath='/tmp/weights.hdf5')\r\nmodel.predict_generator(validation_generator, steps = math.ceil(val_samples/batch_size) )\r\n>> 0.62\r\nmodel.predict_generator(validation_generator, steps = math.ceil(val_samples/batch_size) )\r\n>> 0.587\r\n```\r\n", "comments": ["About the zero accuracy using `tf.contrib.keras`, please file a separate bug.\r\n\r\nOtherwise, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12788, "title": "imagePickerController for iOS , not recognize ", "body": "Hello I have created pb and txt file for some specific images and want to test with same image. taking that image  from Gallery in iPhone Device and trying to recognise but its not working properly.\r\n\r\nCan you please suggest.\r\n\r\nHow we can use tensor flow in Gallery Images. \r\nwhat size are required to use it.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12787, "title": "Sqeezenet pretrained model", "body": "Where can I find the pretrained squeezenet model using CIFAR or image net data sets? (if possible on tensorflow)\r\nI would like to use that to re train for some new classes for classification.\r\n", "comments": ["There are few resources related to Squeezenet at the below link : \r\n\r\nhttps://github.com/DeepScale/SqueezeNet\r\n\r\n", "Ya this I have seen. But I am very new to ML field. I want to know how to use it too.\r\nIt is not so descriptive.", "@nitish11 Request you to answer specifically. Many generic resources/material\r\n are available online. My requirement is retraining for classification using image net, CIFAR for squeezenet.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12786, "title": "Tensor roll op implementation", "body": "Closes #10761\r\nAdded a tf.roll op that works similarly to numpy's np.roll. This was a feature requested in #10761 and was marked as contributions welcome.\r\n\r\n### Usage:\r\n\r\nRolls the elements of a tensor by the offsets of `shift` along the dimensions\r\nof `axis`. Elements that roll passed the last position will wrap around\r\nto the first.\r\nFor example:\r\n```\r\n# 't' is [0, 1, 2, 3, 4]\r\nroll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\r\n# shifting along multiple dimensions\r\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\nroll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\r\n# shifting along the same axis multiple times\r\n# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\nroll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\r\n```\r\nshift: `shift[i]` specifies the number of places by which elements are shifted\r\n  along the dimension specified by `axis[i]`. Negative shifts will roll the\r\n  elements in the opposite direction.\r\naxis: `axis[i]` specifies the dimension that the shift `shift[i]` should occur.\r\n  if the same axis is referenced more than once, the total shift for that axis\r\n  will be the sum of all the shifts that belong to that axis.\r\noutput: Has the same shape and size as the input. The elements are shifted by\r\n  the offsets of `shift` along the dimensions of `axis`.\r\n\r\n### Unit tests:\r\n```\r\nRunning main() from test_main.cc\r\n[==========] Running 14 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 14 tests from RollOpTest\r\n[ RUN      ] RollOpTest.ScalarIndices\r\n2017-09-03 14:43:12.697389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n[       OK ] RollOpTest.ScalarIndices (10 ms)\r\n[ RUN      ] RollOpTest.ScalarIndices_Complex\r\n[       OK ] RollOpTest.ScalarIndices_Complex (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD32\r\n[       OK ] RollOpTest.Simple_TwoD32 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD32\r\n[       OK ] RollOpTest.Simple_ThreeD32 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_TwoD64\r\n[       OK ] RollOpTest.Simple_TwoD64 (0 ms)\r\n[ RUN      ] RollOpTest.Simple_ThreeD64\r\n[       OK ] RollOpTest.Simple_ThreeD64 (0 ms)\r\n[ RUN      ] RollOpTest.ZeroShift_ThreeD32\r\n[       OK ] RollOpTest.ZeroShift_ThreeD32 (0 ms)\r\n[ RUN      ] RollOpTest.ZeroSize_ThreeD32\r\n[       OK ] RollOpTest.ZeroSize_ThreeD32 (0 ms)\r\n[ RUN      ] RollOpTest.DuplicateShifts_TwoD32\r\n[       OK ] RollOpTest.DuplicateShifts_TwoD32 (1 ms)\r\n[ RUN      ] RollOpTest.Error_InputMustBeVectorOrHigher\r\n[       OK ] RollOpTest.Error_InputMustBeVectorOrHigher (0 ms)\r\n[ RUN      ] RollOpTest.Error_AxisMustBeScalarOrVector\r\n[       OK ] RollOpTest.Error_AxisMustBeScalarOrVector (0 ms)\r\n[ RUN      ] RollOpTest.Error_ShiftMustBeScalarOrVector\r\n[       OK ] RollOpTest.Error_ShiftMustBeScalarOrVector (0 ms)\r\n[ RUN      ] RollOpTest.Error_ShiftAndAxisMustBeSameSize\r\n[       OK ] RollOpTest.Error_ShiftAndAxisMustBeSameSize (0 ms)\r\n[ RUN      ] RollOpTest.Error_AxisOutOfRange\r\n[       OK ] RollOpTest.Error_AxisOutOfRange (0 ms)\r\n[----------] 14 tests from RollOpTest (11 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 14 tests from 1 test case ran. (11 ms total)\r\n[  PASSED  ] 14 tests.\r\n```\r\n\r\n### Benchmarks:\r\n```\r\nRunning main() from test_main.cc\r\nBenchmark                     Time(ns) Iterations\r\n-------------------------------------------------\r\n2017-09-03 14:44:03.626696: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\nBM_cpu_roll_outer/256/256        50332      15930\t 5208.3MB/s 1302.1M items/s\r\nBM_cpu_roll_outer/512/512       123096       4603\t 8518.3MB/s 2129.6M items/s\r\nBM_cpu_roll_outer/1024/1024     689598       1085\t 6082.2MB/s 1520.6M items/s\r\nBM_cpu_roll_outer/2048/2048    5394380        100\t 3110.1MB/s 777.5M items/s\r\nBM_cpu_roll_all/256/256          42984      16130\t 6098.7MB/s 1524.7M items/s\r\nBM_cpu_roll_all/512/512          96114       6405\t 10909.7MB/s 2727.4M items/s\r\nBM_cpu_roll_all/1024/1024       622506       1000\t 6737.8MB/s 1684.4M items/s\r\nBM_cpu_roll_all/2048/2048      5073920        100\t 3306.6MB/s 826.6M items/s\r\n```\r\n\r\nI also tried to implement this op for the GPU but gave up on it after struggling with it for a week. This is the error I received:\r\n```\r\n2017-09-03 20:21:33.871843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-03 20:21:33.872374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-09-03 20:21:33.872433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n2017-09-03 20:21:35.386734: E tensorflow/stream_executor/cuda/cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n2017-09-03 20:21:35.386839: F tensorflow/core/common_runtime/kernel_benchmark_testlib.cc:164] Non-OK-status: device_->Sync() status: Internal: GPU sync failed\r\nAborted (core dumped)\r\n```\r\nIf anyone has a clue of what might be the problem, I'd appreciate the help. The code for the gpu implementation is on this branch on my fork: https://github.com/tensorflow/tensorflow/compare/master...kobejean:roll-op-gpu \r\n\r\nThanks!", "comments": ["Can one of the admins verify this patch?", "@kobejean, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @josh11b to be potential reviewers.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.\r\n\r\n@aselle what do you think?", "I've been able to fix most of these errors but I've found a bug in a very specific test case that I need to figure out. I'll commit the changes when its fixed.", "Jenkins, test this please.", "Oh, that was in core. Usually, we ask for the new ops to go to contrib first, so we can canary them for a while, before merging into core.", "Yes, I was told to go ahead and put it in core when asking about it here #10761 but I can move it to contrib if we need to do that. \r\n\r\nSo one of the errors I see is the api compatibility test failure which I thought would be fixed by running\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\nbut it still seems to be showing up.\r\n\r\nThere also seems to be some other issues that came up on the Windows Cmake Tests that I need to address. I might not be able to work on it until next weekend.\r\n\r\nLet me know if you still want me to move it to contrib after all.", "Moving it to contrib will definitely make the api compatibility test pass. I would almost suggest we put it in `tf.manip.roll` to start the new namespace where we put things like slice, concat, stack, unstack. What do you think @drpngx?", "@aselle Yay! Great idea. Could you advise @kobejean on how to do that? He'll need to create some new file and put it in the `BUILD` file.\r\n\r\n@kobejean you'll need to change the goldens once this is done.", "To add a tf.manip namespace, you can follow the pattern used for adding tf.linalg i.e.\r\nhttps://github.com/tensorflow/tensorflow/commit/d8347c81628377ce99bb5d9e5fa55ddc8b7d47f3\r\nLet me know if you have any questions on this process.", "Alright sounds great! I'll work on it when I get a chance.", "Hi @aselle I've finally had a chance to work on this again and I just have a few questions. Am I supposed to move the code for tf.roll to tf.contrib.manip.roll or still in core but to tf.manip. I figured looking at the example with tf.linalg you probably wanted me to put it into core but I just wanted to check with you. Also did you want me to move ops like slice, concat, stack, and unstack to the tf.manip namespace as well? It seems like a lot of ops in array_ops.py could be considered in the tensor manipulation category but I also wanted to check with whether or not I should add them or just leave them alone. Thanks! ", "Also do you have an example of adding a namespace that involves c++ op kernels. Basically I had no problem when I registered my op in `tensorflow/core/ops/array_ops.cc` but now that I have it in a new file `tensorflow/core/ops/manip_ops.cc` it doesn't seem to get registered. Running the unit tests gives me\r\n```\r\nOp type not registered 'Roll' in binary running on Okar.local. Make sure the Op and Kernel are registered in the binary running in this process. while building NodeDef 'myop'\r\n```\r\nI've been trying to find all the BUILD files that I need to change but a reference would help.", "Ok never mind I think I've almost got it done", "@aselle I've finished moving to code to the tf.manip namespace by following the pattern here d8347c8. Can we run a test on this code?", "Jenkins, test this please!", "@aselle, can you add a review?", "Could these failed tests be because the main branch was failing at the time the tests were run? I'm not seeing any errors that have to do with the changes I made.", "Quite possibly. Jenkins, test this please.", "Can one of the admins verify this patch?", "Is this feature implemented? Can I find it in the nightly builds?", "@nthakor Not yet. I accidentally closed this pull request but I've reopened it here #14953. I believe its working though so you can clone my fork and compile tensorflow from the source if you want: https://github.com/kobejean/tensorflow, https://www.tensorflow.org/install/install_sources.", "> Closes #10761\r\n> Added a tf.roll op that works similarly to numpy's np.roll. This was a feature requested in #10761 and was marked as contributions welcome.\r\n> \r\n> ### Usage:\r\n> Rolls the elements of a tensor by the offsets of `shift` along the dimensions\r\n> of `axis`. Elements that roll passed the last position will wrap around\r\n> to the first.\r\n> For example:\r\n> \r\n> ```\r\n> # 't' is [0, 1, 2, 3, 4]\r\n> roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]\r\n> # shifting along multiple dimensions\r\n> # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\n> roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]\r\n> # shifting along the same axis multiple times\r\n> # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\r\n> roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]\r\n> ```\r\n> shift: `shift[i]` specifies the number of places by which elements are shifted\r\n> along the dimension specified by `axis[i]`. Negative shifts will roll the\r\n> elements in the opposite direction.\r\n> axis: `axis[i]` specifies the dimension that the shift `shift[i]` should occur.\r\n> if the same axis is referenced more than once, the total shift for that axis\r\n> will be the sum of all the shifts that belong to that axis.\r\n> output: Has the same shape and size as the input. The elements are shifted by\r\n> the offsets of `shift` along the dimensions of `axis`.\r\n> \r\n> ### Unit tests:\r\n> ```\r\n> Running main() from test_main.cc\r\n> [==========] Running 14 tests from 1 test case.\r\n> [----------] Global test environment set-up.\r\n> [----------] 14 tests from RollOpTest\r\n> [ RUN      ] RollOpTest.ScalarIndices\r\n> 2017-09-03 14:43:12.697389: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> [       OK ] RollOpTest.ScalarIndices (10 ms)\r\n> [ RUN      ] RollOpTest.ScalarIndices_Complex\r\n> [       OK ] RollOpTest.ScalarIndices_Complex (0 ms)\r\n> [ RUN      ] RollOpTest.Simple_TwoD32\r\n> [       OK ] RollOpTest.Simple_TwoD32 (0 ms)\r\n> [ RUN      ] RollOpTest.Simple_ThreeD32\r\n> [       OK ] RollOpTest.Simple_ThreeD32 (0 ms)\r\n> [ RUN      ] RollOpTest.Simple_TwoD64\r\n> [       OK ] RollOpTest.Simple_TwoD64 (0 ms)\r\n> [ RUN      ] RollOpTest.Simple_ThreeD64\r\n> [       OK ] RollOpTest.Simple_ThreeD64 (0 ms)\r\n> [ RUN      ] RollOpTest.ZeroShift_ThreeD32\r\n> [       OK ] RollOpTest.ZeroShift_ThreeD32 (0 ms)\r\n> [ RUN      ] RollOpTest.ZeroSize_ThreeD32\r\n> [       OK ] RollOpTest.ZeroSize_ThreeD32 (0 ms)\r\n> [ RUN      ] RollOpTest.DuplicateShifts_TwoD32\r\n> [       OK ] RollOpTest.DuplicateShifts_TwoD32 (1 ms)\r\n> [ RUN      ] RollOpTest.Error_InputMustBeVectorOrHigher\r\n> [       OK ] RollOpTest.Error_InputMustBeVectorOrHigher (0 ms)\r\n> [ RUN      ] RollOpTest.Error_AxisMustBeScalarOrVector\r\n> [       OK ] RollOpTest.Error_AxisMustBeScalarOrVector (0 ms)\r\n> [ RUN      ] RollOpTest.Error_ShiftMustBeScalarOrVector\r\n> [       OK ] RollOpTest.Error_ShiftMustBeScalarOrVector (0 ms)\r\n> [ RUN      ] RollOpTest.Error_ShiftAndAxisMustBeSameSize\r\n> [       OK ] RollOpTest.Error_ShiftAndAxisMustBeSameSize (0 ms)\r\n> [ RUN      ] RollOpTest.Error_AxisOutOfRange\r\n> [       OK ] RollOpTest.Error_AxisOutOfRange (0 ms)\r\n> [----------] 14 tests from RollOpTest (11 ms total)\r\n> \r\n> [----------] Global test environment tear-down\r\n> [==========] 14 tests from 1 test case ran. (11 ms total)\r\n> [  PASSED  ] 14 tests.\r\n> ```\r\n> ### Benchmarks:\r\n> ```\r\n> Running main() from test_main.cc\r\n> Benchmark                     Time(ns) Iterations\r\n> -------------------------------------------------\r\n> 2017-09-03 14:44:03.626696: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\n> BM_cpu_roll_outer/256/256        50332      15930\t 5208.3MB/s 1302.1M items/s\r\n> BM_cpu_roll_outer/512/512       123096       4603\t 8518.3MB/s 2129.6M items/s\r\n> BM_cpu_roll_outer/1024/1024     689598       1085\t 6082.2MB/s 1520.6M items/s\r\n> BM_cpu_roll_outer/2048/2048    5394380        100\t 3110.1MB/s 777.5M items/s\r\n> BM_cpu_roll_all/256/256          42984      16130\t 6098.7MB/s 1524.7M items/s\r\n> BM_cpu_roll_all/512/512          96114       6405\t 10909.7MB/s 2727.4M items/s\r\n> BM_cpu_roll_all/1024/1024       622506       1000\t 6737.8MB/s 1684.4M items/s\r\n> BM_cpu_roll_all/2048/2048      5073920        100\t 3306.6MB/s 826.6M items/s\r\n> ```\r\n> I also tried to implement this op for the GPU but gave up on it after struggling with it for a week. This is the error I received:\r\n> \r\n> ```\r\n> 2017-09-03 20:21:33.871843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n> 2017-09-03 20:21:33.872374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\n> name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n> pciBusID: 0000:00:1e.0\r\n> totalMemory: 11.17GiB freeMemory: 11.11GiB\r\n> 2017-09-03 20:21:33.872433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n> 2017-09-03 20:21:35.386734: E tensorflow/stream_executor/cuda/cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n> 2017-09-03 20:21:35.386839: F tensorflow/core/common_runtime/kernel_benchmark_testlib.cc:164] Non-OK-status: device_->Sync() status: Internal: GPU sync failed\r\n> Aborted (core dumped)\r\n> ```\r\n> If anyone has a clue of what might be the problem, I'd appreciate the help. The code for the gpu implementation is on this branch on my fork: [master...kobejean:roll-op-gpu](https://github.com/tensorflow/tensorflow/compare/master...kobejean:roll-op-gpu)\r\n> \r\n> Thanks!\r\n\r\nHi! Any news on a GPU implementation of this op? It would be hugely appreciated! :)\r\n\r\nBW,\r\nGavin"]}, {"number": 12785, "title": "Exported eager API TFE_* symbols in the dynamic library", "body": "Dicussed in #12784.", "comments": ["@eaplatanios, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @ebrevdo and @jhseu to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please", "@asimshankar Thank a lot! I was actually wondering why I couldn't link in Windows but I thought it was a problem with the cross-compilation docker container I was setting up. I made the changes for Windows too now. :)", "Jenkins, test this please.", "I'm not really sure what's wrong here. Shouldn't this change have no impact on existing code?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "I believe some of the tests were cancelled. I don't know why. I resolved the conflicts and triggered again.", "Sounds good. Thanks! :)", "@eaplatanios There seem to be honest build errors. E.g., \r\n\r\n> ERROR: /workspace/tensorflow/python/BUILD:2884:1: SWIGing tensorflow/python/tensorflow.i failed: swig failed: error executing command \r\n>   (cd /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace && \\\r\n>   exec env - \\\r\n>   bazel-out/local-opt/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Iexternal/curl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jemalloc -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Ibazel-out/local-opt/genfiles/external/jemalloc -Iexternal/snappy -Iexternal/highwayhash -Iexternal/gif_archive -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Ibazel-out/local-opt/genfiles/external/curl -Ibazel-out/local-opt/genfiles/external/snappy -Iexternal/farmhash_archive -Iexternal/local_config_cuda -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n> tensorflow/c/eager/c_api.h:33: Error: Syntax error in input(1).\r\n\r\nSee https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6606/consoleFull", "@caisq Thanks for pointing that out! I made a small edit that would probably fix them. Could you please run the tests again? Thanks!", "@tensorflow-jenkins test this please", "I think the tests are simply timing out now and that there should be no bugs.", "@eaplatanios Yes, the tests look good. But your CLA isn't signed yet. Can you take care of that?", "I've already done that long time ago. It only started showing at not signed after @martinwicke merged master into this PR. Not sure what to do to make it show it as signed again.", "You can ignore issuebot on this one. It gets confused by multiple authors.\n\nOn Sep 5, 2017 17:13, \"Anthony Platanios\" <notifications@github.com> wrote:\n\n> I've already done that long time ago. It only started showing at not\n> signed after @martinwicke <https://github.com/martinwicke> merged master\n> into this PR. Not sure what to do to make it show it as signed again.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12785#issuecomment-327337421>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_eyxdfKWjyINAP8C2CM2s1hc6Kc4ks5sfeOTgaJpZM4PLTQD>\n> .\n>\n"]}, {"number": 12784, "title": "Eager API symbols not exported in the CI libtensorflow.so", "body": "The eager API symbols (i.e., `TFE_*`) do not seem to be exported in the `libtensorflow.so` built by the CI system. I tried the nightly builds and they're missing from them. I checked using `nm -D libtensorflow.so` as well as `readelf -Ws libtensorflow.so` and `objdump -TC libtensorflow.so`. I'm not sure where to look to fix this though as the Mac nightly build seems fine. @asimshankar @alextp ", "comments": ["Note that the eager APIs are NOT stable, so use at your own risk. They may change at any time (as we're experimenting with them) and we do not plan to restrict even trivial changes (like changing symbol names) to maintain compatibility at this time\r\n\r\nWith that disclaimer on the way, this is likely because [`version_script.lds`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/version_script.lds) prunes out all the `TFE_*` symbols. You are welcome to send in a PR :)\r\n", "Thanks for pointing me to the file. I also made a couple small changes to two other files that I found may be relevant and submitted a PR (#12785).\r\n\r\nThanks for the disclaimer! I'm aware of that and I'm actually trying to find out what may be broken in the API. Thanks for releasing it early in any case. It's super useful for languages that do not have anything like numpy available. It certainly helps a lot with my Scala API.", "Just a heads up, but I have noticed that `TFE_Execute` does not seem to be thread-safe. If I have multiple threads calling it I often end up with malloc double free errors, but I haven't tracked this down yet.", "Closing this since the relevant PRs have been merged."]}, {"number": 12783, "title": "Behavior of SVM in tensorflow.contrib.learn.python.learn.estimators inconsistent between dense and sparse inputs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nArch Linux\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary (community arch linux repo)\r\n- **TensorFlow version (use command below)**:\r\n1.3.0-1\r\n- **Python version**: \r\n3.6.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI'm trying to use the SVM in tensorflow.contrib.learn with sparse input. The minimal test with real dense valued column works. However when I try to reproduce the exact same classification problem but wrapped in sparse feature columns with sparse_columns_with_integerized_features and weighted_sparse_columns, it fails. It might be a misuse of the sparse columns from me but I've spent a good amount of time reading the doc and the source code without understanding why.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n# TEST WITH REAL VALUED COLUMNS THAT WORKS\r\ndef input_fn():\r\n  return {\r\n      'example_id': constant_op.constant(['1', '2', '3']),\r\n      'feature1': constant_op.constant([[0.0], [1.0], [3.0]]),\r\n      'feature2': constant_op.constant([[1.0], [-1.2], [1.0]]),\r\n  }, constant_op.constant([[1], [0], [1]])\r\n\r\nfeature1 = feature_column.real_valued_column('feature1')\r\nfeature2 = feature_column.real_valued_column('feature2')\r\nsvm_classifier = svm.SVM(feature_columns=[feature1, feature2],\r\n                         example_id_column='example_id',\r\n                         l1_regularization=0.0,\r\n                         l2_regularization=0.0)\r\nsvm_classifier.fit(input_fn=input_fn, steps=30)\r\nmetrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\r\nloss = metrics['loss']  # 0 as expected\r\naccuracy = metrics['accuracy']  # 1 as expected\r\n\r\n# TEST WITH SAME VALUES BUT SPARSE COLUMNS, FAILS\r\ndef input_fn():\r\n  return {\r\n      'example_id': constant_op.constant(['1', '2', '3']),\r\n      'feature1_id': tf.SparseTensor(\r\n                      indices=tf.constant([[0,0],[1,0],[2,0]], dtype=tf.int64),\r\n                      values=tf.constant([0, 0, 0], dtype=tf.int64),\r\n                      dense_shape=[3,1],\r\n                      ),\r\n      'feature2_id': tf.SparseTensor(\r\n                      indices=tf.constant([[0,0],[1,0],[2,0]], dtype=tf.int64),\r\n                      values=tf.constant([0, 0, 0], dtype=tf.int64),\r\n                      dense_shape=[3,1],\r\n                      ),\r\n      'feature1_w': tf.SparseTensor(\r\n                      indices=[[0,0],[1,0],[2,0]],\r\n                      values=[0.0, 1.0, 3.0],\r\n                      dense_shape=[3,1]\r\n                      ),\r\n      'feature2_w': tf.SparseTensor(\r\n                      indices=[[0,0],[1,0],[2,0]],\r\n                      values=[1.0, -1.2, 1.0],\r\n                      dense_shape=[3,1]\r\n                      )\r\n      }, constant_op.constant([[1], [0], [1]])\r\n\r\nfeature1_id = feature_column.sparse_column_with_integerized_feature('feature1_id',\r\n                                                                    bucket_size=2,)\r\nfeature2_id = feature_column.sparse_column_with_integerized_feature('feature2_id',\r\n                                                                    bucket_size=2,)\r\nfeature1_w = feature_column.weighted_sparse_column(feature1_id, 'feature1_w')\r\nfeature2_w = feature_column.weighted_sparse_column(feature2_id, 'feature2_w')\r\n\r\nsvm_classifier = svm.SVM(feature_columns=[feature1_w, feature2_w],\r\n                         example_id_column='example_id',\r\n                         l1_regularization=0.0,\r\n                         l2_regularization=0.0)\r\nsvm_classifier.fit(input_fn=input_fn, steps=30)\r\nmetrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\r\nloss = metrics['loss'] # should be 0 but is 0.36363636\r\naccuracy = metrics['accuracy'] # should be 1 but is 0.666667\r\n\r\n# TEST WITH SPARSE COLUMNS IN ONE FEATURE COLUMN, SAME RESULTS\r\ndef input_fn():\r\n  return {\r\n      'example_id': constant_op.constant(['1', '2', '3']),\r\n      'feature1_id': tf.SparseTensor(\r\n                      indices=tf.constant([[0,0],[0,1],[1,0],[1,1],[2,0],[2,1]], dtype=tf.int64),\r\n                      values=tf.constant([0, 1, 0, 1, 0, 1], dtype=tf.int64),\r\n                      dense_shape=[3,2],\r\n                      ),\r\n      'feature1_w': tf.SparseTensor(\r\n                      indices=[[0,0],[0,1],[1,0],[1,1],[2,0],[2,1]],\r\n                      values=[0.0, 1.0, 1.0, -1.2, 3.0, 1.0],\r\n                      dense_shape=[3,2],\r\n                      )\r\n      }, constant_op.constant([[1], [0], [1]])\r\n\r\nfeature1_id = feature_column.sparse_column_with_integerized_feature('feature1_id',\r\n                                                                    bucket_size=2,)\r\nfeature1_w = feature_column.weighted_sparse_column(feature1_id, 'feature1_w')\r\n\r\nsvm_classifier = svm.SVM(feature_columns=[feature1_w],\r\n                         example_id_column='example_id',\r\n                         l1_regularization=0.0,\r\n                         l2_regularization=0.0)\r\nsvm_classifier.fit(input_fn=input_fn, steps=30)\r\nmetrics = svm_classifier.evaluate(input_fn=input_fn, steps=1)\r\nloss = metrics['loss']\r\naccuracy = metrics['accuracy']\r\n\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12782, "title": "Removing /contrib/tensorboard and its references", "body": "New pull request to master branch, as requested at https://github.com/tensorflow/tensorflow/pull/12768.\r\n\r\nCleaning up \"/contrib/tensorboard\" since Tensorboard have been moved and we don't need the \"/contrib/tensorboard\" additionally. (As mentioned before, it creates confusion)", "comments": ["Can one of the admins verify this patch?", "@goktay, thanks for your PR! By analyzing the history of the files in this pull request, we identified @dsmilkov, @jart and @rohan100jain to be potential reviewers.", "@goktay do you mind resolve the conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I have pulled the current master branch, did the necessary changes, copied all the files to my local repo and pushed it to my github.\r\n\r\nOnly conflict I have is inside contrib/tensorboard which should be deleted, I don't know why it shows up as a conflict.\r\n\r\nGit is not my cup of tea I guess.", "CLAs look good, thanks!\n\n<!-- ok -->", "@goktay could you pull rebase and push again?", "@goktay can you resolve the merge conflicts", "@goktay, please resolve conflicts.\r\n@jart, please review?", "I really appreciate that you took the time to send us this change @goktay. I've actually been doing some work in contrib/tensorboard recently so I'd like to have this folder stick around for the time being. Some of the things in here could go on the other hand. I'll try to take care of that as soon as I have the cycles."]}, {"number": 12781, "title": "Compile from source on Skylake-X (Intel i9)", "body": "Hello,\r\n\r\nCompiling tensorflow source on Skylake-X Intel i9 with \"--config=opt\" gives the following error in the snappy external module:\r\n\r\n-----------------------\r\n\r\n> ERROR: /home/armafire/.cache/bazel/_bazel_armafire/efbef35334c587b69e16a82829bb0e2d/external/snappy/BUILD:19:1: C++ compilation of rule '@snappy//:snappy' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n>   (cd /home/armafire/.cache/bazel/_bazel_armafire/efbef35334c587b69e16a82829bb0e2d/execroot/org_tensorflow && \\\r\n>   exec env - \\\r\n>     CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n>     CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n>     GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n>     PWD=/proc/self/cwd \\\r\n>     PYTHON_BIN_PATH=/usr/bin/python \\\r\n>     PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n>     TF_CUDA_CLANG=0 \\\r\n>     TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n>     TF_CUDA_VERSION=8.0 \\\r\n>     TF_CUDNN_VERSION=6 \\\r\n>     TF_NEED_CUDA=1 \\\r\n>     TF_NEED_OPENCL=0 \\\r\n>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.d '-frandom-seed=bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o' -fPIC -iquote external/snappy -iquote bazel-out/local_linux-opt/genfiles/external/snappy -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-shift-negative-value -Wno-implicit-function-declaration -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/snappy/snappy.cc -o bazel-out/local_linux-opt/bin/external/snappy/_objs/snappy/external/snappy/snappy.pic.o)\r\n> cc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\n> external/snappy/snappy.cc: In member function 'void snappy::SnappySinkAllocator::Flush(size_t)':\r\n> external/snappy/snappy.cc:1403:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n>      for (int i = 0; i < blocks_.size(); ++i) {\r\n>                      ~~^~~~~~~~~~~~~~~~\r\n> In file included from external/snappy/snappy-internal.h:34:0,\r\n>                  from external/snappy/snappy.cc:30:\r\n> external/snappy/snappy.cc: In instantiation of 'bool snappy::SnappyScatteredWriter<Allocator>::AppendFromSelf(size_t, size_t) [with Allocator = snappy::SnappySinkAllocator; size_t = long unsigned int]':\r\n> external/snappy/snappy.cc:715:13:   required from 'void snappy::SnappyDecompressor::DecompressAllTags(Writer*) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>]'\r\n> external/snappy/snappy.cc:799:3:   required from 'bool snappy::InternalUncompressAllTags(snappy::SnappyDecompressor*, Writer*, snappy::uint32) [with Writer = snappy::SnappyScatteredWriter<snappy::SnappySinkAllocator>; snappy::uint32 = unsigned int]'\r\n> external/snappy/snappy.cc:1460:78:   required from here\r\n> external/snappy/snappy.cc:1316:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n>      if (PREDICT_TRUE(offset - 1u < op_ptr_ - op_base_ && op_end <= op_limit_)) {\r\n>                       ~~~~~~~~~~~~^~~~~~~~~~~~~\r\n> external/snappy/snappy-stubs-internal.h:80:25: note: in definition of macro 'PREDICT_TRUE'\r\n>  #define PREDICT_TRUE(x) x\r\n>                          ^\r\n> /tmp/ccxBWytY.s: Assembler messages:\r\n> **_/tmp/ccxBWytY.s:389: Error: no such instruction: `kmovq %rdx,%k3'\r\n> /tmp/ccxBWytY.s:391: Error: no such instruction: `kshiftrq $32,%k3,%k2'\r\n> /tmp/ccxBWytY.s:394: Error: no such instruction: `kmovq %k2,%rdx'\r\n> /tmp/ccxBWytY.s:600: Error: no such instruction: `kmovq %rsi,%k1'\r\n> /tmp/ccxBWytY.s:602: Error: no such instruction: `kshiftrq $32,%k1,%k0'\r\n> /tmp/ccxBWytY.s:605: Error: no such instruction: `kmovq %k0,%rsi'_**\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> INFO: Elapsed time: 34.595s, Critical Path: 14.66s\r\n> FAILED: Build did NOT complete successfully\r\n\r\n-----------------------\r\n\r\nThe command I use is:\r\n\r\nbazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures -j 64\r\n\r\nIf I change to --config=mkl, then it compiles fine. Therefore, it seems that the problem is \"--march=native\" (generated by \"--config=opt\"), which forces the snappy module to generate AVX512, however, the generated assembly is not correct for Skylake-X Intel i9 (which supports AVX512). I tried \"--march=skylake-avx512\" and all other AVX512 options like \"-mavx512f\" and similar, and all result in the same error.\r\n\r\nMy goal is to compile tensorflow with Eigen AVX512. Any ideas how this can be done?", "comments": ["Could you share information about your operating system, distro, gcc version, clang version, and all that jazz? I ask because it looks like there might be something wrong with the compiler.", "Operating System:\r\n\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 14.04.5 LTS\r\nRelease:\t14.04\r\nCodename:\ttrusty\r\n\r\nGCC:\r\n\r\ngcc (Ubuntu 5.4.1-2ubuntu1~14.04) 5.4.1 20160904\r\n\r\nCLANG:\r\n\r\nUbuntu clang version 3.4-1ubuntu3 (tags/RELEASE_34/final) (based on LLVM 3.4)\r\nTarget: x86_64-pc-linux-gnu\r\nThread model: posix\r\n\r\n\r\nIt looks like the instruction \"kmovq\" (the one that fails to be identified) belongs to \"AVX512BW\", a category of AVX instructions not supported on Skylake-X i9. These are supported on KNL (Xeon Phi Knight Landings), but not on Skylake-X. \r\n", "@armafire thanks for the report. Are you sure you are using the proper binutils? It looks like a gas error.", "What should I do to verify the gas?", "Maybe now the sources are more stable. \r\nI've manage compile on i9-7900x:\r\nhttps://github.com/thupalo/TF1.4_GPU_CUDA9_AVX512F\r\nCould you please check if this works for you?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have the same issue on Core i9-7900X with r1.4.1\r\n\r\n```bash\r\n$ gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\n```\r\n\r\n```bash\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.3 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n```\r\n\r\n.tf_configure.bazelrc:\r\n> build --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> build --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\n> build --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> build --define PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\n> build --force_python=py3\r\n> build --host_force_python=py3\r\n> build --python_path=\"/usr/bin/python3\"\r\n> test --force_python=py3\r\n> test --host_force_python=py3\r\n> test --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> test --define PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\n> run --define PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n> run --define PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\"\r\n> build --define with_jemalloc=true\r\n> build --define with_gcp_support=true\r\n> build --define with_hdfs_support=true\r\n> build --define with_s3_support=true\r\n> build:xla --define with_xla_support=true\r\n> build:gdr --define with_gdr_support=true\r\n> build:verbs --define with_verbs_support=true\r\n> build --action_env TF_NEED_OPENCL=\"0\"\r\n> build --action_env TF_NEED_CUDA=\"1\"\r\n> build --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\n> build --action_env TF_CUDA_VERSION=\"9.1\"\r\n> build --action_env CUDNN_INSTALL_PATH=\"/usr/lib/x86_64-linux-gnu\"\r\n> build --action_env TF_CUDNN_VERSION=\"7.0.5\"\r\n> build --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,6.1,6.1\"\r\n> build --action_env TF_CUDA_CLANG=\"0\"\r\n> build --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\n> build --config=cuda\r\n> test --config=cuda\r\n> build:opt --cxxopt=-mavx --copt=-mavx\r\n> build:opt --cxxopt=-mavx2 --copt=-mavx2\r\n> build:opt --cxxopt=-mfma --copt=-mfma\r\n> build:opt --cxxopt=-mfpmath=both --copt=-mfpmath=both\r\n> build:opt --cxxopt=-msse4.2 --copt=-msse4.2\r\n> build:opt --cxxopt=-msse4.1 --copt=-msse4.1\r\n> build:opt --cxxopt=-mavx512f --copt=-mavx512f\r\n> build:mkl --define using_mkl=true\r\n> build:mkl -c opt\r\n> build:mkl --copt=\"-DEIGEN_USE_VML\"\r\n> build:monolithic --define framework_shared_object=false\r\n> build --define framework_shared_object=true\r\n> \r\n\r\nbazel command:\r\n```bash\r\n$ bazel build --config=mkl --config=cuda --config=opt //tensorflow/tools/pip_package:build_pip_package --incompatible_load_argument_is_label=false\r\n```\r\n\r\n> ERROR: /home/rjulian/.cache/bazel/_bazel_rjulian/2559b7af9a3bf764c301c93c1a3616a7/external/fft2d/BUILD.bazel:21:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1)\r\n> external/fft2d/fft/fftsg.c: In function 'cftf162':\r\n> external/fft2d/fft/fftsg.c:3028:1: error: insn does not satisfy its constraints:\r\n>  }\r\n>  ^\r\n> (insn 486 485 130 2 (set (reg:DF 50 xmm13 [orig:218 D.7467 ] [218])\r\n>         (plus:DF (reg:DF 50 xmm13 [orig:218 D.7467 ] [218])\r\n>             (reg:DF 56 xmm19 [orig:218 D.7467 ] [218]))) external/fft2d/fft/fftsg.c:2933 796 {*fop_df_comm_mixed}\r\n>      (nil))\r\n> external/fft2d/fft/fftsg.c:3028:1: internal compiler error: in extract_constrain_insn, at recog.c:2246\r\n> Please submit a full bug report,\r\n> with preprocessed source if appropriate.\r\n> See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\n> Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I just tested this on tags v1.5.0, v1.6.0-rc0, v1.6.0-rc1 and master -- -mavx512 is still broken on all of them.", "Just a little housekeeping. Duplicate of #10220"]}, {"number": 12780, "title": "Branch 167401527", "body": "", "comments": ["@martinwicke, thanks for your PR! By analyzing the history of the files in this pull request, we identified @theweiho, @meheffernan and @tensorflower-gardener to be potential reviewers."]}, {"number": 12779, "title": "C++ tensorflow interface is taking too long to terminate and return the calculated graph output.", "body": "Hi All,\r\n\r\nI am loading a trained graph (from python) in c++ as explained in the below links,\r\n\r\n1. https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183\r\n\r\n2. https://medium.com/jim-fleming/loading-a-tensorflow-graph-with-the-c-api-4caaff88463f\r\n\r\nI am using this tf feature in larger c++ application, the working is as follows, main function creates a process that will load the graph, calculate and return the output.  This process will be called every  ~0.1 seconds.  The interface works fine, however it takes rather long (~1 second) to terminate and return the value to main.  I have also timed duration from loading the graph until return, within the tf api, it takes ~0.015 seconds. \r\n\r\nI am unable to figure out the reason for this delay, I am not sure if it is due to c++ createprocess or tf sesion handling, can someone help me to address this issue. \r\n\r\nI have also posted the same question in  stackoverflow, which got down voted a lot, as I am unable to solve this problem I am posting it here too, sorry if it is not the correct platform.\r\n\r\nI am using python 3.5 (Anaconda 4.2.0) on a Windows 7 system. \r\ntensorflow CPU version 1.2.0\r\nbazel 0.5.3\r\nVS 2015\r\n \r\n### Describe the problem\r\nC++ tensorflow interface is taking too long to terminate/return the calcualted graph output. \r\n\r\n### Source code / logs\r\nhttps://github.com/snageshrao/tfCPPExample", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi Justine,\r\nI have already asked this question in stackoverflow and had no luck over there, it seems like the delay is caused due to loading of tensorflow dependencies which are linked during bazel build, can you suggest me how to address this issue. \r\nThank you,"]}, {"number": 12778, "title": "Fix undefined reference to libtensorflow-core.a", "body": "Adjust gcc parameter order to fix linker error. Tested on gcc version 5.4.0 20160609 (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4).", "comments": ["@chenzhiwo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden and @drag0 to be potential reviewers.", "Can one of the admins verify this patch?", "@petewarden could you take a look at the change? Thanks!", "Jenkins, test this please.", "Transient failures, and stuck?\r\n\r\nJenkins, test this please.", "@tensorflow-jenkins test this please"]}, {"number": 12777, "title": "Fix undefined reference to libtensorflow-core.a", "body": "Adjust gcc parameter order to fix linker error. Tested on gcc version 5.4.0 20160609 (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.4) ", "comments": ["@chenzhiwo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden and @drag0 to be potential reviewers.", "Can one of the admins verify this patch?", "Can one of the admins verify this patch?"]}, {"number": 12776, "title": "nest.flatten() does not work with list", "body": "\r\n### System information\r\n== cat /etc/issue ===============================================\r\nLinux mews3153 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\r\nVERSION_ID=\"8\"\r\nVERSION=\"8 (jessie)\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Debian 4.9.2-10) 4.9.2\r\nCopyright (C) 2014 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux mews3153 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.5)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/lib/libipp-intel:/opt/boost/lib/:/usr/lib64/:/mobileye/shared/boost-python2.7/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n/tmp/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\nThe function *tensorflow.contrib.data.python.util.nest.flatten* fails to flatten a list. Given a flat list, it returns a nested list. This causes a problem in dataset_ops.py, when the datasource is created by zipping several datasources and then attempting to create an iterator.\r\n\r\n### Source code / logs\r\n#### Example 1:\r\nfrom tensorflow.contrib.data.python.util import nest\r\nnest.flatten([1,2,3,4])\r\n> [[1, 2, 3, 4]]\r\n\r\nnest.flatten((1,2,3,4)) # works with a tuple\r\n> [1, 2, 3, 4]\r\n\r\n\r\n#### Example 2:\r\nimport tensorflow as tf\r\n\r\ndataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\r\ndataset2 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4]))\r\ndataset3 = tf.contrib.data.Dataset.zip([dataset1, dataset2])\r\n\r\niterator = dataset3.make_initializable_iterator()\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(iterator.initializer)\r\nnext1, next2 = iterator.get_next()\r\n\r\n> running the above code:\r\nTraceback (most recent call last):\r\n  File \"/tmp/tds.py\", line 7, in <module>\r\n    iterator = dataset3.make_initializable_iterator()\r\n  File \"/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 464, in make_initializable_iterator\r\n    return Iterator.from_dataset(self, shared_name)\r\n  File \"/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 97, in from_dataset\r\n    output_types=nest.flatten(dataset.output_types),\r\n  File \"/homes/elyassaf/venv/local/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 1199, in output_types\r\n    ds.output_types for ds in nest.flatten(self._datasets)\r\nAttributeError: 'list' object has no attribute 'output_types'\r\n", "comments": ["In `tensorflow.contrib.data.python.util`,  `list` is not treated as a sequence:\r\nhttps://github.com/tensorflow/tensorflow/blob/64e54423bbffa4161ba2d85dc913e1973ae52cc6/tensorflow/contrib/data/python/util/nest.py#L103-L110\r\n\r\nI don't know enough history about this part though in your sample above I think you could change `list` to `dict`, e.g., \r\n```\r\n>>> from tensorflow.contrib.data.python.util import nest\r\n>>> nest.flatten((1,2,3,4))\r\n[1, 2, 3, 4]\r\n>>> \r\n```\r\n\r\n/cc @mrry ", "Closing for now since this seems to be working as intended but please reopen if you think there's an issue."]}, {"number": 12775, "title": "Fix undefined reference to libtensorflow-core.a", "body": "Adjust parameter order to fix undefined reference error.", "comments": ["@chenzhiwo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden and @drag0 to be potential reviewers.", "Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12774, "title": "Formatting typo.", "body": "", "comments": ["@MtDersvan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @tensorflower-gardener and @saeta to be potential reviewers.", "Can one of the admins verify this patch?"]}, {"number": 12773, "title": "Halted-partial dataset download for speech_commands example renders example ~broken", "body": "If a user attempts to run `tensorflow/examples/speech_commands/train.py` for the first time against a new `data_dir` location and then halts the execution of Python before the download can finish, subsequent attempts to run the script using the same data_dir specification (or lack of specification) will result in Python exiting in error with the gzip libraries complaining about incorrect CRC value.\r\n\r\nThis is because `AudioProcessor.maybe_download_and_extract_dataset(...)` in `tensorflow/examples/speech_commands/input_data.py` only downloads if the file doesn't already exist (sensibly) and assumes that `tarfile.open(...).extractall(...)` will complete error-free (less sensibly) - which it won't on the partially downloaded gzipped-tar asset. \r\n\r\nThis probably merits a documentation inclusion as opposed to a code re-write since it's example code, but that decision is above my pay-grade.\r\n", "comments": ["In my opinion, the exception about incorrect CRC value is sufficient clear for user.", "@facaiy I agree. It seems the except clause of maybe_download_and_extract_dataset does print the URL and the directory, in case the user wants to manually run wget or simply rm the file and try again. Although it might not be super noticeable since it appears to be a log and throw.\r\n\r\nThis might be worth worrying about if it was production code. For example, I worked on Bazel's downloader. We went to indescribable lengths to make it download files in the most reliable, secure, redundant, and high performance manner possible. But I think a little bit of less sophistication makes sense here, to keep the example code simple, and make it easier for some of our users who have to do the download in a more manual way.\r\n\r\nThank you for the report."]}, {"number": 12772, "title": "Using Sparse tensors to apply gradients in BackPropogation. ", "body": "Hi All,\r\n\r\nI am trying to use sparse tensors while applying gradients and i see below error.\r\n **Tensor conversion requested dtype int64 for Tensor with dtype float32: 'Tensor(\"Adam_1/update_conv1_1/weights/sub_2:0\", shape=(), dtype=float32)'**\r\n\r\nI am assuming this to be a bug as the dtype int64 must have been hardcoded for sparse tensors. \r\nIs there any workaround for this issue? \r\n\r\nHere is what I am trying to do:\r\n#################### this function multiplies gradients with prune weights so that gradient updation happens on pruned gradients.\r\n```\r\ndef apply_prune_on_grads(grads_and_vars, sess, dict_n):\r\n     print(\"im inside pply_prune_on_grads\")\r\n     i=0\r\n     for k, v in dict_n.items():\r\n         count=0\r\n         for grad, var in grads_and_vars:\r\n             if var.name == k:\r\n                 op = (var.name).split(\"/\")\r\n                 if op[1] != 'biases:0':\r\n                     v = tf.cast(tf.constant(v), tf.float32)\r\n                     mask_tensor = tf.multiply(v,grad)\r\n                     idx = tf.where(tf.not_equal(mask_tensor, 0))\r\n                     sparse = tf.IndexedSlices(idx,tf.gather_nd(mask_tensor, idx))\r\n                     grads_and_vars[count] = (sparse, var)\r\n             count = count+1\r\n         i=i+1\r\n     return grads_and_vars\r\n\r\ndef prune_weights(weights_prune, sess,threshold = 0.01):\r\n         print(\"im inside prune weights\")\r\n         sparse_weights = {}\r\n         for v in weights_prune:\r\n             value = sess.run(v) \r\n             under_threshold = abs(value) < threshold\r\n             value[under_threshold] = 0\r\n             sess.run(v.assign(value))\r\n             sparse_weights[v.name] = np.logical_not(under_threshold)\r\n         return sparse_weights\r\n```\r\n\r\n############################### The order in which i am calling the above functions in sess.run\r\n```\r\n sparse_weights = prune_weights(variables_to_restore, sess)\r\n grads_and_vars = train_op.compute_gradients(loss, )\r\n grads_and_vars =  apply_prune_on_grads(grads_and_vars,sess,  sparse_weights)\r\n train_step = train_op.apply_gradients(grads_and_vars) \r\n\r\n```\r\n\r\n\r\n\r\n", "comments": ["I could resolve the issue by typecasting tf.gather_nd(mask_tensor, idx) to tf.to_float(tf.gather_nd(mask_tensor, idx))\r\n\r\nbut now im seeing a different error. ValueError: Shapes must be equal rank, but are 1 and 5 for 'GradientDescent_1/update_conv1_1/weights/ScatterSub' (op: 'ScatterSub') with input shapes: [3,3,3,64], [?,4], [?].", "Hi, @raginisharma14 . Could you give a more concise (minimal, self-contained) reproduced code?\r\n\r\nIf I understand correctly, `Tensor conversion requested dtype int64 for Tensor with dtype float32` means that int64 is expected here, not float32. Hence it seems a little surprising for me to use `tf.to_float(xxxx)`.", "Hi @facaiy , in tf.IndexedSlice , the first argument is values and second argument is indices, But I have passed them in the reverse order, I think that is the reason why i was seeing the error.\"Tensor conversion requested dtype int64 for Tensor with dtype float32\". When I changed it to values, indices. I don't see that error anymore.\r\nbut I am still facing the Rank issue while scatter_sub is called in apply_gradients. \r\n\"Shapes must be equal rank, but are 1 and 5 for 'GradientDescent_1/update_conv1_1/weights/ScatterSub' (op: 'ScatterSub') with input shapes: [3,3,3,64], [?,4], [?]\"\r\nIs there a way I can calculate gradients on sparse matrices? Is the issue because scatter  sub does not support multidimensional matrix?\r\n\r\n", "Sorry, @raginisharma14 . I am not familiar with SparseTensor, hence the question is beyond my scope. \r\n\r\nBy the way,  it would be wonderful if the test code could be more concise.", "@facaiy : Here is what I am trying to do: 1. I am applying pruning on to the weights of each layer and then multiplying the pruned weights(boolean matrix) with the computed gradients  so that I get pruned gradients. Now I convert the pruned gradients to a sparse tensor(using tf.IndexedSlices). After this, I would apply these pruned gradients. In tensorflow, while applying gradients you can either pass sparse gradients or dense.  But while the scatter_sub is called it is throwing error  Shapes must be equal rank, but are 1 and 5 for 'GradientDescent_1/update_conv1_1/weights/ScatterSub' (op: 'ScatterSub') with input shapes: [3,3,3,64], [?,4], [?]\r\nIt's failing while calling scatter_sub in the below line\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/training/gradient_descent.py#L67", "I met the same problem with you. and have you solved it? @raginisharma14 ", "Yes, I think it's a usage issue. Feel free to continue commenting on the issue, but I am closing this. Please re-open if you think it's a bug in tensorflow.", "Hi! @raginisharma14 @cuixue I met the problem,too . have you fix it ? "]}, {"number": 12771, "title": "[android demo] add check to SpeechActivity.java for call requestPermissions only with API>=23", "body": "`requestPermissions` call possible only with API>=23\r\n`minSdkVersion` set to 21 for the Project. \r\nHence check required, otherwise, on devices with API 21 and 22 we'll get the error:\r\n```\r\n...\r\n09-02 15:14:21.979 24890-24890/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                     Process: org.tensorflow.demo, PID: 24890\r\n                                                                     java.lang.NoSuchMethodError: No virtual method requestPermissions([Ljava/lang/String;I)V in class Lorg/tensorflow/demo/SpeechActivity; or its super classes (declaration of 'org.tensorflow.demo.SpeechActivity' appears in /data/app/org.tensorflow.demo-1/split_lib_slice_7_apk.apk)\r\n                                                                         at org.tensorflow.demo.SpeechActivity.requestMicrophonePermission(SpeechActivity.java:160)\r\n...\r\n```", "comments": ["Can one of the admins verify this patch?", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Sorry guys. Looks like I've messed up with this PR GitHub branch :( Trying just to add another small commit and rebased on the upstream branch...\r\nJust cleaned up my local branch and push it as new remote branch here: https://github.com/ArtsiomCh/tensorflow/tree/SpeechActivity\r\nShould I force push to this PR branch my cleaned local branch? Or Should I close that PR and reopen another one based on my clean branch? Or may be you know a better solution how to solve that?...\r\nI'm sorry again, unfortunately, I'm not too proficient with Git...", "Thanks for this PR, it's a good catch!\r\n\r\n> Should I force push to this PR branch my cleaned local branch?\r\n\r\nI think this might be the easiest way. I'm not too proficient with git though either, so if this gets too sticky feel free to open a clean PR and link to it in a comment here, we can continue the review there.", "CLAs look good, thanks!\n\n<!-- ok -->", "Just did force push. Looks like it solved the mess. :)\r\nMy second commit 84218e21424d64078c5dc2102443a1d66e4233b6 add call of `startRecording()` for the API<23. Originally it was called only at `onRequestPermissionsResult()` which is never executed when API<23, so Recording thread was never invoked.\r\nUnfortunately, even after that fix, I can see through the debugger (and injecting some additional Log output into the code) that recognition works but do not animate label at `labelsListView` on the Activity screen. \r\nFor me it looks like the problem is somewhere inside that piece of code:\r\n```Java\r\n...\r\n                final View labelView = (View) labelsListView.getChildAt(labelIndex - 2);\r\n                ValueAnimator colorAnimation =\r\n                     ValueAnimator.ofArgb(0x00b3ccff, 0xffb3ccff, 0x00b3ccff);\r\n                colorAnimation.setDuration(750);\r\n                colorAnimation.addUpdateListener(\r\n                    new ValueAnimator.AnimatorUpdateListener() {\r\n                      @Override\r\n                      public void onAnimationUpdate(ValueAnimator animator) {\r\n                        labelView.setBackgroundColor((int) animator.getAnimatedValue());\r\n                      }\r\n                    });\r\n                colorAnimation.start();\r\n...\r\n```", "@tensorflow-jenkins test this please", "@petewarden are we ready to merge this PR? Thanks!", "@petewarden good to go?", "@petewarden ping", "Jenkins, test this please"]}, {"number": 12770, "title": "GPU support on Mac OS X? compile by oneself?", "body": "I'm \u7ea0\u7ed3 to select ubuntu or Hackintosh as my PC.\r\n\r\nNote: As of version 1.2, TensorFlow no longer provides GPU support on Mac OS X.\r\n\r\nCould anyone tell the reason?\r\n\r\nCould I compile tensorflow with gpu[cuda] support by myself? Why or Why not?\r\n\r\n\r\n", "comments": ["The reason is Apple has not released a Mac with an NVIDIA GPU since 2014. We tested with external boxes, but there were too many issues around the drivers and software TF depended on.\r\nTherefore, we decided to drop support for GPUs on MacOS.\r\n\r\nYou may still be able to compile on MacOS with GPU support. But we do not know if it works, what needs to fix the issues you may face. You wont have the official support from TensorFlow development team when trying to use GPUs on MacOS.", "@gunan Would [this](https://developer.apple.com/development-kit/external-graphics/) be a reason to pick up support again?", "It is offered with an AMD GPU, so no.", "\ud83d\ude22"]}, {"number": 12769, "title": "Allow tfcompile_flags to be a list", "body": "This fix tries to fix the issue raised in #12767 where\r\nit was not possible to specify tfcompile_flags as a list:\r\n```\r\ntf_library(\r\n  ...\r\n  tfcompile_flags = [\"--target_cpu='core-avx2'\", \"--xla_enable_fast_math=false\"]\r\n)\r\n```\r\nwill crash upon build with '+' operator applied to incompatible types (select of string, list)\r\n\r\nThis is inconsistent with other rules like 'copts' in cc_binary.\r\n\r\nThe issue is from tfcompile.bzl:\r\n```\r\n\" \" + (tfcompile_flags or \"\")),\r\n```\r\n\r\nThis fix uses `\" \".join(tfcompile_flags or [])` instead so that it\r\nis possible to specify the list.\r\n\r\nThis fix fixes #12767.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @eliben to be potential reviewers.", "@jart Thanks for the review! The PR has been updated with string supported. Please take a look.", "@tensorflow-jenkins test this please", "@jart does the change look good to you?", "Jenkins, test this please.\r\n\r\n@jart it looks like the comments were addressed?", "Jenkins, test this please.\r\n\r\n@yongtang seems like there is a test failure\r\n```\r\nFAIL: //tensorflow/cc:gradients_nn_grad_test (see /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/testlogs/tensorflow/cc/gradients_nn_grad_test/test.log).\r\nINFO: From Testing //tensorflow/cc:gradients_nn_grad_test:\r\n==================== Test output for //tensorflow/cc:gradients_nn_grad_test:\r\nRunning main() from test_main.cc\r\n[==========] Running 11 tests from 1 test case.\r\n[----------] Global test environment set-up.\r\n[----------] 11 tests from NNGradTest\r\n[ RUN      ] NNGradTest.SoftmaxGrad\r\n2017-09-17 21:03:57.572898: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n[       OK ] NNGradTest.SoftmaxGrad (134 ms)\r\n[ RUN      ] NNGradTest.LogSoftmaxGrad\r\n[       OK ] NNGradTest.LogSoftmaxGrad (5 ms)\r\n[ RUN      ] NNGradTest.ReluGrad\r\n[       OK ] NNGradTest.ReluGrad (3 ms)\r\n[ RUN      ] NNGradTest.Relu6Grad\r\n[       OK ] NNGradTest.Relu6Grad (4 ms)\r\n[ RUN      ] NNGradTest.EluGrad\r\n[       OK ] NNGradTest.EluGrad (3 ms)\r\n[ RUN      ] NNGradTest.SeluGrad\r\n[       OK ] NNGradTest.SeluGrad (3 ms)\r\n[ RUN      ] NNGradTest.L2LossGrad\r\n[       OK ] NNGradTest.L2LossGrad (3 ms)\r\n[ RUN      ] NNGradTest.BiasAddGradHelper\r\n[       OK ] NNGradTest.BiasAddGradHelper (6 ms)\r\n[ RUN      ] NNGradTest.Conv2DGrad\r\n[       OK ] NNGradTest.Conv2DGrad (4 ms)\r\n[ RUN      ] NNGradTest.MaxPoolGradHelper\r\n[       OK ] NNGradTest.MaxPoolGradHelper (2 ms)\r\n[ RUN      ] NNGradTest.MaxPoolGradV2Helper\r\ntensorflow/cc/gradients/nn_grad_test.cc:39: Failure\r\nExpected: (max_error) < (2e-4), actual: 0.42407 vs 0.0002\r\n[  FAILED  ] NNGradTest.MaxPoolGradV2Helper (3 ms)\r\n[----------] 11 tests from NNGradTest (170 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 11 tests from 1 test case ran. (170 ms total)\r\n[  PASSED  ] 10 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] NNGradTest.MaxPoolGradV2Helper\r\n\r\n 1 FAILED TEST\r\n```", "I think that was a transient error.\r\n\r\nJenkins, test this please.", "Pint @jart any chance to take a look at the changes? I think the review comments have been addressed.", "Thank you!"]}, {"number": 12768, "title": "Removing /contrib/tensorboard and its references", "body": "The only reference to /contrib/tensorboard was in /contrib/__init.py__ and /contrib/keras/python/keras/callbacks.py.\r\n\r\nBoth of those references did not include and tensorboard plugins that the Tensorboard repo does not have. So I have added multiple if-exists statements inside /keras/callbacks.py and removed /contrib/tensorboard completely.\r\n\r\nThe reason for this, some guy at Arch Linux repos have added a \"rm -rf\" to his PKGBUILD and now me and anybody who has the latest version of Tensorflow on Arch Linux is unable to use it. I thought maybe we can clear up the confusion since Kenser will be included with Tensorflow by default.\r\n\r\nLink to my ticket: [Arch Linux - Tensorflow Ticket](https://bugs.archlinux.org/task/55477?project=5&status%5B0%5D=open) ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I don't think I have fully understood the problem, but I think you want to make this change to master, not 1.3.", "I have made a new PR to the master branch at https://github.com/tensorflow/tensorflow/pull/12782.\r\n\r\nThe reason behind the PR is that having another tensorboard in tensorflow repo seems to create confusion here and there. It is not a bug, just a recommendation to clean it up."]}, {"number": 12767, "title": "tf_library syntax error: '+' operator applied to incompatible types (select of string, list)", "body": "Forgive me if I'm missing something obvious here (new to Bazel) but the tf_library Bazel rule seems to assume that `tfcompile_flags` is a string, while similar rules allow lists (cc_binary's copts for example). This is a gotcha that probably could be fixed easily.\r\n\r\n# Example\r\n```\r\ntf_library(\r\n  name = \"graph\",\r\n  cpp_class = \"Graph\",\r\n  graph = \"graph.pb\",\r\n  config = \"graph.config.pb\",\r\n  tfcompile_flags = [\"--target_cpu='core-avx2'\", \"--xla_enable_fast_math=false\"]\r\n)\r\n```\r\nwill crash upon build with `'+' operator applied to incompatible types (select of string, list)`\r\n\r\nThe solution is to manually concatenate options in the BUILD file: \r\n```\r\ntf_library(\r\n  name = \"graph\",\r\n  cpp_class = \"Graph\",\r\n  graph = \"graph.pb\",\r\n  config = \"graph.config.pb\",\r\n  tfcompile_flags = \"--target_cpu='core-avx2' --xla_enable_fast_math=false\"\r\n)\r\n```", "comments": ["@carlthome Are you referring to tfcompile.bzl (below)?\r\nhttps://github.com/tensorflow/tensorflow/blob/da3aa93758508a37741dad1d8b28e62782330171/tensorflow/compiler/aot/tfcompile.bzl#L148\r\n\r\nI am new to Bazel as well though I think using `\" \".join(tfcompile_flags or [])` instead will allows `tfcompile_flags` to be a list.\r\n", "Created a PR #12769 for this issue.", "I support making this change, so long as it maintains support for passing a string and the items in the list are shell quoted. I left that feedback in the review that @yongtang was generous enough to send us."]}, {"number": 12766, "title": "Update monitors.py", "body": "Refactor contrib", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@martinwicke does this look good?"]}]