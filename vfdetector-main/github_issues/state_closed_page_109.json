[{"number": 51795, "title": "tf.GradientTape not working properly.", "body": "Recently I made new network & trained it with tf.Gradient.\r\n\r\nBut losses didn't decrease.\r\n\r\nSo I tested in different examples in [tensorflow tutorial](https://www.tensorflow.org/tutorials/images/cnn?hl=en)\r\n\r\nWhen I tested, I used tensorflow turorial code except training part.\r\n\r\nPlease let me know if there is any wrong usage.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 20.04 \r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version (use command below): 2.4.0 / 2.6.0\r\n- Python version: Python 3.8\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.4 \r\n- GPU model and memory: GeForce 1080 Ti * 2\r\n\r\nv2.6.0-rc2-32-g919f693420e 2.6.0\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2021-09-02 19-45-42](https://user-images.githubusercontent.com/18378446/131830787-5992bee1-7967-403b-b75e-81c74c0cc27e.png)\r\n\r\n**Describe the expected behavior**\r\n![Screenshot from 2021-09-02 19-46-27](https://user-images.githubusercontent.com/18378446/131830878-9d5a2349-4cff-4bd5-bced-48138beefb60.png)\r\n", "comments": ["@uoyssim In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Thanks for the quick response :)\r\n\r\n## tf.GradientTape code\r\n```python\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\n\r\nseq_model = models.Sequential()\r\nseq_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nseq_model.add(layers.MaxPooling2D((2, 2)))\r\nseq_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nseq_model.add(layers.MaxPooling2D((2, 2)))\r\nseq_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nseq_model.add(layers.Flatten())\r\nseq_model.add(layers.Dense(64, activation='relu'))\r\nseq_model.add(layers.Dense(10))\r\n\r\ntrain_loss_results = []\r\ntrain_accuracy_results = []\r\n\r\nnum_epochs = 10\r\ntotal_size = len(train_images)\r\nbatch_size = 128\r\nn_steps = int(total_size / batch_size)\r\n\r\nfor epoch in range(num_epochs):\r\n    epoch_loss_avg = tf.keras.metrics.Mean()\r\n    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\r\n\r\n    np.random.seed(i)\r\n    np.random.shuffle(train_images)\r\n    np.random.seed(i)\r\n    np.random.shuffle(train_labels)\r\n    \r\n    for i in range(n_steps):\r\n        upper = min((i+1) * batch_size, total_size)\r\n        x = train_images[i*batch_size:upper]\r\n        y = train_labels[i*batch_size:upper]\r\n        \r\n        with tf.GradientTape() as tape:\r\n            preds = seq_model(x, training=True)\r\n            loss = loss_object(y, preds)\r\n        \r\n        grads = tape.gradient(loss, seq_model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, seq_model.trainable_variables))\r\n\r\n        epoch_loss_avg(loss) \r\n        epoch_accuracy(y,seq_model(x))\r\n\r\n    train_loss_results.append(epoch_loss_avg.result())\r\n    train_accuracy_results.append(epoch_accuracy.result())\r\n\r\n    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch+1,\r\n                                                            epoch_loss_avg.result(),\r\n                                                            epoch_accuracy.result()))\r\n```\r\n\r\n## fit code\r\n\r\n```python\r\nfrom tensorflow.keras import datasets, layers, models\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\r\n               'dog', 'frog', 'horse', 'ship', 'truck']\r\n\r\nseq_model2 = models.Sequential()\r\nseq_model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nseq_model2.add(layers.MaxPooling2D((2, 2)))\r\nseq_model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nseq_model2.add(layers.MaxPooling2D((2, 2)))\r\nseq_model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nseq_model2.add(layers.Flatten())\r\nseq_model2.add(layers.Dense(64, activation='relu'))\r\nseq_model2.add(layers.Dense(10))\r\n\r\nseq_model2.compile(optimizer='adam', \r\n                             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                             metrics=['accuracy'])\r\n\r\nseq_model2.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\r\n```", "@uoyssim Thank you for the response! \r\nPlease post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@uoyssim Could you please close this ticket if you have posted the issue in Keras repo ?Thank you!", "[Keras Issues](https://github.com/keras-team/keras/issues/15306) is closed.\r\n\r\nIt is my mistake. \r\n\r\nWhen I use tf.keras.losses.SparseCategoricalCrossentropy,I missed 'from_logits=True'\r\n\r\nThank you for your helps!\r\n"]}, {"number": 51793, "title": "Unable to restore the model with shared ops using from_config()", "body": "**System information**\r\n- OS Platform and Distribution:  Linux Ubuntu 18.04\r\n- TensorFlow installed from pip\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.7.11\r\n- CUDA Version: 11.2\r\n-  Driver Version: 460.80\r\n\r\n**Describe the current behavior**\r\nCan not load model with shared ops from config. During loading, the outputs of the shared operation are placed in the wrong positions.\r\n\r\n**Describe the expected behavior**\r\nThe original model and the restored model are the same\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ndef get_test_model():\r\n    input_1 = tf.keras.Input(shape=(5, 5, 2))\r\n    input_2 = tf.keras.Input(shape=(5, 5, 2))\r\n    image_shape = tf.keras.Input(shape=(1, 2), name='image_shape')\r\n\r\n    clips = []\r\n    relu = tf.keras.layers.ReLU()\r\n    for inp in [input_1, input_2]:\r\n        dy = inp[..., 0:1]\r\n        dx = inp[..., 1:2]\r\n\r\n        y = relu(relu(dy) + dx)\r\n        x = relu(relu(dx) + dy)\r\n        cat = tf.concat([y, x], -1)\r\n\r\n        height, width = tf.unstack(image_shape, axis=-1)\r\n        max_length = tf.stack(\r\n            [relu(height), relu(width)], axis=-1)\r\n\r\n        clip = tf.math.minimum(cat, max_length)\r\n        clips.append(clip)\r\n\r\n    out = tf.concat(clips, axis=1)\r\n    return tf.keras.Model(inputs=[input_1, input_2, image_shape], outputs=out)\r\n\r\n\r\ninput_1 = np.random.rand(1, 5, 5, 2)\r\ninput_2 = np.random.rand(1, 5, 5, 2)\r\nimage_shape = np.random.rand(1, 1, 2)\r\ninputs = [input_1, input_2, image_shape]\r\n\r\nmodel = get_test_model()\r\n\r\nref = model.predict(inputs)\r\nprint(ref.shape)  # succsess (1, 10, 5, 2)\r\n\r\nrestored_model = tf.keras.Model.from_config(model.get_config())  # failed\r\nout = restored_model.predict(inputs)\r\nprint(out.shape)\r\n```\r\noriginal frozen graph\r\n![image](https://user-images.githubusercontent.com/22346860/131807492-4cc0a0e4-c94b-4928-9deb-c96945770510.png)\r\n\r\n\r\nError with restore model from config:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1880, in _create_c_op\r\n    c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 4 but is rank 2 for '{{node tf.concat/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, Placeholder_1, tf.concat/concat/axis)' with input shapes: [?,5,5,1], [?,1], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_bug.py\", line 42, in <module>\r\n    restored_model = tf.keras.Model.from_config(model.get_config())  # failed\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/training.py\", line 2397, in from_config\r\n    functional.reconstruct_from_config(config, custom_objects))\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/functional.py\", line 1283, in reconstruct_from_config\r\n    process_node(layer, node_data)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/functional.py\", line 1231, in process_node\r\n    output_tensors = layer(input_tensors, **kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 977, in __call__\r\n    input_list)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1115, in _functional_construction_call\r\n    inputs, input_masks, args, kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 848, in _keras_tensor_symbolic_call\r\n    return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 888, in _infer_output_signature\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/layers/core.py\", line 1350, in _call_wrapper\r\n    return self._call_wrapper(*args, **kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/keras/layers/core.py\", line 1382, in _call_wrapper\r\n    result = self.function(*args, **kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1769, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1228, in concat_v2\r\n    \"ConcatV2\", values=values, axis=axis, name=name)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 750, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 601, in _create_op_internal\r\n    compute_device)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3569, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2042, in __init__\r\n    control_input_ops, op_def)\r\n  File \"/home/miniconda3/envs/tf2.6.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1883, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Shape must be rank 4 but is rank 2 for '{{node tf.concat/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, Placeholder_1, tf.concat/concat/axis)' with input shapes: [?,5,5,1], [?,1], [].\r\n```\r\n", "comments": ["@l-bat,\r\n\r\nI see that the error occurs in `2.6.0` whereas with `tf-nightly` it works fine. Please take a look at the [gist here](https://colab.research.google.com/gist/sanatmpa1/4dcda01fe2bbafe77f98d787e0feee8e/51793.ipynb). Thanks!", "@sanatmpa1\r\nGreat!\r\nCan we close the issue when TF 2.7.0 is released?", "@l-bat,\r\n\r\nAs its working in nightly, can you close this issue and open it again if its still an issue on later release. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51793\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51793\">No</a>\n"]}, {"number": 51792, "title": "weighted_moments produces NaNs when weights are all zeros", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): git\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): 4.2\r\n- GCC/Compiler version (if compiling from source): gcc-10\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.nn.weighted_moments` produces NaNs when all weights are zeros.\r\n**Describe the expected behavior**\r\nCorrect result in this case should be zeros.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\nDo not divide by 0 unless you are Chuck Norris.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nx = tf.random.uniform((5, 3))\r\nw = tf.zeros((5, 1))\r\n\r\ntf.nn.weighted_moments(x, axes=0, frequency_weights=w)\r\n\r\n(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([nan, nan, nan], dtype=float32)>,\r\n <tf.Tensor: shape=(3,), dtype=float32, numpy=array([nan, nan, nan], dtype=float32)>)\r\n\r\n\r\n```\r\n\r\n\r\n", "comments": ["Hi @eli-osherovich !You should provide only positive weights  in w , Here 0's are not positive weights.\r\nReference -https://www.tensorflow.org/api_docs/python/tf/nn/weighted_moments#args", "I understand the rationale, yet, I believe that all-zeros is still a valid case.  Obviously, the weights are expected to be non-negative.", "Hi @Saduf2019 , Could you look into this please . providing[ gist](https://colab.research.google.com/gist/mohantym/bfc557013a0db04183e0c2d2ba4cec79/github_51792.ipynb) for reference . issue is replicating in 2.5 ,2.6 and nightly", "@eli-osherovich \r\nThe weighted_moments function sums your weights with zero vector and  then determines the divisor by taking a reciprocal of the broadcasted vector. In this case, your sum will be zero and since the reciprocal of zero is infinity, nans are appearing.\r\nDon't use zero weights, logically it makes no sense, instead 1's weight every element equally.\r\nFor any further queries i suggest to open an issue at tf [discussion forum](https://discuss.tensorflow.org/) as there is a larger community to support/respond.", "@Saduf2019 \r\nI know why it happens. Let me assure you zeroes are perfectly logical. I do not see why zeros should be replaced with ones....\r\n\r\nP. S. \r\nIt is a bit difficult to understand is this the TF team answer or a recommendation from a fellow user....", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51792\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51792\">No</a>\n", "The PR was reverted :( The issue still exists."]}, {"number": 51791, "title": "[PluggableDevice] Add TF_OpkernelConstruction_GetAttrNamesList", "body": "Attribute names, along with their corresponding values, could be used in TF 1.15 by devices to compute hashes of kernels and uniquely identify them for caching purposes. I believe this is an important feature to keep because some device backends have relatively slow kernel creation times and need a way to know whether a kernel that was created previously can be reused or not.", "comments": []}, {"number": 51790, "title": "Transfer learning ResNet mode performance drop in recent versions", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04, 18.04, 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\n2.0.0, 2.2.0, 2.4.0, 2.5.0, 2.6.0\r\n- Python version:\r\n3.7, 3.8, 3.9\r\n- Bazel version (if compiling from source):\r\nNo\r\n- GCC/Compiler version (if compiling from source):\r\nNo\r\n- CUDA/cuDNN version:\r\n10.2, 11.1\r\n- GPU model and memory:\r\n1660 Super (6GB), 3070 Laptop (8GB), 3090 (24GB), whatever google's colab provides\r\n\r\n**Describe the current behavior**\r\nResNet (50, 101 and 152) perform at about 40-50% Accuracy and F1 score after transfer learning.\r\n**Describe the expected behavior**\r\n6-8 months ago, using the same exactly dataset and script, the same models performed above 75% at the same metrics.\r\nThe initial good results were obtained using google colab.\r\nNow, I have tested the **same code and dataset** in google colab using all the tensorflow versions mentioned above.\r\nHave also tested on Ubuntu 16.04 with CUDA 10.2 and CUDNN 7.6 and Ubuntu 18.04 and 20.04 with CUDA 11.1 CUDNN 8.2.\r\n\r\nThe same results (old and new) are consistent (+- 1%) for all the other models I have used (All DenseNets, VGG16 and 19 and MobileNet).\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\nNo\r\n- Briefly describe your candidate solution(if contributing):\r\nNot sure\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nDataset is private so I can't.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nTransfer learning process follows the one provided [by keras](https://keras.io/guides/transfer_learning/).\r\n", "comments": ["@GCidd Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51790\">No</a>\n"]}, {"number": 51788, "title": "fix op does not dominate this use after mhlo-fusion pass", "body": "test: tests/mhlo-fusion.mlir: fused_unfused_mix", "comments": ["@joker-eph Sorry, What do I need to do? Why is the check display always pending?", "You don't need to do anything, it is in the queue for merging, it has to go through our internal integration testing (this is \"mostly automated\", but not a super fast process): I expect it'll get merged today.", "> You don't need to do anything, it is in the queue for merging, it has to go through our internal integration testing (this is \"mostly automated\", but not a super fast process): I expect it'll get merged today.\r\n\r\nThank you for your answer, Programmer Daniel."]}, {"number": 51787, "title": "tf.nn.max_pool_with_argmax to support multiple dimensions (ND)", "body": "Currently, there is a function called [tf.nn.max_pool_with_argmax](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax). \r\nThis function is useful to, for example, implement [MaxUnpooling](https://www.oreilly.com/library/view/hands-on-convolutional-neural/9781789130331/6476c4d5-19f2-455f-8590-c6f99504b7a5.xhtml).\r\n\r\n\r\nHowever, it would be great if the function supported several dimensions, not only 2D images.  [`tf.nn.max_pool`](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool) for example supports any dimensionality. \r\n\r\nThis feature will enable the implementation of ND max unpooling.\r\nIn particular, it will help me implement the [feature request](https://github.com/NEGU93/cvnn/issues/10) asked on my repository.\r\n\r\n\r\n", "comments": ["Hi @NEGU93 ,Please fill the template for feature request too. Thanks! \r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 51786, "title": "Type Error: __init__() got an unexpected keyword argument 'epochs'", "body": "\r\n![1](https://user-images.githubusercontent.com/26819449/131669104-e1037302-ddde-4eda-9ecf-bfd57a583653.JPG)\r\n![2](https://user-images.githubusercontent.com/26819449/131669108-9b95b63c-1054-4195-95bc-445a29095346.JPG)\r\n![3](https://user-images.githubusercontent.com/26819449/131669115-228d7a39-91f2-4aa3-9482-86fc1dc9944e.JPG)\r\n![4](https://user-images.githubusercontent.com/26819449/131669118-a39aab97-1181-4cb3-b600-4284912004e8.JPG)\r\n \r\nI tried to solve this particular error. I was trying to implement a particular library from Github.\r\ntensorflow=2.2 \r\nkeras=2.4.3\r\nI am only using these two libraries in this Code.\r\n\r\nAnd please also tell me which documentation to refer to, it helps.\r\nThank You.\r\n\r\n\r\n\r\n", "comments": ["@starboyvarun In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This is the code snippet:\r\nhttps://paste.ofcode.org/9F3526z5nUnrHgLNencmY6\r\nPlease also tell me particular documents to refer to solve this particular issue.\r\nThankYou.", "@starboyvarun We see that the issue is more specific to Janggu.Please post this issue in Janggu repo to get the right help!Thanks!", "\r\n@sushreebarsa you must be getting other errors when compiling it. because you haven't done the previous import.\r\nBut the error I have reported here is related to TensorFlow or Keras. This is a code snippet.\r\nAlso, tell me why do we get such errors and also give the reference.\r\nThanks!"]}, {"number": 51784, "title": "Repeat weights in dense layer / Forum registration not possible", "body": "Hello I tried to ask this question in the Forum, but somehow the Forum won't let me register saying something about timeout even if I refresh the page for registering. So instead I post this here as feature request.\r\n\r\nWhat I want to achieve is that say for a 3x8x8 input for a dense layer I want the same 8x8 weights to be trained and repeated over the whole input. Is there a condition I can set for the training to repeat the weights multiple times so I train 8x8 weights over a Nx8x8 input instead of training Nx8x8 individual weights over the input?", "comments": ["@Sur3,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Also can you please elaborate about your Feature and please specify the Use Cases for this feature. Thanks!", "**System information**\r\n- TensorFlow version (you are using): tensorflow-2.5.0\r\n- Are you willing to contribute it (Yes/No): maybe\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nTraining a dense layer on say 3x4 inputs results in 3x4 weights for every output:\r\n```\r\na b c d\r\ne f g h\r\ni j k l\r\n```\r\n\r\nIt would be nice to tell the layer that some weights are equal, meaning for example with a repeating pattern of four weights:\r\n```\r\na b c d\r\na b c d\r\na b c d\r\n```\r\n\r\n**Who will benefit with this feature?**\r\n\r\nThis greatly reduces the number of weights in cases where the inputs for a layer have similar properties, e.g. in my case training on configurations of thousand water molecules where every molecule is a 8x8 matrix, I could calculate them with only 8x8=64 weights for every output on the first layer instead of 64000 weights for every output which should greatly increase training speed, accuracy and inference.\r\n\r\n**Will this change the current api? How?**\r\nSomehow specifying a condition for the layer to repeat specific weights.\r\nBut I just realized this could be seen as a subset of convolutional layers with valid padding where the filter is strided by the filters size, so I will probably try to implement it that way. Though this is not a real convolution as no sliding is happening as the stride is exactly the filter size, therefore this could get more optimized than a convolutional layer.\r\n**But implementing it as a convolutional layer is of course a convenient workaround, so I guess this issue could be closed for now.\r\nBut still I cant register for the Forum should I still keep the issue open for that regard?**", "Ok registration in the Forum did now work as well, therefore closing this issue for now."]}, {"number": 51783, "title": "TypeError: 'NoneType' object is not callable", "body": "\r\n![3](https://user-images.githubusercontent.com/26819449/131661895-8f6100c7-9d01-43c4-bf4f-034853cafaa4.JPG)\r\n\r\n![2](https://user-images.githubusercontent.com/26819449/131661655-91576b86-8977-475a-b107-13dad63be3d1.JPG)\r\n\r\n![1](https://user-images.githubusercontent.com/26819449/131661662-97ec164b-8596-4b7b-b7ab-e17b39b65d82.JPG)\r\nThe error is in this part only before that is nothing just library import.\r\nThis is a bug I think.\r\n\r\n", "comments": ["Hi @starboyvarun !\r\nCould you provide the code  snippet for \"trainseq\" in `model.fit(trainseq,epochs=100)` operation.\r\nI have also  gone through this  [Janggu documentation](https://janggu.readthedocs.io/en/latest/tutorial.html#fit-a-neural-network-on-dna-sequences) on fitting genomic data in Janggu. have explained for the model.fit operation in an example in following [GIST](https://colab.research.google.com/gist/mohantym/5cb3c61f95b40a50f82b470574d9375a/github_51789.ipynb).\r\n\r\nThanks!\r\n\r\n\r\n\r\n", "@mohantym Thank you so much. Can you please change type: support to type: Bug.\r\nThanks!", "```\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import AveragePooling2D\r\nfrom janggu import inputlayer\r\nfrom janggu import outputconv\r\nfrom janggu import DnaConv2D\r\nfrom janggu.data import ReduceDim\r\n\r\n\r\n# load the dataset which consists of\r\n# 1) a reference genome\r\nREFGENOME = resource_filename('janggu', 'resources/pseudo_genome.fa')\r\n# 2) ROI contains regions spanning positive and negative examples\r\nROI_FILE = resource_filename('janggu', 'resources/roi_train.bed')\r\n# 3) PEAK_FILE only contains positive examples\r\nPEAK_FILE = resource_filename('janggu', 'resources/scores.bed')\r\n\r\n# DNA sequences are loaded directly from the reference genome\r\nDNA = Bioseq.create_from_refgenome('dna', refgenome=REFGENOME,\r\n                                   roi=ROI_FILE,\r\n                                   binsize=200)\r\n\r\n# Classification labels over the same regions are loaded\r\n# into the Coverage dataset.\r\n# It is important that both DNA and LABELS load with the same\r\n# binsize, stepsize to ensure\r\n# the correct correspondence between both datasets.\r\n# Finally, the ReduceDim dataset wrapper transforms the 4D Coverage\r\n# object into a 2D table like object (regions by conditions)\r\nLABELS = ReduceDim(Cover.create_from_bed('peaks', roi=ROI_FILE,\r\n                               bedfiles=PEAK_FILE,\r\n                               binsize=200,\r\n                               resolution=None), aggregator='mean')\r\n\r\n\r\n# 2. define a simple conv net with 30 filters of length 15 bp\r\n# and relu activation.\r\n# outputconv as opposed to outputdense will put a conv layer as output\r\n@inputlayer\r\n@outputdense('sigmoid')\r\ndef double_stranded_model(inputs, inp, oup, params):\r\n    with inputs.use('dna') as layer:\r\n        # The DnaConv2D wrapper can be used with Conv2D\r\n        # to scan both DNA strands with the weight matrices.\r\n        layer = DnaConv2D(Conv2D(params[0], (params[1], 1),\r\n                                 activation=params[2]))(layer)\r\n\r\n    output = GlobalAveragePooling2D(name='motif')(layer)\r\n    return inputs, output\r\n\r\n\r\n# 3. instantiate and compile the model\r\nmodel = Janggu.create(template=double_stranded_model,\r\n                      modelparams=(30, 15, 'relu'),\r\n                      inputs=DNA, outputs=LABELS)\r\nmodel.compile(optimizer='adadelta', loss='binary_crossentropy',\r\n              metrics=['acc'])\r\n\r\n# 4. fit the model\r\nmodel.fit(DNA, ReduceDim(LABELS, epochs=100)\r\n\r\n```\r\n\r\n@mohantym actually I am trying to run this from the Janggu documentation, but it gives several errors related to tensor flow and some other errors too\r\n.Can you try running in the gist and let me know if this runs.\r\n\r\n![9](https://user-images.githubusercontent.com/26819449/133374059-3f9a49fb-62f1-40ab-a321-5e3f60dc6d2e.JPG)\r\n![9 1](https://user-images.githubusercontent.com/26819449/133374075-6ec205b6-9fa1-45b8-91f7-478fa5c8b011.JPG)\r\n\r\nThanks!\r\n\r\n", "Should I close this issue now?\r\n@mohantym ", "@starboyvarun ! Sorry for the late response . Actually tried to replicated above issue , was getting  different error. Attaching [Gist](https://colab.research.google.com/gist/mohantym/c426a30b58b0d295c1bdf4d16ae5b259/github_51783.ipynb) for reference . Took time to check your BEDtools issue too.Can you  please share a Colab gist from your end with above error stack trace?", "@mohantym \r\nYa, I attached the gist can you have a look [Gist](https://colab.research.google.com/gist/starboyvarun/9ab39f94d3c33afd039a9ab0b774c221/14sept.ipynb).\r\nI am trying to implement the last parts of this example in my example(Examples shown in link). [Link](https://github.com/BIMSBbioinfo/janggu/blob/master/src/examples/janggu_convnet_examples.ipynb)\r\nI saw your gist I have already solved the issue. Now can you tell me how can I do the last 3 parts of the link?\r\n\r\nThank you.!", "@mohantym  Did you check?", "@mohantym Did you see it ?? Can you what should I do?", "Hi @starboyvarun , Please open a separate issue  to proceed further and Feel free to close this one . Thank you!", "ok, thanks! @mohantym ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51783\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51783\">No</a>\n"]}, {"number": 51782, "title": "`distutils` is deprecated in Python 3.10 #51776", "body": "The `distutils` is deprecated in Python 3.10.\r\n\r\nAs of #51776, In `python_configure.bzl`, the deprecation message will be printed prior to the include path, causing error on return.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F51782) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 51781, "title": "Tflite GPU OpenGL Delegate Allow HWC Const Add", "body": "Allow addition (Add) of constant HWC tensor in OpenGL GPU delegate of Tensorflow Lite.", "comments": ["@impjdi If you want to refactor code of add.cc to your new coding conventions, like e.g. in mul.cc, this is an independent issue. Until that is done, I do not feel like it to have a single code file with multiple coding conventions.", "While I agree in general that your change should only modify the least amount of change necessary, it's also true that your code adds decent amount of lines and make it harder to read with 3 variables instead of 2.  That should probably go together.", "Code is refactored as requested. The readability of the commit is now 0.", "lets see if third time's the charm", "Bazel GPU CI build failed probably due to missing new dep, hopefully fixed. (I build using cmake)", "Has there been some issue with copybara? Looks like all pull requests were affected but now it should be fixed.", "I received your PR in the internal script which I have to approve as well, but it only includes the `add.cc` change without the change in the `BUILD` file.  I'm not familiar with the github UI, but can you make sure that both files are exported?", "Any idea on what exactly am I supposed to do? I suppose it is the copybara's task to copy the changed files into the internal system. Maybe rerun the copybara task? Other approach would be to make new PR with the final changes in a single commit.", "@gbaned Do you know what needs to be done?", "Possibly the same issue happened at the same time with [#PR51866](https://github.com/tensorflow/tensorflow/pull/51866), fixed by @cheshire (by rerunning kokoro:force-run?)."]}, {"number": 51780, "title": "2021-09-01 16:39:11.050592: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operatiWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001A3554913A0> and will run it as-is. Please report this to the TensorFlow team. To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@githlam In order to expedite the trouble-shooting process,Can you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51779, "title": "SavedModel imported in ML.NET throws vacuous error", "body": "I create a model in Python and save it with `tf.saved_model.save(model, model_save_path)` into a `.pb` file.\r\nThen I load the model in ML.NET, create a pipeline, and make a prediction with the following code:\r\n```csharp\r\nMLContext ml_context = new MLContext();\r\nTensorFlowModel tensor_flow_model = ml_context.Model.LoadTensorFlowModel(model_save_path);\r\nIEstimator<ITransformer> pipeline = tensor_flow_model.ScoreTensorFlowModel(new[] { output }, new[] { input_1, input_2, input_3, input_4 }, addBatchDimensionInput: true);\r\nvar estimator = pipeline.Fit(data);\r\nvar transformed_values = estimator.Transform(data);\r\nvar out_scores = ml_context.Data.CreateEnumerable<Prediction>(transformed_values, reuseRowObject: false);\r\nforeach (var prediction in out_scores)\r\n{\r\n    int num_classes = 0;\r\n    foreach (var class_score in prediction.StatefulPartitionedCall)\r\n    {\r\n        Console.WriteLine($\"Class #{num_classes++} score = {class_score}\");\r\n    }\r\n    Console.WriteLine(new string('-', 10));\r\n}\r\n```\r\nThe first `foreach` loop throws an error, which I do not understand. The error message reads:\r\n```\r\n2021-09-01 09:38:03.364008: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at partitioned_function_ops.cc:118 : Invalid argument: Expected input[1] == 'TensorArrayV2_1/element_shape:output:0' to be a control input.\r\n    In {{node TensorArrayV2Stack/TensorListStack}}\r\n```\r\nWhat does the error message mean? What is happening? And how do I fix it?\r\n\r\nEDIT:\r\nI just found out, that `out_scores` is empty at the time reaching `foreach (var prediction in out_scores)` and therefore causing this error.", "comments": ["Reason seems to be my data pipeline, it outputs `float[]` filled with `null` instead of actuall values (even `0.0f` would be okay).\r\nI need to further investigate this.\r\nClosing this issue. "]}, {"number": 51778, "title": "When I check the TensorBoard for a Worker of ParameterServer traning, I can't see any send/recv or push/pull op to the PS? Is this normal?", "body": "Hi, developers of tensorflow.\r\nThe environment, code are listed below:\r\n\r\nTF version: nightly\r\nOS: ubuntu16.04\r\ncuda: 11.0\r\n\r\n\r\nWorker code:\r\n\r\n```\r\nimport multiprocessing\r\nimport os\r\nimport json\r\nimport random\r\nimport portpicker\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\r\n\r\ndef create_in_process_cluster(num_workers, num_ps):\r\n  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n\r\n  cluster_dict = {}\r\n  cluster_dict[\"worker\"] = [\"ip:port\"]\r\n  if num_ps > 0:\r\n    cluster_dict[\"ps\"] = [\"ip:port\"]\r\n  print(\"==========================\", cluster_dict[\"worker\"], cluster_dict[\"ps\"])\r\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n\r\n  # Workers need some inter_ops threads to work properly.\r\n  worker_config = tf.compat.v1.ConfigProto()\r\n  if multiprocessing.cpu_count() < num_workers + 1:\r\n    worker_config.inter_op_parallelism_threads = num_workers + 1\r\n  \r\n  os.environ[\"TF_CONFIG\"] = json.dumps({\r\n    \"cluster\": {\r\n        \"worker\": cluster_dict[\"worker\"],\r\n        \"ps\": cluster_dict[\"ps\"],\r\n    },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n  })\r\n  cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n  return cluster_resolver\r\n\r\n# Set the environment variable to allow reporting worker and ps failure to the\r\n# coordinator. This is a workaround and won't be necessary in the future.\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n\r\nNUM_WORKERS = 1\r\nNUM_PS = 1\r\n\r\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\nif cluster_resolver.task_type in (\"worker\", \"ps\"):\r\n    server = tf.distribute.Server(\r\n        cluster_resolver.cluster_spec(),\r\n        job_name=cluster_resolver.task_type,\r\n        task_index=cluster_resolver.task_id,\r\n        protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n        start=True)\r\n\r\n    \r\nprint(\"==========================\")\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n    \r\ndef dataset_fn(input_context):\r\n  global_batch_size = 64\r\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\r\n\r\n  x = tf.random.uniform((10, 10))\r\n  y = tf.random.uniform((10,))\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\r\n  dataset = dataset.shard(\r\n      input_context.num_input_pipelines,\r\n      input_context.input_pipeline_id)\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.prefetch(2)\r\n\r\n  return dataset\r\n\r\ndc = tf.keras.utils.experimental.DatasetCreator(dataset_fn)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\r\n\r\nmodel.compile(tf.keras.optimizers.SGD(), loss='mse', metrics=['accuracy'], steps_per_execution=10)\r\n\r\n\r\nworking_dir = './my_working_dir'\r\nlog_dir = os.path.join(working_dir, 'log')\r\nckpt_filepath = os.path.join(working_dir, 'ckpt')\r\nbackup_dir = os.path.join(working_dir, 'backup')\r\ntb_dir = os.path.join(working_dir, 'tb')\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\r\n    tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir),\r\n    tf.keras.callbacks.TensorBoard(log_dir=tb_dir, profile_batch = 100000000)\r\n]\r\n\r\nmodel.fit(dc, epochs=10, steps_per_epoch=20, callbacks=callbacks)\r\n```\r\n\r\nThe graph of tensorboard:\r\n\r\n![graph](https://user-images.githubusercontent.com/14342657/131599322-5e9ca91b-c49c-4a2c-9f79-171cb828c321.PNG)\r\n\r\n\r\nI can't confirm whether this Worker process is interacting with PS to update the parameters in each step.\r\nAre the communication operations not compiled into the graph?\r\n\r\n\r\n", "comments": ["Hi @zpcalan ,Could you please raise the issue in [TensorBoard ](https://github.com/tensorflow/tensorboard/issues/new/choose)repo. ", "OK, I'll try later.", "Hi @mohantym, this seems to be a question for **TensorFlow** rather than **TensorBoard**, since TensorBoard just _renders_ whatever is in the op graph it receives, it doesn't control what distribution-related ops are included. Thanks!\r\n\r\n\r\n\r\n", "Hi @zpcalan @sanatmpa1 , Could you please look at this issue. Was getting error in TF 2.6 and TF nightly (2.7.0-dev20210903) , providing [gist ](https://colab.research.google.com/gist/mohantym/882d94eada74ec09a470c37a6023cae7/github_51778.ipynb#scrollTo=bMMrN_v3DVbk)for reference . ", "Hello zpcalan@, assuming you were mainly trying to confirm the interactions between workers and ps, can you call `tf.debugging.set_log_device_placement(True)` at the start of the program and see if it provides useful device placement information?", "As for the reason why you don't see send/recv ops in the graph, it is because the `TensorBoard` callback only collects the graph of the \"train function\", and not the communications.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> Hello zpcalan@, assuming you were mainly trying to confirm the interactions between workers and ps, can you call `tf.debugging.set_log_device_placement(True)` at the start of the program and see if it provides useful device placement information?\r\n\r\nSorry for the late feedback.\r\nI added `tf.debugging.set_log_device_placement(True)` and the output is below:\r\n```\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> myip:18267}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> myip:21351}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://myip:21351\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> myip:18267}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> myip:21351}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:38320}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> myip:18267}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> myip:21351}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:38320}\r\n I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:272] Creating sync eager service context with rendezvous_id on host node124 /job:worker/replica:0/task:0\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job ps -> {0 -> myip:18267}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> myip:21351}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job chief -> {0 -> localhost:38320}\r\n I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:38320\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op Fill on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op Fill on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op VarHandleOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/common_runtime/eager/execute.cc:1403] Executing op AssignVariableOp on task /job:ps/replica:0/task:0/device:CPU:0\r\n I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\r\n I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\r\n I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1630] Profiler found 8 GPUs\r\n```\r\nThe reason I want to see the communication ops is that I'm tring to understand the inner implementation of send/recv ops in parameter server startegy.\r\n\r\nIs there a way to visualize it? Because if only computation ops are displayed and no optimizer ops, it may confuse users.", "@mohantym Maybe this issue could be reopened. : )", "From the log you can see that PS is indeed executing some ops related to the variables.\r\n\r\n> Is there a way to visualize it? Because if only computation ops are displayed and no optimizer ops, it may confuse users.\r\n\r\nI'm not aware of a way with eager execution; if you can re-build TF library, you can make sure all the logs in `execute.cc` or `placer.cc` in `common_runtime` directory are logged regardless, which should give you a more detailed view.", "As well as `process_function_library_runtime.cc`", "@rchao Thanks for your reply.\r\nNevermind I will check the code in `execute.cc`. Other parts of the log have printed the `_send` op.", "Sounds good! Let me know if you find anything."]}, {"number": 51777, "title": "[PluggableDevice] add int32 DEVICE_DEFAULT for BroadcastTo/Pack/StridedSlice", "body": "Add int32 DEVICE_DEFAULT for BroadcastTo/Pack/StridedSlice, this PR is for PluggableDevice.", "comments": ["Hi @penpornk , There are also 3 PRs for `int32` `DEVICE_DEFAULT`. Thanks.\r\n```\r\nhttps://github.com/tensorflow/tensorflow/pull/51777\r\nhttps://github.com/tensorflow/tensorflow/pull/51762\r\nhttps://github.com/tensorflow/tensorflow/pull/51761\r\n```", "@penpornk  Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 51776, "title": "Building from source failed on Python 3.10", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n  - [`manylinux2014` docker image](https://github.com/pypa/manylinux#manylinux2014-centos-7-based) from [pypa / manylinux](https://github.com/pypa/manylinux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 B\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.10.0rc1\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n### `distutils` is deprecated\r\n\r\nThe `distutils` is deprecated in Python 3.10. In [python_configure.bzl](https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L150), the deprecation message will be printed prior to the include path, causing error on return.\r\n\r\n### ABC is removed\r\n\r\n(detailed logs below)\r\n\r\nAccording to the [collections](https://docs.python.org/3/library/collections.html) package:\r\n\r\n> Deprecated since version 3.3, will be removed in version 3.10: Moved Collections Abstract Base Classes to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.9.\r\n\r\n` _message.so` from `protobuf` seems to cause this problem.\r\n\r\n```\r\n    from google.protobuf.pyext import _message\r\nAttributeError: module 'collections' has no attribute 'MutableSequence'\r\n```\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n[root@129a8b5b98ad tensorflow]# BAZEL_LINKLIBS=-l%:libstdc++.a bazel build --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=91\r\nINFO: Reading rc options for 'build' from /opt/tf/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /opt/tf/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /opt/tf/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/opt/python/cp310-cp310/bin/python3.10 --action_env PYTHON_LIB_PATH=/opt/python/cp310-cp310/lib/python3.10/site-packages --python_path=/opt/python/cp310-cp310/bin/python3.10\r\nINFO: Found applicable config definition build:short_logs in file /opt/tf/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /opt/tf/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:noaws in file /opt/tf/tensorflow/.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file /opt/tf/tensorflow/.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file /opt/tf/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file /opt/tf/tensorflow/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:linux in file /opt/tf/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /opt/tf/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /opt/tf/tensorflow/tensorflow/python/keras/api/BUILD:133:19: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 32, in <module>\r\n    from tensorflow.core.framework import function_pb2\r\n  File \"/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/core/framework/function_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/root/.cache/bazel/_bazel_root/91a00cca6722ee1c07e7cb452ed32c40/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/com_google_protobuf/python/google/protobuf/descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nAttributeError: module 'collections' has no attribute 'MutableSequence'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /opt/tf/tensorflow/tensorflow/python/tools/BUILD:81:10 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\r\nINFO: Elapsed time: 4.857s, Critical Path: 2.92s\r\nINFO: 5 processes: 5 internal.\r\nFAILED: Build did NOT complete successfully\r\n[root@129a8b5b98ad tensorflow]#\r\n```", "comments": ["@KumaTea Could you please have a look at the similar [issue](https://github.com/python/typed_ast/issues/156) and [link](https://www.workaround.cz/howto-build-compile-install-latest-python-37-38-39-centos-7-8/).Please let us know if it helps?Thank you!", "> @KumaTea Could you please have a look at the similar [issue](https://github.com/python/typed_ast/issues/156) and [link](https://www.workaround.cz/howto-build-compile-install-latest-python-37-38-39-centos-7-8/).Please let us know if it helps?Thank you!\r\n\r\nHi, thank you for your reply!\r\n\r\nI've checked the links, but it seems irrelevant to `typed_ast`, `setuptools` or `mypy`. The Python environment should also be okay, because it was pulled from [pypa / manylinux](https://github.com/pypa/manylinux)'s [`manylinux2014` docker image](https://github.com/pypa/manylinux#manylinux2014-centos-7-based).\r\n\r\nI still believe the errors were caused by deprecated imports.\r\n\r\nThank you!", "For the first error I've created a pull request (#51782) with a minor fix, and Python 3.6 - 3.10 compabilities have been tested.", "Added a PR #51865 to resolve another distutils related issue.", "We don't yet support py3.10 but now given that it has a final release we can start on the roadmap to support it by next release.\r\n\r\nDepending on our dependencies, plan is to start supporting python 3.10 in tf-nightly first and then in the TF 2.8 release.", "@KumaTea Please see the discussion on [TF Forum ](https://discuss.tensorflow.org/t/support-python-3-10/124?u=mihaimaruseac).Thanks!", "> @KumaTea Please see the discussion on [TF Forum ](https://discuss.tensorflow.org/t/support-python-3-10/124?u=mihaimaruseac).Thanks!\r\n\r\nHi, I've check the thread and will wait for official support. Thank you!", "With [this commit](https://github.com/tensorflow/tensorflow/commit/14966ee409b89df5b9144a6d1a19359dd0bb1f68#diff-bc2b58339883ae848d484471d86dc99189a40a83ae65ce675a5e636c6b876d4aR70), the second error which might be only caused by protobuf was fixed.\r\n\r\nI've successfully built TensorFlow **2.7.0** for Python **3.10**, so I think it's completely compatible now.\r\n\r\n(Except for these dependencies: `h5py` and `tensorflow-io`)", "@KumaTea Thank you for the  [update](https://github.com/tensorflow/tensorflow/issues/51776#issuecomment-967311582)!\r\nCould you please see [TF Forum](https://discuss.tensorflow.org/t/support-python-3-10/124/26) for more details and move this issue to closed status if it is resolved ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51776\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51776\">No</a>\n"]}, {"number": 51774, "title": "Sizing problem with BatchDataset in Tensorflow 2.3.1", "body": "I want to extract the tensor contained within a BatchDataset. It appears I run into a sizing issue -- if I call a batch of 4500 elements no problem (i.e., extractable), but if I create a batch of 5000 elements no dice.\r\n\r\n![image](https://user-images.githubusercontent.com/6710629/131588513-c9ddacbd-c5ec-4ff0-b5de-bc9a75def56b.png)\r\n\r\nIf `inp` does not exist in the second example above, then when trying to call `inp` there is a `NameError: name 'inp' not defined.`\r\n\r\nBroadly speaking, anyone know a work around? I am looking to create (and draw from) with a batch size of ~7500.\r\n\r\n**System information**\r\n- Stock Tensorflow installed using `pip`\r\n- Ubuntu 20.04\r\n- Sony Vaio Laptop\r\n- Tensorflow: `v2.3.0-54-gfcc4b966f1 2.3.1`\r\n- Python: `3.8.5`\r\n", "comments": ["Hi @jimcost , Could you also provide the complete code which will help us to expedite the issue?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51774\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51774\">No</a>\n"]}, {"number": 51773, "title": "tensorboard no runs found for PyTorch 1.9 in VSCode in CentOS 7", "body": "<img width=\"1127\" alt=\"Screen Shot 2021-08-31 at 7 22 57 PM\" src=\"https://user-images.githubusercontent.com/1892917/131588221-3152f224-9a08-40d6-8474-f404e26dd9a7.png\">\r\n\r\nAny tips on how to use Tensorboard in VSCode for PyTorch 1.9?\r\n", "comments": ["@monajalal Could you please refer to the [link](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html),[link1 ](https://www.tensorflow.org/tensorboard/get_started?hl=en)and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51773\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51773\">No</a>\n"]}, {"number": 51772, "title": "AttributeError: 'NoneType' object has no attribute 'label_map'", "body": "I was running the notebook in Jupyter Notebook (the version that can be downloaded from the Colab notebook), and ran into this error in step 3 shown in the image.\r\n\r\n![label_map error](https://user-images.githubusercontent.com/81659220/131559999-97b99027-43f1-4ff0-8198-c79286fdb1b4.png)\r\n \r\nAnyone know how to fix this?  ", "comments": ["Are you sure that you have initialized the train_data variable ", "@remotepilotsam,\r\n\r\nCan you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) for us to expedite the trouble-shooting process. Thanks!"]}, {"number": 51769, "title": "Saving a NSL model", "body": "**NotImplementedError: Saving `AdversarialRegularization` models is currently not supported. Consider using `save_weights` or saving the `base_model`.**\r\nI trained a Neural Structured Learning model with AdversarialRegularization and tried to save the model as a SavedModel format as well as a HDF5 model. And in both case I faced this error. Can anyone let me know how to save the model or a workaround.\r\nA simple example can be found in this [notebook](https://colab.research.google.com/drive/1CV6uCAys9lcbD33oZv39BvU5AfnHDYya?usp=sharing)\r\n", "comments": ["@old-school-kid,\r\n\r\nCan you take a look at this [issue](https://github.com/tensorflow/neural-structured-learning/issues/23) in `\r\nneural-structured-learning` repo, where a similar type of question is already answered. Let me know if it helps. Thanks!", "Thank you @sanatmpa1 \r\nI was trying to save the model with the regularization. But saving the base model works too. So we can close the issue. \r\nI was wondering if we could save the nsl model directly as a tensorflow model and not save the base model explicitly. Would be easy in that way. ", "@old-school-kid,\r\n\r\nThanks for the update, glad its working fine for you, kindly move this issue to closed status as it is resolved.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51769\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51769\">No</a>\n"]}, {"number": 51768, "title": "When use keras + TensorBoard with parameter server strategy, error on worker is raised and core is dumped.", "body": "Hi, developers of tensorflow.\r\nThe environment, code, and issue I encountered are listed below:\r\n\r\nTF version: nightly\r\nOS: ubuntu16.04\r\ncuda: 11.0\r\n\r\nWorker code: (PS code is almost the same as this one)\r\n```\r\nimport multiprocessing\r\nimport os\r\nimport json\r\nimport random\r\nimport portpicker\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers.experimental import preprocessing\r\n\r\n\r\ndef create_in_process_cluster(num_workers, num_ps):\r\n  \"\"\"Creates and starts local servers and returns the cluster_resolver.\"\"\"\r\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\r\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\r\n\r\n  cluster_dict = {}\r\n  cluster_dict[\"worker\"] = [\"ip:port\"]\r\n  if num_ps > 0:\r\n    cluster_dict[\"ps\"] = [\"ip:port\"]\r\n  print(\"==========================\", cluster_dict[\"worker\"], cluster_dict[\"ps\"])\r\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\r\n\r\n  # Workers need some inter_ops threads to work properly.\r\n  worker_config = tf.compat.v1.ConfigProto()\r\n  if multiprocessing.cpu_count() < num_workers + 1:\r\n    worker_config.inter_op_parallelism_threads = num_workers + 1\r\n  \r\n  os.environ[\"TF_CONFIG\"] = json.dumps({\r\n    \"cluster\": {\r\n        \"worker\": cluster_dict[\"worker\"],\r\n        \"ps\": cluster_dict[\"ps\"],\r\n    },\r\n    \"task\": {\"type\": \"worker\", \"index\": 0}\r\n  })\r\n  cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n  return cluster_resolver\r\n\r\n# Set the environment variable to allow reporting worker and ps failure to the\r\n# coordinator. This is a workaround and won't be necessary in the future.\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\n\r\nNUM_WORKERS = 1\r\nNUM_PS = 1\r\n\r\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\r\nos.environ[\"GRPC_FAIL_FAST\"] = \"use_caller\"\r\nif cluster_resolver.task_type in (\"worker\", \"ps\"):\r\n    server = tf.distribute.Server(\r\n        cluster_resolver.cluster_spec(),\r\n        job_name=cluster_resolver.task_type,\r\n        task_index=cluster_resolver.task_id,\r\n        protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n        start=True)\r\n\r\n\r\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\r\n    \r\ndef dataset_fn(input_context):\r\n  global_batch_size = 64\r\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\r\n\r\n  x = tf.random.uniform((10, 10))\r\n  y = tf.random.uniform((10,))\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()\r\n  dataset = dataset.shard(\r\n      input_context.num_input_pipelines,\r\n      input_context.input_pipeline_id)\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.prefetch(2)\r\n\r\n  return dataset\r\n\r\ndc = tf.keras.utils.experimental.DatasetCreator(dataset_fn)\r\n\r\nwith strategy.scope():\r\n  model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\r\n\r\nmodel.compile(tf.keras.optimizers.SGD(), loss='mse', metrics=['accuracy'], steps_per_execution=10)\r\n\r\n\r\nworking_dir = './my_working_dir'\r\nlog_dir = os.path.join(working_dir, 'log')\r\nckpt_filepath = os.path.join(working_dir, 'ckpt')\r\nbackup_dir = os.path.join(working_dir, 'backup')\r\n\r\ncallbacks = [\r\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir),\r\n    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),\r\n    tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=backup_dir),\r\n    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n]\r\n\r\ntf.summary.trace_on()\r\n\r\nmodel.fit(dc, epochs=50000, steps_per_epoch=20, callbacks=callbacks)\r\n\r\nimport time\r\ntime.sleep(100000)\r\n```\r\n\r\nThe error of the Worker process:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"worker_in_process_ps.py\", line 99, in <module>\r\n    model.fit(dc, epochs=50000, steps_per_epoch=20, callbacks=callbacks)\r\n  File \"keras/utils/traceback_utils.py\", line 67, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"tensorflow/python/ops/summary_ops_v2.py\", line 1342, in trace_export\r\n    raise ValueError(\"Must enable trace before export.\")\r\nValueError: Must enable trace before export.\r\nterminate called without an active exception\r\nAborted (core dumped)\r\n```\r\n\r\nI want to check the graph on the worker by TensorBoard. Anyone knows how to fix this issue?\r\n\r\n", "comments": ["https://stackoverflow.com/questions/56663388/tensorboard-errortrace-already-enabled-how-to-solve \r\nI refer to the link above and I pass `profile_batch = 100000000` to the tensorboard callback and solve this issue.\r\nAnyway, issue closed.", "Hi @zpcalan ,  look at this [comment](https://github.com/tensorflow/tensorboard/issues/2819#issuecomment-572678589) too. For further queries ,Raise an issue in [Tensorboard ](https://github.com/tensorflow/tensorboard)Repository. Thanks!", "Thanks for your reply! I've fixed this issue using the method in the comment.\r\nNow I have another question. Please check this [issue](https://github.com/tensorflow/tensorflow/issues/51778)."]}, {"number": 51764, "title": "Capsule Layer (With Dynamic Routing Between Capsules)", "body": "This is an implementation of capsule neural networks as described in \"Dynamic Routing Between Capsules\" paper (https://arxiv.org/abs/1710.09829).\r\n\r\nI made a capsule layer, which was not originally implemented in Tensorflow, which made the developers and researchers job a bit tedious.\r\n\r\nI have implemented fuctions given below:\r\n1.) __init__ :  where you can do all input-independent initialization\r\n2.) build: where you know the shapes of the input tensors and can do the rest of the initialization\r\n3.) call: where you know the shapes of the input tensors and can do the rest of the initialization\r\n4.) squash: an activation function discussed in the paper\r\n5.) softmax: re-implemented softmax\r\n6.) get_config: this function is important for storing the layer config while saving the model.\r\n7.) compute_output_shape: This function helps in parsing the input dimension to the next layer\r\n\r\n\r\n", "comments": ["@Silver-Whiskey It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n", "> @Silver-Whiskey It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras) repository instead. Thankyou.\r\n\r\nOhhh thanks ! I thought all new contributions to keras should be made here. I have made the PR there: https://github.com/keras-team/keras/pull/15287 "]}, {"number": 51763, "title": "How is TensorFlow compatible with Python2 and Python3 \uff1f", "body": "I'm doing some development based on the community's TensorFlow, but it only supports a single version of Python.It seems that the community\u2019s TensorFlow supports both Python2 and Python3\uff1a\r\n![image](https://user-images.githubusercontent.com/54703662/131481441-bf9f99a2-5456-4e1d-b3d1-c7b3e35240e8.png)\r\n\r\nI have read the [Google Python Style Guide](https://github.com/google/styleguide/blob/gh-pages/pyguide.md#s2.20-modern-python), and know that \"from __future__ import\" is used to make Python 2 and 3 compatible. However, it seems to involve only a few aspects, but there are many differences between Python2 and Python3(e.g. zip()). I want to know, in addition to the above, what work has TensorFlow done to achieve compatibility between Python 2 and 3 ?\r\n\r\nBesides, what should I do to test and verify the compatibility of my code?", "comments": ["Hi @shlee007 !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "> Hi @shlee007 !We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n\r\nhi @mohantym , the issue template\uff1a\r\n\r\n**tf version:** 1.15.0\r\n**Python version:** 2.7 & 3.6\r\n**the problem:** I am not asking a specific issue, I just want to know the implementation of TensorFlow on Py2/3 compatibility. I have learned about the functions and usage of the [six](https://pypi.org/project/six/), [future](https://pypi.org/project/future/), and [past](https://pypi.org/project/past/) libraries. Learn how to write Py2/3 compatible code and convert Py2 or Py3 style code to Py2/3 compatible code. Below is my question:\r\n- Does tensorflow have any development specifications on Py2/3 compatibility?\r\n- How does tensorflow test the Py2/3 compatibility of the code? If I missed something in the development process (make it only support Py2 or Py3), how can I find it?\r\n\r\nThanks for answering !", "Supported Python versions for Tensorflow are Python 3.6-3.9 . \r\nFor further details on supported hardware and OS ,Please refer - https://www.tensorflow.org/install", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51763\">No</a>\n"]}, {"number": 51762, "title": "[PluggableDevice] add DEVICE_DEFAULT for ParallelDynamicStitch", "body": "Add DEVICE_DEFAULT for ParallelDynamicStitch, this PR is for PluggableDevice.", "comments": ["@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 51761, "title": "[PluggableDevice] add int32 DEVICE_DEFAULT register for Unpack/Sum ops", "body": "Add int32 DEVICE_DEFAULT register for Unpack/Sum ops,  this is PR is for PluggableDevice.", "comments": ["@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!"]}, {"number": 51758, "title": "[PluggableDevice] Add range checks to TF_ExpectedOutputDataType in non-debug builds", "body": "The header comments of `TF_ExpectedOutputDataType` mention that the program aborts when passing an out-of-bounds index, but in non-debug builds we get an AV instead since `OpKernelContext::expected_output_dtype` uses `DCHECK_GE` and `DCHECK_LT` instead of `CHECK_GE` and `CHECK_LT`.", "comments": ["@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk Can you please review this PR ? Thanks!", "@penpornk How can I see the reason for the Code Check and Intel oneDNN failures?", "@PatriceVignola You can click on the `Details` links.\r\n\r\nFor [Code Check details page](https://storage.cloud.google.com/tensorflow-nightly/prod/tensorflow/github/docker/code_check_changed_files_presubmit/1636/20220315-074701/index.html), click the [ResultStore](https://source.cloud.google.com/results/invocations/bb9bb501-fd97-4d9b-93a6-3cf2b137fec1) link under the `Non-Googler Links` section. You will see a list of failed targets. Click on [pkg/code_check_changed_files](https://source.cloud.google.com/results/invocations/bb9bb501-fd97-4d9b-93a6-3cf2b137fec1/targets/pkg%2Fcode_check_changed_files) to see the error message. (Just C++ formatting errors.)\r\n\r\n```\r\n(in test file /usertools/code_check_changed_files.bats, line 36)\r\n  `[[ ! -s $BATS_TEST_TMPDIR/needs_help.txt ]]' failed\r\nclang-format is recommended. Here are the suggested changes:\r\n=============================\r\ndiff --git a/tensorflow/c/kernels_test.cc b/-\r\nindex ac09d2f633d..00000000000 100644\r\n--- a/tensorflow/c/kernels_test.cc\r\n+++ b/-\r\n@@ -28,7 +28,6 @@ limitations under the License.\r\n\r\n #include \"absl/container/inlined_vector.h\"\r\n #include \"absl/strings/str_format.h\"\r\n-#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n #include \"tensorflow/c/c_api.h\"\r\n #include \"tensorflow/c/tf_datatype.h\"\r\n #include \"tensorflow/c/tf_status.h\"\r\n@@ -53,6 +52,7 @@ limitations under the License.\r\n #include \"tensorflow/core/platform/status.h\"\r\n #include \"tensorflow/core/platform/test.h\"\r\n #include \"tensorflow/core/platform/types.h\"\r\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\n struct MyCustomKernel {\r\n   bool created;\r\n@@ -678,13 +678,11 @@ TEST(TestKernel, TestInputAndOutputCount) {\r\n\r\n     EXPECT_EQ(TF_UINT8, TF_ExpectedOutputDataType(ctx, 0));\r\n\r\n-    EXPECT_DEATH({\r\n-      TF_ExpectedOutputDataType(ctx, 1);\r\n-    }, \"Check failed: i < cc_ctx->num_outputs\");\r\n+    EXPECT_DEATH({ TF_ExpectedOutputDataType(ctx, 1); },\r\n+                 \"Check failed: i < cc_ctx->num_outputs\");\r\n\r\n-    EXPECT_DEATH({\r\n-      TF_ExpectedOutputDataType(ctx, -1);\r\n-    }, \"Check failed: i >= 0\");\r\n+    EXPECT_DEATH({ TF_ExpectedOutputDataType(ctx, -1); },\r\n+                 \"Check failed: i >= 0\");\r\n\r\n     TF_DeleteStatus(s);\r\n     if (input != nullptr) {\r\nYou can use clang-format --style=Google -i <file> to apply changes to a file.\r\n```\r\n\r\nFor [oneDNN details page](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu-pr/12675/), click on [oob_unittest.log](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu-pr/12675/artifact/oob_unittest.log) under the `Build Artifacts` section. The failures seem to be unrelated to this PR.\r\n```\r\n======================================================================\r\nERROR: testKerasInteropSequential (__main__.VariableTest)\r\nVariableTest.testKerasInteropSequential\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/ops/numpy_ops/np_interop_test_cpu.runfiles/org_tensorflow/tensorflow/python/ops/numpy_ops/np_interop_test.py\", line 316, in testKerasInteropSequential\r\n    model = tf.keras.Sequential(\r\n  File \"/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/ops/numpy_ops/np_interop_test_cpu.runfiles/org_tensorflow/tensorflow/python/training/tracking/base.py\", line 620, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 60, in error_handler\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/keras/engine/sequential.py\", line 109, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n  File \"/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/ops/numpy_ops/np_interop_test_cpu.runfiles/org_tensorflow/tensorflow/python/training/tracking/base.py\", line 620, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 60, in error_handler\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/keras/engine/training.py\", line 279, in __init__\r\n    self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\r\n  File \"/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/ops/numpy_ops/np_interop_test_cpu.runfiles/org_tensorflow/tensorflow/python/training/tracking/util.py\", line 2111, in __init__\r\n    _assert_trackable(root, \"root\")\r\n  File \"/home/tensorflow_ci_jenkins/workspace/workspace/workspace/tensorflow-mkl-linux-cpu-pr/bazel-ci_build-cache/.cache/bazel/_bazel_tensorflow_ci_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/ops/numpy_ops/np_interop_test_cpu.runfiles/org_tensorflow/tensorflow/python/training/tracking/util.py\", line 1552, in _assert_trackable\r\n    raise ValueError(\r\nValueError: `Checkpoint` was expecting root to be a trackable object (an object derived from `Trackable`), got <weakref at 0x7f1264db84a0; to 'Sequential' at 0x7f1264fdb9a0>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.\r\n```\r\n\r\nI don't think you need to do anything. I'll just pull this PR in manually and fix the formatting."]}, {"number": 51757, "title": "mips toolchain compile tflite2.6 error", "body": "I extract tflite 2.6 source code, and make it with different toolchains. Arm and linux toolchains works fine. But mips-mti-elf-gcc/g++ appears the following error:\r\n\r\nI confirm that my mips toolchain has supported c++11. Because mips toolchain not define _GLIBCXX11_USE_C99_STDIO,  it does not support to_string function. \r\n\r\nI don't know why mips not support _GLIBCXX11_USE_C99_STDIO, and if tf can help solve this problem?\r\n\r\n\r\ntflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc: In function \u2018std::__cxx11::string tflite::GetShapeDebugString(const TfLiteIntArray*)\u2019:\r\ntflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:423:24: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n       str = \"[\" + std::to_string(shape->data[d]);\r\n                        ^~~~~~~~~\r\ntflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:423:24: note: suggested alternative: \u2018__sso_string\u2019\r\n       str = \"[\" + std::to_string(shape->data[d]);\r\n                        ^~~~~~~~~\r\n                        __sso_string\r\ntflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:425:26: error: \u2018to_string\u2019 is not a member of \u2018std\u2019\r\n       str += \", \" + std::to_string(shape->data[d]);\r\n                          ^~~~~~~~~\r\ntflite/tf_latest/tensorflow/lite/kernels/kernel_util.cc:425:26: note: suggested alternative: \u2018__sso_string\u2019\r\n       str += \", \" + std::to_string(shape->data[d]);\r\n                          ^~~~~~~~~\r\n                          __sso_string\r\n<builtin>: recipe for target 'tflite/tf_latest/tensorflow/lite/kernels/kernel_util.o' failed\r\nmake: *** [tflite/tf_latest/tensorflow/lite/kernels/kernel_util.o] Error 1", "comments": ["Could you use the following for tensorflow/lite/kernels/kernel_util.cc?\r\n\r\n```\r\nstd::string GetShapeDebugString(const TfLiteIntArray* shape) { \r\n std::stringstream ss;\r\n  for (int d = 0; d < shape->size; ++d) {\r\n    if (str.empty())\r\n      ss << \"[\" << shape->data[d];\r\n    else\r\n      ss << \", \" << shape->data[d];\r\n  }\r\n  ss << \"]\";\r\n  return ss.str();\r\n}\r\n```", "@KennyTang1988, Did you tried as suggested [here](https://github.com/tensorflow/tensorflow/issues/51757#issuecomment-938333253)? and let us know?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51757\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51757\">No</a>\n"]}, {"number": 51756, "title": "[tflite conversion] Invalid file dropped for invalid shape without error", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package version 2.6.0\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): N/A\r\n\r\n### 2. Code\r\n_Provide code to help us reproduce your issues using one of the following options:_\r\n\r\nDownload model:\r\n```\r\nwget https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz\r\n\r\ntar zxvf inception_v3_2018_04_27.tgz\r\n```\r\n\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\ninput_path = \"./inception_v3.pb\"\r\noutput_path = \"./inception_v3.tflite\"\r\n\r\ninput_shapes = {\"input\": [0, 299, 299, 3]}\r\ninput_nodes = [\"input\"]\r\noutput_nodes = [\"InceptionV3/Predictions/Reshape_1\"]\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n            input_path, input_nodes, output_nodes, input_shapes)\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\nopen(output_path, \"wb\").write(tflite_model)\r\n```\r\nI suppose `input_shapes = {\"input\": [0, 299, 299, 3]}` is invalid where batch is 0.\r\nAnd expect there be some error reporting.\r\n\r\n### 3. Failure after conversion\r\n_If the conversion is successful, but the generated model is wrong, then state what is wrong:_\r\n\r\n- tflite model has only input and output nodes without any valid network\r\n\r\n### 5. (optional) Any other info / logs\r\n_Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached._\r\n\r\nInvalid input shape is a NEGATIVE test to check TensorFlow is working OK.\r\nThis time it didn't.\r\n\r\nTF2.3.0 showed this error. But TF 2.6.0 ended without error but dropped invalid tflite file.\r\n\r\n> loc(\"InceptionV3/InceptionV3/Mixed_7c/concat\"): error: 'tfl.concatenation' op failed to verify that values and output must have same element type\r\nTraceback (most recent call last):\r\nFile \".../tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\nenable_mlir_converter)\r\nFile \".../tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\nenable_mlir_converter)\r\nException: :0: error: loc(\"InceptionV3/InceptionV3/Mixed_7c/concat\"): 'tfl.concatenation' op failed to verify that values and output must have same element type\r\n:0: note: loc(\"InceptionV3/InceptionV3/Mixed_7c/concat\"): see current operation: %0 = \"tfl.concatenation\"() {axis = 3 : i32, fused_activation_function = \"NONE\"} : () -> tensor<0x8x8x2048xf32>\r\n\r\n", "comments": ["From https://github.com/Samsung/ONE/issues/7589#issuecomment-908819727, this is a capture image using netron with generated tflite file.\r\n\r\n![image](https://user-images.githubusercontent.com/4616940/131426761-b4629572-5ffd-4853-97b0-76744e614237.png)\r\n", "If above script to convert to tflite has any problems, it would be grateful to point it out... ", "Hi @seanshpark , I found this comment\r\n\r\n>  \"Tuple of strings representing input tensor names and list of integers representing input shapes (e.g., [(\"foo\" : [1, 16, 16, 3])]). Use only when graph cannot be loaded into TensorFlow and **when input_tensors and output_tensors are None.** \"(default None)\r\n\r\n for input_arrays_with_shape from [v1 document.](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter) .  Changed the code\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\ninput_path = \"./inception_v3.pb\"\r\noutput_path = \"./inception_v3.tflite\"\r\n\r\ninput_shapes = {\"input\": [0, 299, 299, 3]}\r\ninput_nodes = [\"input\"]\r\noutput_nodes = [\"InceptionV3/Predictions/Reshape_1\"]\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n            input_path, input_nodes, output_nodes, input_shapes = None)\r\nconverter.allow_custom_ops = True\r\ntflite_model = converter.convert()\r\nopen(output_path, \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n\r\nproviding [gist](https://colab.research.google.com/gist/mohantym/fcf5822199ec52816176025e941712c1/github_51756.ipynb) fore reference\r\n\r\n", "@mohantym , thanks for the comment, but I am not asking correct script to get correct tflite\r\n\r\nWhat I expect from is that, \r\n- if invalid input shape, like `input_shapes = {\"input\": [0, 299, 299, 3]}` , is given, I expect `tflite_model = converter.convert()` to drop an error\r\n\r\nAs there is no error, this happens\r\n- Current TF2.6.0 doesn't drop and error\r\n- I cannot catch the error\r\n- An invalid TensorFlow lite file is generarted (like the one in the netron capture)\r\n\r\nThis is a problem to me...\r\n", "Hi @ymodak ,Could you please look into this issue!", "If i am not mistaken, when you specify a shape with 0 batch, the converter just optimizes your model away.\r\nIt is not invalid, but may be useless.", "> It is not invalid, but may be useless.\r\n\r\nThanks for the vertification :) I'll think this as optimization result.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51756\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51756\">No</a>\n"]}, {"number": 51755, "title": "Import tensor error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: 2.x\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Mircrosoft Hyper-V video \r\n\r\n\r\n**Describe the problem**\r\nI used pip install tensorflow for installation.  Run import tensorflow as tf have the following error.  Attached below the trace log.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n     42 \r\n\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\jimmy\\anaconda3\\envs\\py37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["@jimmylai-hk Could you please check on latest stable version of TF 2.6 and   the instructions at this [link ](https://www.tensorflow.org/install/source_windows) for installing Tensorflow in windows from Source. Please let us know if it helps?Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51755\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51755\">No</a>\n"]}]