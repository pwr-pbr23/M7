[{"number": 10966, "title": "Implementation of algorithms like longest commom subsequence and editting distance?", "body": "Hi everyone, \r\n\r\nI am wondering if there are some shortcuts to implement an new API of algorithms like lcs. Should I implement it from scratch using C++ or C? It seems not efficient to do it in Python.  How can I just get started?\r\n\r\nThanks, \r\nBest, \r\nlerner", "comments": []}, {"number": 10965, "title": "Correct the learning rate as per the code snippet", "body": "The training step, `train_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)` states the learning rate as 0.05, whereas the documentation states 0.5. This needs to be corrected.", "comments": ["Can one of the admins verify this patch?", "Thanks, @patelrishabh "]}, {"number": 10964, "title": "R1 1", "body": null, "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 10963, "title": "tf.TensorArray.stack(name=\"aname\") bug", "body": "### System information\r\n- **Code + Interpreter**: Notice the print out is \"stackme/stackme:0\" instead of \"stackme:0\".\r\nThis is a really minor error. But I had spent at least half an hour wondering why my operation was called twice. Would be really nice to be fixed :)\r\n\r\n```\r\nimemory = tf.TensorArray(tf.float32, size=0, dynamic_size=True, name=\"memory\")\r\nimemory.write(0, tf.constant(0))\r\nimemory = imemory.write(0, tf.constant(0))\r\nimemory = imemory.write(1, tf.constant(1))\r\nimemory = imemory.write(2, tf.constant(2))\r\nresult = imemory.stack(name=\"stackme\")\r\nprint(result)\r\n\r\n<print out>Tensor(\"stackme/stackme:0\", shape=(?,), dtype=float32)\r\n\r\n```\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n\r\n", "comments": ["@melvinma Thanks for the very clear issue including all necessary info!\r\n\r\nI believe this is working-as-intended.  Note that when you call `TensorArray.stack`, more than one operation gets added to the graph.  Thus the `name=stackme` you're passing in is used both as a name scope for all of the operations, as well as the name of the last operation.\r\n\r\nThe behavior may be more clear if you try removing the name:\r\n```\r\nimemory = tf.TensorArray(tf.float32, size=0, dynamic_size=True, name=\"memory\")\r\nresult = imemory.stack()\r\nprint(result)\r\n\r\n<output> Tensor(\"TensorArrayStack/TensorArrayGatherV3:0\", dtype=float32)\r\n```\r\n\r\nHere the first component `TensorArrayStack` is the name scope for all operations, and the second component `TensorArrayGatherV3` is the name of the last operation."]}, {"number": 10962, "title": "expand inline for debug builds to limit number of symbols", "body": "https://github.com/tensorflow/tensorflow/issues/10867\r\ndebug build does not expand inlines which will generate lots of unused symbols that result in too many entries in the def file for python wrapper dll.\r\nChange debug build to expand inlines.\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "The failure is due to jenkins tool issue, irrelevant to this PR. Merging."]}, {"number": 10961, "title": "cudaCheckError() failed on any gpu apart from gpu:0", "body": "I am experiencing a strange issue (similar to  #9489) on both a 4 GPUs and a 2 GPUs machine. \r\n\r\nBasically I have a tensorflow model that trains and performs very well on one GPU, I now want to distribute the training phase by using multiple GPUs at once. I followed the CIFAR10 example to parallelize the model but keep getting errors a few iterations into the training process. I get \r\n```\r\n\"cudaCheckError() failed : invalid resource handle\r\n2017-06-21 19:50:28.168726: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:639] failed to record completion event; therefore, failed to create inter-stream dependency\" \r\n```\r\nmost times but I have also seen shape errors on reshape tensors (while the graph is already running) and segmentation errors. \r\n\r\nI have been playing around with configurations and so far I have noticed that the crash only happens if gpu:1 is being called. I tried using only gpu:1 and encountered the same problem. On the other end, the same script using with tf.device('gpu:0') and only this gpu works fine.\r\n\r\nWhat I tried. \r\nI reinstalled tensorflow from sources, set up and tested another machine with more GPUs to no avail.\r\n\r\n[test_out.txt](https://github.com/tensorflow/tensorflow/files/1092625/test_out.txt)\r\n\r\nother errors that have occurred : \r\n`iter: 1 / 70000, total loss: 2.5591 / 2.5591, rpn_loss_cls: 0.6961, rpn_loss_box: 0.0025, loss_cls: 1.8603, loss_box: 0.0003, lr: 0.001000\r\nspeed: 4.586s / iter\r\niter: 2 / 70000, total loss: 1.3976 / 1.3976, rpn_loss_cls: 0.6869, rpn_loss_box: 0.0078, loss_cls: 0.7021, loss_box: 0.0007, lr: 0.001000\r\nspeed: 2.638s / iter\r\niter: 3 / 70000, total loss: 1.0764 / 1.0764, rpn_loss_cls: 0.6849, rpn_loss_box: 0.0251, loss_cls: 0.2764, loss_box: 0.0900, lr: 0.001000\r\nspeed: 1.999s / iter\r\niter: 4 / 70000, total loss: 0.9941 / 0.9941, rpn_loss_cls: 0.6804, rpn_loss_box: 0.0118, loss_cls: 0.2407, loss_box: 0.0611, lr: 0.001000\r\nspeed: 1.673s / iter\r\ncudaCheckError() failed : invalid resource handle\r\n2017-06-22 10:07:47.528133: F tensorflow/stream_executor/cuda/cuda_driver.cc:312] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)\r\n./experiments/scripts/faster_rcnn_end2end.sh: line 58: 30171 Aborted                 (core dumped) python ./tools/train_net.py --device ${DEV} --number_of_devices ${NUM_DEVICES} --weights data/pretrain_model/VGG_imagenet.npy --imdb ${TRAIN_IMDB} --iters ${ITERS} --cfg experiments/cfgs/faster_rcnn_end2end.yml --network VGGnet_train ${EXTRA_ARGS}\r\n`\r\n\r\n`2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\ncudaCheckError() failed : invalid resource handle\r\n2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED\r\n2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n`\r\n`2017-06-22 10:10:58.935801: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\niter: 1 / 70000, total loss: 2.8609 / 2.8609, rpn_loss_cls: 0.6944, rpn_loss_box: 0.0092, loss_cls: 2.1569, loss_box: 0.0004, lr: 0.001000\r\nspeed: 4.461s / iter\r\niter: 2 / 70000, total loss: 1.4497 / 1.4497, rpn_loss_cls: 0.6778, rpn_loss_box: 0.0152, loss_cls: 0.7555, loss_box: 0.0012, lr: 0.001000\r\nspeed: 2.556s / iter\r\niter: 3 / 70000, total loss: 1.5660 / 1.5660, rpn_loss_cls: 0.7197, rpn_loss_box: 0.0325, loss_cls: 0.6074, loss_box: 0.2063, lr: 0.001000\r\nspeed: 1.921s / iter\r\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655654: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655687: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n2017-06-22 10:11:03.655692: W tensorflow/core/framework/op_kernel.cc:1165] Invalid argument: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\nTraceback (most recent call last):\r\n  File \"./tools/train_net.py\", line 100, in <module>\r\n    max_iters=args.max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\r\n    sw.train_model(sess, max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 281, in train_model\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 896, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1108, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1261, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1280, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n\r\nCaused by op u'tower_1/Reshape', defined at:\r\n  File \"./tools/train_net.py\", line 100, in <module>\r\n    max_iters=args.max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 375, in train_net\r\n    sw.train_model(sess, max_iters)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 208, in train_model\r\n    cross_entropy_t, loss_box_t, rpn_cross_entropy_t, rpn_loss_box_t = self.tower_loss(i)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/fast_rcnn/train.py\", line 148, in tower_loss\r\n    outputs_roi_data, outputs_cls_score, outputs_bbox_pred = self.net.inference(gpu_id)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/VGGnet_train.py\", line 89, in inference\r\n    .reshape_layer(2, name='rpn_cls_score_reshape'.format(i))\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 26, in layer_decorated\r\n    layer_output = op(self, layer_input, *args, **kwargs)\r\n  File \"/home/SERILOCAL/a.larreche/Faster-RCNN_TF/tools/../lib/networks/network.py\", line 217, in reshape_layer\r\n    int(d),tf.cast(tf.cast(input_shape[1],tf.float32)*(tf.cast(input_shape[3],tf.float32)/tf.cast(d,tf.float32)),tf.int32),input_shape[2]]),[0,2,3,1],name=name))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2472, in reshape\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2528, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1203, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Input to reshape is a tensor with 178500 values, but the requested shape has 365072219990\r\n\t [[Node: tower_1/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/transpose, tower_1/Reshape/shape)]]\r\n`\r\n\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Hi @tatatodd , sorry if my issue is confusing, I wrote it after quite a long few days of hitting that problem so I might have been a bit unclear. \r\n\r\n\r\n**### System information**\r\n2 machines have been tested with this code and raised similar issues : \r\n\r\n- 64Gb RAM/ Intel i7 6800K CPu / 2 x Geforce GTX 1080 / Ubuntu 16.04 64 bits \r\n- 64Gb RAM/ Intel(R) Core(TM) i7-6850K CPU / 4 x Geforce GTX 1080ti / Ubuntu 16.04 64 bits \r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, I have written a large part of the code, some of it comes from external sources as well. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04 64 bits\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\ntried from pip version and from sources. Problem remained on both.\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-1126-gb7acb6a', '1.2.0') and ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.5.1\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA : 8.0 \r\ncuDNN : tested both 5.1 / 6.0 \r\n\r\n- **GPU model and memory**:\r\nGTX 1080 and GTX 1080ti\r\n\r\n- **Exact command to reproduce**:\r\ndifficult to provide as this stems from my code.\r\n\r\n### Describe the problem\r\nThe problem arose while I was trying to make my training process take advantage of the multi GPU systems I have. Initially, my model was running on GPU 0 with no problems even after 500k iterations. I used the CIFAR10 example to build the model on each GPU but was met with the error : \"cudaCheckError() failed : invalid resource handle\".\r\nAfter more testing I realized that even if I ran the training on a single GPU, as long as that GPU was not GPU 0, it would crash eventually with a similar error. \r\nThe errors I have observed are not always consistent, but they always appear when GPU 1 ( or any GPU other than 0) is being used. \r\nAfter more testing I have narrowed down the part of the code that seems to cause the crash to the line : \r\n`tf.reshape(tf.py_func(proposal_layer_py,[input[0],input[1],input[2], cfg_key, _feat_stride, anchor_scales], [tf.float32]),[-1,5],name =name)`\r\n \r\nI have put every other operation of the graph on different GPUs without problems but when I do not force this one to be computed on GPU 0 it seems to cause the crash. \r\n\r\n\r\n### Source code / logs\r\nincluded in the previous message. \r\n\r\nI hope this clarifies my problem. If you need more, I could try to build a very lightweight model that demonstrates the same issue. I have not done so yet. \r\n\r\n", "@Vargeel yes, please try to create a minimal example that demonstrates your issue.  Otherwise it's hard to speculate as to what might be going wrong.  Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "I  have the same problem.\r\n> \"cudaCheckError() failed : invalid resource handle\r\n\r\n**System information**\r\n* Ubuntu 14.04\r\n* Tensorflow 0.12 (installed from pip)\r\n* cuda 8.0\r\n* cudnn 5.1/6.0(tested all)\r\n* Tesla P4 *4\r\n\r\nWhen I run with `tf.device('/gpu:0')`  inside the program , it is ok. If I use  GPU 1 2 3..., get the error.\r\nYou can try it:\r\nwhen run program,`CUDA_VISIBLE_DEVICES=1 python demo.py` , it means  assign GPU1.\r\nIn the code alway is `tf.device('/gpu:0')`.\r\n\r\nrefer:\r\nhttps://stackoverflow.com/questions/40726039/tensorflow-cuda-visible-devices-doesnt-seem-to-work\r\nhttps://stackoverflow.com/questions/35951487/tensorflow-only-works-with-gpu-0/35959188#35959188\r\n\r\n\r\n\r\n\r\n", "I'm copying a section from the logs reported by @vargeel above, which I think may be important.\r\nA difficulty is that when things start going wrong within a GPU execution, it can easily result in secondary errors that distract attention from the primary cause.  In this case it may be that the original problem is running out of memory on the GPU.   A similar problem was noticed in #9489.   \r\n\r\n2017-06-22 10:10:04.886367: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.51GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available. cudaCheckError() failed : invalid resource handle 2017-06-22 10:10:06.489802: E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_DEINITIALIZED 2017-06-22 10:10:06.489835: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:203] Unexpected Event status: 1", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "I know this is an old thread but I just wanted to say I was also having issues similar to this post, but using Tensorflow 2.  I was trying to use GPU 1 while training on another GPU (tweaking hyperparameters...).  I was a dum-dum and saw in the tutorials that you could specify GPU placement with \"`with tf.device('/gpu:0'):`\".  \r\n\r\nDon't assume that GPU 1+ follows this same pattern!! \r\n\r\nIn my case, putting \"`with tf.device('/gpu:1')`:\" didn't work, and tensorflow gracefully fell back to just using GPU 0 as default (with no warning that I saw).  I was able to finally diagnose this using \"`tf.debugging.set_log_device_placement(True)`\" at the top of my code.  Since I was running something else on GPU 0, when the other code defaulted to GPU 0, it ran out of memory and was manifested as this weird error message mentioned by the OP.  \r\n\r\nCorrectly specifying the physical device solved all of my problems.  It's a rookie mistake, but I hope this helps other rookies not make the same error.\r\n\r\nCheers"]}, {"number": 10960, "title": "Branch 159649743", "body": "", "comments": []}, {"number": 10959, "title": "ModuleNotFoundError: No module named 'tensorflow.tensorboard.tensorboard'", "body": "Tensorboard not working on Tensorflow built from sources\r\n\r\n### System information\r\n- **Linux Ubuntu 16.04**\r\n- **TensorFlow installed from sources using Bazel**\r\n- **TensorFlow version v1.2.0-1126-gb7acb6a**\r\n- **Bazel version 0.51**\r\n- **CUDA/cuDNN version 8.0/6.0**\r\n- **Found device 0 with properties: \r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 5.15GiB**\r\n- **Exact command to reproduce**: `$ tensorboard`\r\nInstalled into fresh anaconda3 environment 'tensorflow', environment is activated when performing command.\r\n### Description: The TensorBoard visualization doesn't work\r\n### Source code / logs\r\n```\r\n$ tensorboard\r\nTraceback (most recent call last):\r\n  File \"/home/gpu/anaconda3/envs/tensorflow/bin/tensorboard\", line 7, in <module>\r\n    from tensorflow.tensorboard.tensorboard import main\r\nModuleNotFoundError: No module named 'tensorflow.tensorboard.tensorboard'\r\n```\r\n", "comments": ["@captify-alazorenko It sounds like there's some problem with your installation.  How exactly did you install from sources?\r\n\r\nPerhaps you can try this:\r\n```\r\n$ python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow\r\n<module 'tensorflow' from '/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.pyc'>\r\n>>> quit()\r\n$ ls /usr/local/lib/python2.7/dist-packages/tensorflow \r\n__init__.py   contrib  examples  python       tools\r\n__init__.pyc  core     include   tensorboard\r\n```\r\n\r\nThe idea is to figure out where your tensorflow installation is, and see whether it makes sense.  You might be able to debug more depending on what you find there.", "I got same problem, installed tensorflow-gpu via whl\r\n\r\n```\r\nPython 2.7.12 (default, Jul  1 2016, 15:12:24) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow\r\n<module 'tensorflow' from '/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.pyc'>\r\n>>> exit()\r\nsteven@ubuntu-1080:/data/gits/models/adversarial_text/models$ ls /usr/local/lib/python2.7/dist-packages/tensorflow/\r\ncontrib  core  examples  include  __init__.py  __init__.pyc  python  tensorboard  tools\r\n\r\n```\r\n", "I installed from sources using the guide on tensorflow page. More specifically, after configuring tensorflow, I executed the following:\r\n\r\n```\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package \r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\ncd /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-1.2.0-cp36-cp36m-linux_x86_64.whl\r\n```\r\nI also attach the results of aforementioned command:\r\n```\r\n(tensorflow) gpu@gpuserver:/tmp/tensorflow_pkg$ python\r\nPython 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:09:58) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> tensorflow\r\n<module 'tensorflow' from '/home/gpu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/__init__.py'>\r\n>>> quit()\r\n(tensorflow) gpu@gpuserver:/tmp/tensorflow_pkg$ ls /home/gpu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow\r\ncontrib  examples  __init__.py  python       tools\r\ncore     include   __pycache__  tensorboard\r\n```\r\n", "@dandelionmane Do you have any ideas on what might be going wrong here?", "We took TensorBoard out of the TensorFlow repo and into http://github.com/tensorflow/tensorboard.\r\n\r\nSo it's expected that attempting to build or depend on TensorBoard source from the TF repo won't work.\r\n\r\nAh, but if the pip command is broken, I should look into that.", "Ok, I installed tensorflow via pip from the nightly, and tensorboard works fine.\r\n\r\nGenerally speaking, it's not expected that running tensorflow/tensorboard/tensorboard will work if you install the TF repo, since TensorBoard has moved out. I'm going to close this as working as intended, and scrub the docs for referneces to doing this.", "cc: @dandelionmane \r\n\r\nI am having issues with tensorboard when I updated to 1.3 this morning. I ran the following command from my conda environment based on the suggestions [here](https://www.tensorflow.org/install/install_mac).\r\n\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.3.0-py2-none-any.whl`\r\n\r\nHowever, when I try to run tensorboard '`python -m tensorflow.tensorboard --logdir=$MODEL_DIR`' I am getting the error 'No module named tensorflow.tensorboard'.\r\n\r\nFurther, if I look in my tensorflow package, I am not seeing tensorboard.\r\n\r\n```\r\nls /Users/username/anaconda/envs/tensorflow-cloud/lib/python2.7/site-packages/tensorflow\r\n__init__.py\tcontrib\t\texamples\tpython\r\n__init__.pyc\tcore\t\tinclude\t\ttools\r\n```\r\nThis seems like the pip command is not working anymore, unless I am doing something wrong here?\r\n\r\nThanks!", "Also, looking back at the logs it does look like the pip package collected and isntalled tensorflow-tensorboard but somehow it seems to not be getting placed in the proper place (likely a conflict with conda?): \r\n\r\n```\r\nCollecting setuptools (from protobuf>=3.3.0->tensorflow==1.3.0)\r\n  Using cached setuptools-36.2.7-py2.py3-none-any.whl\r\nInstalling collected packages: funcsigs, pbr, six, mock, setuptools, protobuf, html5lib, bleach, numpy, wheel, markdown, werkzeug, tensorflow-tensorboard, backports.weakref, tensorflow\r\nSuccessfully installed backports.weakref-1.0rc1 bleach-1.5.0 funcsigs-1.0.2 html5lib-0.9999999 markdown-2.6.9 mock-2.0.0 numpy-1.13.1 pbr-3.1.1 protobuf-3.4.0 setuptools-36.2.7 six-1.10.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.4 werkzeug-0.12.2 wheel-0.29.0\r\n```\r\n\r\nThanks for any advice you can provide.", "For anybody else finding this issue, since tensorboard was split out from tensorflow the command to run is just `tensorboard --logdir=$MODEL_DIR` instead of `python -m tensorflow.tensorboard --logdir=$MODEL_DIR` as it was previously. ", "Running `tensorboard --logdir=$MODEL_DIR` works for me but not `python -m tensorflow.tensorboard --logdir=$MODEL_DIR`. Intalled TF 1.3.0 throught `pip install --upgrade tensorflow-gpu`.", "cc: @dandelionmane\r\nWould you please update the documentation https://www.tensorflow.org/get_started/summaries_and_tensorboard \r\nto reflect this change? Thank you very much!\r\nIt still states that we can use `python -m tensorflow.tensorboard --logdir=$MODEL_DIR` to launch TensorBoard.", "Windows users might want to check out these two batch lines:\r\n```\r\ncd C:/Users/$USERNAME/AppData/Local/Programs/Python/Python35/Scripts/\r\ntensorboard.exe --logdir=$MODEL_DIR\r\n```", "Thanks @jcomfort4. After Cloud SDK update 171.0.0 I had the same issue. Solved with \r\n`tensorboard --logdir=output` \r\nas replacement for: \r\n`python -m tensorflow.tensorboard --logdir=output` \r\nPlease update: https://cloud.google.com/ml-engine/docs/how-tos/getting-started-training-prediction", "getting this error:\r\n\r\n```\r\nMon Nov 13 00:11:49 :~$ sudo pip install tensorboard\r\nPassword:\r\nThe directory '/Users/mona/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/Users/mona/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nCollecting tensorboard\r\n  Downloading tensorboard-1.0.0a6-cp27-cp27m-macosx_10_12_x86_64.whl (10.8MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10.8MB 94kB/s\r\nCollecting numpy>=1.11.0 (from tensorboard)\r\n  Downloading numpy-1.13.3-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.6MB 139kB/s\r\nCollecting mock>=2.0.0 (from tensorboard)\r\n  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.1MB/s\r\nRequirement already satisfied: wheel in /Library/Python/2.7/site-packages (from tensorboard)\r\nCollecting protobuf>=3.1.0 (from tensorboard)\r\n  Downloading protobuf-3.4.0-py2.py3-none-any.whl (375kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 378kB 1.0MB/s\r\nCollecting Pillow>=4.0.0 (from tensorboard)\r\n  Downloading Pillow-4.3.0-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (3.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.6MB 159kB/s\r\nRequirement already satisfied: six>=1.10.0 in /Library/Python/2.7/site-packages (from tensorboard)\r\nCollecting werkzeug>=0.11.10 (from tensorboard)\r\n  Downloading Werkzeug-0.12.2-py2.py3-none-any.whl (312kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 317kB 1.2MB/s\r\nRequirement already satisfied: pbr>=0.11 in /Library/Python/2.7/site-packages (from mock>=2.0.0->tensorboard)\r\nCollecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorboard)\r\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools in ./Library/Python/2.7/lib/python/site-packages (from protobuf>=3.1.0->tensorboard)\r\nCollecting olefile (from Pillow>=4.0.0->tensorboard)\r\n  Downloading olefile-0.44.zip (74kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 81kB 1.9MB/s\r\nRequirement already satisfied: packaging>=16.8 in /Library/Python/2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: appdirs>=1.4.0 in /Library/Python/2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: pyparsing in ./Library/Python/2.7/lib/python/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorboard)\r\nInstalling collected packages: numpy, funcsigs, mock, protobuf, olefile, Pillow, werkzeug, tensorboard\r\n  Found existing installation: numpy 1.8.0\r\n    Uninstalling numpy-1.8.0:\r\n      Successfully uninstalled numpy-1.8.0\r\n  Found existing installation: mock 1.3.0\r\n    Uninstalling mock-1.3.0:\r\n      Successfully uninstalled mock-1.3.0\r\n  Running setup.py install for olefile ... done\r\n  Found existing installation: Pillow 2.9.0\r\n    Uninstalling Pillow-2.9.0:\r\n      Successfully uninstalled Pillow-2.9.0\r\n  Found existing installation: Werkzeug 0.10.4\r\n    Uninstalling Werkzeug-0.10.4:\r\n      Successfully uninstalled Werkzeug-0.10.4\r\nSuccessfully installed Pillow-4.3.0 funcsigs-1.0.2 mock-2.0.0 numpy-1.13.3 olefile-0.44 protobuf-3.4.0 tensorboard-1.0.0a6 werkzeug-0.12.2\r\nMon Nov 13 00:19:57 :~$ tensorboard --logdir=/var/folders/bk/2b2d879912bgnl5s768s7crh0000gn/T/tmpzy0cj6xc\r\nTraceback (most recent call last):\r\n  File \"/Users/mona/anaconda/bin/tensorboard\", line 4, in <module>\r\n    import tensorflow.tensorboard.tensorboard\r\nModuleNotFoundError: No module named 'tensorflow.tensorboard'\r\nMon Nov 13 00:22:01 :~$\r\n```\r\n\r\nand \r\n\r\n```\r\nMon Nov 13 00:25:14 :~$ sudo /Users/mona/anaconda/bin/pip install tensorboard\r\nPassword:\r\nThe directory '/Users/mona/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nThe directory '/Users/mona/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nRequirement already satisfied: tensorboard in ./anaconda/lib/python3.6/site-packages\r\nRequirement already satisfied: protobuf>=3.1.0 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: Pillow>=4.0.0 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: wheel>=0.26 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: numpy>=1.11.0 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: six>=1.10.0 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: werkzeug>=0.11.10 in ./anaconda/lib/python3.6/site-packages (from tensorboard)\r\nRequirement already satisfied: setuptools in ./anaconda/lib/python3.6/site-packages (from protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: olefile in ./anaconda/lib/python3.6/site-packages (from Pillow>=4.0.0->tensorboard)\r\nMon Nov 13 00:25:28 :~$ tensorboard --logdir=/var/folders/bk/2b2d879912bgnl5s768s7crh0000gn/T/tmpzy0cj6xc\r\nTraceback (most recent call last):\r\n  File \"/Users/mona/anaconda/bin/tensorboard\", line 4, in <module>\r\n    import tensorflow.tensorboard.tensorboard\r\nModuleNotFoundError: No module named 'tensorflow.tensorboard'\r\n```", "I use python -m `tensorflow.tensorboard --logdir=output` and it works.\r\nBut `tensorboard --logdir=output` showed ModuleNotFound error", "\"conda install -c anaconda tensorflow-tensorboard\" solved my problem.", "I've been getting the same set of errors\r\n\r\nUsing macOSX Sierra, and spyder with a tensorflow environment installed through conda.\r\n\r\nAnaconda navigator says that tensorboard is installed in _both_ the base and tensorflow environments, and there are two tensorboard executables: one in /anaconda3/bin and the other in /anaconda3/envs/tensorflow/bin. Just running either of these files on its own gives two different errors. The base tensorboard file gives: \r\n**ModuleNotFoundError: No module named 'tensorflow'** \r\n\r\nThe tensorflow env version gives:\r\n**ValueError: A logdir must be specified when db is not specified. Run `tensorboard --help` for details and examples.**\r\n\r\nTrying to run some of the above suggested fixes from the terminal gives the following\r\npython -m tensorflow.tensorboard\r\n**/anaconda3/bin/python: Error while finding module specification for 'tensorflow.tensorboard' (ModuleNotFoundError: No module named 'tensorflow')**\r\n\r\ntensorboard\r\n**ModuleNotFoundError: No module named 'tensorflow'**\r\n\r\nAs a workaround, on my configuration I can run the tensorflow env tensorboard file from the terminal:\r\n/anaconda3/envs/tensorflow/bin/tensorboard --logdir=path\r\n\r\nI hope that helps anyone struggling with the same issue. Taking tensorboard out of tensorflow seems to have caused a problem with the conda install (at least if you use multiple environments) that isn't fixed yet.\r\n\r\n"]}, {"number": 10958, "title": "tf.summary.FileWriter doesn't support unicode paths.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes. In fact, I use code from Denny Britz's CNN tutorial: \r\nhttps://github.com/dennybritz/cnn-text-classification-tf\r\n\r\nLine 116, (https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nWindows 10, x86_64, locale: Russian\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary, pip install tensorflow\r\n- **TensorFlow version (use command below)**:\r\nb'unknown' 1.2.0\r\n\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\n8.0, 5.1\r\n- **GPU model and memory**:\r\nnvidia 970\r\n- **Exact command to reproduce**:\r\n```\r\nout_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\r\ntrain_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\r\ntrain_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph, flush_secs=5)\r\n```\r\n\r\n### Describe the problem\r\nThe running directory is: \r\n\r\n`C:\\Users\\Locky\\Google \u0414\u0438\u0441\u043a\\MachineLearning\\wildml-cnn-nlp\\cnn-text-classification-tf\r\n`\r\nBut FileWriter writes summaries to \r\n\r\n`C:\\Users\\Locky\\Google \u0420\u201d\u0420\u0451\u0421\u0403\u0420\u0454\\MachineLearning\\wildml-cnn-nlp\\cnn-text-classification-tf\r\n`\r\n\"\u0420\u201d\u0420\u0451\u0421\u0403\u0420\u0454\" doesn't make sense in Russian, and perhaps, comes from:\r\n\r\n```\r\nFile \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\event_file_writer.py\", line 70, in __init__\r\n    compat.as_bytes(os.path.join(self._logdir, \"events\")))\r\n\r\n```\r\nI suspect that 'as_bytes' corrupts the string with Russian characters. Therefore, I think, this behaviour is a bug.\r\n\r\n### Source code / logs\r\n\r\n    ```\r\n    # Output directory for models and summaries\r\n        timestamp = str(int(time.time()))\r\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\r\n        print(\"Writing to {}\\n\".format(out_dir))\r\n       \r\n        # Summaries for loss and accuracy\r\n        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\r\n        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\r\n\r\n        # Train Summaries\r\n        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\r\n        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\r\n        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph, flush_secs=5)\r\n\r\n        # Dev summaries\r\n        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\r\n        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\r\n        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\r\n\r\n```", "comments": ["I'm not a SWIG expert but seems like something odd is going on when the bytes are passed from the python code into C++.", "@lockywolf I don't have a windows dev machine at the moment. However, the non-ASCII path for Windows could also be impacted by the sys default encoding. Can you use `sys.getdefaultencoding()` to see the output if possible?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of activity but please reopen if this is still a problem."]}, {"number": 10957, "title": "Fixed path to seq2seq.py and minor formatting", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10956, "title": "Improve docs for tf.nn.depthwise_conv2d_native", "body": "Hi,\r\n\r\nIn [`depthwise_conv2d_native` docs](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d_native), the formula to calculate the output tensor is not written inside \"code\" tag for markdown.\r\n\r\nScreenshot of the docs -\r\n\r\n![screenshot from 2017-06-21 21-38-21](https://user-images.githubusercontent.com/3175743/27394554-461c57aa-56ca-11e7-874e-587a02d53f7a.png)\r\n", "comments": ["Can one of the admins verify this patch?", "Looks like it is not supposed to be code snippet, but this does make it look better. Thanks!"]}, {"number": 10955, "title": "Fix mismatched delete in mkl_tfconv_op.cc", "body": "This fix fixes mismatched new[]-delete in mkl_tfconv_op.cc\r\n\r\nThis fix fixes #10853 \r\n(the file went through clang-format so there are some additional\r\nchanges)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10954, "title": "Supervisor: SummaryWriter and Saver stop after some time", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 / anaconda3 / python 3.6\r\n- **TensorFlow installed from (source or binary)**: binary, (pip install tensorflow-gpu)\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 5.1\r\n- **GPU model and memory**: 1080 / Titan X / K80\r\n- **Exact command to reproduce**: - \r\n\r\nSystem information: [tf_env.txt](https://github.com/tensorflow/tensorflow/files/1091842/tf_env.txt)\r\n\r\n### Describe the problem\r\nHey guys!\r\n\r\nI get the problem on 3 different machines (all on ubuntu 16.04 and the tensorflow 1.2). All experiments were executed on a single machine with a single GPU.\r\nTo initialize a session I use a `tf.Supervisor` and `supervisor.managed_sessions()` with the default \r\n`summary_writer` and `saver`.  It works all well for up to 30mins - 1h30mis. But after that time the\r\n`summary_writer` stops to write events, and the `saver` also stops to save the model parameters. However, the model still runs and produces valid outputs.\r\n\r\nI also checked the python-log for tensorflow with level `DEBUG`. But all I got was some information logs, until it suddenly stops (see below).\r\n\r\nIs there a way to track the `SVSummaryThread` / `SVTimerCheckpointThread`?\r\n\r\nThanks in advance and keep up the good work!\r\nCheers\r\n\r\n### Source code / logs\r\n\r\n```python\r\nsupervisor = tf.train.Supervisor(logdir=model_path, save_summaries_secs=60, global_step=model.global_step)\r\n  sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True), allow_soft_placement=True)\r\n\r\n  with supervisor.managed_session(config=sess_config) as sess, sess.as_default():\r\n    step_time, loss = 0.0, 0.0\r\n    epoch = 0\r\n\r\n    dataset_size = reader.train_size\r\n    epoch_time = time.time()\r\n    avg_lm = 0.\r\n\r\n    while epoch < config[\"max_epochs\"]:\r\n      sess.run(reader.iterator.initializer)\r\n\r\n      while True:\r\n        try:\r\n          step_loss, step_lm = model.step(sess)\r\n          avg_lm += step_lm / float(dataset_size) * float(model.batch_size)\r\n          loss += step_loss / float(dataset_size) * float(model.batch_size)\r\n        except tf.errors.OutOfRangeError:\r\n          break\r\n\r\n``` \r\n\r\ntensorflow python log:\r\n\r\n```\r\n017-06-21 15:57:33,386 - tensorflow - INFO - Starting queue runners.\r\n2017-06-21 15:57:33,393 - tensorflow - INFO - global_step/sec: 0\r\n2017-06-21 15:57:34,539 - tensorflow - INFO - Recording summary at step 0.\r\n2017-06-21 15:58:33,385 - tensorflow - INFO - Recording summary at step 15264.\r\n2017-06-21 15:58:33,391 - tensorflow - INFO - global_step/sec: 254.444\r\n2017-06-21 15:59:33,384 - tensorflow - INFO - Recording summary at step 31239.\r\n2017-06-21 15:59:33,391 - tensorflow - INFO - global_step/sec: 266.25\r\n2017-06-21 16:00:33,385 - tensorflow - INFO - Recording summary at step 47206.\r\n2017-06-21 16:00:33,391 - tensorflow - INFO - global_step/sec: 266.117\r\n2017-06-21 16:01:33,385 - tensorflow - INFO - Recording summary at step 62157.\r\n2017-06-21 16:01:33,391 - tensorflow - INFO - global_step/sec: 249.183\r\n2017-06-21 16:02:33,384 - tensorflow - INFO - Recording summary at step 77621.\r\n2017-06-21 16:02:33,391 - tensorflow - INFO - global_step/sec: 257.733\r\n2017-06-21 16:03:33,383 - tensorflow - INFO - Recording summary at step 93657.\r\n2017-06-21 16:03:33,391 - tensorflow - INFO - global_step/sec: 267.283\r\n2017-06-21 16:04:33,391 - tensorflow - INFO - global_step/sec: 263.883\r\n2017-06-21 16:04:33,545 - tensorflow - INFO - Recording summary at step 109493.\r\n2017-06-21 16:05:33,391 - tensorflow - INFO - global_step/sec: 262.483\r\n2017-06-21 16:05:33,558 - tensorflow - INFO - Recording summary at step 125242.\r\n2017-06-21 16:06:33,383 - tensorflow - INFO - Recording summary at step 141072.\r\n2017-06-21 16:06:33,391 - tensorflow - INFO - global_step/sec: 263.883\r\n2017-06-21 16:07:33,384 - tensorflow - INFO - Recording summary at step 157265.\r\n...\r\n``` \r\n\r\n\r\n\r\n", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It didn't happen wit tf-1.4 anymore. So I'll close it. Thanks!"]}, {"number": 10953, "title": "Feature Request for BitwiseXOR for Tensor", "body": "My project requests a new layer, which needs the new operator of Tensor to compute bitwiseXOR between input x and constant Key k.\r\nE.g. x = 4 (bit form: 100), k = 7 (111), the bitwiseXOR(x, k) expects as 3 (011).\r\n\r\nAs far as I know, Tensor only has LogicXOR operator for bool type. Luckily, Tensorflow has the extended ability to have a new Op. However, I read the document in https://www.tensorflow.org/extend/adding_an_op, I can get the basic idea, but that is far from the implementation, maybe because of the lack of c++ knowledge. If you guys could help to implement the new operator that will be helpful. Then I can use that new Op of Tensor to build the new layers.\r\n", "comments": ["The bitwise ops were added recently and will be integrated when PR https://github.com/tensorflow/tensorflow/pull/10859 is merged in (specifically commit https://github.com/tensorflow/tensorflow/pull/10859/commits/a94a333762b4d2c3fcae0a9e610d654bba6980de).\r\n\r\nOnce that is merged, you should have these available if you build from sources and in the next release."]}, {"number": 10952, "title": "Cannnot convert Graphs/Models via DarkFlow", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L69\r\n> // ./flow --model cfg/tiny-yolo-voc.cfg --load bin/tiny-yolo-voc.weights --savepb --verbalise=True\r\n\r\nI already reported about this on **darkflow** git https://github.com/thtrieu/darkflow/issues/295 but still didn't get any reply\r\n\r\nI shows next error:\r\n\r\n`ERROR - Invalid argument: --verbalise=True`", "comments": ["solved", "May I ask how you solved the issue? I am having the same now.", "@patrick-ucr `--verbalise true` or use just  `--verbalise`"]}, {"number": 10951, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 10950, "title": "TF binary incompatible to protobuf?", "body": "I tried with TensorFlow 1.1.0 and 1.2.0, on Python 2.7 (and 3.5).\r\nNote that I also asked about this [on StackOverflow](https://stackoverflow.com/questions/44455722/create-my-own-resource-types-tf-resource) but I think this might actually be a bug (either in the pip packaging of TF or protobuf or sth else), so I post it here.\r\n\r\nSummary:\r\n\r\nI think the protobuf pip package is binary incompatible with the TF pip package but I'm not exactly sure on this.\r\n\r\nI try to write some own operator which creates some own `tf.resource`. I use the C++ header files from the TF pip install, and I link it to the `_message.so` file from the protobuf 3.3.0 pip package, because anything `tf.resource` related will need linking to protobuf.\r\n\r\nMy current code:\r\n\r\n\r\n    // For Eigen::ThreadPoolDevice.\r\n    #define EIGEN_USE_THREADS 1\r\n\r\n    #include \"tensorflow/core/framework/op.h\"\r\n    #include \"tensorflow/core/framework/shape_inference.h\"\r\n    #include \"tensorflow/core/framework/op_kernel.h\"\r\n    #include \"tensorflow/core/framework/resource_mgr.h\"\r\n    #include \"tensorflow/core/framework/resource_op_kernel.h\"\r\n    #include \"tensorflow/core/framework/tensor.h\"\r\n    #include \"tensorflow/core/framework/tensor_shape.h\"\r\n    #include \"tensorflow/core/framework/types.h\"\r\n    #include \"tensorflow/core/platform/macros.h\"\r\n    #include \"tensorflow/core/platform/mutex.h\"\r\n    #include \"tensorflow/core/platform/types.h\"\r\n\r\n    using namespace tensorflow;\r\n\r\n    REGISTER_OP(\"ArrayContainerCreate\")\r\n    .Attr(\"T: type\")\r\n    .Attr(\"container: string = ''\")\r\n    .Attr(\"shared_name: string = ''\")\r\n    .Output(\"resource: resource\")\r\n    .SetIsStateful()\r\n    .SetShapeFn(shape_inference::ScalarShape)\r\n    .Doc(R\"doc(Array container, random index access)doc\");\r\n\r\n    REGISTER_OP(\"ArrayContainerGetSize\")\r\n    .Input(\"handle: resource\")\r\n    .Output(\"out: int32\")\r\n    .SetShapeFn(shape_inference::ScalarShape)\r\n    ;\r\n\r\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_mgr.h\r\n    struct ArrayContainer : public ResourceBase {\r\n      ArrayContainer(const DataType& dtype) : dtype_(dtype) {}\r\n\r\n      string DebugString() override { return \"ArrayContainer\"; }\r\n      int64 MemoryUsed() const override { return 0; };\r\n\r\n      mutex mu_;\r\n      const DataType dtype_;\r\n\r\n      int32 get_size() {\r\n        mutex_lock l(mu_);\r\n        return (int32) 42;\r\n      }\r\n\r\n    };\r\n\r\n    // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_op_kernel.h\r\n    class ArrayContainerCreateOp : public ResourceOpKernel<ArrayContainer> {\r\n    public:\r\n      explicit ArrayContainerCreateOp(OpKernelConstruction* context) : ResourceOpKernel(context) {\r\n        OP_REQUIRES_OK(context, context->GetAttr(\"T\", &dtype_));\r\n      }\r\n\r\n    private:\r\n      virtual bool IsCancellable() const { return false; }\r\n      virtual void Cancel() {}\r\n\r\n      Status CreateResource(ArrayContainer** ret) override EXCLUSIVE_LOCKS_REQUIRED(mu_) {\r\n        *ret = new ArrayContainer(dtype_);\r\n        if(*ret == nullptr)\r\n          return errors::ResourceExhausted(\"Failed to allocate\");\r\n        return Status::OK();\r\n      }\r\n\r\n      Status VerifyResource(ArrayContainer* ar) override {\r\n        if(ar->dtype_ != dtype_)\r\n          return errors::InvalidArgument(\"Data type mismatch: expected \", DataTypeString(dtype_),\r\n                                         \" but got \", DataTypeString(ar->dtype_), \".\");\r\n        return Status::OK();\r\n      }\r\n  \r\n      DataType dtype_;\r\n    };\r\n    REGISTER_KERNEL_BUILDER(Name(\"ArrayContainerCreate\").Device(DEVICE_CPU), ArrayContainerCreateOp);\r\n\r\n    class ArrayContainerGetSizeOp : public OpKernel {\r\n    public:\r\n      using OpKernel::OpKernel;\r\n\r\n      void Compute(OpKernelContext* context) override {\r\n        ArrayContainer* ar;\r\n        OP_REQUIRES_OK(context, GetResourceFromContext(context, \"handle\", &ar));\r\n        core::ScopedUnref unref(ar);\r\n\r\n        int32 size = ar->get_size();\r\n        Tensor* tensor_size = nullptr;\r\n        OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}), &tensor_size));\r\n        tensor_size->flat<int32>().setConstant(size);\r\n      }\r\n    };\r\n    REGISTER_KERNEL_BUILDER(Name(\"ArrayContainerGetSize\").Device(DEVICE_CPU), ArrayContainerGetSizeOp);\r\n\r\nI compile that. Note that I first got some `undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE` error but I resolved that by adding these additional compiler flags:\r\n\r\n    from google.protobuf.pyext import _message as msg\r\n    lib = msg.__file__\r\n    \r\n    extra_compiler_flags = [\r\n        \"-Xlinker\", \"-rpath\", \"-Xlinker\", os.path.dirname(lib),\r\n        \"-L\", os.path.dirname(lib), \"-l\", \":\" + os.path.basename(lib)]\r\n\r\nI read about that [here](https://github.com/tensorflow/tensorflow/issues/1419).\r\n\r\nI end up with flags like these:\r\n\r\n`-shared -O2 -std=c++11 -I /u/zeyer/.local/lib/python2.7/site-packages/tensorflow/include -I /usr/local/cuda-8.0/include -L /usr/local/cuda-8.0/lib64 -x cu -DGOOGLE_CUDA=1 -Xcompiler -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 TFArrayContainer.cc -o TFArrayContainer.so -Xlinker -rpath -Xlinker /u/zeyer/.local/lib/python2.7/site-packages/google/protobuf/pyext -L /u/zeyer/.local/lib/python2.7/site-packages/google/protobuf/pyext -l :_message.so`\r\n\r\nThen I load that as a module via `tf.load_op_library`.\r\n\r\nThen, I have this Python code:\r\n\r\n    handle = mod.array_container_create(T=tf.int32)\r\n    size = mod.array_container_get_size(handle=handle)\r\n\r\nWhen I try to evaluate `size`, I get the error:\r\n\r\n\r\n    InvalidArgumentError (see above for traceback): Trying to access resource located in device 14ArrayContainer from device /job:localhost/replica:0/task:0/cpu:0\r\n             [[Node: ArrayContainerGetSize = ArrayContainerGetSize[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](array_container)]]\r\n\r\nThe device name (`14ArrayContainer`) somehow seem to be messed up. Why is that? What is the problem with the code?\r\n\r\nFor some more testing, I added this additional code in the `ArrayContainerCreateOp`:\r\n\r\n        ResourceHandle rhandle = MakeResourceHandle<ArrayContainer>(context, cinfo_.container(), cinfo_.name());\r\n        printf(\"created. device: %s\\n\", rhandle.device().c_str());\r\n        printf(\"container: %s\\n\", rhandle.container().c_str());\r\n        printf(\"name: %s\\n\", rhandle.name().c_str());\r\n        printf(\"actual device: %s\\n\", context->device()->attributes().name().c_str());\r\n        printf(\"actual name: %s\\n\", cinfo_.name().c_str());\r\n\r\nThis gives me the output:\r\n\r\n    created. device: 14ArrayContainer\r\n    container: 14ArrayContainer\r\n    name: 14ArrayContainer\r\n    actual device: /job:localhost/replica:0/task:0/cpu:0\r\n    actual name: _2_array_container\r\n\r\nSo clearly, there is some of the problem.\r\n\r\nThis looks like something is messed up with the protobuf? Maybe I am linking the wrong lib? But I have not found which lib to link instead.\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@ali01  Well, I wrote already the TF versions, and I wrote that I used the pip package, and this should be independent of the platform, but if it matters, I tried both Linux and MacOSX. I also posted all the code already, both the C++ part and the TF part. You can use `g++` for compiling, e.g. with the flags which I wrote. Maybe here how to load the module, if that is unclear:\r\n```\r\nmod = tf.load_op_library(\"TFArrayContainer.so\")\r\n```\r\nThen, as I already wrote, the computation graph:\r\n```\r\nhandle = mod.array_container_create(T=tf.int32)\r\nsize = mod.array_container_get_size(handle=handle)\r\n```\r\nThen, this is how you evaluate it (given that you have a default session):\r\n```\r\nsize.eval()\r\n```\r\nI'm not exactly sure what information is missing.\r\n", "@albertz could you please fill in the exact version numbers that you're using, perhaps just copy/pasting the form from ali01's response?\r\n\r\nAlso, it's not clear to me whether you can construct a smaller exhibitor code sample, but that would help a bunch. Thanks!", "Ok, I reduced the test case a bit.\r\n\r\n`demo.cpp`:\r\n```\r\n// Demo for: https://github.com/tensorflow/tensorflow/issues/10950\r\n// See demo.py.\r\n\r\n#include <vector>\r\n\r\n// For Eigen::ThreadPoolDevice.\r\n#define EIGEN_USE_THREADS 1\r\n\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/resource_mgr.h\"\r\n#include \"tensorflow/core/framework/resource_op_kernel.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/core/framework/tensor_shape.h\"\r\n#include \"tensorflow/core/framework/types.h\"\r\n#include \"tensorflow/core/platform/macros.h\"\r\n#include \"tensorflow/core/platform/mutex.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ResourceBugDemo\")\r\n.Attr(\"container: string = ''\")\r\n.Attr(\"shared_name: string = ''\")\r\n.Output(\"resource: resource\")\r\n.SetIsStateful()\r\n.SetShapeFn(shape_inference::ScalarShape)\r\n.Doc(\"demo\");\r\n\r\n// https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_mgr.h\r\nstruct ArrayContainer : public ResourceBase {\r\n    ArrayContainer() {}\r\n\r\n    string DebugString() override { return \"ArrayContainer\"; }\r\n    int64 MemoryUsed() const override { return 0; };\r\n};\r\n\r\n// https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_op_kernel.h\r\nclass ResourceBugDemoOp : public ResourceOpKernel<ArrayContainer> {\r\npublic:\r\n    explicit ResourceBugDemoOp(OpKernelConstruction* context) : ResourceOpKernel(context) {}\r\n\r\n    void Compute(OpKernelContext* context) override {\r\n        ResourceOpKernel<ArrayContainer>::Compute(context);\r\n        mutex_lock l(mu_);\r\n        ResourceHandle rhandle = MakeResourceHandle<ArrayContainer>(context, cinfo_.container(), cinfo_.name());\r\n        printf(\"rhandle.device: %s\\n\", rhandle.device().c_str());\r\n        printf(\"rhandle.container: %s\\n\", rhandle.container().c_str());\r\n        printf(\"rhandle.name: %s\\n\", rhandle.name().c_str());\r\n        printf(\"rhandle.device should be: %s\\n\", context->device()->attributes().name().c_str());\r\n        printf(\"rhandle.name should be: %s\\n\", cinfo_.name().c_str());\r\n    }\r\n    \r\nprivate:\r\n    virtual bool IsCancellable() const { return false; }\r\n    virtual void Cancel() {}\r\n\r\n    Status CreateResource(ArrayContainer** ret) override EXCLUSIVE_LOCKS_REQUIRED(mu_) {\r\n        *ret = new ArrayContainer();\r\n        if(*ret == nullptr)\r\n            return errors::ResourceExhausted(\"Failed to allocate\");\r\n        return Status::OK();\r\n    }\r\n\r\n    Status VerifyResource(ArrayContainer* ar) override {\r\n        return Status::OK();\r\n    }\r\n};\r\nREGISTER_KERNEL_BUILDER(Name(\"ResourceBugDemo\").Device(DEVICE_CPU), ResourceBugDemoOp);\r\n```\r\n\r\n`demo.py`:\r\n```\r\n#!/usr/bin/env python\r\n\r\n# Demo for: https://github.com/tensorflow/tensorflow/issues/10950\r\n\r\nfrom __future__ import print_function\r\nimport os\r\nimport sys\r\nimport tensorflow as tf\r\n\r\n\r\nmy_dir = os.path.dirname(os.path.abspath(__file__))\r\nso_filename = \"demo.so\"\r\ncc_filename = \"demo.cpp\"\r\n\r\n\r\ndef compile():\r\n    # Fix for undefined symbol: _ZN6google8protobuf8internal26fixed_address_empty_stringE.\r\n    # https://github.com/tensorflow/tensorflow/issues/1419\r\n    from google.protobuf.pyext import _message as msg\r\n    lib = msg.__file__\r\n    ld_flags = [\r\n        \"-Xlinker\", \"-rpath\", \"-Xlinker\", os.path.dirname(lib),\r\n        \"-L\", os.path.dirname(lib), \"-l\", \":\" + os.path.basename(lib)]\r\n    common_opts = [\"-shared\", \"-O2\", \"-std=c++11\"]\r\n    if sys.platform == \"darwin\":\r\n        common_opts += [\"-undefined\", \"dynamic_lookup\"]\r\n    common_opts += [\"-I\", tf.sysconfig.get_include()]\r\n    common_opts += [\"-fPIC\"]\r\n    common_opts += [\"-D_GLIBCXX_USE_CXX11_ABI=0\"]  # might be obsolete in the future\r\n    opts = common_opts + [cc_filename, \"-o\", so_filename]\r\n    opts += ld_flags\r\n    cmd_bin = \"g++\"\r\n    cmd_args = [cmd_bin] + opts\r\n    from subprocess import Popen, PIPE, STDOUT, CalledProcessError\r\n    print(\"compile call: %s\" % \" \".join(cmd_args))\r\n    proc = Popen(cmd_args, stdout=PIPE, stderr=STDOUT)\r\n    stdout, stderr = proc.communicate()\r\n    assert stderr is None  # should only have stdout\r\n    if proc.returncode != 0:\r\n      print(\"compile failed: %s\" % cmd_bin)\r\n      print(\"Original stdout/stderr:\")\r\n      print(stdout)\r\n      raise CalledProcessError(returncode=proc.returncode, cmd=cmd_args)\r\n    assert os.path.exists(so_filename)\r\n\r\n\r\ndef main():\r\n    print(\"TensorFlow version:\", tf.GIT_VERSION, tf.VERSION)\r\n    os.chdir(my_dir)\r\n    compile()\r\n    mod = tf.load_op_library(\"%s/%s\" % (my_dir, so_filename))\r\n    handle = mod.resource_bug_demo()\r\n    with tf.Session() as session:\r\n        session.run(handle)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nIf you call `demo.py`, with Python 2.7.12, on Ubuntu 16.04, I get this output:\r\n```\r\nTensorFlow version: v1.2.0-5-g435cdfc 1.2.1\r\ncompile call: g++ -shared -O2 -std=c++11 -I /u/zeyer/.local/lib/python2.7/site-packages/tensorflow/include -fPIC -D_GLIBCXX_USE_CXX11_ABI=0 demo.cpp -o demo.so -Xlinker -rpath -Xlinker /u/zeyer/.local/lib/python2.7/site-packages/google/protobuf/pyext -L /u/zeyer/.local/lib/python2.7/site-packages/google/protobuf/pyext -l :_message.so\r\n...\r\nrhandle.device: 14ArrayContainer\r\nrhandle.container: 14ArrayContainer\r\nrhandle.name: 14ArrayContainer\r\nrhandle.device should be: /job:localhost/replica:0/task:0/cpu:0\r\nrhandle.name should be: _0_ResourceBugDemo\r\n```\r\n\r\nBut as I already guessed in the title, the problem is probably that the TF binary from the pip package is binary-incompatible to the protobuf pip package (3.3.0).\r\nAnd the solution, as written maybe indirectly in #1419, is to just export the protobuf symbols by TF itself (in `_pywrap_tensorflow_internal.so`).\r\n", "@jart @jhseu Could this be related to (or fixed by) the protobuf version change we had recently?", "Someone also encountered this bug internally (b/62536806), and it was fixed by syncing. Pinged to see if they know what the original issue was.\r\n\r\nPossibly fixed by a newer protobuf version?", "This issue is fixed by https://github.com/tensorflow/tensorflow/pull/9994, which didn't make it into the TF 1.2 release."]}, {"number": 10949, "title": "[OpenCL] Registers AdjustContrastv2", "body": "- Registers AdjustContrastv2 for SYCL\r\n- Extends adjust_contrast_op_benchmark_test to cover SYCL", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10948, "title": "fix error string format", "body": "I just got this helpful error message, and I'm submitting a fix for the format.\r\n\r\nValueError: ('num_outputs should be int or long, got %s.', <tf.Tensor 'mul:0' shape=() dtype=int32>)\r\n\r\nbecomes\r\n\r\nValueError: num_outputs should be int or long, got Tensor(\"mul:0\", shape=(), dtype=int32).", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Test failure is unrelated. Merging PR.\r\n\r\nThanks, @dgboy2000 !"]}, {"number": 10888, "title": "IOS Expected namespace tensorflow::ops", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**:\r\n    binary(pip install)\r\n- **TensorFlow version (use command below)**:\r\n    1.1.0 CPU Only\r\n- **Bazel version (if compiling from source)**:\r\n    0.4.5\r\n\r\n### Describe the problem\r\nI want to use pb file in IOS.And I must use `tensorflow::ops::reshape` and ` tensorflow::ops::argmax ` api.But when I add ` using namespace tensorflow::ops; `I get a error --` Expected namespace name `.My TensorFlow-experimental come from Pod.  \r\nHow to add namespace tensorflow::ops...\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10875, "title": "how do i retrain net after quantization?", "body": "i had used tensorflow tool, quantization_graph.py, to quantize my graph successfully. how do i retrain this net after quantization? since the tool requires using freeze_graph.py to convert variables in graph to constant. \r\n\r\nBTW, can i set up my graph using quantize ops, and train from the scratch? if can, is there any example?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10874, "title": "where is a complete API document", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorflow/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["_Warning: As you've not filled in the required info above you may not get the support you're looking for from the devs. Perhaps this is an issue for Stack Overflow?_\r\n\r\nFull documentation is available here: https://www.tensorflow.org/api_docs/\r\n\r\nPlease note this Issue tracker is for bugs and features. More help is available on Stack Overflow."]}, {"number": 10873, "title": "Splitting model and using threads.", "body": "Hello everyone. I am new to tensorflow and had a doubt. How can I split a pre trained model into two parts and then load it onto the GPU simultaneously using different threads ? Is it possible, if yes, please guide me for the same. Thanks :) :)  ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10872, "title": "[win7-64bit,Anaconda,tensorflow 1.2.0,chrome]Tensorboard : No scalar was found ", "body": "I can't see any data. It turned out to be : No scalar\r\n\r\n was found \r\n\r\nwhen I run the tensorboard ,there is no error or warning\r\n\r\n**Environment info**\r\n\r\nWindows 7 64-bit\r\nAnaconda Python 3.5\r\nTensoflow installed from binary pip package\r\ntensorflow version:1.2.0\r\nBrowser: Chrome 58\r\n\r\n**About the code**\r\n\r\nI just run the example code mnist_with_summries.py\r\n\r\nit is in D:\\Anaconda\\Lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\mnist_with_summaries.py\r\n\r\n**What have you tried?**\r\n\r\nI can see a log file called events.out.tfevents.*  in the folder I set. The file is about 16Mb big.\r\n\r\nI call tensorboard --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries --debug and I can verify the log dir is correct.\r\n\r\nOn the browser I can't see any data or graph is shown.\r\n\r\nIf I call tensorboard --inspect --logdir=/tmp/tensorflow/mnist/logs/mnist_with_summaries  It shows how  the tfevent file contains.\r\n", "comments": ["This is a question better asked on the tensorboard repository:\r\nhttps://github.com/tensorflow/tensorboard/issues"]}, {"number": 10871, "title": "Add the shared libraries in the quickstart", "body": "When following the quickstart steps of the golang README.md, below issue is found: \r\n\r\n$ uname -a\r\nLinux ubuntu-16 4.4.0-57-generic #78-Ubuntu SMP Fri Dec 9 23:50:32 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\n/tmp/go-build828413616/github.com/tensorflow/tensorflow/tensorflow/go/_test/go.test: error while loading shared libraries: libtensorflow.so: cannot open shared object file: No such file or directory\r\nFAIL\tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.001s\r\n\r\nIt's due to the env variable, after adding the LIBRARY_PATH and LD_LIBRARY_PATH, it became ok.\r\n\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\nok  \tgithub.com/tensorflow/tensorflow/tensorflow/go\t0.246s\r\n\r\nso I updated the README.md file.\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!  cla", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 10870, "title": " tensorflow-1.2.0  import tensorflow  Segmentation fault", "body": "hi, \r\nI installed tensorflow-1.2.0  in my machine, and met a segment fault as below.\r\n\r\nlinux-swfm:~/workarea/test> python\r\nPython 2.7.13 (default, Jun 20 2017, 20:03:45) \r\n[GCC 4.9.2] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nSegmentation fault\r\n\r\nmy system is : USE Linux Enterprise Server 11 SP3.\r\ncuda sdk version is 8.0 and cudnn is 6.0.\r\nmy command to build tensorflow is below : \r\n     bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n", "comments": ["Could you try `gdb python` and then `import tensorflow`? This at least give us a hint of what's going on.", "below is my gdb backtrace ouput: \r\n\r\n```gdb\r\n(gdb) r\r\nThe program being debugged has been started already.\r\nStart it from the beginning? (y or n) y\r\nStarting program: /home/zhangdingfei/tools/Python-2.7.13/bin/bin/python \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib64/libthread_db.so.1\".\r\nPython 2.7.13 (default, Jun 20 2017, 20:03:45) \r\n[GCC 4.9.2] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n[New Thread 0x7ffff375a700 (LWP 33742)]\r\n[New Thread 0x7ffff2f59700 (LWP 33743)]\r\n[New Thread 0x7ffff0758700 (LWP 33744)]\r\n...\r\n[New Thread 0x7fff5cf1d700 (LWP 33803)]\r\n[New Thread 0x7fff5a71c700 (LWP 33804)]\r\nwarning: File \"/home/zhangdingfei/tools/gcc-4.9.2/bin/lib64/libstdc++.so.6.0.20-gdb.py\" auto-loading has been declined by your `auto-load safe-path' set to \"$debugdir:$datadir/auto-load\".\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fff4a6c71c4 in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n(gdb) bt\r\n#0  0x00007fff4a6c71c4 in void std::call_once<void (&)()>(std::once_flag&, void (&)()) ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#1  0x00007fff4a81a3de in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007fff46cf2701 in tensorflow::port::(anonymous namespace)::CheckFeatureOrDie(tensorflow::port::CPUFeature, std::string const&) () from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fff46cf2754 in _GLOBAL__sub_I_cpu_feature_guard.cc ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fff4aa91086 in __do_global_ctors_aux ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fff46b3e363 in _init ()\r\n   from /home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007ffff71d28a8 in ?? () from /lib64/libc.so.6\r\n#7  0x00007ffff7dec1b8 in call_init () from /lib64/ld-linux-x86-64.so.2\r\n#8  0x00007ffff7dec2e7 in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2\r\n#9  0x00007ffff7df0606 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\n#10 0x00007ffff7debe46 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#11 0x00007ffff7defdfb in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\n#12 0x00007ffff79bdf9b in dlopen_doit () from /lib64/libdl.so.2\r\n#13 0x00007ffff7debe46 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\n#14 0x00007ffff79be33c in _dlerror_run () from /lib64/libdl.so.2\r\n#15 0x00007ffff79bdf01 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2\r\n#16 0x000000000053561b in _PyImport_GetDynLoadFunc (fqname=fqname@entry=0x7fff525d5e54 \"_pywrap_tensorflow_internal\", \r\n    shortname=shortname@entry=0x7fff525d5e54 \"_pywrap_tensorflow_internal\", \r\n    pathname=pathname@entry=0x7fff525fbc34 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", fp=fp@entry=0xb6fa30) at Python/dynload_shlib.c:130\r\n#17 0x000000000051211e in _PyImport_LoadDynamicModule (name=name@entry=0x7fff525d5e54 \"_pywrap_tensorflow_internal\", \r\n    pathname=0x7fff525fbc34 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", fp=0xb6fa30) at ./Python/importdl.c:42\r\n#18 0x00000000005103ab in load_module (loader=0x0, type=3, pathname=<optimized out>, fp=<optimized out>, \r\n    name=0x7fff525d5e54 \"_pywrap_tensorflow_internal\") at Python/import.c:1937\r\n#19 imp_load_module (self=<optimized out>, args=<optimized out>) at Python/import.c:3207\r\n#20 0x00000000004f6c4a in call_function (oparg=<optimized out>, pp_stack=<optimized out>) at Python/ceval.c:4352\r\n#21 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2989\r\n#22 0x00000000004f88b6 in fast_function (nk=<optimized out>, na=<optimized out>, n=<optimized out>, pp_stack=<optimized out>, \r\n    func=<optimized out>) at Python/ceval.c:4437\r\n#23 call_function (oparg=<optimized out>, pp_stack=<optimized out>) at Python/ceval.c:4372\r\n#24 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2989\r\n#25 0x00000000004f915b in PyEval_EvalCodeEx (co=0x7fff52539630, globals=0x0, globals@entry=0x7fff5258a398, locals=0x7fffffffb4e0, \r\n    locals@entry=0x7fff5258a398, args=0x0, argcount=1261680879, argcount@entry=0, kws=0x10de358, kws@entry=0x0, kwcount=0, \r\n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3584\r\n#26 0x00000000004f9269 in PyEval_EvalCode (co=co@entry=0x7fff52539630, globals=globals@entry=0x7fff5258a398, \r\n    locals=locals@entry=0x7fff5258a398) at Python/ceval.c:669\r\n#27 0x000000000050e668 in PyImport_ExecCodeModuleEx (name=0xb94aa0 \"tensorflow.python.pywrap_tensorflow_internal\", \r\n    co=0x7fff52539630, pathname=<optimized out>) at Python/import.c:731\r\n#28 0x000000000050e9be in load_source_module (name=0xb94aa0 \"tensorflow.python.pywrap_tensorflow_internal\", \r\n    pathname=0xb6ea20 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.pyc\", fp=0x7fff52539630) at Python/import.c:1121\r\n---Type <return> to continue, or q <return> to quit---\r\n#29 0x000000000050f759 in import_submodule (mod=0x7ffff7e8b248, subname=0xb94ab2 \"pywrap_tensorflow_internal\", \r\n    fullname=0xb94aa0 \"tensorflow.python.pywrap_tensorflow_internal\") at Python/import.c:2725\r\n#30 0x000000000051089b in load_next (p_buflen=<synthetic pointer>, buf=0xb94aa0 \"tensorflow.python.pywrap_tensorflow_internal\", \r\n    p_name=<synthetic pointer>, altmod=0x7ffff7e8b248, mod=0x7ffff7e8b248) at Python/import.c:2539\r\n#31 import_module_level (locals=<optimized out>, level=<optimized out>, fromlist=0x7ffff7e8aa10, globals=<optimized out>, \r\n    name=<optimized out>) at Python/import.c:2256\r\n#32 PyImport_ImportModuleLevel (name=<optimized out>, globals=<optimized out>, locals=<optimized out>, fromlist=0x7ffff7e8aa10, \r\n    level=<optimized out>) at Python/import.c:2312\r\n#33 0x00000000004edaf4 in builtin___import__ (self=<optimized out>, args=<optimized out>, kwds=<optimized out>)\r\n    at Python/bltinmodule.c:49\r\n#34 0x00000000004616aa in PyObject_Call (func=0x7ffff7fb4fc8, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2547\r\n#35 0x00000000004f248b in PyEval_CallObjectWithKeywords (kw=<optimized out>, arg=<optimized out>, func=<optimized out>)\r\n    at Python/ceval.c:4221\r\n#36 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2624\r\n#37 0x00000000004f915b in PyEval_EvalCodeEx (co=0x7ffff7e8e530, globals=0x0, globals@entry=0x7fff5258a6e0, locals=0x7fffffffb4e0, \r\n    locals@entry=0x7fff5258a6e0, args=0x0, argcount=1261680879, argcount@entry=0, kws=0x10de358, kws@entry=0x0, kwcount=0, \r\n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3584\r\n#38 0x00000000004f9269 in PyEval_EvalCode (co=co@entry=0x7ffff7e8e530, globals=globals@entry=0x7fff5258a6e0, \r\n    locals=locals@entry=0x7fff5258a6e0) at Python/ceval.c:669\r\n#39 0x000000000050e668 in PyImport_ExecCodeModuleEx (name=0xe982b0 \"tensorflow.python.pywrap_tensorflow\", co=0x7ffff7e8e530, \r\n    pathname=<optimized out>) at Python/import.c:731\r\n#40 0x000000000050e9be in load_source_module (name=0xe982b0 \"tensorflow.python.pywrap_tensorflow\", \r\n    pathname=0xe2f0b0 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.pyc\", fp=0x7ffff7e8e530) at Python/import.c:1121\r\n#41 0x000000000050f759 in import_submodule (mod=0x7ffff7e8b248, subname=0x7ffff7e8b2bc \"pywrap_tensorflow\", \r\n    fullname=0xe982b0 \"tensorflow.python.pywrap_tensorflow\") at Python/import.c:2725\r\n#42 0x000000000050fa25 in ensure_fromlist (mod=mod@entry=0x7ffff7e8b248, fromlist=fromlist@entry=0x7ffff7e7d990, \r\n    buf=buf@entry=0xe982b0 \"tensorflow.python.pywrap_tensorflow\", buflen=buflen@entry=17, recursive=recursive@entry=0)\r\n    at Python/import.c:2631\r\n#43 0x0000000000510973 in import_module_level (locals=<optimized out>, level=<optimized out>, fromlist=0x7ffff7e7d990, \r\n    globals=<optimized out>, name=<optimized out>) at Python/import.c:2293\r\n#44 PyImport_ImportModuleLevel (name=<optimized out>, globals=<optimized out>, locals=<optimized out>, fromlist=0x7ffff7e7d990, \r\n    level=<optimized out>) at Python/import.c:2312\r\n#45 0x00000000004edaf4 in builtin___import__ (self=<optimized out>, args=<optimized out>, kwds=<optimized out>)\r\n    at Python/bltinmodule.c:49\r\n#46 0x00000000004616aa in PyObject_Call (func=0x7ffff7fb4fc8, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2547\r\n#47 0x00000000004f248b in PyEval_CallObjectWithKeywords (kw=<optimized out>, arg=<optimized out>, func=<optimized out>)\r\n    at Python/ceval.c:4221\r\n#48 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2624\r\n#49 0x00000000004f915b in PyEval_EvalCodeEx (co=0x7ffff7e71eb0, globals=0x0, globals@entry=0x7ffff7e84d70, locals=0x7fffffffb4e0, \r\n    locals@entry=0x7ffff7e84d70, args=0x0, argcount=1261680879, argcount@entry=0, kws=0x10de358, kws@entry=0x0, kwcount=0, \r\n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3584\r\n#50 0x00000000004f9269 in PyEval_EvalCode (co=co@entry=0x7ffff7e71eb0, globals=globals@entry=0x7ffff7e84d70, \r\n    locals=locals@entry=0x7ffff7e84d70) at Python/ceval.c:669\r\n#51 0x000000000050e668 in PyImport_ExecCodeModuleEx (name=0xb68f50 \"tensorflow.python\", co=0x7ffff7e71eb0, \r\n    pathname=<optimized out>) at Python/import.c:731\r\n#52 0x000000000050e9be in load_source_module (name=0xb68f50 \"tensorflow.python\", \r\n    pathname=0xb6d780 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/python/__init__.pyc\", \r\n---Type <return> to continue, or q <return> to quit---\r\n    fp=0x7ffff7e71eb0) at Python/import.c:1121\r\n#53 0x000000000050fcdc in load_package (name=0xb68f50 \"tensorflow.python\", pathname=<optimized out>) at Python/import.c:1188\r\n#54 0x000000000050f759 in import_submodule (mod=0x7ffff7e80bb0, subname=0xb68f5b \"python\", fullname=0xb68f50 \"tensorflow.python\")\r\n    at Python/import.c:2725\r\n#55 0x000000000051089b in load_next (p_buflen=<synthetic pointer>, buf=0xb68f50 \"tensorflow.python\", p_name=<synthetic pointer>, \r\n    altmod=0x7ffff7e80bb0, mod=0x7ffff7e80bb0) at Python/import.c:2539\r\n#56 import_module_level (locals=<optimized out>, level=<optimized out>, fromlist=0x7ffff7e7d590, globals=<optimized out>, \r\n    name=<optimized out>) at Python/import.c:2256\r\n#57 PyImport_ImportModuleLevel (name=<optimized out>, globals=<optimized out>, locals=<optimized out>, fromlist=0x7ffff7e7d590, \r\n    level=<optimized out>) at Python/import.c:2312\r\n#58 0x00000000004edaf4 in builtin___import__ (self=<optimized out>, args=<optimized out>, kwds=<optimized out>)\r\n    at Python/bltinmodule.c:49\r\n#59 0x00000000004616aa in PyObject_Call (func=0x7ffff7fb4fc8, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2547\r\n#60 0x00000000004f248b in PyEval_CallObjectWithKeywords (kw=<optimized out>, arg=<optimized out>, func=<optimized out>)\r\n    at Python/ceval.c:4221\r\n#61 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2624\r\n#62 0x00000000004f915b in PyEval_EvalCodeEx (co=0x7ffff7e71cb0, globals=0x0, globals@entry=0x7ffff7e82d70, locals=0x7fffffffb4e0, \r\n    locals@entry=0x7ffff7e82d70, args=0x0, argcount=1261680879, argcount@entry=0, kws=0x10de358, kws@entry=0x0, kwcount=0, \r\n    defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3584\r\n#63 0x00000000004f9269 in PyEval_EvalCode (co=co@entry=0x7ffff7e71cb0, globals=globals@entry=0x7ffff7e82d70, \r\n    locals=locals@entry=0x7ffff7e82d70) at Python/ceval.c:669\r\n#64 0x000000000050e668 in PyImport_ExecCodeModuleEx (name=0xb645e0 \"tensorflow\", co=0x7ffff7e71cb0, pathname=<optimized out>)\r\n    at Python/import.c:731\r\n#65 0x000000000050e9be in load_source_module (name=0xb645e0 \"tensorflow\", \r\n    pathname=0xb67f40 \"/home/zhangdingfei/tools/Python-2.7.13/bin/lib/python2.7/site-packages/tensorflow/__init__.pyc\", \r\n    fp=0x7ffff7e71cb0) at Python/import.c:1121\r\n#66 0x000000000050fcdc in load_package (name=0xb645e0 \"tensorflow\", pathname=<optimized out>) at Python/import.c:1188\r\n#67 0x000000000050f759 in import_submodule (mod=0xa14fd0 <_Py_NoneStruct>, subname=0xb645e0 \"tensorflow\", \r\n    fullname=0xb645e0 \"tensorflow\") at Python/import.c:2725\r\n#68 0x00000000005107e6 in load_next (p_buflen=<synthetic pointer>, buf=0xb645e0 \"tensorflow\", p_name=<synthetic pointer>, \r\n    altmod=0xa14fd0 <_Py_NoneStruct>, mod=0xa14fd0 <_Py_NoneStruct>) at Python/import.c:2539\r\n#69 import_module_level (locals=<optimized out>, level=<optimized out>, fromlist=0xa14fd0 <_Py_NoneStruct>, \r\n    globals=<optimized out>, name=<optimized out>) at Python/import.c:2247\r\n#70 PyImport_ImportModuleLevel (name=0x7ffff7e88174 \"tensorflow\", globals=<optimized out>, locals=<optimized out>, \r\n    fromlist=0xa14fd0 <_Py_NoneStruct>, level=<optimized out>) at Python/import.c:2312\r\n#71 0x00000000004edaf4 in builtin___import__ (self=<optimized out>, args=<optimized out>, kwds=<optimized out>)\r\n    at Python/bltinmodule.c:49\r\n#72 0x00000000004616aa in PyObject_Call (func=0x7ffff7fb4fc8, arg=<optimized out>, kw=<optimized out>) at Objects/abstract.c:2547\r\n#73 0x00000000004f248b in PyEval_CallObjectWithKeywords (kw=<optimized out>, arg=<optimized out>, func=<optimized out>)\r\n    at Python/ceval.c:4221\r\n#74 PyEval_EvalFrameEx (f=0x7ffff7fc1010, throwflag=0) at Python/ceval.c:2624\r\n#75 0x00000000004f915b in PyEval_EvalCodeEx (co=0x7ffff7ec11b0, globals=0x0, locals=0x7fffffffb4e0, args=0x0, argcount=1261680879, \r\n    argcount@entry=0, kws=0x10de358, kws@entry=0x0, kwcount=0, defs=0x0, defcount=0, closure=0x0) at Python/ceval.c:3584\r\n#76 0x00000000004f9269 in PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>)\r\n    at Python/ceval.c:669\r\n#77 0x000000000052266c in run_mod (arena=<optimized out>, flags=<optimized out>, locals=<optimized out>, globals=<optimized out>, \r\n    filename=<optimized out>, mod=<optimized out>) at Python/pythonrun.c:1376\r\n#78 PyRun_InteractiveOneFlags (fp=0x7ffff7fc1010, filename=0x0, flags=0x7ffff7ec11b0) at Python/pythonrun.c:857\r\n---Type <return> to continue, or q <return> to quit---\r\n#79 0x00000000005228de in PyRun_InteractiveLoopFlags (fp=fp@entry=0x7ffff753b6e0 <_IO_2_1_stdin_>, \r\n    filename=filename@entry=0x6fa023 \"<stdin>\", flags=flags@entry=0x7fffffffd400) at Python/pythonrun.c:777\r\n#80 0x0000000000522df6 in PyRun_AnyFileExFlags (fp=0x7ffff753b6e0 <_IO_2_1_stdin_>, filename=<optimized out>, closeit=0, \r\n    flags=0x7fffffffd400) at Python/pythonrun.c:746\r\n#81 0x000000000045437b in Py_Main (argc=<optimized out>, argv=<optimized out>) at Modules/main.c:640\r\n#82 0x00007ffff71e8c36 in __libc_start_main () from /lib64/libc.so.6\r\n#83 0x0000000000453541 in _start () at ../sysdeps/x86_64/elf/start.S:113\r\n```", "Seems the problem is on [`TestCPUFeature`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cpu_info.cc#L205). Since you are compiling from source, could you try it again with `--copt=-g` and find the debugger trace?", " I tried to build with -copt=-g , but met another failure:\r\n\r\nlinux-swfm:~/workarea/tensorflow/tensorflow-1.2.0> bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg              \r\nWed Jun 21 15:52:42 CST 2017 : === Using tmpdir: /tmp/tmp.NsE555eNWY\r\n~/workarea/tensorflow/tensorflow-1.2.0/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/workarea/tensorflow/tensorflow-1.2.0\r\n~/workarea/tensorflow/tensorflow-1.2.0\r\n/tmp/tmp.NsE555eNWY ~/workarea/tensorflow/tensorflow-1.2.0\r\nWed Jun 21 15:52:43 CST 2017 : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package: line 38: 71192 Segmentation fault      \"${PYTHON_BIN_PATH:-python}\" setup.py bdist_wheel ${GPU_FLAG} > /dev/null", "hi,  I switched to python3 and solved the above build failure, but still have the \"import tensorflow segment fault\".\r\n\r\nThe debug info is : \r\n\r\n```gdb\r\nlinux-swfm:~/workarea/test> gdb python3\r\n...\r\n(gdb) r\r\n...\r\n>>> import tensorflow \r\n[New Thread 0x7ffff3543700 (LWP 73450)]\r\n[New Thread 0x7ffff2d42700 (LWP 73451)]\r\n..\r\n[New Thread 0x7ffff0541700 (LWP 73452)]\r\n[New Thread 0x7fffedd40700 (LWP 73453)]\r\n..\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fff337680c1 in std::call_once<void (&)()> (__once=..., __f=\r\n    @0x7fff3543c7b6: {void (void)} 0x7fff3543c7b6 <tensorflow::port::(anonymous namespace)::CPUIDInfo::Initialize()>)\r\n    at /home/zhangdingfei/tools/gcc-4.9.2/bin/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/../../../../include/c++/4.9.2/mutex:736\r\n736           __once_callable = &__bound_functor;\r\n(gdb) \r\n(gdb) bt\r\n#0  0x00007fff337680c1 in std::call_once<void (&)()> (__once=..., __f=\r\n    @0x7fff3543c7b6: {void (void)} 0x7fff3543c7b6 <tensorflow::port::(anonymous namespace)::CPUIDInfo::Initialize()>)\r\n    at /home/zhangdingfei/tools/gcc-4.9.2/bin/lib/gcc/x86_64-unknown-linux-gnu/4.9.2/../../../../include/c++/4.9.2/mutex:736\r\n#1  0x00007fff3543d582 in tensorflow::port::(anonymous namespace)::InitCPUIDInfo () at tensorflow/core/platform/cpu_info.cc:306\r\n#2  0x00007fff3543d17d in tensorflow::port::(anonymous namespace)::CPUIDInfo::TestFeature (feature=tensorflow::port::SSE)\r\n    at tensorflow/core/platform/cpu_info.cc:206\r\n#3  0x00007fff3543d599 in tensorflow::port::TestCPUFeature (feature=tensorflow::port::SSE)\r\n    at tensorflow/core/platform/cpu_info.cc:315\r\n#4  0x00007fff3543be7a in tensorflow::port::(anonymous namespace)::CheckFeatureOrDie (feature=tensorflow::port::SSE, \r\n    feature_name=...) at tensorflow/core/platform/cpu_feature_guard.cc:29\r\n#5  0x00007fff3543bfbc in tensorflow::port::(anonymous namespace)::CPUFeatureGuard::CPUFeatureGuard (\r\n    this=0x7fff5368415c <tensorflow::port::(anonymous namespace)::g_cpu_feature_guard_singleton>)\r\n    at tensorflow/core/platform/cpu_feature_guard.cc:62\r\n#6  0x00007fff3543c507 in __static_initialization_and_destruction_0 (__initialize_p=1, __priority=65535)\r\n    at tensorflow/core/platform/cpu_feature_guard.cc:91\r\n#7  0x00007fff3543c51c in _GLOBAL__sub_I_cpu_feature_guard.cc(void) () at tensorflow/core/platform/cpu_feature_guard.cc:130\r\n#8  0x00007fff3586d026 in __do_global_ctors_aux ()\r\n   from /home/zhangdingfei/tools/Python-3.4.6/bin/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007fff2e73112b in _init ()\r\n   from /home/zhangdingfei/tools/Python-3.4.6/bin/lib/python3.4/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n ...\r\n```", "Try with cuDNN 5/5.1 instead of 6.0.  ", "As mentioned on https://www.tensorflow.org/install/install_linux, please try with cudnn 5.1 and see if that helps.", "I build with cudnn 5.1.  the failure still exists in my tensorflow 1.2.0. \r\n-------------\r\ncuda sdk: 8.0\r\ncudnn 5.1\r\n\r\nwhile my tensorflow 1.1.0 works in above environment.\r\n\r\n", "@jart, thoughts about this?", "I'm running into the exact same problem. I have GPU TF on CentOS 7 and it's working fine. I built it on SUSE Enterprise Linux 11 SP4 (gcc 4.8.4) and it's showing this problem.", "Out of curiosity, why did you guys build TensorFlow from source? Did pip installing not work?\r\n\r\nI'm noticing SUSE 11 was released in 2009 but surprisingly enough isn't EOL and seems to keep relatively up to date with certain things.\r\n\r\nI'm also noticing that the GDB trace is tracing through a function that is checking to see if SSE is available on an x86 CPU. What kind of CPU have you guys got?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Hi,\r\n\r\nI am having the same problem but with Tensorflow 1.5.1. I compiled tensorflow from source \r\nin debug mode and after executing \r\n\r\n````\r\n$ python -c \"import tensorflow\"\r\n````\r\ni got the following\r\n\r\n![gdb](https://user-images.githubusercontent.com/3264637/39952361-03c8c51c-5595-11e8-803b-4b37b789677d.png)\r\n\r\nbasically, the segfault occurs inside the function call https://github.com/tensorflow/tensorflow/blob/v1.5.1/tensorflow/core/platform/cpu_info.cc#L305\r\n\r\ni am investigating what could be causing the segfault. It does not seem to be caused by cudnn.\r\n\r\n\r\n", "Did you try using a more recent version of tensorFlow?", "yes, i have the same outcome with tensorflow 1.7.1\r\nThe content of https://github.com/tensorflow/tensorflow/blob/v1.5.1/tensorflow/core/platform/cpu_info.cc has not changed much since then.", "Are you using a custom toolchain? Could you share more details about how TensorFlow was compiled?\r\n\r\nIt seems __tls_get_addr is returning an invalid pointer.  See also:\r\n\r\n- https://sourceware.org/bugzilla/show_bug.cgi?id=21609\r\n- https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58104", "tnx for the links. here are the details of my build (based on the issue template)\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n       - redhat 6.5 \r\n       - native glibc 2.12-1\r\n- **TensorFlow installed from (source or binary)**:\r\n       - from source\r\n- **TensorFlow version **:\r\n       - 1.5.1, 1.7.1\r\n- **Python version**: \r\n       - 3.6.5 (compiled from source with gcc 5.4.0)  \r\n             - i have the same issue with anaconda 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n       - 0.9.0 for tf v1.5.1\r\n       - 0.10.0 for tf v1.7.1\r\n- **GCC/Compiler version (if compiling from source)**:\r\n       - 5.4.0\r\n       - i have the same issue with gcc 4.8.5\r\n- **CUDA/cuDNN version**:\r\n       - cuda 8.0 installed from the runfile for rhel 6 (i.e not from the rhel repo rpms)\r\n       - same problem with cuda 9.0 installed from the runfile\r\n- **GPU model and memory**:\r\n       - nvidia tesla k20m\r\n- **Exact command to reproduce**:\r\n       $ python -c \"import tensorflow\"\r\n\r\ni build tensorflow with the following script\r\n````bash\r\nexport PYTHON_BIN_PATH=\"/progs/usr/bin/tensorflow-1.5.1-py3/bin/python\"\r\nexport PYTHON_LIB_PATH=\"/progs/usr/bin/tensorflow-1.5.1-py3/lib/python3.6/site-packages\"\r\nexport TF_NEED_JEMALLOC=\"0\"\r\nexport TF_NEED_GCP=\"0\"\r\nexport TF_NEED_KAFKA=\"0\"\r\nexport TF_NEED_TENSORRT=\"0\"\r\nexport TF_NEED_HDFS=\"0\"\r\nexport TF_ENABLE_XLA=\"0\"\r\nexport TF_NEED_VERBS=\"0\"\r\nexport TF_NEED_OPENCL_SYCL=\"0\"\r\nexport TF_NEED_OPENCL=\"0\"\r\nexport TF_NEED_CUDA=\"1\"\r\nexport TF_CUDA_CLANG=\"0\"\r\nexport TF_CUDA_VERSION=\"8.0\"\r\nexport CUDA_TOOLKIT_PATH=\"/progs/usr/bin/cuda/cuda-8.0\"\r\nexport GCC_HOST_COMPILER_PATH=\"/progs/usr/bin/gcc-5.4.0/bin/gcc\"\r\nexport TF_CUDNN_VERSION=\"6\"\r\nexport CUDNN_INSTALL_PATH=\"/progs/usr/bin/cuda/cuda-8.0\"\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=\"3.5\"\r\nexport TF_NEED_MPI=\"0\"\r\nexport TF_NEED_GDR=\"0\"\r\nexport TF_NEED_S3=\"0\"\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n#export CC_OPT_FLAGS=\"-O0 -g\"\r\n#export TF_SET_ANDROID_WORKSPACE=\"0\"\r\n\r\n./configure\r\nbazel build -s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone \\\r\n               --jobs=32 --config=opt --config=cuda --linkopt='-lrt -lm' \\\r\n               //tensorflow/tools/pip_package:build_pip_package \r\n````\r\n", "I just tried the example in the link \r\n\r\n````c++\r\n#include <iostream>\r\n#include <thread>\r\n#include <mutex>\r\n \r\nstd::once_flag flag;\r\n \r\nvoid do_once()\r\n{\r\n    std::call_once(flag, [](){ std::cout << \"Called once\" << std::endl; });\r\n}\r\n \r\nint main()\r\n{\r\n    std::thread t1(do_once);\r\n    std::thread t2(do_once);\r\n    std::thread t3(do_once);\r\n    std::thread t4(do_once);\r\n \r\n    t1.join();\r\n    t2.join();\r\n    t3.join();\r\n    t4.join();\r\n}\r\n````\r\ncompiled with \r\n\r\n````bash\r\ngcc -std=c++11 -Wall -Wextra  -pthread -g use_once.cpp -lstdc++ -o use_once\r\n````\r\nand it did not segfault", "i just compiled tensorflow without cuda support and i succeeded in importing tensorflow and running the hello world example\r\n\r\n````bash\r\n   $ python hellow_world.py\r\n2018-05-12 19:32:43.852871: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\nb'Hello, TensorFlow!'\r\n````\r\n\r\nso it looks like when compiling the cuda support some compiler flags are messing up things. (i have the full bazel build command log for the build with cuda and without cuda if that would help)", "@jart should this issue be re-opened? or maybe i can create a new issue since this is also occurring with the master branch and the latest release.", "I wish you'd mentioned RHEL6 earlier. GCC5 is ABI incompatible with GCC4.2. It's not entirely possible to have multiple versions of glibc / libstdc++ shared libraries on the same system. You *might* be able to statically link those libraries using the modern toolchain, although that'd likely make TensorFlow GPLv3, and could potentially cause other issues. There's also `patchelf` hacks. None of this is recommended. We can't provide any official support for RHEL6, because it only supports C++98. Please reach out to the [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) community for further help on working around this.", "tnx @jart for the explanation. That is helpful.\r\nI will look into ``patchelf`` and take this to stack overflow.\r\nBut I am still puzzeled with the fact that i was able to execute a ``hello world`` by building a C++ example and linking it against the ``tensorflow:libtensorflow_cc.so`` shared lib (just for clarity, this shared lib was built with gcc5.4.0)", "ran into the same issue on `GPU` with `conda install tensorflow-gpu` and/or `build from source`, however tensorflow worked on `CPU`,\r\n\r\n```\r\n(gdb) run\r\nStarting program: /anaconda3/envs/tensorflow-gpu/bin/python \r\n[Thread debugging using libthread_db enabled]\r\nPython 3.6.5 | packaged by conda-forge | (default, Apr  6 2018, 13:39:56) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nMissing separate debuginfo for /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/numpy/../../../libiomp5.so\r\nTry: yum --disablerepo='*' --enablerepo='*-debug*' install /usr/lib/debug/.build-id/2f/ffee478c58c351d3624c7aeee95c351cdacfea.debug\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007fffd7b7c5e4 in _ZSt9call_onceIRFvvEJEEvRSt9once_flagOT_DpOT0_ () from /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.132.el6_5.4.x86_64\r\n(gdb) bt full\r\n#0  0x00007fffd7b7c5e4 in _ZSt9call_onceIRFvvEJEEvRSt9once_flagOT_DpOT0_ () from /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\nNo symbol table info available.\r\n#1  0x00007fffd7b7c64e in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) () from /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\nNo symbol table info available.\r\n#2  0x00007fffd7b7c341 in tensorflow::port::(anonymous namespace)::CheckFeatureOrDie(tensorflow::port::CPUFeature, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()\r\n   from /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\nNo symbol table info available.\r\n#3  0x00007fffd7b7c394 in _GLOBAL__sub_I_cpu_feature_guard.cc () from /anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\nNo symbol table info available.\r\n#4  0x000000307ae0e59f in _dl_init_internal () from /lib64/ld-linux-x86-64.so.2\r\nNo symbol table info available.\r\n#5  0x000000307ae12cb5 in dl_open_worker () from /lib64/ld-linux-x86-64.so.2\r\nNo symbol table info available.\r\n#6  0x000000307ae0e1b6 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\nNo symbol table info available.\r\n#7  0x000000307ae124fa in _dl_open () from /lib64/ld-linux-x86-64.so.2\r\nNo symbol table info available.\r\n#8  0x000000368ae00f66 in dlopen_doit () from /lib64/libdl.so.2\r\nNo symbol table info available.\r\n#9  0x000000307ae0e1b6 in _dl_catch_error () from /lib64/ld-linux-x86-64.so.2\r\nNo symbol table info available.\r\n#10 0x000000368ae0129c in _dlerror_run () from /lib64/libdl.so.2\r\nNo symbol table info available.\r\n#11 0x000000368ae00ee1 in dlopen@@GLIBC_2.2.5 () from /lib64/libdl.so.2\r\nNo symbol table info available.\r\n#12 0x00007ffff7c79161 in _PyImport_FindSharedFuncptr (prefix=0x7ffff7d026a6 \"PyInit\", shortname=0x7fffebee3410 \"_pywrap_tensorflow_internal\", \r\n    pathname=0x7fffebe67050 \"/anaconda3/envs/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\", fp=0x0) at ./Python/dynload_shlib.c:95\r\n        p = <value optimized out>\r\n        handle = <value optimized out>\r\n        funcname = \"PyInit__pywrap_tensorflow_internal\\000\\000\\001\", '\\000' <repeats 11 times>, \"\\006\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\062.\\273\\367\\377\\177\\000\\000@\u05b6\\361\\377\\177\\000\\000 k\\377\\377\\377\\177\", '\\000' <repeats 18 times>, \" k\\377\\377\\377\\177\\000\\000\\240\\003\\362\\353\\377\\177\\000\\000\\240\\003\\362\\353\\377\\177\\000\\000\\022\\026\\276\\367\\377\\177\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000X \\320\\367\\377\\177\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\241D\\276\\367\\377\\177\\000\\000\\330p!\\354\\377\\177\\000\\000\\360\\063\\356\\353\\377\\177\\000\\000H2\\356\\353\\377\\177\\000\\000\\251\\005\\304\\367\\377\\177\\000\\000\\240\\003\\362\\353\\377\\177\\000\\000\\004\\273\\267\\367\\377\\177\\000\\000\\320j\\377\\377\\377\\177\\000\\000\\020k\\377\\377\\377\\177\\000\\000\\020k\\377\\377\\377\\177\\000\\000\\377\\177\\000\\000\\000\\000\\000\\000\\210\"\r\n        pathbuf = \"\\033T(\\004\\212;=\\245X \\320\\367\\377\\177\\000\\000origin\\000\\000\\330p!\\354\\377\\177\\000\\000\\360\\330\\355\\353\\377\\177\\000\\000\\030\\363\\355\\353\\377\\177\\000\\000\\240\\227\\273\\361\\377\\177\\000\\000\\033T(\\004\\212;=\\245@\\322\\355\\353\\377\\177\\000\\000\\370=a\", '\\000' <repeats 13 times>\"\\246, \u0577\\367\\377\\177\\000\\000\\000\\311\\355\\353\\377\\177\\000\\000 k\\377\\377\\377\\177\\000\\000\\310q\\227\\000\\000\\000\\000\\000\\316\\b\\271\\367\\377\\177\\000\\000\\001\\000\\000\\000\\177\\000\\000\\000\\265/\\271\\367\\377\\177\\000\\000\\022\\000\\000\\000\\000\\000\\000\\000a3\\271\\367\\377\\177\\000\\000\\240\\227\\273\\361\\377\\177\\000\\000\\227\\000\\000\\000\\000\\000\\000\\000\\260k\\377\\377\\377\\177\\000\\000\\200\\035\\362\\353\\377\\177\\000\\000Pp\\346\\353\\377\\177\\000\\000\\375\\003\\264\\367\\377\\177\\000\\000\\360\\063\\356\\353\\377\\177\\000\\000v\\000\\000\\000\\000\\000\\000\\000\\260k\\377\\377\\377\\177\\000\\000\\200\\234d\\000\\000\\000\\000\\000P\\035\\362\\353\\377\\177\\000\\000;R\\275\\367\\377\\177\\000\\000utf_\"\r\n        dlopenflags = <value optimized out>\r\n#13 0x00007ffff7c4b08f in _PyImport_LoadDynamicModuleWithSpec (spec=0x7fffebedd240, fp=0x0) at ./Python/importdl.c:129\r\n        pathbytes = 0x7fffebe67030\r\n        name_unicode = 0x7fffebf203a0\r\n        name = <value optimized out>\r\n        path = 0x7fffebf21d50\r\n        m = 0x0\r\n        name_buf = 0x7fffebee3410 \"_pywrap_tensorflow_internal\"\r\n        hook_prefix = 0x7ffff7d026a6 \"PyInit\"\r\n        oldcontext = <value optimized out>\r\n        exportfunc = <value optimized out>\r\n        def = <value optimized out>\r\n        p0 = <value optimized out>\r\n#14 0x00007ffff7c4921b in _imp_create_dynamic_impl (module=Unhandled dwarf expression opcode 0xf3\r\n) at Python/import.c:1982\r\n        mod = 0x0\r\n        name = 0x7fffebf203a0\r\n---Type <return> to continue, or q <return> to quit---\r\n        path = 0x7fffebf21d50\r\n        fp = 0x0\r\n#15 _imp_create_dynamic (module=Unhandled dwarf expression opcode 0xf3\r\n) at Python/clinic/import.c.h:289\r\n        return_value = 0x0\r\n        spec = 0x7fffebedd240\r\n        file = 0x0\r\n#16 0x00007ffff7b8c1b9 in PyCFunction_Call (func=0x7ffff1bcbee8, args=0x7fffebedd2b0, kwds=Unhandled dwarf expression opcode 0xf3\r\n) at Objects/methodobject.c:114\r\n        f = 0x7ffff1bcbee8\r\n        meth = 0x7ffff7c49110 <_imp_create_dynamic>\r\n        self = 0x7ffff1bcc4a8\r\n        arg = <value optimized out>\r\n        res = <value optimized out>\r\n        size = <value optimized out>\r\n        flags = 1\r\n#17 0x00007ffff7c2cbe8 in do_call_core (f=Unhandled dwarf expression opcode 0xf3\r\n) at Python/ceval.c:5089\r\n        result = <value optimized out>\r\n        tstate = <value optimized out>\r\n#18 _PyEval_EvalFrameDefault (f=Unhandled dwarf expression opcode 0xf3\r\n) at Python/ceval.c:3391\r\n        func = 0x7ffff1bcbee8\r\n        callargs = 0x7fffebedd2b0\r\n        kwargs = 0x7fffebedfcf0\r\n        stack_pointer = 0x8081f0\r\n        next_instr = 0x7ffff1bd8268\r\n        opcode = <value optimized out>\r\n        oparg = <value optimized out>\r\n        why = <value optimized out>\r\n        fastlocals = <error reading variable fastlocals (Unhandled dwarf expression opcode 0xf3)>\r\n        freevars = <value optimized out>\r\n        retval = 0x0\r\n        tstate = <value optimized out>\r\n        co = <value optimized out>\r\n        instr_ub = -1\r\n        instr_lb = 0\r\n        instr_prev = -1\r\n        first_instr = <value optimized out>\r\n        names = <value optimized out>\r\n        consts = <value optimized out>\r\n        opcode_targets = {0x7ffff7c2aa60, 0x7ffff7c26ffc, 0x7ffff7c2aa17, 0x7ffff7c2a5e6, 0x7ffff7c274f9, 0x7ffff7c2749e, 0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c2745f, 0x7ffff7c273df, 0x7ffff7c27365, 0x7ffff7c272d9, \r\n          0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c2a389, 0x7ffff7c29b88, 0x7ffff7c27f96, 0x7ffff7c2aa60, 0x7ffff7c27ef2, 0x7ffff7c27e55, 0x7ffff7c2aa60, 0x7ffff7c27d8c, 0x7ffff7c27cc3, 0x7ffff7c27c26, 0x7ffff7c27b89, 0x7ffff7c27aec, \r\n          0x7ffff7c27a4f, 0x7ffff7c279b2, 0x7ffff7c27915, 0x7ffff7c2aa60 <repeats 20 times>, 0x7ffff7c27837, 0x7ffff7c2777e, 0x7ffff7c276aa, 0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c275e1, 0x7ffff7c27544, 0x7ffff7c2723c, 0x7ffff7c2aa60, \r\n          0x7ffff7c2719f, 0x7ffff7c270e9, 0x7ffff7c2704f, 0x7ffff7c2992d, 0x7ffff7c29890, 0x7ffff7c26a06, 0x7ffff7c26969, 0x7ffff7c268cc, 0x7ffff7c26828, 0x7ffff7c28bc9, 0x7ffff7c28b35, 0x7ffff7c28a90, 0x7ffff7c289fa, 0x7ffff7c286e8, \r\n          0x7ffff7c2865e, 0x7ffff7c2aa60, 0x7ffff7c285c1, 0x7ffff7c28524, 0x7ffff7c28946, 0x7ffff7c288a9, 0x7ffff7c2880c, 0x7ffff7c28801, 0x7ffff7c26ef1, 0x7ffff7c26e3b, 0x7ffff7c289e3, 0x7ffff7c26c30, 0x7ffff7c2825d, 0x7ffff7c28231, \r\n          0x7ffff7c281df, 0x7ffff7c2813c, 0x7ffff7c280d7, 0x7ffff7c28033, 0x7ffff7c291ee, 0x7ffff7c290f0, 0x7ffff7c2907f, 0x7ffff7c28f99, 0x7ffff7c28ef2, 0x7ffff7c28e62, 0x7ffff7c28dd0, 0x7ffff7c2a45d, 0x7ffff7c2aa60, 0x7ffff7c2a409, \r\n          0x7ffff7c29a12, 0x7ffff7c299ca, 0x7ffff7c29aaa, 0x7ffff7c28d50, 0x7ffff7c28cc9, 0x7ffff7c28c43, 0x7ffff7c29f67, 0x7ffff7c2a75b, 0x7ffff7c2a990, 0x7ffff7c2a4d1, 0x7ffff7c2849a, 0x7ffff7c28413, 0x7ffff7c283bb, 0x7ffff7c2830e, \r\n          0x7ffff7c29265, 0x7ffff7c2a637, 0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c2a73a, 0x7ffff7c2d1de, 0x7ffff7c26756, 0x7ffff7c26756, 0x7ffff7c2aa60, 0x7ffff7c2a57e, 0x7ffff7c2a514, 0x7ffff7c2a6c1, 0x7ffff7c29e63, 0x7ffff7c2aa60, \r\n          0x7ffff7c2aa60, 0x7ffff7c29e2b, 0x7ffff7c29daf, 0x7ffff7c2a034, 0x7ffff7c29f96, 0x7ffff7c2aa60, 0x7ffff7c2a322, 0x7ffff7c29310, 0x7ffff7c29c90, 0x7ffff7c29c25, 0x7ffff7c2aa60, 0x7ffff7c2aa60, 0x7ffff7c297f8, 0x7ffff7c29645, \r\n          0x7ffff7c29537, 0x7ffff7c2951c, 0x7ffff7c29490, 0x7ffff7c29404, 0x7ffff7c29d0a, 0x7ffff7c26aa3, 0x7ffff7c266b7, 0x7ffff7c29af5, 0x7ffff7c26b3b, 0x7ffff7c266b7, 0x7ffff7c2a157, 0x7ffff7c2a2a3, 0x7ffff7c2a1d8, 0x7ffff7c2a8c1, \r\n          0x7ffff7c29384, 0x7ffff7c2cd72, 0x7ffff7c2aa60 <repeats 97 times>}\r\n#19 0x00007ffff7c2501e in _PyEval_EvalCodeWithName (_co=0x7ffff1c03db0, globals=Unhandled dwarf expression opcode 0xf3\r\n) at Python/ceval.c:4153\r\n        co = 0x7ffff1c03db0\r\n        f = 0x808058\r\n        retval = 0x0\r\n        fastlocals = 0x8081d0\r\n---Type <return> to continue, or q <return> to quit---q\r\nQuit\r\n(gdb)\r\n```", "@caot what operating system are you testing this on? version of glibc? ", "@mherkazandjian \r\n\r\n```\r\nglibc-2.12-1.132.el6_5.4.x86_64\r\nCentOS release 6.5 (Final)\r\n```\r\n\r\nAlso tried on glibc/2.14.1", "can you try higher versions of glibc, ? I am not sure how high you should go, but \r\nI solved my problem by using a singularity container of ubuntu16.04 with the default glibc version 2.23.\r\n", "It's kind of challenging to get  glibc 2.23 compiled in CentOS 6.", "All of our newer packages should be using glibc 2.19, as they are back to building on ubuntu 14.\r\nInstead of rebuilding glibc, you may be able to rebuild tf on your machine."]}, {"number": 10869, "title": "GPU option allow_growth and CUDA_ERROR_OUT_OF_MEMORY", "body": "Hi,\r\n\r\nI tried to build a LSTM model on 30G data with a aws machine p2.xlarge with 60G memory and a gpu with 12G memory. The performance is inconsistent. Sometimes we get it to work but sometimes we encounter CUDA_ERROR_OUT_OF_MEMORY. The answer at https://stackoverflow.com/questions/39465503/cuda-error-out-of-memory-in-tensorflow suggests setting a gpu option allow_growth to False. It refers to the source code where the change could be made:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/core/protobuf/config.proto\r\n\r\nBut in the code I notice the only line containing \"allow_growth\" is this:\r\n  bool allow_growth = 4;\r\nIt looks strange to me. Could you explain why allow_growth is assigned a integer value 4? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nFor reference, config.proto is written in the Protocol Buffers language.  The '4' is the field id, not a value that's being assigned.  Details here: \r\nhttps://developers.google.com/protocol-buffers/docs/overview"]}, {"number": 10868, "title": "[XLA] Add override of Permute to fix compile errors", "body": "This is an attempted fix for older compilers that can't deduce the template args.", "comments": ["@tatatodd is this one ready to be merged?", "@yifeif No, this isn't ready to be merged; it's an attempted fix for #10843, which we're still working out.\r\n\r\nThanks for checking though!  I'll ping when it is eventually ready.", "Got it, sounds good. Thanks @tatatodd!", "Jenkins, please test this.", "@asimshankar For context, this is the fix for the variadic template issue you helped me look at."]}, {"number": 10867, "title": "C++ api in Debug x64 mode not building for VS2015", "body": "Hi,\r\n\r\nI'm not sure if this is expected behaviour or a problem with the solution file generated by CMake, so my apologies if I'm writing in the wrong place.\r\n\r\nI'm trying to build Tensorflow C++ API on Windows using VS2015, and so far I didn't have problems to build Release x64 binaries, which just required me to ensure that the toolset for 64 bits is in use when creating the def file (dump bin.exe and undname.exe in particular).\r\n\r\nI could verify that the build works as expected, being able to load models saved in Python and perform inference successfully.\r\n\r\nHowever, when building the debug x64 version of the .dll, I'm being blocked by the following error:\r\n\r\n```\r\n104>  tensorflow_static.vcxproj -> D:\\out\\Debug\\tensorflow_static.lib\r\n104>  symbols=1016688, taken=140782, dupes=3472\r\n105>------ Build started: Project: tensorflow, Configuration: Debug x64 ------\r\n105>  Building Custom Rule D:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n105>  CMake does not need to re-run because D:/out/CMakeFiles/generate.stamp is up-to-date.\r\n105>LINK : fatal error LNK1189: library limit of 65535 objects exceeded\r\n========== Build: 104 succeeded, 1 failed, 1 up-to-date, 0 skipped ==========\r\n```\r\n\r\nIs there any workaround for this issue? Has anyone managed to build the debug version of the library? Is it possible at all?\r\n\r\nFrom what I could google around, this linker issue is not easy to overcome, so feedback is most welcome.", "comments": ["Perhaps @mrry or @asimshankar might have some ideas here.", "As a sidenote, I'm trying to build other configurations available in the solution like: MinSizeRel or RelWithDebInfo and get hit by compiler errors of the form:\r\n\r\n`d:\\out\\external\\eigen_archive\\eigen\\src/Core/products/GeneralMatrixVector.h(351): fatal error C1060: compiler is out of heap space (compiling source file D:\\tensorflow\\tensorflow\\core\\kernels\\matmul_op.cc)`", "After digging for a while I found the following (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md):\r\n\r\n> Note that the -DCMAKE_BUILD_TYPE=Release flag must match the build configuration that you choose when invoking msbuild. The known-good values are Release and RelWithDebInfo. The Debug build type is not currently supported, because it relies on a Debug library for Python (python35d.lib) that is not distributed by default.\r\n> \r\n> There are various options that can be specified when generating the solution and project files:\r\n> \r\n> -DCMAKE_BUILD_TYPE=(Release|RelWithDebInfo): Note that the CMAKE_BUILD_TYPE option must match the build configuration that you choose when invoking MSBuild in step 4. The known-good values are Release and RelWithDebInfo. The Debug build type is not currently supported, because it relies on a Debug library for Python (python35d.lib) that is not distributed by default.\r\n\r\nWhich is consistent with my observations. I will try with RelWithDebInfo on CMake and see what happens. ", "I suppose that if you need only C++ API then you need only python3.exe to run script that check versions and create_def_file.py. The problem is related to the second one script.", "@mrlzla What do you mean by second one script? \r\n\r\nMy problem -just to rephrase- is that I could not manage to build the debug version of tensorflow.dll. As far as I see this is not supported... Does this mean that I cannot have a project built in debug configuration which uses tensorflow.dll in VS2015?", "```create_def_file.py```.It seems like we have to reduce the number of symbols that is exported from tensorflow.dll file.\r\nAs we can see in ```create_def_file.py```:\r\n```\r\nBecause the linker allows only 64K symbols to be exported per dll\r\nwe filter the symbols down to the essentials. The regular expressions\r\nwe use for this are specific to tensorflow.\r\n```\r\nthe symbols are already filtered, but the number of them still more than 65535", "As a workaround compiling tensorflow_static.lib in debug should work.\r\n\r\nThe `compiler is out of heap space` error happens if you haven't activated the 64bit toolset. Note that the `vcvarsall.bat` command only affects the `MSBuild` command and if you want visual studio to use the 64bit toolset you need to launch it with this commands\r\n`set PreferredToolArchitecture=x64`\r\n`\"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\devenv.exe\"`", "@guschmue as he looked into this specific issue", "I know the reason  why the debug build runs into the 65535 symbol linker limit:\r\nThe debug build does not expand inlines. The fix is to change it CMakeLists.txt:\r\nset(CMAKE_CXX_FLAGS_DEBUG \"/D_DEBUG /MDd /Ob0\")\r\nto\r\nset(CMAKE_CXX_FLAGS_DEBUG \"/D_DEBUG /MDd /Ob2\")\r\n\r\nI'll send a PR for this.", "@Garoe I have made sure that the 64 bit toolset is being used and while I can build Release, VS2015 still fails to build the other configurations due to ` compiler is out of heap space`.\r\n\r\n@guschmue Thank you very much for pointing out this issue. I tried changing CMakeLists.txt but now Debug build crashes earlier due to not enough heap space for the compiler:\r\n\r\n`57>d:\\tensorflow-1.2.0\\tensorflow\\contrib\\cmake\\build\\external\\eigen_archive\\eigen\\src\\core\\products\\generalblockpanelkernel.h(1902): fatal error C1002: compiler is out of heap space in pass 2`\r\n\r\nI'm not sure if this fix that you propose will not uncover different issues.\r\n", "I have seen this on a 8GB box in the past. msvc uses lots memory for some of files in tf, believed to be related to the amount of templates used in tf. I know there are bugs filed for msvc and hope this will improve in the future.\r\nIn the meantime you could try the following:\r\n- kill other apps that use a lot of memory (in my case I had pycharm running in the background)\r\n- check the pagefile size and give it 16 or 24GB", "hi @pabloxrl, how do you build the tf with vs? Would you please give some detail about the process?I'm trying to debug the code with an IDE on Mac,but did't know how to do it.Do you have any blog or gist?", "@haiy It is worth noting that I could only manage so far to build Release x64 using VS2015. Building Debug x64 doesn't seem problematic according to @guschmue remarks, but I have space limitations in my machine so it will take me few days to verify that.\r\n\r\nFor Release x64 I followed the steps specified in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md, with the exception that I didn't enable CUDA GPU support. I encourage you to use the recommended setup as it is the less-painful way to get started. Nevertheless, the whole page is worth a read (see, current known limitations section).\r\n\r\nThe only special thing to add, if the C++ API is what you're after, would be to enable the flag: \r\n`-Dtensorflow_BUILD_SHARED_LIB` when you run cmake, which is what on Windows gives you the option to build `tensorflow_static.lib`, `tensorflow.lib` and `tensorflow.dll`.\r\n\r\nI'll try to drop by to comment on my progress with the Debug build.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "https://github.com/BowieHsu/Tensorflow-windows-dll wish this could help you, complied tensorflow gpu dll", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. If you are still facing this problem, please create a new issue using one of the issue templates. Thanks!"]}, {"number": 10866, "title": "word2vec error sequence index must be integer, not 'slice'", "body": "Im tring to run the code :\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L117\r\n\r\nBut get the exception on the Line 117 \r\nas \r\nsequence index must be integer, not 'slice'\r\n\r\nI want to ask if this really a problem in the tensorflow file or its just me facing this error.", "comments": ["I guess it's an issue on your end. I have no issue executing it with `Python 2` and `Python 3.` on `Ubuntu 17.04` distribution  \r\n\r\nI would suggest doing the following for `Python 2` or you may use pip3 for `Python 3.`\r\n\r\n-  `pip install --upgrade tensorflow` \r\n\r\n-   Update all dependencies which you are required for `word2vec_basic.py`\r\n\r\n-   Try running it again. If you don't get success than share your OS, Tensor flow and Python log.\r\n \r\n\r\n", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "I also have this error. \r\n\r\nplatform: mac os 10.12.5\r\ntensorflow version: r1.0.1\r\npython version: 2.7.13\r\n", "@endymecy Did you tried executing `pip install --upgrade tensorflow` as stated in my above post?", "After I upgrade tensorflow to 1.2.0 and I don't get success", "I changed code in that line to make it work", "@shanalikhan What did you change to make it work?  I'm having the same trouble.", "I'm in office cant access that code\r\ninstead of \r\n> buffer[:] = data[:span]\r\n\r\nI used something like\r\n\r\n```\r\n for word in data[:span]\r\n   buffer.append(data)\r\n```", "@shanalikhan When you have a moment with access, it'd be lovely to have the precise answer, and then we could close this issue. Thanks for your help!", "The actual code that worked for me was to replace:\r\n~~~~\r\n      buffer[:] = data[:span]\r\n~~~~\r\n\r\nWith:\r\n\r\n~~~~\r\n      for word in data[:span]:\r\n        buffer.append(word)\r\n~~~~\r\n\r\n", "@cy89 its the same one i wrote :)\r\nAs this issue was opened by me, I should close this issue as its solved"]}]