[{"number": 13320, "title": "eager: Correctly convert a list of Dimensions to a TFE_TensorHandle", "body": "", "comments": ["Jenkins, test this please"]}, {"number": 13319, "title": "Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10.\r\n- **TensorFlow installed from (source or binary)**: Binary.\r\n- **TensorFlow version (use command below)**: tensorflow-1.3.0-cp36-cp36m-win_amd64.whl\r\n- **Python version**: 3.6.\r\n- **Bazel version (if compiling from source)**: N/A.\r\n- **CUDA/cuDNN version**: N/A.\r\n- **GPU model and memory**: GT 650M.\r\n- **Exact command to reproduce**: `python` followed by `import tensorflow as tf`.\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI followed exactly on how to install Tensorflow, but whenever I try to do something involving Tnesorflow this error comes up. I installed Tensorflow using Anaconda 4.4. I first typed `conda create -n tensorflow python=3.6` then `activate tensorflow` then `pip install --ignore-installed --upgrade tensorflow`. After that, I typed `python` then `import tensorflow as tf` I get this error: `Intel MKL FATAL ERROR: Cannot load mkl_intel_thread.dll.`\r\n![untitled](https://user-images.githubusercontent.com/31815007/30871645-1adda5ac-a31a-11e7-8ec8-8a323e8054a8.png)\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["The installation guide includes this warning:\r\n \"The Anaconda installation is community supported, not officially supported.\"\r\n\r\nYou might try the pip install, or see whether you can get help with the conda install on stackoverflow.", "Also, we have no MKL support on Windows yet.\r\nAll our build rules are only written for ubuntu at the moment.", "I had 2 versions of Python installed. One basic version of Python and one installed by Anaconda. When running my machine learning script it threw the same error. When I uninstalled one of the two versions (in my case I removed Anaconda) everything works fine again. I use deeplearning4j as library and apparently it has difficulties to resolve this dll, cause there were multiple Python installations installed."]}, {"number": 13318, "title": "The pip upgrade in windows does not get up-to-date tensorflow codes on github", "body": "Tensorflow version: 1.3\r\nOS: Windows 10\r\n\r\nWhen I use pip upgrade to get tensorflow, the codes I got did not reflect the newest updates in Github. For example, in \"tensorflow/tensorflow/contrib/layers/python/layers/layers.py\" the up-to-date batch_norm() function supports not None param_regularizers. However, the codes updated by pip still not support param_regularizers, as the codes are not up-to-date.\r\n\r\nIs there any other way to get the most updated tensorflow in Windows? Thank you.", "comments": ["run `pip list` and report the line for tensorflow.\r\nrun python and do\r\n```\r\nimport tensorflow as tf\r\nprint (tf.GIT_VERSION)\r\n```\r\nand report that as well.\r\n", "Pip list results:\r\nbleach (1.5.0)\r\nhtml5lib (0.9999999)\r\nMarkdown (2.6.9)\r\nnumpy (1.13.1)\r\npip (9.0.1)\r\nprotobuf (3.4.0)\r\nsetuptools (36.5.0)\r\nsix (1.11.0)\r\ntensorflow-gpu (1.3.0)\r\ntensorflow-tensorboard (0.1.6)\r\nWerkzeug (0.12.2)\r\nwheel (0.30.0)\r\n\r\nThe tf.GIT_VERSION shows: b'unknown'", "What about tf.VERSION?", "tf.VERSION is '1.3.0'", "Amit, is it possible we uploaded a wrong binary for windows?", "So the reason for this is that the 1.3 branch was cut prior to that feature being updated.\r\n\r\nMaster: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L620\r\n\r\n1.3: https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/layers/python/layers/layers.py#L548\r\n\r\nClosing as designed."]}, {"number": 13317, "title": "TF v1.3 slower than v1.2 when used with ResNets", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nIt's a custom code with fully convolutional ResNet using tf.slim implementation (with diluted kernels)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDebian 8.9\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary (pip)\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\nPython 3.4\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0/cuDNN 6.0\r\n- **GPU model and memory**:\r\nNVIDIA K40m 12Gb\r\n\r\nI run fully convolutional ResNet-101 on the images which vary in size. When moving from TF 1.2 to TF 1.3 inference became about 3x slower. With TF1.2 I use CUDA 8.0 and cudnn 5.1. To make sure variable sized images are processed fast I set env variable TF_CUDNN_USE_AUTOTUNE=0 to switch off auto-tuning of convolutions.\r\n\r\nIn case it is not to do with convolutions, but the data loading, here's how I feed the input data (numpy arrays) into the convnet:\r\n\r\n```python\r\noutputs_np = sess.run(outputs, feed_dict={inputs: batch})\r\n```\r\n\r\nCould you suggest how I can troubleshoot that?", "comments": ["The training performance has been stable in the official benchmarks (https://benchmarks-dot-tensorflow-testing.appspot.com/test/tf-cnn-benchmark-resnet50). But it uses fixed size image, with autotune enabled. Maybe you can try fixed-size autotune as well to narrow down the causes.", "What makes you think the slowdown is due to data loading, as opposed to convolutional computation?  What kinds of measurements have you made?  I notice that you're using a pip binary instead of building from source.  Could the slowdown be due to lack of SIMD support for CPU operations?", "@poxvoculi Hi, I may have not been precise. I only indicated the possibility it is not the computations that slow it down, but the data processing part. I am not sure how to benchmark it, because from the user point of view sess.run() does both compute and data feeding, so I cannot evaluate them individually. I use GPU version and not CPU, and in both cases (TF 1.2 and 1.3) I use pre-combiled builds, so it's comparable setting.\r\n\r\n@ppwwyyxx I'll try to narrow down the the issue.."]}, {"number": 13316, "title": "Concatenate in alternate fashion two tensors", "body": "Good Afternoon.\r\nI'd like to concatenate two tensors of shape (None,16) in alternate fashion.\r\nFor example, with simple arrays, if the inputs are:\r\na=[[1,2,3],[1,2,3],...]\r\nb=[[4,5,6],[4,5,6],...]\r\nI want this output:\r\nc=[[1,4,2,5,3,6],[1,4,2,5,3,6],...]\r\nHow can I do it? \r\nI can't loop on tensors because of unknown shape[0], zip function isn't supported for tensors (tensor object is not iterable).\r\nThank u all in advance", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nHint: Some combination of `tf.stack, tf.reshape, and tf.transpose` should be able to do this."]}, {"number": 13315, "title": "Branch 170043510", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13314, "title": "typo fix", "body": "found while trying to compile on ARM based Raspberry Pi 2 in branch `r.1.3`", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "`I signed it!`", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 13313, "title": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables", "body": "I download a code about vgg from web, and use it on my own datasets. But it shows \"No gradients provided for any variable, check your graph for ops that do not support gradients, between variables\" error.\r\ncode as `below:`\r\n# -*- coding: UTF-8 -*-\r\nimport tensorflow as tf\r\nimport read_data\r\nimport numpy as np\r\ndef conv_op(input_op, name, kh, kw, n_out, dh, dw, p):\r\n    '''\r\n    Args:\r\n    input_op\uff1a\u8f93\u5165\u7684tensor\r\n    name\uff1a\u8fd9\u4e00\u5c42\u7684\u540d\u79f0\r\n    kh\uff1akernel height\u5373\u5377\u79ef\u6838\u7684\u9ad8\r\n    kw\uff1akernel weight\u5373\u5377\u79ef\u6838\u7684\u5bbd\r\n    n_out\uff1a\u5377\u79ef\u6838\u6570\u91cf\u5373\u8f93\u51fa\u901a\u9053\u6570\r\n    dh\uff1a\u6b65\u957f\u7684\u9ad8\r\n    dw\uff1a\u6b65\u957f\u7684\u5bbd\r\n    p\uff1a\u53c2\u6570\u5217\u8868\r\n    '''\r\n    n_in = input_op.get_shape()[-1].value # \u83b7\u53d6input_op\u7684\u901a\u9053\u6570\r\n\r\n    with tf.name_scope(name) as scope: # \u8bbe\u7f6escope\uff0c\u751f\u6210\u7684Variable\u4f7f\u7528\u9ed8\u8ba4\u7684\u547d\u540d\r\n        kernel = tf.get_variable(scope+\"w\",  # kernel\uff08\u5373\u5377\u79ef\u6838\u53c2\u6570\uff09\u4f7f\u7528tf.get_variable\u521b\u5efa\r\n                                 shape=[kh, kw, n_in, n_out], # \u3010\u5377\u79ef\u6838\u7684\u9ad8\uff0c\u5377\u79ef\u6838\u7684\u5bbd\u3001\u8f93\u5165\u901a\u9053\u6570\uff0c\u8f93\u51fa\u901a\u9053\u6570\u3011\r\n                                 dtype=tf.float32, \r\n                                 initializer=tf.contrib.layers.xavier_initializer_conv2d()) # \u53c2\u6570\u521d\u59cb\u5316\r\n        # \u4f7f\u7528tf.nn.conv2d\u5bf9input_op\u8fdb\u884c\u5377\u79ef\u5904\u7406\uff0c\u5377\u79ef\u6838kernel\uff0c\u6b65\u957fdh*dw\uff0cpadding\u6a21\u5f0f\u4e3aSAME\r\n        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding='SAME') \r\n        bias_init_val = tf.constant(0.0, shape=[n_out], dtype=tf.float32) # biases\u4f7f\u7528tf.constant\u8d4b\u503c\u4e3a0\r\n        biases = tf.Variable(bias_init_val, trainable=True, name='b') # \u5c06bias_init_val\u8f6c\u6210\u53ef\u8bad\u7ec3\u7684\u53c2\u6570\r\n        z = tf.nn.bias_add(conv, biases) # \u5c06\u5377\u79ef\u7ed3\u679cconv\u548cbias\u76f8\u52a0\r\n        activation = tf.nn.relu(z, name=scope) # \u5bf9z\u8fdb\u884c\u975e\u7ebf\u6027\u5904\u7406\u5f97\u5230activation\r\n        p += [kernel, biases]  # \u521b\u5efa\u5377\u79ef\u5c42\u65f6\u7528\u5230\u7684\u53c2\u6570kernel\u548cbias\u6dfb\u52a0\u8fdb\u53c2\u6570\u5217\u8868\r\n        return activation # \u5c06\u5377\u79ef\u5c42\u7684\u8f93\u51faactivation\u4f5c\u4e3a\u51fd\u6570\u7ed3\u679c\u8fd4\u56de\r\n\r\n# \u5b9a\u4e49\u5168\u8fde\u63a5\u5c42\u7684\u521b\u5efa\u51fd\u6570\r\ndef fc_op(input_op, name, n_out, p):  \r\n    n_in = input_op.get_shape()[-1].value # \u83b7\u53d6tensor\u7684\u901a\u9053\u6570 \r\n    with tf.name_scope(name) as scope:\r\n        kernel = tf.get_variable(scope+\"w\", # \u4f7f\u7528tf.get_variable\u521b\u5efa\u5168\u8fde\u63a5\u5c42\u7684\u53c2\u6570\r\n                                 shape=[n_in, n_out], # \u53c2\u6570\u7684\u7ef4\u5ea6\u6709\u4e24\u4e2a\uff0c\u8f93\u5165\u901a\u9053\u6570\u548c\u8f93\u51fa\u901a\u9053\u6570\r\n                                 dtype=tf.float32, \r\n                                 initializer=tf.contrib.layers.xavier_initializer())\r\n        # biases\u8d4b\u503c0.1\u4ee5\u907f\u514ddead neuron\r\n        biases = tf.Variable(tf.constant(0.1, shape=[n_out], dtype=tf.float32), name='b') \r\n        # \u5bf9\u8f93\u5165\u53d8\u91cfinput_op\u548ckernel\u505a\u77e9\u9635\u4e58\u6cd5\u5e76\u52a0\u4e0abiases\u3002\u518d\u505a\u975e\u7ebf\u6027\u53d8\u6362activation\r\n        activation = tf.nn.relu_layer(input_op, kernel, biases, name=scope) \r\n        p += [kernel, biases]\r\n        return activation\r\n\r\n# \u5b9a\u4e49\u6700\u5927\u6c60\u5316\u5c42\u7684\u521b\u5efa\u51fd\u6570\r\ndef mpool_op(input_op, name, kh, kw, dh, dw): \r\n    return tf.nn.max_pool(input_op,\r\n                          ksize=[1, kh, kw, 1], # \u6c60\u5316\u5c42\u5c3a\u5bf8kh*kw\r\n                          strides=[1, dh, dw, 1], # \u6b65\u957fdh*dw\r\n                          padding='SAME',\r\n                          name=name)\r\n\r\n\r\n########\u5f00\u59cb\u521b\u5efaVGGNet-16\u7684\u7f51\u7edc\u7ed3\u6784########\r\ndef inference_op(input_op, keep_prob):\r\n    '''\r\n    VGGNet-16\u7684\u7f51\u7edc\u7ed3\u6784\u4e3b\u8981\u5206\u4e3a6\u4e2a\u90e8\u5206\uff1a\u524d\u4e94\u6bb5\u4e3a\u5377\u79ef\u7f51\u7edc\uff0c\u6700\u540e\u4e00\u6bb5\u662f\u5168\u8fde\u63a5\u7f51\u7edc\u3002\r\n    Args:\r\n    input_op\uff1a\u8f93\u5165Tensor\r\n    keep_prob\uff1a\u63a7\u5236Dropout\u7684\u4e00\u4e2aplaceholder\r\n    '''\r\n    # \u521d\u59cb\u5316\u53c2\u6570\u5217\u8868p\r\n    p = []\r\n    # assume input_op shape is 224x224x3\uff08\u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\u7684\u8f93\u5165input_op\uff09\r\n\r\n    # \u521b\u5efa\u7b2c\u4e00\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 112x112x64\r\n    # \u4e24\u4e2a\u5377\u79ef\u5c42\u7684\u5377\u79ef\u6838\u90fd\u662f3*3\uff0c\u5377\u79ef\u6838\u6570\u91cf\uff08\u8f93\u51fa\u901a\u9053\u6570\uff09\u5747\u4e3a64\uff0c\u6b65\u957f1*1\uff0c\u5168\u50cf\u7d20\u626b\u63cf\u3002\r\n    conv1_1 = conv_op(input_op, name=\"conv1_1\", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64\r\n    conv1_2 = conv_op(conv1_1,  name=\"conv1_2\", kh=3, kw=3, n_out=64, dh=1, dw=1, p=p) # outputs 224x224x64\r\n    pool1  = mpool_op(conv1_2,  name=\"pool1\",   kh=2, kw=2, dw=2, dh=2) # \u6807\u51c6\u76842*2\u7684\u6700\u5927\u6c60\u5316-outputs 112x112x64\r\n\r\n    # \u521b\u5efa\u7b2c\u4e8c\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 56x56x128\r\n    conv2_1 = conv_op(pool1,    name=\"conv2_1\", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)\r\n    conv2_2 = conv_op(conv2_1,  name=\"conv2_2\", kh=3, kw=3, n_out=128, dh=1, dw=1, p=p)\r\n    pool2 = mpool_op(conv2_2,   name=\"pool2\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # \u521b\u5efa\u7b2c\u4e09\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 28x28x256\r\n    # conv3_1 = conv_op(pool2,    name=\"conv3_1\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)\r\n    # conv3_2 = conv_op(conv3_1,  name=\"conv3_2\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)\r\n    # conv3_3 = conv_op(conv3_2,  name=\"conv3_3\", kh=3, kw=3, n_out=256, dh=1, dw=1, p=p)    \r\n    # pool3 = mpool_op(conv3_3,   name=\"pool3\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # # \u521b\u5efa\u7b2c\u56db\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 14x14x512\r\n    # conv4_1 = conv_op(pool3,    name=\"conv4_1\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv4_2 = conv_op(conv4_1,  name=\"conv4_2\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv4_3 = conv_op(conv4_2,  name=\"conv4_3\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # pool4 = mpool_op(conv4_3,   name=\"pool4\",   kh=2, kw=2, dh=2, dw=2)\r\n\r\n    # # \u521b\u5efa\u7b2c\u4e94\u6bb5\u5377\u79ef\u7f51\u7edc -- outputs 7x7x512\r\n    # conv5_1 = conv_op(pool4,    name=\"conv5_1\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv5_2 = conv_op(conv5_1,  name=\"conv5_2\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # conv5_3 = conv_op(conv5_2,  name=\"conv5_3\", kh=3, kw=3, n_out=512, dh=1, dw=1, p=p)\r\n    # pool5 = mpool_op(conv5_3,   name=\"pool5\",   kh=2, kw=2, dw=2, dh=2)\r\n\r\n    # \u5907\u6ce8\uff1aVGGNet-16\u7684\u6bcf\u4e00\u6bb5\u5377\u79ef\u7f51\u7edc\u90fd\u4f1a\u5c06\u56fe\u50cf\u7684\u8fb9\u957f\u7f29\u5c0f\u4e00\u534a\uff0c\u4f46\u662f\u5c06\u5377\u79ef\u8f93\u51fa\u901a\u9053\u6570\u7ffb\u500d\u3002\r\n    # \u7b2c\u4e94\u6bb5\u5377\u79ef\u8f93\u51fa\u7684\u901a\u9053\u6570\u4e0d\u518d\u589e\u52a0\u3002\r\n\r\n    # flatten \u5c06\u7b2c\u4e94\u6bb5\u5377\u79ef\u7f51\u7edc\u7684\u8f93\u51fa\u7ed3\u679c\u8fdb\u884c\u6241\u5e73\u5316\r\n    shp = pool2.get_shape()\r\n    flattened_shape = shp[1].value * shp[2].value * shp[3].value\r\n\r\n    # tf.reshape\u51fd\u6570\u5c06\u6bcf\u4e2a\u6837\u672c\u5316\u4e3a\u957f\u5ea67*7*512 = 25088\u7684\u5411\u91cf\r\n    resh1 = tf.reshape(pool2, [-1, flattened_shape], name=\"resh1\") \r\n\r\n    # fully connected \u9690\u542b\u8282\u70b94096\u7684\u5168\u8fde\u63a5\u5c42\r\n    fc6 = fc_op(resh1, name=\"fc6\", n_out=4096, p=p)\r\n    fc6_drop = tf.nn.dropout(fc6, keep_prob, name=\"fc6_drop\")\r\n\r\n    fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=4096, p=p)\r\n    fc7_drop = tf.nn.dropout(fc7, keep_prob, name=\"fc7_drop\")\r\n\r\n    fc8 = fc_op(fc7_drop, name=\"fc8\", n_out=10, p=p)\r\n    softmax = tf.nn.softmax(fc8) # \u5f97\u5230\u5206\u7c7b\u8f93\u51fa\u6982\u7387\r\n    predictions = tf.argmax(softmax, 1) # tf.argmax\u6c42\u8f93\u51fa\u6982\u7387\u6700\u5927\u7c7b\u522b\r\n    print predictions.get_shape()\r\n    return predictions, softmax, fc8, p\r\n\r\ndef main(file_name, batch_size, iter_times):\r\n    x = tf.placeholder('float', shape = [batch_size, 32, 32, 3])\r\n    y = tf.placeholder('float', shape = [batch_size, 1])\r\n    predictions, _, _, _ = inference_op(x, keep_prob = 0.5)\r\n    predictions = tf.cast(predictions, tf.float32)\r\n    \r\n    ##### tf.equal \u8fd4\u56de\u7684\u662fbool tensor  ######\r\n    ##### tf.reduce_mean() \u4e0d\u80fd\u662fbool\u503c ######\r\n    print predictions.get_shape()\r\n    loss = tf.nn.softmax_cross_entropy_with_logits(labels= tf.transpose(y, perm = [1, 0]), logits = predictions)\r\n    loss = tf.reduce_mean(loss)\r\n    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\r\n\r\n    sess = tf.Session()\r\n    init = tf.global_variables_initializer()\r\n\r\n    for i in xrange(iter_times):\r\n        train_x, train_y = read_data.fetch_data(file_name, batch_size)\r\n        sess.run(init)\r\n        train_x, train_y = sess.run([train_x, train_y])\r\n        train.eval(feedict = {x: train_x, y: train_y})\r\n        print sess.run(predictions)\r\n        if i % 100 == 0 :\r\n            print \"%d step accuarcy is %f\" % (i, sess.run(loss))\r\n\r\nmain('train.tfrecords', 30, 2000)\r\n\r\n", "comments": ["The main reason is the incorrrect type of y and prediction .", "@LaiPiXiong \u80fd\u8bf4\u6e05\u695a\u70b9\u513f\u4e48\uff1f\u6211\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u5728Tensorboard\u7684graph\u4e0a\uff0cGradient\u90a3\u4e2aop, \u6240\u6709\u7684bias\u90fd\u4e0d\u5728Gradient\u7684\u8f93\u5165\u4e2d\uff0c\u597d\u5947\u602a\u3002\u3002\u3002", "\u5c31\u662f\u8bf4\u4f60\u7528\u5728\u68af\u5ea6\u4e2d\u7684y \u6216\u8005 \u201cprediction\u201d\u7684\u7c7b\u578b\u8981\u518d\u68c0\u67e5\u4e00\u4e0b\u5bf9\u4e0d\u5bf9\uff0c\u6211\u4e5f\u9047\u5230\u8fd9\u4e2a\u95ee\u9898\uff0c\u53d1\u73b0\u6211\u5728\u8ba1\u7b97\u4ea4\u53c9\u71b5\u7684\u65f6\u5019\u7528\u9519\u4e86\u53c2\u6570\uff0c\u4f60\u4e3b\u8981\u68c0\u67e5\u4f60\u7528\u68af\u5ea6\u51fd\u6570\u7684\u65f6\u5019\u7528\u7684\u53c2\u6570\u95ee\u9898\uff0c\u6211\u7684\u662f\u8fd9\u4e48\u89e3\u51b3\u7684\uff1b\r\n\u518d\u6b21\u611f\u8c22LaiPiXiong@LaiPiXiong"]}, {"number": 13312, "title": "Support TensorArray in BeamSearchDecoder state.", "body": "#13208 attempted to fix #13154 by representing the `alignment_history` field with a `Tensor` instead of a `TensorArray`. However, @ebrevdo pointed out that this approach led to a quadratic time and space overhead.\r\n\r\nThis PR fixes the issue by directly adding the support for `TensorArray` in the `BeamSearchDecoder` state as proposed by @ebrevdo.\r\n\r\n@ebrevdo Let me know what you think of this implementation. Thanks!", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "We can also update gathertree to support more input dtypes.\n\nOn Sun, Oct 8, 2017 at 7:51 AM, Steven Ding <notifications@github.com>\nwrote:\n\n> *@steven-hh-ding* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r143356707>\n> :\n>\n> > @@ -120,13 +120,79 @@ def tile_batch(t, multiplier, name=None):\n>      return nest.map_structure(lambda t_: _tile_batch(t_, multiplier), t)\n>\n>\n> -def _check_maybe(t):\n> +def _map_function(fn, t):\n> +  \"\"\"Maps the function fn on each element of the TensorArray t.\"\"\"\n> +  loop_stop = lambda mapped, elems, i: (\n> +      i == elems.size())\n> +  loop_body = lambda mapped, elems, i: (\n> +      mapped.write(i, fn(elems.read(i))), elems, i + 1)\n> +  mapped = tensor_array_ops.TensorArray(t.dtype, size=0, dynamic_size=True)\n> +  mapped, _, _ = control_flow_ops.while_loop(\n>\n> Thank you very much for the PR. I am also working on the same problem.\n> Instead of picking out the right beams at each time step, we use\n> gather_tree to sort out the right beams to avoid a quadratic complexity.\n> Unfortunately the gather_tree only supports int32, so we did a dirty cast.\n> It is fine for us as the alignments are only used for visualization on\n> tensorboard. We update the finalize function and skip tensorarray for\n> _merge_batch_beams and _split_batch_beams.\n>\n>   def finalize(self, outputs, final_state, sequence_lengths):\n>     predicted_ids = beam_search_ops.gather_tree(\n>         outputs.predicted_ids, outputs.parent_ids,\n>         sequence_length=sequence_lengths)\n>     def gather_tree_from_array(t):\n>         if not isinstance(t, tensor_array_ops.TensorArray):\n>             return t\n>         history = t.stack()\n>         history = rnn._transpose_batch_time(history)\n>         history = math_ops.cast(history * 10000, dtypes.int32)\n>         history = beam_search_ops.gather_tree(\n>             history, outputs.parent_ids,\n>             sequence_length=sequence_lengths)\n>         history = math_ops.cast(history / 10000, t.dtype)\n>         return history\n>     final_state = nest.map_structure(gather_tree_from_array, final_state)\n>     outputs = FinalBeamSearchDecoderOutput(\n>         beam_search_decoder_output=outputs, predicted_ids=predicted_ids)\n>     return outputs, final_state\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r143356707>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8EkA9QD7lC3xBaNi7yf7LJHYzq0ks5sqOGNgaJpZM4PkNmd>\n> .\n>\n", "Thank you ebrevdo. I deleted the previous post since it contains several errors in our old code. The gather_tree only support tensor rank of 3. The dtype is fine. We can use gather_tree to sort out the right index of beams for each time step. And collect the alignment history using the index. I will post our code here once it is tested. ", "Thanks!\n\nOn Sun, Oct 8, 2017 at 1:31 PM, Steven Ding <notifications@github.com>\nwrote:\n\n> Thank you ebrevdo. I deleted the previous post since it contains several\n> errors in our old code. The gather_tree only support tensor rank of 3. The\n> dtype is fine. We can use gather_tree to sort out the right index of beams\n> for each time step. And collect the alignment history using the index. I\n> will post our code here once it is tested.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-335036127>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwjoADpP3v1cQKBh700RcM9psa6-ks5sqTEIgaJpZM4PkNmd>\n> .\n>\n", "```python\r\ndef _gather_tree_for_array(t, parent_ids, sequence_length):\r\n  \"\"\"\r\n  Convert tensor array that contains tensor with unsorted beams.\r\n  Each element has shape [batch*beam, depth]\r\n  Return sorted beam tensor array contains elements of shape\r\n  [batch, beam, depth]\r\n  Padding has a value of 0.\r\n  \"\"\"\r\n  # (time, batch, beam)\r\n  time = array_ops.shape(parent_ids)[0]\r\n  batch = array_ops.shape(parent_ids)[1]\r\n  beam = array_ops.shape(parent_ids)[2]\r\n  sorted_indx = array_ops.expand_dims(\r\n      array_ops.expand_dims(math_ops.range(beam), 0), 0)\r\n  sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\r\n  sorted_beams = beam_search_ops.gather_tree(\r\n      step_ids=sorted_indx, parent_ids=parent_ids,\r\n      sequence_length=sequence_length)\r\n  # all index of beam increase by 1. (-1 padding index becomes 0)\r\n  # 0 indicates a padding beam which has zero alignments.\r\n  sorted_beams = sorted_beams + 1\r\n\r\n  def collect(collector, i):\r\n      # concate a padding alignment (zeros) for each batch\r\n      value = array_ops.reshape(t.read(i), [batch, beam, -1])\r\n      padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\r\n      value = array_ops.concat([padding, value], axis=1)\r\n\r\n      # collect value according to the sorted_beams\r\n      # cannot use gather_helper as we increase the beam size by one\r\n      # so the final_shape has a different beam size\r\n      range_ = array_ops.expand_dims(\r\n          math_ops.range(batch) * (beam + 1), 1)\r\n      gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\r\n      sorted_value = array_ops.gather(\r\n          array_ops.reshape(\r\n              value, [batch * (beam + 1), -1]),\r\n          gather_indices)\r\n      sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\r\n\r\n      collector = collector.write(i, sorted_value)\r\n      return collector, i + 1\r\n\r\n  collected = tensor_array_ops.TensorArray(\r\n      size=t.size(), dynamic_size=True, dtype=dtypes.float32)\r\n  collected, _ = control_flow_ops.while_loop(\r\n      lambda _, i: i < t.size(),\r\n      collect,\r\n      loop_vars=(collected, 0),\r\n      parallel_iterations=1)\r\n  return collected\r\n```\r\n\r\ntest code: (extended from testGatherTree)\r\n\r\n```python\r\n\r\nstep_ids = _transpose_batch_time(\r\n    [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]], \r\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\r\nparent_ids = _transpose_batch_time(\r\n    [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]], \r\n    [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\r\nsequence_length = [[3, 3, 3],[3, 1, 3]]\r\n\r\n# make a dummy alignment history, which is the tiled step_ids\r\nstep_ids = ops.convert_to_tensor(step_ids)\r\ntime = array_ops.shape(step_ids)[0]\r\nbatch = array_ops.shape(step_ids)[1]\r\nbeam = array_ops.shape(step_ids)[2] \r\nalignment_length = array_ops.constant(10)\r\nalignment_history = array_ops.tile(\r\n    array_ops.expand_dims(step_ids,-1), \r\n    [1,1,1,alignment_length])\r\nalignment_history = math_ops.cast(\r\n    alignment_history, dtypes.float32)\r\nalignment_history = array_ops.reshape(\r\n    alignment_history, [time, batch, beam, alignment_length])\r\nalignment_history = tensor_array_ops.TensorArray(\r\n    size=0, dynamic_size=True, \r\n    dtype=dtypes.float32).unstack(alignment_history)\r\n\r\nsorted_history = _gather_tree_for_array(\r\n    alignment_history, parent_ids, sequence_length).stack()\r\n\r\nsorted_step_ids = beam_search_ops.gather_tree(\r\n    step_ids=step_ids, parent_ids=parent_ids,\r\n    sequence_length=sequence_length)\r\n\r\nsorted_step_ids = array_ops.tile(\r\n    array_ops.expand_dims(sorted_step_ids,-1), \r\n    [1,1,1,alignment_length])\r\nsorted_step_ids = math_ops.cast(\r\n    clip_ops.clip_by_value(\r\n        sorted_step_ids, \r\n        0, \r\n        math_ops.reduce_max(step_ids)),\r\n    dtypes.float32)\r\n\r\n# the sorted history should be the same as the tiled sorted step id\r\nprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\r\n```\r\nWe also tested with beam_search_decoder and dynamic_decode. It looks fine (in the finalized func).", "Steven, very nice!  Guillaume, are you interested in incorporating this\ninto your PR?\n\nOn Sun, Oct 8, 2017 at 8:57 PM, Steven Ding <notifications@github.com>\nwrote:\n\n> def _gather_tree_for_array(t, parent_ids, sequence_length):\n>   \"\"\"  Convert tensor array that contains tensor with unsorted beams.  Each element has shape [batch*beam, depth]  Return sorted beam tensor array contains elements of shape  [batch, beam, depth]  Padding has a value of 0.  \"\"\"\n>   # (time, batch, beam)\n>   time = array_ops.shape(parent_ids)[0]\n>   batch = array_ops.shape(parent_ids)[1]\n>   beam = array_ops.shape(parent_ids)[2]\n>   sorted_indx = array_ops.expand_dims(\n>       array_ops.expand_dims(math_ops.range(beam), 0), 0)\n>   sorted_indx = array_ops.tile(sorted_indx, [time, batch, 1])\n>   sorted_beams = beam_search_ops.gather_tree(\n>       step_ids=sorted_indx, parent_ids=parent_ids,\n>       sequence_length=sequence_length)\n>   # all index of beam increase by 1. (-1 padding index becomes 0)\n>   # 0 indicates a padding beam which has zero alignments.\n>   sorted_beams = sorted_beams + 1\n>\n>   def collect(collector, i):\n>       # concate a padding alignment (zeros) for each batch\n>       value = t.read(i)\n>       padding = array_ops.zeros([batch, 1, array_ops.shape(value)[-1]])\n>       value = array_ops.concat([padding, value], axis=1)\n>\n>       # collect value according to the sorted_beams\n>       # cannot use gather_helper as we increase the beam size by one\n>       # so the final_shape has a different beam size\n>       range_ = array_ops.expand_dims(\n>           math_ops.range(batch) * (beam + 1), 1)\n>       gather_indices = array_ops.reshape(sorted_beams[i] + range_, [-1])\n>       sorted_value = array_ops.gather(\n>           array_ops.reshape(\n>               value, [batch * (beam + 1), -1]),\n>           gather_indices)\n>       sorted_value = array_ops.reshape(sorted_value, [batch, beam, -1])\n>\n>       collector = collector.write(i, sorted_value)\n>       return collector, i + 1\n>\n>   collected = tensor_array_ops.TensorArray(\n>       size=t.size(), dynamic_size=True, dtype=dtypes.float32)\n>   collected, _ = control_flow_ops.while_loop(\n>       lambda _, i: i < t.size(),\n>       collect,\n>       loop_vars=(collected, 0),\n>       parallel_iterations=1)\n>   return collected\n>\n> test code: (extended from testGatherTree)\n>\n> step_ids = _transpose_batch_time(\n>     [[[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]],\n>     [[1, 2, 3], [4, 5, 6], [7, 8, 9], [-1, -1, -1]]])\n> parent_ids = _transpose_batch_time(\n>     [[[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]],\n>     [[0, 0, 0], [0, 1, 1], [2, 1, 2], [-1, -1, -1]]])\n> sequence_length = [[3, 3, 3],[3, 1, 3]]\n> # make a dummy alignment history, which is the tiled step_ids\n> step_ids = ops.convert_to_tensor(step_ids)\n> alignment_length = array_ops.constant(10)\n> alignment_history = tensor_array_ops.TensorArray(\n>     size=0, dynamic_size=True, dtype=dtypes.float32)\n> alignment_history = alignment_history.unstack(\n>     math_ops.cast(array_ops.tile(array_ops.expand_dims(step_ids,-1), [1,1,1,alignment_length]),\n>             dtypes.float32))\n>\n> sorted_history = _gather_tree_for_array(\n>     alignment_history, parent_ids, sequence_length).stack()\n>\n> sorted_step_ids = beam_search_ops.gather_tree(\n>     step_ids=step_ids, parent_ids=parent_ids,\n>     sequence_length=sequence_length)\n>\n> sorted_step_ids = array_ops.tile(\n>     array_ops.expand_dims(sorted_step_ids,-1),\n>     [1,1,1,alignment_length])\n> sorted_step_ids = math_ops.cast(\n>     clip_ops.clip_by_value(\n>         sorted_step_ids,\n>         0,\n>         math_ops.reduce_max(step_ids)),\n>     dtypes.float32)\n> # the sorted history should be the same as the tiled sorted step idprint(np.array_equal(sorted_history.eval(), sorted_step_ids.eval()))\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#issuecomment-335062833>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim0RRKMp8tXCR7InNebWmvZmpKLn4ks5sqZmogaJpZM4PkNmd>\n> .\n>\n", "Thank you @steven-hh-ding. I integrated your code with some minor restyling and renaming.\r\n\r\nSo now, `TensorArray`s are ignored in `_maybe_split_batch_beams`, `_maybe_merge_batch_beams`, and `_maybe_tensor_gather_helper` and beams are eventually sorted in `finalize()`. Is that more appropriate?\r\n\r\nIf yes, @ebrevdo can review for details.", "@guillaumekln  Thank you! Please feel free to modify anything. It was done in a rush. ", "The implementation should be revised after some changes on master. Will try to work on this again in the coming days.", "@guillaumekln any updates? It has accumulated another conflict.", "Yes, I looked at it again yesterday. I will update this thread again when it's ready for review. Thanks.", "I rebased the branch on master and revised the implementation following the recent changes made to `gather_tree`.\r\n\r\n@ebrevdo Could you take a look when you have some time? Thanks.", "ping @ebrevdo", "Expose a flag to disable TensorArray reordering.\n\nOn Thu, Dec 21, 2017 at 7:53 AM, Guillaume Klein <notifications@github.com>\nwrote:\n\n> *@guillaumekln* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158311174>\n> :\n>\n> > +\n> +\n> +def _maybe_sort_array_beams(t, parent_ids, sequence_length):\n> +  \"\"\"Maybe sorts beams within a `TensorArray`.\n> +\n> +  Args:\n> +    t: A `TensorArray` of size `max_time` that contains `Tensor`s of shape\n> +      `[batch_size, beam_width, depth]`.\n> +    parent_ids: The parent ids of shape `[max_time, batch_size, beam_width]`.\n> +    sequence_length: The sequence length of shape `[batch_size, beam_width]`.\n> +\n> +  Returns:\n> +    A `TensorArray` where beams are sorted in each `Tensor` or `t` itself if it\n> +    is not a `TensorArray`.\n> +  \"\"\"\n> +  if isinstance(t, tensor_array_ops.TensorArray):\n>\n> Currently, the assumptions are:\n>\n>    1. the TensorArrays size is the time dimension\n>    2. the TensorArrays contain Tensors with the same shape assumption as\n>    Tensors that are directly in the cell state\n>\n> Guard against it; enable by default, but if there's a failure raise a\n> warning message that the user should disable the TensorArray sorting.\n>\n> Do you mean to try/catch the call to gather_tree_from_array and expose a\n> flag to disable any post process on TensorArrays?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158311174>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-SQNT66YoqoLtr3M4okRjk_e3-rks5tCn77gaJpZM4PkNmd>\n> .\n>\n", "tf.where(tf.ones_like(beam_ids) > -1)[:,:-1]\n\n\nthat line looks relatively expensive.\n\n\ni guess you can do tf.where(beam_ids > -1)\n\ntruthfully i like range and tile better because it's a much faster\nkernel than tf.where (tf.where is *really* expensive, especially on\ngpu)\n\n\nOn Thu, Dec 21, 2017 at 2:29 PM, Steven Ding <notifications@github.com>\nwrote:\n\n> *@steven-hh-ding* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158392952>\n> :\n>\n> > +      math_ops.cast(mask, dtypes.bool), x=sorted_beam_ids, y=beam_ids)\n> +\n> +  # Gather from each tensor in t according to sorted_beam_ids.\n> +  def _collect(collector, i):\n> +    gathered = _tensor_gather_helper(\n> +        gather_indices=sorted_beam_ids[i],\n> +        gather_from=t.read(i),\n> +        batch_size=batch_size,\n> +        range_size=beam_width,\n> +        gather_shape=[batch_size * beam_width, -1])\n> +    return collector.write(i, gathered), i + 1\n> +\n> +  collected = tensor_array_ops.TensorArray(\n> +      t.dtype, size=t.size(), dynamic_size=False)\n> +  collected, _ = control_flow_ops.while_loop(\n> +      lambda _, i: i < t.size(),\n>\n> if i remember the shapes here correctly:\n>\n> time = 3\n> batch = 4\n> beam = 5\n> depth = 6\n> # [time, batch, beam, depth]\n> a = tf.constant(np.arange(time * batch * beam * depth), shape=[time, batch, beam, depth], dtype=tf.int64)\n> # [time, batch, beam] (reverse beam as example)\n> beam_ids = tf.constant(np.flip(np.arange(beam), 0), shape=[1,1,beam], dtype=tf.int64)\n> beam_ids = tf.tile(beam_ids, [3,4,1])\n> # we need [time, batch, beam, 3]# assume beam_ids > -1\n> t_bt_ind = tf.where(tf.ones_like(beam_ids) > -1)[:,:-1]\n> t_bt_ind = tf.reshape(t_bt_ind, [time,batch,beam,-1])\n> t_bt_be_ind = tf.concat([t_bt_ind, tf.expand_dims(beam_ids, -1)], -1)\n> # gather\n> tf.gather_nd(a, t_bt_be_ind)\n>\n> Please double-check. Another way is to combine range and tile; but it is\n> hardly readable. We use this method a lot in our code (tf.where(constant)),\n> it looks okay. ebrevdo may better comment on this. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13312#discussion_r158392952>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwHa50-96kt9b0v8H5obAAtzqlE2ks5tCtvAgaJpZM4PkNmd>\n> .\n>\n", "Oh cool. Good to know. Thank you @ebrevdo . @guillaumekln so we better use range and tile to have the indexes  like below. Please double check. Thanks!\r\n\r\n```python\r\nt_ind = tf.tile(tf.reshape(tf.range(time), [-1, 1, 1, 1]), [1, batch, beam, 1])\r\nbe_ind = tf.tile(tf.reshape(tf.range(batch), [-1, 1, 1, 1]), [1, time, beam, 1])\r\nbe_ind = tf.transpose(be_ind, perm=[1,0,2,3])  \r\nt_be_ind = tf.concat([t_ind, be_ind, tf.expand_dims(beam_ids, -1)], -1)\r\n\r\ntf.gather_nd(a, t_be_ind)\r\n```\r\n\r\n", "@steven-hh-ding Thanks for your help on this.\r\n@ebrevdo I pushed new commits trying to address your feedback. Hopefully we are getting close.\r\n\r\nIn particular, I added more tests to check the reordering on elements with shape:\r\n\r\n* `[batch_size, beam_width]`\r\n* `[batch_size, beam_width, s1]`\r\n* `[batch_size * beam_width, s1]`\r\n* `[batch_size, beam_width, s1, s2]`\r\n\r\nReordering does not happen when the elements' shape cannot be inferred. Otherwise, it is check that they can be reshaped to `[batch_size, beam_width, -1]`. If not, an exception in raised suggesting to disable `reorder_tensor_arrays`.", "looking very good!  main concern now is with cases where the batch_size is not known.", "@guillaumekln it looks very close to done.  looking forward to approving this PR.", "* Regarding the case where the batch size is not know, does it make sense to just try/catch the call to `gather_tree_from_array` instead of trying to figure out the batch and beam dimensions?\r\n* For the returned value, I applied your suggestion to return the stacked form for reordered arrays and added a notice in the docstring.\r\n  ", "Sorry for the delay. I revised the static and dynamic dimensions checking as requested, although it makes some heavy conditionals. Let me know if that is OK with you.", "looks great!  minor nits.  a unit test with input placeholders to ensure the dynamic checks work correctly would be super helpful.  otherwise LGTM.", "@ebrevdo another look?", "OK; two final comments:\r\n - 1. convert an error into a warning\r\n - 2. add a small unit test.  it LGTM otherwise.  no more review is required from me once you've made those changes.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@guillaumekln any change you could address the last minor comments?", "Yes, sorry for the delay. I had other priorities lately but I will finish this soon.\r\n\r\n@ebrevdo Good call about unit testing the dynamic checks as it actually does not work as is. As all logical operands are evaluated at graph build time (I missed that), expressions like:\r\n\r\n``` \r\nmath_ops.equal(array_ops.shape(t)[2], beam_width)\r\n```\r\n\r\nwill fail if `t` has a rank lower than 3.", "Hi, no pressure, but is there a chance this will be merged soon? Many thanks", "Sorry, this was long overdue. I fixed the dynamic checks and added some tests for them. I also converted the error raised when static checks fail into a warning.", "@ebrevdo another look, please?", "@guillaumekln It looks like there are some linter errors. Could you take a look: https://source.cloud.google.com/results/invocations/9513608d-b992-4986-a1e6-b120c7235214/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log", "Thanks, I fixed them.", "@guillaumekln Thanks!", "Thanks @guillaumekln for this PR.  It's very nice and will help users be able to get attention alignment histories with beam search."]}, {"number": 13311, "title": "gru_ops.py trying to load a .so file on windows", "body": "gru_ops.py is attempting to load an .so library on windows when using CPU version (vs GPU), is this the correct behavior? It fails in my case. \r\n\r\nLINE 33\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/gru_ops.py\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "The error was my setup, fixed, no issue.\nPlatform Windows 10/64. Install was from a .whl.\nThis item can be closed.\nThank you.\n\nOn Wed, Sep 27, 2017 at 1:12 PM, Paul Tucker <notifications@github.com>\nwrote:\n\n> Please provide details about what platform you are using (operating\n> system, architecture). Also include your TensorFlow version. Also, did you\n> compile from source or install a binary? Make sure you also include the\n> exact command if possible to produce the output included in your test case.\n> If you are unclear what to include see the issue template displayed in the\n> Github new issue template\n> <https://github.com/tensorflow/tensorflow/issues/new>.\n>\n> We ask for this in the issue submission template, because it is really\n> difficult to help without that information. Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13311#issuecomment-332589083>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABXVTXUSTlM_T17rvzVioshVg41pN0Qbks5smoHsgaJpZM4PkKEa>\n> .\n>\n"]}, {"number": 13310, "title": "Feature Request: Mixed Sparse and Dense Tensors", "body": "### Describe the problem\r\nI am trying to implement a sparse convolution operation. This means I have spatially sparse locations(in this case *NHW* of *NHWC* in 2d convolutions)\r\nand at each of these locations I have a dense vector of values (*C* of *NHWC*).\r\nIn tensorflow there are currently two classes for sparse representations:\r\n\r\n1. **SparseTensor**:\r\nConsisting of an indices matrix, value and shape vector.\r\n  This allows for a fully sparse tensor, however it does not fit this use case, because representing the channels as just another sparse dimension, I cannot simply compute a matrix multiplication, with the corresponding kernel parameters. This extremely reduces efficiency. Apart from this, also the storage\r\nis inefficient, since It redundantly stores the indices for the dense sub tensor.\r\n2. **IndexedSlices**\r\nConsisting of an indices vector and an arbitrarily shaped value tensor.\r\nThis is a mixed sparse dense data structure. So the individual sub tensors are stored sparsely. It does however not fit the use case, because the indexing is only a vector and not (as compared to the sparse\r\ntensor) a matrix. So we can only address only a single index. While it is possible to encode an index vector\r\nas a scalar index, it imposes this effort on the user, which seems to me to be non optimal.\r\n\r\nSo both data structures are inadequate for this use case. While I am talking about a single use case, I cannot imagine, that no one else stumbled across this issue, since its so general. \r\n\r\n### Proposed Solution\r\nTo solve this issue I propose to introduce a new class **MixedSparseDenseTensor**, which takes an indices matrix (as opposed to IndexedSlices vector), an arbitrarily shaped values tensor (as opposed to SparseTensors vector) and a shape vector. So it would thus be either a generalization of IndexedSlices to multiple dimensional indices or a generalization of SparseTensor to arbitrarily shaped sub tensors.\r\n\r\nThe dimensionality of the shape vector should then be the rank of the sparse tensor + the rank of the dense tensor.\r\n\r\n", "comments": ["@ebrevdo, could you comment on this issue, given your current work.", "SparseTensor could generally be relaxed to support a non-vector values.  In fact, the `scatter_nd` and `gather_nd` ops already support this structure - so creating a ST with the given block structure is straightforward.\r\n\r\nIf we allow this relaxation, then a `SparseTensor` would also subsume `IndexedSlices`; since a special case of `scatter_nd` and `gather_nd` are `scatter` and `gather`.\r\n\r\nMany of the existing ops would continue to work, emitting a specific type of `SparseTensor`.  I haven't investigated what ops need to be really updated to support them in the new generality.  For example, `tf.sparse_tensor_to_dense` would need to be updated to just use `tf.scatter_nd` (very easy change); and new unit tests would have to be added.  There are probably a couple more ops we would want to update to support block sparse tensors.", "I think probably the main blocker to replacing `IndexedSlices` with `SparseTensor` is making `gather_nd` as fast as `gather` (it's probably already pretty close), and making `scatter_nd` as fast as `scatter` (here there's a lot of room for improvement - it's slower than `scatter` when handling large blocks).", "(this would also be equivalent to your `MixedSparseDenseTensor` representation)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm afraid I won't have cycles to work on this in the forseeable future.  Marking as contributions welcome.", "(we do have someone at google who proposed the exact same thing, but work has stalled).  fwiw; `scatter_nd`, `gather_nd` and their ilk already support this behavior so what you're talking about is just a less restrictive `SparseTensor` wrapper.", "Assigning to @elibixby who has a design doc for this.", "@ebrevdo I'll schedule a design review for the design doc since there don't seem to be any large outstanding comments. @wechselstrom Note the planned design is actually an in-place change to `SparseTensor`, \r\n\r\nAfter the change you should be able to use `SparseTensor`s with ops that support `IndexedSlice`s (with a little bit of rewrapping), and create `SparseTensor`s from `IndexedSlice`s (again with re-wrapping), paving the way for the eventual merging of the two.\r\n\r\nOnce the restrictions on the python `SparseTensor` wrapper have been loosened we'll gradually add support for \"block-sparse\" tensors to the existing sparse tensor ops.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "@wechselstrom The design just went through design review committee and was approved, but I'm a little short on time to implement it. Would you be interested in contributing if I published the design somewhere public? I'm happy to be a resource for this as well.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing until the OP responds"]}, {"number": 13309, "title": "Rdma params configuration", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 13308, "title": "Attempting to use the CPU Work Sharder segfaults on g++ 5.4.0", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n  \r\nI've adapted the ZeroOut operator from the [Adding a New Op](https://www.tensorflow.org/extend/adding_an_op) example.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nbinary GPU 1.3.0\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n\r\n- **Python version**: \r\n\r\n2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nN/A\r\n\r\n- **GPU model and memory**:\r\n\r\nN/A\r\n\r\n- **Exact command to reproduce**:\r\n\r\nTest operator: [shard_fails.zip](https://github.com/tensorflow/tensorflow/files/1332745/shard_fails.zip)\r\n\r\n```bash\r\n$ make\r\n$ python test_op.py\r\n```\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen the above C++ operator runs, it'll print the number of threads in the pool (8) and then segfault on the Shard call.\r\n\r\n### Source code / logs\r\n\r\nC++ operator code:\r\n\r\n```cpp\r\n#define EIGEN_USE_THREADS\r\n\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/util/work_sharder.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ZeroOut\")\r\n    .Input(\"to_zero: int32\")\r\n    .Output(\"zeroed: int32\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0, c->input(0));\r\n      return Status::OK();\r\n    });\r\n\r\nclass ZeroOutOp : public OpKernel {\r\n public:\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* context) override {\r\n    // Grab the input tensor\r\n    const Tensor& input_tensor = context->input(0);\r\n    auto input = input_tensor.flat<int32>();\r\n\r\n    // Create an output tensor\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\r\n                                                     &output_tensor));\r\n    auto output_flat = output_tensor->flat<int32>();\r\n\r\n    // Set all but the first element of the output tensor to 0.\r\n    const int N = input.size();\r\n\r\n    auto pool = context->device()->tensorflow_cpu_worker_threads()->workers;\r\n    printf(\"Pool Threads %d\\n\", pool->NumThreads());\r\n    Shard(pool->NumThreads(), pool, N, 10, [&](int64 start, int64 end) {\r\n        for(int64 i=start; i<end; ++i)\r\n            { output_flat(i) = 0; }\r\n    });\r\n\r\n    if(N > 0)\r\n        { output_flat(0) = input(0); }\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\r\n```\r\n\r\n\r\nSee below the gdb trace:\r\n\r\n```\r\nCore was generated by `python test_op.py'.\r\nProgram terminated with signal SIGSEGV, Segmentation fault.\r\n#0  std::_Function_handler<void (long long, long long), ZeroOutOp::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long&&, std::_Any_data const&) (__functor=..., __args#0=<unknown type in tfop.so, CU 0x0, DIE 0x41c73>, __args#1=<unknown type in tfop.so, CU 0x0, DIE 0x41c78>) at /usr/include/c++/5/functional:1871\r\n1871\t\t(*_Base::_M_get_pointer(__functor))(\r\n[Current thread is 1 (Thread 0x7f38a6605700 (LWP 3771))]\r\n(gdb) bt\r\n#0  std::_Function_handler<void (long long, long long), ZeroOutOp::Compute(tensorflow::OpKernelContext*)::{lambda(long long, long long)#1}>::_M_invoke(std::_Any_data const&, long long&&, std::_Any_data const&) (__functor=..., __args#0=<unknown type in tfop.so, CU 0x0, DIE 0x41c73>, __args#1=<unknown type in tfop.so, CU 0x0, DIE 0x41c78>) at /usr/include/c++/5/functional:1871\r\n#1  0x00007f3879dcc75d in tensorflow::thread::ThreadPool::Impl::ParallelFor(long long, long long, std::function<void (long long, long long)>) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f3879dcc93f in tensorflow::thread::ThreadPool::ParallelFor(long long, long long, std::function<void (long long, long long)>) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f3879d4d995 in tensorflow::Shard(int, tensorflow::thread::ThreadPool*, long long, long long, std::function<void (long long, long long)>) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f3850bfb79e in ZeroOutOp::Compute (this=0x62dacc0, context=0x7ffd1a20fe30) at tf_op.cpp:42\r\n#5  0x00007f3879a2563c in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f38799f5a58 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f38799f61fa in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f3879a035c4 in std::_Function_handler<void (std::function<void ()>), tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*)::{lambda(std::function<void ()>)#1}>::_M_invoke(std::_Any_data const&, std::function<void ()>) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f38799e895b in std::function<void (std::function<void ()>)>::operator()(std::function<void ()>) const ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f38799e9043 in tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(tensorflow::gtl::InlinedVector<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, 8> const&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*) [clone .part.246] () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f38799ecf5e in tensorflow::(anonymous namespace)::ExecutorImpl::RunAsync(tensorflow::Executor::Args const&, std::function<void (tensorflow::Status const&)>) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f3879a045e4 in tensorflow::GraphRunner::Run(tensorflow::Graph*, tensorflow::FunctionLibraryRuntime*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f38799dce27 in tensorflow::ConstantFold(tensorflow::ConstantFoldingOptions const&, tensorflow::FunctionLibraryRuntime*, tensorflow::Env*, tensorflow::Device*, tensorflow::Graph*, bool*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f3879a02fea in tensorflow::GraphOptimizer::Optimize(tensorflow::FunctionLibraryRuntime*, tensorflow::Env*, tensorflow::Device*, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007f38799a9469 in tensorflow::DirectSession::GetOrCreateExecutors(tensorflow::thread::ThreadPool*, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::gtl::ArraySlice<std::string>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007f38799aa06c in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00007f387799b2d7 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#18 0x00007f387799b604 in TF_Run () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#19 0x00007f38778037e2 in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\r\n   from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#20 0x00007f3877803be1 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#21 0x00007f38777ca793 in _wrap_TF_Run () from /home/sperkins/venv/mb/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#22 0x00000000004c468a in PyEval_EvalFrameEx ()\r\n#23 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n#24 0x00000000004de6fe in ?? ()\r\n#25 0x00000000004b0cb3 in PyObject_Call ()\r\n#26 0x00000000004c6ad1 in PyEval_EvalFrameEx ()\r\n#27 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n#28 0x00000000004ca8d1 in PyEval_EvalFrameEx ()\r\n#29 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#30 0x00000000004ca8d1 in PyEval_EvalFrameEx ()\r\n#31 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n#32 0x00000000004ca8d1 in PyEval_EvalFrameEx ()\r\n#33 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n#34 0x00000000004ca099 in PyEval_EvalFrameEx ()\r\n#35 0x00000000004c2765 in PyEval_EvalCodeEx ()\r\n#36 0x00000000004c2509 in PyEval_EvalCode ()\r\n#37 0x00000000004f1def in ?? ()\r\n#38 0x00000000004ec652 in PyRun_FileExFlags ()\r\n#39 0x00000000004eae31 in PyRun_SimpleFileExFlags ()\r\n#40 0x000000000049e14a in Py_Main ()\r\n#41 0x00007f38a5e4c830 in __libc_start_main (main=0x49dab0 <main>, argc=2, argv=0x7ffd1a213618, init=<optimised out>, fini=<optimised out>, rtld_fini=<optimised out>, stack_end=0x7ffd1a213608)\r\n    at ../csu/libc-start.c:291\r\n#42 0x000000000049d9d9 in _start ()\r\n```", "comments": ["Is there any feedback on this issue? The above code segfaults on both the CPU and GPU pip binaries. I think this is fairly serious since the Shard function is the [recommended](https://www.tensorflow.org/extend/adding_an_op#multi-threaded_cpu_kernels) method for achieving parallelism in CPU operators.", "`Shard` also segfaults if I pass through a lambda with no side-effects. e.g. :\r\n\r\n```cpp\r\n    Shard(pool->NumThreads(), pool, N, 10, [](int64 start, int64 end) {\r\n        for(int64 i=start; i<end; ++i)\r\n            { }\r\n    });\r\n```", "btw, you can get exact line number/values of args by using debug build\r\nhttps://github.com/mind/wheels/releases/tag/tf1.3.1-cpu-debug", "@yaroslavvb Thanks for the link.\r\n\r\nBoth standard and debug versions of tinymind's 1.3.1 wheels run the Sharding function correctly.\r\n\r\ntensorflowers, will this be fixed in 1.4.0?", "Hmmm, this still segfaults for me on `tf-nightly-gpu`.", "Here is the minimal test case [shard_fails.zip](https://github.com/tensorflow/tensorflow/files/1374909/shard_fails.zip) updated for building the op on 1.4.0.\r\n", "This seems like a regression in 1.4", "I digged a little further and this seems to be caused by compiling the custom op with g++ 5.4.0 on 16.04.  Note that the `-D_GLIBCXX_USE_CXX11_ABI=0` flag is correctly present to attempt to use the older ABI.\r\n\r\nIf I install g++ 4.8 and compile with that the segfault disappears and the op runs correctly.", "Note that the op runs successfully when compiled with g++ 5.4.0 on 16.04 and the `Shard` call is removed.", "Any thoughts on this issue?", "Looks like https://github.com/tensorflow/tensorflow/pull/13496 fixes this. I'll close once I've tested on a nightly containing https://github.com/tensorflow/tensorflow/pull/14159.", "Looks like this got fixed at some point. https://github.com/tensorflow/tensorflow/pull/13496 is a nice addition for Makefiles.", "Made a mistake when testing this. Shard still segfaults when the op is compiled with g++ 5.4.0 (as opposed to the tensorflow binary compiled with g++ 4.8.4)", "This is quite problematic - the default compiler on 16.04LTS is g++ 5.4.0. As it stands, not having access to multithreading (and control thereof) with a default install is causing me headaches. Would be awesome to see a fix.", "This still occurs on tensorflow 1.8.0 CPU and Ubuntu 16.04, g++ 5.4.0.\r\n\r\nIs there an alternative way to submit work to the intra-op threadpool for custom ops? It also might be worth modifying the [Adding an Op](https://www.tensorflow.org/extend/adding_an_op) docs to indicate that Shard might not work or provide an alternatve.", "Assigning to @asimshankar for further triage.", "This still occurs on tensorflow 1.9.0 and Ubuntu 16.04, g++ 5.4.0.", "Did you try to run with ASAN / MSAN / TSAN? Any finding?", "This problem still occurs when I use TF 1.10.0 on Ubuntu 18.04 g++ 6.4.0.\r\nPlease fix it. Thanks!!!", "Ubuntu 16.04, gcc 5.4, TF 1.6 has problems.\r\nDowngrading to gcc 4.9, TF 1.6 no problem. I don't know why. ", "gcc 4.9 is supposed to work according to [installation doc](https://www.tensorflow.org/install/source). You should be able to use the more recent TensorFlow versions.\r\n", "Ran into this issue on debian stretch. g++ version 6.3.0. It also happened when using the new `TransformRangeConcurrently` function.", "I did a bit of investigation, and this particular failure seems to be due to a change in `std::function` between GCC [4.X](https://github.com/gcc-mirror/gcc/blob/gcc-5_1_0-release/libstdc++-v3/include/std/functional#L1871) and [5.X](https://github.com/gcc-mirror/gcc/blob/gcc-4_8_4-release/libstdc++-v3/include/std/functional#L2069). Interestingly, this change does not affect the ABI, and therefore there is no linking error.\r\n\r\nThe failure could be fixed by swapping `std::function` types for function pointers, but for now the bottom line is that a custom op **must be** compiled with the same GCC/libstdc++ version as TF itself. ", "Thanks @superbobry !", "Shall we close the issue @drpngx?", "I've changed it to `contributions welcome`. We can change to a function pointer, or provide an interface that has a function pointer.", "@superbobry, thanks - you just saved us from spending a lot of time in uber/horovod#542 digging through this.", "@drpngx segment fatal when compile custom ops under 1.14 docker images.", "It may duplicate to [https://github.com/tensorflow/tensorflow/issues/29820](url)", "@mttbx Any progress?", "I think I might be running into this exact same issue with tf1.13.1 and MacOS see: https://stackoverflow.com/questions/57427277/tensorflow-customop-multiprocessing-not-working-for-cpu#comment101753938_57427277", "Hi @sjperkins !\r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Could you please test the issue in latest version in[ TF 2.7](https://www.tensorflow.org/install/pip) as many bug  and performance issue has been fixed in  latest versions ? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 13307, "title": "[XLA] Register the input arguments with the XlaAllocator", "body": "If an op returns one if its inputs as an output, then the XlaAllocator needs to be able to notice the buffer and find the original tensor.\r\n\r\nThis adds a method to the XlaAllocator to allow this action, and calls it with all of the inputs and variables (but not the runtime context).\r\n\r\n", "comments": ["@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @girving to be potential reviewers.", "Can one of the admins verify this patch?", "This is related to the comments in https://groups.google.com/forum/#!topic/xla-dev/pFiI1sO8ZhE\r\n", "Jenkins, test this please.", "@sb2nov @hawkinsp \r\n\r\nCheers guys.  judging by the error 'cannot remove some set of files or another', I think that the Linux CPU tests Makefile is not related to this change.", "@DavidNorman I don't think the Makefile trigger ran anything.\r\n\r\nJenkins, test this please.", "@gunan any ideas about the Makefile test?", "@sb2nov \r\n\r\nhmm.  the failures are now something to do with git, and something to do with some tests that are nothing to do with XLA.  again, i suspect nothing to do with my changes.\r\n\r\n", "Jenkins, test this please."]}, {"number": 13306, "title": "How to set include path and lib path when building custom code on macOS", "body": "Firstly, I set CC=/usr/local/bin/gcc-6 and CXX=/usr/local/bin/g++-6. Then I built tensorflow from source using `sh tensorflow/contrib/makefile/build_all_ios.sh` on macOS 10.12.5 and it done successfully. Lastly, I built a test cpp using CMake but it failed. \r\n\r\nThe reasons I guess maybe:\r\n1. Tensorflow built using default clang but not g++-6. So how to set compiler when using `tensorflow/contrib/makefile/build_all_ios.sh`?\r\n2. The include and lib path in CMakeLists.txt may be wrong.\r\n\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/platform/env.h\"\r\nusing namespace tensorflow;\r\nint main(int argc, char* argv[]) {\r\n  // Initialize a tensorflow session\r\n  Session* session;\r\n  Status status = NewSession(SessionOptions(), &session);\r\n  if (!status.ok()) {\r\n    std::cout << status.ToString() << \"\\n\";\r\n    return 1;\r\n  }\r\n  session->Close();\r\n  return 0;\r\n}\r\n```\r\nIn my CMakeLists.txt, I set include path \r\n```\r\n${PROJECT_SOURCE_DIR}/../tensorflow\r\n${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/proto\r\n${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include\r\n${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/downloads/eigen\r\n${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/downloads/nsync/public\r\n```\r\nand the library path\r\n```\r\nlink_directories(\r\n\t${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64\r\n\t${PROJECT_SOURCE_DIR}/../tensorflow/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64/lib)\r\nset(DEMO_LINKER_LIBS \"\")\r\nlist(APPEND DEMO_LINKER_LIBS libtensorflow-core-x86_64.a libprotobuf-lite.a libprotobuf.a)\r\n```\r\n\r\nCompile ok but errors occur when linking. What's wrong in my use? Thanks.\r\n```\r\nld: warning: URGENT: building for OSX, but linking in object file (/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64/libtensorflow-core-x86_64.a(session.o)) built for iOS. Note: This will be an error in the future.\r\nld: warning: URGENT: building for OSX, but linking in object file (/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/ios_X86_64/libtensorflow-core-x86_64.a(config.pb.o)) built for iOS. Note: This will be an error in the future.(/Users/formath/github/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(config.pb.o)) built for iOS. Note: This will be an error in the future.\r\nUndefined symbols for architecture x86_64:\r\n  \"tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()\", referenced from:\r\n      std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in test.cc.o\r\n  \"nsync::nsync_mu_init(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::Env::Env()   in libtensorflow-core-x86_64.a(env.o)\r\n  \"nsync::nsync_mu_lock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core-x86_64.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core-x86_64.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core-x86_64.a(env.o)\r\n  \"nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core-x86_64.a(session_factory.o)\r\n      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core-x86_64.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core-x86_64.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core-x86_64.a(env.o)\r\n  \"tensorflow::Status::ToString[abi:cxx11]() const\", referenced from:\r\n      _main in test.cc.o\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::at(unsigned long) const\", referenced from:\r\n      google::protobuf::io::Tokenizer::IsIdentifier(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libprotobuf.a(tokenizer.o)\r\n  \"std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::find(char const*, unsigned long, unsigned long) const\", referenced from:\r\n      google::protobuf::GlobalReplaceSubstring(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*) in libprotobuf-lite.a(strutil.o)\r\n      tensorflow::str_util::StringReplace(tensorflow::StringPiece, tensorflow::StringPiece, tensorflow::StringPiece, bool) in libtensorflow-core-x86_64.a(str_util.o)\r\n```", "comments": []}, {"number": 13305, "title": "not able to install tensor flow from pip using 3.5 64 bit version of python.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Thanks a ton!! it is resolved.\n\nRegards,\nSuhal vemu\n\nBest Regards,\n*Suhal Vemu*\nMob #9986768698.\n\n_____________________________________________________________\n*reachout to suhalvemu@icloud.com <suhalvemu@icloud.com>*\n\nOn Thu, Jan 4, 2018 at 12:42 AM, Alfred <notifications@github.com> wrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue? Please update the label and/or status\n> accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13305#issuecomment-355097315>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQmDRjnph8Q0cAvhavH8SyL2pkU8n7bwks5tG9B9gaJpZM4PjoY_>\n> .\n>\n"]}, {"number": 13304, "title": "Branch 169998131", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @zheng-xq to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 13303, "title": "[Missing Feature] Input Pipeline for Models with Multi-step Optimization", "body": "### Describe the problem\r\n\r\nAfter some searching and reading (e.g. #7951), I found the current input pipeline framework\r\nis generally lack of support for models with multi-step optimization.\r\n\r\nIn most of the models before GAN (General Adversarial Networks), a model almost has one\r\noptimization function, that is, we optimize over a mini-batch of input data once in a step. While\r\nstarting from GAN, many models have two or more optimization functions, in other words, they\r\nsequentially optimize on several functions using the same batch of data (Adversarial Autoencoders). \r\n\r\nThe current input pipeline works fine with a unique optimization operation per step, where the `tf.Session` will pull a batch of data from the input queue once. For multi-step optimization models, if you link all steps with a unique input queue, then the queue will be pulled several times\r\nif you link all sequential optimization steps with that queue. Apparently, this is completely wrong.\r\n\r\nFrom my point of view, I think we should add a peek() op in `tf.QueueBase` for supporting\r\nmultistep sequential optimization. So for example, for GAN, we can link optimization over discriminator with a peek_many op and link optimization over generator with a dequeue_many op.\r\n\r\nIn general, for multiple steps, we can do `peek_many` -> `peek_many` -> ... -> `dequeue_many`.\r\n\r\nFor the new tf.data.Dataset API, when we called `session.run()`, we advance the iterator. So in the new data importing API, we still lack of this feature.\r\n\r\nI think currently a workaround of this would be building a buffer with tf.Variable. and make all subsequent optimization step depends on the snapshot of the buffer. Like,\r\n\r\n```\r\nbuffer = tf.get_variable(\"buffer\",\r\n                                        shape=(**, **),\r\n                                        trainable=False,\r\n                                        initializer=tf.zeros_initializer)\r\ninput_op = input_function()\r\nassign_op = tf.assign(buffer, input_op)\r\n\r\ncache = buffer.value()\r\n# based everything on cache for the subsequent step.\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(assign_op)\r\n    # then run all other steps.\r\n```\r\n\r\nIn this case, we have to copy the entire batch of data for each step.", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Catching up on old issues: I don't think this kind of functionality is a good fit for the core `tf.data` APIs. It would be more natural to write this code in Eager mode, using a `tfe.Iterator` on a `tf.data.Dataset` to provide access to the input data as one or more `tfe.Tensor` objects, and then using Python control flow to pass the same `tfe.Tensor` objects to different sub-steps."]}, {"number": 13302, "title": "setuptools pip wheel failed with error code 2", "body": "```\r\ndengw-iMac:~ chars$ virtualenv --system-site-packages ~/tensorflow\r\nNew python executable in /Users/chars/tensorflow/bin/python\r\nInstalling setuptools, pip, wheel...\r\n  Complete output from command /Users/chars/tensorflow/bin/python - setuptools pip wheel:\r\n  Collecting setuptools\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/commands/install.py\", line 324, in run\r\n    requirement_set.prepare_files(finder)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_set.py\", line 380, in prepare_files\r\n    ignore_dependencies=self.ignore_dependencies))\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_set.py\", line 554, in _prepare_file\r\n    require_hashes\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/req/req_install.py\", line 278, in populate_link\r\n    self.link = finder.find_requirement(self, upgrade)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py\", line 465, in find_requirement\r\n    all_candidates = self.find_all_candidates(req.name)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py\", line 423, in find_all_candidates\r\n    for page in self._get_pages(url_locations, project_name):\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py\", line 568, in _get_pages\r\n    page = self._get_page(location)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py\", line 683, in _get_page\r\n    return HTMLPage.get_page(link, session=self.session)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/index.py\", line 792, in get_page\r\n    \"Cache-Control\": \"max-age=600\",\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py\", line 488, in get\r\n    return self.request('GET', url, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/download.py\", line 386, in request\r\n    return super(PipSession, self).request(method, url, *args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py\", line 475, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/sessions.py\", line 596, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/cachecontrol/adapter.py\", line 47, in send\r\n    resp = super(CacheControlAdapter, self).send(request, **kw)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py\", line 390, in send\r\n    conn = self.get_connection(request.url, proxies)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py\", line 290, in get_connection\r\n    proxy_manager = self.proxy_manager_for(proxy)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py\", line 184, in proxy_manager_for\r\n    **proxy_kwargs\r\n  File \"/Library/Python/2.7/site-packages/virtualenv_support/pip-9.0.1-py2.py3-none-any.whl/pip/_vendor/requests/adapters.py\", line 43, in SOCKSProxyManager\r\n    raise InvalidSchema(\"Missing dependencies for SOCKS support.\")\r\nInvalidSchema: Missing dependencies for SOCKS support.\r\n----------------------------------------\r\n...Installing setuptools, pip, wheel...done.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/virtualenv\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Library/Python/2.7/site-packages/virtualenv.py\", line 713, in main\r\n    symlink=options.symlink)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv.py\", line 945, in create_environment\r\n    download=download,\r\n  File \"/Library/Python/2.7/site-packages/virtualenv.py\", line 901, in install_wheel\r\n    call_subprocess(cmd, show_stdout=False, extra_env=env, stdin=SCRIPT)\r\n  File \"/Library/Python/2.7/site-packages/virtualenv.py\", line 797, in call_subprocess\r\n    % (cmd_desc, proc.returncode))\r\nOSError: Command /Users/chars/tensorflow/bin/python - setuptools pip wheel failed with error code 2\r\n```\r\n\r\nHow can I do it? Thanks\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "This looks like an issue strictly in your installation of python, virtualenv and pip.\r\nAs this is not a TensorFlow problem, there is nothing we can do on TF side.\r\nI am closing this issue."]}, {"number": 13301, "title": "Add int64 support for `tf.reduce_max` on GPU", "body": "This fix tries to address the issue raised in #13293 where `tf.reduce_max` on GPU does not have int64 support.\r\n\r\nTest cases have been added to cover the changes. \r\n\r\nThis fix fixes #13293.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @jart and @tensorflower-gardener to be potential reviewers.", "Jenkins, test this please."]}, {"number": 13300, "title": "GitHub only 1.3.1 release.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @aselle, @martinwicke and @tensorflower-gardener to be potential reviewers."]}, {"number": 13299, "title": "Add optional CheckpointSaverListener to MonitoredSession", "body": "", "comments": ["Can one of the admins verify this patch?", "@magnusja, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ispirmustafa, @tensorflower-gardener and @jhseu to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Could you elaborate on the motivation here? \r\n\r\nIIUC, this provides a hook to the checkpoint hook :). Is there an alternative possible here where you provide your own MyCheckpointHook that composes CheckpointSaverHook with your custom code?", "@asimshankar Yes that's right ;)\r\nI have not thought about that, but that would be an option as well. Would you prefer an CheckpointSaverHook as an optional argument which is used when not None instead of creating one?", "@martinwicke @ispirmustafa : Could you guys take a look? It seems this is very similar to a change that was proposed to `tf.Estimator` that was also adding such a hook.\r\n\r\nDo we want to do this in both places, or just in `Estimator`?", "We're adding listener argument to `tf.estimator.Estimator.train` since there are edge cases related to creation of `CheckpointSaverHook`.  User can create his/her own `CheckpointSaverHook` before calling `MonitoredTrainingSession`. It's also not a common thing. So I recommend not to change `MonitoredTrainingSession`.", "Closing in response to Mustafa's comment."]}, {"number": 13298, "title": "wrong tf.svd documentation in tensorflow 1.3.0 version", "body": "In tf.svd documentation, it was said that the tensorflow implementation of svd is simply np.linalg.svd\r\n\r\nin fact it was not the same, in tensorflow, the index for V matrix is transposed..different from np.linalg.svd default setting. Funny thing is, this type of problem is a famous bug in matlab code people usually write, which is row selection and transpose is not equal to transpose then column selection.\r\n\r\ncode demonstration:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.__version__\r\n\r\n## benchmark using numpy svd\r\nimport numpy as np\r\n\r\nx = np.array([1.0,2.0,3.0,4.0,5.0,6,7,8,9])\r\nx = x.reshape(3,3)\r\n\r\nu_,s_,v_=np.linalg.svd(x,compute_uv=True)\r\n# print s_\r\nprint u_\r\nprint v_\r\n\r\nr = 2\r\ndiagS = np.diag(s_)\r\nprint diagS\r\ndiagS = diagS[:r,:r]\r\nx_rank1 = np.matmul(u_[:,:r], np.matmul(diagS, v_[:r,:]))\r\n\r\nprint 'original = \\n',x\r\nprint 'rank1 approximation\\n',x_rank1\r\n```\r\nResult is\r\n```\r\n[[-0.21483724  0.88723069  0.40824829]\r\n [-0.52058739  0.24964395 -0.81649658]\r\n [-0.82633754 -0.38794278  0.40824829]]\r\n[[-0.47967118 -0.57236779 -0.66506441]\r\n [-0.77669099 -0.07568647  0.62531805]\r\n [-0.40824829  0.81649658 -0.40824829]]\r\n[[  1.68481034e+01   0.00000000e+00   0.00000000e+00]\r\n [  0.00000000e+00   1.06836951e+00   0.00000000e+00]\r\n [  0.00000000e+00   0.00000000e+00   4.41842475e-16]]\r\noriginal = \r\n[[ 1.  2.  3.]\r\n [ 4.  5.  6.]\r\n [ 7.  8.  9.]]\r\nrank1 approximation\r\n[[ 1.  2.  3.]\r\n [ 4.  5.  6.]\r\n [ 7.  8.  9.]]\r\n```\r\n\r\nBut in tensorflow svd\r\n```\r\nX = tf.constant([1.0,2.0,3.0,4.0,5.0,6,7,8,9],shape=[3,3])\r\n\r\nsess = tf.InteractiveSession()\r\n    \r\nprint sess.run(X)\r\n\r\ns,u,v = tf.svd(X)\r\n\r\nprint sess.run(u)\r\nprint sess.run(v)\r\n\r\ndiagS_tf = tf.diag(s)\r\n# print sess.run(diagS_tf)\r\n\r\ntmp_correct =  tf.matmul(diagS_tf[:r,:r], tf.transpose(v[:,:r]))\r\ntmp_wrong = tf.matmul(diagS_tf[:r,:r], v[:r,:])\r\n\r\nprint 'following differs'\r\nprint sess.run(tf.transpose(v[:,:r]))\r\nprint sess.run(v[:r,:])\r\n\r\n# print sess.run(tmp)\r\n\r\nx_rank1_tf_correct= tf.matmul(u[:,:r], tmp_correct)\r\nx_rank1_tf_wrong= tf.matmul(u[:,:r], tmp_wrong)\r\n\r\nprint 'printing rank1 approximation from tensorflow'\r\nprint sess.run(x_rank1_tf_correct)\r\nprint sess.run(x_rank1_tf_wrong)\r\n```\r\n\r\nthe output is \r\n```\r\n[[ 1.  2.  3.]\r\n [ 4.  5.  6.]\r\n [ 7.  8.  9.]]\r\n[[ 0.21483716  0.88723052 -0.40824857]\r\n [ 0.52058721  0.24964423  0.81649649]\r\n [ 0.82633758 -0.38794291 -0.4082481 ]]\r\n[[ 0.47967106 -0.77669096  0.40824836]\r\n [ 0.57236761 -0.07568647 -0.81649655]\r\n [ 0.66506428  0.62531805  0.40824822]]\r\nfollowing differs\r\n[[ 0.47967106  0.57236761  0.66506428]\r\n [-0.77669096 -0.07568647  0.62531805]]\r\n[[ 0.47967106 -0.77669096  0.40824836]\r\n [ 0.57236761 -0.07568647 -0.81649655]]\r\nprinting rank1 approximation from tensorflow\r\n[[ 0.99999887  1.99999869  2.99999857]\r\n [ 3.9999969   4.99999666  5.99999666]\r\n [ 6.99999809  7.99999809  8.99999809]]\r\n[[  2.27875805  -2.88305187   0.7037462 ]\r\n [  4.35980749  -6.83247042   3.36293888]\r\n [  6.44085884 -10.78189278   6.0221343 ]]\r\n\r\n```\r\n\r\n\r\n", "comments": ["Note that the latest version (master) mentions the order difference\r\n\r\n```\r\n@compatibility(numpy)\r\nMostly equivalent to numpy.linalg.svd, except that the order of output\r\narguments here is `s`, `u`, `v` when `compute_uv` is `True`, as opposed to\r\n`u`, `s`, `v` for numpy.linalg.svd.\r\n\r\n```", "I knew this. But this is not the error I mean. \n\nIn numpy, svd is u * s *v, but tensorflow is u *s *v'. \n\n> \u5728 2017\u5e749\u670825\u65e5\uff0c16:15\uff0cYaroslav Bulatov <notifications@github.com> \u5199\u9053\uff1a\n> \n> Note that the latest version mentions the order difference\n> \n> @compatibility(numpy)\n> Mostly equivalent to numpy.linalg.svd, except that the order of output\n> arguments here is `s`, `u`, `v` when `compute_uv` is `True`, as opposed to\n> `u`, `s`, `v` for numpy.linalg.svd.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Ah I see...wow, good catch....looks like I've been using TensorFlow SVD wrong this whole time...", "Please have a look at [this](https://github.com/tensorflow/tensorflow/pull/13850) PR for fix.", "Guys, TensorFlow is using the standard mathematical definition of the SVD: A = U \\Sigma V^H. The right singular vectors of A are the columns of V. The left singular vectors of A are the columns of U. It is true that some libraries return V^H instead of V (for performance reasons). ", "See:\r\n\r\nhttps://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/kernel_tests/svd_op_test.py?rcl=171836140&l=120", "The issue is that the docs say that tensorflow result is equivalent to `v` returned by `numpy.linalg.svd` but in fact it is equivalent to `v'`"]}, {"number": 13297, "title": "Feature request for graph visualizer: turn off node collapsing", "body": "There's a simple feed-forward pbtxt [here](https://github.com/yaroslavvb/stuff/blob/master/resnet_8_simple.pbtxt) \r\n\r\nIt can be visualized without any edges crossing. However latest TensorBoard collapses Relu-Relu_6 nodes together, so that complicates the diagram and introduces edge crossings. It would be nice to have a way to visualize without node collapsing\r\n\r\n![screenshot 2017-09-25 11 35 53](https://user-images.githubusercontent.com/23068/30824844-bf81f922-a1e5-11e7-9cf4-5995ba19e5a4.png)\r\n\r\ncc @dsmilkov ", "comments": ["Here's what it looks like when hacked to turn off node collapsing. Note that softmax layer is now at the top as expected, instead of middle of the graph as before\r\n\r\n![screenshot 2017-09-25 12 16 59](https://user-images.githubusercontent.com/23068/30826436-775c2f86-a1eb-11e7-8532-1d895014f17d.png)\r\n", "that's a tensorboard issue isn't it?\r\nhttps://github.com/tensorflow/tensorboard/issues", "Good point, didn't realize that was separate"]}, {"number": 13296, "title": "TF Docs fix for 1.3", "body": "*fix broken links, add links check to sanity (#11394)\r\n\r\n* fix broken links, add links check to sanity\r\n\r\n* fix broken link in export.md", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 13294, "title": "Input Pipeline for High-performance Model", "body": "### Describe the problem\r\nWhen using data_flow_op.RecordInput in input pipeline on distributed TensorFlow, every input thread seems load all files in the data_dir to the local buffer after shuffling and left-shifting all matched file names in data_dir (https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/record_yielder.cc#L139). \r\nBased on the code, it seems like data input for each epoch ends with loading all files instead of part of the files on every worker.\r\n\r\n If I understand correctly, each input thread from each worker task should read a portion of the files, which should be the shift_ratio * file_num. It will be very helpful, if anyone can explain this. \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13293, "title": "BUG: tf.reduce_max does not support int64 tensor on GPU.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip)\r\n- **TensorFlow version (use command below)**:\r\nv1.3.0-rc2-20-g0787eee 1.3.0\r\nAlso tested with:\r\nv1.3.0-rc1-2523-g1e1b3d9 1.4.0-dev20170925\r\n- **Python version**: \r\nPython 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n- **CUDA/cuDNN version**:\r\nCUDA-8.0 / cuDNN-5.1\r\n- **GPU model and memory**:\r\nNVidia GeForce GTX TITAN with 5.93GiB\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    values = tf.reshape(tf.range(10, dtype=tf.int64), [-1, 1])\r\n    max_val = tf.reduce_max(values)\r\n\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(sess.run(max_val))\r\n```\r\n\r\n### Describe the problem\r\n**tf.reduce_max** does not support **int64** tensor on GPU.\r\n\r\n### Source code / logs\r\n\r\n```\r\n2017-09-25 17:14:14.906614: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2\r\n2017-09-25 17:14:15.106125: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-09-25 17:14:15.106623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\r\nname: GeForce GTX TITAN major: 3 minor: 5 memoryClockRate(GHz): 0.8755\r\npciBusID: 0000:04:00.0\r\ntotalMemory: 5.93GiB freeMemory: 5.59GiB\r\n2017-09-25 17:14:15.106706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:04:00.0, compute capability: 3.5)\r\nTraceback (most recent call last):\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1293, in _run_fn\r\n    self._extend_graph()\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1354, in _extend_graph\r\n    self._session, graph_def.SerializeToString(), status)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 467, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=\"/device:GPU:0\"](Reshape, Const)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"bug2.py\", line 9, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=\"/device:GPU:0\"](Reshape, Const)]]\r\n\r\nCaused by op 'Max', defined at:\r\n  File \"bug2.py\", line 6, in <module>\r\n    max_val = tf.reduce_max(values)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 1525, in reduce_max\r\n    name=name)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2436, in _max\r\n    keep_dims=keep_dims, name=name)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3090, in create_op\r\n    op_def=op_def)\r\n  File \"/home/daniyar/anaconda2/envs/nightly3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1638, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Max': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: Max = Max[T=DT_INT64, Tidx=DT_INT32, keep_dims=false, _device=\"/device:GPU:0\"](Reshape, Const)]]\r\n\r\n```", "comments": ["Added a PR #13301 for that."]}, {"number": 13292, "title": "cudnn PoolBackward launch failed when using tf.nn.max_pool on Tensorflow GPU (Windows 10)", "body": "When using max_pool, the error below shows and stops the code. I used the code available here: [https://github.com/charlesashby/CharLSTM](https://github.com/charlesashby/CharLSTM). Specifically, I used the `lib_model/bidirectional_lstm.py` and the error occurs at the `tdnn` function when `tf.nn.max_pool` is ran.\r\n\r\nWhat does this error mean? Thanks!\r\n\r\n```\r\nCaused by op 'gradients/TDNN/MaxPool_grad/MaxPoolGrad', defined at:\r\n  File \"main.py\", line 33, in <module>\r\n    network.train()\r\n  File \"E:\\rktamplayo\\Personalized\\CharLSTM\\lib_model\\bidirectional_lstm.py\", line 165, in train\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\", line 526, in _MaxPoolGrad\r\n    data_format=op.get_attr(\"data_format\"))\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1752, in _max_pool_grad\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n...which was originally created as op 'TDNN/MaxPool', defined at:\r\n  File \"main.py\", line 31, in <module>\r\n    network.build()\r\n  File \"E:\\rktamplayo\\Personalized\\CharLSTM\\lib_model\\bidirectional_lstm.py\", line 103, in build\r\n    cnn = tdnn(self.X, kernels, kernel_features)\r\n  File \"E:\\rktamplayo\\Personalized\\CharLSTM\\lib_model\\bidirectional_lstm.py\", line 92, in tdnn\r\n    pool = tf.nn.max_pool(tf.nn.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1772, in max_pool\r\n    name=name)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1605, in _max_pool\r\n    data_format=data_format, name=name)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\seung-won\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cudnn PoolBackward launch failed\r\n         [[Node: gradients/TDNN/MaxPool_grad/MaxPoolGrad = MaxPoolGrad[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 1, 16, 1], padding=\"VALID\", strides=[1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TDNN/Tanh, TDNN/MaxPool, gradients/TDNN/Squeeze_grad/Reshape)]]\r\n```\r\n\r\n**What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?**\r\n\r\nI have searched through all websites including GitHub and StackOverflow, and it seems like this error has not been reported anywhere else.\r\n\r\n**Environment info**\r\n\r\n- Tensorflow 1.3.0\r\n- Python 3.5.3\r\n- CUDA 8.0\r\n- cuDNN 6.0\r\n- OS: Windows 10\r\n- GPU: GeForce GTX 1080ti", "comments": ["I got the same issue. By any chance, did you find out a solution to this error?", "I found that decreasing the number of parameters per run (e.g. by decreasing the batch size) solves the error.", "OK, closing for now, since there is an (imperfect) workaround.", "I am also facing the same issue. Please let us know a proper solution to this problem. ", "It can happen when some zombie process (invisible in nvidia-smi) has some memory allocated on the GPU.", "I also faced this issue when using tensorflow-gpu 1.4.0. When I switched to tensorflow cpu version 1.9.0 and ran the same code, this issue disappeared. So I updated my tensorflow-gpu version to 1.9.0 as well as CUDA and cuDNN to 9.0 and the issue is solved. ", "I had the same problem\r\nSolve it like this:\r\n\r\nCheck CUDA version with\r\n`nvidia-smi`\r\nResult was 11.0\r\n\r\nCheck CUDA version using\r\n`nvcc --version`\r\nResult was 10.0\r\n\r\n`cd usr/local`\r\n`rm cuda`\r\n`ln -s cuda-11.0 cuda`\r\n\r\nThe problem was that the link wasn't correct.", "I faced the same problem with `cudnn PoolBackward launch failed` when using a `MaxPooling2D` layer in a network. \r\n \r\nSimilar to @ShiraStarL (thanks!!) I had a **mismatch in CUDA versions**. I'm running tensorflow 2.4 and at time of installation had properly installed it with CUDA 11.0. Meanwhile I had installed another CUDA version which updated my `Path` variable. \r\nTo solve the issue on Windows 10, I made sure that `Path` points to the correct CUDA version as specified in https://www.tensorflow.org/install/source_windows#gpu "]}, {"number": 13291, "title": "Re-instate the plugin BUILD", "body": "This change re-instates the plugin BUILD link which was removed in commit 7de939bb74c5edbc2f45e77a5d4696e70bb59e5b", "comments": ["Can one of the admins verify this patch?", "@kayzhu what do you think? This was introduced in 169160056.", "@DavidNorman\r\n\r\nHi David, sorry for removing this in my earlier change: I wasn't quite sure what the empty BUILD file was for, and whether / how you were using it. \r\n\r\nI wonder:\r\n1. depending on your need, is there an alternative solution than introducing a new directory with an empty BUILD file?\r\n2. if not, I would suggest adding more comments in the BUILD file, or add README.md to this directory itself to indicate how it is being used, and why it is unwise to remove it :)\r\n\r\nThanks!\r\n", "Thanks.  I realize it looks a bit like a nothing.  However, since the repo keepers are (rightly) suggesting that our 3rd party code remains out of the main repo - I think it is important to have a sensible and clean way of integrating it.  Having to make edits to other BUILD files, where there are many more targets, seems messy.\r\n\r\nIt is a shame that there isn't a 'find and include a target if it exists' option in bazel.   I played around with a few possibilities, but as you can see in the conversations in the my original pull requests, they were considered more messy than this way.\r\n\r\nI'll add a README.md file to that directory.  \r\n", "I'm not 100% sure that this is needed. Can we iterate internally with @jart or @gunan and @yifeif on this?", "Adding @hawkinsp as well who may have some idea too.\r\n\r\n@drpngx Sounds good -- it's possible that they may be aware of a more standard way for this.\r\n\r\n@DavidNorman thanks for the update! I will see if @jart @gunan @yifeif has any suggestions for a possibly better solution for this issue.", "I have limited familiarity with XLA but I understand the problem you're describing.\r\n\r\nIf the labels were all knowable, you could have a `config_setting` like we do for jemalloc where the user passes `--config=with_xla_foo=true` to Bazel, which then gets triggered by `select()`. But if the labels are unknowable, then Bazel's design is you can only really do it by having the user define a new rule at the tip of the build tree.\r\n\r\nFor example, if XLA compiler was a cc_binary, you'd have your own rule with the standard set of plugins and a shell main.cc file that statically registers them and invokes the compiler. Then the user swaps that out to change the set of plugins. But since I'm assuming XLA is baked into TensorFlow's big .so file, it's probably not as trivial as writing a cc_binary() to define a custom tip for the entire TensorFlow build.\r\n\r\nIn that case, have you considered doing what custom ops do? Plugin is an alwayslink=1 library with that C++11 lambda singleton registration hack, which a Python file dynamically links at runtime.", "@jart - I've tried the dynamic linking thing but both bazel and protobuf are not up to the job.  protobuf doesn't like having multiple copies of itself in the same address space, and bazel didn't have the features for a DSO which is linked down to other DSOs.  or something like that.  i was in touch with a person from the bazel team who was fixing this, so it probably does now. \r\n", "I think a recent change by @allenlavoie together with some pending work will help with having multiple copies of protobuf. Ill let Allen comment more on dynamic linking.", "In principle we could do the same thing for XLA extensions that we do for op libraries and filesystems: include the registries and symbols needed to register new things in //tensorflow:libtensorflow_framework.so, then use an equivalent of tf.load_op_library to load a shared object (that object references symbols in libtensorflow_framework.so and does the registration; we could also add an XLA dlopen() wrapper to the C API for access in other languages). Right now all of the XLA symbols get added to pywrap_tensorflow directly, so that would require some rearranging.\r\n\r\nAfter the RTLD_GLOBAL removal you should need to explicitly include protobufs to break things; otherwise if you use the @protobuf_archive//:protobuf_headers you'll just pick up the version in libtensorflow_framework.so, which should work fine.\r\n\r\nThis sort of registration via dynamic linking would be a bit of work, so in the meantime I don't see a problem with doing plugins this way...", "That's good news regarding protobuf.  I did try, for a week, to generate a plugin DSO (hence the name of the driver directory).  I can see that protobuf was just not designed to support dynamic loading. Why would it be given where it has come from.  It doesn't have 16 bit type support for the same reason.\r\n\r\nI would be delighted to be able to release just a loadable driver though, so I will watch the neatening of the interfaces with interest. \r\n\r\ncheers for the responses.\r\n\r\n", "Hi all - so until the DSO changes that are required to get proper plugins working - can we reinstate these changes?\r\n\r\nthanks\r\n\r\n", "To be clear, I'm in favor but don't have much to do with XLA development. @kayzhu do you have objections?", "Jenkins, test this please.", "@allenlavoie No objections from me. I will take another look at it shortly for final approve if other folks are fine with this change too.", "Also no objections from me.", "And the failures are unrelated.", "I'm good with that. Sorry to delay this, I thought there would be a simpler solution.", "awesome.  thanks guys\r\n"]}, {"number": 13290, "title": "issue while installing tensorflow ", "body": "sws@sws-Aspire-5830T:~$ source activate tensorflow\r\n(tensorflow) sws@sws-Aspire-5830T:~$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nbash: syntax error near unexpected token `$'\r\n(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nbash: syntax error near unexpected token `$'\r\n(tensorflow) sws@sws-Aspire-5830T:~$ (tensorflow)$ pip3 install --ignore-installed --upgrade TF_BINARY_URL\r\nbash: syntax error near unexpected token `$'\r\n(tensorflow) sws@sws-Aspire-5830T:~$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nCollecting tensorflow==0.12.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl (43.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43.1MB 22kB/s \r\nCollecting protobuf>=3.1.0 (from tensorflow==0.12.1)\r\n  Downloading protobuf-3.4.0-cp35-cp35m-manylinux1_x86_64.whl (6.2MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.2MB 124kB/s \r\nCollecting wheel>=0.26 (from tensorflow==0.12.1)\r\n  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51kB 3.6MB/s \r\nCollecting numpy>=1.11.0 (from tensorflow==0.12.1)\r\n  Downloading numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.9MB 62kB/s \r\nCollecting six>=1.10.0 (from tensorflow==0.12.1)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow==0.12.1)\r\n  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481kB 202kB/s \r\nInstalling collected packages: six, setuptools, protobuf, wheel, numpy, tensorflow\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/wheel.py\", line 323, in clobber\r\n    shutil.copyfile(srcfile, destfile)\r\n  File \"/usr/local/lib/python3.5/shutil.py\", line 115, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.5/site-packages/six.py'\r\n(tensorflow) sws@sws-Aspire-5830T:~$ pip3 install --ignore-installed --upgrade $TF_BINARY_URL\r\nCollecting tensorflow==0.12.1 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl\r\n  Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.1-cp35-cp35m-linux_x86_64.whl\r\nCollecting six>=1.10.0 (from tensorflow==0.12.1)\r\n  Using cached six-1.11.0-py2.py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow==0.12.1)\r\n  Using cached wheel-0.30.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow==0.12.1)\r\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow==0.12.1)\r\n  Using cached protobuf-3.4.0-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow==0.12.1)\r\n  Using cached setuptools-36.5.0-py2.py3-none-any.whl\r\nInstalling collected packages: six, wheel, numpy, setuptools, protobuf, tensorflow\r\nException:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/basecommand.py\", line 215, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/commands/install.py\", line 342, in run\r\n    prefix=options.prefix_path,\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_set.py\", line 784, in install\r\n    **kwargs\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_install.py\", line 851, in install\r\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\r\n    isolated=self.isolated,\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/wheel.py\", line 345, in move_wheel_files\r\n    clobber(source, lib_dir, True)\r\n  File \"/usr/local/lib/python3.5/site-packages/pip/wheel.py\", line 329, in clobber\r\n    os.utime(destfile, (st.st_atime, st.st_mtime))\r\nPermissionError: [Errno 1] Operation not permitted\r\n\r\n", "comments": ["I believe the problem is with the USER PERMISSSIONS. Try : \r\n`sudo pip3 install --ignore-installed --upgrade $TF_BINARY_URL`\r\n", "Does this solver your problem @swaritras?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]