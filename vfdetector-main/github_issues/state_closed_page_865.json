[{"number": 27557, "title": "tf.keras lambda layer with sparse tensor caused AttributeError: 'SparseTensor' object has no attribute 'tocoo'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.14\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 1.13\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nsparse tensor operation inside a custom keras layer should not affect outside behavior if returning the expected type\r\n\r\n**Describe the expected behavior**\r\n`AttributeError: 'SparseTensor' object has no attribute 'tocoo'`\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport scipy.sparse\r\nimport numpy as np\r\n\r\ndef input_fn():\r\n\r\n    x = scipy.sparse.random(1, 400)\r\n    y = scipy.random.randint(2, size=(1,1))\r\n\r\n    indices = np.mat([x.row, x.col]).transpose()\r\n    sp = tf.sparse.SparseTensor(indices, x.data, x.shape)\r\n    d = tf.data.Dataset.from_tensors((sp,y))\r\n    return d\r\n\r\ninput_layer = tf.keras.layers.Input(shape=(400, ), sparse=True)\r\nweights = tf.get_variable(name='weights', shape=(400, 1))\r\n\r\nweights_mult = lambda x: tf.sparse_tensor_dense_matmul(x, weights)\r\noutput_layer=  tf.keras.layers.Lambda(weights_mult)(input_layer)\r\nmodel = tf.keras.Model([input_layer], output_layer)\r\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nd = input_fn()\r\nmodel.fit(d.make_one_shot_iterator(), epochs=3, steps_per_epoch=1)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nEpoch 1/3\r\nTraceback (most recent call last):\r\n  File \"sparse.py\", line 24, in <module>\r\n    model.fit(d.make_one_shot_iterator(), epochs=3, steps_per_epoch=1)\r\n  File \"/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 266, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/Users/jz/coding/t1/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3046, in __call__\r\n    sparse_coo = value.tocoo()\r\nAttributeError: 'SparseTensor' object has no attribute 'tocoo'\r\n```", "comments": ["I have the same issue. Any updates?", "[Looks like tf.keras expects you to be passing a scipy sparse matrix if your input is a sparse tensor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/backend.py#L3327)\r\n\r\n\r\n    for tensor, value in zip(self.inputs, inputs):\r\n      ...\r\n      if is_sparse(tensor):\r\n        sparse_coo = value.tocoo()", "I ended up using the fact that you can assign an input to a tensor:\r\n`sparse_tensor=tf.SparseTensor(np.stack(sparse.nonzero(), axis=1), sparse.data, sparse.shape)\r\ninput=tf.keras.layers.Input(shape=(None,), tensor=sparse_tensor, dtype=tf.float32)`\r\n\r\n(sparse is a scipy csr sparse matrix)", "For those who is trying to find the solution for the same issue: This is a bug on TF 1.13 and 1.14, which gets resolved on 1.15.\r\n\r\nExample code can be found here with 1.15:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/31819\r\n\r\n", "@jianlingzhong,\r\nSorry for the delayed response. Your code runs now without error. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/d7a7eaa328652f16b477eab176042890/gh_27557.ipynb) of the working code. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27557\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27557\">No</a>\n"]}, {"number": 27556, "title": "Wheel and libtensorflow archives contain duplicated .so files instead of symlinks", "body": "Although https://github.com/tensorflow/tensorflow/pull/27493 resolves our CI failures, there's still a critical issue to address that **must be fixed before April 15**: we couldn't figure out how to get symlinks working properly in Bazel-generated archives, so the extra `tensorflow_framework.so.*.*` files are all duplicates. The pip wheel is now ~120MB:\r\n\r\n```\r\n... ... 120M Apr  5 16:05 tensorflow-1.13.1-cp27-cp27mu-linux_x86_64.whl\r\n```\r\nhttps://github.com/pypa/packaging-problems/issues/101 seems to be the most recent documentation explaining TF's maximum wheel size limit, which is currently 100MB. We won't be able to upload the 1.14 release until this is addressed. TF is [just scraping the limit at 92MB](https://pypi.org/project/tensorflow/#files), so all extra files will have to be symlinks to fix this properly.\r\n\r\nThe libtensorflow archives are affected by this as well.\r\n\r\nFYI @gunan, assigning to myself and @perfinion.", "comments": ["For some more context, even if Bazel is configured to generate symlinks, the `pkg_tar` rule (and whatever `build_pip_package` does) will dereference them when packaging the files. We'd like our binary files to include e.g. `libtensorflow.so` and `libtensorflow.so.1` which both symlink to `libtensorflow.so.1.13.0` (I think). I can put together a demonstration of this if we need it.", "Also somewhat related: https://github.com/tensorflow/tensorflow/issues/27430\r\n\r\nSo the only one that is strictly required in the pip wheel is `libtensorflow_framework.so.1` since that is the one that matches the soname. The fuller length `libtensorflow_framework.so.1.13.1` is used in distros a lot (with the symlinks) but for the pip wheel i have no issues dropping it.\r\n`ldd $(find . -iname \"*\\.so*\") | grep '^\\s' | sed 's/(0x[0-9a-fA-F]*)$//' | sort -u ` in the `site-packages/tensorflow` dir shows what things depend on (ie only the .so.1 file)\r\n\r\nThe unversioned `libtensorflow_framework.so` one is only required for compiling. It makes `gcc foo.c -ltensorflow_framework` work nicely but compiling with the full name directly does also work, you just have to do `gcc foo.c -l:libtensorflow_framework.so.1`, so we could drop the unversioned one too to save space. I'd rather have a symlink obviously but there are other ways to do it need be.", "I think this is fixed, or at least no longer an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27556\">No</a>\n"]}, {"number": 27555, "title": "Updated doc for py_function", "body": "Since `tf.contrib.eager.py_func` is deprecated, updating the docs to point to the new function `tf.py_function`.\r\n\r\nNote: This is my first PR to this repo.", "comments": ["Thanks!"]}, {"number": 27554, "title": "Fixing a bug in tf.parallel_stack", "body": "This PR fixes the bug reported in this issue #27521 .\r\nIt also updates the unit test to add one more test case since this bug only shows up with large tensors. ", "comments": ["@rthadur it looks like the failures here are not related to the PR.", "@rthadur can you let us know what internal test (feedback/copybara) is failing?", "@mahmoud-abuzaina it looks like unrelated flakes", "Woohoo!\n\nOn Tue, Apr 9, 2019 at 2:32 PM TensorFlow Copybara <notifications@github.com>\nwrote:\n\n> Merged #27554 <https://github.com/tensorflow/tensorflow/pull/27554> into\n> master.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/27554#event-2264284029>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQH9DiuOYrzs8kGaAtmCWEXSu40Kxqjhks5vfQbcgaJpZM4cfuB2>\n> .\n>\n"]}, {"number": 27553, "title": "TFTRT: Complete IActivationLayer coverage for TRT 5.1", "body": "* The following ops are now supported for TRT >= 5.1.2: ClipByValue, Elu, Selu, Softsign, Softplus\r\n* Relu6 and LeakyRelu will use the new accelerated IActivationLayer implementations if TRT 5.1.2+ is present\r\n* ConvertActivation now uses `ActivationTypeMap()` similar to how ConvertUnary does.\r\n* Unit tests for all new ops", "comments": ["@trevor-m could you please resolve the conflicts?"]}, {"number": 27552, "title": "GcsFilesystem not able to set dns cache when using long constructor", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-alpla0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe GcsFileSystem has a default constructor and a long constructor. While the default constructor initialize variables either to default option or from environment variables, the long constructor initialize a few variables that's passed in from constructor parameters, but left out the others (e.g. dns cache).\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo. We can still initialize the other things from environment variables.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nanyone who uses GcsFileSystem can have more options configuring the filesystem while using the long constructor.\r\n\r\n**Any Other info.**\r\n", "comments": ["If this looks good I can submit a PR.", "@saeta What's your thought as the dns cache author? \ud83d\ude42", "Hi @ziliangpeng ! Thanks for reaching out. I think this would be great to fix. (Out of curiosity, do you have an alternate DNS cache implementation in mind, or some other use case?)", "@saeta Hey, thanks for the response! \r\n\r\nTaking a deeper look into the code, it looks like the DNS cache can be bake into the http connection factory, which is part of the long constructor's parameter, so I think there is a way to do DNS caching without modifying the constructor (tho not sure if this is the recommended way).\r\n\r\nDon't have an alternate impl in head, but even if there is another impl, it looks like it can be baked into the connection factory too.", "Closing this out, as it appears to not be making progress. If someone is interested in pushing this along, please do feel free. Thanks! -Brennan"]}, {"number": 27551, "title": "Fixed the statility issue of the autotuning process", "body": "(1) Add an environment variable TF_AUTOTUNE_EPSILON (default=0.03) to enforce a fussy time comparison to reduce the chances of the autotuning process flipping back and forth between close algorithms.\r\n\r\n(2) Add a max *global* autotuning iteration count to completely avoid the case that the autotuner cannot stabilize after many iterations if the two algorithms are very close to each other.  \r\n\r\nTo problem (2), for example: \r\nSuppose the winner sequence is 0 1 0 1 0 1 0 1..., (meaning algorithm 0 and 1 are very close to each other, and they appear as the winner alternatively).\r\nAccording to the current auto-tuning algorithm and if we have set the TF_AUTOTUNE_THRESHOLD = 2, meaning the target score=2, and max (local) count = 5x2x2=20. \r\n\r\nThe auto tuner will do this: \r\n0 wins: create node 0 (1, 1) # the numbers in () mean the current score and local_iteration_count\r\n1 wins: demote node 0 (0, 2) and delete node 0 # because the winner changes\r\n0 wins: create node 0 (1, 1) \r\n1 wins: demote node 0 (0, 2) \r\n....\r\nThis means the auto-tuner will never end, since the score and the local iteration count can never reach 2 and 20.\r\n\r\nFYI @nluehr ", "comments": ["Any updates on this PR?", "@jlebar Can you review this PR for us?", "> @jlebar Can you review this PR for us?\r\n\r\nI work on XLA, and this is a TensorFlow change, so I can't.  @gunan is also not the right reviewer, that may be why you haven't gotten a response.  cc @tatianashp @chsigg.\r\n\r\n(That said, as I understand it this PR says that if algorithm N is 2.9% slower than algorithm N+1, TF will always choose algorithm N.  If we were trying to make this change in XLA I'd have a lot of questions as far as whether this is the best and simplest way to accomplish the goal...)", "Kaixi, do you think just adding the max global iterations would be sufficient to prevent endless toggling? Then we wouldn't risk the issue that Justin is describing.\r\n", "Hi @chsigg , thanks for your comments. \r\n\r\nYes. I have removed other parts and only keep the max global iteration now.  "]}, {"number": 27550, "title": "Fixing capitalization of PyPi.", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27550) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27550) for more info**.\n\n<!-- ok -->"]}, {"number": 27549, "title": "CUBLAS_STATUS_NOT_INITIALIZED", "body": "Hi I am trying to run ResNet50 (v1.5) with data parallelism using horovod. When I run the model on one GPU it works. However, when I try to train the model on multiple GPUs, I get crazy errors, as follows: \r\n\r\n2019-04-05 18:59:27.749865: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 28.82G (30950542336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.754252: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 25.94G (27855489024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.772229: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 23.35G (25069938688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.779624: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 21.01G (22562945024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.783168: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 18.91G (20306649088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.787585: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 17.02G (18275983360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.791698: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 15.32G (16448385024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.795240: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 13.79G (14803546112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.798853: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 12.41G (13323191296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.802389: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 11.17G (11990872064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.805892: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 10.05G (10791784448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.809407: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 9.04G (9712606208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.812910: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 8.14G (8741345280 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.816456: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 7.33G (7867210752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.819975: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 6.59G (7080489472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.823475: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 5.93G (6372440576 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.826972: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 5.34G (5735196160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.830468: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.81G (5161676288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.834110: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.33G (4645508608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.837658: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.89G (4180957696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.841171: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.50G (3762861824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.844700: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.15G (3386575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.848335: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.84G (3047918080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.851924: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.55G (2743126272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.859055: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 2.07G (2221932032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory                                                  [257/1934]\r\n2019-04-05 18:59:27.862563: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.86G (1999738880 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-04-05 18:59:27.866054: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 1.68G (1799764992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nI0405 18:59:28.200468 140411113244416 tf_logging.py:115] Running local_init_op.\r\nI0405 18:59:28.241837 140411113244416 tf_logging.py:115] Done running local_init_op.\r\nI0405 18:59:28.467390 139802759370496 tf_logging.py:115] Running local_init_op.\r\nI0405 18:59:28.531050 139802759370496 tf_logging.py:115] Done running local_init_op.\r\nI0405 18:59:30.066596 140411113244416 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.\r\nI0405 18:59:30.281409 139802759370496 tf_logging.py:115] Saving checkpoints for 0 into /tmp/cifar10_model/model.ckpt.\r\n2019-04-05 18:59:31.456932: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:31.464278: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:31.469508: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:31.695706: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:31.708371: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:31.723334: E tensorflow/stream_executor/cuda/cuda_blas.cc:464] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2019-04-05 18:59:32.145557: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2019-04-05 18:59:32.152672: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n\r\nFor looking online, the CUDA_ERROR_OUT_OF_MEMORY errors don't matter because the program keeps running afterwards. However, I don't know why I am getting the other errors. Why can I train the model on one GPU and not multiple? My packages should be compatible with one another. I am using: \r\ntensorflow-gpu==1.12.0\r\ncuda-9.0 \r\ncudnn-7\r\nThese packages should be compatible with one another, right? \r\n\r\nIf anyone has any idea what the issue may be, I am open to any suggestions. \r\n\r\nThank you \r\n", "comments": ["@pstefanou12 This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nThere are many resources pointing to this error. Here is [one](https://stackoverflow.com/questions/39465503/cuda-error-out-of-memory-in-tensorflow) that is more relevant. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am closing the issue as it is not related to Build/Installation or Bug/Performance issue. Thanks!"]}, {"number": 27548, "title": "tf.contrib.labeled_tensor incorrectly handling or failing with None dimensions", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution : macOS Sierra\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 2.7\r\n\r\n**labeled_tensor.mul fails with None dimensions when tf.multiply doesn't.**\r\n\r\nipdb> X\r\n\r\n<LabeledTensor 'X:0' shape=(1, 1, 2, 12, 1, 300) dtype=float32\r\n axes=[('a', Dimension(1)),\r\n       ('b', Dimension(1)),\r\n       ('c', Dimension(2)),\r\n       ('d', Dimension(12)),\r\n       ('e', Dimension(1)),\r\n       ('f', Dimension(200))]>\r\n\r\nipdb> Y\r\n\r\n<LabeledTensor 'Y:0' shape=(?, 6, 1, 1, 200, 300) dtype=float32\r\n axes=[('a', Dimension(None)),\r\n       ('b', Dimension(6)),\r\n       ('c', Dimension(1)),\r\n       ('d', Dimension(1)),\r\n       ('e', Dimension(200)),\r\n       ('f', Dimension(300))]>\r\n\r\nipdb> X*Y\r\n\r\n*** ValueError: Mismatched 'a' axis on input tensors: Axis('a', Dimension(1)) and Axis('a', Dimension(None))\r\n\r\nipdb> lt.mul(X,Y)\r\n\r\n*** ValueError: Mismatched 'a' axis on input tensors: Axis('a', Dimension(1)) and Axis('a', Dimension(None))\r\n\r\nipdb> tf.multiply(X,Y)\r\n\r\n<tf.Tensor  'Mul_2:0' shape=(?, 6, 2, 12, 200, 300) dtype=float32>\r\n", "comments": ["@lw394 Thanks for the question! Endpoints housed in `tf.contrib.*` are unsupported, with volatile functionality, and [`tf.contrib.labeled_tensor`](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md#list-of-projects) will be deprecated in TensorFlow 2.0. \r\n\r\n`lt.mul`, `*`, and `tf.multiply` have slightly different functionality (for example, the latter supports broadcasting), and it is expected that they would have different output.", "Thanks for the reply!  Will there something serving this purpose in 2.0?\n\nOn Sat, Apr 6, 2019, 8:55 AM Paige Bailey <notifications@github.com> wrote:\n\n> @lw394 <https://github.com/lw394> Thanks for the question! Endpoints\n> housed in tf.contrib.* are unsupported, with volatile functionality, and\n> tf.contrib.labeled_tensor\n> <https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md#list-of-projects>\n> will be deprecated in TensorFlow 2.0.\n>\n> lt.mul, *, and tf.multiply have slightly different functionality (for\n> example, the latter supports broadcasting).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/27548#issuecomment-480501759>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APJ-By4y4k21MpL6Z25t7K-CtnRfmgqaks5veJk1gaJpZM4cfcRv>\n> .\n>\n", "I think it was resolved. I am closing the issue. But, please let me know if I'm mistaken.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27548\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27548\">No</a>\n"]}, {"number": 27547, "title": "Add Tensorflow.Distributions to 2.0", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf_nightly_gpu_2.0_preview-2.0.0.dev20190327-cp36-cp36m-win_amd64\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWill tensorflow.distributions be added to 2.0? Currently, TFP doesn't work with 2.0 due to its absence.\r\n\r\nThanks!\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers of Tensorflow Probability\r\n\r\n**Any Other info.**\r\n", "comments": ["``tfp-nightly`` should works fine with Tensorflow 2.0, at least for my package CI test cases.", "distributions is part of tfp : https://www.tensorflow.org/probability/api_docs/python/tfp/distributions", "@armando-fandango intuitively, this is what I would expect, but I get this error. and if you open up the actual distribution modules in tfp, you'll see that they import from tf\r\n\r\n```\r\nIn [1]: import tensorflow_probability\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-03228327e8d6> in <module>()\r\n----> 1 import tensorflow_probability\r\n\r\nC:\\Users\\asus\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\__init__.py in <module>()\r\n     19\r\n     20 # from tensorflow_probability.google import staging  # DisableOnExport\r\n---> 21 from tensorflow_probability.python import *  # pylint: disable=wildcard-import\r\n     22 from tensorflow_probability.python.version import __version__\r\n\r\nC:\\Users\\asus\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py in <module>()\r\n     20\r\n     21 from tensorflow_probability.python import bijectors\r\n---> 22 from tensorflow_probability.python import distributions\r\n     23 from tensorflow_probability.python import edward2\r\n     24 from tensorflow_probability.python import glm\r\n\r\nC:\\Users\\asus\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\__init__.py in <module>()\r\n     20 # pylint: disable=unused-import,line-too-long,g-importing-member\r\n     21\r\n---> 22 from tensorflow_probability.python.distributions.autoregressive import Autoregressive\r\n     23 from tensorflow_probability.python.distributions.batch_reshape import BatchReshape\r\n     24 from tensorflow_probability.python.distributions.binomial import Binomial\r\n\r\nC:\\Users\\asus\\Anaconda3\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\autoregressive.py in <module>()\r\n     24\r\n     25\r\n---> 26 class Autoregressive(tf.distributions.Distribution):\r\n     27   \"\"\"Autoregressive distributions.\r\n     28\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'distributions'\r\n\r\nIn [2]:\r\n```\r\n\r\n@henrysky, thanks, I'll give it a try.\r\n", "@dorian821 You might need to install `tfp-nightly`, if you had a previous version of TensorFlow Probability (i.e., not compliant with TF 2.0). Let us know if you run into any snags! \ud83d\ude42 ", "@dynamicwebpaige thanks! I've installed it and it's working fine. I've noticed that tfp-nightly-gpu however is still missing a lot of the layers and distributions present in tfp-nightly. Is the gpu distribution likely to be updated soon?", "I think they abandoned the tfp-gpu since tfp is completely based on tf now. So tfp-gpu is not neccessary anymore", "ah ok, great. thanks for clarifying."]}, {"number": 27546, "title": "Fix configure.py to properly compare \"X.Y.Z\" with \"X.Y\"", "body": "Right now, \"0.24\" is treated as lower than \"*.*.*\" because of the odd comparison method that adds digits to each existing section. This change converts \"0.24\" to \"0.24.0\" to fix that.\n\nThis will probably need to be pulled into r2.0.\n\nPiperOrigin-RevId: 241950768", "comments": ["@bananabowl @mihaimaruseac can we merge this ?", "I think so. Added @goldiegadde to confirm", "Can one of the admins verify this patch?"]}, {"number": 27545, "title": "[TF 2.0]  tf.estimator.ProfilerHook... is not compatible with eager execution", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 8.8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): `tf.__git_version__ : 'v1.12.0-9492-g2c319fb415', version tensorflow==2.0.0-alpha0`\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nWhen constructing a `tf.estimator.ProfilerHook` a error is thrown: `RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.`\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.estimator.ProfilerHook(10)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nI'm not very familiar with tf 2.0, so maybe this is just user error? Is it possible to use tf2 not in eager mode?\r\n\r\nFull traceback: \r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-4eefc50dcc45> in <module>\r\n----> 1 tf.estimator.ProfilerHook(10)\r\n\r\n~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py in __init__(self, save_steps,save_secs, output_dir, show_dataflow, show_memory)\r\n   1013     \"\"\"\r\n   1014     self._output_file = os.path.join(output_dir, \"timeline-{}.json\")\r\n-> 1015     self._file_writer = SummaryWriterCache.get(output_dir)\r\n   1016     self._show_dataflow = show_dataflow\r\n   1017     self._show_memory = show_memory\r\n\r\n~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer_cache.py in get(logdir)\r\n     61       if logdir not in FileWriterCache._cache:\r\n     62         FileWriterCache._cache[logdir] = FileWriter(\r\n---> 63             logdir, graph=ops.get_default_graph())\r\n     64       return FileWriterCache._cache[logdir]\r\n\r\n~/tf2-venv/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py in __init__(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\r\n    358     if context.executing_eagerly():\r\n    359       raise RuntimeError(\r\n--> 360           \"tf.summary.FileWriter is not compatible with eager execution. \"\r\n    361           \"Use tf.contrib.summary instead.\")\r\n    362     if session is not None:\r\n\r\nRuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.\r\n```\r\n", "comments": ["@lendle Could you try this to disable eager execution in 2.0\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\ntf.estimator.ProfilerHook(10)\r\n\r\nThanks!", "@jvishnuvardhan `tf.compat.v1.disable_eager_execution()` worked, and then I was able to use the ProfilerHook with the estimator api. \r\n\r\nI was also able to get something working by copying the implementation of ProfilerHook and changing `self._file_writer = SummaryWriterCache.get(output_dir)` to `self._file_writer = create_file_writer(output_dir, name='my_profiler')` in the constructor and removing `self._file_writer.add_run_metadata` in `after_run`....Doesn't look like anything else references `_file_writer`, so that's probably not actually doing anything at all, but I was at least able to get tracing information from the profiler in eager mode that way.\r\n", "@lendle looks like original issue was resolved and this is another issue. Could you open a new issue and add more details about the issue and its context. Please also mention overall goal. Thanks!", "@jvishnuvardhan as far as I can tell the only way to disable eager execution is with `tf.compat.v1.disable_eager_execution()`. If I understand correctly, the purpose of the `tf.compat.v1` module is to help transition from legacy v1 to v2, right?\r\n\r\nGiven that `tf.estimator.ProfilerHook` is in the public API in TF 2.0, and it cannot be used without also using a legacy api, I would argue that this is not fixed.", "@reedwm Could you please take a look at this issue? Thanks!", "I also have this problem. Moreover, since `RunOptions` has been moved to compat.v1, there seem to be no proper way of generating a profiling timeline in TensorFlow 2.\r\n\r\nEDIT: apparently you now can using TensorBoard. See https://www.tensorflow.org/tensorboard/r2/ and https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/trace_on", "[`tf.train.ProfilerHook`](https://github.com/tensorflow/tensorflow/blob/c4b61514434646defb4c17ab10afe0e83fb9e969/tensorflow/python/training/basic_session_run_hooks.py#L979) is only exposed in Tensorflow 1. But, [`tf.estimator.ProfilerHook`](https://github.com/tensorflow/estimator/blob/b47b26cb0ecbd816e2e47146636caf58c54e57d7/tensorflow_estimator/python/estimator/hooks/basic_session_run_hooks.py#L49) is also exposed in TensorFlow 2, despite not working with TensorFlow 2. Either `tf.estimator.ProfilerHook` should be made to work in TF 2, or should not be exposed in TF 2.\r\n\r\n@rchao  can you take a look?", "I think we should make it work in TF2. Will take a look what the options are here.", "The bug still exists in tensorflow==2.0.0-beta1\r\n### System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Ubuntu 18.04 \r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version (use command below): tf.__git_version__ : 'v2.0.0-beta0-17-g8e423e3', version -- - tensorflow==2.0.0-beta1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA Titan XP\r\n\r\n### Code to reproduce the issue\r\n    profiler_hook = tf.estimator.ProfilerHook(save_steps=2000, output_dir=MODEL_DIR + '/profiler/', show_memory=True)\r\n    train_spec = tf.estimator.TrainSpec(lambda: train_input_fn(train_raw, BATCH_SIZE, EPOCH, TRAIN_BUFFER_SIZE),  hooks=[profiler_hook])\r\n    eval_spec = tf.estimator.EvalSpec(lambda: test_input_fn(test_raw, BATCH_SIZE),\r\n                                      throttle_secs=100)\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n    model.summary()\r\n### Other Info / Logs\r\nThe full traceback is listed as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 124, in <module>\r\n    main()\r\n  File \"main.py\", line 110, in main\r\n    profiler_hook = tf.estimator.ProfilerHook(save_steps=2000, output_dir=MODEL_DIR + '/profiler/', show_memory=True)\r\n  File \"/data/wangzejun/anadonda/envs/tf2.0/lib/python3.7/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 1024, in __init__\r\n    self._file_writer = SummaryWriterCache.get(output_dir)\r\n  File \"/data/wangzejun/anadonda/envs/tf2.0/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer_cache.py\", line 63, in get\r\n    logdir, graph=ops.get_default_graph())\r\n  File \"/data/wangzejun/anadonda/envs/tf2.0/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py\", line 360, in __init__\r\n    \"tf.summary.FileWriter is not compatible with eager execution. \"\r\nRuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.\r\n```\r\n\r\nThe instruction from compat.v1 still works fine, which makes the code a little bit ugly.  <br>\r\nThank for your attention.", "@eecshope I could reproduce the issue with `!pip install tf-nightly-2.0-preview`. Here is a [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/1d4a46f570cf5c2447d45ac48661b1e0/untitled466.ipynb) with !pip install `tf-nightly-2.0-preview`. Thanks!", "I'd also be curious how to use this with `model.fit` to generate a Chrome Trace.", "It is also not compatible with tf.distribute.MirroredStrategy.\r\n```\r\n2019-11-01 09:03:00.713665: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n2019-11-01 09:03:00.714991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.0\r\n2019-11-01 09:03:02.435473: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 2969 kernel records, 109 memcpy records.\r\nTraceback (most recent call last):\r\n  File \"/home/steve/source/core/python/sonia/models/model/train.py\", line 26, in <module>\r\n    ds.triplet_training()\r\n  File \"/home/steve/source/core/python/sonia/models/model/model.py\", line 160, in triplet_training\r\n    steps=None))\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1221, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1332, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1493, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1360, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\r\n    run_metadata=run_metadata)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 959, in run\r\n    run_metadata.ParseFromString(compat.as_bytes(proto_data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n\r\nProcess finished with exit code 1```", "> It is also not compatible with tf.distribute.MirroredStrategy.\r\n> \r\n> ```\r\n> 2019-11-01 09:03:00.713665: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n> 2019-11-01 09:03:00.714991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.0\r\n> 2019-11-01 09:03:02.435473: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 2969 kernel records, 109 memcpy records.\r\n> Traceback (most recent call last):\r\n>   File \"/home/steve/source/core/python/sonia/models/model/train.py\", line 26, in <module>\r\n>     ds.triplet_training()\r\n>   File \"/home/steve/source/core/python/sonia/models/model/model.py\", line 160, in triplet_training\r\n>     steps=None))\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n>     return executor.run()\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n>     return self.run_local()\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n>     saving_listeners=saving_listeners)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\r\n>     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1158, in _train_model\r\n>     return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1221, in _train_model_distributed\r\n>     self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1332, in _actual_train_model_distributed\r\n>     saving_listeners)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1493, in _train_with_estimator_spec\r\n>     _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\r\n>     run_metadata=run_metadata)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\r\n>     run_metadata=run_metadata)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1360, in run\r\n>     raise six.reraise(*original_exc_info)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n>     raise value\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\r\n>     return self._sess.run(*args, **kwargs)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\r\n>     run_metadata=run_metadata)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\r\n>     return self._sess.run(*args, **kwargs)\r\n>   File \"/home/steve/venv/sonia/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 959, in run\r\n>     run_metadata.ParseFromString(compat.as_bytes(proto_data))\r\n> google.protobuf.message.DecodeError: Error parsing message\r\n> \r\n> Process finished with exit code 1```\r\n> ```\r\n\r\nI am facing the same issue. Any update?", "@wangsiyu No. I still find it incomprehensible that TF 2.0 shipped without profiling working properly. We are in the process of abandoning estimators which I am very sad about.", "facing the same problem. tf.estimator.profilehook must work in eager mode. but in eager mood, I get\r\n\r\n> RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead.", "Is there any solution of this? Facing this same problem too.", "Discussed with @reedwm and we think the best solution is to un-export ProfilerHook from TF2. Will send out a fix for this.", "@rchao  - What would be the replacement for the ProfilerHook in TF2? is it possible to see the chromo json logs in another way?", "@eyalhir74 - training with profiling can be enabled with using [`tf.keras.callbacks.TensorBoard(profile_batch=...)`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard?version=nightly). Can you check if this fits your needs?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27545\">No</a>\n"]}, {"number": 27543, "title": "tf2.0 failed to save model if an input is used not used in the model directly", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0a0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI have this use case where the model requires multiple inputs and some inputs go into the model layers and some inputs go into the loss function directly. It works fine in the past until I updated my package unit test to tf2.0 to test drive and find out tf2.0 failed test that run fine with tf1.x\r\n\r\nI noticed that its mainly due to the tensor that goes directly to the loss function in this case is not presented in ``self._network_nodes`` in the line ``if node_key not in nn.keras_model._network_nodes:`` in tensorflow keras network.py. \r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the model to be saved successfully with tf2.0 just as tf1.x\r\n\r\n**Code to reproduce the issue**\r\n```python3\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as tfk\r\nSequence = tfk.utils.Sequence\r\n\r\nDense = tfk.layers.Dense\r\nInput = tfk.layers.Input\r\n\r\nModel = tfk.models.Model\r\n\r\ndef special_loss(weights):\r\n    def special_loss_internal(true, pred):\r\n        return (true - pred / weights)\r\n    return special_loss_internal\r\n\r\n# Model 1 which does not have Flatten\r\ninput_tensor1 = Input(shape=[200], name='input_1')\r\ninput_tensor2 = Input(shape=[10], name='input_2')\r\noutput_tensor1 = Dense(units=10, name='output_1')(input_tensor1)\r\noutput_tensor2 = Dense(units=10, name='output_2')(input_tensor1)\r\n\r\nneuralnet = Model(inputs=[input_tensor1, input_tensor2], outputs=[output_tensor1, output_tensor2])\r\nneuralnet.compile(loss=special_loss(input_tensor2), optimizer='adam')\r\n\r\nneuralnet.save(\"test.h5\")\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in pack_sequence_as(structure, flat_sequence, expand_composites)\r\n    430     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\r\n--> 431                                                     0, is_seq)\r\n    432     if final_index < len(flat_sequence):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in _packed_nest_with_indices(structure, flat, index, is_seq)\r\n    380     else:\r\n--> 381       packed.append(flat[index])\r\n    382       index += 1\r\n\r\nIndexError: list index out of range\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-6f88ca6a69df> in <module>\r\n     26 neuralnet.compile(loss=special_loss(input_tensor2), optimizer='adam')\r\n     27\r\n---> 28 neuralnet.save(\"test.h5\")\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in save(self, filepath, overwrite, include_optimizer)\r\n   1312\r\n   1313     from tensorflow.python.keras.models import save_model  # pylint: disable=g-import-not-at-top\r\n-> 1314     save_model(self, filepath, overwrite, include_optimizer)\r\n   1315\r\n   1316   def save_weights(self, filepath, overwrite=True, save_format=None):\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py in save_model(model, filepath, overwrite, include_optimizer)\r\n     99         {\r\n    100             'class_name': model.__class__.__name__,\r\n--> 101             'config': model.get_config()\r\n    102         },\r\n    103         default=serialization.get_json_type).encode('utf8')\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in get_config(self)\r\n   1107       model_inputs.append(\r\n   1108           tf_utils.ListWrapper([layer.name, new_node_index, tensor_index]))\r\n-> 1109     model_inputs = nest.pack_sequence_as(self._nested_inputs, model_inputs)\r\n   1110     model_inputs = tf_utils.convert_inner_node_data(model_inputs)\r\n   1111     config['input_layers'] = model_inputs\r\n\r\n~\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\nest.py in pack_sequence_as(structure, flat_sequence, expand_composites)\r\n    438           \"Could not pack sequence. Structure had %d elements, but \"\r\n    439           \"flat_sequence had %d elements.  Structure: %s, flat_sequence: %s.\" %\r\n--> 440           (len(flat_structure), len(flat_sequence), structure, flat_sequence))\r\n    441   return _sequence_like(structure, packed)\r\n    442\r\n\r\nValueError: Could not pack sequence. Structure had 2 elements, but flat_sequence had 1 elements.  Structure: [<tf.Tensor 'input_1:0' shape=(None, 200) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 10) dtype=float32>], flat_sequence: [<tensorflow.python.keras.utils.tf_utils.ListWrapper object at 0x000001958CF2DDA0>].\r\n```\r\n", "comments": ["I could reproduce the issue with TF2.0.0-alpha0 in Google colab. Thanks!", "is there a schedule for this issue to be fixed?? Because I really want this issue to be fixed before 2.0.0a1 so that I can see if there is any further issue in my CI tests.", "Same problem with 1.14.RC1 on colab now.\r\nIt can't load model from an h5 file either.", "@jvishnuvardhan and @k-w-w:\r\n\r\nIs there any ETA for a fix or any direction so I can contribute for a fix?? Thanks!", "@henrysky As you are interested to contribute, You can raise a PR. Thanks!", "Hi henrysky@ Thanks so much for implementing your fix! Sorry for not updating sooner! I submitted a change that addressed this issue earlier here: https://github.com/tensorflow/tensorflow/commit/401bbfc33684c21325d81a03708fe123d59ce527#diff-4ee308ea180d49ae81691348531a2b6d", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27543\">No</a>\n", "same issue in 1.14.0", "> same issue in 1.14.0\r\n\r\nIt is just a one line patch, you can patch your 1.14.0 like me by yourself\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/401bbfc33684c21325d81a03708fe123d59ce527#diff-4ee308ea180d49ae81691348531a2b6d", "> > same issue in 1.14.0\r\n> \r\n> It is just a one line patch, you can patch your 1.14.0 like me by yourself\r\n> \r\n> [401bbfc#diff-4ee308ea180d49ae81691348531a2b6d](https://github.com/tensorflow/tensorflow/commit/401bbfc33684c21325d81a03708fe123d59ce527#diff-4ee308ea180d49ae81691348531a2b6d)\r\n\r\noh, thank you!\r\nI have tried it but there will be another problem. \r\n\r\n```\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in save_model_to_hdf5(model, filepath, overwrite, include_optimizer)\r\n     99             'config': model.get_config()\r\n    100         },\r\n--> 101         default=serialization.get_json_type).encode('utf8')\r\n    102 \r\n    103     model_weights_group = f.create_group('model_weights')\r\n\r\n/usr/local/lib/python3.6/json/__init__.py in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\r\n    236         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\r\n    237         separators=separators, default=default, sort_keys=sort_keys,\r\n--> 238         **kw).encode(obj)\r\n    239 \r\n    240 \r\n\r\n/usr/local/lib/python3.6/json/encoder.py in encode(self, o)\r\n    197         # exceptions aren't as detailed.  The list call should be roughly\r\n    198         # equivalent to the PySequence_Fast that ''.join() would do.\r\n--> 199         chunks = self.iterencode(o, _one_shot=True)\r\n    200         if not isinstance(chunks, (list, tuple)):\r\n    201             chunks = list(chunks)\r\n\r\n/usr/local/lib/python3.6/json/encoder.py in iterencode(self, o, _one_shot)\r\n    255                 self.key_separator, self.item_separator, self.sort_keys,\r\n    256                 self.skipkeys, _one_shot)\r\n--> 257         return _iterencode(o, 0)\r\n    258 \r\n    259 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/util/serialization.py in get_json_type(obj)\r\n     67     return dict(obj)\r\n     68 \r\n---> 69   raise TypeError('Not JSON Serializable:', obj)\r\n\r\nTypeError: ('Not JSON Serializable:', b'\\n\\x03Mul\\x12\\x03Mul\\x1a\\x0fconv2d7/Sigmoid\\x1a\\x0cdepth_result*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n```\r\n\r\nAnd I use tf.saved_model.save to save the model and there is no error.\r\n", "@TachikakaMin What model are you trying to save out (where is b'\\n\\x03Mul\\x12\\x03Mul\\x1a\\x0fconv2d7/Sigmoid\\x1a\\x0cdepth_result*\\x07\\n\\x01T\\x12\\x020\\x01' coming from?)", "> > same issue in 1.14.0\r\n> \r\n> It is just a one line patch, you can patch your 1.14.0 like me by yourself\r\n> \r\n> [401bbfc#diff-4ee308ea180d49ae81691348531a2b6d](https://github.com/tensorflow/tensorflow/commit/401bbfc33684c21325d81a03708fe123d59ce527#diff-4ee308ea180d49ae81691348531a2b6d)\r\n\r\nby this it gives error as \r\n\r\n> TypeError: ('Not JSON Serializable:', b'\\n\\rdense/Softmax\\x12\\x07Softmax\\x1a\\rdense/BiasAdd*\\x07\\n\\x01T\\x12\\x020\\x01')"]}, {"number": 27542, "title": "[proposal] Implementing K.rnn(reverse_sequence=True)", "body": "Hi,\r\n\r\nI am a Tensorflow.js user and I made a proposed change to enable RNNs to return their output sequence in the reversed order, which is especially useful if you use go_backwards.\r\n\r\n> https://github.com/tensorflow/tfjs-layers/pull/517\r\n\r\nIt was mentioned there that compatibility with the Python version of Tensorflow was something they cared about, and therefore it was suggested to also create a pull request on the Python side to gather feedback, and see if this implementation is something your team would support.\r\n\r\nI'm not a Python developer but could make the necessary changes I believe, though I'd rather get an approval before going deeper because this might take a fair chunk of my time since this change has to be propagated to the layers etc. Now if in addition to approving the idea someone else also wanted to help me out and advance this pull request, I'd appreciate it of course :)", "comments": ["Hi @FremyCompany, sorry for the late reply. \r\n\r\nThe proposed API can easily be achieved by processing the output tensor with keras.backend.reverse(). I think the currently rnn() API is already having too much knob on it, and adding the \"reverse_sequence\" will further complicate it, especially with the combination of existing go_backwards.", "@qlzh727 Thanks for the reply. No problem with the delay.\r\n\r\nI would agree with you, but doing that has significant performance implications however, while doing the work at the `rnn` level would be free of cost, because it happens before the `concat` which is why I was considering this.", "I see, it does have some performance implication, but I am not sure how significant it is. Adding @fchollet for this since it also affect the keras-team/keras.", "The impact might possibly be larger in tf.js than on the other implementation because there is no built-in reverse layer, and custom functions might be slower there than there are in Python (and they don't serialize) but this could be fixed at the tf.js level by introducing a built-in reverse layer I guess. If this group believes this is a better avenue to fix this, that's probably fine as well, but it would be slower.", "I checked with Francois about this, and we think the proposed API change will be quite confusing, and we prefer the readability over slight performance issue.\r\n\r\nThe issue here is because of the combination of \"reverse_sequence\" and \"go_backwards\". We can't easily set default reverse_sequence to False, since it will modify the existing behavior when go_backwards is True. We could set default to None, and then choose the value based on go_backwards. However, at that stage, the API is bit complicated, and require user to read detailed API docstring to understand the combinations of the params, whereas usually params should be self contained and not affecting each other. Ideally, user should be able to just read the method signature, and understand the behavior of each parameter.\r\n\r\nPerformance wise, we are already doing reverse input and mask when go_backwards is True, we don't foresee any big issue if user has to reverse the output sequence by themselves. If tf.js is implementing same API surface as keras does, it should have the coverage of keras.backend.reverse(), which is also just a simple wrapper around array_ops.reverse().", "Okay, thanks for investigating!"]}, {"number": 27541, "title": "[TF==2.0.0-alpha0] tf.keras.Model reinitializes set weights. ", "body": "### System information\r\n- Have I written custom code: **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**\r\n- TensorFlow installed from (source or binary): **pip install tensorflow==2.0.0-alpha0**\r\n- TensorFlow version (use command below): **2.0.0-alpha0**\r\n- Python version: **3.7.1**\r\n- **Using only CPU**\r\n\r\n### Describe the current behavior\r\nWhen I initialize a convolution layer with some weights, instead of the weights being set, it will be randomly re-initialized. Print output returns at each run a different weight.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow.keras\r\nimport numpy as np\r\n\r\nweight = np.ndarray(\r\n    shape=[1, 1, 1, 1],\r\n    dtype='float32')\r\nweight[0, 0, 0, 0] = -0.8888\r\nbias = np.ndarray(\r\n    shape=[1],\r\n    dtype='float32')\r\nbias[0] = 0\r\n\r\nweights = [weight, bias]\r\n\r\ninputs = tensorflow.keras.layers.Input(shape=(1,1,1), name='inputs')\r\noutputs = tensorflow.keras.layers.Conv2D(filters=1,\r\n                                 kernel_size=1,\r\n                                 strides=(1,1),\r\n                                 kernel_regularizer=tensorflow.keras.regularizers.l2(0.0005),\r\n                                 weights=weights,\r\n                                 use_bias=True,\r\n                                 activation=None,\r\n                                 padding='same')(inputs)\r\nmodel = tensorflow.keras.Model(inputs=inputs,\r\n                              outputs=outputs)\r\n\r\nfor layer in model.layers:\r\n    for weight in layer.get_weights():\r\n        print(weight) # wrong output: [[[[1.6804]]]] [0.]\r\n```\r\n\r\n### Describe the expected behavior\r\nwith:\r\n- keras version 2.1.5 \r\n- using tensorflow backend version 1.6.0\r\n\r\nWhen I initialize a convolution layer with some weights, the weights will be set and stick. Print output returns as expected: [[[[-0.8888]]]] [0.]\r\n\r\nThis is an useful feature, especially when one is converting from one model format to another. For example when converting darknet (model.cfg, model.weights) to keras (model.h5). \r\n\r\n**Code to reproduce**\r\n```\r\nimport keras\r\nimport numpy as np\r\n\r\nweight = np.ndarray(\r\n    shape=[1, 1, 1, 1],\r\n    dtype='float32')\r\nweight[0, 0, 0, 0] = -0.8888\r\nbias = np.ndarray(\r\n    shape=[1],\r\n    dtype='float32')\r\nbias[0] = 0\r\nweights = [weight, bias]\r\ninputs = keras.layers.Input(shape=(1,1,1), name='inputs')\r\noutputs = keras.layers.Conv2D(filters=1,\r\n                                 kernel_size=1,\r\n                                 strides=(1,1),\r\n                                 kernel_regularizer=keras.regularizers.l2(0.0005),\r\n                                 weights=weights,\r\n                                 use_bias=True,\r\n                                 activation=None,\r\n                                 padding='same')(inputs)\r\nmodel = keras.Model(inputs=inputs,\r\n                              outputs=outputs)\r\n\r\nfor layer in model.layers:\r\n    for weight in layer.get_weights():\r\n        print(weight)  # correct output: [[[[-0.8888]]]] [0.]\r\n```\r\n\r\n### Questions:\r\n Is this intended? Or how can this be disabled? How can I help you solve this issue. Thanks for taking your time looking into this. ", "comments": ["@mauricelucy I am closing this as it was resolved in `tf-nightly-gpu-2.0-preview==2.0.0.dev20190723`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/2df53e68f336fa3e02524632e7612bbf/tf_27541.ipynb). Please feel free to open if the issue persisits. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27541\">No</a>\n"]}, {"number": 27540, "title": "Update issue template command to 2.0.0 (still works in 1.3.1)", "body": "still works with 1.13, but the previous version failed on 2.0", "comments": []}, {"number": 27539, "title": "ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank on Colab script downloaded to local machine", "body": "As a companion issue to #27538 with tensorflow 2.0.0a. Script fails with\r\n\r\n> ValueError: Input 0 of layer dense is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.\r\n\r\nin the line\r\n\r\n```  layers.Dense(image_data.num_classes, activation='softmax')```\r\n**System information**\r\n- TensorFlow version:\r\n> ('v1.12.0-9492-g2c319fb415', '2.0.0-alpha0')\r\n\r\nusing an updated script\r\n\r\n```python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\nwhich also works for tensorflow 1.\r\n- Doc Link: https://www.tensorflow.org/tutorials/images/hub_with_keras\r\n\r\n\r\n**Describe the documentation issue**\r\nThe downloaded .py script (see #27538) fails to run (it's not clear-cut if this is a documentation issue or simply a bug) at\r\n```\r\nmodel = tf.keras.Sequential([\r\n  feature_extractor_layer,\r\n  layers.Dense(image_data.num_classes, activation='softmax')\r\n])\r\n```\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\nIf it's a doc issue, I'd love to. It could go deeper than that. The same issue appears for the MCVE at stackoverflow: https://stackoverflow.com/questions/55490885/error-converting-keras-model-to-tfjs-duplicate-weight-name-variable\r\n", "comments": ["@arnoegw solved this on stackoverflow:\r\n\r\n> Does it help to replace tensorflow=2.0.0-alpha0 with tf-nightly-2.0-preview\r\n\r\nSo just `pip install tf-nightly-2.0-preview` and it works.", "> @arnoegw solved this on stackoverflow:\r\n> \r\n> > Does it help to replace tensorflow=2.0.0-alpha0 with tf-nightly-2.0-preview\r\n> \r\n> So just `pip install tf-nightly-2.0-preview` and it works.\r\n\r\nThis doesn't work in Windows 10 as the windows builds are not available.", "Reopened for @abbazs.", "Reopened for @abbazs.", "tf-nightly-2.0-preview works in Linux", "@jasonzhang2022 : yes, like a charm. It also works on Mac OS X. There is no nightly version for windows, though. (this should be closed, as a windows nightly build is arguably a feature request)"]}, {"number": 27538, "title": "AttributeError: 'module' object has no attribute 'KerasLayer' on Colab script when downloaded to local, non-cuda laptop", "body": "The Colab for https://www.tensorflow.org/tutorials/images/hub_with_keras fails to run when downloaded to a local machine.\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.13.1-0-g6612da8951', '1.13.1')\r\n- Python version: 2.7.15rc1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: n.a.\r\n- GPU model and memory:  Mesa DRI Intel(R) HD Graphics 520 (Skylake GT2) \r\n32 GB max as of https://www.intel.com/content/www/us/en/support/products/88355/graphics-drivers/graphics-for-6th-generation-intel-processors/intel-hd-graphics-520.html\r\n\r\n**Describe the current behavior**\r\nFails at line\r\n```\r\n    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))\r\n```\r\nwith\r\n\r\n> AttributeError: 'module' object has no attribute 'KerasLayer'\r\n\r\n**Describe the expected behavior**\r\nScript runs to completion\r\n\r\n**Code to reproduce the issue**\r\n1. Open in Browser: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/hub_with_keras.ipynb\r\n2. export as .py file (File -> Download .py)\r\n3. remove pip commands from file, run tf-hub install by hand, avoid tf-gpu-nightly as no CUDA gpu available\r\n```pip install tensorflow tensorflow-hub```\r\n4. execute script: ```python hub_with_keras.py ```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n> $ python hub_with_keras.py \r\n> WARNING: Logging before flag parsing goes to stderr.\r\n> W0405 12:54:22.202450 140138733332288 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\r\n> Traceback (most recent call last):\r\n>   File \"hub_with_keras.py\", line 28, in <module>\r\n>     hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))\r\n> AttributeError: 'module' object has no attribute 'KerasLayer'\r\n\r\nWhen upgrading to 2.0.0a (1.14 seems to be unavailable on pip) the script fails with another error, see #27539.\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3047549/tf_env.txt)\r\n", "comments": ["As @arnoegw [wrote on stackoverflow](https://stackoverflow.com/questions/55682557/valueerror-input-0-of-layer-dense-is-incompatible-with-the-layer-its-rank-is-u?noredirect=1#comment98130637_55682557), this is fixed in `tf-nightly-2.0-preview`."]}, {"number": 27536, "title": "Unexpected UnicodeDecodeError: invalid start byte when reading lines from a file", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nUnexpected and undocumented runtime exception/error when handling malformed data.\r\n\r\n**Describe the expected behavior**\r\nExpected a \"TypeError\" or an empty list as a result.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```\r\nRun with the path to the attached file as a command line argument.\r\n\r\n**Other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 9, in <module>\r\n    for line in reader:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 220, in \\_\\_next\\_\\_\r\n    return self.next()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 214, in next\r\n    retval = self.readline()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 184, in readline\r\n    return self._prepare_value(self._read_buf.ReadLineAsString())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py\", line 100, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 107, in as_str_any\r\n    return as_str(value)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py\", line 80, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb in position 1: invalid start byte\r\n\r\n\r\n[corrupted_file0.zip](https://github.com/tensorflow/tensorflow/files/3047441/corrupted_file0.zip)\r\n", "comments": ["duplicate #27537 "]}, {"number": 27535, "title": "FileSystem destructor is not being called when done", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **CentOS 7**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **v1.13.1**\r\n- Python version: **Python 3.6.3**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nWhen a FileSystem is registered it is allocated in the heap, but not destroyed on exit when using a Python script.\r\n\r\nThe FileSystem destructor (~FileSystem()) must be called on exit.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n--- /opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/include/tensorflow/core/platform/file_system.h\t2019-04-05 10:05:27.253673315 +0200\r\n+++ /test/tensorflow/tensorflow/core/platform/file_system.h\t2019-04-05 10:22:15.999269084 +0200\r\n@@ -225,7 +225,7 @@\r\n \r\n   FileSystem() { printf(\"create FileSystem\\n\"); }\r\n \r\n-  virtual ~FileSystem();\r\n+  virtual ~FileSystem() { printf(\"destroy FileSystem\\n\"); }\r\n };\r\n```\r\nThen we run python and load our FileSystem plugin:\r\n```\r\n[alexey@workstation test]$ python -c \"import tensorflow as tf; tf.load_library('build/lib/libfilesystem_tensorflow.so')\"\r\ncreate FileSystem\r\n[alexey@workstation test]$\r\n```\r\n**Other info / logs**\r\nIt is necessary for our custom FileSystem plugin to call some functions on destruction. It seems that C-style pointers are used when registering a FileSystem:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.h#L450-L457\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/file_system.h#L336\r\n\r\nThe scheme is actually mapped to a unique pointer to the class:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.cc#L64\r\n\r\nBut I suspect that move could be the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/6612da89516247503f03ef76e974b51a434fb52e/tensorflow/core/platform/env.cc#L114\r\n", "comments": ["(I'm not a good assignee for this issue.)", "@mihaimaruseac recently looked through all of filesystem code.\r\nMaybe he can help.", "@gunan since you've looked at filesystems recently do you know who could look into this issue? A cursory reading of the code shows the right destructors all in the right places and nothing which would be able to prevent the destructors from running.\r\n\r\nThis will be a more serious problem once we make a C API for file systems, I think.", "I assigned it to myself. I hit this same issue while preparing the filesystem API proof of concept so I have some idea about what's happening.\r\n\r\nI'll run some tests tomorrow and update.", "Sorry for the delay. I confirmed that the destructors are not properly called by doing the following changes:\r\n\r\n```patch\r\ndiff --git a/tensorflow/core/platform/env_test.cc b/tensorflow/core/platform/env_test.cc\r\nindex 593727e8..19865268 100644\r\n--- a/tensorflow/core/platform/env_test.cc\r\n+++ b/tensorflow/core/platform/env_test.cc\r\n@@ -284,6 +284,16 @@ TEST_F(DefaultEnvTest, SleepForMicroseconds) {\r\n \r\n class TmpDirFileSystem : public NullFileSystem {\r\n  public:\r\n+  TmpDirFileSystem() {\r\n+    buff = malloc(42);\r\n+    printf(\"TmpDirFileSystem created\\n\");\r\n+  }\r\n+\r\n+  ~TmpDirFileSystem() {\r\n+    free(buff);\r\n+    printf(\"TmpDirFileSystem deleted\\n\");\r\n+  }\r\n+\r\n   Status FileExists(const string& dir) override {\r\n     StringPiece scheme, host, path;\r\n     io::ParseURI(dir, &scheme, &host, &path);\r\n@@ -316,6 +326,7 @@ class TmpDirFileSystem : public NullFileSystem {\r\n \r\n  private:\r\n   bool flushed_ = false;\r\n+  void *buff;\r\n };\r\n \r\n REGISTER_FILE_SYSTEM(\"tmpdirfs\", TmpDirFileSystem);\r\n```\r\n\r\nAnd then `bazel test //tensorflow/core:platform_env_test`. The constructor message is printed, but not the destructor. Curiously, there's no memory leak reported either.\r\n\r\nI'll investigate further.", "Reduced all the code to\r\n\r\n```cpp\r\n#include<memory>\r\n#include<unordered_map>\r\n\r\nclass FileSystem {\r\n public:\r\n  FileSystem() { std::cout << \"FileSystem constructor called\\n\"; }\r\n  ~FileSystem() { std::cout << \"FileSystem destructor called\\n\"; }\r\n};\r\n\r\nclass FileSystemRegistry {\r\n public:\r\n  typedef std::function<FileSystem*()> Factory;\r\n\r\n  FileSystemRegistry() { std::cout << \"FileSystemRegistry constructor called\\n\"; }\r\n  ~FileSystemRegistry() { std::cout << \"FileSystemRegistry destructor called\\n\"; }\r\n\r\n  bool Register(const std::string& scheme, Factory factory) {\r\n    return registry_.emplace(std::string(scheme), std::unique_ptr<FileSystem>(factory())).second;\r\n  }\r\n\r\n private:\r\n  mutable std::unordered_map<std::string, std::unique_ptr<FileSystem>> registry_;\r\n};\r\n\r\nclass Env {\r\n public:\r\n  Env() : file_system_registry_(new FileSystemRegistry) { std::cout << \"Env constructor called\\n\"; }\r\n  ~Env() { std::cout << \"Env destructor called\\n\"; }\r\n\r\n  static Env* Default() {\r\n    static Env* default_env = new Env;\r\n    return default_env;\r\n  }\r\n\r\n  bool RegisterFileSystem(const std::string& scheme, FileSystemRegistry::Factory factory) {\r\n    return file_system_registry_->Register(scheme, std::move(factory));\r\n  }\r\n\r\n private:\r\n  std::unique_ptr<FileSystemRegistry> file_system_registry_;\r\n};\r\n\r\nint main () {\r\n  Env::Default()->RegisterFileSystem(\"x\", []() -> FileSystem* { return new FileSystem; });\r\n  return 0;\r\n}\r\n```\r\n\r\nand of course the destructors don't get called. Valgrind reports still reachable memory bytes as it should.\r\n\r\nI think I have an idea of the root cause of this, will run one more experiment and be back in at most an hour or so.", "Ok, so the `Env` destructor is not called and that causes all the other relevant destructors to not be called either. The `Env` destructor is not called because of the static instance.\r\n\r\nI have a fix incoming, but we also need to consider if we want to call `Env` destructor (as now it is [marked as illegal to delete `Env` instance](https://github.com/tensorflow/tensorflow/blob/17e49b339b2b9a58ed967c69b7acb714dcd9b465/tensorflow/core/platform/posix/env.cc#L57))", "Also, @reflectored, since you're working with filesystems, you might want to also look at [a proposed modularization C API](https://github.com/tensorflow/community/pull/101) for them as feedback is welcomed.", "@mihaimaruseac Yeah I actually stumbled upon that in your profile. I will take a look at that.", "So, there is an issue with forcing `Env` destructor to be called: we will surely get into fiascos based on the order of calling static destructors. Instead, it is better if you use `atexit` to directly call the destructor/resource deinitializer of the filesystem, instead of relying on the destructor chain.", "I think we should close this, as solution and reasoning behind why destructor is not called are given above.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27535\">No</a>\n"]}, {"number": 27534, "title": "What's the license of pre-trained TensorFlow.js models?", "body": "I want to use tfjs-models in a commercial application. While source code in the [tfjs-models repository](https://github.com/tensorflow/tfjs-models) is licensed under Apache 2.0, I did not find any info about the license of the models hosted on storage.googleapis.com.\r\n\r\nFor example [body-pix](https://github.com/tensorflow/tfjs-models/tree/master/body-pix) is loading its model from [storage.googleapis.com/tfjs-models](https://storage.googleapis.com/tfjs-models)\r\ne.g.\r\n[posenet_mobilenet_025_partmap/model.json](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/model.json)\r\n[posenet_mobilenet_025_partmap/tensorflowjs_model.p](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/tensorflowjs_model.pb)\r\n[posenet_mobilenet_025_partmap/weights_manifest.json](https://storage.googleapis.com/tfjs-models/savedmodel/posenet_mobilenet_025_partmap/weights_manifest.json)\r\n\r\nAre the files on [storage.googleapis.com/tfjs-models](https://storage.googleapis.com/tfjs-models) licensed under Apache 2.0 as well?\r\n\r\nIf yes, it would be great to document it, and I'm willing to submit a PR.\r\n\r\n", "comments": ["The [Apache 2.0 license](https://github.com/tensorflow/tfjs-models/blob/master/LICENSE) has been added to the TensorFlow.js models repo. Am closing this issue; thank you for the request!"]}, {"number": 27533, "title": "Fix logdet documentation error", "body": "Example incorrectly refers to tf.logdet rather than tf.linalg.logdet", "comments": ["@ebrevdo Hi, Could you PTAL and approve.", "@ebrevdo Could you PTAL and approve."]}, {"number": 27532, "title": "Custom Object Detection crashes when convert_to_grayscale is enabled.", "body": "**System Information:**\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.2\r\n* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy): NA\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version: 1.12.0\r\n* Python version: 3.6\r\n* Installed using virtualenv? pip? conda?: Conda\r\n* Bazel version (if compiling from source): NA\r\n* GCC/Compiler version (if compiling from source): NA\r\n* CUDA/cuDNN version: NA\r\n* GPU model and memory: NA\r\n\r\n\r\n**Problem Description:**\r\n\r\nEnabling convert_to_grayscale option in image_resizer pipeline.config for FASTER_RCNN Object detection causes failure during export. The training runs without any issue. \r\n\r\n**Error Log:**\r\n\r\n2019-04-05 09:39:01.001689: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2019-04-05 09:39:01.002097: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\r\nTraceback (most recent call last):\r\n  File \"/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/Users/madhukandasamy/miniconda3/envs/cs-object-detection/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: **Assign requires shapes of both tensors to match. lhs shape= [7,7,1,8] rhs shape= [7,7,3,8]**\r\n\t [[{{node save/Assign_10}} = Assign[T=DT_FLOAT, _class=[\"loc:@FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights, save/RestoreV2:10)]]\r\n\r\n**Steps to reproduce:**\r\n\r\nFollow the documented steps to do custom object detection with pretained model faster_rcnn_inception_v2_coco[http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz]  as a starting point. \r\nUse default values wherever applicable in the pipeline config.\r\nEnable convert_to_grayscale option for image_resizer in the pipeline config.\r\n```python   image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 600\r\n        max_dimension: 1024\r\n        convert_to_grayscale: true\r\n      }\r\n```\r\n\r\nRun the training(just one or two steps is sufficient) and export the model as per documentation.Notice the failure during export.\r\n\r\n**Initial Analysis:**\r\n\r\nThe convert_to_grayscale() image resizer modifies the input image from (H, W, 3, 8) => (H, W, 1, 8) during pre-process. This works fine during training and models is being checkpointed without any issue. When we try to freeze the model by reading from the checkpoint, its done as two parts. The graph definition is built using _build_detection_graph() and values are read later from the check-point data. The graph definition is built without considering the changes to the number of channels(it should be taking care of width and height) by the image resizer. This keeps the number of channels un-changed(i.e kept as 3) in defintion, where as the actual check-point data is accounted only for one channel.", "comments": ["Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/27532\">No</a>\n"]}, {"number": 27531, "title": "TF2.0 / `SequenceFeatures` doesn't have a `_is_feature_layer` property.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0-alpha\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nI follow the example [Classify structred data](https://www.tensorflow.org/alpha/tutorials/keras/feature_columns) as below:\r\n```\r\nfeature_layer = SequenceFeatures(columns, name=INPUT_NAME)\r\n\r\nmodel = keras.Sequential([\r\n    feature_layer,\r\n    keras.layers.LSTM(1, return_sequence=True),\r\n    keras.layers.LSTM(1),\r\n    keras.layers.Dense(1, activation='relu', name=OUTPUT_NAME)\r\n])\r\n\r\nmodel.compile(optimizer='adam', loss='mse')\r\nmodel.fit(DATASET, epochs=5)\r\n```\r\nand it shows an error:\r\n`ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.`\r\n\r\nHowever, I found the reason that `SequenceFeatures` doesn't have a property `is_feature_layer` as below:\r\n```\r\n## tensorflow/python/keras/engine/training_utils.py\r\n# TODO(rohanj): This is a hack to get around not depending on feature_column and\r\n# create a cyclical dependency. Figure out a cleaner solution\r\ndef is_feature_layer(layer):\r\n  \"\"\"Returns whether `layer` is a FeatureLayer or not.\"\"\"\r\n  return getattr(layer, '_is_feature_layer', False)\r\n\r\n## tensorflow/python/feature_column/feature_column_v2.py\r\n@keras_export('keras.layers.DenseFeatures')\r\nclass DenseFeatures(_BaseFeaturesLayer):\r\n  def __init__(self, feature_columns, trainable=True, name=None, **kwargs):\r\n\r\n    super(DenseFeatures, self).__init__(\r\n        feature_columns=feature_columns,\r\n        trainable=trainable,\r\n        name=name,\r\n        expected_column_type=DenseColumn,\r\n        **kwargs)\r\n\r\n  @property\r\n  def _is_feature_layer(self):\r\n    return True\r\n   \r\n  def _target_shape(self, input_shape, total_elements):\r\n    return (input_shape[0], total_elements)\r\n    ...\r\n\r\n## tensorflow/python/feature_column/sequence_feature_column.py\r\n@keras_export('keras.experimental.SequenceFeatures')\r\nclass SequenceFeatures(fc._BaseFeaturesLayer):\r\n    def __init__(self, feature_columns, trainable=True, name=None, **kwargs):\r\n\r\n    super(SequenceFeatures, self).__init__(\r\n        feature_columns=feature_columns,\r\n        trainable=trainable,\r\n        name=name,\r\n        expected_column_type=fc.SequenceDenseColumn,\r\n        **kwargs)\r\n\r\n  def _target_shape(self, input_shape, total_elements):\r\n    return (input_shape[0], input_shape[1], total_elements)\r\n   ...\r\n```\r\nI think it would be better to move the `_is_feature_layer` property to the`_BaseFeaturesLayer` for a while.\r\n \r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi seungbaeji, I think sparse tensor input to keras models is not supported yet. This is needed in order for the SequenceFeatures layer to work. I think there is some work in progress to add this support, so that can be expected soon. Once this is done, yes adding the `_is_feature_layer` property as you suggested should allow to use SequenceFeatures with such models.", "There might be a work around if you use categorical columns:\r\nI think that sequence categorical columns can handle dense tensors, but you have to be cautious about the way the sequence length is handled in that case.\r\nAnd with a subclassed keras model, you can use the SequenceFeatures layer without running into the `_is_feature_layer ` error that you described.\r\nThat should not work with `sequence_numeric_column` though.", "@seungbaeji,\r\nThe Tutorial, [Classify structured data with feature columns](https://www.tensorflow.org/tutorials/structured_data/feature_columns) can be executed without any error. Please find [the Gist](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/feature_columns.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 27530, "title": "Duplicate layer name when using a + b and Add()([c, d]) in the same Keras model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\ntf.version.VERSION=2.0.0-dev20190404\r\ntf.version.GIT_VERSION=v1.12.0-11729-g98c3cfbf74\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nWhen using `a + b` then `keras.layers.Add` in the same Keras model, I get an exception saying the name `\"add\"` was used twice. The problem goes away if I use `keras.layers.Add` first, or if I use twice the same operation (either `+` twice, or `keras.layers.Add` twice).\r\n\r\n**Describe the expected behavior**\r\nI don't expect any name conflicts.\r\n\r\n**Code to reproduce the issue**\r\nThe following code raises a `ValueError` exception (full stacktrace below).\r\n\r\n```python\r\nfrom tensorflow import keras\r\n\r\ninputs = keras.layers.Input(shape=[2])\r\nadd1 = inputs + inputs\r\nadd2 = keras.layers.Add()([inputs, inputs])\r\nmodel = keras.models.Model(inputs=[inputs], outputs=[add1, add2])\r\n```\r\n\r\n**Other info / logs**\r\nHere is the full stacktrace:\r\n\r\n```python\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-07b5faf0201d> in <module>\r\n      4 add1 = inputs + inputs\r\n      5 add2 = keras.layers.Add()([inputs, inputs])\r\n----> 6 model = keras.models.Model(inputs=[inputs], outputs=[add1, add2])\r\n\r\n.../tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)\r\n    121\r\n    122   def __init__(self, *args, **kwargs):\r\n--> 123     super(Model, self).__init__(*args, **kwargs)\r\n    124     # initializing _distribution_strategy here since it is possible to call\r\n    125     # predict on a model without compiling it.\r\n\r\n.../tensorflow/python/keras/engine/network.py in __init__(self, *args, **kwargs)\r\n    137         'inputs' in kwargs and 'outputs' in kwargs):\r\n    138       # Graph network\r\n--> 139       self._init_graph_network(*args, **kwargs)\r\n    140     else:\r\n    141       # Subclassed network\r\n\r\n.../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    456     self._setattr_tracking = False  # pylint: disable=protected-access\r\n    457     try:\r\n--> 458       result = method(self, *args, **kwargs)\r\n    459     finally:\r\n    460       self._setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n.../tensorflow/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name)\r\n    284     # Keep track of the network's nodes and layers.\r\n    285     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\r\n--> 286         self.inputs, self.outputs)\r\n    287     self._network_nodes = nodes\r\n    288     self._nodes_by_depth = nodes_by_depth\r\n\r\n.../tensorflow/python/keras/engine/network.py in _map_graph_network(inputs, outputs)\r\n   1916     if all_names.count(name) != 1:\r\n   1917       raise ValueError('The name \"' + name + '\" is used ' +\r\n-> 1918                        str(all_names.count(name)) + ' times in the model. '\r\n   1919                        'All layer names should be unique.')\r\n   1920   return network_nodes, nodes_by_depth, layers, layers_by_depth\r\n\r\nValueError: The name \"add\" is used 2 times in the model. All layer names should be unique.\r\n```", "comments": ["I am able to repro the issue, will look into the fix.", "@ageron can you verify if this is fixed? https://github.com/tensorflow/tensorflow/commit/0a9f68955ec7916f6f57916ccd60cd5e9c93d901#diff-8eb7e20502209f082d0cb15119a50413", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27530\">No</a>\n", "Yes, problem fixed, thank you very much!"]}, {"number": 27529, "title": "LIte: Depthwise conv refatcor", "body": "1:> Unnecessary Template usage removed.\r\n2:> Error message updated.", "comments": []}, {"number": 27528, "title": "add/update comments for word2vec_basic.py", "body": "refine comments and provide further references for curious reader", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27528) for more info**.\n\n<!-- need_sender_cla -->", "@Albert-Z-Guo  please sign CLA", "OK. I have signed the CLA.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F27528) for more info**.\n\n<!-- ok -->", "@aselle, @drpngx  Could you PTAL and approve."]}, {"number": 27527, "title": "Tensorflow C++ API on Windows/VS2015: unresolved external symbol", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.2\r\n- GPU model and memory: Nvidia GTX 1070\r\n\r\n**Describe the problem**\r\n\r\nI successfully built Tensorflow using Bazel from source.\r\n\r\nI obtained the following files.\r\n\r\n- bazel-bin\\tensorflow\\libtensorflow_cc.dll (renamed from libtensorflow_cc.so)\r\n- bazel-bin\\tensorflow\\libtensorflow_cc.lib (renamed from liblibtensorflow_cc.so.ifso)\r\n- bazel-bin\\tensorflow\\libtensorflow_framework.dll (renamed from libtensorflow_framework.so)\r\n- bazel-bin\\tensorflow\\libtensorflow_framework.lib (renamed from liblibtensorflow_framework.so.ifso)\r\n\r\nIn Visual Studio 2015\r\n\r\n- I included headers from bazel-genfiles (include/, third_pary/, com_google_absl, com_googlesource_code_re2, protobuf_archive\\src, etc)\r\n- I also configured Linked to link with .lib files.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nIn the code, I simply declare the following param:\r\nauto root = tensorflow::Scope::NewRootScope();\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nSeverity\tCode\tDescription\tProject\tFile\tLine\tSource\tSuppression State\r\nError\tLNK1120\t3 unresolved externals\tProgram\tD:\\Project_Tensorflow\\x64\\Release\\Program.exe\t1\tBuild\t\r\nError\tLNK2011\tprecompiled object not linked in; image may not run\tProgram\tD:\\Project_Tensorflow\\Program\\TestTensorflow.obj\t1\tBuild\t\r\nError\tLNK2001\tunresolved external symbol \"public: static class tensorflow::Scope __cdecl tensorflow::Scope::NewRootScope(void)\" (?NewRootScope@Scope@tensorflow@@SA?AV12@XZ)\tProgram\tD:\\Project_Tensorflow\\Program\\utils.obj\t1\tBuild\t\r\nError\tLNK2001\tunresolved external symbol \"public: __cdecl tensorflow::Scope::~Scope(void)\" (??1Scope@tensorflow@@QEAA@XZ)\tProgram\tD:\\Project_Tensorflow\\Program\\utils.obj\t1\tBuild\t\r\n\r\nPlease help if you have an idea. Thank you very much.\r\n", "comments": ["According to this guide (https://joe-antognini.github.io/machine-learning/windows-tf-project), \u201cunresolved external symbol\u201d errors occur if you don't include sufficient Additional Dependencies.\r\n\r\nHowever, bazel generates only libtensorflow_cc.lib and libtensorflow_framework.lib, and I already included them both. \r\n\r\nPlease give me a hand. Thanks.\r\n\r\n@fo40225 @HackersSpirit @MetaPeak @lxgyChen I read your discussion on https://github.com/fo40225/tensorflow-windows-wheel/issues/30# \r\n\r\nDo you guys have any ideas related to this?", "\u5144\u5f1f\uff0c\u89e3\u51b3\u4e86\u5417\uff1f\u89e3\u51b3\u4e86\u544a\u8bc9\u6211\u8bf4\u4e00\u58f0\u554a = =\uff0c\u6211\u4e5f\u9047\u5230\u540c\u6837\u7684\u95ee\u9898\u4e86\u3002\u3002", "@alpha-gradient did you solve the problem ?", "Please check out a tutorial I wrote concerning this subject here: https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows\r\n\r\nIts starts at the very beginning of the process of building and using the tensorflow C++ API on Windows, so you may want to skip the initial setup steps and go the part labelled Identify Missing Symbols. Also, I have only tested 1.14, please let me know if you get an implementation working for another version. \r\n\r\nFeel free to ask me questions by responding to this comment or emailing ashley.tharp@gmail.com\r\n\r\n:)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 27526, "title": "[Deleted]", "body": "Deleted. I just realized I made a mistake.", "comments": []}]