[{"number": 7723, "title": "arm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)", "body": "I'm trying to create a Docker image which has all the dependencies required to build the TensorFlow Android demo app from source (including the Android SDK, NDK and build tools).  \r\n\r\nI'm simply starting with `gcr.io/tensorflow/tensorflow:latest-devel` and then adding the Android dependencies on top.  This lists all the steps I've done so far:\r\nhttps://medium.com/@daj/docker-image-for-building-tensorflow-android-demo-app-97c98ce37d9e#.y0kwcx63j\r\n\r\nUnfortunately I keep hitting this error:\r\n```\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 71 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThese GitHub issues had similar titles, but the details looked a bit different:\r\nhttps://github.com/tensorflow/tensorflow/issues?q=is%3Aissue+BadExitStatusException+Process+exited+with+status+4++cc1plus+is%3Aopen\r\n\r\n### Environment info\r\nOperating System: Docker version 1.13.1, build 092cba3 (running inside Mac OS X, I tried on both El Capitan and Sierra with the same results)\r\n\r\n```\r\n# git rev-parse HEAD\r\n07bb8ea2379bd459832b23951fb20ec47f3fdbd4\r\n\r\n# bazel version\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI've pushed my Docker image (after hitting the build error) to a public location, so you should be able to pull it and then run the build:\r\n```\r\ndocker pull danjarvis/tensorflow-android:1.0.0\r\ndocker run -it danjarvis/tensorflow-android:1.0.0\r\ncd /tensorflow\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n - I tried on two different laptops.  \r\n - I built my Docker image again following [my own instructions](https://medium.com/@daj/docker-image-for-building-tensorflow-android-demo-app-97c98ce37d9e#.niaa80w9p) and saw the same error.\r\n\r\n### Logs or other output that would be helpful\r\n\r\nNormal build (without the `--verbose_failures` option - scroll down for the `--verbose_failures`, I've included that too):\r\n```\r\n# bazel build -c opt //tensorflow/examples/android:tensorflow_demo\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/src/google/protobuf/io/coded_stream.cc:\r\nexternal/protobuf/src/google/protobuf/io/coded_stream.cc: In member function 'google::protobuf::int64 google::protobuf::io::CodedInputStream::ReadVarint32Fallback(google::protobuf::uint32)':\r\nexternal/protobuf/src/google/protobuf/io/coded_stream.cc:445:12: warning: 'temp' may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     return temp;\r\n            ^\r\nINFO: From Processing Android resources for //tensorflow/examples/android:tensorflow_demo:\r\nFeb 21, 2017 2:50:45 AM com.google.devtools.build.android.AndroidDataMerger doMerge\r\nWARNING: \r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/mobile_multibox/WORKSPACE\r\n\texternal/inception5h/WORKSPACE\r\nCONFLICT: asset:WORKSPACE is provided with ambiguous priority from:\r\n\texternal/stylize/WORKSPACE\r\n\texternal/mobile_multibox/WORKSPACE\r\nINFO: From ProtoCompile tensorflow/examples/android/proto/box_coder.pb.cc:\r\nbazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\nbazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc [for host]:\r\nbazel-out/host/genfiles/external/protobuf/src: warning: directory does not exist.\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 71 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://source.android.com/source/report-bugs.html> for instructions.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1527.289s, Critical Path: 1501.49s\r\n```\r\n\r\nHere's the result with the `--verbose_failures` option set:\r\n```\r\n# bazel --verbose_failures build -c opt //tensorflow/examples/android:tensorflow_demo\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nUnknown Bazel startup option: '--verbose_failures'.\r\n  For more info, run 'Bazel help startup_options'.\r\nroot@a80d6a5002cd:/tensorflow# bazel build -c opt --verbose_failures //tensorflow/examples/android:tensorflow_demo\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/core/kernels/split_op.cc:\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = Eigen::QUInt8]':\r\ntensorflow/core/kernels/split_op.cc:357:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n         prefix_dim_size, split_dim_output_size, suffix_dim_size};\r\n                                                                ^\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = float]':\r\ntensorflow/core/kernels/split_op.cc:357:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = int]':\r\ntensorflow/core/kernels/split_op.cc:357:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:159:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: arm-linux-androideabi-gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.d '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles -iquote external/protobuf -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/gemmlowp -isystem external/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/cwise_op_mul_2.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_mul_2.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\r\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://source.android.com/source/report-bugs.html> for instructions.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nINFO: Elapsed time: 160.782s, Critical Path: 138.06s\r\n```", "comments": ["This seems similar to https://github.com/tensorflow/serving/issues/97, probably due to some interaction with Docker.\r\n\r\nCan you try adding `--spawn_strategy=sandboxed --genrule_strategy=sandboxed` to the build flags to see if that helps?", "Also, are both of the laptops you tried this on running osx?", "They are both OS X, one is El Capitan, one is Sierra (I updated the description to record the details).\r\n\r\nIf you can point me at Docker container which supports building the TensorFlow Android demo, then I can try using that. I couldn't find one when I searched online.\r\n\r\nI added the `--spawn_strategy=sandboxed --genrule_strategy=sandboxed` flags, here's the result:\r\n```\r\n# bazel build -c opt --verbose_failures --spawn_strategy=sandboxed --genrule_strategy=sandboxed //tensorflow/examples/android:tensorflow_demo\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nERROR: /tensorflow/tensorflow/core/kernels/BUILD:3749:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed: process-wrapper failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/bazel-sandbox/383bb6e0-70e1-4f65-b0b0-feb77e82e2c2-6/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n  /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/_bin/process-wrapper -1 5 - - external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater.d '-frandom-seed=bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater.o' -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles -iquote external/protobuf -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/gemmlowp -isystem external/protobuf/src -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-mfpu=neon' '-std=c++11' -DTF_LEAN_BINARY -O2 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm' -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include -isystem external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/include-fixed -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c tensorflow/core/kernels/cwise_op_greater.cc -o bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/_objs/android_tensorflow_kernels/tensorflow/core/kernels/cwise_op_greater.o).\r\narm-linux-androideabi-gcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://source.android.com/source/report-bugs.html> for instructions.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nINFO: Elapsed time: 159.378s, Critical Path: 134.32s\r\n```", "Can you ensure that you have enough memory available in your docker environment? This seems like a common issue: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/zjwDXhD6k1w", "Awesome, adding `--local_resources 4096,4.0,1.0 -j 1` resolved the issue - thanks!\r\n\r\nFull output:\r\n```\r\n# bazel build -c opt --verbose_failures --local_resources 4096,4.0,1.0 -j 1 //tensorflow/examples/android:tensorflow_demo\r\nINFO: Reading 'startup' options from /root/.bazelrc: --batch\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /tensorflow/tensorflow/core/BUILD:816:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nINFO: From Compiling tensorflow/core/kernels/unpack_op.cc:\r\ntensorflow/core/kernels/unpack_op.cc: In instantiation of 'void tensorflow::UnpackOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\r\ntensorflow/core/kernels/unpack_op.cc:177:1:   required from here\r\ntensorflow/core/kernels/unpack_op.cc:113:75: warning: narrowing conversion of 'before_dim' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n         Eigen::DSizes<Eigen::DenseIndex, 3> sizes{1, before_dim, after_dim};\r\n                                                                           ^\r\ntensorflow/core/kernels/unpack_op.cc: In instantiation of 'void tensorflow::UnpackOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = int]':\r\ntensorflow/core/kernels/unpack_op.cc:177:1:   required from here\r\ntensorflow/core/kernels/unpack_op.cc:113:75: warning: narrowing conversion of 'before_dim' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nINFO: From Compiling tensorflow/core/kernels/split_v_op.cc:\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = tensorflow::bfloat16; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:401:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n           prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                                                               ^\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = float; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:401:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = int; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:401:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nINFO: From Compiling tensorflow/core/kernels/tensor_array_ops.cc:\r\ntensorflow/core/kernels/tensor_array_ops.cc: In instantiation of 'void tensorflow::TensorArraySplitOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\r\ntensorflow/core/kernels/tensor_array_ops.cc:1400:1:   required from here\r\ntensorflow/core/kernels/tensor_array_ops.cc:1242:72: warning: narrowing conversion of 'previous_length' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n       Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, previous_length, 0};\r\n                                                                        ^\r\ntensorflow/core/kernels/tensor_array_ops.cc:1244:65: warning: narrowing conversion of '(Scalar)tensor_lengths_t.Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::operator()<Eigen::Tensor<const long long int, 1, 1, int>, 16, Eigen::MakePointer>(i)' from 'Scalar {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n                                                 elements_per_row};\r\n                                                                 ^\r\ntensorflow/core/kernels/tensor_array_ops.cc: In instantiation of 'void tensorflow::TensorArraySplitOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = int]':\r\ntensorflow/core/kernels/tensor_array_ops.cc:1400:1:   required from here\r\ntensorflow/core/kernels/tensor_array_ops.cc:1242:72: warning: narrowing conversion of 'previous_length' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n       Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, previous_length, 0};\r\n                                                                        ^\r\ntensorflow/core/kernels/tensor_array_ops.cc:1244:65: warning: narrowing conversion of '(Scalar)tensor_lengths_t.Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::operator()<Eigen::Tensor<const long long int, 1, 1, int>, 16, Eigen::MakePointer>(i)' from 'Scalar {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n                                                 elements_per_row};\r\n                                                                 ^\r\nSlow read: a 756175936-byte read from /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo took 5357ms.\r\nTarget //tensorflow/examples/android:tensorflow_demo up-to-date:\r\n  bazel-bin/tensorflow/examples/android/tensorflow_demo_deploy.jar\r\n  bazel-bin/tensorflow/examples/android/tensorflow_demo_unsigned.apk\r\n  bazel-bin/tensorflow/examples/android/tensorflow_demo.apk\r\nINFO: Elapsed time: 945.344s, Critical Path: 66.98s\r\n```"]}, {"number": 7722, "title": "R1.0", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Looks like misoperation. Closing this PR."]}, {"number": 7721, "title": "Remove outdated link to nightly-matrix-android builds", "body": "This has already been replaced with the nightly-android links.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@rbspark The commit you're referencing here was already merged into master, so I think this will have no effect? (Just trying to make sure I understand what the intent here is)"]}, {"number": 7720, "title": "Feature: Hide Metrics for Which Data Exists for Zero Runs", "body": "The list of metrics for for plotting on the scalars tab is cluttered with a number of metrics that don't exist for the selected runs. Why not side these rather than taking up real estate:\r\n![image](https://cloud.githubusercontent.com/assets/51059/23149169/2c980ac0-f7b8-11e6-8493-b4085e089be1.png)\r\n", "comments": ["This is[ a related SO post](http://stackoverflow.com/questions/42357850/tensorboard-scalars-count): but the count on the right side shows 1 even if there is no data:\r\n![image](https://cloud.githubusercontent.com/assets/51059/23184161/b1da1342-f84c-11e6-993d-7a61e4868a61.png)\r\n", "Agreed; I was just thinking about doing this yesterday.\r\n\r\nThe count at right is correct; it indicates how many charts there are. There is indeed one chart\u2026it's just useless. :-)\r\n\r\nI've migrated this to our new repository at https://github.com/tensorflow/tensorboard/issues/55. Feel free to continue discussing there."]}, {"number": 7719, "title": "Feature request: convenience functions for getting variables from scopes", "body": "There does not currently seem to be a good way to get a collection of variables that belong to a given scope. I've seen issue #7295, but the answer given by Yaroslav (at http://stackoverflow.com/questions/42073239/tf-get-collection-to-extract-variables-of-one-scope) does not nest:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef bar():\r\n  with tf.variable_scope(\"bar\") as scope:\r\n    x = tf.Variable(0, name=\"x\")\r\n    print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)))\r\n    print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name + \"/\")))\r\n  print list(map(str, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"bar\")))\r\n\r\ndef foo():\r\n  with tf.variable_scope(\"foo\") as scope:\r\n    with tf.variable_scope(\"bar_\") as scope:\r\n      y = tf.Variable(0, name=\"y\")\r\n    bar()\r\n\r\nwith tf.Graph().as_default():\r\n  bar()\r\nprint\r\nwith tf.Graph().as_default():\r\n  foo()\r\n```\r\n\r\nThe snippet above outputs (on TF 1.0):\r\n```\r\n['Tensor(\"bar/x/read:0\", shape=(), dtype=int32)']\r\n['Tensor(\"bar/x/read:0\", shape=(), dtype=int32)']\r\n['Tensor(\"bar/x/read:0\", shape=(), dtype=int32)']\r\n\r\n['Tensor(\"foo/bar_/y/read:0\", shape=(), dtype=int32)', 'Tensor(\"foo/bar/x/read:0\", shape=(), dtype=int32)']\r\n['Tensor(\"foo/bar/x/read:0\", shape=(), dtype=int32)']\r\n[]\r\n```\r\n\r\nThe output shows that wrapping the scope \"bar\" in another scope (\"foo\") breaks Yaroslav's proposal of using `tf.get_collection(..., \"bar\")`. It also breaks `tf.get_collection(..., scope.name)` by including variables from outside the scope. The only version that works regardless of context is `tf.get_collection(..., scope.name + \"/\")`, which is ugly and too informed.\r\n\r\nIt would be great to have `scope.trainable_variables()`, or `scope.get_collection(...)`, or even `tf.get_collection(..., scope)`.", "comments": ["@cooijmanstim the function `tf.get_collection(key, scope)` does a prefix match in all variables belonging to the `key` collection. The Variable `foo/bar_/y/read:0` and the scope `foo/bar` have same prefix.", "@suiyuan2009 Indeed I set it up that way, and I would argue that it's an implementation detail leaking through. User code shouldn't have to muck with strings (i.e. `+ \"/\"`) when they have a reference to the Actual VariableScope object to which they want to constrain their selection.", "@cooijmanstim we can modify the function `def get_collection(self, name, scope=None)` and check `scope`'s type. I'll submit a pr.", "I think add a function `scope.get_collection(key)` is better, since It's not a good idea to import ops lib in framework lib.", "Looks like someone fixed this but didn't close the bug."]}, {"number": 7718, "title": "Uncaught TypeError: Cannot read property 'url' of undefined", "body": "Shown in the web console for a tensorboard instance:\r\n```\r\nUncaught TypeError: Cannot read property 'url' of undefined\r\n    at HTMLElement.redraw (tf-tensorboard.html:5739)\r\n    at HTMLElement._toggleExpanded (tf-tensorboard.html:1995)\r\n    at handler (polymer.html:561)\r\n    at HTMLElement.decorated (polymer.html:4462)\r\n    at HTMLElement.fire (polymer.html:1327)\r\n    at Object.fire (polymer.html:899)\r\n    at Object.forward (polymer.html:1196)\r\n    at Object.click (polymer.html:1181)\r\n    at HTMLElement.handleNative (polymer.html:789)\r\n```\r\nI am using the 1.0.0 Docker image.", "comments": ["1. Can you give us a repro?\r\n2. Does this correspond to any visible defect in TensorBoard? (Does it die entirely?)", "2. Yes, it corresponds to the run filter box ceasing to work.\r\n![image](https://cloud.githubusercontent.com/assets/51059/23631666/2773bb4a-028d-11e7-83b2-23a758d618dc.png)\r\n", "Can you please provide a reproduction of the issue? Does this happen any time you use the run filter box?", "Thanks for reporting this. Given that it's a few months old and there's no repro, I'm closing it for now. Please feel free to reopen it in our new repository at https://github.com/tensorflow/tensorboard/issues."]}, {"number": 7717, "title": "Segmentation fault with TensorFlow 1.0", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n* No\r\n\r\n### Environment info\r\nOperating System:\r\n* Ubuntu 14.04/3.13.0-100-generic\r\n* CentOS 7/3.10.0-123.el7.x86_64\r\n\r\nPython Version:\r\n* 2.7.5\r\n* 3.5\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n * Both CPU/GPU version crash, not related to CUDA\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n  * https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl\r\n  * https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl\r\n  * https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n  * 1.0.0\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI can make repro with ubuntu 14.04 + python 2.7, 3.5 and centos 7 + python 2.7, but not with ubuntu 16.04 + python 2.7:\r\nhttps://gist.github.com/llhe/6d95d2e31ad3c5a886dc8c3bd6ace95b\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nTypical outputs:\r\n```\r\nExtracting /tmp/data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nTraceback (most recent call last):\r\n  File \"coredump.py\", line 18, in <module>\r\n    train()\r\n  File \"coredump.py\", line 15, in train\r\n    tf.image_summary('input', x, 10)\r\nAttributeError: 'module' object has no attribute 'image_summary'\r\nSegmentation fault (core dumped)\r\n```\r\nStack trace:\r\n```\r\n(gdb) bt\r\n#0  visit_decref (op=<unknown at remote 0x-1>, data=0x0) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:429\r\n#1  0x00007f610674d8de in dict_traverse (op=\r\n    {0x0: <unknown at remote 0x-1>, '__ne__': <wrapper_descriptor at remote 0x1f7d730>, '__ror__': <wrapper_descriptor at remote 0x1f7da00>, '__nonzero__': <wrapper_descriptor at remote 0x1f7d820>, '__new__': <built-in method __new__ of type object at remote 0x7f60fde34f60>, '__rand__': <wrapper_descriptor at remote 0x1f7d8c0>, '__doc__': None, '__xor__': <wrapper_descriptor at remote 0x1f7d910>, '__and__': <wrapper_descriptor at remote 0x1f7d870>, '__le__': <wrapper_descriptor at remote 0x1f7d690>, '__or__': <wrapper_descriptor at remote 0x1f7d9b0>, '__gt__': <wrapper_descriptor at remote 0x1f7d780>, '__hash__': <wrapper_descriptor at remote 0x1f7d5f0>, '__index__': <wrapper_descriptor at remote 0x1f7da50>, '__lt__': <wrapper_descriptor at remote 0x1f7d640>, '__eq__': <wrapper_descriptor at remote 0x1f7d6e0>, '__rxor__': <wrapper_descriptor at remote 0x1f7d960>, '__ge__': <wrapper_descriptor at remote 0x1f7d7d0>}, visit=0x7f61067db1f0 <visit_decref>, arg=0x0)\r\n    at /usr/src/debug/Python-2.7.5/Objects/dictobject.c:2123\r\n#2  0x00007f61067db5b7 in subtract_refs (containers=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:456\r\n#3  collect (generation=generation@entry=2) at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:999\r\n#4  0x00007f61067dc078 in PyGC_Collect () at /usr/src/debug/Python-2.7.5/Modules/gcmodule.c:1514\r\n#5  0x00007f61067c9339 in Py_Finalize () at /usr/src/debug/Python-2.7.5/Python/pythonrun.c:444\r\n#6  0x00007f61067da545 in Py_Main (argc=<optimized out>, argv=<optimized out>) at /usr/src/debug/Python-2.7.5/Modules/main.c:665\r\n#7  0x00007f6105a07af5 in __libc_start_main () from /usr/lib64/libc.so.6\r\n#8  0x0000000000400721 in _start ()\r\n```", "comments": ["Dupe of: https://github.com/tensorflow/tensorflow/issues/6968 ?", "Thanks @cancan101 , the issue can be resolved with:\r\n```\r\npip install --no-binary=:all: numpy\r\n```"]}, {"number": 7716, "title": "Upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl on Windows and get warning", "body": "I upgrade Tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl, CPU version on Windows server 2012 R2\r\n\r\npip install --upgrade   \r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=cpu,OS=windows/artifact\r\n/cmake_build/tf_python/dist/tensorflow-1.0.0rc2-cp35-cp35m-win_amd64.whl\r\n\r\nIt shows \r\n\"Successfully installed tensorflow-1.0.0rc2 werkzeug-0.11.15\"\r\n\r\nBut I get the following warning when I run the hello example, please advise what's the problem.\r\n\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow')\r\n>>> sess = tf.Session()\r\n2017-02-20 16:53:15.825086: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n2017-02-20 16:53:15.826037: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE2 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-02-20 16:53:15.827289: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE3 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-02-20 16:53:15.828478: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE4.1 instructions, but these are available on you\r\nr machine and could speed up CPU computations.\r\n2017-02-20 16:53:15.829644: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use SSE4.2 instructions, but these are available on you\r\nr machine and could speed up CPU computations.\r\n2017-02-20 16:53:15.830997: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use AVX instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n2017-02-20 16:53:15.832161: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use AVX2 instructions, but these are available on your\r\nmachine and could speed up CPU computations.\r\n2017-02-20 16:53:15.833336: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cp\r\nu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow li\r\nbrary wasn't compiled to use FMA instructions, but these are available on your m\r\nachine and could speed up CPU computations.\r\n>>> print(sess.run(hello))\r\nb'Hello, TensorFlow'", "comments": ["I'm not very knowledgeable on installation, but I believe that a pip install uses a pre-compiled binary rather than building one specific to your machine characteristics.  The warning messages you see just warn that the binary is not fully optimized for the instructions available.  To build a better binary you may need to invoke bazel.  Perhaps this stackoverflow thread may help you:  \r\nhttp://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions", "Just adding on top of what @poxvoculi already said, they say if you build TensorFlow from source it runs faster on your machine when enabling those instructions. I think to extend compatibility to most CPUs as possible it is not enabled by default. Nothing really to worry though.", "What @poxvoculi and @Carmezim stated is correct.\r\nWe build and distribute binaries that can run on all/most available hardware.\r\nHowever, if you have a newer CPU, you can build TF from sources.\r\n\r\nClosing the issue as Working as Intended.", "I installed Tensorflow on Windows10 and when testing in Python I get the following error:\r\n\r\n`C:\\Python35\\python.exe C:/Users/loc/PycharmProjects/Practice/tensorflow_test.py\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\nE c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\framework\\op_kernel.cc:943] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\nb'Hello, TensorFlow!'`\r\n\r\nCan anyone let me know what else should I do to resolve this issue?\r\n\r\nI use the sample hello world test code:\r\n\r\n`import tensorflow as tf \r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))`", "@davidbrai download the respective wheel from https://pypi.python.org/pypi/tensorflow after uninstalling TF and install it again. This error is harmless and resolved on 1.1.0r.", "I updated my TF to tensorflow-1.1.0rc2 version. Now I am getting the following warnings:\r\n\r\n`2017-04-19 16:35:22.533979: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.534756: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.535027: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.535245: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.535462: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.535680: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.536664: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-04-19 16:35:22.536925: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nb'Hello, TensorFlow!'`\r\n\r\nAny idea how to solve it?", "That is intended behavior.\r\nhttp://stackoverflow.com/questions/43335531/how-to-use-sse4-1-instructions-without-install-tensorflow-from-source/43335630#43335630", "@Carmezim Thanks a lot! That helped to silence the warnings."]}, {"number": 7715, "title": "Feature: nadam optimizer ", "body": "Adam with nesterov momentum. \r\n\r\nKeras has it: https://keras.io/optimizers/#nadam\r\nTF implementation (old and doesn't quite work) : https://github.com/tdozat/Optimization/blob/master/tensorflow/nadam.py", "comments": ["I'm thinking about the same approach like `MomentumOptimizer` with a `use_nesterov` flag. But I don't know if it is still preferable to change the interface now. ", "Is there a reason this shouldn't simply be a community supplied option, like the cited version?   Are you suggesting a specific modification to TF?  Otherwise perhaps it would be better to discuss such an extension on stack overflow.", "@poxvoculi Do you think it makes sense to add a `use_nesterov` flag into Adam (in the tf repos)?", "@ebrevdo: who would have an informed opinion on this?", "@poxvoculi A C++ implementation would be faster than python hacks like above. Also, maybe an awaiting googler label?", "Paul: probably vincent would know how popular / useful this optimizer is\nfor the general case.\n\nOn Wed, Feb 22, 2017 at 11:53 AM, Aaron Hu <notifications@github.com> wrote:\n\n> @poxvoculi <https://github.com/poxvoculi> A C++ implementation would be\n> faster than python hacks like above. Also, maybe an awaiting googler label?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7715#issuecomment-281783349>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1Y1n3pYI1s1-68zine8Pr6ees1Jks5rfJJHgaJpZM4MGufx>\n> .\n>\n", "Plausible. I have no experience with it. If someone implements it, it sounds like something we would incorporate.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7714, "title": "[Distributed Training] Model parallelism support", "body": "Hi,\r\nLooking in the [TF distributed training tutorial](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/) it provides distributed training via data parallelism example only. Does TensorFlow currently support model parallelism? If yes where are docs/examples for that?\r\n\r\nThanks,\r\nOgail", "comments": ["Here's a simple example of model parallelism -- http://stackoverflow.com/questions/42069147/implementation-of-model-parallelism-in-tensorflow .  You may get more examples on stackoverflow, closing this as we are trying to keep this list for bugs in TensorFlow itself"]}, {"number": 7713, "title": "Attempt to \"import tensorflow as tf\" on MacOS 10.10.5, Python 2.7.10, numpy 1.12.0", "body": " - Installed tensorflow using pip on MacOS Yosemite 10.10.5, running Python 2.7.10, with current numpy 1.12.0.  Python runs ok, but attempt to \"import tensorflow as tf\", as per the documentation for TensorFlow install on Mac OS, generates exactly the same error sequence as posted already on github by another user.  The other users posting was closed without comment.  The specific error message is:\r\n      AttributeError: type object 'NewBase' has no attribute 'is_abstract'\r\nThe url of the other user, which documented the traceback (which is essentially same as I am getting) is:\r\n     https://github.com/tensorflow/tensorflow/issues/5707\r\n\r\nIf I can resolve or workaround this, I will post the fix here.   ", "comments": ["Ok, got the answer.  The location of the installed python packages is different on the MacOS than on Linux, but the info provided for Linux users on other posts, solves the problem.  If running TensorFlow, not only is the \"numpy\" version stuff critical (ie. looks like you need the multi-array features, which aren't in lower versions?), but package \"six\" is needed, and has to be above version 1.10.  Even though I explicitly installed \"six\" version 1.10, when I would run Python, I was getting \"six\" version 1.4.1.  There was a default Python 2.6 and 2.7 installed on my Yosemite MacOS, which puts the numpy and six packages in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python.  But when pip and easy_install are used to install new packages, they must get put somewhere else.  As a kludge to get TensorFlow working, I just renamed the numpy and six stuff in the  ..../Frameworks directory (the one indicated above) to numpy_old and six_old.  But that didn't quite work, since the \"six\" program had a .pyc as well as a .py file, which I have learned is compiled bytecode.  You have to rename six.pyc to something different also.  Then, Python and the TensorFlow stuff will load ok.  You can check the versions of the python modules with:  \r\n      import modulename \r\n      print modulename.__version__\r\nI would use pip and/or easy_install to get the latest modules, but my Python 2.7 was still grabbing the old versions from the ..../Frameworks/...  directory.   Renaming the prgms there allowed the newer versions to be loaded via the \"import tensorflow as tf\" command.\r\nI have just run the 'Hello TensorFlow!' test program from the tutorial below, and confirmed it worked. (URL is below)\r\n   https://www.tensorflow.org/install/install_mac#TF_BINARY_URL\r\nIf there is a cleaner workaround or fix for the MacOS, which I have missed, I will post it here later.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7711, "title": "Update nightly pip wheel and history URLS after upgrade to ubuntu:16.04", "body": "", "comments": ["Thanks @caisq !"]}, {"number": 7710, "title": "[WIP] MPI Support for Tensor data communication", "body": "This adds a second communication path to the distributed TensorFlow implementation, next to the gRPC communication path. This new communication path uses the Message Passing Interface (MPI) API to handle communication between processes allowing TensorFlow to take advantage of modern high performance networks such as Infiniband.\r\nThis pull request is used to start a discussion about this implementation, any draw backs, alternatives and requests for feedback and participation from the open source community.\r\n\r\n\r\n**The current pull request makes the following changes**\r\n- A new communication path has been added for sending and receiving Tensors between distinct processes.\r\n- This new communication path uses the MPI API to handle setting up the connections and the data transfer.\r\n- When using a CUDA-Aware (and/or GPUDirect RDMA) MPI implementation this path works for both CPU and GPU based Tensor data. \r\n\r\nThis new path is implemented by modifying the current 'send' and 'receive' operations. The original gRPC code stays in place and is responsible for setting up connections and all non-tensordata communications. \r\nAlthough MPI supports one sided RDMA operations, this example implementation uses plain blocking send/receive operations. This because the current TensorFlow (memory) model makes it difficult to efficiently implement the communication using MPI_Get/MPI_Put (see below). \r\n\r\n\r\n**Usage**\r\nIn order to setup the required MPI environment the Python instances have to be launched using 'mpirun'. An example launch script, which does not modify the original TF script, is supplied in: tensorflow/tools/dist_test/mpi/start-openmpi.sh\r\n\r\nThe launching can be done differently if one handles the task type/id selection inside the TF python script, by either reading the environment variables as set by mpirun or by using the mpi4py Python library to retrieve process IDs and hostnames. \r\n\r\nThe MPI execution path can be disabled by setting the environment variable 'MPI_PATH_DISABLED' to 1, like:\r\nexport MPI_PATH_DISABLED=1\r\n\r\n\r\n**MPI Requirements**\r\n- The MPI implementation should be built with support for MPI_THREAD_MULTIPLE\r\n- To enable this path for GPU data, the MPI implementation should be built with CUDA support (CUDA-Aware MPI)\r\n- Tested using OpenMPI-2.0.1 \r\n- Currently the path to the OpenMPI library is hard-coded in the \"third_party/gpus/crosstool/CROSSTOOL.tpl\" file, this should be made an option in the configure code. \r\n\r\n\r\n**Implementation details**\r\n\r\n\r\n**MPI process ID mapping to gRPC names**\r\nTo handle communication between processes MPI identifies processes using a unique process-ID. TensorFlow uses gRPC which uses names based on things like 'worker', 'gpu', 'task'. To enable the MPI path in co-existence with the gRPC communication stack the names of the gRPC 'server' are mapped to the unique MPI IDs during the initialization of the gRPC stack. This conversion is then available once the tensor-data will be communicated.\r\n\r\n\r\n**Send details**\r\nThe original 'send' operation places a tensor in a table which is then picked up by the gRPC thread once a request for that data arrives. In the MPI path we place an MPI_Send call which will block until the matching MPI_Recv call is made on the receiving side. By making a hash of the tensor description we create a unique combination of sending-process/tensor-id which will be matched by the receiving process. This enables multiple tensors to be in flight from a single process using different send threads. Sending is a two-step process, the first message contains the properties of the Tensor (data type & shape) followed by a message containing the actual data. \r\nThe send call is intercept in the file: 'tensorflow/core/distributed_runtime/base_rendezvous_mgr.cc'\r\nIn this function: BaseRemoteRendezvous::Send\r\nThe code verifies that the destination is a different process from the sending process, if this is the case (and the implementation is enabled at run-time) then it will enter the new 'SendToRemote' functions.  Otherwise it will progress through the original code.\r\nTo keep the changes as organized as possible, the actual implementation of the sending-functions are in:  tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc\r\nThe other file modifications are required to get the path enabled through out the calling stack.\r\n\r\n**Receive details**\r\nThe original 'receive' operation requests a named-tensor from a remote process by placing a request through the gRPC stack. In the MPI path this request has been replaced by a set of receive calls that match the send operations. If there is no matching send operation yet, then the path will block until the sending process has arrived at the same point. The send and receive operations are matched using a hash of the requested tensor name. \r\nThe first message will describe the tensor, this description is passed on to the memory allocation functions after which the actual tensor data flows directly into the newly allocated memory buffer.\r\nAll the modified receive functionality is in: tensorflow/core/distributed_runtime/rpc/grpc_remote_worker.cc\r\nBy inheriting the 'GrpcRemoteWorker' and replacing the 'RecvTensorAsync' function. \r\n\r\nNote: It is possible that a tensor with the same name is send (received) multiple times between the same set of processes. This is not a problem as messages are handled as unique units and have unique follow up IDs (based on thread IDs).\r\n\r\n\r\n**TensorFlow limitations**\r\n- Memory is constantly being (de)allocated in between iterations. This hinders the performance when using Infiniband connections as memory buffers must be pinned/mapped before they can be used. This can impact the effective bandwidth by a factor 3 as this pinning is a relatively costly operation. \r\n- It would be more efficient if memory buffers for the same tensors would be reused/retained in between iterations, this makes the pinning of memory buffers a one-time occurrence. It also allows for the removal of the allocation step in the 'receive' functions.\r\n\r\n\r\nMinds.ai\r\nJeroen B\u00e9dorf", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "What's the status of this? Is it still WIP? @poxvoculi are you reviewing it?", "Yes, turns out I'm reading it right now.  Note that there's overlap in functionality with 2 other PRs, so I'm looking at them all together.", "Which are two other PRs being reviewed? Appreciate if you could point me there.", "@byronyi \r\n\r\nOne is limited to AllReduce over MPI, but has a different approach to MPI integration: https://github.com/baidu-research/tensorflow-allreduce/pull/2\r\n\r\nThe other one is not yet a PR, but probably will generate one.  It bypasses MPI and gets TF to work directly on verbs:  https://github.com/yahoo/tensorflow/tree/22fd3aad2dac96d521b5224519f443cef5269efb/tensorflow", "@jbedorf could you merge and update?\r\n@poxvoculi any comments?", "I'm working on a refactoring that is less intrusive to TensorFlow and solves some problems that surfaced with the current implementation. Once that is working I'll update this pull-request. ", "this is a great work", "@jbedorf should we keep this PR open or do you think it would make sense to send a new PR?", "@jbedorf told me he will be sending a new PR soon, so this one can be closed."]}, {"number": 7709, "title": "Adjust code to 38f6fd96793e031427be801612abf07d8d649d50", "body": "", "comments": ["Can one of the admins verify this patch?", "What is the rationale for this PR?", "Thanks for the change! The documentation has been updated, so closing this out."]}, {"number": 7708, "title": "Visualize experiment arguments in Tensorboard", "body": "**Feature request**\r\n\r\nI am wondering if it's possible to store the arguments that we pass to the training script to be stored as part of other summaries, and have a way to see those arguments/parameters as notes in a separate tab in Tensorboard.\r\nReason: It's often hard to track individual training runs and relate them to the training/network config if you change training and network parameters frequently which always happens during hyperparameter search. We can do it manually too but if included in Tensorboard, it would make it one go-to visualizer and comparison tool for everything.\r\nOne method of doing this that comes to mind is using the `tf.app.flags.FLAGS` arguments. This would keep everything standardized. Or we could also support `argparse` directly.\r\nIs this something in line with Tensorboard's philosophy or is it too straightforward to be a special feature?", "comments": ["One thing I've intended to add for a while is explicit hyperparameter support to TensorBoard.\r\nThe way I imagine this working is, when you create a summary.FileWriter, you would add a configuration protobuf for the \"run\" which has a universally unique ID, as well as a bunch of configurable metadata for keeping hyperparameter settings. \r\n\r\nThen, this data would be encoded with the run and we could build hyperparameter awareness into TensorBoard.\r\n\r\nThe proto would look like this: \r\n```\r\nmessage Run {\r\n  string uuid = 1;\r\n  string name = 2;\r\n  string description = 3;\r\n  string dataset = 4; // optional param with common dataset name\r\n  repeated HParam hparams = 5;\r\n  int64 start_time = 6;\r\n  // End is 0 until run is complete or if failed.\r\n  int64 end_time = 7;\r\n}\r\n\r\nmessage HParam {\r\n// Protocol buffer holding hyperparameters.\r\n// Examples of hyperparameters:\r\n//   learning_rate = 0.1,\r\n//   num_hidden_units = 100,\r\n//   activations = ['relu', 'tanh']\r\nmessage HParam {\r\n  // Required: unique name for this hyperparameter.\r\n  // Names can be nested into groups using '/' as a delimiter.\r\n  string name = 1;\r\n\r\n  // Required: tensor representing the value of the hyperparameter.\r\n  TensorProto value = 2;\r\n\r\n  // Optional: A human-readable, short, display name for display in a tool UI. If unset, name will be used\r\n  string display_name = 3;\r\n\r\n  // Optional: A long-form description of this hyperparameter tool UI.\r\n  string description = 4;\r\n}\r\n```\r\n\r\nThis wouldn't be a direct integration with flags; the user code would be responsible for encoding the hyperparameters. \r\n\r\nWhat do you think of this design?", "I have more details here which may help this discussion:\r\nhttps://github.com/tensorflow/tensorflow/issues/4714", "Hi @dandelionmane, that looks good as long as it's simple enough to do a single loop over all argument flags to put them in the protobuf.\r\nLooking at the above referenced issue, it would also be good to allow pushing python variables (not tensors) to the existing tabs like Scalars/Histograms/Images. I do it right now with `tf.py_func()` which is an overkill to add varying python scalars. (Example use case is for monitoring non-tf native python queues.)\r\nOnce we have some solution, it would be cool to allow filtering of runs using these hyperparameters too.", "@dandelionmane you mentioned that Hyperparameter & run description for tensorboard was under active development here #2454.\r\nI wonder did you mean something else that is already implemented as of now? \r\n", "[**chiboard**](https://github.com/rmst/chi#visualization-with-chiboard) shows run arguments for experiements/runs automatically. Would be also happy about run descriptions in tensorboard though, especially for comparing multiple runs.", "I'm migrating this to our new repository at https://github.com/tensorflow/tensorboard/issues/46. See you there! :-)"]}, {"number": 7707, "title": "estimator with batch size and data feeding", "body": "Could you provide an example of using the high-level API Estimators with placeholders and feeding batches  like for a basic use:\r\nfor step in xrange(max_steps):\r\n   batch_of_inputs,batch_of_targets= get_batch_from_disk(step) # e.g. batches are stored as list where step is and index of the list\r\n    feed_dict = {x:batch_of_inputs,y:batch_of_targets}\r\n    _, loss_value = sess.run([train_op, loss],\r\n                             feed_dict=feed_dict)\r\nHow to do the same with Estimator API?\r\nEstimator takes batch_size, steps, input_fuc or feed_fun as an argument and but it is not clear for me how to implement a function which will load data of batch size  e.g. in every iteration from disk?", "comments": ["Personally, I've found using feed_fn to be the most useful option w/ Estimators, and I ignore the other possibilities.\r\n\r\nCheck out: https://www.tensorflow.org/get_started/input_fn\r\n\r\nThere are many examples of feed functions offered - modify as needed.", "This sort of usage question is best asked on stackoverflow."]}, {"number": 7706, "title": "Trouble installing python wheel for tensorflow on Windows 7", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nMy problem:\r\nhttp://stackoverflow.com/questions/42280894/tensorflow-wheel-install-not-supported?noredirect=1#comment71724208_42280894\r\n\r\nFollowing the instructions:\r\nhttps://www.tensorflow.org/install/install_windows\r\n\r\nI had Python 3.5.2 installed, but following the instructions to install tensorflow according to the website automatically upgrades python to 3.6. The wheel is not supported.\r\n\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nWindows 7 \r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nNot applicable yet, as I am still trying to install the software, prior to running the scripts.\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\n\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Please see #6999, Python 3.6 is not supported on Windows yet and currently the only way is to use TensorFlow with Python 3.6 is to build it from source.  You can keep track there and this is a duplicate and can be closed.", "Closing as duplicate of #6999"]}, {"number": 7705, "title": "ImportError: No module named '_pywrap_tensorflow' (MSVCP140.dll is present)", "body": "I installed the nightly build windows 64bit release of tensorflow from http://ci.tensorflow.org/view/Nightly/job/nightly-win/85/DEVICE=gpu,OS=windows/\r\nusing pip install in the Anaconda distribution of Python 3.5 (v4.1.1.0)\r\n\r\nWhen I try to import tensorflow, I get the following error:\r\n\r\n``` Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"C:\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nFailed to load the native TensorFlow runtime. \r\n```\r\n\r\nI checked the msvcp140.dll and it seems to be present in multiple locations of my %PATH% (in the anaconda folder, in system32, sysWOW64 and some other locations).\r\n\r\nI also have environment variables setup for the CUDA path.\r\n\r\nThe issue filed here: https://github.com/tensorflow/tensorflow/issues/7529 is essentially the same as mine but the user resolved it by shifting development to a VPS running Ubuntu. It still doesn't solve the problem though.\r\n\r\nAny help would be appreciated! :)\r\n", "comments": ["@SiddGururani Please try changing your directory to a directory other than the one where you installed tf.", "Today is a corporate Google holiday so it might take a little to get a googler response. \r\nSo just confirming, you have the  Visual C++ Redistributable 2015 x64 installed, right? \r\nDid the suggestion above generate any result for you? \r\n", "@daxlab I tried that. Didn't help. I'm assuming you're talking about changing the working directory from where I run python and subsequently import tensorflow.\r\n\r\n@Carmezim Yes, I checked MSVCP140.dll before trying to import tensorflow and after it failed, I did a fresh install of the VC redist package. Nothing changed.", "If you want to try uninstalling TensorFlow and installing with `conda install` in case you need to get it running for now. The package is not officially supported though, or try installing the PYPI package with `pip install tensorflow-gpu` and see if it gives some result. \r\nYou have one Python distribution installed right?", "So when I use the PYPI package, it works. But gives the issues listed on https://github.com/tensorflow/tensorflow/issues/7621\r\nThe solution there was to install the nightly build. Maybe, for now I'll revert to the PYPI package.\r\n\r\nYes, I have one Python distribution.", "Are you getting OpKernel error? There are more recent nightlies (89) if you want to try as well.", "Yes, I get the OpKernel errors with the PYPI package.\r\n\r\nI'll try out using the latest nightly and see if it works. Shall keep you posted!", "The most recent nightly build also gave the same error. \r\n\r\nJust to make sure I'm right in saying that the MSVCP140.dll is in the path, here is the output of:\r\n```\r\nC:\\>dir msvcp140.dll /b/s\r\nC:\\Anaconda3\\msvcp140.dll\r\nC:\\Anaconda3\\envs\\tensorflow\\msvcp140.dll\r\nC:\\Anaconda3\\envs\\tensorflow\\Library\\bin\\msvcp140.dll\r\nC:\\Anaconda3\\Library\\bin\\msvcp140.dll\r\nC:\\Anaconda3\\pkgs\\vs2015_runtime-14.0.25123-0\\msvcp140.dll\r\nC:\\Anaconda3\\pkgs\\vs2015_runtime-14.0.25123-0\\Library\\bin\\msvcp140.dll\r\nC:\\ProgramFiles\\CommonFiles\\microsoftshared\\ClickToRun\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\Cisco\\CiscoAnyConnectSecureMobilityClient\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\Debugger\\target\\armv4i\\MSVCP140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\Debugger\\target\\x86\\MSVCP140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\DiagnosticsHub\\target\\armv4i\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\DiagnosticsHub\\target\\armv4i\\Collector\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\DiagnosticsHub\\target\\x86\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\CommonFiles\\MicrosoftShared\\PhoneTools\\14.0\\DiagnosticsHub\\target\\x86\\Collector\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\MicrosoftOffice\\root\\Flattener\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\MicrosoftOffice\\root\\vfs\\System\\msvcp140.dll\r\nC:\\ProgramFiles(x86)\\MicrosoftOffice\\root\\vfs\\SystemX86\\msvcp140.dll\r\nC:\\Windows\\Panther\\MigrationShims\\MigShim1\\System32\\msvcp140.dll\r\nC:\\Windows\\Panther\\MigrationShims\\MigShim1\\SysWOW64\\msvcp140.dll\r\nC:\\Windows\\System32\\msvcp140.dll\r\nC:\\Windows\\SysWOW64\\msvcp140.dll\r\n```\r\nAnd at least the Anaconda and the Windows system folders are in my `%PATH%`", "This issue was common with Python.org distribution and not Anaconda at least AFAIK. I will comment @mrry and @gunan here if you are sure your CUDA and cuDNN are also properly set as they will know better what can be happening.", "I believe my CUDA and cuDNN are setup properly since the PYPI tensorflow-gpu package is able to find the CUDA dlls, but then it gives me the OpKernel errors.", "@mrry: windows build issue", "@SiddGururani Can you check if the CUDA and cuDNN DLLs are in directories named in your `%PATH%`? It looks like commit https://github.com/tensorflow/tensorflow/commit/191658d54f90ac03c15b339326129cd52d1f56a3 changed the way the CUDA-related DLLs were loaded after the 1.0 release branch was cut, so it's possible (although I'm not exactly sure why) that the library resolution would be different in the two versions. ", "Woah! So it looks like cuDNN wasn't setup properly. I put the dll, lib and .h file in CUDA's respective folders.\r\nAnd it works.", "Thanks for help resolving the previous issue. When I start a new session now though, it gives me these warnings:\r\n```\r\n2017-02-21 15:24:55.373328: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373452: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373552: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373626: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373695: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373764: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373833: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-02-21 15:24:55.373901: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations. \r\n```\r\nShould I create a new issue for this?", "Those are simply warnings about performance.\r\nThey just say if you build from source, TF can be faster on your machine.\r\n", "@SiddGururani Thanks for digging into the problem with cuDNN paths! Let us know if we can improve the installation instructions to avoid this.", "I think it would be helpful if you specifically instruct users to move the cuDNN files (the dll, lib and the header) from the cuDNN extracted folder into:\r\na.) A single folder containing all of these files and then add that folder to the `%PATH%` env variable.\r\nb.) move the dll into CUDA's bin folder, the lib to CUDA's lib folder and the header to CUDA's include folder.\r\nThanks for helping me through the whole process! It's funny that the cuDNN path was an issue after I was so confident that wasn't the case!", "I have similar problem. After installation of MS VC++ 2015 Redistributable Update 3 x64, the problem still exists. I have tried the above method. However, it still does not work.\r\nCan anyone help? Thank you.\r\n\r\nMy environment:\r\n Windows 7 64 bit\r\n Python 3.5.3\r\n Tensorflow GPU 1.10rc1\r\n Cuda 8.0\r\n Cudnn 6.0\r\n\r\n---------------Update--------------------\r\nIt's CuDNN 6's problem, which does NOT work on my environment! When I switch to Cudnn5.1, everything works fine! Thanks to @ZacDiggum who found the problem at [9066](https://github.com/tensorflow/tensorflow/issues/9066)\r\n\r\n\r\nHere are the error message when I \"import tensorflow\":\r\n\r\nTraceback (most recent call last):\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 18, in swig_import_helper\r\n return importlib.import_module(mname)\r\n File \"D:\\Programs\\Python35\\lib\\importlib_init_.py\", line 126, in import_mod\r\n ule\r\n return _bootstrap._gcd_import(name[level:], package, level)\r\n File \"\", line 986, in _gcd_import\r\n File \"\", line 969, in _find_and_load\r\n File \"\", line 958, in _find_and_load_unlocked\r\n File \"\", line 666, in _load_unlocked\r\n File \"\", line 577, in module_from_spec\r\n File \"\", line 914, in create_module\r\n File \"\", line 222, in _call_with_frames_removed\r\n ImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow.py\", line 41, in \r\n from tensorflow.python.pywrap_tensorflow_internal import *\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 21, in \r\n _pywrap_tensorflow_internal = swig_import_helper()\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 20, in swig_import_helper\r\n return importlib.import_module('pywrap_tensorflow_internal')\r\n File \"D:\\Programs\\Python35\\lib\\importlib_init.py\", line 126, in import_mod\r\n ule\r\n return _bootstrap._gcd_import(name[level:], package, level)\r\n ImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n File \"\", line 1, in \r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow_init_.py\", line 24,\r\n in \r\n from tensorflow.python import *\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python_init_.py\", l\r\n ine 51, in \r\n from tensorflow.python import pywrap_tensorflow\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow.py\", line 52, in \r\n raise ImportError(msg)\r\n ImportError: Traceback (most recent call last):\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 18, in swig_import_helper\r\n return importlib.import_module(mname)\r\n File \"D:\\Programs\\Python35\\lib\\importlib_init_.py\", line 126, in import_mod\r\n ule\r\n return _bootstrap._gcd_import(name[level:], package, level)\r\n File \"\", line 986, in _gcd_import\r\n File \"\", line 969, in _find_and_load\r\n File \"\", line 958, in _find_and_load_unlocked\r\n File \"\", line 666, in _load_unlocked\r\n File \"\", line 577, in module_from_spec\r\n File \"\", line 914, in create_module\r\n File \"\", line 222, in _call_with_frames_removed\r\n ImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow.py\", line 41, in \r\n from tensorflow.python.pywrap_tensorflow_internal import *\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 21, in \r\n _pywrap_tensorflow_internal = swig_import_helper()\r\n File \"D:\\Programs\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorfl\r\n ow_internal.py\", line 20, in swig_import_helper\r\n return importlib.import_module('pywrap_tensorflow_internal')\r\n File \"D:\\Programs\\Python35\\lib\\importlib_init.py\", line 126, in import_mod\r\n ule\r\n return _bootstrap._gcd_import(name[level:], package, level)\r\n ImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\n ems\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\n above this error message when asking for help.\r\n", "@ybsave cuDNN 8.0? I suggest you review your CUDA related DLLs making sure they're properly set in your `%PATH%` and cuDNN libs are properly installed.", "@Carmezim  Sorry for my typo. It was cuDNN 6.0. I have found the problem due to the cuDNN 6.0; when I switch to 5.1, everything works fine.", "@ybsave Glad to hear you figured it out. ", "The problem was the cuDNN Library for me - for whatever reason cudnn-8.0-windows10-x64-v6.0 was NOT working - I used cudnn-8.0-windows10-x64-v5.1 - ALL GOOD!\r\n\r\nMy setup working with Win10 64 and the Nvidia GTX780M:\r\n\r\n- Be sure you have the lib MSVCP140.DLL by checking your system/path - if not get it [here](https://www.microsoft.com/en-us/download/details.aspx?id=48145)\r\n- Run the windows installer for python 3.5.3-amd64 from [here](https://www.python.org/downloads/release/python-352/) - DO NOT try newer versions as they probably won't work \r\n- Get the cuDNN v5.1 for CUDA 8.0 from [here](https://developer.nvidia.com/rdp/cudnn-download) - put it under your users folder or in another known location (you will need this in your path)\r\n- Get CUDA 8.0 x86_64 from [here](https://developer.nvidia.com/cuda-downloads)\r\n- Set PATH vars as expected to point at the cuDNN libs and python (the python path should be added during the python install)\r\n\r\nIf you run Windows 32 be sure to get the 32 bit versions of the files mentioned above.", "I had the same issue, was able to solve it by using cuDNN 5.1 instead of 6.0. Is there a reason why 6.0 is not working? Does it have something to do with Windows? Just curious", "I was having this issue and installing cuDNN 5.1 fixed it for me as well.", "I just solved the problem with reinstalling python35 using customize installation by checking all the boxes and then next, checking all the boxes again (especially the last two - Download debugging symbols and binaries).\r\n\r\nAfter that, using `pip3 install tensorflow` to reinstall tensorflow again, it works, though I've installed tensorflow before. \r\n\r\nStill, it has many warnings when I test the program \ud83d\ude1e ", "@ZacheryGuan Are you getting the warnings that were posted a bit above?\r\n```\r\nThe TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nI think you could get rid of them if you build tensorflow on your machine first.", "@vburca Yes, I got that \ud83d\ude1e ", "@vburca @ZacheryGuan \r\nplease see http://stackoverflow.com/questions/43335531/how-to-use-sse4-1-instructions-without-install-tensorflow-from-source/43335630#43335630", "@Carmezim Thanks very much. Just like that I get used to using `gcc -w`. \ud83d\ude06 ", "Thank you SiddGururani, copying the cuDNN dll, lib, and header to the CUDA folder's respective subfolders worked for me as well. I have Cuda 8.0 and cuDNN 5.1, and (native) pip install tensorflow-gpu had claimed success, but I was still having this problem.", "I would like to confirm that downgrading to cuDNN 5.1 (from 6.0) resolved the issue for me as well.", "I had the same issue. Downgrading cuDNN from 6.0 to 5.1 solved it for me as well.", "I had the same problem. Followed your instruction (b) and the problem is solved! Thanks a lot!", "I am getting the exact same error message as Sidd originally got with the only difference being that I am in an anaconda environment that I created.  \r\n\r\nI tried to follow the suggestions above suggested by Sidd but I am still getting the error.  Here is my current setup -- Windows 10 + cuDNN 5.1 + CUDA 8.0 and I tried both of sidd's options a) adding a single folder C:\\Users\\username\\cuda that contains the cudnn.h, cudnn.lib and cudnn64_5.dll and then also added this folder to the PATH variable  then I also tried b) by adding the cudnn.h, cudnn.lib and cudnn64_5.dll files to the respective CUDA folders which I took to be C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin & C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\include & C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64.  I then deactivated the tensorflowgpu conda environment and then activated it again and am still getting the same error.  Is there anything else that I am not considering?\r\n\r\nThe only other change that I did was I am trying to use python 3.6 because the recent tensorflow 1.2 release supposedly was compatible -- could this be causing the error by any chance? To create the environment I ran 'conda create -n tensorflow python=3.6' and then ran (tensorflow)C:> pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.2.0-cp36-cp36m-win_amd64.whl' where I specifically change dto cp36-cp36m (to download the 3.6 installation) -- could this be the reason why I am still having issues? Any help would be much appreciated\r\n\r\nSee stacktrace below:\r\n\r\n```C:\\Users\\username>activate tensorflowgpu\r\n\r\n(tensorflowgpu) C:\\Users\\username>python\r\nPython 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'```'\r\n", "Just to add context, I created a conda python 3.5 environment and am still getting the same error so something else must be missing. Is there any more specific documentation on exactly which folders need to be in the PATH variable? I see other responses such as \"copying the cuDNN dll, lib, and header to the CUDA folder's respective subfolders worked for me as well.\" and I believe that I copied those into the subfolders so there must be some other issue that I'm not seeing?\r\n\r\n@mrry you had mentioned to Sid to \"Can you check if the CUDA and cuDNN DLLs are in directories named in your %PATH%?\" but I don't see a list of all the DLLs that I need to confirm are in directories named in my path?\r\n\r\nFor further context if I check what is contained in my %PATH% you see as follows (removed some that obviously had nothing to do with CUDA or CUDNN) and as I mentioned above I have placed the cuDNN files (cudnn.h, cudnn.lib and cudnn64_5.dll) in a) C:\\Users\\username\\dev\\cud and b) individually in CUDA\\v8.0\\bin, CUDA\\v8.0\\include & CUDA\\v8.0\\lib\\x64 respectively..\r\n\r\nMy best guess is that the cudnn64_5.dll file does not actually exist in the Anaconda3 environment for some reason.  For example, using 'dir msvcp140.dll /b/s' I can find a file: C:\\Users\\jcomfort\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflowgpu\\msvcp140.dll but if I type dir cudnn64_5.dll /b/s into the command prompt I only see these in the local environment.  So my guess is that in the tensorflow gpu installation to the anaconda environment I am somehow still not grabbing these dll files properly.  Hopefully this helps somebody get a better idea of what is going wrong as I'm not sure what else to try.\r\n\r\n`C:\\Users\\username>PATH\r\nPATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp;\r\nC:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;\r\nC:\\Users\\username\\dev\\cuda;\r\nC:\\Users\\username\\dev\\cuda\\bin;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\include;\r\nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64;\r\nC:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3;\r\nC:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\Library\\mingw-w64\\bin;\r\nC:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\Library\\usr\\bin;\r\nC:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\Library\\bin;\r\nC:\\Users\\username\\AppData\\Local\\Continuum\\Anaconda3\\Scripts;\r\nC:\\Users\\username\\AppData\\Local\\Microsoft\\WindowsApps`", "@jcomfort4 I have solved the problem by the method discribed above. I guess the program just need a file like 'cudnn64_5.dll' and named 'cudnn64_5.dll'. If you changed the name of  'cudnn64_6.dll' (which in cuDNN6.0) to  'cudnn64_5.dll' , it still worked.(But I haven' t tested it for further use.) \r\nI advise you to check that you've put the file in right folder, because I noticed that you type them in wrong order.\r\ncudnn64_5.dll -----------------CUDA\\v8.0\\bin\r\ncudnn.h------------------------CUDA\\v8.0\\include\r\ncudnn.lib-----------------------CUDA\\v8.0\\lib\\x64\r\n", "@drophit I have the same issue and this solution works for me. Thanks a lot. ", "I'm getting the same issue, and followed all of the above advice, no luck.\r\n\r\nIt's super hard to debug, if its one or the other file that is missing. Couldn't Tensorflow be better at telling you what file it can't include?\r\n\r\n I'm trying this combination Windows 10 + Python 3.5.2 + cuDNN 5.1 + CUDA 8.0 + Tensorflow 1.3.\r\n\r\nI've tried all cuDNN versions from 7 to 5.1 (All the DLL's can exist in same dir, as they are named differently)\r\nI made PATH point here. I even print os.getenv('PATH') in the start of my script to check that it's there.\r\nIt's only the DLL's that need to go in the path, right? (not the .lib)\r\n\r\nmsvcp140.dll is located all over my system, including a lot which PATH points to.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@DinoP I believe the released version of TensorFlow 1.3 depends on cuDNN 6, so it's probably looking for `cudnn64_6.dll`.", "Thank for the answer.\r\n\r\nAs I wrote, I both have version 5.1, 6 and 7 installed in that dir. So should'nt be be that.\r\n\r\nThe \"v8.0\\bin\\\" dir looks like this:\r\n03/06/2017  19:08        47,600,128 cublas64_80.dll\r\n07/11/2016  11:08        83,148,288 cudnn64_5.dll\r\n12/04/2017  13:59       152,321,536 cudnn64_6.dll\r\n25/07/2017  19:18       213,226,496 cudnn64_7.dll\r\n03/06/2017  19:09           226,304 nvblas64_80.dll\r\n\r\n@mrry Also back to my point about Tensorflow not letting me know what file it's expects in the error output, make is tooo much harder to debug.\r\n", "So, I thought I had a breakthrough... did \"pip uninstall tensorflow tensorflow-gpu\" and then \"pip install tensorflow-gpu\"\r\n\r\nMy suddenly script then didn't fail at the import of TF.... hurray... but only cpu devices were available (no gpu). \r\n\r\nMakes sense that it did not fail then, but I'm not sure why it stopped showing my gpu.\r\n\r\nAnyone knows how to force my script to use tensorflow-gpu or \"re-enable\" my gpu devices?\r\n\r\n", "Greetings,\r\n\r\ni had the same issues described above:\r\n\r\n[Import errors](https://github.com/tensorflow/tensorflow/issues/7705#issuecomment-309605389) as well as the case described [4 hours ago](https://github.com/tensorflow/tensorflow/issues/7705#issuecomment-323145931). What helped me in my case was in fact the cuDNN Version 6.\r\n\r\nI am using python 3.6, CUDA 8, cuDNN 6 and tensorflow-gpu 1.3.0 on Windows 10 x64. For cuDNN I extracted the content of bin, include and lib to the respective folders in ...\\CUDA\\v8.0\\\r\n\r\nI hope this resolves the issue for some of you guys.\r\n\r\nEDIT: What may help as well to check if tensorflow really uses GPU is [this solution on stackoverflow](https://stackoverflow.com/questions/38009682/how-to-tell-if-tensorflow-is-using-gpu-acceleration-from-inside-python-shell).\r\n", "Been trying to install tensorflow since a few days now and guides hardly ever keep up with updates to newer components. Thanks @Adrian-Steinert, your combination of packages is the one that finally worked for me!", "@mrry @Adrian-Steinert \r\nThanks a lot for mentioning this! \r\n\r\nI can confirm that at least for tensorflow version` 1.3.0`, we need to download `cudnn-8.0-windows10-x64-v6.0` for cuDNN 6.0, because **it is looking for `cudnn64_6.dll`**, not `cudnn64_5.dll` anymore!\r\n\r\nWasted so much times due to this kind of hidden dll naming constraint, hope this can be helpful to other people who encountered `loading dll failed` issue.\r\n", "Ok, so i think i finally found the solution to my problem. I've followed every guide out there, so was just about to give up.\r\n\r\nFirst of all, thanks @mrry creating this script: https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c\r\nIt made me aware of my problem. It told me that cudart64_80.dll was missing.\r\n\r\nSo when going to the Nvidia download page, you get presented with 2 downloads, Base installer and \"Patch 2\". Because Patch 2 was a larger file size than base I thought this version contained everything. Also \"Patch 2\" installs with no problems, even though Base installation is missing.\r\n\r\nSo it turned out that I missed the Base installation and didn't have CUDA properly installed, only the Patch.\r\n\r\nHope this helps the others.\r\n\r\n\r\n", "..... only to discover that GPU is twice as slow as CPU at the task I'm trying solve (GTX1080)\r\n", "@guitarmind I can also confirm that this worked for me! This is huge, I spent ages and ages trying to solve this while having cudnn 5.1 because every single guide said that cudnn 6 doesn't work, but in reality cudnn 6 is required for the current tensorflow-gpu 1.3.0", "@Innixma I did exactly the same....tried to reinstall python, CUDA, cuDNN, check environment variables so many times.\r\n\r\nTotally misled by all public guides until I saw this issue page. Could anyone update the official installation guide of TensorFlow to mention about this for the updates after `tensorflow-gpu 1.3.0`?", "I solve the exact problem by using Cudnn 6.0 instead of Cudnn 5.0 recently.  (cudnn-8.0-windows10-x64-v6.0). \r\n\r\nWhile the document metioned [https://www.tensorflow.org/versions/r1.3/install/install_windows](https://www.tensorflow.org/versions/r1.3/install/install_windows) is still wrong about Cudnn's version", "I spent forever on this issue only to find this open issue. Someone really needs to update the Windows guide", "@av8ramit Can we modify the webpage to point to cudnn 6?", "I happened to avoid the problem because I was lucky enough to read [the 1.3.0 changelog](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md#release-130) where they mention that binaries are now build against cuDNN 6.0. Time to update the install guides (there's already people on StackOverflow scratching their heads)", "We are in the process of updating the website right now. Thank you for your patience and sorry for any inconvenience. ", "Oh god....I fixed it on my machine!\r\n\r\nYou guys are right! Tensorflow 1.3 requires cudnn64_6.dll not cudnn64_7.dll\r\n\r\nYou should use the 6.0 CUDNN version with TF 1.3", "cudnn64_6.dll work for me ,a \"pan\" help for me , ^o^,tks\r\nlink for cuDNN 6.0:   [https://developer.nvidia.com/compute/machine-learning/cudnn/secure/v6/prod/8.0_20170307/cudnn-8.0-windows10-x64-v6.0-zip](url)   size:104M", "I had the same issue today in upgrading to 1.3.0. I had both cudnnv5.1 and cudnnv6.0, but in the PATH system variable, cudnnv5.1 was used. replaced it with cudnnv6 and all errors are gone!", "I just wrestled with this for a couple hours (Windows 10, Anaconda 4.4.0 64 bit). I had been trying to install tensorflow-gpu using pip with no success. I followed all the guides I could find, tried using Python 3.5.3, installed cudnn 5.1 and 6.0, tried different combinations using conda environments, and still ran into the same error. \r\n\r\nI found that installing via conda rather than pip worked the first time. Conda installed cudatoolkit, cudnn, libprotobuf, and protobuf and updated itself and vs2015_runtime. \r\n\r\nApparently conda is better at putting all this stuff in the right place than I am. Praise conda.", "To resolve the problem in windows 10, I did the following:\r\n`1. pip uninstall tensorflow-gpu`\r\n\r\nOpen Anancoda prompt with administrative access and then run the following command.\r\n`conda install tensorflow-gpu`\r\n\r\nThis will take care of everything paths, cuda, cudnn and dll files and will install everything in the particular place.\r\n```\r\n\r\nPackage plan for installation in environment C:\\Anaconda3:\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    cudatoolkit:    8.0-1\r\n    cudnn:          6.0-0\r\n    libprotobuf:    3.2.0-vc14_0       [vc14]\r\n    protobuf:       3.2.0-py35_0\r\n    tensorflow-gpu: 1.1.0-np112py35_0\r\n    vc:             14-0\r\n\r\nThe following packages will be UPDATED:\r\n\r\n    astropy:        1.3.3-np111py35_0  --> 2.0.2-py35h891525e_4\r\n    bottleneck:     1.2.1-np111py35_0  --> 1.2.1-np112py35_0\r\n    h5py:           2.7.0-np111py35_0  --> 2.7.0-np112py35_0\r\n    matplotlib:     2.0.2-np111py35_0  --> 2.0.2-np112py35_0\r\n    numba:          0.33.0-np111py35_0 --> 0.33.0-np112py35_0\r\n    numexpr:        2.6.2-np111py35_0  --> 2.6.2-np112py35_0\r\n    numpy:          1.11.3-py35_0      --> 1.12.1-py35_0\r\n    pandas:         0.20.1-np111py35_0 --> 0.20.3-py35_0\r\n    pytables:       3.2.2-np111py35_4  --> 3.2.2-np112py35_4\r\n    pywavelets:     0.5.2-np111py35_0  --> 0.5.2-np112py35_0\r\n    scikit-image:   0.13.0-np111py35_0 --> 0.13.0-np112py35_0\r\n    scikit-learn:   0.18.1-np111py35_1 --> 0.19.0-np112py35_0\r\n    scipy:          0.19.0-np111py35_0 --> 0.19.1-np112py35_0\r\n    statsmodels:    0.8.0-np111py35_0  --> 0.8.0-np112py35_0\r\n    vs2015_runtime: 14.00.23026.0-0    --> 14.0.25420-0\r\n    werkzeug:       0.11.4-py35_0      --> 0.12.2-py35_0\r\n\r\nProceed ([y]/n)? y\r\n\r\ncudatoolkit-8. 100% |###############################| Time: 0:00:03 103.92 MB/s\r\ncudnn-6.0-0.ta 100% |###############################| Time: 0:00:01  61.40 MB/s\r\nvs2015_runtime 100% |###############################| Time: 0:00:00  50.37 MB/s\r\nvc-14-0.tar.bz 100% |###############################| Time: 0:00:00 117.44 kB/s\r\nlibprotobuf-3. 100% |###############################| Time: 0:00:00  59.48 MB/s\r\nnumpy-1.12.1-p 100% |###############################| Time: 0:00:00  54.71 MB/s\r\nwerkzeug-0.12. 100% |###############################| Time: 0:00:00  32.00 MB/s\r\nbottleneck-1.2 100% |###############################| Time: 0:00:00   6.92 MB/s\r\nh5py-2.7.0-np1 100% |###############################| Time: 0:00:00  39.03 MB/s\r\nnumba-0.33.0-n 100% |###############################| Time: 0:00:00  54.14 MB/s\r\nnumexpr-2.6.2- 100% |###############################| Time: 0:00:00  15.75 MB/s\r\nprotobuf-3.2.0 100% |###############################| Time: 0:00:00  31.49 MB/s\r\npywavelets-0.5 100% |###############################| Time: 0:00:00  52.60 MB/s\r\nscipy-0.19.1-n 100% |###############################| Time: 0:00:00  61.34 MB/s\r\nastropy-2.0.2- 100% |###############################| Time: 0:00:00  59.28 MB/s\r\npandas-0.20.3- 100% |###############################| Time: 0:00:00  52.79 MB/s\r\npytables-3.2.2 100% |###############################| Time: 0:00:00  42.35 MB/s\r\nscikit-learn-0 100% |###############################| Time: 0:00:00  51.41 MB/s\r\ntensorflow-gpu 100% |###############################| Time: 0:00:01  36.20 MB/s\r\nmatplotlib-2.0 100% |###############################| Time: 0:00:00  56.54 MB/s\r\nstatsmodels-0. 100% |###############################| Time: 0:00:00  56.49 MB/s\r\nscikit-image-0 100% |###############################| Time: 0:00:00  49.93 MB/s\r\n```", "@sulaimanvesal it seems conda is not up to date for the windows platform, since it installs version 1.1 which belongs to almost 5 months ago while the current version is 1.3.0!\r\n", "@gunan would be a good idea to lock this thread? ", "Locking due to this becoming a catchall for unrelated windows issues.\r\nMost of the questions here are better suited for Stackoverflow."]}, {"number": 7704, "title": "Op type not registered 'TensorArrayV3'", "body": "I've been building graph in python and deploy to c++ since r0.10. After upgrading to r1.0, I've retrained the model, re-created the graph, and re-link the library all in r1.0. However, I got this error when try to use the graph in the c++:\r\n `Not found: Op type not registered 'TensorArrayV3'` \r\nHere is how I created the tensorflow shared lib:\r\n```\r\ncc_binary(\r\n    name = \"libtensorflow_all.so\",\r\n    linkshared = 1,\r\n    deps = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\nI don't find `TensorArrayV3` symbol in the official lib package either ( e.g: `https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-1.0.0.tar.gz`)\r\n\r\nCan you please advise how to fix it?", "comments": ["Any update on this?\r\n```\r\n> find tensorflow -type f | xargs grep TensorArrayV3 |grep -v '/go/'\r\ntensorflow/cc/ops/op_gen_overrides.pbtxt:op { name: \"TensorArrayV3\" rename_to: \"TensorArray\" }\r\ntensorflow/core/kernels/tensor_array_ops.cc:REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\").Device(DEVICE_CPU),\r\ntensorflow/core/kernels/tensor_array_ops.cc:  REGISTER_KERNEL_BUILDER(Name(\"TensorArrayV3\")              \\\r\ntensorflow/core/ops/compat/ops_history.v0.pbtxt:  name: \"TensorArrayV3\"\r\ntensorflow/core/ops/compat/ops_history.v1.pbtxt:  name: \"TensorArrayV3\"\r\ntensorflow/core/ops/data_flow_ops.cc:REGISTER_OP(\"TensorArrayV3\")\r\ntensorflow/core/ops/data_flow_ops.cc:    .Deprecated(16, \"Use TensorArrayV3\");\r\ntensorflow/core/ops/data_flow_ops.cc:    .Doc(\"Deprecated. Use TensorArrayV3\");\r\ntensorflow/core/ops/ops.pbtxt:    explanation: \"Use TensorArrayV3\"\r\ntensorflow/core/ops/ops.pbtxt:  summary: \"Deprecated. Use TensorArrayV3\"\r\ntensorflow/core/ops/ops.pbtxt:  name: \"TensorArrayV3\"\r\ntensorflow/core/public/version.h://     division and mod semantics. TensorArrayV3. (12dec2016)\r\ntensorflow/python/ops/hidden_ops.txt:TensorArrayV3\r\ntensorflow/python/ops/tensor_array_grad.py:ops.NotDifferentiable(\"TensorArrayV3\")\r\n```\r\n\r\nIt suggests to register `TensorArrayV3`, but on one line, it also says rename `TensorArrayV3` to `TensorArray`. Do these mismatches cause issues?\r\n(I'm looking at commit 904510eeaa40b0c8f982fbb679d827688cb35b01 in the master now.)", "This is fine: the rename is for the python layer, in which the op is still called TensorArray. The dependencies look ok, you should definitely get the TensorArrayV3 kernel. built into your .so file. \r\n\r\nI am wondering whether maybe there is a bad interaction between linkshared and the op registrations. Have you tried the official tensorflow dynamic library build rule (`//tensorflow:libtensorflow.so`)?", "Tried `//tensorflow:libtensorflow.so` now, it doesn't have `TensorArrayV3` symbol either.", "Eugene, do you have an idea about why TensorArray may be missing?", "No, all seems in order. Maybe Josh can help?", "r1.1 also has the problem", "If you include linkstatic=1 as well, does it work?\r\nIt appears that with linkshared only, it will be in the DYNAMIC case listed for linkstatic here: https://bazel.build/versions/master/docs/be/c-cpp.html\r\n\r\n(this is guessing that there is a .so available at runtime that provides the operators from an older release of tensorflow)", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 7703, "title": "fail building tensorflow. Error is error trying to exec 'cc1plus'", "body": "\r\nI try to build tensorflow from the source code, as https://www.tensorflow.org/install/install_sources shows. But it fails at basel building like following.\r\n\r\ncommands: bazel build --config=opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nINFO: Found 1 target...\r\nERROR: /local/xian_titan/.cache/bazel/bazel_xian_titan/d96b63d3ff03346cfbf37aac2a75fe2c/external/protobuf/BUILD:230:1: C++ compilation of rule '@protobuf//:js_embed' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n(cd /local/xian_titan/.cache/bazel/bazel_xian_titan/d96b63d3ff03346cfbf37aac2a75fe2c/execroot/tensorflow && \r\nexec env - \r\nLD_LIBRARY_PATH=/usr/local/lib/:/usr/local/cuda-8.0/lib64 \r\nPATH=/usr/local/cuda-8.0/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/local/xian_titan/bin \r\nexternal/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -MD -MF bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.d '-frandom-seed=bazel-out/host/bin/external/protobuf/objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o' -iquote external/protobuf -iquote bazel-out/host/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE=\"redacted\"' '-D__TIMESTAMP=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/protobuf/src/google/protobuf/compiler/js/embed.cc -o bazel-out/host/bin/external/protobuf/_objs/js_embed/external/protobuf/src/google/protobuf/compiler/js/embed.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2.283s, Critical Path: 1.68s\r\n\r\nI found some other people also had similar problems before. But I cannot successfully address the problem from previous threads.\r\n\r\nI tried bazel 0.4.4 and 0.4.2 releases, they have the same issues.\r\n\r\nMy system is Red Hat Enterprise Linux Workstation release 6.7 (Santiago),\r\njava version \"1.8.0_121\",\r\ngcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)\r\ng++ (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)\r\n\r\nHope someone can figure out where's wrong", "comments": ["This looks like the same problem as #7543; here, bazel is not the issue, but the compiler wrapper script resets PATH.", "closing as duplicate of #7543 "]}, {"number": 7702, "title": "dynamic_rnn_decoder returns shape [?, batch_size, cell.output_size]", "body": "According to [the docs](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder),  the dynamic_rnn_decoder returns a tuple contaning `outputs`, which is a Tensor of shape `[max_time, batch_size, cell.output_size]`(provided `time_major==True`).\r\n\r\nIn my case, however, the first dimension of that Tensor is returned as underspecified (Dimension `?`), and in fact depending on the provided batch when running the RNN. \r\n\r\nIf this is the intended behaviour, it should probably be highlighted in the documentation that `max_time` is variable.\r\n\r\nReproduce with:\r\n\r\n```import tensorflow as tf\r\nimport numpy as np\r\n\r\n# toy data, timesteps between 1 and 10\r\ntimesteps = np.random.randint(1, 11, [10])\r\nX=np.random.randint(0, 20, [10,10,1])\r\n\r\nbatch_size = 2\r\nmax_ts = 10\r\ninputs = tf.placeholder(tf.float32, \r\n                        (max_ts, batch_size, 1), name=\"X_in\")\r\n\r\ncell_fw = tf.contrib.rnn.LSTMCell(50)\r\ncell_bw = tf.contrib.rnn.LSTMCell(50)\r\ncell_dec = tf.contrib.rnn.LSTMCell(50)\r\n\r\nseq_lens = tf.placeholder(tf.int32, batch_size, name=\"seq_lens\")\r\n\r\nenc_outputs, states = tf.nn.bidirectional_dynamic_rnn(\r\n    cell_fw, cell_bw, inputs, time_major=True, sequence_length=seq_lens, dtype=tf.float32)\r\n\r\ndecoder_inp = tf.concat(enc_outputs, axis=2) \r\n\r\nattention_states = tf.zeros([batch_size, 1, cell_dec.output_size],\r\n                                    name=\"attention_states\")\r\n\r\natt_keys, att_vals, att_score_fn, att_construct_fn = \\\r\n            tf.contrib.seq2seq.prepare_attention(attention_states,\r\n                                                 attention_option=\"luong\",\r\n                                                 num_units=50)\r\n\r\ndynamic_fn_train = tf.contrib.seq2seq.attention_decoder_fn_train(\r\n            states[0], att_keys, att_vals, att_score_fn, att_construct_fn)\r\n\r\noutputs, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\r\n            cell_dec, dynamic_fn_train, decoder_inp, time_major=True,\r\n            sequence_length=seq_lens)\r\n\r\nwith tf.Session() as sess:\r\n    feed_dict = {inputs: X[:,:2,:], seq_lens: ts[:2]}\r\n    sess.run(tf.global_variables_initializer())\r\n    out = sess.run(outputs, feed_dict=feed_dict)\r\n    print(out.shape[0])\r\n```\r\n\r\nThe very last print statement will show that the first output dimension is not max_ts, but the max timestep of the batch (<= 10)", "comments": ["Indeed, we should make it clear that dynamic_rnn will not know - at graph build time - the max_time value; that value is a Tensor whose value is tf.shape(inputs)[0] (0 when time_major=True, 1 when time_major=False).  PRs to improve the documentation are welcome!", "Anyone working on this? I've been using dynamic_rnn/seq2seq ops a lot lately and would be happy to write the requested documentation. I've been looking to start contributing here and this seems like a good issue to start with. ", "Keep in mind that a lot of the functions in tf.contrib.seq2seq have been\nrewritten in object-oriented style (on github master).  So for example:\n\ntf.contrib.seq2seq.dynamic_rnn_decoder\n\nand\n\ntf.contrib.seq2seq.attention_decoder_fn_train\n\nno longer exist.  a new attention mechanism is being pushed by next week.\n a new decoder already exists but is object oriented.\n\nOn Fri, Mar 10, 2017 at 10:33 AM, Brandon McKinzie <notifications@github.com\n> wrote:\n\n> Anyone working on this? I've been using dynamic_rnn a lot lately and would\n> be happy to write the requested documentation. I've been looking to start\n> contributing here and this seems like a good issue to start with.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7702#issuecomment-285748008>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6wcov9HOHr5-OKqpP0OJh02P7gzks5rkZdigaJpZM4MGS6H>\n> .\n>\n", "Yes, I noticed that, but figured it may be useful to update the current docs for users not on the master branch. In any case, let me know if any help is needed for seq2seq documentation. I've been following the object-oriented code in master closely and I'd be happy to help with either.\r\n\r\nAlong the same lines, are there any other issues that would be good \"starter\" contributions? Not sure where the best place is for suggesting/asking such things so any info would be useful. I read the contribution guidelines, and they suggested that the comments here are the place to discuss. Thanks!", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Contributions welcome to the existing OO-based API.   cc += @qlzh727 ", "@jbingel Could you please let us know, if you still need help on this issue. Since contrib has been depreciated in Tensorflow 2.x, Please do upgrade to a latest Tensorflow version 2.8 . Attaching [migration](https://www.tensorflow.org/guide/migrate) guide for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 7701, "title": "ERROR: Evaluation of query \"deps((//tensorflow/... - //tensorflow/examples/android/...))\" failed", "body": "root@cjliux-comp:/usr/local/lib/python2.7/dist-packages/tensorflow-1.0.0# ./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nPlease specify optimization flags to use during compilation [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? (Linux only) [Y/n] y\r\njemalloc enabled on Linux\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] n\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\n  /usr/local/caffe/python\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] n\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.......\r\nERROR: infinite symlink expansion detected\r\n[start of symlink chain]\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/local/lib/python2.7/dist-packages/tensorflow-1.0.0/util/python/python_lib\r\n[end of symlink chain]\r\n.\r\nERROR: Evaluation of query \"deps((//tensorflow/... - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.\r\n\r\nDo any one know what that mean and how to fix it?", "comments": ["I have found the solution. It's because I have been trying to build tensorflow in `/usr/local/lib/python2.7/dist-distributions`, and that conflicts with the installation path of tensorflow, which I have left it default. I solved that by placing the source of tensorflow in `/usr/local`."]}, {"number": 7700, "title": "Can't enforce shape invariants with TensorArrays in while_loop", "body": "I can't enforce shape invariants in a while_loop if one of the inputs is a TensorArray. Here's a minimal example: \r\n```\r\nimport tensorflow as tf\r\n\r\ndef body(i,ta):\r\n    ta = ta.write(i,1.0)\r\n    return (i+1,ta)\r\n\r\narr_size = 10\r\nta = tf.TensorArray(tf.float32, size=arr_size)\r\n\r\ni = tf.constant(0,tf.int32)\r\ninput = (i,ta)\r\ncond = lambda i,_ : i < arr_size\r\noutput = tf.while_loop(cond, body,input,shape_invariants=(i.get_shape(),tf.TensorShape(arr_size)))\r\n\r\n#works fine without shape_invariants:\r\n#output = tf.while_loop(cond, body,input)\r\n\r\nmat = output[1].stack()\r\nsess = tf.InteractiveSession()\r\nprint(mat.eval())\r\n```\r\n\r\nThe code above works fine if the while_loop is not fed shape_invariants. Using shape_invariants though, I get the following error: \r\n```\r\nValueError: The shape invariant specified for TensorArray:1 is not compatible with the initial shape of the loop variable. It enters the loop with shape <unknown>, but the specified shape invariant is (10,).\r\n```\r\n\r\nAm I doing something wrong or is this a bug?\r\n\r\nThanks! ", "comments": ["I think this problem is better suited for StackOverflow as apparently it's not bug with TensorFlow itself. They monitor there under the TensorFlow tag as well.", "@Carmezim Why do you think it is not a bug? ", "@yuanbyu can you look at this?", "@yuanbyu may not get to this anytime soon.  @ebrevdo?", "TensorArray objects have a shape invariant value of None.  This is a\ndocumentation bug.\n\nOn Jun 16, 2017 11:26 AM, \"Geoffrey Irving\" <notifications@github.com>\nwrote:\n\n> Assigned #7700 <https://github.com/tensorflow/tensorflow/issues/7700> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7700#event-1127296666>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3ASqQialP_XFtQUBe3k9LSq9ychks5sEsjIgaJpZM4MGSMB>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 343 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 358 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 373 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 7699, "title": "Error building from source", "body": "Hi,\r\n\r\nI'm trying to build from master branch and getting this error during the ./configure step\r\n\r\n`Please specify the location of python. [Default is /Users/lucasliu/anaconda/bin/python]: \r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /Users/lucasliu/anaconda/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Users/lucasliu/anaconda/lib/python2.7/site-packages]\r\n\r\nUsing python library path: /Users/lucasliu/anaconda/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] N\r\nNo CUDA support will be enabled for TensorFlow\r\nConfiguration finished\r\n\r\nERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:174:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:segmenter_utils'.\r\nERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:95:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:utils'.\r\nERROR: /Users/lucasliu/tensorflow/tensorflow/models/syntaxnet/syntaxnet/BUILD:162:1: no such package 'util/utf8': BUILD file not found on package path and referenced by '//tensorflow/models/syntaxnet/syntaxnet:char_properties'.\r\nERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.`\r\n\r\nOperating System: Mac OS Sierra Version 10.12.3\r\nBazel: 0.4.4-homebrew\r\nPython: 2.7 \r\n\r\ntf/master\r\nNo CUDA\r\nNo cuDNN\r\n\r\nI suspect this is due to my network timeout to bazel mirror site. Could you please advise?", "comments": ["It appears to me you have cloned the models repo inside the tensorflow repo, and now bazel is confused. \r\n\r\nsyntaxnet isn't part of the tensorflow repo, and shouldn't appear inside the tensorflow directory.", "Thanks Martin. You are right. I was trying to get the retrain of Inception's final layer working so cloned the model repo myself. Removing it solved the issue. Thanks a lot."]}, {"number": 7698, "title": "Quantize nodes via Transform Graph tool cause error", "body": "I am trying to quantize my model by following this [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md)\r\n\r\nI have faced with issue **No attr named 'T' in NodeDef:** when I run **transform_graph** tool with **quantize_nodes** option both for my model and example model (Inception V3).\r\n\r\n```\r\npath/to/transform_graph --in_graph=model.pb --out_graph=optimized.pb --inputs='input:0' --outputs='regression:0,classification:0' --transforms='quantize_weights quantize_nodes'\r\n```\r\n\r\nHere is relevant part of output:\r\n\r\n```\r\n2017-02-20 17:14:19.944778: I tensorflow/tools/graph_transforms/transform_graph.cc:257] Applying quantize_nodes\r\n2017-02-20 17:14:20.599637: E tensorflow/tools/graph_transforms/transform_graph.cc:203] No attr named 'T' in NodeDef:\r\n\t [[Node: pool = MaxPool[ksize=[1, 3, 3, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](pool/control_dependency)]]\r\n```\r\n\r\n### Environment info\r\n\r\nDoesn't seems too important. Tested on two machines with different configurations (referred as M for Mac and U for Ubuntu below).\r\n \r\nOperating System:\r\nM: macOS Sierra 10.12.3\r\nU: Ubuntu 16.04.2 LTS x86_64\r\n\r\nTF has been installed from source. Commit hash:\r\nM: b6f16b8166e3a7761f607be66d46acbd37dfaf43\r\nU: c56c873fbaf976d26d487ad57c8efbc87f05331c\r\n\r\nBazel version / build label\r\nM: 0.4.4-homebrew\r\nU: 0.4.4\r\n\r\n### Minimal reproducible example\r\nDownload & unpack inception V3. Run the tool (modify path to tool and model if necessary).\r\n\r\n\r\n```\r\ncurl http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz -o /tmp/inceptionv3.tgz\r\ntar xzf /tmp/inceptionv3.tgz -C /tmp/\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/classify_image_graph_def.pb --out_graph=optimized.pb --inputs='input:0' --outputs='regression:0,classification:0' --transforms='quantize_weights quantize_nodes'\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nI have tried to run quantisation on different OS (Mac and Ubuntu) and slightly different revisions of master. Also, initially, I have tried to quantize nodes of my own model, not Inception. In all cases result is the same: error.", "comments": ["@petewarden, could you please take a look?", "@Gubarev, \r\nyou can try this :\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=/tmp/classify_image_graph_def.pb --out_graph=optimized.pb --inputs='input:0' --outputs='regression:0,classification:0' --transforms='**add_default_attributes** quantize_weights quantize_nodes'\r\n", "Now I see that README I referred has been updated since I opened my issue. With **add_default_attributes** option the transform_graph does not produce errors. Thanks to @scangh for noticing! So this issue is appeared to be README issue and already resolved."]}, {"number": 7697, "title": "How to upgrade Tensorflow from V0.10.0 to V1.0 ?", "body": "Hi ,dear\r\n    I installed tensorflow V0.10.0 by compiling source code in 2016.   \r\n    Now Is there an simplest way for me to upgrade tensorflow from V0.10.0 to V1.0?\r\n    Thank you & Best Regards,\r\n\r\n### Environment info\r\nOperating System: Ubuntu16.04\r\n\r\nsyj@syj-dl:~/tensorflow$ cat /etc/issue\r\nUbuntu 16.04 LTS \\n \\l\r\n\r\nsyj@syj-dl:~/tensorflow$ which python\r\n/home/syj/anaconda2/bin/python\r\n\r\nsyj@syj-dl:~/tensorflow$ python --version\r\nPython 2.7.12 :: Anaconda custom (64-bit)\r\n\r\nInstalled version of CUDA and cuDNN: CUDA8.0 CUDNN5.0\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nsyj@syj-dl:/usr/local/cuda-8.0/lib64$ ls libcud*\r\nlibcudadevrt.a    libcudart.so.8.0.27  libcudnn.so.5\r\nlibcudart.so      libcudart_static.a   libcudnn.so.5.1.5\r\nlibcudart.so.8.0  libcudnn.so          libcudnn_static.a\r\n\r\n\r\nsyj@syj-dl:~/tensorflow/tensorflow/models/rnn$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.8.0 locally\r\n0.10.0\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nsyj@syj-dl:~/tensorflow$ git rev-parse HEAD\r\n69d67717f7b3da135b8904822838823658da183f\r\n\r\n2. The output of `bazel version`\r\nsyj@syj-dl:~$ bazel version\r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392\r\n\r\n", "comments": ["There was an email to TensorFlow discuss mailing list with upgrade instructions, cc @aselle in case there's a more canonical place", "@yaroslavvb  Thank you !\r\n\r\nI have successfully installed tensorflow v1.0 by  following the 'Virtualenv installation' section of [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md](url)\r\n\r\nThank you\r\n`\r\nsyj@syj-dl:~$ virtualenv --system-site-packages ~/tensorflow-v1.0\r\n\r\nRunning virtualenv with interpreter /home/syj/anaconda2/bin/python2\r\nNew python executable in /home/syj/tensorflow-v1.0/bin/python2\r\nAlso creating executable in /home/syj/tensorflow-v1.0/bin/python\r\nInstalling setuptools, pkg_resources, pip, wheel...done.\r\n\r\nsyj@syj-dl:~$ source ~/tensorflow-v1.0/bin/activate\r\n(tensorflow-v1.0) syj@syj-dl:~$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl\r\n\r\n(tensorflow-v1.0) syj@syj-dl:~$ pip install --upgrade $TF_BINARY_URL\r\nCollecting tensorflow-gpu==1.0.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp27-none-linux_x86_64.whl (95.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 95.0MB 19kB/s \r\nRequirement already up-to-date: mock>=2.0.0 in ./anaconda2/lib/python2.7/site-packages (from tensorflow-gpu==1.0.0)\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==1.0.0)\r\n  Downloading numpy-1.12.0-cp27-cp27mu-manylinux1_x86_64.whl (16.5MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.5MB 38kB/s \r\nCollecting protobuf>=3.1.0 (from tensorflow-gpu==1.0.0)\r\n  Downloading protobuf-3.2.0-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.6MB 51kB/s \r\nRequirement already up-to-date: wheel in ./tensorflow-v1.0/lib/python2.7/site-packages (from tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: six>=1.10.0 in ./tensorflow-v1.0/lib/python2.7/site-packages (from tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: pbr>=0.11 in ./anaconda2/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: setuptools in ./tensorflow-v1.0/lib/python2.7/site-packages (from protobuf>=3.1.0->tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: appdirs>=1.4.0 in ./tensorflow-v1.0/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: packaging>=16.8 in ./tensorflow-v1.0/lib/python2.7/site-packages (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.0)\r\nRequirement already up-to-date: pyparsing in ./tensorflow-v1.0/lib/python2.7/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.0)\r\nInstalling collected packages: numpy, protobuf, tensorflow-gpu\r\n  Found existing installation: numpy 1.10.4\r\n    Not uninstalling numpy at /home/syj/anaconda2/lib/python2.7/site-packages, outside environment /home/syj/tensorflow-v1.0\r\n  Found existing installation: protobuf 3.0.0b2\r\n    Not uninstalling protobuf at /home/syj/anaconda2/lib/python2.7/site-packages, outside environment /home/syj/tensorflow-v1.0\r\nSuccessfully installed numpy-1.12.0 protobuf-3.2.0 tensorflow-gpu-1.0.0\r\n(tensorflow-v1.0) syj@syj-dl:~$ deactivate\r\nsyj@syj-dl:~$ source ~/tensorflow-v1.0/bin/activate\r\n(tensorflow-v1.0) syj@syj-dl:~$ python\r\nPython 2.7.12 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:42:40) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n>>> tf.__version__\r\n'1.0.0'\r\n>>> \r\n`\r\n"]}, {"number": 7696, "title": "freeze_graph inspect_checkpoint,  Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNothing directly applicable to the observed error.\r\n\r\n### Environment info\r\nTF 0.12.1, Windows 10/64, Python 3.5/64\r\n\r\nInstalled version of CUDA and cuDNN: \r\n8.0, 5.1\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n**inspect_checkpoint**\r\n\r\nlib\\python\\python lib\\tensorflow_cpu\\tensorflow\\python\\tools\\inspect_checkpoint.py --file_name model.ckpt-250514.data-00000-of-00001 --tensor_name-''\r\n\r\n2017-02-20 08:23:19.291800: W c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\cpu\\os\\windows\\tensorflow\\core\\util\\tensor_slice_reader.cc:95] Could not open model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\nUnable to open table file model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n\r\n\r\n\r\n**freeze_graph**\r\n\r\nSimilar error\r\ntensorflow.python.framework.errors_impl.DataLossError: Unable to open table file model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n         [[Node: save/RestoreV2_445 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_445/tensor_names, save/RestoreV2_445/shape_and_slices)]]\r\n\r\nDataLossError (see above for traceback): Unable to open table file C:\\g\\vx\\data\\jhole\\model\\model.ckpt-250514.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\r\n         [[Node: save/RestoreV2_780 = RestoreV2[dtypes=[DT_INT64], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_780/tensor_names, save/RestoreV2_780/shape_and_slices)]]\r\n\r\n### What other attempted solutions have you tried?\r\n1. A few different checkpoint files.\r\n2. Tried freeze_graph using TF 1.0, same result\r\n\r\n### Logs or other output that would be helpful\r\nMessages similar \"Similar error\" above is repeated many times.\r\n", "comments": ["similar issue was solved here http://stackoverflow.com/questions/41048819/how-to-restore-a-model-by-filename-in-tensorflow-r12", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Anyone who solve this problem?", "just regard the prefix as checkpoint,  like:\r\nlib\\python\\python lib\\tensorflow_cpu\\tensorflow\\python\\tools\\inspect_checkpoint.py --file_name model.ckpt-250514", "I saved the model using V2 format, which gave me three files:\r\n\r\n- model-10000.data-00000-of-00001\r\n- model-10000.index\r\n- model-10000.meta\r\n\r\nAs @tangyanlin metioned, use the prefix. My input parameter is `--input_checkpoint=checkpoints/model-10000` (NOT including the period) which works.\r\n", "Get this when I train model\r\n\r\nINFO:tensorflow:Summary name /clone_loss is illegal; using clone_loss instead.\r\n/Library/Python/2.7/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\n\r\nthen can't freeze model\r\n\r\npython object_detection/export_inference_graph.py --input_type image_tensor --pipeline_config_path models/faster_rcnn_numobj.config --trained_checkpoint_prefix models/train/model.ckpt-* --output_directory models/result/\r\n\r\n2017-11-19 22:26:38.039878: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open train/model.ckpt-0.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?", "Thank you @LingjiaDeng  ", "> I saved the model using V2 format, which gave me three files:\r\n> \r\n> * model-10000.data-00000-of-00001\r\n> * model-10000.index\r\n> * model-10000.meta\r\n> \r\n> As @tangyanlin metioned, use the prefix. My input parameter is `--input_checkpoint=checkpoints/model-10000` (NOT including the period) which works.\r\n\r\nThank you! And I think it's import to check whether GPU has enough space. I killed other works on NVIDIA and did what you said and I succeeded!", "ty to @tangyanlin @LingjiaDeng \r\nAvoid doing this on the line on the corresponding config file that you are using:\r\n```\r\nfine_tune_checkpoint: \"/home/ubuntu/models/ssd_inception_v2_coco_2018_01_28/model.ckpt.data-00000-of-00001\"\r\n``` \r\nas this gives you the error. I **solved** mine using this:\r\n```\r\nfine_tune_checkpoint: \"/home/ubuntu/models/ssd_inception_v2_coco_2018_01_28/model.ckpt\"\r\n```", "i can't find any solution for that \r\n", "> ty to @tangyanlin @LingjiaDeng\r\n> Avoid doing this on the line on the corresponding config file that you are using:\r\n> \r\n> ```\r\n> fine_tune_checkpoint: \"/home/ubuntu/models/ssd_inception_v2_coco_2018_01_28/model.ckpt.data-00000-of-00001\"\r\n> ```\r\n> \r\n> as this gives you the error. I **solved** mine using this:\r\n> \r\n> ```\r\n> fine_tune_checkpoint: \"/home/ubuntu/models/ssd_inception_v2_coco_2018_01_28/model.ckpt\"\r\n> ```\r\n\r\nI changed `places2.ckpt-6666.data-00001-of-00002` to `places2.ckpt-6666` and it worked."]}, {"number": 7695, "title": "How to use the Distributed Tensorflow", "body": "I have three node, which two use the CPU-mode tensorflow, and the left use the GPU-mode tensorflow.\r\nSome website may just use the official example, but I don't know how to execute the python code on the terminal, some use themselves example. but I cannot run it successful. Can you tell me how to write the python code and how to execute it on my terminal. Should I use the docker?\r\nAnyone can teach me?", "comments": ["This seems more appropriate for stackoverflow, this list is for bugs/feature in Tensorflow"]}, {"number": 7694, "title": "TypeError: run() got an unexpected keyword argument 'feedis_dict'", "body": "I get this error when trying to add the feedis_dict to a self.sess.run command:\r\n\r\n`Traceback (most recent call last):\r\n  File \"spritegen.py\", line 71, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"spritegen.py\", line 54, in main\r\n    dcgan.train(FLAGS)\r\n  File \"/home/lewis/Documents/Sprite Generator/Sprite-Generator/dcgan.py\", line 211, in train\r\n    summary_str = self.sess.run([dis_optim, self.dis_sum],feedis_dict = {self.inputs: batch_images, self.z: batch_z})\r\nTypeError: run() got an unexpected keyword argument 'feedis_dict'\r\n`\r\nAny suggestions as to why feedis_dict is unexpected ? I believe im using tensorflow gpu 1.0.0\r\n", "comments": ["This question may be more appropriate for stackoverflow"]}, {"number": 7693, "title": "How to build tensorflow to use SSE3 instructions?", "body": "When I am running tensorflow code, it prompts that `The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.`. I have installed tensorflow using pip and don't know how to deal with that. Any one can help with me?", "comments": ["I have the same issue. But I also do not how to solve it.\r\n\r\nMy warning is as follows:\r\n\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n```\r\n\r\nAnyone can fix it?", "@sydney0zq Make sure you enable `--config=opt` in compile time.", "Am facing the same issue with tensor flow. can you please elaborate how to use above solution", "Me too", "Same here:\r\n\r\n```\r\n$ python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello World!')\r\n>>> sess = tf.Session()\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n>>> print(sess.run(hello))\r\nb'Hello World!'\r\n```", "The binary distribution of tensorflow-gpu, which is provided through pip, is compiled with little support of advanced features of main stream cpu for maximum compatibiliy. So in order to use these features, you have to download the source from github and compile it youself following the steps illustrated by the official site of tensorflow. That's all.", "Yeah I was hoping I could build it within 'Bash for Windows' but no luck... stuck at \"Extracting Bazel installation...\"", "I had encountered that problem before in linux. You may try to reinstall bazel and do it again. It will work, but patience is needed. Linux subsystem in windows is currently nothing different from the ordinary ubuntu.", "Patience?  Oh dear, then I'm really in trouble.\r\n\r\nOK, I'll try - thanks for your help.\r\n\r\nOn Wed, 2017-02-22 at 03:02 -0800, cjliux wrote:\r\n\r\nI had encountered that problem before in linux. You may try to reinstall bazel using apt-get and do it again. It will work, but patience is needed.\r\n\r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/7693#issuecomment-281637440>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKrzBnnzb1Ruz0c8QYCQ94oOrYGej8o0ks5rfBW7gaJpZM4MGD9h>.\r\n", "Successfully built it from source (Github Master branch) in my Ubuntu machine: (No more SSE3 warnings...)\r\nhttps://youtu.be/1Gd6e5BLkwo   ", "@melvincabatuan Do you notice any significant improvement in performance. I'm thinking of building from source as well.", "For me, not much improvement actually. But for larger scale training, it will be significant perhaps..."]}]