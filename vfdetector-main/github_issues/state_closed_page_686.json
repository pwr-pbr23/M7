[{"number": 33014, "title": "tflite.allocate_tensors() fails after changing input size", "body": "(using tensorflow 1.14.0)\r\n\r\nI'm trying to use a tflite model to do inference on a batch. I use the following code:\r\n\r\n```python\r\ninterpreter = tf.lite.Interpreter(model_path=model_path)\r\ninput_details = interpreter.get_input_details()\r\ninterpreter.resize_tensor_input(input_details[0][\"index\"], [batch_size, 513, 513, 3])\r\ninterpreter.allocate_tensors()\r\n```\r\n\r\nThe code crashes and gives the following error:\r\n```python\r\nRuntimeError: tensorflow/lite/kernels/reshape.cc:58 num_input_elements != num_output_elements (1579014 != 789507)Node number 0 (RESHAPE) failed to prepare.\r\n````\r\n\r\nWhen looking at the output details, it still has the shape ```[1, 513, 513, output_channels]``` and not ```[batch_size, 513, 513, output_channels]``` as I would expect.\r\n\r\nAny ideas?\r\n", "comments": ["Stumbled upon this a few days ago,\r\nwhen you convert your model you need to specify your input shape as `[None, 512, 512, 3]`.\r\n", "> Stumbled upon this a few days ago,\r\n> when you convert your model you need to specify your input shape as `[None, 512, 512, 3]`.\r\n\r\nDo you mean invoking `interpreter.resize_tensor_input(input_details[0][\"index\"], [None, 513, 513, 3])\r\n`? Because this just raises an error: `TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'`.", "No.\r\nWhen you define your model, you need to use `tf.placeholder(..., shape=(None, 512, 512, 3))`", "This is a pre-trained model that I receive as a black box though", "Is there a solution for this? I haven't found anywhere and I'm having exactly the same problem regarding reshape for the batch size\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/33014#issue-501996008", "I experienced the same issue, here is an example to reproduce the error and view a similar model that does work.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport os.path\r\n\r\nDESIRED_OUTCOME = \"Fail\" # Change to Success if you want to see a similar case that succeeds\r\n\r\n#Build Model\r\nif DESIRED_OUTCOME == \"Success\":\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(10, activation='relu', input_shape=(28*28,)),\r\n    keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n  model.build([None, 28*28])\r\nelse:\r\n  model = tf.keras.models.Sequential([\r\n    keras.layers.Dense(10, activation='relu', input_shape=(28,28)),\r\n    keras.layers.Reshape((10*28,)),\r\n    keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n  model.build([None, 28,28])\r\n\r\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n#Train Model\r\n(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n\r\ntrain_labels = train_labels[:1000]\r\ntest_labels = test_labels[:1000]\r\n\r\nif DESIRED_OUTCOME == \"Success\":\r\n  train_images = train_images[:1000].reshape(-1, 28*28) / 255.0\r\n  test_images = test_images[:1000].reshape(-1, 28*28) / 255.0\r\nelse:\r\n  train_images = train_images[:1000].reshape(-1, 28,28) / 255.0\r\n  test_images = test_images[:1000].reshape(-1, 28,28) / 255.0\r\n\r\nmodel.fit(train_images,\r\n          train_labels,\r\n          epochs=10,\r\n          validation_data=(test_images,test_labels))\r\n\r\ntf.saved_model.save(model, \"test\")\r\n\r\n#Convert Model\r\nflowers_model = tf.keras.models.load_model(\"test\")\r\nTFLITE_MODEL = \"test.tflite\"\r\nTFLITE_QUANT_MODEL = \"test_quant.tflite\"\r\n\r\n# Get the concrete function from the Keras model.\r\nrun_model = tf.function(lambda x : flowers_model(x))\r\n\r\n# Save the concrete function.\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec(flowers_model.inputs[0].shape, flowers_model.inputs[0].dtype)\r\n)\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverted_tflite_model = converter.convert()\r\nopen(TFLITE_MODEL, \"wb\").write(converted_tflite_model)\r\n\r\n# Convert the model to quantized version with post-training quantization\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\nopen(TFLITE_QUANT_MODEL, \"wb\").write(tflite_quant_model)\r\n\r\n#Resize\r\ntflite_interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL)\r\n\r\ninput_details = tflite_interpreter.get_input_details()\r\noutput_details = tflite_interpreter.get_output_details()\r\n\r\nif DESIRED_OUTCOME == \"Success\":\r\n  tflite_interpreter.resize_tensor_input(input_details[0]['index'], (5, 784))\r\n  tflite_interpreter.resize_tensor_input(output_details[0]['index'], (5, 10))\r\nelse:\r\n  tflite_interpreter.resize_tensor_input(input_details[0]['index'], (5, 28, 28))\r\n  tflite_interpreter.resize_tensor_input(output_details[0]['index'], (5, 10))\r\n\r\ntflite_interpreter.allocate_tensors()\r\n\r\ninput_details = tflite_interpreter.get_input_details()\r\noutput_details = tflite_interpreter.get_output_details()\r\n\r\nprint(\"== Input details ==\")\r\nprint(\"name:\", input_details[0]['name'])\r\nprint(\"shape:\", input_details[0]['shape'])\r\nprint(\"type:\", input_details[0]['dtype'])\r\n\r\nprint(\"\\n== Output details ==\")\r\nprint(\"name:\", output_details[0]['name'])\r\nprint(\"shape:\", output_details[0]['shape'])\r\nprint(\"type:\", output_details[0]['dtype'])\r\n\r\n```", "Using TF 1.12 I experienced the same issue. [https://github.com/tensorflow/tensorflow/issues/33711](33711) After look into issue 33711. I noticed that dynamic resizing only supported in tensorflow v2. So I changed TF to 2.1.0 and used convert saved_model(saved by TF1.12) to tflite model by using tflite_convert --saved_model_dir=/path/to/savedModel/ --output_file=tfv2Model.tflite. After convert the resize tensor works by using TF v2.0's API", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33014\">No</a>\n"]}, {"number": 33013, "title": "ERROR: Config value android_arm64 is not defined in any .rc file", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.15\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:virtual.pip\r\n- Bazel version (if compiling from source):0.26\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n\r\n\r\nwhen run this:\r\n\r\nbazel build -c opt \\\r\n  --config=android_arm64 \\\r\n  --cxxopt='--std=c++11' \\\r\n  //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:run_eval\r\n\r\n\r\nget error:\r\n\r\nERROR: Config value android_arm64 is not defined in any .rc file\r\n\r\n\r\nhow can I fix it?\r\n\r\n", "comments": ["Have you run the `/.configure` script from the root directory, and set up your build environment against your local Android SDK/NDK directories?", "The `android_arm64` config should be defined in the root TensorFlow checkout's `.bazelrc` file, see https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L9.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33013\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33013\">No</a>\n"]}, {"number": 33012, "title": "tf.function fails with tf.ragged.boolean_mask", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: NO GPU\r\n\r\n**Describe the current behavior**\r\nA function containing `tf.ragged.boolean_mask` and decorated with `tf.function` works on first execution but fails when executed with different inputs. Setting `experimental_relax_shapes=True` does not help.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n## Define some RaggedTensors\r\nprint(\"`a2` and `b2` are ragged tensors with batch-length = 2\")\r\na2 = tf.ragged.constant([[1, 2, 3], [4, 5]], dtype=tf.float32, \r\n                       ragged_rank=1, inner_shape=tuple())\r\nprint(\"a2 =\", a2)\r\n\r\nb2 = tf.ragged.constant([[1], [2, 3]], dtype=tf.float32, \r\n                       ragged_rank=1, inner_shape=tuple())\r\nprint(\"b2 =\", b2)\r\n\r\nprint(\"`a3` and `b3` are ragged tensors with batch-length = 3\")\r\na3 = tf.ragged.constant([[1, 2, 3], [4, 5], [3]], dtype=tf.float32, \r\n                       ragged_rank=1, inner_shape=tuple())\r\nprint(\"a3 =\", a3)\r\n\r\nb3 = tf.ragged.constant([[1], [2, 3], [5]], dtype=tf.float32, \r\n                       ragged_rank=1, inner_shape=tuple())\r\nprint(\"b3 =\", b3)\r\n\r\n## Define a function\r\nprint(\"We define a function `fun` with `tf.ragged.boolean_mask`.\")\r\ndef fun(x):\r\n    maximums = tf.reduce_max(x, axis=1)\r\n    mask = maximums > 4\r\n    selection = tf.ragged.boolean_mask(x, mask)\r\n    return tf.reduce_sum(selection)\r\n\r\n## Run the function in eager-mode\r\nprint(\"Running in eager-mode.\")\r\nprint(\"fun(a2) =\", fun(a2))\r\nprint(\"fun(a3) =\", fun(a3))\r\nprint(\"fun(b2) =\", fun(b2))\r\nprint(\"fun(b3) =\", fun(b3))\r\n\r\n## Now running the same in graph-mode\r\nfun = tf.function(fun, experimental_relax_shapes=True)\r\n\r\nprint(\"Running in graph-mode.\")\r\nprint(\"fun(a2) =\", fun(a2))\r\nprint(\"fun(a3) =\", fun(a3))\r\nprint(\"fun(b2) =\", fun(b2))\r\nprint(\"fun(b3) =\", fun(b3))\r\n```\r\n**Output of the code**\r\n```\r\n`a2` and `b2` are ragged tensors with batch-length = 2\r\n2019-10-04 10:19:32.835325: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\na2 = <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0]]>\r\nb2 = <tf.RaggedTensor [[1.0], [2.0, 3.0]]>\r\n`a3` and `b3` are ragged tensors with batch-length = 3\r\na3 = <tf.RaggedTensor [[1.0, 2.0, 3.0], [4.0, 5.0], [3.0]]>\r\nb3 = <tf.RaggedTensor [[1.0], [2.0, 3.0], [5.0]]>\r\nWe define a function `fun` with `tf.ragged.boolean_mask`.\r\nRunning in eager-mode.\r\nfun(a2) = tf.Tensor(9.0, shape=(), dtype=float32)\r\nfun(a3) = tf.Tensor(9.0, shape=(), dtype=float32)\r\nfun(b2) = tf.Tensor(0.0, shape=(), dtype=float32)\r\nfun(b3) = tf.Tensor(5.0, shape=(), dtype=float32)\r\nRunning in graph-mode.\r\nfun(a2) = tf.Tensor(9.0, shape=(), dtype=float32)\r\nfun(a3) = tf.Tensor(9.0, shape=(), dtype=float32)\r\n2019-10-04 10:19:33.187674: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Input to reshape is a tensor with 3 values, but the requested shape has 5\r\n         [[{{node RaggedMask/RaggedMask/boolean_mask/Reshape}}]]\r\nTraceback (most recent call last):\r\n  File \"Bug in tensorflow tf-function on ragged-boolean_mask.py\", line 43, in <module>\r\n    print(\"fun(b2) =\", fun(b2))\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 494, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 3 values, but the requested shape has 5\r\n         [[node RaggedMask/RaggedMask/boolean_mask/Reshape (defined at D:\\python_projects\\workon_hrab2\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1751) ]] [Op:__inference_fun_1126]\r\n\r\nFunction call stack:\r\nfun\r\n\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.0.0-rc2, 2.0.0-dev20191002 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2229ff515d40f06a322a1df614c564e7/untitled246.ipynb). Thanks!", "@PistaSaki Looks like this was resolved in `tf-nightly`. I was not able to reproduce the issue with `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/129fd452f492f59d3789779155dbbc6d/untitled246.ipynb). Thanks!\r\n\r\nI am closing this issue. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33012\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33012\">No</a>\n"]}, {"number": 33011, "title": "Using Tflite in Java for getting image as float array", "body": "I have own tranied model and it works right(already implemented on Swift). My inputs for Tflite in Java - two float one-dimensional arrays with size 1*512*512*3 with values from 0.0 to 1.0. They are images and I have to insert them sequentially in Tflite and get new image as one-dimensional float array with the same size. The Tflite docs give only common info about input and output and tell that it should be either ByteBuffer or one-dimensional or multidimensional array of primitive. In examples and internet I have found the task with classification of objects from bitmap or I have found the example with multidimensional array as inputs and outputs, but it seems they are not my cases. The main problem is - I always get bad result because I definitely can\u2019t figure out how to properly configure the inputs and outputs, because he documentation gives very little information and I do not see my case in all examples because I have two float one-dimensional arrays as inputs and float one-dimensional array as output as I said previously.\r\n\r\nI try to implement in Java on Android Studio 3.4 on Ubuntu 18.04.\r\n\r\nNote: Also I use C++ code in my project. And for set up image from float[] I convert float[] to opentCv Mat, after this I convert Mat into Bitmap. For all this I have method with name:\r\n\r\ninitializeSecond()\r\nI have checked this method with other images as float[] and it works fine.\r\n\r\nSo I tried many cases and I will show the main of them:\r\n\r\n1)\r\n\r\nfloat masked[] = coreResult.getMasked();\r\n            float inverted[] = coreResult.getInverted();\r\n            float output[]= new float[1*512*512*3];\r\n            Object[] inputs = new Object[]{masked, inverted};\r\n            Map<Integer, Object> outputs = new HashMap();\r\n            outputs.put(0, output);\r\n            interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n            initializeSecond(coreRequest, output);\r\n2)\r\n\r\nfloat masked[] = coreResult.getMasked();\r\n            float inverted[] = coreResult.getInverted();\r\n            float output[]= new float[1*512*512*3];\r\n            ByteBuffer byteBufferMasked = ByteBuffer.allocateDirect(masked.length*4);\r\n            byteBufferMasked.order(ByteOrder.nativeOrder());\r\n            byteBufferMasked.asFloatBuffer().put(masked);\r\n\r\n            ByteBuffer byteBufferInverted = ByteBuffer.allocateDirect(inverted.length*4);\r\n            byteBufferInverted.order(ByteOrder.nativeOrder());\r\n            byteBufferInverted.asFloatBuffer().put(inverted);\r\n            Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};\r\n            Map<Integer, Object> outputs = new HashMap();\r\n            outputs.put(0, output);\r\n            interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n            initializeSecond(coreRequest, output);\r\n3)\r\n\r\nfloat masked[] = coreResult.getMasked();\r\n            float inverted[] = coreResult.getInverted();\r\n            float output[]= new float[1*512*512*3];\r\n            ByteBuffer byteBufferOutput = ByteBuffer.allocateDirect(output.length*4);\r\n            byteBufferOutput.order(ByteOrder.nativeOrder());\r\n            ByteBuffer byteBufferMasked = ByteBuffer.allocateDirect(masked.length*4);\r\n            byteBufferMasked.order(ByteOrder.nativeOrder());\r\n            byteBufferMasked.asFloatBuffer().put(masked);\r\n\r\n            ByteBuffer byteBufferInverted = ByteBuffer.allocateDirect(inverted.length*4);\r\n            byteBufferInverted.order(ByteOrder.nativeOrder());\r\n            byteBufferInverted.asFloatBuffer().put(inverted);\r\n            Object[] inputs = new Object[]{byteBufferMasked, byteBufferInverted};\r\n            Map<Integer, Object> outputs = new HashMap();\r\n            outputs.put(0, byteBufferOutput);\r\n            interpreter.runForMultipleInputsOutputs(inputs, outputs);\r\n            output = byteBufferOutput.asFloatBuffer().array();\r\n            initializeSecond(coreRequest, output);\r\n4) I put in code from 3 case\r\n\r\nfor (int i = 0; i < output.length; i++) {\r\n                output[i] = byteBufferOutput.getFloat(i);\r\n            }\r\ninstead\r\n\r\n output = byteBufferOutput.asFloatBuffer().array();\r\nThe results of cases:\r\n\r\n1) Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/conv.cc:235 input->dims->size != 4 (1 != 4) Node number 2 (CONV_2D) failed to prepare.\r\n\r\n2) Cannot copy between a TensorFlowLite tensor with shape [1, 512, 512, 3] and a Java object with shape [786432].\r\n\r\n3) java.lang.UnsupportedOperationException. It is the result of byteBufferOutput.asFloatBuffer().array(). It seems I can not get the array by this\r\n\r\n4) I have not errors, but it works very slowly and I have the bad result: image with black background and colorful dots", "comments": ["(3) and (4) both look reasonable. I actually have an in-flight CL that will allow feeding/fetching FloatBuffers directly, which should make interop a little less awkward.  You can also use the get(float[]) overload of FloatBuffer to put into an output float array.\r\n\r\nHowever, one thing you should try doing is caching the ByteBuffers, that is, only create them once, and reuse them across inference calls.", "When I try byteBufferOutput.asFloatBuffer().get(output)  I get BufferUnderflowException. What CL do you have? And how can I try to caching the ByteBuffers if I use three in one moment(for two inputs and one output)? Or do you mean than I should use consistently buffer.run(input, output) instead runForMultipleInputsOutputs?", "> And how can I try to caching the ByteBuffers if I use three in one moment(for two inputs and one output)? \r\n\r\nI mean, if this is running inside a class, make those scratch buffers member variables, and reuse them each time you run inference. Or put them in some kind of context-like class that can be reused. That way you avoid unnecessary (re)allocations every single inference call.", "For the BufferUnderflowException, make sure you rewind your `byteBufferOutput` before you extract the array from it, e.g.,\r\n```\r\nbyteBufferOutput.rewind();\r\noutput = byteBufferOutput.asFloatBuffer().array();\r\n```", "We'll be sure to update the documentation to make this behavior more clear. Cheers.\r\n", "About byteBufferOutput.rewind(); Yes! it works. Thanks! I also have found the solution - use `byteBufferOutput.flip();\r\n            for (int i = 0; i < masked.length; i++) {\r\n                output[i] = byteBufferOutput.getFloat();\r\n            }  `  Now my code above works I get the result and convert it as bitmap. One problem - very slowly. About 15 sec. I need at least 4-5. Okay, I will think about context class for them. For example in My MainActivity. But anyway it is about creating them and allocate, righ? And I need to use three ByteBuffer in one moment and every time put new arrays in them(it's not fast too). I understand that I work with three arrays and they are big arrays but can I do it more fastly also in other ways? Need I use ByteArray anyway? Can I put as inputs my array considering that Tflite doc says tha input may be primitive array.", "What device are you running this on? Have you tried using our [benchmark tooling](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) to profile the model itself? It's possible this model isn't well-suited for mobile, or it may benefit from using an accelerator like the [GPU](https://www.tensorflow.org/lite/performance/gpu_advanced) or from using [quantization](https://www.tensorflow.org/lite/performance/model_optimization). See also [this guide on performance](https://www.tensorflow.org/lite/performance/best_practices).", "For Xiaomi Mi6 it is 15 sec, for Huawei Nexus 6P - 30 sec. It is really bad. Okay I will talk with developer of model about its performance. So I will investigate the guide on performance too. Thanks! But I need to be sure that I make all right in my Java code. And besides your case with creating buffers in context class, it seems I can not do anything. "]}, {"number": 33010, "title": "Add tests in TFLite micro for Relu/Relu6 Int8/Uint8/Float", "body": "Adding reference code and tests for Relu and Relu6 for  Int8/Uint8/Float", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33010) for more info**.\n\n<!-- need_author_cla -->", "@giuseros thank you for your contribution, please sign CLA", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33010) for more info**.\n\n<!-- ok -->", "Can someone please re-run this tests? They should not be affected by this change.", "Ready to merge", "Here is the internal error,please take a look.\r\n`ld.lld: error: undefined symbol: tflite::ops::micro::Register_RELU()\r\n>>> referenced by all_ops_resolver.cc\r\n>>>               tensorflow/lite/experimental/micro/kernels/_objs/all_ops_resolver/all_ops_resolver.o:(tflite::ops::micro::AllOpsResolver::AllOpsResolver())\r\n\r\nld.lld: error: undefined symbol: tflite::ops::micro::Register_RELU6()\r\n>>> referenced by all_ops_resolver.cc\r\n>>>              tensorflow/lite/experimental/micro/kernels/_objs/all_ops_resolver/all_ops_resolver.o:(tflite::ops::micro::AllOpsResolver::AllOpsResolver())\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)`\r\n", "Hi @rthadur , \r\nIt works when I compile (and run) with:\r\n$ make -f tensorflow/lite/experimental/micro/tools/make/Makefile test\r\n\r\nWas this a Bazel failure? I added the \"activations.cc\" file into tensorflow/lite/experimental/micro/kernels/BUILD, hoping that this was the issue. \r\n\r\nAlternatively, how can I reproduce this?\r\n\r\nThanks,\r\nGiuseppe", "Ready to merge"]}, {"number": 33009, "title": "Memory leak ", "body": "**System information**\r\n**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):** Yes, see below\r\n**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Ubuntu 18.04 (also demonstrated in Windows 7)\r\n**- TensorFlow installed from (source or binary):** Binary\r\n**- TensorFlow version (use command below):** v2.0.0-rc2-26-g64c3d38 2.0.0 (problem disappears on v1.14.0-rc1-22-gaf24dc91b5 1.14.0)\r\n**- Python version:** 3.6.8\r\n**- CUDA/cuDNN version:** Cuda v10.0, CuDNN v7.6.2.24\r\n**- GPU model and memory:** Nvidia GeForce 840M, but problem persists in non-GPU version\r\n\r\n**Describe the current behavior**\r\nWhen creating a trivially simple model and then entering a loop that calls predict() with dummy input, memory consumption increases indefinitely over time. On my system, a model with a single hidden layer of only 32 nodes will consume all available system RAM (>10gb) after only 10 minutes. The problem happens on v2.0 (GPU or CPU) of tensorflow, but disappears when running identical code on v1.14.\r\n\r\n**Describe the expected behavior**\r\nExpect memory consumption to quickly stabilize but it never does.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.layers import Dense\r\nimport numpy as np\r\n\r\n# Build model\r\nIn = Input(shape=(10,))\r\nx = Dense(32)(In)\r\nOut = Dense(2)(x)\r\n\r\n# Compile\r\nmodel = Model(inputs=In, outputs=Out)\r\nmodel.compile(optimizer='adam', loss='mse')        \r\n\r\n# Create dummy input data\r\nfake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))\r\n\r\nwhile True:\r\n    # Repeatedly predict:\r\n    model.predict(fake_data) # No memory leak if this line is replaced with \"pass\"\r\n```", "comments": ["I have also experienced this critical issue.", "I have managed to get around this error by using `model.predict_on_batch()` instead of `model.predict()`. This returns an object of type `<class 'tensorflow.python.framework.ops.EagerTensor'>` - not a numpy array as claimed in [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict_on_batch) - but it can be cast by calling `np.array(model.predict_on_batch(input_data))` to get the output I want. \r\n\r\nSide note: I also noticed a similar memory leak problem with calling `model.fit()` in a loop, albeit with a slower memory accumulation, but this can be fixed in a similar way using `model.train_on_batch()`.", "I think same error here.\r\nThis code (https://github.com/ipsec/dqn_tf2/blob/master/nchain-train.py) not finishing.", "@MProx ,\r\nCan you confirm if the issue is resolved with the workaround? Thanks!", "Hi. Yes it did. I tested it overnight with no leak. I'm not sure why the original functions do not work as expected, but the workaround has fixed my problem.", "This issue is NOT resolved. I use predict_on_batch and still get an OOM error after some time of processing data.", "Interesting. I think that OOM is telling you your GPU memory is being depleted, and that sounds like a different problem because mine was filling up system RAM, not GPU. Using predict_on_batch() and train_on_batch(), I ran ~800k iterations of my model over 10 hours last night, and it seemed to work fine.\r\n\r\nAnother thing is that I had to set the environment variable TF_FORCE_GPU_ALLOW_GROWTH=true to avoid tensorflow errors on startup. Maybe try that?", "Yeah, I don't have any critical errors when running on CPU. It's definitely GPU memory that's leaky. The GPU memory doesn't deplete immediately, but only after some number of calls to `predict_on_batch`. I am able to train using `fit` with no problems, which runs over that same data set over many epochs.", "I can confirm actually, that when run on CPU, the leak still persists.", "In that case I'm going to re-open this issue so that maybe the TF team can help you.", "Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/07da0eef3fbfbd8ff74e6c840ad5b6d5/33009.ipynb) of colab.Thanks!", "This is an issue with how certain object lifetimes are being managed in the tf function cache. Behind the scenes `Model.predict` is creating some functions which aren't being spun down properly, which is why there is a small but consistent leak each iteration. We are currently working on a fix.", "@robieta, would the particular problem that you mentioned affect GPU memory, or just system memory? Is there a Github Issue associated with it?", "I was also running a `predict` loop and getting OOM errors after many iterations. Using `predict_on_batch` instead eliminated this problem. It also sped up the prediction rate by a factor of 4. Thanks for the workaround!\r\n\r\n@ialdencoots, could you confirm whether or not you get an OOM error using the specific example code that @MProx provided in the issue description except using `predict_on_batch`?", "@novog Hmm, using the code that MProx provided with `predict_on_batch`, I do not get the OOM error. In my considerably less minimal code, where I am feeding inputs via a `TFRecordDataset` and definitely using `predict_on_batch`, I still encounter the memory leak. ", "Ok, so I messed around a bit w/ my model and I've narrowed down the problem a little bit more. When I load my trained model from an `.h5` file and run `predict_on_batch` repeatedly, I get the OOM error eventually. However, if I create a new model with the loaded model's inputs and outputs, I can `predict_on_batch` for my whole dataset without problem. If I then run `compile` on the model with an optimizer and loss, I get the OOM error again. So it seems to be a problem only for compiled models.", "@ialdencoots It sounds like you might be having a different OOM issue than the author of this issue and I were having. Incidentally, in my case, I am also loading my model from an .h5 file, and `predict_on_batch` fixed it. I would suggest trying to create a simpler version of your code that still has the problem to narrow down the cause and post it as a separate issue.\r\n\r\n@MProx, I would recommend leaving this issue open. Although we have a workaround, there is a defect in the TensorFlow version of Keras's `predict` function that should be fixed.", "In case it helps, this issue does not appear to be present in release 2.0.0.alpha0. You can confirm this by running @oanush's Colab with that version.", "Please switch to 2.0.0 (final release). You might need to upgrade `pip` and `setuptools` to get it, but 2.0 has been available for a while, no need to still be on alpha", "I know. But this problem, along with several others, is not present on alpha.", "Oh, you meant that the breakage happens in between alpha and rc2.\r\n\r\nIf it's not too much to ask, would you be willing to compare rc1?", "The issue does appear to be present in rc1.\r\nI went ahead and tried beta0 as well, and it does *not* seem to be present there.\r\n\r\nEDIT: It is also present in rc0, but not in beta1.", "Thank you. so the failure is in between beta1 and rc0.\r\n\r\nUnfortunately, we fast-forwarded the branch in between these two points, so probably we got the failure from master.\r\n\r\nOne more test, please. Is the issue also manifesting on latest master?", "Oh, I see @robieta already [pinned the issue](https://github.com/tensorflow/tensorflow/issues/33009#issuecomment-539226841). Apologies for the extra tests, we are expecting this to be fixed on master soon (if not already)", "No worries about the tests. I just hope a new release with the fix comes out soon.", "It's worth noting that the solution that I was referring to will only fix CPU OOM; it wouldn't explain a GPU leak.", "I was experiencing the same issue on the 2.0.0 docker (latest-gpu-py3) image; I changed to the nightly image (tensorflow/tensorflow:nightly-gpu-py3 to be exact) and the problem seems to be fixed, no more memory leak!  \r\n\r\nSo anyone else here looking for a workaround until the changes are reflected in the stable release, I'd try this out", "Avoiding `model.predict` and using `model.predict_on_batch` solves the Memory Error for me. Here's an example to create batched predictions on your test set. \r\n\r\n```\r\n# custom batched prediction loop to avoid memory leak issues for now in the model.predict call\r\ny_pred_probs = np.empty([len(X_test), VOCAB_SIZE], dtype=np.float32)  # pre-allocate required memory for array for efficiency\r\n\r\nBATCH_INDICES = np.arange(start=0, stop=len(X_test), step=BATCH_SIZE)  # row indices of batches\r\nBATCH_INDICES = np.append(BATCH_INDICES, len(X_test))  # add final batch_end row\r\n\r\nfor index in np.arange(len(BATCH_INDICES) - 1):\r\n    batch_start = BATCH_INDICES[index]  # first row of the batch\r\n    batch_end = BATCH_INDICES[index + 1]  # last row of the batch\r\n    y_pred_probs[batch_start:batch_end] = model.predict_on_batch(X_test[batch_start:batch_end])\r\n```\r\n\r\n(Note that if only pre-allocating the results array already results in a MemoryError then simply the array does not fit in your available memory regardless of the memory leak issue.)", "`model.predict_on_batch` solved this problem for me too", "Same issue with TF 2.0 stable\r\nSolved with `tf.compat.v1.disable_eager_execution()`", "I got the same problem both on rc2 and on official 2.0.0 release.\r\n`model.predict_on_batch` workaround worked for me", "For anyone visiting this thread with similar issues of memory leakage when invoking `model.evaluate()` in a loop, the work-around for this case is similar to the one proposed by @MProx, use `model.test_on_batch()` instead.", "Building a model by subclassing the `Model` class can avoid this problem.\r\n\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\r\n        self.dense2 = tf.keras.layers.Dense(2, activation=tf.nn.softmax)\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        return self.dense2(x)\r\nmodel = MyModel()\r\nmodel.compile(optimizer='adam', loss='mse')        \r\n\r\nfake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))\r\nwhile True:\r\n    # Repeatedly predict:\r\n    # model.predict(fake_data) # memory leak will happen if execute this line\r\n    model(fake_data) # There is no memory leak\r\n```", "@MProx, about a month ago we added a fix for memory leak and there's a possibility it has fixed this. Can you try `!pip install tf-nightly` and see if it resolves your issue?", "Hi. I have done what you ask and tested `tf-nightly` with my trivial code example from the original issue above, and it does indeed seem to fix the problem. I also tried it in a different application that I've been using the predict_on_batch workaround with, and there seem to be no memory leaks there either. So yes, the leak seems to have been dealt with. \r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33009\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33009\">No</a>\n", "Thanks @MProx for the updates!", "Hi, \r\n\r\nI am trying to call `model.predict()` on CPU (model trained on GPU) multiple times and I observe RAM memory leak. `clear_session` with model reload doesn't solve the issue. `model.predict_on_batch()` fails to solve the issue as well. Is there a workaround for this issue? I am using TF 1.13 and Python 3.6. Have been struggling to solve this problem since so long. Kinda need help. ", "Does the issue persist of you use the minimal code example I supplied in\nthe original issue post?\n\nOn Fri, Jan 17, 2020, 05:27 Yash Ubale <notifications@github.com> wrote:\n\n> Hi,\n>\n> I am trying to call model.predict() on CPU (model trained on GPU)\n> multiple times and I observe RAM memory leak. clear_session with model\n> reload doesn't solve the issue. model.predict_on_batch() fails to solve\n> the issue as well. Is there a workaround for this issue? I am using TF 1.13\n> and Python 3.6. Have been struggling to solve this problem since so long.\n> Kinda need help.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33009?email_source=notifications&email_token=AIITZUJCKIFIPYWKAKTL7YLQ6E6URA5CNFSM4I5AMYCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJGPZCY#issuecomment-575470731>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIITZUNSS6RQ32Q3GPU4HMDQ6E6URANCNFSM4I5AMYCA>\n> .\n>\n", "@MProx\r\nYes, the original code as shown below causes memory leak.\r\n\r\n```\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.layers import Dense\r\nimport numpy as np\r\n\r\n# Build model\r\nIn = Input(shape=(10,))\r\nx = Dense(32)(In)\r\nOut = Dense(2)(x)\r\n\r\n# Compile\r\nmodel = Model(inputs=In, outputs=Out)\r\nmodel.compile(optimizer='adam', loss='mse')        \r\n\r\n# Create dummy input data\r\nfake_data = np.random.uniform(low=0, high=1.0, size=(1, 10, ))\r\n\r\nwhile True:\r\n    # Repeatedly predict:\r\n    model.predict(fake_data) # No memory leak if this line is replaced with \"pass\"\r\n```\r\n\r\nBelow is the memory consumption over time (it increases initially which is expected, but after certain point when it's supposed to be constant, it rather increases although by a very small amount, I ran it for only 30 minutes)\r\n```\r\nCMDLINE python test.py\r\nCMDLINE python test.py\r\nMEM 0.609375 1579247671.2321\r\nMEM 22.660156 1579247671.3397\r\nMEM 68.218750 1579247671.4467\r\nMEM 104.500000 1579247671.5552\r\nMEM 114.675781 1579247671.6605\r\nMEM 124.125000 1579247671.7657\r\nMEM 131.191406 1579247671.8710\r\nMEM 146.972656 1579247671.9783\r\nMEM 156.164062 1579247672.0838\r\nMEM 168.886719 1579247672.1893\r\nMEM 182.667969 1579247672.2945\r\nMEM 192.363281 1579247672.4000\r\nMEM 202.539062 1579247672.5052\r\nMEM 211.316406 1579247672.6102\r\nMEM 211.968750 1579247672.7151\r\nMEM 215.250000 1579247672.8199\r\nMEM 215.464844 1579247672.9256\r\nMEM 215.738281 1579247673.0316\r\nMEM 215.890625 1579247673.1366\r\nMEM 216.050781 1579247673.2414\r\nMEM 216.468750 1579247673.3461\r\nMEM 221.535156 1579247673.4508\r\nMEM 226.820312 1579247673.5557\r\nMEM 227.011719 1579247673.7651\r\nMEM 227.261719 1579248604.1910\r\nMEM 227.511719 1579249397.2364\r\nMEM 228.515625 1579250955.6305\r\nMEM 228.304688 1579251348.9152\r\n```", "I've had the same problem and none of the above solutions worked for me.\r\nUltimately, setting `inter_op_parallelism_threads=1` in `tf.Config` fixed it.\r\ntensorflow/tensorflow#22098", "Still getting this error with tf-nightly (2.2.0-dev20200415)", "@rchao @MProx Still getting this error with TensorFlow version 2.2.0-rc3 or tf-nightly (2.2.0-dev20200416) on Google Colab. See this [gist](https://colab.research.google.com/gist/bentyeh/5a624a95f8614a7d140592e62b7967fa/tf_keras_issue33009.ipynb).", "Same getting error even after using `train_on_batch` i am using custom keras layers ", "The issue persists with tensorflow 2.2.0. Would be better if someone from tensorflow team made a statement about it. It severely hinders the ability to train for larger number of epochs ", "Hi,\r\n\r\nI am trying to call `model.predict()` on CPU multiple times and I observe RAM memory leak. `clear_session()` with model reload and `gc.collect()` doesn't solve the issue. I ran the code on tensorflow 2.1 and 2.3 as well but issue still persists. Is there a workaround for this issue? I am using TF 1.14 and Python 3.6. Have been struggling to solve this problem since so long. ", "Having similar issue in latest TF 2.4.1. Growing from about 60GB (I'm using a large shuffle buffer) to 128GB over the course of a few hours. Not sure if it't the same issue as originally mentioned here, since it seems to be more subtle and any number of causes could be the memory leak. Would file a new issue but it's challenging to create a minimal reproducible example"]}, {"number": 33008, "title": "fix typos", "body": "fix typos", "comments": []}, {"number": 33007, "title": "deepspeech running issue", "body": "![image](https://user-images.githubusercontent.com/36496141/66105607-bd81a980-e570-11e9-8b01-77e08a7f27d4.png)\r\nFacing this issue,any help would be appreciated.\r\nThanks", "comments": ["@samhithaaaa Its too vague for me to respond to this issue but you can take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/17702) which should help you in solving the problem.\r\n\r\nAlso if that issue doesn't help you in solving the problem,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "> @samhithaaaa Its too vague for me to respond to this issue but you can take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/17702) which should help you in solving the problem.\r\n> \r\n> Also if that issue doesn't help you in solving the problem,\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\nI am using ubuntu with arm v8 architecture.My tensorflow version is 1.14.I compiled this tensorflow from source . I am using python3 ,while implementing deepspeech :https://github.com/tensorflow/models/tree/master/research/deep_speech this link after donwloading data I am facing this issue while training.python deepspeech.py.I still get the same issue:absl.flags._exceptions.UnrecognizedFlagError: Unknown command line flag 'export_dir'\r\n", "Can you provide me the series of commands that you followed before running into this issuse?", "1.export PYTHONPATH=\"$PYTHONPATH:/path/to/models\"\n<https://github.com/tensorflow/models/tree/master/research/deep_speech#install-dependencies>\n2.pip3 install -r requirements.txt\n3.python data/download.py\n4.python deep_speech.py\n\n\nI am stuck at this step.\n\n5.sh run_deep_speech.sh\n\nOn Thu, Oct 3, 2019 at 3:00 PM gowthamkpr <notifications@github.com> wrote:\n\n> Can you provide me the series of commands that you followed before running\n> into this issuse?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33007?email_source=notifications&email_token=AIWOGDOLG46VV2VAA47QIQDQMZTPJA5CNFSM4I47GYX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAJWYLA#issuecomment-538143788>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIWOGDJQKCUG5MC3W4APQBDQMZTPJANCNFSM4I47GYXQ>\n> .\n>\n", "Hi,\r\nI'm also facing the \"UnrecognizedFlagError\" issue of Abseil library while running below command \r\n`python3 deep_speech.py`\r\n\r\nI'm using following system config -\r\nOS - Debian GNU/Linux 9.9 (stretch)\r\nPython 3.5.3\r\nTensorflow 1.14.0\r\n\r\nAnd ran below commands in order\r\n1. export PYTHONPATH=\"$PYTHONPATH:/path/to/models\"\r\n2. pip3 install -r requirements.txt\r\n3. python data/download.py\r\n4. sh run_deep_speech.sh\r\n\r\n![image](https://user-images.githubusercontent.com/24800950/66310270-64779400-e929-11e9-9793-3b0125febcf8.png)\r\n", "Can you try it with Tensorflow 1.13 and let me know if it works?", "I have already tried with 1.13 too but that doesn\u2019t help either\n\nOn Mon, Oct 7, 2019 at 5:24 PM gowthamkpr <notifications@github.com> wrote:\n\n> Can you try it with Tensorflow 1.13 and let me know if it works?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33007?email_source=notifications&email_token=AIWOGDI4IPJLWI3FRRUHGRTQNPHMTA5CNFSM4I47GYX2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEASG3WQ#issuecomment-539258330>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIWOGDLMBZQDGJ3FZOJV2KLQNPHMTANCNFSM4I47GYXQ>\n> .\n>\n", "Hi @samhithaaaa ,\r\nI debugged the issue, I found that in file- https://github.com/tensorflow/models/blob/master/official/utils/flags/_base.py\r\nIn below function, train_epochs=False by default.\r\n\r\n`def define_base(data_dir=True, model_dir=True, clean=False, train_epochs=False, \r\n                epochs_between_evals=False, stop_threshold=False,\r\n                batch_size=True, num_gpu=False, hooks=False, export_dir=False,\r\n                distribution_strategy=False, run_eagerly=False):\r\n `\r\n\r\nwhereas while calling the same function in https://github.com/conqueror7/models/blob/master/research/deep_speech/deep_speech.py\r\nIn Line 303, flags_core.define_base(), train_epochs is not set to True. \r\n`def define_deep_speech_flags():`\r\n      `flags_core.define_base(`\r\n      `data_dir=False  # we use train_data_dir and eval_data_dir instead`\r\n  `)`\r\nThus, it not getting defined in flag obj and not recognised later.\r\n\r\nSo, you can edit the code in deep_speech.py as follows-\r\n\r\n`def define_deep_speech_flags():`\r\n      `flags_core.define_base(`\r\n      `train_epochs=True,`\r\n      `data_dir=False  # we use train_data_dir and eval_data_dir instead`\r\n`  )`", "Glad that you were able to reslove the issue. \r\n@samhithaaaa Can you please try the recommendation given by @conqueror7 and let me know if that helps you in solving the issue. ", "Closing this issue as it has been resolved. Please add additional comments and we can open the issue again. Thanks!"]}, {"number": 33006, "title": "Stop inheriting `tf.keras.Model` to build custom layers in tutorials and guides.", "body": "## URL(s) with the issue:\r\n- https://www.tensorflow.org/tutorials/text/nmt_with_attention\r\n`Encoder`, `BahdanauAttention` and `Decoder` inherit `tf.keras.Model`.\r\n- https://www.tensorflow.org/guide/eager#variables_and_optimizers\r\nModel does not need to be a subclass of `tf.keras.Model` because we don't use `Model`'s utility methods.\r\n- https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers\r\n  > The main class used when creating a layer-like thing which contains other layers is tf.keras.Model. Implementing one is done by inheriting from tf.keras.Model.\r\n\r\n  This is not accurate since TF 1.13 (and standalone keras 2.3.0)\r\n\r\nThere are other tutorials which use `tf.keras.Model` when `tf.keras.layers.Layer` is sufficient.\r\n\r\n## Description of issue (what needs changing):\r\n\r\nWe should stop inheriting `tf.keras.Model` in tutorials when we don't use utility functions of `Model`.\r\n\r\n### Clear description\r\n\r\nSince tf1.13 and [standalone keras 2.3.0](https://github.com/keras-team/keras/releases/tag/2.3.0), *Layers set as attributes of a Layer are now tracked*.\r\nAlso, [Writing custom layers and models with Keras](https://www.tensorflow.org/guide/keras/custom_layers_and_models) says *A Model is just like a Layer, but with added training and serialization utilities.*\r\n\r\nWe don't need to inherit `Model` unless we use \"utility methods\" of `Model`. I think the idea like *\"Always extend Model because Model has more features\"* is not correct because\u00a0utilities of Model work only with special subsets of Layer ([Layers whose call receive only one input](https://github.com/tensorflow/tensorflow/blob/c8ef33dd913463ced8cc347c03945a88b34da7f8/tensorflow/python/keras/engine/training.py#L1461)).\r\n\r\nThus, I think we should stop inheriting `tf.keras.Model` in tutorials when `tf.keras.layers.Layer` is enough.\r\n\r\nMy original question on stackoverflow as a context of this bug:\r\nhttps://stackoverflow.com/questions/58118334/when-should-we-inherits-keras-model-instead-of-keras-layers-layer-even-if-we-don\r\n### Submit a pull request?\r\n\r\nI'm sending a pull requests to fix them.\r\n", "comments": ["The above PR resolves these questions."]}, {"number": 33004, "title": "Update the docstring of tf.strings.substr to cover negative len case", "body": "While using tf.strings.substr, initially I wasn't sure if negative `len` will be recognized.\r\nThe docstring does not mention this situation (normally it should, just like most of\r\nthe languages)\r\n\r\nVerifired  that `tf.strings.substr` indeed recognizes negative `len` so believe it makes\r\nsense to update the docstring to cover this scenario.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @martinwicke for the review. The PR has been pushed with docstring updated. Please take a look."]}, {"number": 33003, "title": "Loss of the model ran with TF2.0 is much higher compared to 1.x", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0 GPU\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter migrating to TF 2.0 I tried to run the exact same model and got much worse results\r\n\r\n**Describe the expected behavior**\r\nThe results should be the same as in TF1.x\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nBelow is the model definition:\r\n\r\n```\r\nimport os\r\nos.environ['TF_KERAS'] = '1'\r\n\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\r\nfrom heatmaps import *\r\nimport numpy as np\r\nfrom keras_radam import RAdam\r\n\r\nimage_size = 200\r\n## output shape is the same as input\r\nn = 32 * 5\r\nnClasses = 6\r\nnfmp_block1 = 64\r\nnfmp_block2 = 128\r\nbatch_size = 64\r\n\r\nIMAGE_ORDERING = \"channels_last\"\r\nimg_input = tf.keras.Input(shape=(image_size, image_size, 3))\r\n\r\n# Encoder Block 1\r\nx = Conv2D(nfmp_block1, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format=IMAGE_ORDERING)(\r\n    img_input)\r\nx = Conv2D(nfmp_block1, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING)(x)\r\nblock1 = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING)(x)\r\n\r\n# Encoder Block 2\r\nx = Conv2D(nfmp_block2, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING)(\r\n    block1)\r\nx = Conv2D(nfmp_block2, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING)(x)\r\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING)(x)\r\n\r\n## bottleneck\r\no = (Conv2D(n, (int(image_size / 4), int(image_size / 4)),\r\n            activation='relu', padding='same', name=\"bottleneck_1\", data_format=IMAGE_ORDERING))(x)\r\no = (Conv2D(n, (1, 1), activation='relu', padding='same', name=\"bottleneck_2\", data_format=IMAGE_ORDERING))(o)\r\n\r\n## Decoder Block\r\n## upsampling to bring the feature map size to be the same as the input image i.e., heatmap size\r\noutput = Conv2DTranspose(nClasses, kernel_size=(4, 4), strides=(4, 4), use_bias=False, name='upsample_2',\r\n                         data_format=IMAGE_ORDERING)(o)\r\n\r\n## Reshaping is necessary to use sample_weight_mode=\"temporal\" which assumes 3 dimensional output shape\r\n## See below for the discussion of weights\r\noutput = Reshape((image_size * image_size * nClasses, 1))(output)\r\nmodel = tf.keras.Model(img_input, output)\r\nmodel.summary()\r\n\r\nradam = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=1e-5)\r\nmodel.compile(optimizer=radam, loss='mse', sample_weight_mode=\"temporal\")\r\n\r\ndata_folder = 'data'\r\nid2filename, filename2id, annotated_images = dataloader.get_image_annotations(data_folder)\r\ndf = dataloader.get_annotation_dataframe(id2filename, annotated_images)\r\nmsk = np.random.rand(len(df)) < 0.8\r\ntrain = df[msk]\r\ntest = df[~msk]\r\nencoding = MultiPointHeatmapEncoding(image_size, df, batch_size=64)\r\n\r\nmodel_name = 'stacked_hourglass_tf2'\r\nlog_dir = \"logs/{}\".format(model_name)\r\nmodel_filename = \"saved-models/{}.h5\".format(model_name)\r\n\r\ntrain_gen = encoding.generator(train, batch_size)\r\ntest_gen = encoding.generator(test, batch_size, get_weights=True)\r\n\r\nsteps_per_epoch = len(train) // batch_size\r\nvalidation_steps = len(test) // batch_size\r\nif validation_steps == 0:\r\n    validation_steps = 1\r\nif steps_per_epoch == 0:\r\n    steps_per_epoch = 1\r\n\r\ncb_tensorboard = TensorBoard(log_dir=log_dir)\r\ncallback_save_images = CallbackHeatmapOutput(model, get_generator(test_gen), log_dir, encoding)\r\ncheckpoint = ModelCheckpoint(model_filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n\r\nhistory = model.fit_generator(\r\n            get_generator(train_gen),\r\n            validation_data=get_generator(test_gen),\r\n            steps_per_epoch=steps_per_epoch,\r\n            epochs=5000,\r\n            validation_steps=validation_steps,\r\n            verbose=2,\r\n            use_multiprocessing=True,\r\n            callbacks=[checkpoint, callback_save_images, cb_tensorboard]\r\n        )\r\n\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\noutput log while running the epochs:\r\n\r\n> 2019-10-03 00:16:43.397431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-10-03 00:16:45.096352: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\n> Relying on driver to perform ptx compilation. This message will be only logged once.\r\n> 2019-10-03 00:17:16.303969: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n> 2019-10-03 00:17:16.304314: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n> 2019-10-03 00:17:16.304328: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n> 2019-10-03 00:17:17.242794: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.\r\n> 2019-10-03 00:17:17.276983: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n", "comments": ["@nikogamulin \r\nI tried to execute the code in colab with TF 2.0.0-rc2 , i am getting the below error message `NameError: name 'dataloader' is not defined` , also `ModuleNotFoundError: No module named 'heatmaps'` .Please, help me in reproducing the issue in our environment.Thanks!\r\n", "@ravikyram , thanks for help. The project is fairly complex, but the archtecture is actually the same is [this one](https://fairyonice.github.io/Achieving-top-5-in-Kaggles-facial-keypoints-detection-using-FCN.html). Also, it would be helpful to benchmark the model using the same dataset (facial keypoints expression from kaggle) as I can't share the data.", "I have a similar problem. My project is training a model to predict Q value in Reinforcement Learning. The error is TD error, which has similar fashion as MSE. I'm using compact.v1.AdamOptimizer\r\n\r\nIn Tensorflow 1.14 and before, the learning curve in training starts from 8 and drop to 7.\r\n\r\nHowever, from Tensorflow 1.15 and afterwards, the learning curve rises up to 17 fairly quickly.\r\n\r\nIs there some modification to optimizers or underlying operations after Tensorflow 1.14?", "May I ask whether you try to run it in 1.15? For my project, the loss gets larger starting from Tensorflow 1.15.", "Apologies for the delay in response. \r\nAs mentioned in the stack trace `Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; `\r\nIt is looking for `cuda 10.0` whereas you have installed `cuda 10.1`.\r\nYou may want to upgrade your TF to 2.1 which supports `cuda10.1`.\r\nThanks!", "It has been 41 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33003\">No</a>\n"]}, {"number": 33002, "title": "tensorflow-gpu CUPTI errors", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.0\r\n- Python version:3\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GeForce RTX 2080 Ti \r\n\r\n\r\n\r\n**Describe the problem**\r\nI upgraded TensorFlow from 1.x to 2.0 and tried to run the same model as I successfully ran with  TF 1.x.\r\n\r\nPreviously, I already had NVIDIA drivers and CUDA toolkit installed, and therefore I just installed tensorflow-gpu in new virtual environment. Upon running, I got the following output with errors/warnings:\r\n\r\n> 2019-10-03 00:16:43.397431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n> 2019-10-03 00:16:45.096352: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\n> Relying on driver to perform ptx compilation. This message will be only logged once.\r\n> 2019-10-03 00:17:16.303969: I tensorflow/core/profiler/lib/profiler_session.cc:184] Profiler session started.\r\n> 2019-10-03 00:17:16.304314: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.0'; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64::/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n> 2019-10-03 00:17:16.304328: W tensorflow/core/profiler/lib/profiler_session.cc:192] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n> 2019-10-03 00:17:17.242794: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.\r\n> 2019-10-03 00:17:17.276983: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.\r\n\r\nIn order to fix these warnings, I ran the NVIDIA/CUDA-related commands, listed on tensorflow-gpu installation page:\r\n\r\n# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-418\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install development and runtime libraries (~4GB)\r\nsudo apt-get install --no-install-recommends \\\r\n    cuda-10-0 \\\r\n    libcudnn7=7.6.2.24-1+cuda10.0  \\\r\n    libcudnn7-dev=7.6.2.24-1+cuda10.0\r\n\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\nsudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \\\r\n    libnvinfer-dev=5.1.5-1+cuda10.0\r\n\r\nafter runnug this, I got the following errir:\r\n\r\n> : Version '7.6.2.24-1+cuda10.1' for 'libcudnn7' was not found\r\n> E: Unable to locate package libcudnn7-dev\r\n\r\nnvidia-smi outputs the following:\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.50       Driver Version: 430.50       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   40C    P8     4W / 260W |  11012MiB / 11019MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nActually the biggest issue, that might not be related to this error is that the performance of the exact same model is much worse when trying to execute using TF2.0 and am not sure whether these errors might be the cause, although I doubt it. I guess I should open another issue for this, but am not sure which category is most suitable for this issue.\r\n", "comments": ["Drive by comment: CUPTI lives in `cuda/extras` (namely `extras/CUPTI/lib64`), whereas typically LD_LIBRARY_PATH only includes `cuda/include` and `cuda/lib64` out of the box.", "You need to add CUPTI path to your environment variable.\r\nIt should be something like  ```/usr/local/cuda/extras/CUPTI/lib64``` to ```LD_LIBRARY_PATH``` ", "How to fix that error in win10? ", "@rorypeck , lets say your base path is: `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\<version>\\`, then go to `<base>\\extras\\CUPTI\\libx64` and copy the file `cupti64_100.dll` and paste it into `<base>\\bin` folder. In my case (with cuda 10.0):\r\n\r\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\extras\\CUPTI\\libx64\\cupti64_100.dll`\r\nbecomes\r\n`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cupti64_100.dll`\r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33002\">No</a>\n", "`/usr/local/cuda/extras/CUPTI/lib64 to LD_LIBRARY_PATH` and  logout and login  on `Ubuntu 16.04` works. `source ~/.bashrc` didn't help. ", "Another way is to create a file `/etc/ld.so.conf.d/cupti-10-1.conf` containing:\r\n```\r\n/usr/local/cuda/extras/CUPTI/lib64\r\n```\r\nThen run\r\n```\r\nsudo ldconfig\r\n```\r\nto rebuild the library cache."]}, {"number": 33001, "title": "ERROR: C:/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1314:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter' failed (Exit 2): python.exe failed: error executing command", "body": "I'm trying to build Tensorflow 2.0 from source on my RTX 2070 machine. I get this error towards the end of the compilation process (https://pastebin.com/zuuQqZTb):\r\n\r\n```\r\nERROR: C:/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1314:1: C++ compilation of rule '//tensorflow/compiler/xla/service/gpu:cudnn_fused_conv_rewriter' failed (Exit 2): python.exe failed: error executing command\r\n  cd C:/users/qrabbani/_bazel_qrabbani/xv6zejqw/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/QRabbani/Anaconda3/envs/tf_gpu/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/QRabbani/Anaconda3/envs/tf_gpu/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\QRabbani\\AppData\\Local\\Temp\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0\r\n    SET TF_NEED_CUDA=1\r\n    SET TMP=C:\\Users\\QRabbani\\AppData\\Local\\Temp\r\n  C:/Users/QRabbani/Anaconda3/envs/tf_gpu/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/llvm /Ibazel-out/x64_windows-opt/bin/external/llvm /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/llvm/lib/IR /Ibazel-out/x64_windows-opt/bin/external/llvm/lib/IR /Iexternal/llvm/include/llvm/IR /Ibazel-out/x64_windows-opt/bin/external/llvm/include/llvm/IR /Iexternal/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /DNDEBUG -w -DWIN32_LEAN_AND_MEAN -DNOGDI /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/gpu/_objs/cudnn_fused_conv_rewriter/cudnn_fused_conv_rewriter.o /c tensorflow/compiler/xla/service/gpu/cudnn_fused_conv_rewriter.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(645): note: see reference to class template instantiation 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>' being compiled\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\include\\xtr1common(159): note: see reference to class template instantiation 'std::integral_constant<bool,false>' being compiled\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.23.28105\\include\\xtr1common(159): note: see reference to class template instantiation 'std::disjunction<_Traits...>' being compiled\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=const xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: 'NewImpl=xla::match::detail::LayoutPatternEqualImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=const xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2672: 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=const xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<const xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(655): note: see reference to class template instantiation 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>' being compiled\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(499): note: 'NewImpl=xla::match::detail::LayoutPatternEqualImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(505): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2672: 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): error C2893: Failed to specialize function template 'xla::match::detail::LayoutPattern<LayoutType,unknown-type> xla::match::detail::LayoutPattern<LayoutType,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            LayoutType=xla::Layout\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(456): note: see declaration of 'xla::match::detail::LayoutPattern<xla::Layout,xla::match::detail::LayoutPatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(511): note: 'NewImpl=xla::match::detail::LayoutPatternFormatImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see reference to class template instantiation 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>' being compiled\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: 'NewImpl=xla::match::detail::ShapePatternEqualImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: 'NewImpl=xla::match::detail::ShapePatternCompatibleImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: 'NewImpl=xla::match::detail::ShapePatternElementTypeImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: 'NewImpl=xla::match::detail::ShapePatternIsScalarImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: 'NewImpl=xla::match::detail::ShapePatternIsArrayImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: 'NewImpl=xla::match::detail::ShapePatternIsTupleImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: 'NewImpl=xla::match::detail::ShapePatternEffectiveScalarImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=const xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: 'NewImpl=xla::match::detail::ShapePatternRankImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2672: 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2784: 'unknown-type xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<const xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1080): error C2923: 'xla::match::detail::AllOfPattern': 'xla::match::Shape' is not a valid template type argument for parameter 'Item'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see declaration of 'xla::match::Shape'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1080): error C3203: 'AllOfPattern': unspecialized class template can't be used as a template argument for template parameter 'Impl', expected a real type\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1093): error C2923: 'xla::match::detail::AllOfPattern': 'xla::match::Shape' is not a valid template type argument for parameter 'Item'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1116): note: see declaration of 'xla::match::Shape'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1093): error C3203: 'AllOfPattern': unspecialized class template can't be used as a template argument for template parameter 'Impl', expected a real type\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1124): note: see reference to class template instantiation 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>' being compiled\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(997): note: 'NewImpl=xla::match::detail::ShapePatternEqualImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1004): note: 'NewImpl=xla::match::detail::ShapePatternCompatibleImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1010): note: 'NewImpl=xla::match::detail::ShapePatternElementTypeImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1016): note: 'NewImpl=xla::match::detail::ShapePatternIsScalarImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1022): note: 'NewImpl=xla::match::detail::ShapePatternIsArrayImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1028): note: 'NewImpl=xla::match::detail::ShapePatternIsTupleImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1033): note: 'NewImpl=xla::match::detail::ShapePatternEffectiveScalarImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): error C2893: Failed to specialize function template 'xla::match::detail::ShapePattern<ShapeType,unknown-type> xla::match::detail::ShapePattern<ShapeType,xla::match::detail::ShapePatternBaseImpl>::AppendImpl(NewImpl) const'\r\n        with\r\n        [\r\n            ShapeType=xla::Shape\r\n        ]\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(948): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::AppendImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: With the following template arguments:\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1039): note: 'NewImpl=xla::match::detail::ShapePatternRankImpl'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1053): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1058): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2672: 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout': no matching overloaded function found\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1063): error C2784: 'unknown-type xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout(const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &) const': could not deduce template argument for 'const xla::match::detail::LayoutPattern<LayoutType,LayoutImpl> &' from 'unknown-type'\r\n.\\tensorflow/compiler/xla/service/pattern_matcher.h(1046): note: see declaration of 'xla::match::detail::ShapePattern<xla::Shape,xla::match::detail::ShapePatternBaseImpl>::WithLayout'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2238.604s, Critical Path: 469.42s\r\nINFO: 4001 processes: 4001 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nAny idea what's going on and how to fix it?\r\n\r\n**Edit:**\r\nHere's the output of ./configure (https://pastebin.com/DhvwwyZM):\r\n\r\n```\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.1 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\QRabbani\\Anaconda3\\envs\\tf_gpu\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\QRabbani\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\QRabbani\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n\r\nAnd here's what's in my .bazelrc (https://pastebin.com/h6aLHX0d):\r\n\r\n```\r\n# Android configs. Bazel needs to have --cpu and --fat_apk_cpu both set to the\r\n# target CPU to build transient dependencies correctly. See\r\n# https://docs.bazel.build/versions/master/user-manual.html#flag--fat_apk_cpu\r\nbuild:android --crosstool_top=//external:android/crosstool\r\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:android_arm --config=android\r\nbuild:android_arm --cpu=armeabi-v7a\r\nbuild:android_arm --fat_apk_cpu=armeabi-v7a\r\nbuild:android_arm64 --config=android\r\nbuild:android_arm64 --cpu=arm64-v8a\r\nbuild:android_arm64 --fat_apk_cpu=arm64-v8a\r\nbuild:android_x86 --config=android\r\nbuild:android_x86 --cpu=x86\r\nbuild:android_x86 --fat_apk_cpu=x86\r\nbuild:android_x86_64 --config=android\r\nbuild:android_x86_64 --cpu=x86_64\r\nbuild:android_x86_64 --fat_apk_cpu=x86_64\r\n \r\n# Sets the default Apple platform to macOS.\r\nbuild --apple_platform_type=macos\r\n \r\n# Config to use a mostly-static build and disable modular op registration\r\n# support (this will revert to loading TensorFlow with RTLD_GLOBAL in Python).\r\n# By default, TensorFlow will build with a dependence on\r\n# //tensorflow:libtensorflow_framework.so.\r\nbuild:monolithic --define framework_shared_object=false\r\n \r\n# For projects which use TensorFlow as part of a Bazel build process, putting\r\n# nothing in a bazelrc will default to a monolithic build. The following line\r\n# opts in to modular op registration support by default.\r\nbuild --define framework_shared_object=true\r\n \r\n# Flags for open source build, always set to be true.\r\nbuild --define open_source_build=true\r\ntest --define open_source_build=true\r\n \r\n# Please note that MKL on MacOS or windows is still not supported.\r\n# If you would like to use a local MKL instead of downloading, please set the\r\n# environment variable \"TF_MKL_ROOT\" every time before build.\r\nbuild:mkl --define=build_with_mkl=true --define=enable_mkl=true\r\nbuild:mkl --define=tensorflow_mkldnn_contraction_kernel=0\r\nbuild:mkl -c opt\r\n \r\n# This config option is used to enable MKL-DNN open source library only,\r\n# without depending on MKL binary version.\r\nbuild:mkl_open_source_only --define=build_with_mkl_dnn_only=true\r\nbuild:mkl_open_source_only --define=build_with_mkl_dnn_v1_only=true\r\nbuild:mkl_open_source_only --define=build_with_mkl=true --define=enable_mkl=true\r\nbuild:mkl_open_source_only --define=tensorflow_mkldnn_contraction_kernel=0\r\n \r\nbuild:download_clang --crosstool_top=@local_config_download_clang//:toolchain\r\nbuild:download_clang --define=using_clang=true\r\nbuild:download_clang --action_env TF_DOWNLOAD_CLANG=1\r\n# Instruct clang to use LLD for linking.\r\n# This only works with GPU builds currently, since Bazel sets -B/usr/bin in\r\n# auto-generated CPU crosstool, forcing /usr/bin/ld.lld to be preferred over\r\n# the downloaded one.\r\nbuild:download_clang_use_lld --linkopt='-fuse-ld=lld'\r\n \r\n# This config refers to building with CUDA available. It does not necessarily\r\n# mean that we build CUDA op kernels.\r\nbuild:using_cuda --define=using_cuda=true\r\nbuild:using_cuda --action_env TF_NEED_CUDA=1\r\nbuild:using_cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\r\n \r\n# This config refers to building CUDA op kernels with nvcc.\r\nbuild:cuda --config=using_cuda\r\nbuild:cuda --define=using_cuda_nvcc=true\r\n \r\n# This config refers to building CUDA op kernels with clang.\r\nbuild:cuda_clang --config=using_cuda\r\nbuild:cuda_clang --define=using_cuda_clang=true\r\nbuild:cuda_clang --define=using_clang=true\r\n \r\nbuild:tensorrt --action_env TF_NEED_TENSORRT=1\r\n \r\nbuild:rocm --crosstool_top=@local_config_rocm//crosstool:toolchain\r\nbuild:rocm --define=using_rocm=true --define=using_rocm_hipcc=true\r\nbuild:rocm --action_env TF_NEED_ROCM=1\r\n \r\nbuild:sycl --crosstool_top=@local_config_sycl//crosstool:toolchain\r\nbuild:sycl --define=using_sycl=true\r\nbuild:sycl --action_env TF_NEED_OPENCL_SYCL=1\r\n \r\nbuild:sycl_nodouble --config=sycl\r\nbuild:sycl_nodouble --cxxopt -DTENSORFLOW_SYCL_NO_DOUBLE\r\n \r\nbuild:sycl_nodouble --config=sycl\r\nbuild:sycl_asan --copt -fno-omit-frame-pointer --copt -fsanitize-coverage=3 --copt -DGPR_NO_DIRECT_SYSCALLS --linkopt -fPIC --linkopt -fsanitize=address\r\n \r\nbuild:sycl_nodouble --config=sycl\r\nbuild:sycl_trisycl --define=using_trisycl=true\r\n \r\n# Options extracted from configure script\r\nbuild:gdr --define=with_gdr_support=true\r\nbuild:ngraph --define=with_ngraph_support=true\r\nbuild:verbs --define=with_verbs_support=true\r\nbuild:numa --define=with_numa_support=true\r\n \r\n# Options to disable default on features\r\nbuild:noaws --define=no_aws_support=true\r\nbuild:nogcp --define=no_gcp_support=true\r\nbuild:nohdfs --define=no_hdfs_support=true\r\nbuild:nokafka --define=no_kafka_support=true\r\nbuild:noignite --define=no_ignite_support=true\r\nbuild:nonccl --define=no_nccl_support=true\r\n \r\nbuild --define=use_fast_cpp_protos=true\r\nbuild --define=allow_oversize_protos=true\r\n \r\nbuild --spawn_strategy=standalone\r\nbuild --strategy=Genrule=standalone\r\nbuild -c opt\r\n \r\n# Make Bazel print out all options from rc files.\r\nbuild --announce_rc\r\n \r\n# Other build flags.\r\nbuild --define=grpc_no_ares=true\r\n \r\n# Modular TF build options\r\nbuild:dynamic_kernels --define=dynamic_loaded_kernels=true\r\nbuild:dynamic_kernels --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n \r\n# Build TF with C++ 17 features.\r\nbuild:c++17 --cxxopt=-std=c++1z\r\nbuild:c++17 --cxxopt=-stdlib=libc++\r\nbuild:c++1z --config=c++17\r\n \r\n# Default paths for TF_SYSTEM_LIBS\r\nbuild --define=PREFIX=/usr\r\nbuild --define=LIBDIR=$(PREFIX)/lib\r\nbuild --define=INCLUDEDIR=$(PREFIX)/include\r\n \r\n# Suppress all warning messages.\r\nbuild:short_logs --output_filter=DONT_MATCH_ANYTHING\r\n \r\n# Options when using remote execution\r\nbuild:rbe --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1\r\nbuild:rbe --auth_enabled=true\r\nbuild:rbe --auth_scope=https://www.googleapis.com/auth/cloud-source-tools\r\nbuild:rbe --define=EXECUTOR=remote\r\nbuild:rbe --flaky_test_attempts=3\r\nbuild:rbe --jobs=200\r\nbuild:rbe --remote_accept_cached=true\r\nbuild:rbe --remote_cache=remotebuildexecution.googleapis.com\r\nbuild:rbe --remote_executor=remotebuildexecution.googleapis.com\r\nbuild:rbe --remote_local_fallback=false\r\nbuild:rbe --remote_timeout=600\r\nbuild:rbe --spawn_strategy=remote\r\nbuild:rbe --strategy=Genrule=remote\r\nbuild:rbe --strategy=Closure=remote\r\nbuild:rbe --strategy=Javac=remote\r\nbuild:rbe --strategy=TestRunner=remote\r\nbuild:rbe --tls_enabled\r\ntest:rbe --test_env=USER=anon\r\n \r\n# Options to build TensorFlow 1.x or 2.x.\r\nbuild:v1 --define=tf_api_version=1\r\nbuild:v2 --define=tf_api_version=2\r\ntest:v1 --test_env=TF2_BEHAVIOR=0\r\ntest:v2 --test_env=TF2_BEHAVIOR=1\r\nbuild --config=v2\r\ntest --config=v2\r\n \r\n# Default options should come above this line\r\n \r\n# Options from ./configure\r\ntry-import %workspace%/.tf_configure.bazelrc\r\n \r\n# Put user-specific options in .bazelrc.user\r\ntry-import %workspace%/.bazelrc.user\r\n```", "comments": ["@QRabbani, Please include the ./configure output. Thanks", "@gadagashwini, Done!", "@gunan @chsigg is this (XLA on windows) expected to work?", "I do not think we have it working yet.\n\nOn Fri, Oct 4, 2019, 5:57 PM Sanjoy Das <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan> @chsigg <https://github.com/chsigg> is\n> this (XLA on windows) expected to work?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33001?email_source=notifications&email_token=AB4UEOOPZNGDU6EOZLXAAT3QM7RBHA5CNFSM4I44E22KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEANGSQI#issuecomment-538601793>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AB4UEONYCSJRMZFLWA7IPSLQM7RBHANCNFSM4I44E22A>\n> .\n>\n", "@QRabbani I'd be happy to review changes that fix the windows build, but as per Gunan it isn't expected to be working yet.", "@sanjoy Okay, I see. Fixing it myself is a little beyond my understanding, unfortunately. Do you have any idea how soon XLA might be supported on Windows for TF 2.0?\r\n\r\nAlso, do the pip and Anaconda builds generally include XLA optimization? I normally don't build from scratch like this, but TF 2.0 on pip was throwing issues as well, and it isn't on Anaconda yet.", "@QRabbani We are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @kumariko, I now get a different error with Bazel upon trying to build from config, but as far as I can tell, there\u2019s no XLA issue. (Mind you I\u2019m using a different machine.) I was able to pip install TF 2.6 from pip successfully though!", "@QRabbani,\r\n\r\nAs you're not getting the mentioned `XLA` error while building `2.6.0`, I am closing this issue. Please feel to open a new issue for the different error. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33001\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33001\">No</a>\n"]}, {"number": 33000, "title": "Performance Degradation on MLPerf's GNMT implementation after converting to frozen graph ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): ('v1.14.0-rc1-22-gaf24dc91b5', '1.14.0')\r\n- Python version: 2.7.15+\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): `gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0`\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: NVIDIA Titan V\r\n\r\nI am noticing a significant performance degradation in inference time when using the GNMT model provided for the MLPerf v0.5 benchmark with a trained model loaded from a checkpoint vs a trained model represented as a frozen graph. On the first iteration of inference (with batch_size = 6), the checkpoint graph takes ~2.9sec to perform inference while the frozen_graph implementation takes > 40sec. I would expect the frozen_graph to have better performance than the checkpoint model since my impression was that frozen graphs were meant for inference. \r\n\r\nOutput from model loaded from checkpoint:\r\n```\r\n# Start decoding\r\n  decoding to output /foobar/g_nmt-out\r\n  infer_mode beam_search, beam_width 10, num translations per input 1.\r\n  total iterations count 1.\r\ninfer time: 2.9462 secs\r\n  done, num sentences 6, num translations per input 1, time 2s, Wed Oct  2 16:02:07 2019.\r\nsingle_worker_inference time: 3.0597 secs\r\n  bleu: 16.8\r\n```\r\n\r\nOutput from model loaded from a frozen graph:\r\n```\r\n# Start decoding\r\n  decoding to output /foobar/g_nmt-out\r\n  infer_mode beam_search, beam_width 10, num translations per input 1.\r\n  total iterations count 1.\r\ninfer time: 40.5799 secs\r\n  done, num sentences 6, num translations per input 1, time 40s, Wed Oct  2 15:40:09 2019.\r\nsingle_worker_inference time: 40.7543 secs\r\n  bleu: 16.8\r\n```\r\n\r\nI've pushed up my own fork of the [mlperf/inference](https://github.com/mlperf/inference) repository with a branch that exposes this performance degradation [found here](https://github.com/brycearden/inference/tree/gnmt-frozen-perf-degradation). The fast / slow behavior can be toggled here, and can be reproduced by running `python run_task.py --run=accuracy`:\r\n\r\nhttps://github.com/brycearden/inference/blob/gnmt-frozen-perf-degradation/v0.5/translation/gnmt/tensorflow/nmt/inference.py#L131-L160\r\n\r\nNote that on the second iteration (after the caches warm up), the frozen graph still seems to be ~3x slower than the checkpoint model (7sec vs 2sec)\r\n\r\nAny additional insight would be greatly appreciated.\r\n", "comments": ["@brycearden Can you please try the recently released `TF1.15.0rc2` and let us know how it progresses. Thanks!", "After trying on `1.15.0rc2` with Python 3.6.8, there seemed to be very little improvement:\r\n```\r\n\u276f python3 \r\nPython 3.6.8 (default, Aug 20 2019, 17:12:48)\r\n[GCC 8.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print (tf.__version__)\r\n1.15.0-rc2\r\n```\r\nLoading from checkpoint:\r\n```\r\n# Start decoding\r\n  decoding to output /foo/g_nmt-out\r\n  infer_mode beam_search, beam_width 10, num translations per input 1.\r\n  total iterations count 1.\r\ninfer time: 2.9555 secs\r\n  done, num sentences 6, num translations per input 1, time 2s, Thu Oct  3 20:07:45 2019.\r\nsingle_worker_inference time: 3.0558 secs\r\n  bleu: 16.8\r\n```\r\nLoading from the frozen graph:\r\n```\r\n# Start decoding\r\n  decoding to output /foo/g_nmt-out\r\n  infer_mode beam_search, beam_width 10, num translations per input 1.\r\n  total iterations count 1.\r\ninfer time: 40.7798 secs\r\n  done, num sentences 6, num translations per input 1, time 40s, Thu Oct  3 20:11:41 2019.\r\nsingle_worker_inference time: 40.9498 secs\r\n  bleu: 16.8\r\n```\r\n\r\nReproducible using the link(s) provided above running `python3 run_task.py --run=accuracy` \r\n\r\n\r\n", "@brycearden Sorry for the late response. Is this still an issue for you. Thanks!", "The GNMT tensorflow benchmark relies on `tf.contrib` heavily and would be a non-trivial amount of work to get this benchmark converted into TF2. However, this still reproduces on tensorflow `1.15.0`. ", "yes", "@brycearden Can you please try `TF1.15.3` and let us know if the issue persists or not. If the issue persists, can you please share a simple standalone code to reproduce the issue? Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 32999, "title": "Error converting NMT sequence to sequence example to .tflite model", "body": "I am following this example:\r\nhttps://www.tensorflow.org/tutorials/text/nmt_with_attention\r\n\r\nIt is working as it should be and saving checkpoints.\r\nI want to now convert this to a TF Lite model following this example:\r\nhttps://www.tensorflow.org/lite/convert/python_api#converting_a_savedmodel_\r\nor\r\nhttps://www.tensorflow.org/lite/convert/python_api#converting_a_concrete_function_\r\n\r\nHere is what I am running to save and them convert:\r\n```python\r\ntflite_input_tensor = tf.constant(1., shape=[64, 39])\r\ntflite_target_tensor = tf.constant(1., shape=[64, 7])\r\ntflite_enc_hidden_tensor = tf.constant(1., shape=[64, 1024])\r\nexport_dir = \"saved_models\"\r\ncheckpoint.f = train_step\r\nto_save = checkpoint.f.get_concrete_function(tflite_input_tensor, tflite_target_tensor, tflite_enc_hidden_tensor)\r\ntf.saved_model.save(checkpoint, export_dir, to_save)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\ntflite_model = converter.convert()\r\n```\r\nBut I am getting this error:\r\n```\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)\r\n    846                 else:\r\n--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    848 \r\n    849             except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    290   def wrapper(*args, **kwargs):\r\n    291     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n--> 292       return func(*args, **kwargs)\r\n    293 \r\n    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\nTypeError: call() missing 2 required positional arguments: 'hidden' and 'enc_output'\r\n```\r\n\r\nTrained with:\r\n```python\r\n@tf.function\r\ndef train_step(inp, targ, enc_hidden):\r\n    loss = 0\r\n\r\n    with tf.GradientTape() as tape:\r\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\r\n\r\n        dec_hidden = enc_hidden\r\n\r\n        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\r\n\r\n        # Teacher forcing - feeding the target as the next input\r\n        for t in range(1, targ.shape[1]):\r\n            # passing enc_output to the decoder\r\n            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\r\n\r\n            loss += loss_function(targ[:, t], predictions)\r\n            \r\n            # using teacher forcing\r\n            dec_input = tf.expand_dims(targ[:, t], 1)\r\n\r\n    batch_loss = (loss / int(targ.shape[1]))\r\n\r\n    variables = encoder.trainable_variables + decoder.trainable_variables\r\n\r\n    gradients = tape.gradient(loss, variables)\r\n\r\n    optimizer.apply_gradients(zip(gradients, variables))\r\n\r\n    return batch_loss\r\n\r\nEPOCHS = 3\r\n\r\nfor epoch in range(EPOCHS):\r\n    start = time.time()\r\n\r\n    enc_hidden = encoder.initialize_hidden_state()\r\n    total_loss = 0\r\n\r\n    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\r\n        batch_loss = train_step(inp, targ, enc_hidden)\r\n        total_loss += batch_loss\r\n\r\n        if batch % 100 == 0:\r\n            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\r\n                                                         batch,\r\n                                                         batch_loss.numpy()))\r\n    # saving (checkpoint) the model every 2 epochs\r\n    if (epoch + 1) % 1 == 0:\r\n        checkpoint.save(file_prefix = checkpoint_prefix)\r\n\r\n    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\r\n    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\r\n```\r\n\r\n\r\nSomehow the parameters for the Decoder call is not being passed in?\r\n```python\r\nclass Decoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\r\n        super(Decoder, self).__init__()\r\n        self.batch_sz = batch_sz\r\n        self.dec_units = dec_units\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n        self.rnn = tf.keras.layers.GRU(self.dec_units,\r\n                                       return_sequences=True,\r\n                                       return_state=True,\r\n                                       recurrent_initializer='glorot_uniform',\r\n                                       unroll=True)\r\n        \r\n        self.fc = tf.keras.layers.Dense(vocab_size)\r\n\r\n        # used for attention\r\n        self.attention = BahdanauAttention(self.dec_units)\r\n\r\n    def call(self, x, hidden, enc_output):\r\n        # enc_output shape == (batch_size, max_length, hidden_size)\r\n        context_vector, attention_weights = self.attention(hidden, enc_output)\r\n\r\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n        x = self.embedding(x)\r\n\r\n        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n\r\n        # passing the concatenated vector to the GRU\r\n        output, state = self.rnn(x)\r\n\r\n        # output shape == (batch_size * 1, hidden_size)\r\n        output = tf.reshape(output, (-1, output.shape[2]))\r\n\r\n        # output shape == (batch_size, vocab)\r\n        x = self.fc(output)\r\n\r\n        return x, state, attention_weights\r\n```\r\n\r\nI understand there may be some trouble converting the GRU layers, but I will tackle that next. This seems to blow up before it even can check if GRU is able to be converted.", "comments": ["Was able to get around it by passing the params as a list and then extract the variables `x, hidden, enc_output = x` in the call function of the decoder...", "Now it cannot convert a tensor, but not sure what this tensor is coming from:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-40-52d8f9c25fc5> in <module>()\r\n      7 \r\n      8 converter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\n----> 9 tflite_model = converter.convert()\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    403 \r\n    404     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(\r\n--> 405         self._funcs[0], lower_control_flow=False)\r\n    406     input_tensors = [\r\n    407         tensor for tensor in frozen_func.inputs\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow)\r\n    409 \r\n    410   # Get mapping from node name to variable value.\r\n--> 411   tensor_data = _get_tensor_data(func)\r\n    412 \r\n    413   # Get mapping from function name to argument types.\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py in _get_tensor_data(func)\r\n    193       data = map_index_to_variable[idx].numpy()\r\n    194     else:\r\n--> 195       data = val_tensor.numpy()\r\n    196     tensor_data[tensor_name] = {\r\n    197         \"data\": data,\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    931       ValueError: if the type of this Tensor is not representable in numpy.\r\n    932     \"\"\"\r\n--> 933     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    934     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    935 \r\n\r\nValueError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\n", "Could you try using TFLiteConverter.from_saved_model API instead?\r\n\r\nWhich version of Tensorflow are you using?", "Hi @mattc-eostar, \r\n\r\nDid you find a way around the error? I am facing the same error on TF-2.0.0 and 2.1.0-dev20191015 (nightly). Same error with `tf.lite.TFLiteConverter.from_saved_model`\r\n\r\nAlso, the first warning doesn't make any sense to me since the model since the output should have the shape (-1, 4)\r\n\r\n```\r\nIn [1]: converter = tf.lite.TFLiteConverter.from_saved_model(\"exported_models/1571172405/\")\r\n2019-10-15 15:53:25.321143: W tensorflow/core/graph/graph_constructor.cc:772] Node 'StatefulPartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 6 outputs. Output shapes may be inaccurate.\r\n\r\nIn [2]: tflite_model = converter.convert()\r\n2019-10-15 15:53:28.820835: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2019-10-15 15:53:28.820954: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-10-15 15:53:28.828875: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:812] Optimization results for grappler item: graph_to_optimize\r\n2019-10-15 15:53:28.828893: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814]   function_optimizer: Graph size after: 105 nodes (95), 191 edges (181), time = 4.658ms.\r\n2019-10-15 15:53:28.828899: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814]   function_optimizer: function_optimizer did nothing. time = 0.158ms.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-43-c548bab089a8> in <module>\r\n----> 1 tflite_model = converter.convert()\r\n\r\n~/.virtualenvs/tf2-serving/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    407\r\n    408     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(\r\n--> 409         self._funcs[0], lower_control_flow=False)\r\n    410     input_tensors = [\r\n    411         tensor for tensor in frozen_func.inputs\r\n\r\n~/.virtualenvs/tf2-serving/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow)\r\n    435\r\n    436   # Get mapping from node name to variable value.\r\n--> 437   tensor_data = _get_tensor_data(func)\r\n    438\r\n    439   # Get mapping from function name to argument types.\r\n\r\n~/.virtualenvs/tf2-serving/lib/python3.6/site-packages/tensorflow_core/python/framework/convert_to_constants.py in _get_tensor_data(func)\r\n    207       data = map_index_to_variable[idx].numpy()\r\n    208     else:\r\n--> 209       data = val_tensor.numpy()\r\n    210     tensor_data[tensor_name] = {\r\n    211         \"data\": data,\r\n\r\n~/.virtualenvs/tf2-serving/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in numpy(self)\r\n    941       ValueError: if the type of this Tensor is not representable in numpy.\r\n    942     \"\"\"\r\n--> 943     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    944     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    945\r\n\r\n~/.virtualenvs/tf2-serving/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in _numpy(self)\r\n    918       return self._numpy_internal()\r\n    919     except core._NotOkStatusException as e:\r\n--> 920       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    921\r\n    922   @property\r\n\r\n~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\n \r\nModel signature\r\n```\r\nIn [1]: !saved_model_cli show --dir exported_models/1571172405/ --tag_set serve --signature_def serving_default\r\nThe given SavedModel SignatureDef contains the following input(s):\r\n  inputs['keras_layer_input'] tensor_info:\r\n      dtype: DT_STRING\r\n      shape: (-1)\r\n      name: serving_default_keras_layer_input:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\n  outputs['dense_1'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, 4)\r\n      name: StatefulPartitionedCall_2:0\r\nMethod name is: tensorflow/serving/predict\r\n```", "@haozha111 \r\n> Could you try using TFLiteConverter.from_saved_model API instead?\r\n> \r\n> Which version of Tensorflow are you using?\r\n\r\n2.0.0", "@haozha111 \r\n\r\nTf: 2.0.0\r\nNumpy: 1.16.4\r\n\r\nHere is the error if I run:\r\n```python\r\ntflite_input_shape = tf.TensorSpec([64, 39], tf.int32)\r\ntflite_target_shape = tf.TensorSpec([64, 7], tf.float32)\r\ntflite_enc_hidden_shape = tf.TensorSpec([64, 1024], tf.float32)\r\nexport_dir = \"saved_models\"\r\ncheckpoint.f = train_step\r\nto_save = checkpoint.f.get_concrete_function(tflite_input_shape, tflite_target_shape, tflite_enc_hidden_shape)\r\ntf.saved_model.save(checkpoint, export_dir, to_save)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-37-c4f1fc30b8c5> in <module>()\r\n      5 checkpoint.f = train_step\r\n      6 to_save = checkpoint.f.get_concrete_function(tflite_input_shape, tflite_target_shape, tflite_enc_hidden_shape)\r\n----> 7 tf.saved_model.save(checkpoint, export_dir, to_save)\r\n      8 \r\n      9 # converter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    881   # Note we run this twice since, while constructing the view the first time\r\n    882   # there can be side effects of creating variables.\r\n--> 883   _ = _SaveableView(checkpoint_graph_view)\r\n    884   saveable_view = _SaveableView(checkpoint_graph_view)\r\n    885 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in __init__(self, checkpoint_view)\r\n    162     self.checkpoint_view = checkpoint_view\r\n    163     trackable_objects, node_ids, slot_variables = (\r\n--> 164         self.checkpoint_view.objects_ids_and_slot_variables())\r\n    165     self.nodes = trackable_objects\r\n    166     self.node_ids = node_ids\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py in objects_ids_and_slot_variables(self)\r\n    413       A tuple of (trackable objects, object -> node id, slot variables)\r\n    414     \"\"\"\r\n--> 415     trackable_objects, path_to_root = self._breadth_first_traversal()\r\n    416     object_names = object_identity.ObjectIdentityDictionary()\r\n    417     for obj, path in path_to_root.items():\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/graph_view.py in _breadth_first_traversal(self)\r\n    197             % (current_trackable,))\r\n    198       bfs_sorted.append(current_trackable)\r\n--> 199       for name, dependency in self.list_dependencies(current_trackable):\r\n    200         if dependency not in path_to_root:\r\n    201           path_to_root[dependency] = (\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in list_dependencies(self, obj)\r\n    106   def list_dependencies(self, obj):\r\n    107     \"\"\"Overrides a parent method to include `add_object` objects.\"\"\"\r\n--> 108     extra_dependencies = self.list_extra_dependencies(obj)\r\n    109     extra_dependencies.update(self._extra_dependencies.get(obj, {}))\r\n    110 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py in list_extra_dependencies(self, obj)\r\n    133   def list_extra_dependencies(self, obj):\r\n    134     return obj._list_extra_dependencies_for_serialization(  # pylint: disable=protected-access\r\n--> 135         self._serialization_cache)\r\n    136 \r\n    137   def list_functions(self, obj):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _list_extra_dependencies_for_serialization(self, serialization_cache)\r\n   2416   def _list_extra_dependencies_for_serialization(self, serialization_cache):\r\n   2417     return (self._trackable_saved_model_saver\r\n-> 2418             .list_extra_dependencies_for_serialization(serialization_cache))\r\n   2419 \r\n   2420   def _list_functions_for_serialization(self, serialization_cache):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py in list_extra_dependencies_for_serialization(self, serialization_cache)\r\n     76       of attributes are listed in the `saved_model._LayerAttributes` class.\r\n     77     \"\"\"\r\n---> 78     return self.objects_to_serialize(serialization_cache)\r\n     79 \r\n     80   def list_functions_for_serialization(self, serialization_cache):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in objects_to_serialize(self, serialization_cache)\r\n     73   def objects_to_serialize(self, serialization_cache):\r\n     74     return (self._get_serialized_attributes(\r\n---> 75         serialization_cache).objects_to_serialize)\r\n     76 \r\n     77   def functions_to_serialize(self, serialization_cache):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     92 \r\n     93     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n---> 94         serialization_cache)\r\n     95 \r\n     96     serialized_attr.set_and_validate_objects(object_dict)\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     51     objects, functions = (\r\n     52         super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\r\n---> 53             serialization_cache))\r\n     54     functions['_default_save_signature'] = default_signature\r\n     55     return objects, functions\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n    101     \"\"\"Returns dictionary of serialized attributes.\"\"\"\r\n    102     objects = save_impl.wrap_layer_objects(self.obj, serialization_cache)\r\n--> 103     functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n    104     # Attribute validator requires that the default save signature is added to\r\n    105     # function dict, even if the value is None.\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrap_layer_functions(layer, serialization_cache)\r\n    164   call_fn_with_losses = call_collection.add_function(\r\n    165       _wrap_call_and_conditional_losses(layer),\r\n--> 166       '{}_layer_call_and_return_conditional_losses'.format(layer.name))\r\n    167   call_fn = call_collection.add_function(\r\n    168       _extract_outputs_from_fn(layer, call_fn_with_losses),\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in add_function(self, call_fn, name)\r\n    492       # Manually add traces for layers that have keyword arguments and have\r\n    493       # a fully defined input signature.\r\n--> 494       self.add_trace(*self._input_signature)\r\n    495     return fn\r\n    496 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in add_trace(self, *args, **kwargs)\r\n    411             fn.get_concrete_function(*args, **kwargs)\r\n    412 \r\n--> 413         trace_with_training(True)\r\n    414         trace_with_training(False)\r\n    415       else:\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in trace_with_training(value, fn)\r\n    409           utils.set_training_arg(value, self._training_arg_index, args, kwargs)\r\n    410           with K.learning_phase_scope(value):\r\n--> 411             fn.get_concrete_function(*args, **kwargs)\r\n    412 \r\n    413         trace_with_training(True)\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in get_concrete_function(self, *args, **kwargs)\r\n    536     if not self.call_collection.tracing:\r\n    537       self.call_collection.add_trace(*args, **kwargs)\r\n--> 538     return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n    539 \r\n    540 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n    774       if self._stateful_fn is None:\r\n    775         initializer_map = object_identity.ObjectIdentityDictionary()\r\n--> 776         self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n    777         self._initialize_uninitialized_variables(initializer_map)\r\n    778 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    406     self._concrete_stateful_fn = (\r\n    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 408             *args, **kwds))\r\n    409 \r\n    410     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   1846     if self.input_signature:\r\n   1847       args, kwargs = None, None\r\n-> 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   1849     return graph_function\r\n   1850 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2148         graph_function = self._function_cache.primary.get(cache_key, None)\r\n   2149         if graph_function is None:\r\n-> 2150           graph_function = self._create_graph_function(args, kwargs)\r\n   2151           self._function_cache.primary[cache_key] = graph_function\r\n   2152         return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2039             arg_names=arg_names,\r\n   2040             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2041             capture_by_value=self._capture_by_value),\r\n   2042         self._function_attributes,\r\n   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    913                                           converted_func)\r\n    914 \r\n--> 915       func_outputs = python_func(*func_args, **func_kwargs)\r\n    916 \r\n    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    357         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    360 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrapper(*args, **kwargs)\r\n    513         layer, inputs=inputs, build_graph=False, training=training,\r\n    514         saving=True):\r\n--> 515       ret = method(*args, **kwargs)\r\n    516     _restore_layer_losses(original_losses)\r\n    517     return ret\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in wrap_with_training_arg(*args, **kwargs)\r\n    473         kwargs = kwargs.copy()\r\n    474         utils.remove_training_arg(self._training_arg_index, args, kwargs)\r\n--> 475         return call_fn(*args, **kwargs)\r\n    476 \r\n    477       return tf_decorator.make_decorator(\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save_impl.py in call_and_return_conditional_losses(inputs, *args, **kwargs)\r\n    555   layer_call = _get_layer_call_method(layer)\r\n    556   def call_and_return_conditional_losses(inputs, *args, **kwargs):\r\n--> 557     return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n    558   return _create_call_fn_decorator(layer, call_and_return_conditional_losses)\r\n    559 \r\n\r\n<ipython-input-21-981e13bed43a> in call(self, x, hidden)\r\n     12     def call(self, x, hidden):\r\n     13         x = self.embedding(x)\r\n---> 14         output, state = self.rnn(x, initial_state = hidden)\r\n     15         return output, state\r\n     16 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    632       additional_inputs += initial_state\r\n    633       self.state_spec = nest.map_structure(\r\n--> 634           lambda s: InputSpec(shape=K.int_shape(s)), initial_state)\r\n    635       additional_specs += self.state_spec\r\n    636     if constants is not None:\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    533 \r\n    534   return pack_sequence_as(\r\n--> 535       structure[0], [func(*x) for x in entries],\r\n    536       expand_composites=expand_composites)\r\n    537 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py in <listcomp>(.0)\r\n    533 \r\n    534   return pack_sequence_as(\r\n--> 535       structure[0], [func(*x) for x in entries],\r\n    536       expand_composites=expand_composites)\r\n    537 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in <lambda>(s)\r\n    632       additional_inputs += initial_state\r\n    633       self.state_spec = nest.map_structure(\r\n--> 634           lambda s: InputSpec(shape=K.int_shape(s)), initial_state)\r\n    635       additional_specs += self.state_spec\r\n    636     if constants is not None:\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in int_shape(x)\r\n   1183   \"\"\"\r\n   1184   try:\r\n-> 1185     shape = x.shape\r\n   1186     if not isinstance(shape, tuple):\r\n   1187       shape = tuple(shape.as_list())\r\n\r\nAttributeError: 'bool' object has no attribute 'shape'\r\n```", "I have a feeling that the model is not exported to saved model correctly. From the last error message you post, this is an error coming from the tf python saved model API.\r\n\r\nCould you provide a self-contained python script + any model file so that I can give it a try on my machine? It's hard to reason about it without running it myself.\r\n\r\nThanks!", "@haozha111 \r\nScript and model files can be found here: \r\nhttps://drive.google.com/open?id=1TezJfFBduy35uKdxPu7QlN_ukug1JYLG\r\n\r\nIt looks like it could create the SavedModel format (correctly, I think), but still not able to convert into TFLite", "Python==3.6.5\r\nNumpy==1.16.4\r\nTensorflow==2.0.0", "Hi Matthew,\r\n\r\nI took a look on your saved model, it has a metagraph named 'serving_default' but I think it's actually the training graph, not the inference graph.\r\n\r\nThis is probably because you export the saved model using those lines:\r\ncheckpoint.f = train_step\r\nto_save = checkpoint.f.get_concrete_function(tflite_input_shape, tflite_target_shape, tflite_enc_hidden_shape)\r\ntf.saved_model.save(checkpoint, export_dir, to_save)\r\n\r\n(I also visualize your saved model, and I saw those gradient ops that doesn't need during inference).\r\n\r\nAs a first step, I think we need to first create a saved model that contains the correct inference graph. I will take a look into this issue soon.\r\n\r\nHowever, solving that saved model issue alone will not fix the entire problem. Since you are using keras.layers.GRU in both your encoder and decoder, unfortunately currently TOCO can't convert  these ops to TF Lite, because it has control flow ops (While) and some other ops TF Lite doesn't support (mainly TensorList ops). We are working on a new TFLite converter to address this issue. That being said, before that new converter is released(hopefully soon), I don't see any easy ways to convert this NMT model into TF Lite, at least in a less painful way.\r\n", "Thank you so much for looking into this! I have a few follow-up questions and comments.\r\n\r\n> I took a look on your saved model, it has a metagraph named 'serving_default' but I think it's actually the training graph, not the inference graph.\r\n\r\nIs it supposed to do this by default? I have been following the documentation on saving, but maybe I missed something.\r\n\r\n> unfortunately currently TOCO can't convert these ops to TF Lite, because it has control flow ops (While) and some other ops TF Lite doesn't support (mainly TensorList ops). We are working on a new TFLite converter to address this issue\r\n\r\nI may be mistaken, but isn't TOCO getting the ax soon in favor of the TFLiteConverter Pyhton API? Based on the latest TFLite documentation, it states that RNNs can be converted successfully: https://www.tensorflow.org/lite/convert/rnn#currently_supported\r\n\r\n`keras.layers.RNN(cell, unroll=True)` is the drop-in replacement for `tf.compat.v1.nn.static_rnn` and then I could use the `tf.keras.layers.GRUCell` in place of the `tf.compat.v1.nn.rnn_cell.GRUCell`. But the `keras.layers.GRU(unroll=True)` should be equivalent to the previously mentioned composition based on what I saw in the source code. I am not specifying sequence length so this should work, should it not?\r\n\r\nSo that raises the question: is their documentation saying this should work?", "For those looking to do NMT in TFLite, a transformer approach may work for you. It only uses Embeddings, Dense, and Attention layers. I am going to diverge from this and try to see if that can be converted to TFLite and report back if it works with decent accuracy for my use case.\r\n\r\nhttps://www.tensorflow.org/tutorials/text/transformer", "@haozha111 \r\n\r\nI ran into an issue converting that as well, which is tracked here: \r\nThat notebook gave me an idea though to build the concrete function for inferencing only as mentioned in an earlier post on this issue. I can successfully convert the NMT model into a TFLite model and invoke it on dummy data.\r\n\r\nFirst, make sure `unroll = True` and `input_length` is set on the GRU layers in the encoder and decoder.\r\n\r\nBear with me here... still haven't found the best solution..\r\n\r\nThis doesn't work and gives an InaccessibleTensorException. I guess concrete functions do not like loops and you cannot convert to numpy with `.numpy()` within it. But, I want to be able to build the entire output in one call and not need much intervention on the device once the model is converted.\r\n```python\r\n@tf.function\r\ndef eval_step(enc_input):\r\n    results = []\r\n    \r\n    hidden = [tf.zeros((1, units))]\r\n    enc_out, enc_hidden = encoder(enc_input, hidden)\r\n\r\n    dec_hidden = enc_hidden\r\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\r\n\r\n\r\n\r\n    for t in tf.range(max_length_targ):\r\n        predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n\r\n        predicted_id = tf.argmax(predictions[0], output_type=tf.int32)\r\n\r\n        results.append(predicted_id)\r\n\r\n        if tf.equal(predicted_id,tf.constant(2)): # <end> index\r\n            break\r\n\r\n        # the predicted ID is fed back into the model\r\n        dec_input = tf.expand_dims([predicted_id], 0)\r\n    \r\n    return tf.convert_to_tensor(results.values(),dtype=np.int32)\r\n```\r\n\r\nThis works...unrolling the loop...\r\n\r\n\r\n```python\r\n@tf.function\r\ndef eval_step(enc_input):\r\n    hidden = [tf.zeros((1, units))]\r\n    enc_out, enc_hidden = encoder(enc_input, hidden)\r\n\r\n    dec_hidden = enc_hidden\r\n    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\r\n    \r\n    predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n    predicted_id_1 = tf.argmax(predictions[0], output_type=tf.int32)\r\n    dec_input = tf.expand_dims([predicted_id_1], 0)\r\n    \r\n    predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n    predicted_id_2 = tf.argmax(predictions[0], output_type=tf.int32)\r\n    dec_input = tf.expand_dims([predicted_id_2], 0)\r\n    \r\n    predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n    predicted_id_3 = tf.argmax(predictions[0], output_type=tf.int32)\r\n    dec_input = tf.expand_dims([predicted_id_3], 0)\r\n    \r\n    predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n    predicted_id_4 = tf.argmax(predictions[0], output_type=tf.int32)\r\n    dec_input = tf.expand_dims([predicted_id_4], 0)\r\n    \r\n    return tf.convert_to_tensor([predicted_id_1,predicted_id_2,predicted_id_3,predicted_id_4])\r\n```\r\n\r\nThis also works, but may not achieve what I need since it does not behave the same way. This would run the encoder on each call, but the notebook describes this only necessary at the beginning of the translation.\r\n\r\n```python\r\n@tf.function\r\ndef eval_step(enc_input, dec_input):\r\n    enc_output, enc_hidden = encoder(enc_input)\r\n    dec_hidden = enc_hidden\r\n\r\n    predictions, dec_hidden, _ = decoder([dec_input, dec_hidden, enc_output])\r\n\r\n    return predictions\r\n```\r\n\r\nThen the conversion:\r\n\r\n```python\r\ntflite_enc_input_shape = tf.TensorSpec([None,list(dataset.take(1))[0][0].shape[1]], tf.int32)\r\n# tflite_dec_input_shape = tf.TensorSpec([None, 1], tf.int32)\r\ncheckpoint.f = eval_step\r\nto_save = checkpoint.f.get_concrete_function(tflite_enc_input_shape)\r\n\r\ntf.random.set_seed(1234)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([to_save])\r\ntflite_model = converter.convert()\r\n\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nenc_input_shape = input_details[0]['shape']\r\nenc_input_data = np.array(np.random.randint(39,size=enc_input_shape), dtype=np.int32)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], enc_input_data)\r\n# interpreter.set_tensor(input_details[1]['index'], dec_input_data)\r\n\r\ninterpreter.invoke()\r\ntflite_results = interpreter.get_tensor(output_details[0]['index'])\r\n```\r\n\r\nAny input on how I can use loops within a concrete function to step through the sequence would be very helpful!", "> Thank you so much for looking into this! I have a few follow-up questions and comments.\r\n> \r\n> > I took a look on your saved model, it has a metagraph named 'serving_default' but I think it's actually the training graph, not the inference graph.\r\n> \r\n\r\nFor this saving issue, I'm taking a deeper look now and will post my findings if I found something working.\r\n> Is it supposed to do this by default? I have been following the documentation on saving, but maybe I missed something.\r\n> \r\n> > unfortunately currently TOCO can't convert these ops to TF Lite, because it has control flow ops (While) and some other ops TF Lite doesn't support (mainly TensorList ops). We are working on a new TFLite converter to address this issue\r\n> \r\nYes, that documentation is correct. If you unroll the RNN we can always convert it to tflite, no problem.(but it seems that you forgot to unroll your encoder RNN? Generally, if the RNN is unrolled and no sequence length is specified, there will be no control flow ops inside the model, so it should be handled by TOCO correctly. (But note that we are going to launch a new converter soon, which will provide a uniformed way to convert all kinds of RNNs).\r\n> I may be mistaken, but isn't TOCO getting the ax soon in favor of the TFLiteConverter Pyhton API? Based on the latest TFLite documentation, it states that RNNs can be converted successfully: https://www.tensorflow.org/lite/convert/rnn#currently_supported\r\n> \r\n> `keras.layers.RNN(cell, unroll=True)` is the drop-in replacement for `tf.compat.v1.nn.static_rnn` and then I could use the `tf.keras.layers.GRUCell` in place of the `tf.compat.v1.nn.rnn_cell.GRUCell`. But the `keras.layers.GRU(unroll=True)` should be equivalent to the previously mentioned composition based on what I saw in the source code. I am not specifying sequence length so this should work, should it not?\r\n> \r\n> So that raises the question: is their documentation saying this should work?\r\n\r\n", "For anyone following, I found a way to get this working by splitting up the encoder and decoder into two tflite models for the time being and then managing the loop that decodes the sequence externally.\r\n\r\n```python\r\n@tf.function\r\ndef eval_step_enc(enc_input):\r\n    enc_out, enc_hidden = encoder(enc_input, [tf.zeros((1, units))])\r\n    \r\n    return enc_out, enc_hidden\r\n\r\n@tf.function\r\ndef eval_step_dec(dec_input, enc_out, dec_hidden):\r\n    predictions, dec_hidden, _ = decoder([dec_input,dec_hidden,enc_out])\r\n    scores = tf.exp(predictions) / tf.reduce_sum(tf.exp(predictions), axis=1)\r\n    dec_input = tf.expand_dims(tf.argmax(predictions, axis=1, output_type=tf.int32), 1)\r\n    \r\n    return dec_input, enc_out, dec_hidden, scores\r\n\r\n# ...standard TFLite conversion code\r\n```\r\n\r\nI am guessing this is due to the fact certain control flow operations are not supported in TFLite that would allow a concrete function with a loop to convert properly. This will do for now. I am getting great accuracy and have this running in a native Android app on sample data.\r\n\r\nThank you for the help and looking forward to the new updated as we round out the year!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32999\">No</a>\n", "Glad to hear that it's working for you.\r\n\r\nI think with the new converter coming out soon, it's probably easier to combine the encoder/decoder into the single graph and also wrap loop inside of the model. The new converter will be great at handling the control flow ops. Please be patient about it and stay tuned. Thanks!", "Looking forward to it!", "@maatc-eostar,Hi,I am doing the similar thing and following the example:https://tensorflow.google/tutorials/text/image_captioning,and I also want to convert the model to tfLite.I find your model is similiar with my model.Can you give me your code for referance?(Also the Android code,thanks!davidblackinga@gamil.com)"]}, {"number": 32998, "title": "Cannot compile TF 2.0.0 with MacOS 10.14.6 and XCode 11.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs 10.14.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 2.0.0 and 1.15.0-rc2\r\n- **Python version**: 3.7.4\r\n- **Bazel version (if compiling from source)**: either 2.6.1, 2.6.0, 2.5.3\r\n- **GCC/Compiler version (if compiling from source)**: Apple clang version 11.0.0 (clang-1100.0.33.8)\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: within the tensorflow 2.0.0 source directory, after standard ./configure (CPU, no GPU) or particular optimization flags besides default:\r\n\r\n`bazel build --config=opt --config=v2 --verbose_failures //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nCompilation stops with following error:\r\n```\r\nERROR: /Users/feranick/Desktop/tensorflow/tensorflow/core/debug/BUILD:43:1: ProtoCompile tensorflow/core/debug/debug_service.pb.h failed (Segmentation fault): protoc failed: error executing command \r\n  (cd /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/opt/X11/bin:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/ \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/host/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/host/bin' -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src tensorflow/core/debug/debug_service.proto)\r\nExecution platform: @bazel_tools//platforms:host_platform: protoc failed: error executing command \r\n  (cd /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/Users/feranick/Library/Python/3.7/bin/:/opt/local/Library/Frameworks/Python.framework/Versions/3.7/bin:/opt/usr/local/bin/:/Users/feranick/Documents/Work/c/android-sdk-macosx/platform-tools:/opt/local/bin:/opt/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/MacGPG2/bin:/opt/X11/bin:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/:/Users/feranick/Documents/Work/c/android-sdk/platform-tools/ \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/host/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_cpp_plugin' '--grpc_out=bazel-out/host/bin' -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -I. -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -I. -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src -Iexternal/com_google_protobuf/src -Ibazel-out/host/bin/external/com_google_protobuf/src tensorflow/core/debug/debug_service.proto)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nbazel-out/host/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\n\r\n", "comments": ["I should note that compilation proceeded just fine with Xcode 10.3 and TF 2.0.0-rc2.", "As I understand with Xcode 10.3 and using  TF 2.0.0-rc2 the problem is resolved.Please, let us know if it is still an issue?. Thanks!", "Let's say that with XCode 10.3 and TF-2.0.0-rc2 it compiles. The problem is I cannot revert XCode to that, as the system automatically updated it to XCode 11. So I am stuck now.", "BTW, the issue is not unique to TF v2.0.0. TF1.15.0-rc1 and TF 1.15.0-rc2 do not compile either with XCode 11. I updated the bug description to reflect that.", "@feranick As @ravikyram suggested, Xcode 10.x  work fine for me on both macOS 10.14 and 10.15. Xcode 11 does cause problem as you reported. You can download Xcode 10.3  from [apple developer site](https://developer.apple.com/download/more/).\r\n\r\n@ravikyram and @angerson it seems the clang come with **Xcode 11**  is problematic, with **Xcode 11** and  `bazel build --host_copt=-O0 --config opt //tensorflow/tools/pip_package:build_pip_package`, I can pass what @feranick ran into.", "So the only difference for successful compilation is to add `--host_copt=-O0`, i.e. remove any optimization?", "Yes, without optimization the protoc won't crash with segmentation fault. Surely `-O0` is not an answer. Just to make sure it could be a problem of the Xcode 11 toolchain.", "Thanks for the report. This looks to me like an issue with the interaction between the protoc compiler and XCode 11.0, is that right? If that's the case, this looks like an issue for https://github.com/protocolbuffers/protobuf.", "@angerson not exactly, this could be caused by compilation flags or other settings used by TensorFlow, because if I replace the `protoc` with one that I compiled with **cmake + Xcode 11**, `protoc` works well. Easy to reproduce @feranick's problem, run\r\n```\r\nbazel build --config opt tensorflow/core/profiler:profiler_analysis_proto_py_genprot -s\r\n```\r\nwith **Xcode 11**, then you'll see something like\r\n```\r\ncd /private/var/tmp/_bazel_freedom/adcaa36b2ac85ab2f5a2434777d27f58/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH='/Users/freedom/tfpy3/bin:/Users/freedom/Downloads/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin:/opt/local/bin:/Users/freedom/work/google-cloud-sdk/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/Library/Apple/bin:/Applications/VMware Fusion.app/Contents/Public:/opt/X11/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands' \\\r\n    PYTHON_BIN_PATH=/Users/freedom/tfpy3/bin/python \\\r\n    PYTHON_LIB_PATH=/Users/freedom/tfpy3/lib/python3.7/site-packages \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n  bazel-out/host/bin/external/com_google_protobuf/protoc '--python_out=bazel-out/darwin-opt/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_python_plugin' '--grpc_out=bazel-out/darwin-opt/bin' -I. -I. -I. -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -I. -I. -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -I. -I. -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -I. -I. -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -I. -I. -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python -Iexternal/com_google_protobuf/python -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/python tensorflow/core/profiler/profiler_analysis.proto)\r\nERROR: /Volumes/Seagate5T/tf-clean/tensorflow/core/profiler/BUILD:55:1: ProtoCompile tensorflow/core/profiler/profiler_analysis_pb2.py failed (Segmentation fault): protoc failed: error executing command bazel-out/host/bin/external/com_google_protobuf/protoc '--python_out=bazel-out/darwin-opt/bin' '--plugin=protoc-gen-grpc=bazel-out/host/bin/external/grpc/grpc_python_plugin' ... (remaining 33 argument(s) skipped)\r\n```\r\n\r\nHow I compiled protoc:\r\n\r\n1. find the version of `protobuf` used by TensorFlow from `${TF_SOURCE_ROOT}/tensorflow/workspace.bzl`. What I tried just now is:\r\n```\r\n    PROTOBUF_URLS = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n        \"https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n    ]\r\n    PROTOBUF_SHA256 = \"b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59\"\r\n    PROTOBUF_STRIP_PREFIX = \"protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0\"\r\n```\r\n2. get the source; unpack it\r\n3. build it with cmake", "I think the problem is caused by **-march=native + non-O0 optimization**. With **-march=native -O2**, the protoc binary I built with cmake also crashed. With `build:opt --host_copt=-march=native`  removed from `.tf_configure.bazelrc`,  something like\r\n```\r\nbazel build --config opt tensorflow/core/profiler:profiler_analysis_proto_py_genprot\r\n```\r\nwent without problems.", "It is still an issue in that I can't compile with optimization flags. Not sure if anybody is doing much about it, as it seems this is a compiler issue.", "Using MacOS 10.15.4, Xcode 11.4. I have been able to successfully compile (with optimization flags), TF v.2.1.0. I think XCode 11.4 now fixes the issues with the previous 11.3. ", "In this case, can we close this?", "I think it can be closed, previous checking if the documentation/release notes needs to be changed to reflect this.", "In particular, [this patch ](https://github.com/bazelbuild/continuous-integration/pull/878/commits/6786208c5e6a6448f7e3b47cd490b10663c9507d) should be reversed as it seems to no longer apply.\r\n", "In our build scripts it has been reverted (see mention commits attached to the issue). If there are other places, we could accept pull requests but only against `master` branch. The release branches will only be updated when we do a new patch release.\r\n\r\nClosing as resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32998\">No</a>\n"]}, {"number": 32997, "title": "Cannot link to built tensorflow_cc.lib with Visual C++ -- undefined symbols; r2.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n   Windows 10 1903 Build 18362.356\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  NA\r\n- TensorFlow installed from (source or binary): \r\n  source\r\n- TensorFlow version: \r\n  r2.0 as of 29-Sep-2019\r\n- Python version: \r\n  3.6\r\n- Installed using virtualenv? pip? conda?: \r\n  No\r\n- Bazel version (if compiling from source): \r\n  0.29.1\r\n- GCC/Compiler version (if compiling from source): \r\n  Visual C++ 2017 14.16.27023\r\n- CUDA/cuDNN version: \r\n  10.1, 7.6.4.38\r\n- GPU model and memory: \r\n  NVIDIA Quadro P2000, 5GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nFirst off, I know I am not using the version of Bazel called for in the build instructions. In fact the version of bazel called for is a little ambiguous, but none of the versions called for would work for me. They just did not work with Visual C++ 2017 as I have it installed. But 0.29.1 did work in that it could build \"successfully\".\r\n\r\nI am building r2.0 from source to use the C++ API with an external program in a plugin DLL. I built tensorflow_cc.dll and tensorflow_cc.lib using bazel (after much struggle). But when I tried to link to tensorflow_cc.lib (and no other tensorflow objects) I get unresolved symbols from tensorflow packages. With the vanilla r2.0 source, I get unresolved symbols for the following:\r\n```\r\n?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A\r\n\r\n??0SessionOptions@tensorflow@@QEAA@XZ\r\n\r\n?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z\r\n```\r\n\r\n\r\nwhich coresponds to the following when demangled:\r\n```\r\nclass tensorflow::TensorShapeProtoDefaultTypeInternal tensorflow::_TensorShapeProto_default_instance_\r\n\r\npublic: __cdecl tensorflow::SessionOptions::SessionOptions(void) __ptr64\r\n\r\nclass tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const & __ptr64,class tensorflow::RunOptions const & __ptr64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const & __ptr64,class std::unordered_set<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const & __ptr64,struct tensorflow::SavedModelBundle * __ptr64 const)\r\n```\r\nI read some other github bug reports and discovered that there is a complex method used to export a subset of the global symbols into the .lib file that involves a python program tensorflow/tools/def_file_filter/def_file_filter.py.tpl and someone showed how to put extra symbols in that to add to the set exported. I went ahead and did that:\r\n```\r\ndiff --git a/tensorflow/tools/def_file_filter/def_file_filter.py.tpl ...\r\n@@ -154,6 +154,9 @@ def main():\r\n       else:\r\n         def_fp.write(\"\\t\" + decorated + \" DATA\\n\")\r\n       taken.add(decorated)\r\n+    def_fp.write(\"\\t??0SessionOptions@tensorflow@@QEAA@XZ\\n\")\r\n+    def_fp.write(\"\\t?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A DATA\\n\")\r\n+    def_fp.write(\"\\t?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z\\n\")\r\n     def_fp.close()\r\n\r\n   exit_code = proc.wait()\r\n```\r\nThis fixed 2 of the 3 unresolved, but the following was still undefined:\r\n```\r\n?_TensorShapeProto_default_instance_@tensorflow@@3VTensorShapeProtoDefaultTypeInternal@1@A\r\n```\r\nThe only way I could get it to work was to actually link to the object file that defined that global object (tensor_shape.pb.o) and two \".a\" libraries for protobuf (libprotobuf_lite.a & libprotobuf.a):\r\n```\r\ncl.exe -MD ... plugin.obj .wsl.obj/segTask.obj .wsl.obj/tfseg.obj /link /dll ... c:/root/tensorflow/bazel-out/x64_windows-opt/bin/tensorflow/core/_objs/protos_all_proto_cc_impl/tensor_shape.pb.o c:/root/tensorflow/bazel-out/x64_windows-opt/bin/external/com_google_protobuf/libprotobuf.a c:/root/tensorflow/bazel-out/x64_windows-opt/bin/external/com_google_protobuf/libprotobuf_lite.a c:/root/tensorflow/bazel-bin/tensorflow/tensorflow_cc.lib \r\n```\r\nI am not using any exotic tensorflow calls in my C++ code and it seems that it should just work. There are clearly other similar issues with unresolved symbols with Visual C++ builds and I am hoping the above will help to resolve those. As it is, it seems that anyone trying to deploy an inference engine using C++ on Windows will struggle mightily. I think the first issue that was solved with def_file_filter.py.tpl is simple to solve. But I just could not figure out why the 3rd symbol would never resolve. Clearly, linking in the .o file is totally bogus. I looked at the generated def file tensorflow_filtered_def_file.def and it seemed the symbol was actually in that file already without my changes to def_file_filter.py.tpl, but it still would not work.\r\n\r\n(BTW, I also posted a stackoverflow question about this https://stackoverflow.com/questions/58176287/cannot-link-c-using-tensorflow-c-api-tensorflow-cc-lib-on-windows-10-visual/58188482#58188482)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=monolithic //tensorflow:tensorflow_cc.dll  --verbose_failures\r\nbazel build --config=monolithic //tensorflow:tensorflow_cc.lib  --verbose_failures\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nLogs of the bazel link-stage builds of the DLL and LIB are attached.\r\n[build-dll.txt](https://github.com/tensorflow/tensorflow/files/3682666/build-dll.txt)\r\n[build-lib.txt](https://github.com/tensorflow/tensorflow/files/3682667/build-lib.txt)\r\n", "comments": ["Thanks for your report and especially for attaching the command logs. Can you try and replicate this at `master`, please? (`master` is newer than `r2.0`, but has not had the default flags switched over yet, IIRC. If there were any bug fixes that would have helped this, they will have been made to `master`).", "I tried building the master branch checked out Oct 16, 2019. I could not build it with bazel 0.26.1 either. It did build with bazel 0.29.1. I got the same unresolved symbols when linking with my code as above and one more unresolved:\r\n`\r\ntfseg.obj : error LNK2019: unresolved external symbol \"public: virtual __cdecl tensorflow::SavedModelBundleInterface::~SavedModelBundleInterface(void)\" (??1SavedModelBundleInterface@tensorflow@@UEAA@XZ) referenced in function \"int public: __cdecl tensorflow::SavedModelBundle::SavedModelBundle(void)::1'::dtor$0\" (?dtor$0@?0???0SavedModelBundle@tensorflow@@QEAA@XZ@4HA)\r\n`", "`I'm having similar issues.", "@Kramax,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32997\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32997\">No</a>\n"]}, {"number": 32996, "title": "tf.linalg.inv() on GPU", "body": "Hi all,\r\n\r\nIt seems that running tf.linalg.inv() on a GPU has no difference in terms of speed with respect to the CPU.\r\n\r\nIs there a way to use GPU acceleration to compute the matrix inverse faster?\r\n\r\nCheers,", "comments": ["@minaskar ,\r\nCan you please provide a sample code ? Also mention TF version used. Thanks!", "I'm using the following code as a benchmark:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef Covariance(ndim):\r\n  \r\n  a = 2.0\r\n  A = np.array([np.random.randn(ndim) + np.random.randn(1)*a for i in range(ndim)])\r\n  A = np.dot(A,np.transpose(A))\r\n  D_half = np.diag(np.diag(A)**(-0.5))\r\n  return np.dot(np.dot(D_half,A),D_half).astype('float32')\r\n\r\nC = Covariance(10000)\r\n\r\n%timeit tf.linalg.inv(C)\r\n```\r\n\r\nIt seems that setting the dtype of the input matrix to `float32` did the trick.\r\nSo now, GPU performs better than the CPU. However, both GPU and CPU seem to perform better than a TPU.\r\n\r\nI'm using tensorflow 2.0.\r\n\r\n", "@minaskar ,\r\nPlease find the result when tried from our end performance in GPU TPU and CPU in the [gist](https://colab.sandbox.google.com/gist/oanush/ab9ebcc4becb2d720951b4aee2a129b7/32996.ipynb) of colab attached. \r\n\r\n\r\n> I'm using the following code as a benchmark:\r\n> \r\n> ```\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> def Covariance(ndim):\r\n>   \r\n>   a = 2.0\r\n>   A = np.array([np.random.randn(ndim) + np.random.randn(1)*a for i in range(ndim)])\r\n>   A = np.dot(A,np.transpose(A))\r\n>   D_half = np.diag(np.diag(A)**(-0.5))\r\n>   return np.dot(np.dot(D_half,A),D_half).astype('float32')\r\n> \r\n> C = Covariance(10000)\r\n> \r\n> %timeit tf.linalg.inv(C)\r\n> ```\r\n> \r\n> It seems that setting the dtype of the input matrix to `float32` did the trick.\r\n> So now, GPU performs better than the CPU. However, both GPU and CPU seem to perform better than a TPU.\r\n> \r\n> I'm using tensorflow 2.0.\r\n\r\nlet us know if the issue resolved using `Flaot32`?\r\nThanks!", "@minaskar ,\r\nPlease let us know if there is any update on the issue ? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 32995, "title": "[1.15-rc2 CherryPick]: fix missing dependencies of mlir for tf serving oss build", "body": "PiperOrigin-RevId: 272433333", "comments": []}, {"number": 32994, "title": "Fix a minor style issue in embedding_ops", "body": "This PR fixes a minor style issue in the doc of embedding_ops. ", "comments": []}, {"number": 32993, "title": "Run sample of experts is not work on local", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):pip install tensorflow-gpu\r\n- TensorFlow version:2.0.0\r\n- Python version:Python 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:V10.0.130\r\n- GPU model and memory:GTX1080TI\r\n\r\n\r\n\r\n**Describe the problem**\r\nI can obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" . But when running sample of experts, is not work on local. ModuleNotFoundError: No module named 'tensorflow.keras'; 'tensorflow' is not a package\r\nsource file is following:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.layers import Conv2D\r\nfrom tensorflow.keras.layers import Flatten\r\nfrom tensorflow.keras import Model\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n  (x_train, y_train)).shuffle(10000).batch(32)\r\ntest_ds = tf.data.Dataset.from_tensor_slices(\r\n  (x_test, y_test)).batch(32)\r\n\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.flatten = Flatten()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n\r\nmodel = MyModel()\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalCrossentropy()\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.SparseCategoricalCrossentropy(name='test_accuracy')\r\n\r\n\r\n@tf.function\r\ndef train_step(images, labels):\r\n  with tf.GradientTape() as tape:\r\n    predications = model(images)\r\n    loss = loss_object(labels, predications)\r\n  gradients = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n  train_loss(loss)\r\n  train_accuracy(labels, predications)\r\n\r\n\r\n@tf.function\r\ndef test_step(images, labels):\r\n  predictions = model(images)\r\n  t_loss = loss_object(labels, predictions)\r\n\r\n  test_loss(t_loss)\r\n  test_accuracy(labels, predictions)\r\n\r\n\r\nEPOCHS = 5\r\n\r\n\r\nfor epoch in range(EPOCHS):\r\n    for images, labels in train_ds:\r\n      train_step(images, labels)\r\n\r\n    for test_images, test_labels in test_ds:\r\n      test_step(test_images, test_labels)\r\n\r\n    template = 'Epoch: {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n    print (template.format(epoch+1,\r\n                           train_loss.result(),\r\n                           train_accuracy.result()*100,\r\n                           test_loss.result(),\r\n                           test_accuracy.result()*100))\r\n\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython tensorflow.py (tensorflow.py is source code file)\r\n", "comments": ["Tried to reproduce it on google colab but I don't face any error. Heres the [gist](https://colab.sandbox.google.com/gist/gowthamkpr/60b42fb040fd50cb8652a1b11264fc70/untitled159.ipynb). Looks like the issue is that tensorflow is not installed well. \r\n\r\nI cannot really find the rot cause of this issue but you can take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/6548) and let me know if you can figure out what helps you solve the issue. Thanks!", "@yangzhuoran Were you able to solve the issue?", "> @yangzhuoran Were you able to solve the issue?\r\n\r\nwhen I run it on jupyter notebook , I don't face any error. But when I run it in local file by command 'python sourcefile.py', I face this error. I don't understand what happened. I think reinstall tensorflow in local compute.", "Please install all the dependencies in the fresh environment and it should work. Let me know if you are still facing the same issue?", "@yangzhuoran Are you still facing the issue?", "Yes, But I can find .pyc file in __pycache__ folder. I can run .pyc using python command. I\u2019m thinking the reason of this situation. \n\n> 2019\u5e7410\u670816\u65e5 \u4e0a\u53481:56\uff0cgowthamkpr <notifications@github.com> \u5199\u9053\uff1a\n> \n> @yangzhuoran <https://github.com/yangzhuoran> Are you still facing the issue?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/32993?email_source=notifications&email_token=AM3KTQ6FKSEHBWK2P2ISJ6DQOX76DA5CNFSM4I4Y2PFKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEBJUOIQ#issuecomment-542328610>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AM3KTQ7ZB5AZZCJKW6OTDFLQOX76DANCNFSM4I4Y2PFA>.\n> \n\n", "@yangzhuoran Can I see your` pip list`", "My pip list as follows:\r\nPackage               Version            \r\n--------------------- -------------------\r\nabsl-py               0.8.0              \r\napturl                0.5.2              \r\nasn1crypto            0.24.0             \r\nastor                 0.8.0              \r\nbleach                2.1.2              \r\nBrlapi                0.6.6              \r\ncertifi               2018.1.18          \r\nchardet               3.0.4              \r\nchrome-gnome-shell    0.0.0              \r\ncommand-not-found     0.3                \r\ncryptography          2.1.4              \r\ncupshelpers           1.0                \r\ncycler                0.10.0             \r\ndecorator             4.1.2              \r\ndefer                 1.0.6              \r\ndistro-info           0.18ubuntu0.18.04.1\r\nentrypoints           0.2.3.post1        \r\ngast                  0.2.2              \r\ngoogle-pasta          0.1.7              \r\ngrpcio                1.24.0             \r\nh5py                  2.10.0             \r\nhtml5lib              0.999999999        \r\nhttplib2              0.9.2              \r\nidna                  2.6                \r\nipykernel             4.8.2              \r\nipython               5.5.0              \r\nipython-genutils      0.2.0              \r\nipywidgets            6.0.0              \r\nJinja2                2.10               \r\njsonschema            2.6.0              \r\njupyter-client        5.2.2              \r\njupyter-console       5.2.0              \r\njupyter-core          4.4.0              \r\nKeras-Applications    1.0.8              \r\nKeras-Preprocessing   1.1.0              \r\nkeyring               10.6.0             \r\nkeyrings.alt          3.0                \r\nkiwisolver            1.1.0              \r\nlanguage-selector     0.1                \r\nlaunchpadlib          1.10.6             \r\nlazr.restfulclient    0.13.5             \r\nlazr.uri              1.0.3              \r\nlinecache2            1.0.0              \r\nlouis                 3.5.0              \r\nmacaroonbakery        1.1.3              \r\nMako                  1.0.7              \r\nMarkdown              3.1.1              \r\nMarkupSafe            1.0                \r\nmatplotlib            3.1.1              \r\nmistune               0.8.3              \r\nnbconvert             5.3.1              \r\nnbformat              4.4.0              \r\nnotebook              5.2.2              \r\nnumpy                 1.17.2             \r\noauth                 1.0.1              \r\nolefile               0.45.1             \r\nopt-einsum            3.1.0              \r\npandas                0.25.1             \r\npandocfilters         1.4.2              \r\npbr                   3.1.1              \r\npexpect               4.2.1              \r\npickleshare           0.7.4              \r\nPillow                5.1.0              \r\npip                   19.3               \r\nprompt-toolkit        1.0.15             \r\nprotobuf              3.9.2              \r\npycairo               1.16.2             \r\npycrypto              2.6.1              \r\npycups                1.9.73             \r\nPygments              2.2.0              \r\npygobject             3.26.1             \r\npymacaroons           0.13.0             \r\nPyNaCl                1.1.2              \r\npyparsing             2.4.2              \r\npyRFC3339             1.0                \r\npython-apt            1.6.4              \r\npython-dateutil       2.8.0              \r\npython-debian         0.1.32             \r\npytz                  2018.3             \r\npyxdg                 0.25               \r\nPyYAML                3.12               \r\npyzmq                 16.0.2             \r\nreportlab             3.4.0              \r\nrequests              2.18.4             \r\nrequests-unixsocket   0.1.5              \r\nscipy                 1.3.1              \r\nSecretStorage         2.3.1              \r\nsetuptools            41.2.0             \r\nsimplegeneric         0.8.1              \r\nsimplejson            3.13.2             \r\nsix                   1.11.0             \r\nssh-import-id         5.7                \r\nsystem-service        0.3                \r\nsystemd-python        234                \r\ntensorboard           2.0.0              \r\ntensorflow            2.0.0              \r\ntensorflow-estimator  2.0.0              \r\ntensorflow-gpu        2.0.0              \r\ntermcolor             1.1.0              \r\nterminado             0.7                \r\ntestpath              0.3.1              \r\ntestresources         2.0.0              \r\ntornado               4.5.3              \r\ntraceback2            1.4.0              \r\ntraitlets             4.3.2              \r\nubuntu-drivers-common 0.0.0              \r\nufw                   0.36               \r\nunattended-upgrades   0.1                \r\nunittest2             1.1.0              \r\nurllib3               1.22               \r\nusb-creator           0.3.3              \r\nwadllib               1.3.2              \r\nwcwidth               0.1.7              \r\nwebencodings          0.5                \r\nWerkzeug              0.16.0             \r\nwheel                 0.30.0             \r\nwrapt                 1.11.2             \r\nxkit                  0.0.0              \r\nzope.interface        4.3.2 ", "@yangzhuoran I think the installation is not done well. Try uninstalling Tensorflow and installing ot back.\r\nInorder just to test, you can create a clean virtual environment and try installing tensorflow and start running this code and it should work. \r\nAs this issue is not because of Tensorflow, I am closing this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32993\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32993\">No</a>\n"]}, {"number": 32992, "title": "Update CONTRIBUTING.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32992) for more info**.\n\n<!-- need_sender_cla -->", "@deepaksbmj thanks for your contribution , there is no point in having a extra line there , so closing this PR , Thank you", "This looks like Hacktoberfest spam"]}, {"number": 32991, "title": "Test //tensorflow/lite/python:tflite_convert_test is broken", "body": "I run Python tests for TFLite converter using this command \r\n`//tensorflow/lite/python:tflite_convert_test`\r\n\r\nand I have error:\r\n`ERROR: missing input file '//tensorflow/lite/python:tflite_convert.par'`\r\n\r\nPlease take a look and include this test data or maybe I run it wrong way ?\r\nThanks", "comments": ["@gargn Could you please take a look ? Thanks", "@wwwind,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "The test is not expected to work in open source currently.", "@wwwind Can you please retry from the main branch and let me know if it is still failing\r\n\r\nThanks", "Marking as fixed. Please feel free to reopen and assign to me if anything is needed.\r\n\r\nThanks"]}, {"number": 32990, "title": "Basic tutorial from Tensorflow 2.0 no longer runs on small machine", "body": "\r\n**System information**\r\n- Tensorflow basic tutorial code from https://www.tensorflow.org/tutorials/quickstart/beginner\r\n- Python version:3.7.4, standard yum repository, compiled by GCC 7.3.1 20180712 (Red Hat 7.3.1-6)] on linux\r\n- Amazon free-tier EC2 node running amazon linux 2. \r\n- pip installed version of tensorflow version 2.0.0\r\n\r\n\r\n**Describe the current behavior**\r\nDies due to memory constraints on Tensorflow 2.0.0 (but not in 1.14.0)\r\n\r\n**Describe the expected behavior**\r\nWith tensorflow 1.14.0, the tutorial works just fine, but with 2.0.0 it runs out of memory. This is a small machine that Amazon has on it's free tier (t2.micro). It comes with 1 GB of ram, and I'm not expecting it to run anything really large, but it's an ideal machine from a cost perspective to try out tensorflow basics, and it works just fine with tensorflow 1.14 and earlier.\r\n\r\n**Code to reproduce the issue**\r\nJust the tensorflow tutorial code directly from the tensorflow website.\r\n[ec2-user@ip-xxx-xxx-xxx-xxx ~]$ cat test_tf.py\r\n#!/usr/bin/env python3\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\n**Other info / logs**\r\nOutput from test_tf.py:\r\n[ec2-user@ip-xxx-xxx-xxx-xxx ~]$ ./test_tf.py\r\n2019-10-02 13:30:43.682116: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-10-02 13:30:43.775079: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400075000 Hz\r\n2019-10-02 13:30:43.778603: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3798a90 executing computations on platform Host. Devices:\r\n2019-10-02 13:30:43.778635: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2019-10-02 13:30:43.960286: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376320000 exceeds 10% of system memory.\r\nSegmentation fault\r\n", "comments": ["This might be caused due to many reasons but you can take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/18736) and try different solutions provided by users as I am unable to reproduce this issue in [colab](https://colab.sandbox.google.com/gist/gowthamkpr/ee02c43bc293210521d0e9baeb362fb4/untitled158.ipynb).", "One of the suggestions in the issue you linked was to add a swapfile. I added a 1G swapfile. It still shows the warning, which occurs right at the start and later in the process, but it does run successfully. Your colab environment has way more ram - more than 10x what this machine has. Before swap: output of free -m\r\n              total        used        free      shared  buff/cache   available\r\nMem:            983         171         473          12         338         648\r\nSwap:             0           0           0\r\n\r\nAfter  adding swap file:\r\n              total        used        free      shared  buff/cache   available\r\nMem:            983         171         473          12         337         647\r\nSwap:          1023           0        1023\r\n\r\nIt's a decent workaround, but can the increased memory requirement for 2.0  be documented for the tutorial and in the readme? ", "@dawsonlp Sure. Thanks for pointing out the issue!", "This is working-as-intended. There's some minimal test scripts on the install page if you're just verifying the install: https://www.tensorflow.org/install/pip\r\n\r\nAnd you can always jump off to [the Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb) to test.\r\n\r\nA case could be made that memory should be added to the system requirements: https://www.tensorflow.org/install/pip\r\nBut I don't think there's an exact number"]}, {"number": 32989, "title": "Fixed gast dependency in setup.py", "body": "Fixes #32319 (again maybe?)", "comments": ["We don't accept patches on release branches after final release unless they are security fixes and we do a patch release for that version.\r\n\r\nAll changes should be made against master and cherry-picked against release branches for currently ongoing releases (at the moment, that is 1.15, branch `r1.15`). Incidentally, this patch is already present in both `r1.15` and `master` so nothing left to do."]}, {"number": 32988, "title": "The simple model converted by TFLiteConverter not accelerates by Coral USB Accelerator", "body": "**Code to reproduce the issue**\r\nCode to convert the model on Macbook Pro:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ninputs = tf.keras.Input(shape=(256, 256, 3), name='model_input')\r\noutputs = tf.keras.layers.Conv2D(filters=32, kernel_size=3)(inputs)\r\nmodel = tf.keras.Model(inputs, outputs)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\ntflite_model = converter.convert()\r\nopen(\"test_model_tf2.0_by_from_keras_model.tflite\", \"wb+\").write(tflite_model)\r\n```\r\n\r\nCode to inference the model on Raspberry Pi:\r\n```\r\nimport tqdm\r\n\r\ndef tensorflow_lite():\r\n    from tflite_runtime.interpreter import Interpreter\r\n    from tflite_runtime.interpreter import load_delegate\r\n\r\n    import numpy as np\r\n\r\n    interpreter = Interpreter(\r\n            'test_model_tf2.0_by_from_keras_model.tflite',\r\n            experimental_delegates=[load_delegate('libedgetpu.so.1.0')], #with or without it\r\n        )\r\n    interpreter.allocate_tensors()\r\n   \r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    _, height, width, _ = interpreter.get_input_details()[0]['shape']\r\n\r\n    for _ in tqdm.tqdm(range(100000)):\r\n        image = np.zeros((1, 256, 256, 3,), dtype=np.float32)\r\n        set_input_tensor(interpreter, image)\r\n        interpreter.invoke()\r\n        output = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\r\n\r\ndef main():\r\n    tensorflow_lite()\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**System information**\r\nAbout machine that model had been converted on:\r\n- Macbook Pro Mid 2014, macOS Mojave 10.14.6\r\n- TensorFlow installed by `sudo pip install -U tensorflow`\r\n- TensorFlow version: v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n\r\nAbout machine that tflite model had been run on:\r\n- `Raspberry Pi 3B` + `Coral USB Accelerator`\r\n```\r\n$ cat /etc/debian_version\r\n10.1\r\n\r\n$ cat /etc/os-release\r\nPRETTY_NAME=\"Raspbian GNU/Linux 10 (buster)\"\r\nNAME=\"Raspbian GNU/Linux\"\r\nVERSION_ID=\"10\"\r\nVERSION=\"10 (buster)\"\r\n...\r\n\r\n$ uname -a\r\nLinux raspberrypi 4.19.75-v7+ #1270 SMP Tue Sep 24 18:45:11 BST 2019 armv7l GNU/Linux\r\n```\r\n\r\nTo set up Coral USB Accelerator i was guided by [https://coral.withgoogle.com/docs/accelerator/get-started/](url) with `libedgetpu1-std` variant.\r\nTflite had been installed by:\r\n`pip3 install tflite_runtime-1.14.0-cp37-cp37m-linux_armv7l.whl`\r\n\r\n**Current behavior**\r\nThe inference code on `Raspberry Pi 3B` + `Coral USB Accelerator` shows  ~12it/s and **no white led signaling on `Coral USB Accelerator` observed.**\r\n\r\n**Expected behavior**\r\nI expect at least x5-10 it/s from what i observe now. Also i expect the led signaling on inference because that is what i'm observing while running the `tflite/python/examples/classify_image.py` example.\r\n\r\nAlso here is my text output for `classify_image.py` example:\r\n```\r\n$ python3 classify_image.py --model models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels models/inat_bird_labels.txt --image images/parrot.jpg\r\n\r\nInitializing TF Lite interpreter...\r\nINFO: Initialized TensorFlow Lite runtime.\r\n----INFERENCE TIME----\r\nNote: The first inference on Edge TPU is slow because it includes loading the model into Edge TPU memory.\r\n120.6ms\r\n13.5ms\r\n13.5ms\r\n13.8ms\r\n13.6ms\r\n-------RESULTS--------\r\n923 Ara macao (Scarlet Macaw): 0.76562\r\n```\r\n\r\nI also tried to convert the model with tf-14.0.0 (on MacOS) and got the same result.", "comments": ["@mirth did you [compile](https://coral.withgoogle.com/docs/edgetpu/compiler/) your tflite models into Edge TPU enabled tflite models.", "@freedomtan i didn't. i think it explains everything", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32988\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32988\">No</a>\n"]}, {"number": 32987, "title": "[tf2.0.0] tf.keras.layers.GRU incorrect output of model.fit_generator trying to run Francois Chollet's notebook", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have slightly modified Francoiis Chollet's Jupyter notebook 6.3 from his book \"Deep Learning with Python\" so that it runs on tensorflow.keras rather than keras (i.e. \"from tensorflow.keras import layers\" instead of \"from keras import layers\" etc)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): built from source\r\n- TensorFlow version (use command below): 2.0.0 (i.e. the release)\r\n- Python version: 3.7 conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10 / 7.6.4\r\n- GPU model and memory: RTX 2080 Ti and Tesla V100 (tried on both. error occurs on both)\r\n\r\n**Describe the current behavior**\r\nPlease see attached Jupyter Notebook \r\n[6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip](https://github.com/tensorflow/tensorflow/files/3680903/6.3-advanced-usage-of-recurrent-neural-networks-Copy1.zip).  I am going through Francois Chollet's book \"Deep Learning with Python\" and running the code in his Jupyter Notebooks in Tensorflow 2.0.0 adapting the code to \"import tensorflow.keras\" instead of \"import keras\".  Notebook 6.3, (under the heading \"1.6 Using recurrent dropout to fight overfitting\") has a model with a tensorflow.keras.layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, float_data.shape[-1])).  The data is read earlier in the notebook from jena_climate_2009_2016.csv. With tensorflow 2.0.0 and tensorflow.keras I get a loss of 20417499919998144512.0000 and val_loss: 1.1059 after the first epoch and similar figures after subsequent epochs.  These figures are simply wrong (see below).  It also runs about 10x slower than its supposed to.  I interrupted the kernel after 4 epochs.  The original notebook (from Francois Chollet) is here: [link to github](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb) and includes the correct output.\r\n\r\n**Describe the expected behavior**\r\nI ran the same code with keras (not tf.keras) using a tensorflow 1 backend a while ago and it gave correct results.  The loss after 1 or 2 epochs is supposed to be around 0.3  The validation loss is supposed to be a little less than 0.3.  The graph below this code was produced using keras and a tensorflow 1 backend.  It shows the correct output.  Francis Chollet's original notebook (as linked to github above) also shows the correct output.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nDownload the data as follows:\r\ncd ~\r\nmkdir Datasets\r\ncd ~/Datasets\r\nmkdir jena_climate\r\ncd jena_climate\r\nwget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\r\nunzip jena_climate_2009_2016.csv.zip\r\n\r\nRun jupyter notebook and load the notebook that is attached to this issue in a Python 3.7 environment with tensorflow 2.0.0.\r\nIn the notebook replace /home/daniel with /home/(your username).\r\nRun each cell from the beginning of the notebook so you load the data and create the generators before you get to the example under heading 1.6.  Then try to run the example.  You will find that the loss and val_loss are terribly wrong.\r\n\r\nEDIT: Since writing this, I have tried to run more code in the notebook.  The code under heading \"1.7 Stacking recurrent layers\" also runs incorrectly in tensorflow 2.0.0 using tensorflow.keras.  The loss produced is \"nan\" (it should be around 0.3).  I think it is the same problem with layers.GRU\r\n\r\nThe problem does **not** occur with tensorflow.keras in tensorflow 1.1.4.\r\n\r\nEDIT: The rest of the remaining code in the notebook runs correctly, e.g. bidirectional GRU runs OK", "comments": ["Thanks for reporting the issue. I can reproduce it locally. It seems that only when adding recurrent_dropout, the loss just peaked. I will take a closer look wrt to the recurrent_dropout.", "Checked recurrent_dropout in details, it seems that the issue only occurs in implementation=2 which is the default in tf 2.0 (we changed from implementation 1 to 2 as default value). In short, the implementation 1 and 2 should be numerically same, but just be implemented differently. 1 uses more samll tensors and slide weights into individual gates, while 2 uses more bulk matmul. \r\n\r\nThe issue I believe is caused by an oversight when implementation 2 is introduce. When dropout is enabled, we applied the same dropout masks for all the gates, since it doesn't slide weights into individual gates. This might cause some numerical instability, eg if both candidate gate and forget gate gets the same masked state, they might be contradictory to each other (I don't have the detailed math proof here).\r\n\r\nIn short, for now you can build GRU with \"implementation=1\" in the init arg to walk around issue. I will send a fix very soon to force the implementation == 1 if the recurrent_dropout is specified. ", "Should be fixed now.", "Hi,\nWas this issue fixed on master or r2.0 branch or both?\nCheers,\nDaniel\n\nOn Tue, 22 Oct. 2019, 8:07 am Qianli Scott Zhu, <notifications@github.com>\nwrote:\n\n> Should be fixed now.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32987?email_source=notifications&email_token=AAB26QUJK3BUC43ML53ULALQPYKZHA5CNFSM4I4VK2J2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEB3Y2XA#issuecomment-544705884>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAB26QUUNLNT66JXPZYWRDTQPYKZHANCNFSM4I4VK2JQ>\n> .\n>\n", "This is only fixed in HEAD so nightly will have it, but not 2.0. We will have tf 2.1 soon which will contain this fix.", "Hi, is there any more information on what the issue was (conceptual bug vs numerical instability)?  We've been using implementation=2 on the LSTM and recurrent_dropout for a while without issue.  We found this code change when debugging a 400% increase in inference time for identically defined Kera's models and are considering ways to force implementation 2 (downgrade to TF2.0 before this fix or remove recurrent_dropout).\r\n\r\nAlso, if this issue was made apparent by a change in default implementation, perhaps the default logic could be more advanced (changing default based on recurrent_dropout) without overriding explicitly passed parameters?  \r\n\r\n[edit] alternatively, is it safe to use weights from a model trained with implementation=1, in a model for inference using implementation=2 (since inference won't need to enable dropout)?"]}, {"number": 32986, "title": "Report You must feed a value for placeholder when I do feed the value.", "body": "Tensorflow works in parameter-server mode.\r\nworker1 starts training success, but worker 0 reports an error.\r\n\r\nOS: macOS\r\nPython: 3.7.4\r\nTensorflow: 1.14.0\r\n\r\n```\r\nI1002 19:19:12.119244 123145589936128 coordinator.py:219] Error reported to Coordinator: From /job:worker/replica:0/task:0:\r\nYou must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]\r\n\t [[node attention/onehot_c1 (defined at /Users/liuda/Tencent/code/nlp/attention/transformer/model.py:38) ]]\r\n\r\nOriginal stack trace for 'attention/onehot_c1':\r\n  File \"./train.py\", line 224, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./train.py\", line 77, in main\r\n    train(FLAGS)\r\n  File \"./train.py\", line 135, in train\r\n    model = Model(hidden_units, FLAGS, \"train\")\r\n  File \"/Users/liuda/Tencent/code/nlp/attention/transformer/model.py\", line 38, in __init__\r\n    input_placeholder = tf.placeholder(tf.int64, [None, None, None], name=\"onehot_\" + fn)  # batchsize, seqsize, mfea\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2143, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6262, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nYou must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]\r\n\t [[{{node attention/onehot_c1}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 495, in run\r\n    self.run_loop()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/supervisor.py\", line 1045, in run_loop\r\n    [self._sv.summary_op, self._sv.global_step])\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: From /job:worker/replica:0/task:0:\r\nYou must feed a value for placeholder tensor 'attention/onehot_c1' with dtype int64 and shape [?,?,?]\r\n\t [[node attention/onehot_c1 (defined at /Users/liuda/Tencent/code/nlp/attention/transformer/model.py:38) ]]\r\n\r\nOriginal stack trace for 'attention/onehot_c1':\r\n  File \"./train.py\", line 224, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"./train.py\", line 77, in main\r\n    train(FLAGS)\r\n  File \"./train.py\", line 135, in train\r\n    model = Model(hidden_units, FLAGS, \"train\")\r\n  File \"/Users/liuda/Tencent/code/nlp/attention/transformer/model.py\", line 38, in __init__\r\n    input_placeholder = tf.placeholder(tf.int64, [None, None, None], name=\"onehot_\" + fn)  # batchsize, seqsize, mfea\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2143, in placeholder\r\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 6262, in placeholder\r\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\nI print the feed_dict, onehot_c1 was generated success as follows\r\n\r\n```\r\nattention/onehot_c1:0 <class 'numpy.ndarray'> (10, 15, 5) int64\r\n```\r\n\r\nWorker 1 trains normally and gives training info\r\n```\r\ntrain info 2019-10-02 19:20:16 epo 0 step 931 local_step 931 auc 0.6139 loss 0.0840\r\n```", "comments": ["same issue,do you have slove it?", "same issue, can't trace where I haven't fed the placeholder(I'm convinced I fed them all)"]}, {"number": 32985, "title": "Tensorflow C++ LOG problem", "body": "### System information\r\n- **Linux Ubuntu 18.04**:\r\n- **GCC: 7.4.0**\r\n- **no CUDA or cuDNN used**\r\n- **Tensorflow 1.14**\r\n\r\n### Steps to reproduce\r\n\r\n```c++\r\n#include <memory>\r\n\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n\r\nint main(int, char**) {\r\n    tensorflow::GraphDef graph;\r\n    tensorflow::SessionOptions options;\r\n    tensorflow::Session* session = nullptr;\r\n    tensorflow::Status status = tensorflow::NewSession(options, &session);\r\n    if (session == nullptr) {\r\n        std::cout << \"nullpt session\" << std::endl;\r\n        std::cout << status << std::endl;\r\n        return -1;\r\n    }\r\n    std::cout << status << std::endl;  // should be OK\r\n    status = session->Create(graph);\r\n    std::cout << status << std::endl;  // should be OK\r\n    status = session->Close();\r\n    std::cout << status << std::endl;  // should be OK\r\n}\r\n```\r\n\r\nIf I compile this code with the gcc flags `-Werror -O -g -pthread` the  program runs perfectly. \r\n```\r\n2019-10-02 16:40:34.265739: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-10-02 16:40:34.265786: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-10-02 16:40:34.265803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pcen36738): /proc/driver/nvidia/version does not exist\r\nOK\r\nOK\r\nOK\r\n```\r\n\r\nBut if I use these flags `-Werror -O2 -pthread` the program doesn't create a session and give this error:\r\n\r\n```\r\n2019-10-02 16:38:59.090280: E tensorflow/core/common_runtime/session.cc:81] Not found: No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.\r\nnullpt session\r\nNot found: No session factory registered for the given session options: {target: \"\" config: } Registered factories are {}.\r\n```\r\n\r\nThe problem dissapears when I add this line: `LOG(ERROR) << status;`\r\n\r\n```c++\r\n#include <memory>\r\n\r\n#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n\r\nint main(int, char**) {\r\n    tensorflow::GraphDef graph;\r\n    tensorflow::SessionOptions options;\r\n    tensorflow::Session* session = nullptr;\r\n    tensorflow::Status status = tensorflow::NewSession(options, &session);\r\n    if (session == nullptr) {\r\n        LOG(ERROR) << status;\r\n        std::cout << \"nullpt session\" << std::endl;\r\n        std::cout << status << std::endl;\r\n        return -1;\r\n    }\r\n    std::cout << status << std::endl;  // should be OK\r\n    status = session->Create(graph);\r\n    std::cout << status << std::endl;  // should be OK\r\n    status = session->Close();\r\n    std::cout << status << std::endl;  // should be OK\r\n}\r\n```\r\n\r\n```\r\n2019-10-02 16:42:26.867554: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-10-02 16:42:26.867614: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-10-02 16:42:26.867654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (pcen36738): /proc/driver/nvidia/version does not exist\r\nOK\r\nOK\r\nOK\r\n```\r\n\r\nSo I think there is a problem with the Logger of tensorflow that it makes madatory to declarate it with some optimization flags.\r\n\r\nDo you know why does it happen?\r\n\r\nThanks.", "comments": ["@Shagai \r\nDid you run it on CPU or GPU?\r\nWhy some logs print about the error of GPU?\r\n`could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory`", "Apologies for the delay in response.\r\nFrom the log information  `could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory` it looks like you have installed `tensorflow-gpu==1.14` version which requires [gpu setup](https://www.tensorflow.org/install/gpu).\r\nFeel free to reopen if this is still an issue. Thanks!"]}, {"number": 32983, "title": "BoostedTreesClassifier double free or corruption (!prev)", "body": "I am running the BoostedTreesClassifier tutorial (https://www.tensorflow.org/tutorials/estimator/boosted_trees) on tf2.0 \r\n\r\nDuring the training call:\r\n\r\n> # Train model.\r\n> linear_est.train(train_input_fn, max_steps=100)\r\n> \r\n\r\na \"double free or corruption (!prev)\" is logged and the python process crashes\r\n\r\nTF 2.0.0 (pip installed)\r\nUbuntu 19.04\r\npython 3.7\r\ncuda 10.0.130\r\nGPU: RTX 2080 Ti\r\n\r\nOther tensorflow components (ie keras) work fine\r\n", "comments": ["I am unable to reproduce the above error in colab. The github gist is [here](https://colab.sandbox.google.com/gist/gowthamkpr/e795c91a7fdaa9c7766c5a9381da474e/copy-of-boosted_trees.ipynb). I cannot answer it as it is specific to your system.\r\n\r\nBut I think this is a duplicate of this [issue](https://github.com/tensorflow/tensorflow/issues/6968). Go through the issue and it should help you in resolving it.", "The malloc trick did not work for me:\r\n```\r\nsudo apt-get install libtcmalloc-minimal4\r\nexport LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\r\n```\r\nAnd neither did the numpy trick:\r\n```\r\npip3 install --user --no-binary=:all: --force-reinstall numpy\r\n```\r\n", "@sockfish As mentioned in this comment [here](https://github.com/tensorflow/tensorflow/issues/7268#issuecomment-278130615) this is unlikely a Tensorflow issue. For more details, you can refer to the issue [here](https://github.com/tensorflow/tensorflow/issues/7268).", "I got same error\r\n\r\nDocker container: 2.0.0-gpu-py3-jupyter\r\nGPU: Nvidia GeForce GTX 1660 Ti\r\nDriver: 418.87.00_linux\r\nHost: Ubuntu 18.04 LTS"]}]