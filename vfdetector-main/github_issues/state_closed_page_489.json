[{"number": 39112, "title": "TensorflowLite convertion Fatal error", "body": "**System information**\r\n- Windows Platform\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):1.15\r\n\r\ni first build toco\r\n`bazel run -c opt tensorflow/lite/toco`\r\n\r\nthen\r\n`toco --graph_def_file=object_detection/inference_graph_for_tflite/tflite_graph.pb \\ --output_file=object_detection/inference_graph_for_tflite/detect.tflite \\ --input_shapes=1,300,300,3 \\ --input_arrays=normalized_input_image_tensor \\ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\ --allow_custom_ops`\r\n\r\nresult\r\n```Fatal Python error: Aborted```\r\n\r\n```\r\nCurrent thread 0x00003744 (most recent call first):\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52 in execute\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\site-packages\\absl\\app.py\", line 250 in _run_main\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\site-packages\\absl\\app.py\", line 299 in run\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40 in run\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89 in main\r\n  File \"C:\\Users\\Acer\\Anaconda3\\envs\\tensorflowX\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7 in <module>\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\runpy.py\", line 85 in _run_code\r\n  File \"c:\\users\\acer\\anaconda3\\envs\\tensorflowx\\lib\\runpy.py\", line 193 in _run_module_as_main\r\n```\r\n\r\nI was thinking that the error must be because of the issue i just post here:\r\n```\r\nhttps://github.com/tensorflow/tensorflow/issues/39111\r\n```\r\n\r\n\r\n", "comments": ["@HardworkingPig \r\nPlease provide with simple stand alone code for us to replicate the issue faced.\r\nAlso is there any particular reason to use an older version of tensor flow would you want to try on a later version and see if it helps.", "I don't mind using 2.x version I just use this cause most tutorial i get was version 1.x and there was \r\n issues with pycoco. So, i trained my model in that version. Is it possible to use 2.x conversion to convert 1.x ?", "@HardworkingPig \r\nNo there is no such conversion from 2.x to 1.x as requested for.\r\nkindly try with later versions and see if that helps resolve the issue.", "@Saduf2019\r\nyes, I can't provide the code since, I also tweak with the python and can't remember all. Is there still no support for object tracking in version 2.x? and is it possible to run an application with a train model of version 1.x to version 2.x?", "@HardworkingPig\r\nAs informed earlier there is no such conversion from 2.x to 1.x as requested for., please confirm if we may move this to closed status.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39111, "title": "tensorflow build from source c++", "body": "**System information**\r\n- Windows Platform\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:1.15\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):2.0.0\r\n\r\nBuild Toco using bazel run at the end error show up, is this necessary, or it is ok to leave it be?\r\n\r\n```ERROR: C:/tensorflow/tensorflow/core/framework/BUILD:591:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16'```\r\n\r\nhere it is,\r\n\r\n```ERROR: C:/tensorflowexpire/tensorflow/core/framework/BUILD:591:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16' failed (Exit 2)\r\nc:\\users\\acer\\_bazel_acer\\q66yhmmw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1028): error C2061: syntax error: identifier 'Kind'\r\nc:\\users\\acer\\_bazel_acer\\q66yhmmw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1134): note: see reference to class template instantiation 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>' being compiled\r\nc:\\users\\acer\\_bazel_acer\\q66yhmmw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src/Tensor/TensorBlock.h(1037): error C2061: syntax error: identifier 'Kind'\r\nTarget //tensorflow/lite/toco:toco failed to build\r\nINFO: Elapsed time: 1135.067s, Critical Path: 33.35s\r\nINFO: 508 processes: 508 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully```\r\n", "comments": ["@HardworkingPig \r\n\r\nPlease, provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Please post full error message, not only the first line", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39111\">No</a>\n"]}, {"number": 39110, "title": "MultiWorkerMirroredStrategy doesn't work without memory_growth", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Container image `tensorflow/tensorflow:2.1.0-gpu-py3` (tf-nightly-gpu installed)\r\n- TensorFlow version (use command below): 2.2.0-dev20200501\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: release 10.1, V10.1.243, but I can't find cuDNN libraries using a command `cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2`\r\n- GPU model and memory: (NVIDIA Tesla T4(16G) * 4) * 2 nodes on GCP\r\n- kubeflow 1.0.2 (kubernetes 1.15.11, tf-operator 1.0)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/4569678/tf_env.txt)\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nTensorFlow version:  `v1.12.1-31004-g203aa8b634 2.2.0-dev20200501`\r\n\r\n**Describe the current behavior**\r\nif memory_growth of each GPU doesn't set to true as below\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    except RuntimeError as e:\r\n        print(e)\r\n```\r\n\r\nOut of memory errors occur\r\n```\r\n2020-05-03 02:05:59.765039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-05-03 02:05:59.769309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-03 02:05:59.769338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 2 3\r\n2020-05-03 02:05:59.769346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N Y N N\r\n2020-05-03 02:05:59.769350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   Y N N N\r\n2020-05-03 02:05:59.769354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 2:   N N N Y\r\n2020-05-03 02:05:59.769359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 3:   N N Y N\r\n2020-05-03 02:05:59.770642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.774976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.779921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.785132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.789412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.793147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:0 with 13297 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\r\n2020-05-03 02:05:59.793269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.801442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:1 with 13297 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\r\n2020-05-03 02:05:59.801564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.817011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:2 with 13297 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)\r\n2020-05-03 02:05:59.817149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-03 02:05:59.831155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:3 with 13323 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)\r\n2020-05-03 02:05:59.834444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> dist-worker-0.default.svc:2222, 1 -> dist-worker-1.default.svc:2222, 2 -> dist-worker-2.default.svc:2222, 3 -> dist-worker-3.default.svc:2222, 4 -> localhost:2222}\r\n2020-05-03 02:05:59.834974: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222\r\nWARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\nlocal data directory. If you'd instead prefer to read directly from our public\r\nGCS bucket (recommended if you're running on GCP), you can instead pass\r\n`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\r\n\r\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  7.41 file/s]2020-05-03 02:06:02.069644: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 12.99G (13943597824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.072746: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 11.69G (12549237760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.075769: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 10.52G (11294313472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.078466: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 9.47G (10164881408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.081365: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 8.52G (9148393472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.084128: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 7.67G (8233553920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.086712: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.90G (7410198528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.089443: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.21G (6669178368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.092258: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.59G (6002260480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.094925: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.03G (5402034176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:02.097778: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.53G (4861830656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n.\r\n.\r\n.\r\n.\r\n2020-05-03 02:06:12.431390: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.0K (5120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.433206: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.435134: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.437051: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.438725: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.440388: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.442053: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.444054: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.445745: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.447422: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.449275: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.451169: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.453220: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.454900: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.456610: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.458291: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.460314: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.462209: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-03 02:06:12.464132: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n.\r\n.\r\n.\r\n```\r\n\r\n- nvidia-smi info\r\n```\r\n## Node 1\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\r\n| N/A   64C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\r\n| N/A   66C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |\r\n| N/A   64C    P0    29W /  70W |  14220MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |\r\n| N/A   64C    P0    30W /  70W |  14220MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     11553      C   python                                     14209MiB |\r\n|    1     11553      C   python                                     14209MiB |\r\n|    2     11553      C   python                                     14209MiB |\r\n|    3     11553      C   python                                     14209MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n## Node 2\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\r\n| N/A   59C    P0    28W /  70W |  15103MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\r\n| N/A   64C    P0    29W /  70W |  15103MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |\r\n| N/A   68C    P0    32W /  70W |  15103MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n^C|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |\r\n| N/A   66C    P0    30W /  70W |  15103MiB / 15109MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     18804      C   python                                     13541MiB |\r\n|    0     18917      C   python                                      1013MiB |\r\n|    0     18972      C   python                                       243MiB |\r\n|    0     19285      C   python                                       295MiB |\r\n|    1     18804      C   python                                       241MiB |\r\n|    1     18917      C   python                                       301MiB |\r\n|    1     18972      C   python                                     13537MiB |\r\n|    1     19285      C   python                                      1013MiB |\r\n|    2     18804      C   python                                       301MiB |\r\n|    2     18917      C   python                                      1013MiB |\r\n|    2     18972      C   python                                       243MiB |\r\n|    2     19285      C   python                                     13535MiB |\r\n|    3     18804      C   python                                       251MiB |\r\n|    3     18917      C   python                                     13589MiB |\r\n|    3     18972      C   python                                       239MiB |\r\n|    3     19285      C   python                                      1013MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nMost of all, I would like to know the reason why MultiWorkerMirroredStrategy doesn't work without `memory_growth`\r\nand whether it is normal operation or not.\r\nif it adversely affects performance, the strategy should work without memory_growth as the example code [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n- My simple code\r\n```\r\nimport os, json\r\nos.environ['NCCL_DEBUG'] = 'INFO'\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntfds.disable_progress_bar()\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n    except RuntimeError as e:\r\n        print(e)\r\n\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING\r\n\r\nBATCH_SIZE = 64\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * 2\r\nBUFFER_SIZE = 10000\r\n\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n      tf.keras.layers.MaxPooling2D(),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(64, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()\r\n  options = tf.data.Options()\r\n  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n  train_datasets = train_datasets.with_options(options)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\n\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@jazzsir Can you please check these two resources [one](https://stackoverflow.com/questions/36838770/how-to-interpret-tensorflow-output) and [two](https://stackoverflow.com/questions/44232898/memoryerror-in-tensorflow-and-successful-numa-node-read-from-sysfs-had-negativ). I think setting the following might help you.\r\n\r\n`config.gpu_options.per_process_gpu_memory_fraction=0.3`\r\n\r\nBased on the warning, can you input `try_gcs=True` to `tfds.load`\r\n\r\n> WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\n> local data directory. If you'd instead prefer to read directly from our public\r\n> GCS bucket (recommended if you're running on GCP), you can instead pass\r\n> `try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\r\n\r\n", "Does this happen consistently? Or is it after multiple runs? \r\nThis can happen if you are sharing GPU resources with another process and hence run into errors during initialization. The set_memory_growth option should not be related to using MultiWorkerMirroredStrategy. Have you tried running this with MirroredStrategy on a single machine(4GPUs)? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I figured it out, I ran the Strategy on Kubeflow platform where GPUs are allocated to multiple workers.\r\nThanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39110\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39110\">No</a>\n"]}, {"number": 39109, "title": "TensorFlow Lite Metadata duplication", "body": "@lu-wang-g @petewarden @aselle\r\n\r\n`ModelMetadata` defines another `description` property when one is already defined on `Model`.\r\n\r\nWould it make sense to encode model level properties like `author`, `name`, `license`, `version` as `Metadata` properties on the model itself instead of defining them in `ModelMetadata`. That way all TensorFlow Lite models could carry such information as part of the model without relying on model_schema.fbs for decoding (similar to [metadata_props](https://github.com/onnx/onnx/blob/master/docs/IR.md) in ONNX).\r\n\r\nSee lutzroeder/netron#481", "comments": ["@lu-wang-g Could you please take a look?", "@lutzroeder ModelMetadata is a centralized place to hold all metadata of the model. It is easier for users to get the information at once by parsing ModelMetadata. Since TFLite metadata is still at experimental, we didn't mix it with the existing fields in the TFLite schema. It is a good point that we should remove the duplicated fields in the TFLite schema once metadata_schema.fbs is mature.  \r\n\r\nSeems like you had a protptype to show TFLite metadata in Netron. It will be super help for the metadata use cases. Thanks for working on it. Let me know if you have any questions when implementing the feature. Thanks! ", "@lu-wang-g\r\n\r\n> It is easier for users to get the information at once by parsing ModelMetadata. \r\n\r\nIs that true? Since it's hidden inside the metadata of the `.tflite` file and requires a separate schema there is more complexity involved getting to this information. Especially for top-level model properties like `author`, `description` and `license` why require another standard if they can be added as `(string, byte[])` to the model itself without changing the schema - isn't that the purpose of the metadata table in `.tflite`?\r\n\r\n> It is a good point that we should remove the duplicated fields in the TFLite schema once metadata_schema.fbs is mature.\r\n\r\nWill the files that exist now continue to work if the schema keeps changing or will files that get created with the experimental feature right now be in an invalid format in the future?\r\n\r\nHaving two separate model descriptions is problematic right now. Which one should a tool show? The one in the `.tflite` file or the one in `TFLITE_METADATA`? Showing both is confusing as well...\r\n\r\n> Seems like you had a protptype to show TFLite metadata in Netron. It will be super help for the metadata use cases. \r\n\r\n[Netron](https://github.com/lutzroeder/netron) 4.1.4 will show the metadata if present.\r\n\r\n<img width=\"495\" alt=\"screenshot\" src=\"https://user-images.githubusercontent.com/438516/81023839-8b963580-8e26-11ea-960d-378600ca8455.png\">\r\n\r\n\r\nWas there a reason the metadata variable was called `TFLITE_METADATA` upper case instead of using lower case like other metadata property names? `TFLITE` seems redundant?\r\n\r\n\r\n\r\n", "@lutzroeder \r\n\r\n> Is that true? Since it's hidden inside the metadata of the `.tflite` file and requires a separate schema there is more complexity involved getting to this information. Especially for top-level model properties like `author`, `description` and `license` why require another standard if they can be added as `(string, byte[])` to the model itself without changing the schema - isn't that the purpose of the metadata table in `.tflite`?\r\n\r\nThe metadata can be extracted through the metadata extractor library. We've launched the [Java version](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/metadata/java/src/java/org/tensorflow/lite/support/metadata), and the C++ library will come soon. Users can also convert the metadata into a Json file through the [metadata displayor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/support/metadata/metadata.py#L422). Where the metadata is located in the model should be transparent for users. \r\n\r\nThe metadata table in .tflite is not designed for the information like TFLite metadata, but mainly used as buffers of internal data that is supposed to be hidden from users. Having users editing against it directly is error prone, because the metadata table accepts data in any types and any formats in general. \r\n\r\n> Will the files that exist now continue to work if the schema keeps changing or will files that get created with the experimental feature right now be in an invalid format in the future?\r\n\r\n> Having two separate model descriptions is problematic right now. Which one should a tool show? The one in the `.tflite` file or the one in `TFLITE_METADATA`? Showing both is confusing as well...\r\n\r\nUp to now, we're pretty confident about the Metadata schema and the fields it defines. It works well on common image models such as classification and object detection. It will be moved out from experimental soon. Regarding my previous comment of deprecating certain fields, it was an impression at first sight, and it's mainly just about the description field of the model. I agree it's quite confusing to have two model description fields, and I think the one in .tflite is not used that much so that should be OK to deprecate it. But we'll need to further evaluate if it makes sense to do so. \r\n\r\n> <img alt=\"screenshot\" width=\"495\" src=\"https://user-images.githubusercontent.com/438516/81023839-8b963580-8e26-11ea-960d-378600ca8455.png\">\r\n\r\nThe UI looks so cool! Really appreciate your efforts for support TFLite Metadata in Netron! \r\n\r\n> Was there a reason the metadata variable was called `TFLITE_METADATA` upper case instead of using lower case like other metadata property names? `TFLITE` seems redundant?\r\n\r\nIt's just a token to identify the TFLite Metadata, and to be differentiated from other metadata entries in the metadata table. Again, it is supposed to be transparent to users, a nuance that hopefully won't be noticed by them.\r\n", "Closing this issue for now. Feel free to reopen it if you have another questions. Thanks!"]}, {"number": 39108, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot serialize protocol buffer of type tensorflow.GraphDef as the serialized size (2643020353bytes) would be larger than the limit (2147483647 bytes)", "body": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot serialize protocol buffer of type tensorflow.GraphDef as the serialized size (2643020353bytes) would be larger than the limit (2147483647 bytes)", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "Please fill in issue template and provide a minimal reproducer\r\n\r\nAlthough, this error probably makes sense. To prevent DOS, we include some upper limits on the serialization. Without having a minimal reproducer, there's hardly a way to provide guidance around this", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39108\">No</a>\n"]}, {"number": 39107, "title": "Implementation of custom operations in the layers", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n**Problem Description: If we want to implement our own custom operation in place of dot products there is no such provision to date. Right now I want to experiment with the Conv2d layer with the Coefficient of variation operation instead of Dot products.** \r\nReference Stack Overflow question seeking for same kind of help:\r\nhttps://stackoverflow.com/questions/47968757/how-to-make-a-customized-tf-nn-conv2d-of-tensorflow\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@vikalpjain91,\r\nCould you please elaborate your concern and fill in the issue template along with the code to reproduce the issue. Thanks!", "This is not an issue. It is a feature requirement. Generally, the Convolutional layer is doing dot products with a different kernel. what if we want to do some custom operation just multiplication of the segment with kernel and then find the coefficient of variation. for that case, we can not do it straightforwardly. Is there any feature in TensorFlow we can implement that?", "@vikalpjain91,\r\nSorry for the delayed response. Can you please let us know if this issue is resolved with respect to this [Stack Overflow Answer](https://stackoverflow.com/a/47973905/11530462)? Also, please refer to the Documentation of [Custom Layers](https://www.tensorflow.org/tutorials/customization/custom_layers) and [this Tutorial](https://www.tensorflow.org/guide/keras/custom_layers_and_models) for more details on Custom Operations on Layers.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39106, "title": "[ROCm] AMDGPU XLA compiler bugfixes, HLO slice sorting", "body": "This PR:\r\nImplements HSACO cache, to avoid rerunning (expensive) AMDGPU compilation for identical modules\r\nMakes sure that AMDGPU backend deletes temporary files after compilation\r\nImplements deterministic sorting of HLO slices (without it, identical HLOs may result in different IRs)\r\nSupplies workarounds for two bugs in the ROCm backend", "comments": ["Hi,\r\n\r\nFor commits we generally try to have a self-descriptive message; is it possible to update the title of this PR? (or if there are multiple bugfixes/features, to split it up?)", "@ekuznetsov139 Can you please check @cheshire's comments and keep us posted. Thanks!", "@ekuznetsov139 Can you please check @cheshire's comments and resolve conflicts?. Thanks!", "@ekuznetsov139 additionally tests would be great, if possible.", "Tests for which part?", "All of it?", "@ekuznetsov139 Can you please check @cheshire's comments and resolve conflicts?. Thanks!\r\n", "@ekuznetsov139 gentle ping", "@ekuznetsov139 Any update on this PR? Please. Thanks!", "@gbaned I occasionally come here, look at my changes, try to imagine how any of them (let alone all of them) could be covered with unit tests, and generally fail. Do not close the PR, I'm sure I'll come up with something eventually.", "> Implements HSACO cache, to avoid rerunning (expensive) AMDGPU compilation for identical modules\r\n> Makes sure that AMDGPU backend deletes temporary files after compilation\r\n> Implements deterministic sorting of HLO slices (without it, identical HLOs may result in different IRs)\r\n> Supplies workarounds for two bugs in the ROCm backend\r\n\r\n@ekuznetsov139 From your initial message it seems this PR is doing at least 3 different things. I think it would be simpler to split it first.\r\n\r\n\r\n", "@ekuznetsov139 Can you please check @cheshire's comments and resolve conflicts?. Thanks!", "I've resubmitted part of this PR as #41641.\r\nThe conflict occurs because the part of patch that introduces HLO slice sorting is no longer applicable (because the function has been changed from returning a map to returning a vector). I need to test the change and see if slice sorting is still relevant.\r\n"]}, {"number": 39105, "title": "Added POC for Tensorflow Program (tf_program)", "body": "This PR attempts to add the first prototype for Tensorflow program. It is a direct conversion from a python function to mlir module. The target dialect is `tf dialect` (with some added operations such as tfp.If, tfp.While, tfp.And and tfp.Or). It can transform a simple Fibonacci program (using while loop)\r\n\r\nSigned-off-by: Shraiysh Vaishay <cs17btech11050@iith.ac.in>", "comments": ["@shraiysh Can you please check build failures. Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39105) for more info**.\n\n<!-- need_author_consent -->", "I think I messed up while trying to change my parent commit and syncing my forked repository! I will close this and open a new PR. Apologies for the error.", "You can always force push a new revision to the original branch, it should never be needed to close a PR and open a new one for this purpose I think.", "Okay, will force push next time onwards! Thanks! (Although, I am not sure if force pushing will alter the participants which have been added because of the error)"]}, {"number": 39104, "title": "[Lite] data: Fix memory leak", "body": "Free dynamically data before return.", "comments": ["@gaurav1086 Can you please fix build failures ? Thanks!"]}, {"number": 39103, "title": "WARNING:tensorflow:Skipping full serialization of Keras model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Nvidia K80 12GB\r\n\r\n**Describe the current behavior**\r\nI am trying to save Encoder and Decoder models using tf.saved_model.save as a part of a sequence to sequence problem. This models can be saved fine on my windows laptop, however when running on a remote server with GPU I get the following warning when saving:\r\nWARNING:tensorflow:Skipping full serialization of Keras model <seq2seq_model.Encoder object at 0x7f4f2c2b53c8>, because its inputs are not defined.\r\nWARNING:tensorflow:Skipping full serialization of Keras model <seq2seq_model.Decoder object at 0x7f4f2c1d5eb8>, because its inputs are not defined.\r\n\r\nWhenever I try to then load the models using tf.saved_model.load() and then do some inference, I get TypeError: '_UserObject' object is not callable. This doesn't happen on if the saving was performed on my laptop.\r\n\r\n**Describe the expected behavior**\r\nThe models should be saved without a warning, then loading the model and performing inference should work. I'm not sure why, when the model is saved on my laptop, I dont get a warning and it works fine.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\r\n\r\nDROPOUT = 0.0\r\nLSTM_DIM = 512\r\n\r\nclass Encoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding, n_units, batch_size, use_segment_embedding, segment_embedding_dim):\r\n        super(Encoder, self).__init__()\r\n        self.n_units = n_units\r\n        self.batch_size = batch_size\r\n        \r\n        self.embedding = embedding\r\n        \r\n        # segment embedding are used so that this model can better distinguish between persona and message segments\r\n        # pad segment vectors with 0's exactly like word vectors\r\n        if use_segment_embedding:\r\n            # segment_embedding_dim must be the same as output_dim of word embedding\r\n            self.segment_embedding = Embedding(3, segment_embedding_dim, trainable=True, mask_zero=True, name=\"segment_embedding\")\r\n        else:\r\n            # use a zero segment embedding which will have no effect on the model\r\n            self.segment_embedding = Embedding(3, segment_embedding_dim, weights=[np.zeros((3, segment_embedding_dim))], trainable=False, mask_zero=True, name=\"segment_embedding\")\r\n        \r\n        self.lstm1 = LSTM(n_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\", name=\"enc_lstm1\")\r\n        \r\n    @tf.function\r\n    def call(self, inputs):\r\n        input_utterence, segment_tokens, initial_state = inputs\r\n        input_embed = self.embedding(input_utterence)\r\n        segment_embed = self.segment_embedding(segment_tokens)\r\n        \r\n        combined_embed = tf.add(input_embed, segment_embed)\r\n        \r\n        encoder_states, h1, c1 = self.lstm1(combined_embed, initial_state=[initial_state, initial_state])\r\n        \r\n        return encoder_states, h1, c1\r\n    \r\n    def create_initial_state(self):\r\n        return tf.zeros((self.batch_size, self.n_units))\r\n\r\n\r\nclass Decoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding, n_units, batch_size):\r\n        super(Decoder, self).__init__()\r\n        self.batch_size = batch_size\r\n        self.n_units = n_units\r\n        \r\n        self.embedding = embedding\r\n        \r\n        self.lstm1 = LSTM(n_units, return_sequences=True, return_state=True, recurrent_initializer=\"glorot_uniform\", name=\"dec_lstm1\")\r\n        \r\n        self.dropout = Dropout(DROPOUT)\r\n        \r\n        # attention\r\n        # Ct(s) = V tanh(W1 hs + W2 ht)\r\n        # where hs is encoder state at timestep s and ht is the previous\r\n        # decoder timestep (which is at timestep t - 1)\r\n        self.W1 = Dense(n_units)\r\n        self.W2 = Dense(n_units)\r\n        self.V  = Dense(1)\r\n        \r\n        # from_logits=True in loss function, it will apply the softmax there for us\r\n        self.out_dense1 = Dense(vocab_size)\r\n    \r\n    @tf.function\r\n    def call(self, inputs):\r\n        input_word, encoder_outputs, is_training, hidden = inputs\r\n        h1, c1 = hidden\r\n        \r\n        # ------ Attention ------ #\r\n        # => (batch_size, 1, n_units)\r\n        decoder_state = tf.expand_dims(h1, 1)\r\n        \r\n        # score shape => (batch_size, src_timesteps, 1)\r\n        score = self.V(\r\n            tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_state)) )\r\n        \r\n        attn_weights = tf.nn.softmax(score, axis=1)\r\n        \r\n        # context vector is a weighted sum of attention weights with encoder outputs\r\n        context_vec = attn_weights * encoder_outputs\r\n        # => (batch_size, n_units)\r\n        context_vec = tf.reduce_sum(context_vec, axis=1)\r\n        # ------ ------ #\r\n        \r\n        input_embed = self.embedding(input_word)\r\n        \r\n        # feed context vector as input into LSTM at current timestep\r\n        input_embed = tf.concat([tf.expand_dims(context_vec, 1), input_embed], axis=-1)\r\n        \r\n        decoder_output, h1, c1 = self.lstm1(input_embed, initial_state=[h1, c1])\r\n        \r\n        # (batch_size, 1, n_units) => (batch_size, n_units)\r\n        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\r\n        \r\n        decoder_output = self.dropout(decoder_output, training=is_training)\r\n        decoder_output = self.out_dense1(decoder_output)\r\n        \r\n        return decoder_output, attn_weights, h1, c1\r\n\r\nvocab_size = 100\r\n\r\nembedding_matrix = Embedding(vocab_size, 300, trainable=True, mask_zero=True, name=\"tied_embedding\")\r\n\r\nencoder = Encoder(vocab_size, embedding_matrix, LSTM_DIM, 64, True, 300)\r\ndecoder = Decoder(vocab_size, embedding_matrix, LSTM_DIM, 64)\r\n\r\nenc_state = enc_state = encoder.create_initial_state()\r\nout, h1, c1 = encoder([tf.random.uniform((64, 10), 0, 99),\r\n        tf.random.uniform((64, 10), 0, 2),\r\n        enc_state])\r\n\r\ndecoder([tf.random.uniform((64, 1), 0, 99), out, True, [h1, c1]])\r\n\r\nencoder_fn = \"seq2seq_encoder\"\r\ndecoder_fn = \"seq2seq_decoder\"\r\ndecoder_states_spec = [\r\n            tf.TensorSpec(shape=[None, LSTM_DIM], dtype=tf.float32, name='h1'), \r\n            tf.TensorSpec(shape=[None, LSTM_DIM], dtype=tf.float32, name='c1')]\r\n        \r\ntf.saved_model.save(encoder, encoder_fn , signatures=encoder.call.get_concrete_function(\r\n        [\r\n            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='input_utterence'),\r\n            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='segment_tokens'),\r\n            tf.TensorSpec(shape=[None, 512], dtype=tf.float32, name=\"initial_state\")\r\n        ]\r\n))\r\n    \r\ntf.saved_model.save(decoder, decoder_fn, signatures=decoder.call.get_concrete_function(\r\n        [\r\n            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='input_word'), \r\n            tf.TensorSpec(shape=[None, None, LSTM_DIM], dtype=tf.float32, name=\"encoder_output\"),\r\n            tf.TensorSpec(shape=[], dtype=tf.bool, name=\"is_training\"),\r\n            decoder_states_spec\r\n        ]\r\n))\r\n\r\nencoder = tf.saved_model.load(encoder_fn)\r\ndecoder = tf.saved_model.load(decoder_fn)\r\n\r\nenc_state = enc_state = tf.zeros((64, LSTM_DIM))\r\nout, h1, c1 = encoder([tf.random.uniform((64, 10), 0, 99),\r\n        tf.random.uniform((64, 10), 0, 2),\r\n        enc_state])\r\n\r\ndecoder([tf.random.uniform((64, 1), 0, 99), out, True, [h1, c1]])\r\n```\r\n\r\nAgain the above code does run fine on my windows laptop, but not on the remote server (it's google deep learning vm)\r\n", "comments": ["@psyfb2 \r\nI ran the code shared above but do not face the error reported  \"_TypeError: '_UserObject_' \" , please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/17acf2a4f21841b61c4ec0423180743c/39103.ipynb)\r\nAlso i ran the code on nightly and there are no warnings [[gist for the same](https://colab.sandbox.google.com/gist/Saduf2019/cc4f96bd5e316d3f6d16a384960ff55d/untitled165.ipynb)], could you please try your code on nightly and let us know if that helps.", "I am also seeing this same issue. Would love to hear your solution.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39103\">No</a>\n", "> I am also seeing this same issue. Would love to hear your solution.\r\n\r\nFacing same issue any luck with yours", "I've got the same WARNING, the reason is the tf versions for training and saving are different. "]}, {"number": 39102, "title": "Tensorflow installation issues", "body": "**System information**\r\n- OS Platform and Distribution - Windows 10\r\n- TensorFlow version: latest\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: - virtualenv\r\n\r\nGreetings,\r\n\r\nI hope this is the correct place to submit an inquiry of this nature, if it is not, please forgive my confusion & please point me in the right direction. I greatly appreciate your time & consideration.\r\n\r\nI am new to Python & Tensorflow. I've done some coding with C in the past, mostly when I was in college. I am determined to learn Python & to utilize both Python & Tensorflow for AI & Machine Learning purposes.\r\n\r\nI've had difficulties in getting Tensorflow to install properly. I started by installing the latest version of Python which didn't seem to like my attempts at installing Tensorflow, I then went with Python 3.6.4-amd64. I installed that, created a fresh directory for my environments, then installed pip & virtual env, then created a virtual environment to setup with Tensorflow.\r\n\r\nOne of the confusing issues I keep encountering is that when I install pip & virtualenv, and eventually Tensorflow, it keeps sending it by default to C:\\user\\username\\appdata\\roaming\\python etc, my question is, how do I prevent it from doing that? I am trying to install in the direct being utilized in the command prop, I call up the fresh directory I created for my virtual environment, then activate the virtual environment, and no matter what I do it keeps sending all new install files into the appdata/roaming user directory sub folders.\r\n\r\nThis is causing the incredibly annoying issue of making it impossible for me to proceed with utilizing Tensorflow because I get nothing but errors on missing files, path directory etc etc. I even tried manually moving some of the files over to the virtual environment directory and that worked in some cases, but did not solve the overall problem.\r\n\r\nOkay, now that I've made it painfully apparent how much of an uneducated newbie I am with all of this, may someone please give me some advice. The first step is admitting you need help, and I clearly do as I've spent several hours with my eyes glued to various articles and tutorials that have left me with more questions than answers. I truly appreciate any help you're willing to provide. Just a loner trying to figure this all out & increase my knowledge along the way. Thanks for your time,\r\n", "comments": ["After activating the virtual environment type `where pip` in the Windows Command Prompt. This is just to check which `pip` executable is being used by the virtual environment.", "It may occur because you may not have Visual C++ Redistributable for Windows. Please download it.\r\n\r\nRefer [here](https://www.tensorflow.org/install/pip#windows) to check if you have installed properly\r\n\r\nAlso, refer similar issues.\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer #36167 #38615 and see if it helps you.\r\n\r\nThanks!", "@astrocreep85 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the[ latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Thank you everyone, I was easily able to get Tensorflow all setup via Anaconda with no issues. My proc is a 64bit processor, the only thing I think I missed was the latest visual c++ redistributable. Thanks again,", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39102\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39102\">No</a>\n"]}, {"number": 39101, "title": "TFLite Interpreter fails to load quantized model on iPhone", "body": "**Similar issue: https://github.com/tensorflow/tensorflow/issues/28163**\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock MobilenetV2\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 5s\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.1\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Geforce GTX 1650, 4 GB\r\n\r\n**Describe the current behavior**\r\nI have created a new tflite model based on MobilenetV2.\r\ntf.keras.applications.MobileNetV2(input_shape=[SIZE, SIZE, 3], include_top=False)\r\n\r\nIt works well without quantization using CPU on iOS. I should say that TensorFlow team did a great job, many thanks.\r\n\r\nUnfortunately there is a problem with latency.  I have read TF documentation related to optimization (post trainig quantization) and workerd with dynamic range quantization.\r\n\r\nI executed the following python code:\r\n\r\n```\r\ntflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('C:\\Work\\Python\\NN\\MobileNet_v2_128')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"MobileNet_v2_128_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)\r\n```\r\n\r\nAfter this model was added to XCode project on MAC. Pod file contains the following framework:\r\n_pod 'TensorFlowLite', '~> 1.13.1'_\r\n\r\n**Error:**\r\ntflite::InterpreterBuilder return this error: \"Didn't find op for builtin opcode 'CONV_2D' version '2'\"\r\n\r\n**Describe the expected behavior**\r\nI suppose this should work faster without errors.\r\n\r\n**Other info / logs** \r\nI also tried to use float16 quantization.\r\n\r\nPython code:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('C:\\Work\\Python\\NN\\MobileNet_v2_128')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\ntflite_quant_model = converter.convert()\r\nopen(\"MobileNet_v2_128_qua_float16.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\nIn swift code I used MetalDelegate\r\n\r\nWith _pod 'TensorFlowLiteSwift', '0.0.1-nightly'_\r\nI have no errors, but model doesn\u2019t work\r\n\r\nWith _pod 'TensorFlowLiteSwift', '2.1.0'_\r\nI have the following error:\r\n\r\n2020-05-01 21:36:13.578369+0300 TFL Segmentation[6367:330410] Initialized TensorFlow Lite runtime. 2020-05-01 21:36:20.877393+0300 TFL Segmentation[6367:330397] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)\r\n\r\nIs it possible to use MobilenetV2.tflite quantized model in XCode Swift project?\r\n\r\nBest regards,\r\nDmitriy", "comments": ["Just want to confirm that the workflow works without `converter.optimizations = [tf.lite.Optimize.DEFAULT]?\r\n`", "> Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?\r\n\r\nWith the 2.1 build, does it work if you run *without* the Metal delegate? Using the CPU? In general, quantized models don't work with GPU (Metal) acceleration.", "> > Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?\r\n> \r\n> With the 2.1 build, does it work if you run _without_ the Metal delegate? Using the CPU? In general, quantized models don't work with GPU (Metal) acceleration.\r\n\r\nYes. It works well with CPU (without delegate).\r\nBut is it possible to use  quantized MobilenetV2 model in XCode for Swift project? Maybe I need to use another approach/library?", "> Just want to confirm that the workflow works without `converter.optimizations = [tf.lite.Optimize.DEFAULT]? `\r\n\r\nI'm trying to understand... is it possible to to use quantized MobilenetV2 model in XCode for Swift project?", "> is it possible to to use quantized MobilenetV2 model in XCode for Swift project?\r\n\r\nYes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.", "> > is it possible to to use quantized MobilenetV2 model in XCode for Swift project?\r\n> \r\n> Yes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.\r\n\r\nThanks for answer. \r\nDo you mean pod '**TensorFlowLiteSwift**', '**2.1.0**'?\r\n\r\nI tried to use pod 'TensorFlowLiteSwift', '2.1.0' and float16 quantization.\r\n\r\nBut I have the following error:\r\n2020-05-01 21:36:13.578369+0300 TFL Segmentation[6367:330410] Initialized TensorFlow Lite runtime. 2020-05-01 21:36:20.877393+0300 TFL Segmentation[6367:330397] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)\r\n\r\nP.S. I will check this again tomorrow...", "> > is it possible to to use quantized MobilenetV2 model in XCode for Swift project?\r\n> \r\n> Yes, this is possible, however, that is independent of the issue with using Metal delegation on a quantized model (which isn't yet supported). But note that you need the latest pod (2.1.0) to use the latest set of quantized operators for your model.\r\n\r\nI have changed a code in this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios\r\nand used pod 'TensorFlowLiteSwift', '2.1.0'\r\nMobileNetV2 **works well** with quantization and without quantization.\r\n\r\nAnd one more question...\r\nAs I understand I cannot use GPU acceleration with MobilenetV2.tflite . Because we can use the following list of models with GPU: https://www.tensorflow.org/lite/performance/gpu#supported_models_and_ops.\r\nIs it correct?\r\n\r\n", "> As I understand I cannot use GPU acceleration with MobilenetV2.tflite .\r\n\r\nAre you referring to the quantized version? The floating point version should work, but the quantized version will not. If you're unable to use the GPU with the *floating* point version, that's a bug that we should address.", "> > As I understand I cannot use GPU acceleration with MobilenetV2.tflite .\r\n> \r\n> Are you referring to the quantized version? The floating point version should work, but the quantized version will not. If you're unable to use the GPU with the _floating_ point version, that's a bug that we should address.\r\n\r\nToday I tried to use GPU for my model using this example: https://github.com/tensorflow/examples/tree/master/lite/examples/image_segmentation/ios\r\n\r\nI have tested stock MobilenetV2.tflite model with pod 'TensorFlowLiteSwift', '2.1.0' and Metal delegate. It works, but _slower then CPU_.  \r\n**CPU**: preprocessing - 114ms, postprocessing - 201ms.\r\n**GPU**: preprocessing - 130ms, postprocessing - 212ms\r\n\r\nI thought I can implement float16 quantization to use GPU, but it's not possible with 'TensorFlowLiteSwift', '2.1.0'.\r\nGPU approach work slower than CPU (probably because it's try to convert Float32 to Float16)\r\n\r\nSo I have only one way to speed up segmentation process. \r\n1. Use CPU.\r\n2. Implement Quantization aware training.\r\n\r\nIs it correct?\r\n\r\n", "@Dmitriy90 I think you should be comparing the `Model Inference` time instead, when using the image_segmentation example.\r\n\r\nThe pre-/post-processing logic is run outside of the TFLite inference, so whether or not you use the Metal delegate is irrelevant. The difference you were seeing was likely within the typical margin of error.\r\n\r\nCould you check again whether the Metal delegate helps shortening the model inference time? Also please make sure that you're [using the release build](https://www.tensorflow.org/lite/performance/gpu#step_5_release_mode) when testing these.\r\n\r\nAnother interesting point is that you're using the iPhone 5s model, on which we haven't tested our Metal delegate. The difference in hw specs compared to more recent models might also be one of the reasons if you're not seeing enough speed boost with GPU.", "Hi @yyoon \r\nThanks for answer. I have implemented the following recomendations: https://www.tensorflow.org/lite/performance/gpu#step_5_release_mode\r\n\r\n**CPU**: And it works very fast (40 - 60ms) for different MobileNetV2.tflite versions (input image sizes 128x128, 160x160, 192x192 and 224x224)\r\n\r\n**GPU**: works a bit faster, but for MobileNetV2.tflite with input 128x128.\r\nFor 160x160, 192x192 and 224x224 I see the following error: \r\n\"Ececution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)\"\r\n\r\nAnyway it's **fast enough** for me. Thanks for help. Tensorflow support team is amaising.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39101\">No</a>\n"]}, {"number": 39100, "title": "Python Keras model in C++", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 and Ubuntu 18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): yes\r\n- TensorFlow version (use command below): 2.01\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nIf I set names in Keras and want to use the trained model in C++ then there seem to be no names for the nodes, so I can't set inputs or get outputs.\r\n\r\n**Standalone code to reproduce the issue**\r\nPython Code on the Windows 10 machine:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(layers.Input(2, name='INPUT'))\r\n# Adds a densely-connected layer with 64 units to the model:\r\nmodel.add(layers.Dense(8, name='layer1', input_shape=(2,), activation='tanh'))\r\n# Add another:\r\nmodel.add(layers.Dense(4, name='layer2', input_shape=(8,), activation='sigmoid'))\r\n# Add an output layer with 1 output unit:\r\nmodel.add(layers.Dense(1, name='outputlayer', input_shape=(4,), activation='sigmoid'))\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),\r\n              loss=tf.keras.losses.BinaryCrossentropy(),\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nprint(model.inputs[0].name)\r\nimport numpy as np\r\n\r\ndata = np.array([\r\n    [0, 0],\r\n    [1, 0],\r\n    [0, 1],\r\n    [1, 1]\r\n])\r\nlabels = np.array([\r\n    [0],\r\n    [1],\r\n    [1],\r\n    [0]\r\n])\r\n\r\nmodel.fit(data, labels, epochs=500)\r\ntf.keras.models.save_model(model, 'model')\r\n```\r\nC++ code on the linux machine:\r\n\r\n```\r\ntensorflow::Tensor inputs(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 2}));\r\nauto input_mapped = inputs.tensor<float, 2>();\r\ninput_mapped(0, 0) = 0.0;\r\ninput_mapped(0, 1) = 1.0;\r\n\r\nstd::vector<std::pair<std::string, tensorflow::Tensor>> inp = {{\"INPUT\", inputs}};\r\n\r\nstd::vector<tensorflow::Tensor> outputs;\r\nstd::vector<std::string> output_names({\"output\"});\r\n\r\nstatus = model.session->Run(inp, output_names, {}, &outputs);\r\n\r\n```\r\nWhat is the problem here? Also if I set names for the layers in Keras, save the model, load it again than the names are replaced with others. And if I use these other names in the C++ application it doesn't work.\r\n\r\nHere are the error message:\r\n\r\n2020-05-02 13:38:31.256880: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] model_pruner failed: Internal: Could not find node with name 'outputlayer'\r\n\r\nInvalid argument: Tensor INPUT:0, specified in either feed_devices or fetch_devices was not found in the Graph\r\n\r\n\r\n", "comments": ["@praetorianer777 You can access layer names as below. Except `tf.keras.layers.Input`(which is input to the model), all the Layer names from loaded model are same as from the original model. [Here](https://colab.research.google.com/gist/jvishnuvardhan/59bcca0e7716c4493263537eb028c9fd/untitled151.ipynb) is a gist for y/our reference. \r\n\r\nYou can print layer names and check as \r\n\r\n```\r\nfor layer,layer_loaded in zip(model.layers,loaded_model.layers):\r\n  print('From Original Model: ', layer.name, ' From loaded_model: ',layer_loaded.name)\r\n```\r\n#output\r\n```\r\nFrom Original Model:  layer1  From loaded_model:  layer1\r\nFrom Original Model:  layer2  From loaded_model:  layer2\r\nFrom Original Model:  outputlayer  From loaded_model:  outputlayer\r\n```\r\n `model.summary() also shows only these three layers. There is small change in the name of `Input` layer as shown below\r\n\r\n```\r\nmodel.input_names  #['INPUT']\r\n\r\nloaded_model.input_names. #['layer1_input']\r\n```\r\n ", "Hello, thanks for your reply, but how can I access the input and output in C++?", "Does anybody has an idea?", "As of now, the Keras model isn't restored when loading into C++. Instead what I think you're loading is the default serving signature -- You should use the [saved_model_cli](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface) to get the input and output names \r\n```\r\nsaved_model_cli show --dir model --all\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39100\">No</a>\n"]}, {"number": 39099, "title": "[Regression] Cloud TPU crashes with  UnavailableError: failed to connect to all addresses", "body": "- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly/ 2.2.0-dev20200502\r\n- Python version: 3.7\r\n- TPU model : TPU v3-8 with tf-nightly\r\n\r\n**Describe the current behavior**\r\nI have been using TPU v3-8's with tf-nightly without any issues, however I started  receiving this after upgrading the VM and TPU v3 from 2.2.0-dev20200429 to latest tf-nightly ie. 2.2.0.dev20200502.\r\n\r\nEdit [1] : The notebook runs fine on colab with `2.2.0-rc3`\r\nEdit [2] :  Not fixed in `2.2.0.dev20200504` aswell\r\n\r\nI cannot post my codebase here since it is pretty big, and since the official guide for tpu also triggers the same error, I have attached it here.\r\n\r\n**Standalone code to reproduce the issue**\r\n[notebook to reproduce the bug](https://gist.github.com/srihari-humbarwadi/c9bd4b2dec4666252a8d6319ddd87060)\r\n\r\n**Other info / logs**\r\n```\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-10-8b804771207f> in <module>\r\n      1 model.fit(train_dataset,\r\n      2           epochs=5,\r\n----> 3           validation_data=test_dataset)\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     70   def _method_wrapper(self, *args, **kwargs):\r\n     71     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 72       return method(self, *args, **kwargs)\r\n     73 \r\n     74     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    893       data_handler._initial_epoch = (  # pylint: disable=protected-access\r\n    894           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\r\n--> 895       for epoch, iterator in data_handler.enumerate_epochs():\r\n    896         self.reset_metrics()\r\n    897         callbacks.on_epoch_begin(epoch)\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in enumerate_epochs(self)\r\n   1153     \"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\r\n   1154     with self._truncate_execution_to_epoch():\r\n-> 1155       data_iterator = iter(self._dataset)\r\n   1156       for epoch in range(self._initial_epoch, self._epochs):\r\n   1157         if self._insufficient_data:  # Set by `catch_stop_iteration`.\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __iter__(self)\r\n    706     worker_iterators = _create_iterators_per_worker(self._cloned_datasets,\r\n    707                                                     self._input_workers,\r\n--> 708                                                     enable_legacy_iterators)\r\n    709     if enable_legacy_iterators:\r\n    710       iterator = DistributedIteratorV1(self._input_workers, worker_iterators,\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _create_iterators_per_worker(worker_datasets, input_workers, enable_legacy_iterators)\r\n   1368       if tf2.enabled() and not enable_legacy_iterators:\r\n   1369         iterator = _SingleWorkerOwnedDatasetIterator(worker_datasets[i], worker,\r\n-> 1370                                                      worker_devices)\r\n   1371       else:\r\n   1372         iterator = _SingleWorkerDatasetIterator(worker_datasets[i], worker,\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices, components, element_spec)\r\n   1225         raise ValueError(error_message)\r\n   1226       super(_SingleWorkerOwnedDatasetIterator, self).__init__(dataset, worker,\r\n-> 1227                                                               devices)\r\n   1228 \r\n   1229   def _make_iterator(self):\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices)\r\n   1071     self._devices = devices\r\n   1072     self._element_spec = dataset.element_spec\r\n-> 1073     self._make_iterator()\r\n   1074 \r\n   1075   def _make_iterator(self):\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _make_iterator(self)\r\n   1235     with ops.device(self._worker):\r\n   1236       self._iterator = multi_device_iterator_ops.OwnedMultiDeviceIterator(\r\n-> 1237           self._dataset, self._devices, source_device=host_device)\r\n   1238 \r\n   1239   @property\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py in __init__(self, dataset, devices, max_buffer_size, prefetch_buffer_size, source_device, components, element_spec)\r\n    560                                       incarnation_id, prefetch_buffer_size,\r\n    561                                       experimental_slack)\r\n--> 562           iterator = iter(ds)\r\n    563           self._device_iterators.append(iterator)\r\n    564           iterator_handles.append(iterator._iterator_resource)  # pylint: disable=protected-access\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __iter__(self)\r\n    407     \"\"\"\r\n    408     if context.executing_eagerly() or ops.inside_function():\r\n--> 409       return iterator_ops.OwnedIterator(self)\r\n    410     else:\r\n    411       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __init__(self, dataset, components, element_spec, job_token)\r\n    602           context.context().device_spec.device_type != \"CPU\"):\r\n    603         with ops.device(\"/cpu:0\"):\r\n--> 604           self._create_iterator(dataset)\r\n    605       else:\r\n    606         self._create_iterator(dataset)\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _create_iterator(self, dataset)\r\n    628               output_shapes=self._flat_output_shapes))\r\n    629       if self._job_token is None:\r\n--> 630         gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\r\n    631       else:\r\n    632         gen_experimental_dataset_ops.make_data_service_iterator(\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py in make_iterator(dataset, iterator, name)\r\n   2950       return _result\r\n   2951     except _core._NotOkStatusException as e:\r\n-> 2952       _ops.raise_from_not_ok_status(e, name)\r\n   2953     except _core._FallbackException:\r\n   2954       pass\r\n\r\n~/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6808   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6809   # pylint: disable=protected-access\r\n-> 6810   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6811   # pylint: enable=protected-access\r\n   6812 \r\n\r\n~/tf/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1588419621.643701261\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3937,\"referenced_errors\":[{\"created\":\"@1588419621.215578031\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]} [Op:MakeIterator]\r\n```\r\n@jhseu   ", "comments": ["I can confirm that this bug has started to occur in the past few days on tf-nightly both in our setup and also when using the example [run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py) script. We have since then moved to TF2.1 (which unfortunately is lacking a few features compared to tf-nightly). \r\n\r\nHappy to give additional information to reproduce the bug.", "> I can confirm that this bug has started to occur in the past few days on tf-nightly both in our setup and also when using the example [run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py) script. We have since then moved to TF2.1 (which unfortunately is lacking a few features compared to tf-nightly).\r\n> \r\n> Happy to give additional information to reproduce the bug.\r\n\r\nPlease do! I'm facing a similar issue but my codebase is too big to publish here.", "Hi All, the issues should have been fixed in the latest TPU nightly. Can you retry? Thanks!", "@rxsang confirmed working on `2.2.0-dev20200505`", "Thanks @rxsang! Yep confirmed to be working!", "Thanks closing this!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39099\">No</a>\n", "This bug is still there. I have tried many versions and combinations. When I create dataset using from_tensor_slices, it works fine (but only on small datasets. after that protobuf limitations come into play). But when I try from_generator, or when I'm trying to prepare dataset on th fly (by using tokenization inside of map), it's throwing \"failed to connect to all addresses\". I think TF team needs to do something with it.", "Hi fingoldo@,\r\n\r\nSorry for the issue. `Dataset.from_generator` is expected to not work with TPUs as it uses py_function underneath which is incompatible with Cloud TPU 2VM setup. If you would like to read from large datasets, maybe try to materialize it on disk and use TFRecordDataest instead."]}, {"number": 42801, "title": "!head {train_file_path}\u62a5\u9519", "body": "'head' \u4e0d\u662f\u5185\u90e8\u6216\u5916\u90e8\u547d\u4ee4\uff0c\u4e5f\u4e0d\u662f\u53ef\u8fd0\u884c\u7684\u7a0b\u5e8f\r\n\u6216\u6279\u5904\u7406\u6587\u4ef6\u3002", "comments": []}, {"number": 39098, "title": "Feature request for supporting  stateful RNN with tf.distribute.Strategy", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n   2.2.0-rc3\r\n- Are you willing to contribute it (Yes/No):\r\nNo\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n Currently RNNs with stateful=True not yet supported with tf.distribute.Strategy.\r\n So, my feature request is to make it supported.\r\n**Will this change the current api? How?**\r\nCan't say.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone from the community is going to benefit from this because training RNN models takes a lot of time to train. Therefore, we trained our model via TPUs with distributed strategy. For example, in Shakespear's texts generation project, we've to use stateful RNNs. So, if this feature is going to be added then we can train our model via TPUs and with distributive strategy.\r\n\r\n\r\n", "comments": ["@abhinavsp0730 can you give us more details about this FR? Is there a code snippet that should work with tf.distribute?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39097, "title": "Inconsistent Result between CPU build and GPU build", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Catalina 10.15.2 / Linux Ubuntu 16\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MacBook Air (Retina, 13-inch, 2019)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  10.1\r\n- GPU model and memory: RTX 2080 ti 11GB x4\r\n\r\n\r\n**Describe the current behavior**\r\nTest runs on laptop, \r\n```\r\nWARNING:tensorflow:From /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nWARNING:tensorflow:From /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\nTrain for 3 steps, validate for 3 steps\r\n\r\n1/3 [=========>....................] - ETA: 4s - loss: 0.6949 - accuracy: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n3/3 [==============================] - 3s 961ms/step - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6943 - val_accuracy: 0.5000\r\nok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 282.183s\r\n\r\nOK\r\n```\r\nbut when deployed on server-GPU/CI, it shows error:\r\n```\r\nvenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\r\n         raise ValueError(str(e))\r\n     ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n```\r\n**Describe the expected behavior**\r\nConsistent result between laptop and server.\r\n**Standalone code to reproduce the issue**\r\n### Setup\r\n./data/sample.csv\r\n```csv\r\nid,comment_text,lang,toxic\r\n0,\"Este usuario ni siquiera llega al rango de    hereje   . Por lo tanto deber\u00eda ser quemado en la barbacoa para purificar su alma y nuestro aparato digestivo mediante su ingesti\u00f3n.    Skipe linkin 22px   Honor, valor, leltad.      17:48 13 mar 2008 (UTC)\",es,0\r\n1,\"Il testo di questa voce pare esser scopiazzato direttamente da qui. Immagino possano esserci problemi di copyright, nel fare cio .\",it,0\r\n2,\"Vale. S\u00f3lo expongo mi pasado. Todo tiempo pasado fue mejor, ni mucho menos, yo no quisiera retroceder 31 a\u00f1os a nivel particular. Las volveria a pasar putas.Fernando \",es,1\r\n3,\"Bu maddenin alt ba\u015fl\u0131\u011f\u0131 olarak  uluslararas\u0131 ili\u015fkiler  ile konuyu s\u00fcrd\u00fcrmek ile ilgili teredd\u00fctlerim var.\u00d6nerim siyaset bilimi ana ba\u015fl\u0131\u011f\u0131ndan sonra siyasal ya\u015fam ve toplum, siyasal g\u00fc\u00e7, siyasal \u00e7at\u0131\u015fma, siyasal gruplar, \u00e7a\u011fda\u015f ideolojiler, din, siyasal de\u011fi\u015fme, kamuoyu, propaganda ve siyasal kat\u0131lma temelinde \u00e7o\u011fulcu siyasal sistemler.Bu alt ba\u015fl\u0131klar\u0131n daha anlaml\u0131 olaca\u011f\u0131 kanaatindeyim.\",tr,0\r\n4,\"Bel\u00e7ika n\u0131n \u015fehirlerinin yan\u0131nda il\u00e7e ve beldelerini yaparken san\u0131r\u0131m Portekizi \u00f6rnek alacaks\u0131n. Ben de uzak gelecekte(2-3 y\u0131l) bu tip \u015feyler d\u00fc\u015f\u00fcn\u00fcyorum. Tabii futbol maddelerinin hakk\u0131ndan geldikten sonra..    daha \u00f6nce mesajlar\u0131n\u0131z\u0131 g\u00f6rm\u00fc\u015ft\u00fcm, hatta anon b\u00f6l\u00fcm\u00fcn\u00fc bizzat kullan\u0131yordum   s\u00f6z\u00fcn\u00fc anlamad\u0131m??  tan\u0131\u015fmak bug\u00fcneymi\u015f gibi bir \u015fey eklemeyi d\u00fc\u015f\u00fcnd\u00fcm ama vazge\u00e7tim. oray\u0131 da silmeyi unuttum. bo\u015fverin K\u0131demli   +    \",tr,0\r\n5,\"g\u00fczel, zaten kaynakland\u0131rmas\u0131 zor subjektif kategoriler bunlar. baz\u0131 maddelerden \u00e7\u0131kartmak, kiminden \u00e7\u0131kart\u0131p kiminde b\u0131rakmak, \u00e7\u0131kart\u0131lanlar\u0131 yerine iade etmek yerine Kategori:Milletlere kar\u015f\u0131 \u0131rk\u00e7\u0131 duygular dakilerin tamam\u0131na el atmak gerekiyor.    kibele    \",tr,0\r\n6,\"No es mala idea. De hecho, yo estaba pensando descolgarme ya del reto mensual, pero esto vuelve a ilusionarme. El problema es que contabilizar los art\u00edculos a mano es muy tedioso, as\u00ed que habr\u00eda que disponer de una herramienta tipo escaladix que lo hiciera autom\u00e1ticamente. Y tampoco estar\u00eda mal disponer de resultados parciales cada tres meses o similar. Yo me apunto ) . Saludos de \",es,0\r\n7,\"Kod hatalar\u0131n\u0131 d\u00fczeltmi\u015fsiniz,elinize sa\u011fl\u0131k \u00e7ok te\u015fekk\u00fcrler.\u00d6nceki s\u00fcr\u00fcmleri ara\u015ft\u0131rd\u0131m.13 Haziran 2010 da Kullan\u0131c\u0131:T\u00fcrk S\u00fcvarisi nin yapt\u0131\u011f\u0131 \u015fu de\u011fi\u015fikli\u011fe kadar 107 kaynak var ve benim g\u00f6rebildi\u011fim kadar\u0131yla kaynaklarda hata yok.T\u00fcrk S\u00fcvarisi nin de\u011fi\u015fikli\u011finden sonra kaynak say\u0131s\u0131 111 e \u00e7\u0131k\u0131yor ve kaynak hatalar\u0131 olu\u015fuyor.Bana T\u00fcrk S\u00fcvarisi hatal\u0131 bir \u015fekilde yeni kaynaklar eklemi\u015f gibi geldi.Siz ne dersiniz?Yard\u0131mc\u0131 olursan\u0131z \u00e7ok sevinirim.Bu arada \u0130ran \u0131 KM aday\u0131 g\u00f6stermeyi d\u00fc\u015f\u00fcn\u00fcyorum ve bu nedenle maddeyi d\u00fczenlemeye \u00e7al\u0131\u015f\u0131yorum.Vikipedi:Madde incelemesi/\u0130ran/ar\u015fiv1 de siz de g\u00f6r\u00fc\u015flerinizi belirtirseniz maddenin geli\u015fimi ad\u0131na katk\u0131s\u0131 olur diye d\u00fc\u015f\u00fcn\u00fcyorum.\u0130ngiltere Portal\u0131 nda sizin ve di\u011fer vikipedistlerin bahsetti\u011fi yorumsal ifadeleri ve yaz\u0131m hatalar\u0131n\u0131 d\u00fczelttim.\u0130sterseniz portala tekrar g\u00f6z at\u0131n.Sevgiler, iyivikiler   \u00c2khilleus       mesaj    \",tr,0\r\n```\r\n### Reproduction\r\n\r\n#### requirements.txt\r\n```txt\r\ntensorflow-hub\r\ntensorflow_text>=2.0.0rc0\r\n```\r\n\r\n#### USE.py\r\n```\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text\r\n\r\nmodule_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\r\n\r\n\r\ndef embed_text(input):\r\n    model = hub.load(module_url)\r\n    return model(input)\r\n\r\n```\r\n\r\n#### Model.py\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef build():\r\n    inputs = tf.keras.Input(shape=(512,), name=\"use_input\")\r\n    hl1 = tf.keras.layers.Dense(\r\n        963, activation=tf.nn.relu, name='hidden_layer_1')(inputs)\r\n    hl2 = tf.keras.layers.Dense(\r\n        369, activation=tf.nn.relu,  name='hidden_layer_2')(hl1)\r\n    hl3 = tf.keras.layers.Dense(\r\n        69, activation=tf.nn.relu,  name='hidden_layer_3')(hl2)\r\n    hl4 = tf.keras.layers.Dense(\r\n        3, activation=tf.nn.relu,  name='hidden_layer_4')(hl3)\r\n    outputs = tf.keras.layers.Dense(\r\n        2, activation=tf.nn.softmax, name='prediction')(hl4)\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n    return model\r\n\r\n```\r\n#### DataProcessing.py\r\n```\r\nimport functools\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport USE\r\n\r\nLABEL_COLUMN = 'toxic'\r\nLABELS = [0, 1]\r\n\r\n\r\ndef get_dataset(file_path, batch_size=3, **kwargs):\r\n    dataset = tf.data.experimental.make_csv_dataset(\r\n        file_path,\r\n        # Artificially small to make examples easier to show.\r\n        batch_size=batch_size,\r\n        label_name=LABEL_COLUMN,\r\n        na_value=\"?\",\r\n        num_epochs=1,\r\n        ignore_errors=True,\r\n        **kwargs)\r\n    return dataset\r\n\r\n\r\ndef show_batch(dataset):\r\n    for batch, label in dataset.take(1):\r\n        for key, value in batch.items():\r\n            print(key)\r\n            print(value)\r\n            # print(\"{:20s}: {}\".format(key, value.numpy()))\r\n        print(label)\r\n\r\n\r\ndef embed_batch(dataset):\r\n    embeddings = []\r\n    for batch, label in dataset.take(1):\r\n        for key, value in batch.items():\r\n            # print(key)\r\n            # print(value)\r\n            for comment in value:\r\n                # print(comment)\r\n                emb = USE.embed_text(comment)\r\n                emb = emb.numpy()\r\n                # print(emb)\r\n                embeddings.append(emb)\r\n\r\n            # print(\"{:20s}: {}\".format(key, value.numpy()))\r\n        # print(label)\r\n        # print(embeddings)\r\n        # print(dataset)\r\n        return embeddings, label\r\n\r\n# show_batch(raw_train_data)\r\n# print(filtered_dataset)\r\n# show_batch(filtered_dataset)\r\n\r\n```\r\n#### Training.py\r\n\r\n```python\r\nimport Model\r\nimport DataProcessing\r\nimport tensorflow as tf\r\n\r\n\r\ndef fit(training_file_path='./data/sample.csv', validation_file_path=\"./data/sample.csv\", batch_size=3,\r\n        SELECT_COLUMNS=['toxic', 'comment_text'], model_saved='test'):\r\n    training_dataset = DataProcessing.get_dataset(\r\n        training_file_path, select_columns=SELECT_COLUMNS, batch_size=batch_size)\r\n    validation_dataset = DataProcessing.get_dataset(\r\n        training_file_path, select_columns=SELECT_COLUMNS, batch_size=batch_size)\r\n    useEmbedding, labels = DataProcessing.embed_batch(training_dataset)\r\n    dataset = tf.data.Dataset.from_tensor_slices((useEmbedding, labels))\r\n    useEmbedding, labels = DataProcessing.embed_batch(validation_dataset)\r\n    validation = tf.data.Dataset.from_tensor_slices((useEmbedding, labels))\r\n    model = Model.build()\r\n    model.summary()\r\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy',\r\n                  metrics=['accuracy'])\r\n    model.save('./model/'+model_saved)\r\n    return model.fit(dataset, validation_data=validation)\r\n\r\nTraining.fit()\r\n```\r\n#### Command\r\n```\r\npip install -r requirements.txt && python Training.py\r\n```\r\n**Other info / logs** \r\n\r\n```\r\n======================================================================\r\n ERROR: test_fit (test_training.TestTraining)\r\n ----------------------------------------------------------------------\r\n Traceback (most recent call last):\r\n   File \"/builds/zzj0402/project/test/test_training.py\", line 8, in test_fit\r\n     self.assertTrue(Training.fit(), \"Training should run.\")\r\n   File \"/builds/zzj0402/project/Training.py\", line 21, in fit\r\n     return model.fit(dataset, validation_data=validation)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n     return method(self, *args, **kwargs)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n     tmp_logs = train_function(iterator)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n     result = self._call(*args, **kwds)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n     self._initialize(args, kwds, add_initializers_to=initializers)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n     *args, **kwds))\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n     graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n     graph_function = self._create_graph_function(args, kwargs)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n     capture_by_value=self._capture_by_value),\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n     func_outputs = python_func(*func_args, **func_kwargs)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n     return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n   File \"/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 968, in wrapper\r\n     raise e.ag_error_metadata.to_exception(e)\r\n ValueError: in user code:\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n         outputs = self.distribute_strategy.run(\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\r\n         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\r\n         return self._call_for_each_replica(fn, args, kwargs)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\r\n         return fn(*args, **kwargs)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\r\n         y, y_pred, sample_weight, regularization_losses=self.losses)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:213 __call__\r\n         batch_dim = array_ops.shape(y_t)[0]\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:984 _slice_helper\r\n         name=name)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1150 strided_slice\r\n         shrink_axis_mask=shrink_axis_mask)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py:10179 strided_slice\r\n         shrink_axis_mask=shrink_axis_mask, name=name)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\r\n         attrs=attr_protos, op_def=op_def)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\r\n         compute_device)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\r\n         op_def=op_def)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__\r\n         control_input_ops, op_def)\r\n     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\r\n         raise ValueError(str(e))\r\n     ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\n```", "comments": ["@zzj0402 \r\nI ran the code shared it seems the code shared is incomplete, please find [the gist](https://colab.sandbox.google.com/gist/Saduf2019/f00c66cf7fd797ba9081670504f9b4c6/39052.ipynb) of errors faced.\r\nPlease let us know if this is a \"hub\" related issue,  if not please provide omplete code such that we could replicate the issue faced, if possible share a colab gist so we could analyse the error faced and help.Thanks!", "> @zzj0402\r\n> I ran the code shared it seems the code shared is incomplete, please find [the gist](https://colab.sandbox.google.com/gist/Saduf2019/f00c66cf7fd797ba9081670504f9b4c6/39052.ipynb) of errors faced.\r\n> Please let us know if this is a \"hub\" related issue, if not please provide omplete code such that we could replicate the issue faced, if possible share a colab gist so we could analyse the error faced and help.Thanks!\r\n\r\nThe code is complete. You didn't install dependencies properly and you are supposed to run locally with separate .py files.", "@zzj0402 Did you try with tensorflow 2.2.0-rc3? Are you facing the same issue?", "> @zzj0402 Did you try with tensorflow 2.2.0-rc3? Are you facing the same issue?\r\n\r\nI haven't tried. I used Kaggle kernels instead. I am going to close this issue since I don't need work on this locally anymore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39097\">No</a>\n"]}, {"number": 39096, "title": "Problem in importing Tensorflow: \"ImportError: DLL load failed while importing\"", "body": "I have a problem in importing TensorFlow. My system is protected in ProgramData. So, I used the virtual environment and installed TensorFlow in my user environment. The TensorFlow is installed, but can not be imported. I checked other websites and related issues, but non of them solved my issue.\r\n\r\nThe versions of the python: Python 3.8.2\r\nThe version of TensorFlow:  1.13.1\r\nThe versions of the pip: pip 20.1\r\nCPU: has avx instruction\r\nEnvironment: virtual environment installed from \"https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/\"\r\nError: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\nFailed to load the native TensorFlow runtime.\r\nSee https://www.tensorflow.org/install/errors\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nThanks.", "comments": ["This error occurs to many Users\r\n\r\nThis occurs because you may not have Visual C++ Redistributable for Windows. Please download it.\r\nAlso tensorflow has no support for Python 3.8 as of now.\r\n\r\nRefer [here](https://www.tensorflow.org/install/pip#windows) to check if you have installed properly\r\n\r\nAlso, refer similar issues.\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!\r\n\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nPlease, refer #36167 #38615 and see if it helps you.\r\n\r\nThanks!", "@samiravafa \r\n\r\nSolution should be downloading and installing visual studio 2015-2019 x86 and x64 from here:https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39096\">No</a>\n"]}, {"number": 39095, "title": "Addition in resources section", "body": "Added Coursera course Machine Learning with TensorFlow on GCP", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39095) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39095) for more info**.\n\n<!-- ok -->", "@kyscg Thank you for pointing out the typo in the resources section, I have now resolved it."]}, {"number": 39093, "title": "Keras Custom Layer could not receive `training` parameter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, custom test script. Provided in the description section.\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNot on mobile\r\n\r\n- TensorFlow installed from (source or binary):\r\nbinary version from pip: tensorflow==1.14.0\r\n\r\n- TensorFlow version (use command below):\r\n`v1.14.0-rc1-22-gaf24dc91b5 1.14.0`\r\n\r\n- Python version:\r\n`Python 3.5.6 :: Anaconda, Inc.`\r\n\r\n- Bazel version (if compiling from source):\r\n```\r\nBuild label: 0.25.2\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri May 10 20:47:48 2019 (1557521268)\r\nBuild timestamp: 1557521268\r\nBuild timestamp as int: 1557521268\r\n```\r\n\r\n- GCC/Compiler version (if compiling from source):\r\nNot compiled from Source - `gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~16.04~ppa1)`\r\n\r\n- CUDA/cuDNN version:\r\nNot using GPU version TF\r\n\r\n- GPU model and memory:\r\nNot using GPU version TF\r\n\r\n**Describe the current behavior**\r\n\r\nTested with an example script:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python import keras\r\n\r\nclass MyLayer(keras.layers.Layer):\r\n\r\n    def call(self, inputs, training=None):\r\n        # Expecting training to be set\r\n        if training is not None:\r\n            self.add_loss(math_ops.reduce_sum(inputs))\r\n\r\n        return inputs\r\n\r\n\r\ninputs = keras.Input((3,))\r\nlayer = MyLayer()\r\noutputs = layer(inputs)\r\nmodel = keras.Model(inputs, outputs)\r\nmodel.compile('sgd', 'mse', run_eagerly=False)\r\nloss = model.fit(np.ones((2, 3)), np.ones((2, 3)))\r\n\r\nprint(loss.history)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe print out loss should not 0, instead it should be \"6\", since during training, `self.add_loss` would run.\r\n\r\nThe behavior is more based on tests in tests in TF source code:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/keras/engine/base_layer_test.py#L861\r\n\r\nAnd online doc: https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method\r\n\r\n**Standalone code to reproduce the issue**\r\nPasted above.\r\n\r\n**Other info / logs**\r\nAbove script output:\r\n\r\n```\r\n$: python3 test_layer.py\r\nWARNING:tensorflow:Entity <bound method MyLayer.call of <__main__.MyLayer object at 0x7faaf7402fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyLayer.call of <__main__.MyLayer object at 0x7faaf7402fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n2020-05-01 17:38:01.840747: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-01 17:38:01.863084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2020-05-01 17:38:01.864264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55efb64ae370 executing computations on platform Host. Devices:\r\n2020-05-01 17:38:01.864288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2020-05-01 17:38:01.879314: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2/2 [==============================] - 0s 33ms/sample - loss: 0.0000e+00\r\n{'loss': [0.0]}\r\n```\r\nLoss is \"0\" as the training branch of code didn't run.\r\nAlso, if enabled `eager_execution` in the code, the print out is more reasonable:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nimport numpy as np\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python import keras\r\n\r\nclass MyLayer(keras.layers.Layer):\r\n\r\n    def call(self, inputs, training=None):\r\n        # Expecting training to be set\r\n        if training is not None:\r\n            self.add_loss(math_ops.reduce_sum(inputs))\r\n\r\n        return inputs\r\n\r\n\r\ninputs = keras.Input((3,))\r\nlayer = MyLayer()\r\noutputs = layer(inputs)\r\nmodel = keras.Model(inputs, outputs)\r\nmodel.compile('sgd', 'mse', run_eagerly=True)\r\nloss = model.fit(np.ones((2, 3)), np.ones((2, 3)))\r\n\r\nprint(loss.history)\r\n\r\n# Print out is \"6\" as training branch works.\r\n```\r\n", "comments": ["@golden0080 \r\nI ran the code shared by you and do not face loss [0.0] as mentioned above, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/740bb42d041a34aefc6e76639fabd9c3/39093.ipynb)\r\nAlso is there any particular reason to use tensorflow version 1.14, you may use latest version/later versions and let us know if that helps.", "@Saduf2019 Thanks for the gist and I tried to re-run it, somehow got a different output from yours.\r\nhttps://gist.github.com/golden0080/0087ecd517d03fd12ed4d74f031d21cd\r\n\r\nIt's somehow different from your notebook outputs, is a bit confused.\r\n\r\nI noticed my outputs included something like \"failure to convert to autograph\":\r\n```\r\nWARNING:tensorflow:Entity <bound method MyLayer.call of <__main__.MyLayer object at 0x7face7eb6be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyLayer.call of <__main__.MyLayer object at 0x7face7eb6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <bound method MyLayer.call of <__main__.MyLayer object at 0x7face7eb6be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyLayer.call of <__main__.MyLayer object at 0x7face7eb6be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n```\r\n\r\nWhat's the reason of such warnings?\r\n\r\nAlso tested using TF 1.15.2, got the same wrong answer in loss:\r\nhttps://gist.github.com/golden0080/f37a7850bdd4679c5314c208033e6512", "@golden0080 When using `TF1.x`, I think this is true that training parameter is not passing through `model.fit`. One workaround is to pass training argument like\r\n\r\n`outputs = layer(inputs, training = True)\r\n`\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b7c298b14c2423bfd1c5d8b8b53b5722/39093.ipynb).\r\n\r\nIn TF2.x, this training parameter is passed through `model.fit`, so you can run your code as it is and it works well and output `{'loss': [6.0]}`. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/13981c093b18794f09a8da44b42e58ab/untitled153.ipynb). Can you please upgrade to `TF2.x`. I guess there won't be any updates to `TF1.15.2` unless there is any security related issues. Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "Thank you for patiently review my issue, really appreciate that!\r\n\r\nI see your point, passing `training` manually does work. However, this kills the purpose of this parameter to change your model behavior automatically between training and inference.\r\nI imagine that if I used some model that were frozen with layers with `training=True`, they would behave like `training=True` even I'm running them just for inference.\r\n\r\nThe reason why I want this to work for `1.x` TF is that the use case I'm having is to have a custom kernel that behaves differently when training and inference. It may need more temporary memory for batch size > 1, and not needed so for inference.\r\n\r\nAnother thing I think is really misleading is that: I ran some of the tests in TF source code, one of them is: https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/keras/engine/base_layer_test.py#L861\r\nI ran this test and it passed. Thought that would work robustly in other contexts.\r\n\r\nI guess my updated question is that: what's the difference between this sample code and the tests in TF source code, that makes tests passed. - I think I know what happened in those tests, they were running in eager mode.", "Another I found after digging the source code a little bit more, is that `tf.python.keras.backend.in_train_phase` can somehow work in the sample code.\r\n\r\nCheckout this notebook:\r\nhttps://gist.github.com/golden0080/5b355615735f2dac160979a52f4c523b\r\n\r\nBut my experience is this doesn't work in my real code, which unfortunately I could not show here.\r\nIs there any other API similar to this one that I could use as temporary workaround?", "Any updates?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39093\">No</a>\n", "Though it is a long time since this was posted, I have a comment on this. \r\nIn my case I used TF v2.5 in compat mode (eager execution and TF2 behaviour off),\r\nbut I had exactly the same problem. In TFv2 mode everything worked. \r\n\r\n@golden0080 mentioned `tf.python.keras.backend.in_train_phase` and it also did not work for me. \r\ninstead, I used `tf.compat.v1.keras.backend.learning_phase()` and seems it works. \r\n\r\n"]}, {"number": 39092, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.   Failed to load the native TensorFlow runtime.", "body": "- OS Platform: Windows 10 64-bit\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.6\r\n- pip version: 20.1\r\n\r\nInstalled the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019 from\r\nhttps://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads\r\nand updated using the file: https://aka.ms/vs/16/release/vc_redist.x64.exe \r\nas mentioned in Step 1 from https://www.tensorflow.org/install/pip#windows_1 \r\n\r\n- Using anaconda: \r\nI installed tensorflow CPU only version from\r\n\r\npip install tensorflow==2.0.0\r\n\r\nTo test the successful installation I used the following command:\r\npython -c \"import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))\"\r\n\r\nOutput:\r\n\r\n(base) C:\\Users\\chandru>python -c \"import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))\"\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\chandru\\anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n#Please guide me through the installation process. I'm not able to use the TensorFlow backend for Keras model which gives me the above same error.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "> From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\r\n> \r\n> * For TF-GPU - See point 1\r\n> * For TF-CPU - See point 2\r\n> \r\n> **1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\r\n> \r\n> _TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0._\r\n> \r\n> * If you have above configuration and using _**Windows**_ platform -\r\n>   \r\n>   * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\n>   * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\n> * If you have above configuration and using _**Ubuntu/Linux**_ platform -\r\n>   \r\n>   * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\r\n>   * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\r\n> * If error still persists then, apparently your CPU model does not support AVX instruction sets.\r\n>   \r\n>   * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n> \r\n> **2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\r\n> \r\n> _TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets._\r\n> \r\n> Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n> Apparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n> \r\n> * Try Google Colab to use TensorFlow.\r\n>   \r\n>   * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use`pip install` to install any other preferred TF version.\r\n>   * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n>   * All you need is a good internet connection and you are all set.\r\n> * Try to build TF from sources by changing CPU optimization flags.\r\n> \r\n> _Please let us know if this helps._\r\n\r\n\r\nHow do I check whether I have those AVX instructions set or not?\r\nIs there any way that I could install or get those AVX instructions set working on my laptop?", "Edit: I just checked to enable or disable AVX instructions. These commands are:\r\n\r\nEnable AVX: bcdedit /set xsavedisable 0\r\n\r\nDisable AVX: bcdedit /set xsavedisable 1\r\n\r\nand I got the output as follows:\r\n\r\nC:\\WINDOWS\\system32>bcdedit /set xsavedisable 0\r\nThe operation completed successfully.\r\n\r\n#I re-ran the version testing command i.e.\r\npython -c \"import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.version); print('hello, {}'.format(tf.matmul(x, x)))\"\r\nI'm still getting the same first error.", "@SwapnilMane22 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\nPlease, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39092\">No</a>\n", "It worked!!!!!!!!!!!!!!!\r\n\r\nStep1:  Guys first Download and install  Anaconda Navigator\r\nStep2: Create new environment by writing following command:\r\n             conda create -n myenv python=3.6,\r\n             conda activate myenv,\r\n             conda install tensorflow,\r\n             and other required libraries according to the project with pip install.\r\nStep3: install spyder on Anaconda navigator and run your program in that."]}, {"number": 39091, "title": "Autograph fails to convert the eager mode code to graph", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:N/A\r\n\r\n\r\n\r\n\r\nI wanted to write a simple layer that would work on the output of tf.experiment.make_csv_dataset and i could use to impute the missing values in numeric dtypes with batch mean, maintain a moving mean to be used at test time, create embeddings for categorical columns and keep the dimensions dependent on the predefined list of unique values. \r\n\r\n\r\nBelow is the code i wrote:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import feature_column\r\nclass NUM_TO_DENSE(layers.Layer):\r\n    def __init__(self,num_cols):\r\n        super().__init__()\r\n        self.keys = num_cols\r\n        self.keys_all = self.keys+[str(i)+'__nullcol' for i in self.keys]\r\n    def build(self,input_shape):\r\n        def create_moving_mean_vars():\r\n            return tf.Variable(initial_value=0.,shape=(),dtype=tf.float32,trainable=False)\r\n        self.moving_means_total = {t:create_moving_mean_vars() for t in self.keys}\r\n        self.layer_global_counter = tf.Variable(initial_value=0.,shape=(),dtype=tf.float32,trainable=False)\r\n\r\n    def call(self,inputs, training = True):\r\n        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}\r\n        current_means = {}\r\n        def compute_update_current_means(t):\r\n            current_mean = tf.math.divide_no_nan(tf.reduce_sum(tf.where(null_cols[t],inputs[t],0.),axis=0),\\\r\n                                  tf.reduce_sum(tf.cast(tf.math.is_finite(inputs[t]),tf.float32),axis=0))\r\n            self.moving_means_total[t].assign_add(current_mean)\r\n            return current_mean\r\n        \r\n        if training:\r\n            current_means = {t:compute_update_current_means(t) for t in self.keys}\r\n            outputs = {t:tf.where(null_cols[t],inputs[t],current_means[t]) for t in self.keys}\r\n            outputs.update({str(k)+'__nullcol':tf.cast(null_cols[k],tf.float32) for k in self.keys})\r\n            self.layer_global_counter.assign_add(1.)\r\n        else:\r\n            outputs = {t:tf.where(null_cols[t],inputs[t],(self.moving_means_total[t]/self.layer_global_counter))\\\r\n                       for t in self.keys}\r\n            outputs.update({str(k)+'__nullcol':tf.cast(null_cols[k],tf.float32) for k in self.keys})\r\n        return outputs\r\n\r\n\r\nclass PREPROCESS_MONSOON(layers.Layer):\r\n    def __init__(self,cat_cols_with_unique_values,num_cols):\r\n        '''cat_cols_with_unqiue_values: (dict) {'col_cat':[unique_values_list]}\r\n        num_cols: (list) [num_cols_name_list]'''\r\n        super().__init__()\r\n        self.cat_cols = cat_cols_with_unique_values\r\n        self.num_cols = num_cols\r\n    def build(self,input_shape):\r\n        self.ntd = NUM_TO_DENSE(self.num_cols)\r\n        self.num_colnames = self.ntd.keys_all\r\n        self.ctd = {k:layers.DenseFeatures\\\r\n                    (feature_column.embedding_column\\\r\n                     (feature_column.categorical_column_with_vocabulary_list\\\r\n                      (k,v),tf.cast(tf.math.ceil(tf.math.log(tf.cast(len(self.cat_cols[k]),tf.float32))),tf.int32).numpy()))\\\r\n                   for k,v in self.cat_cols.items()}\r\n        self.cat_colnames = [i for i in self.cat_cols]\r\n        self.dense_colnames = self.num_colnames+self.cat_colnames\r\n    def call(self,inputs,training=True):\r\n        dense_num_d = self.ntd(inputs,training=training)\r\n        dense_cat_d = {k:self.ctd[k](inputs) for k in self.cat_colnames}\r\n        \r\n        dense_num = tf.stack([dense_num_d[k] for k in self.num_colnames],axis=1)\r\n        dense_cat = tf.concat([dense_cat_d[k] for k in self.cat_colnames],axis=1)\r\n        dense_all = tf.concat([dense_num,dense_cat],axis=1)\r\n        return dense_all\r\n```\r\ncreating data to test this\r\n```\r\n    mnist = tf.keras.datasets.mnist\r\n\r\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n    x_train_ = pd.DataFrame(x_train.reshape(60000,-1),columns = ['col_'+str(i) for i in range(28*28)])\r\n    x_test_ = pd.DataFrame(x_test.reshape(10000,-1),columns = ['col_'+str(i) for i in range(28*28)])\r\n    x_train_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]\r\n    x_test_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i','j']) for i in range(x_test_.shape[0])]\r\n    x_train_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]\r\n    x_test_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i','j']) for i in range(x_test_.shape[0])]\r\n    x_train_[np.random.choice([True,False],size = x_train_.shape,p=[0.05,0.95]).reshape(x_train_.shape)] = np.nan\r\n    x_test_[np.random.choice([True,False],size = x_test_.shape,p=[0.05,0.95]).reshape(x_test_.shape)] = np.nan\r\n    x_train_.to_csv('data/x_train.csv',index=False)\r\n    x_test_.to_csv('data/x_test.csv',index=False)\r\n```\r\ngetting one batch of created in ram\r\n```\r\ncdtypes = pd.read_csv('data/x_train.csv',nrows=2).dtypes\r\nxtb = tf.data.experimental.make_csv_dataset('data/x_train.csv',32,header=True,prefetch_buffer_size=1,\r\n                                           column_defaults=[np.nan if i == (float or int) else '__missing__' for i in cdtypes])\r\nfor i in xtb:\r\n    break\r\ndd = pd.read_csv('data/x_train.csv',nrows=2).head()\r\nnum_cols = [i for i in dd.columns if i not in ['col_cat1','col_cat2']]\r\ncat_cols = [i for i in dd.columns if i in ['col_cat1','col_cat2']]\r\n\r\ncol_cat1_unique = ['a','b','c','d','e','f','g','h','i']\r\ncol_cat2_unique = ['a','b','c','d','e','f','g','h','i']\r\n\r\ncol_cat_unique = [col_cat1_unique,col_cat2_unique]\r\n\r\ncatcoldict = {k:v for k,v in zip(cat_cols,col_cat_unique)}\r\n```\r\ntesting it:\r\nthis works: \r\n```\r\npm = PREPROCESS_MONSOON(catcoldict,num_cols)\r\npm(i)\r\n```\r\nthis works with a bug report\r\n```\r\npm = PREPROCESS_MONSOON(catcoldict,num_cols)\r\n@tf.function\r\ndef p(i):\r\n    return pm(i)\r\n\r\np(i)\r\noutput: (along with the expected preprocessed batch)\r\nWARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\nWARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\nWARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\nWARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\n```\r\n\r\nthis fails\r\n```\r\npm = PREPROCESS_MONSOON(catcoldict,num_cols)\r\n\r\ninputs = tf.keras.Input(shape=(None,786))\r\nx = pm(inputs)\r\noutput:\r\nWARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458aa3a90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\nWARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458aa3a90>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: unexpected indent (<unknown>, line 10)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-78-64c553138beb> in <module>\r\n      2 \r\n      3 inputs = tf.keras.Input(shape=(None,786))\r\n----> 4 x = pm(inputs)\r\n      5 # x = tf.keras.layers.Dense(500,tf.keras.layers.ReLU(100.,0.01,0.))\r\n      6 # output = tf.keras.layers.Dense(10,tf.keras.layers.Softmax())\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    771                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    772                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    775                     # circular dependencies.\r\n\r\n~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    235       except Exception as e:  # pylint:disable=broad-except\r\n    236         if hasattr(e, 'ag_error_metadata'):\r\n--> 237           raise e.ag_error_metadata.to_exception(e)\r\n    238         else:\r\n    239           raise\r\n\r\nTypeError: in converted code:\r\n\r\n    <ipython-input-66-936477fe8a70>:62 call  *\r\n        dense_num_d = self.ntd(inputs,training=training)\r\n    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:773 __call__\r\n        outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    <ipython-input-66-936477fe8a70>:20 call\r\n        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}\r\n    <ipython-input-66-936477fe8a70>:20 <dictcomp>\r\n        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}\r\n    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:862 _slice_helper\r\n        _check_index(s)\r\n    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:752 _check_index\r\n        raise TypeError(_SLICE_TYPE_ERROR + \", got {!r}\".format(idx))\r\n\r\n    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'col_0'\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/1b0085d48613ad26aa452403b0557343/39091-2-1.ipynb), [TF v2.2.0-rc4](https://colab.research.google.com/gist/amahendrakar/28ff408939c0030a0b8534321bf6e7f1/39091-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/31a44fa7a22e325195421c0fa02110f3/39091-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@nitinmnsn,\r\nAs per [this comment](https://github.com/tensorflow/tensorflow/issues/26302#issue-416580025), empty arrays cause this error. Could you please take a look at this and let us know if it helps. Thanks!"]}, {"number": 39090, "title": "Problem with tfjs.converters.save_keras_model", "body": "Model: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 224, 224, 32)      896       \r\n_________________________________________________________________\r\nmax_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 56, 56, 64)        36928     \r\n_________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 50176)             0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 512)               25690624  \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 7)                 3591      \r\n=================================================================\r\nTotal params: 25,750,535\r\nTrainable params: 25,750,535\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\nThis is a summary of the model that I made and trained, it's fairly simple, but when I use tensorflowjs to save it to a folder to be used as a javascript model in a browser, when I try to load the model, the console always returns \"Provided weight data has no target variable: conv2d_3/kernel\".  As can be seen, there is no conv2d_3 layer in my model, and I have no idea why the converter makes this 4th conv2d layer.  I tried passing false into the strict argument when using tf.loadLayersModel function, but it's like that pass is ignored.\r\nHere's how I wrote the function call:  const net = await tf.loadLayersModel(\"path to json file\", strict=false).  Am I doing it wrong?  I really need this figured out as this is a part of my senior project for my university.  Any help is appreciated, thank you.", "comments": ["@MrXhojn \r\nThis issue is not related to Tensorflow can you post this issue in relevant repo TFjs. Thanks", "Oh, okay"]}, {"number": 39089, "title": "Ensure that the RandomFourierFeatures layer can be trained in eager mode", "body": "fix #39088 \r\n\r\nWhen executing in eager mode, computing the `kernel` value is done at build time, so it is not possible to calculate the gradient of the output with respect to the trainable parameters.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39089) for more info**.\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39089) for more info**.\n\n<!-- ok -->", "@johnamcleod  Can you please resolve conflicts? Thanks!", "@gbaned - my apologies for the delay.\r\n\r\nI have resolved the conflicts. It appears that the issue I was attempting to fix has been addressed since I opened this PR. As such this PR now only contains a new unit test to check that the RFF layer can be trained. I believe that this is still a valuable test to include in the test suite for this layer.", "@johnamcleod Can you please check @qlzh727's comments and keep us posted ? Thanks!"]}, {"number": 39088, "title": "It is not possible to train the trainable parameters of the RandomFourierFeatures keras layer in eager mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes; minimal working example provided\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-5.3.0-46-generic-x86_64-with-Ubuntu-18.04-bionic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0.dev20200501\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nIt is not possible to train the \"trainable\" parameters of the RandomFourierFeatures keras layer, when using eager execution.\r\n\r\n**Describe the expected behavior**\r\nIt should be possible to train the \"trainable\" parameters of the RandomFourierFeatures keras layer, even when using eager execution.\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom tensorflow_core.python.keras.layers import RandomFourierFeatures\r\n\r\nfourier_features = RandomFourierFeatures(\r\n    1,\r\n    kernel_initializer='gaussian',\r\n    scale=1.0,\r\n    trainable=True,\r\n    dtype=tf.float64\r\n)\r\n\r\ninput = tf.keras.Input(shape=(1,), dtype=tf.float64, name='input')\r\noutput = fourier_features(input)\r\nmodel = tf.keras.Model(inputs=input, outputs=output)\r\nmodel.compile(loss='mean_squared_error')\r\n\r\nmodel.fit(tf.constant([[1.0]]), tf.constant([[1.0]]), epochs=1)\r\n\r\n\r\n**Other info / logs**\r\nThe call to fit throws the following error:\r\nValueError: No gradients provided for any variable: ['random_fourier_features/random_features_scale:0'].\r\n1/1 [==============================] - 0s 17ms/sample\r\n", "comments": ["I have tried in colab with TF 2.1.0, 2.2-rc4 and i am able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9879324436f65e80eda2cc68cdd1eb15/untitled848.ipynb).Thanks!", "Same here. I only use the `fit()` function, without customization..\r\n\r\nIn previous versions of TF, my projects ran smoothly, but now I'm getting this \"No gradients provided for any variable\" message.", "This appears to have been fixed in the latest nightly build (`2.3.0.dev20200531`). ", "@johnamcleod \r\n\r\nI have tried in latest nightly build (`2.3.0-dev20200531`) and i am not seeing any issue. I have changed small update in code by importing  RandomFourierFeatures by (`from tensorflow.python.keras.layers.kernelized import RandomFourierFeatures`).Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/bdf2110e533ab08ae5660620350cdad9/untitled942.ipynb).Please, verify once and close the issue.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39088\">No</a>\n"]}, {"number": 39087, "title": "Fix dynamic display for PyCharm (r1.15)", "body": "### Summary\r\nFixes `self._dynamic_display` not being set to true for printing out the verbose training updates. Previously in PyCharm, it would print a new line every update to the progress bar and with this change it works as expected, clearing each previous update to the line and removing the annoying bug of many, many fast printing lines to the console.\r\n\r\n*This is a cherry-pick of https://github.com/tensorflow/tensorflow/pull/34911, relating to issue #38883*", "comments": []}, {"number": 39086, "title": "Fix dynamic display for PyCharm (r2.0)", "body": "### Summary\r\nFixes `self._dynamic_display` not being set to true for printing out the verbose training updates. Previously in PyCharm, it would print a new line every update to the progress bar and with this change it works as expected, clearing each previous update to the line and removing the annoying bug of many, many fast printing lines to the console.\r\n\r\n*This is a cherry-pick of https://github.com/tensorflow/tensorflow/pull/34911, relating to issue #38883*", "comments": []}, {"number": 39085, "title": "Fix dynamic display for PyCharm (r2.1)", "body": "### Summary\r\nFixes `self._dynamic_display` not being set to true for printing out the verbose training updates. Previously in PyCharm, it would print a new line every update to the progress bar and with this change it works as expected, clearing each previous update to the line and removing the annoying bug of many, many fast printing lines to the console.\r\n\r\n*This is a cherry-pick of https://github.com/tensorflow/tensorflow/pull/34911, relating to issue #38883*", "comments": []}, {"number": 39084, "title": "Improve training runtime reporting during .fit() for different modes.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n2.0\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nyes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe request is to improve the real-time timing reporting across the .fit() train variations.  The format, units, and behavior differ currently\r\n\r\n## Baseline\r\n\"standard reporting\" looks like this:\r\n```\r\nEpoch 9/100\r\n3200/3200 [==============================] - 61s 19ms/sample - loss: 1.4259e-07 - accuracy: 1.0000\r\n```\r\nNote the implicit \"per epoch\" in 61s.  Also that number sometimes is \"ETA\" and then switches to \"per epoch\" once that epoch is over.  Note also that unit time is per sample.  And note that time is in milliseconds.\r\n\r\n## Two worker tf.distribute\r\n### Worker 1:\r\n```\r\nEpoch 1/100\r\n3200/3200 [==============================] - 209s 65ms/sample - loss: 0.1447 - accuracy: 0.9873\r\nEpoch 2/100\r\n 480/3200 [===>..........................] - ETA: 2:44 - loss: 8.5048e-06 - accuracy: 1.0000\r\n```\r\n### Worker 2:\r\n```\r\nEpoch 1/100\r\n  32/3200 [..............................] - ETA: 15:14 - loss: 0.7334 - accuracy: 0.66  64/3200 [..............................] - ETA: 9:05 - loss: 5.8474 - accuracy: 0.718  96/3200 [..............................] - ETA: 7:01 - loss: 3.9763 - accuracy: 0.809 128/3200 [>.............................] - ETA: 5:59 - loss: 3.1775 - accuracy: 0.802 160/3200 [>.............................] - ETA: 5:22 - loss: 2.5869 - accuracy: 0.829\r\n```\r\n#### My belief:  Worker 2 is not honoring the 'verbose' setting of 1 which I set in the chief worker.\r\nThe request is one way or another make the reporting the same for each worker of the same type.\r\n\r\n## dataset versus numpy\r\nThe output of .fit() depends on the training data\r\nCase A: Input and output are numpy arrays\r\nCase B: Input is a dataset.\r\nThe following is for dataset:\r\n```\r\nEpoch 1/100\r\n     17/Unknown - 41s 2s/step - loss: 0.6328 - accuracy: 0.9323\r\n```\r\n, the previous examples were for numpy inputs.\r\nNOTE 1:  The units are now per step instead of per sample\r\nNOTE 2: How many examples are in a \"step\" for a 2 worker job?  Is it batch_size or batch_size/# of workers?  What should it be?\r\nNOTE 3:  There is only one significant digit \"2\".  There should be a fixed number of significant digits or maybe an absolute precision (i.e. always show to tenth of millisecond) or a combination of the two.\r\n\r\nAnything to improve the reporting is appreciated.  I suggest:\r\n\r\n1) Be sure that all workers comply with the \"verbose\" setting properly.  This is speculation as to the problem on my part.\r\n2) Either report /step or /example, or both.  But be consistent.\r\n3) Don't mix \"ETA\" with \"/epoch\".  Maybe show both always.  If ETA is unknown, then ok to report unknown.\r\n4) Figure out a good rule for showing data.  One rule might be: Always show to 1/10th of a unit regardless of unit (so if a time is written in seconds, show number rounded to nearest 0.1 second, if time is in ms, then show in 0.1 ms.  Then consider significant digits.\r\n5) Consider how time should be reported when using tf.distribute (eg reporting when running 1 worker versus reporting when running two workers.  I don't have suggestions for this one.\r\n\r\n\r\n**Will this change the current api? How?**\r\nno\r\n\r\n**Who will benefit with this feature?**\r\nUsers who wish to compare runtime between various training modes\r\n**Any Other info.**\r\n", "comments": ["@robertlugg Sorry for the late response. Are you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39083, "title": "CNN made using tf.keras yields different and worse accuracy if compared to the same CNN built using keras", "body": "**System information**\r\n- Have I written custom code: I'm using the CNN Mnist example from the [keras documentation](https://keras.io/examples/mnist_cnn/) \r\n- OS Platform and Distribution: Linux (Google Colab)\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version: 2.2.0-rc3\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: Colab gpu default\r\n- GPU model and memory: Colab gpu default\r\n\r\n**Describe the current behavior**\r\nIf I train a simple CNN on the mnist dataset following the [Keras Mnist CNN example](https://keras.io/examples/mnist_cnn/) I get different accuracy depending on whether I use `tf.keras` or `keras`.\r\n\r\n**Describe the expected behavior**\r\nI think that the accuracy should be the same.\r\n\r\n**Standalone code to reproduce the issue**\r\nYou can find the code reproducing this possible bug in a colab notebook here: https://colab.research.google.com/drive/1bLOyOt7tJqh2m-4rNdb_t-FZC4xX7DJI\r\n\r\nThis is the first issue I open in this repository, so I hope that the information I have provided is clear enough. \r\nThank you for your work with Tensorflow!\r\n", "comments": ["Apparently it was a matter of learning rate: keras and tf.keras have different default learning rates for Adadelta (keras uses lr=1, tf 0.001), thus it is not a bug but only a slight difference in the api.\r\nSorry for the inconvenience... ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39083\">No</a>\n"]}]