[{"number": 38577, "title": "Create stale.yml", "body": "", "comments": []}, {"number": 38576, "title": "[TFLITE] Modelmaker for (multi-class) custom text classification.", "body": "**System information**\r\n- TensorFlow version (you are using):\r\nNightly version of TensorFlow (2.2.0-dev20200415)\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAbility to use custom train data for text classification.\r\nCurrently on providing custom data instead of **aclImdb** [http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz](url)\r\n\r\n`text_classifier.create(train_data, model_spec=model_spec, epochs=5)` functionality breaks with the following error :\r\n`UnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-48-391f7527d048> in <module>\r\n----> 1 model = text_classifier.create(train_data, model_spec=model_spec, epochs=20)\r\n\r\n~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow_examples/lite/model_maker/core/task/text_classifier.py in create(train_data, model_export_format, model_spec, shuffle, batch_size, epochs, validation_data)\r\n     60 \r\n     61   tf.compat.v1.logging.info('Retraining the models...')\r\n---> 62   text_classifier.train(train_data, validation_data, epochs, batch_size)\r\n     63 \r\n     64   return text_classifier\r\n\r\n~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow_examples/lite/model_maker/core/task/text_classifier.py in train(self, train_data, validation_data, epochs, batch_size)\r\n    126                                                 steps_per_epoch,\r\n    127                                                 validation_steps,\r\n--> 128                                                 self.num_classes)\r\n    129 \r\n    130     return self.model\r\n\r\n~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec.py in run_classifier(self, train_input_fn, validation_input_fn, epochs, steps_per_epoch, validation_steps, num_classes)\r\n    222         steps_per_epoch=steps_per_epoch,\r\n    223         validation_data=validation_ds,\r\n--> 224         validation_steps=validation_steps)\r\n    225 \r\n    226     return model\r\n\r\n~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     69   def _method_wrapper(self, *args, **kwargs):\r\n     70     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 71       return method(self, *args, **kwargs)\r\n     72 \r\n     73     # Running inside `run_distribute_coordinator` already.\r\n\r\n~/anaconda3/envs/py3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    917               end_step = step + data_handler.step_increment\r\n    918               callbacks.on_train_batch_end(end_step, logs)\r\n--> 919         epoch_logs = copy.copy(logs)\r\n    920 \r\n    921         # Run validation.\r\n\r\nUnboundLocalError: local variable 'logs' referenced before assignment\r\n`\r\n**Will this change the current api? How?**\r\nNot sure.\r\n**Who will benefit with this feature?**\r\nEnd users\r\n**Any Other info.**\r\nCustom (multi-class) text classification support.", "comments": ["@dupree \r\ncan you please share a simple standalone code for us to replicate the error faced.", "@Saduf2019  Thanks for the response. I am following the example shared here :\r\n[tf_example_url](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/lite/model_maker/demo/text_classification.ipynb)\r\n\r\nThe only change I am doing is in the block of data parsing, as following :\r\n```\r\nprint (data_path)\r\n\r\nmodel_spec = AverageWordVecModelSpec()\r\ntrain_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), \r\n                                                  model_spec=model_spec, \r\n                                                  class_labels=['accept', 'reject', 'no'])\r\ntest_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'),\r\n                                                 model_spec=model_spec, is_training=False, shuffle=False)\r\ntrain_data, validation_data = train_data.split(0.9)\r\n```\r\n\r\nI am following the precise folder structure as followed by aclIMDB dataset. \r\n`class_labels=['accept', 'reject', 'no'])` point to folders named accept, reject and no respectively in train folder. The dat_path looks like this :\r\n`data/`\r\nwhile the path for accept and other labels looks like :\r\n`data/train/accept`\r\n\r\nI see the error when I create model via either this :\r\n`model = text_classifier.create(train_data, model_spec=model_spec, epochs=8)`\r\n\r\nor\r\n`model = text_classifier.create(train_data, model_spec=model_spec, validatation_data=validation_data,  epochs=8)`\r\n\r\n**Note :** In this setup each class data in `data/train/accept` (and reject, no)  contains around 4-5 `id.txt` files, where id ranges from 0..5 \r\n\r\n\r\nAfter some trials and testing, I noticed this :\r\n\r\n1. once I rename the individual train files from `id.txt` to `id_rating.txt`, where id = range(0..30) and rating = range(0..10). I am able to create/train a model via this :\r\n `model = text_classifier.create(train_data, model_spec=model_spec, epochs=8)`\r\n\r\nBut this fails again once I enable `validation_data` via this  :\r\n\r\n`model = text_classifier.create(train_data, model_spec=model_spec, validatation_data=validation_data,  epochs=8)`\r\n\r\n2. I am able to train/create a model this way but the model is highly susceptible to class imbalance. If one label has marginally higher data, model just learns to predict that for any sample. I tested with data samples for label `yes` 34 and `no` 40 and the model learns to predict no for every sample.\r\n\r\nHow and where can I see the internal working of `model_spec = AverageWordVecModelSpec()` and if I can customize it.\r\n\r\nThanks   ", "@dupree\r\nif possible can you please share colab gist of the code for us to analyse the error", "@dupree \r\nPLease update as per above comment", "@Saduf2019 Unfortunately I am not using colab but my local jupyter notebook, I can share that if thats of any help ? But as I mentioned in my previous comment, I haven't changed much except adding my own data files. If the example expects data/data files in any specific `filename` format, please share.\r\nThanks ", "@dupree Looking at the error trace, i suspect some compatibility issue due to combining `TF1.x` and `TF2.x` functionality.\r\n```\r\n61 tf.compat.v1.logging.info('Retraining the models...')\r\n---> 62 text_classifier.train(train_data, validation_data, epochs, batch_size)\r\n....\r\n.....\r\nUnboundLocalError: local variable 'logs' referenced before assignment \r\n```\r\n\r\nCan you please share a standalone code to reproduce the issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38575, "title": "[Intel MKL] Enabling BFloat16 convolution fusions", "body": "This PR enables:\r\n1) All FP32 fusions currently supported for Conv2D and MatMul\r\n2) Conv2D + Add + Activation fusion", "comments": []}, {"number": 38574, "title": "Custom keras/tensorflow layer, output-shape error", "body": "I wrote my own keras/tensorflow layer. Passing images into it works fine, but using it in combination with other layers, givesan error. Somehow the output shape of my custom layer should be wrong, or some kind of \"Nonetype\".\r\n\r\nI posted my question on Stackoverflow, to reduce redundance,  just follow the underlying link to the code.\r\n\r\nhttps://stackoverflow.com/questions/61231081/custom-keras-tensorflow-layer-output-shape-error", "comments": ["@paulhigazi \r\n\r\nCan you please let us know which Tensorflow version you are using?.Request you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code with proper indentation to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!"]}, {"number": 38573, "title": "Corrected examples of tf.diag() and tf.diag_part()", "body": "Fixes: #38564", "comments": ["@ManishAradwad Can you please check build failures. Thanks!", "@gbaned @alextp Its giving syntax error for lines like this:\r\n```>>> diagonal = [1, 2, 3, 4]```\r\nMaybe because its not a .py file. What should I do? ", "Yes, make it python code.\n\nOn Thu, Apr 23, 2020 at 12:14 AM Manish Aradwad <notifications@github.com>\nwrote:\n\n> *@ManishAradwad* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/api_def/base_api/api_def_Diag.pbtxt\n> <https://github.com/tensorflow/tensorflow/pull/38573#discussion_r413567997>\n> :\n>\n> > @@ -18,12 +18,14 @@ rank 2k with dimensions [D1,..., Dk, D1,..., Dk] where:\n>\n>  For example:\n>\n> -```\n> -# 'diagonal' is [1, 2, 3, 4]\n> -tf.diag(diagonal) ==> [[1, 0, 0, 0]\n> -                       [0, 2, 0, 0]\n> -                       [0, 0, 3, 0]\n> -                       [0, 0, 0, 4]]\n> -```\n> +\"\"\"\n>\n> Do u mean I should make like this?\n> \"\"\" >>> diagonal = [1, 2, 3, 4]\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38573#discussion_r413567997>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKDGHDVZVBZRQTOK6LRN7TEDANCNFSM4MITEV5Q>\n> .\n>\n\n\n-- \n - Alex\n", "Please follow 8582a58 and don't include python code in the base API.", "@mihaimaruseac where should I add the `tensor_diag` and `tensor_diag_part` functions? Also I should return the calls to [diag](https://www.tensorflow.org/api_docs/python/tf/linalg/diag) and [diag_part](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part) functions in those functions, right?", "@mihaimaruseac Can you please assist on above comments from @ManishAradwad. Thanks!", "I see that examples already exist in Python side: https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L2213-L2378", "@ManishAradwad Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Hello,\nSorry for the late response. I won't be able to work on this issue at the moment. Pls assign it to someone else\nThanks", "closing since examples already exist"]}, {"number": 38571, "title": "Failed precondition: Error while reading resource variable block3a_se_expand/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block3a_se_expand/bias/N10tensorflow3VarE does not exist. \t [[{{node block3a_se_expand/BiasAdd/ReadVariableOp}}]] \t [[dense_1/Softmax/_1903]]", "body": "I have three trained models which is using in one script . First model (.h5) is loaded using following piece of code\r\n\r\n```\r\nfrom keras_retinanet import models\r\ndef get_session():\r\n    config = tf.compat.v1.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    return tf.compat.v1.Session(config=config)\r\nkeras.backend.tensorflow_backend.set_session(get_session()) \r\nfirst_model = models.load_model(first_model_path, backbone_name='resnet50')\r\n```\r\nThe first model loaded and predicted successfully. After first model operation,  second model (.hdf5) loaded using following code block\r\n\r\n```\r\nfrom tensorflow.keras.models import load_model\r\nsecond_model =load_model(second_model_path)\r\n```\r\nSecond model loaded successfully and but while prediction , getting the following error.\r\n\r\n`2020-04-15 12:01:36.207346: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2020-04-15 12:01:36.310985: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2020-04-15 12:01:43.026476: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10\r\nTraceback (most recent call last):\r\n  File \"ppe_detection.py\", line 105, in <module>\r\n    predicted , x = predict_helmet(processed_roi,helmet_model)\r\n  File \"/samjith/project/safety-monitoring/classify/helmet/helmet_classy.py\", line 17, in predict_helmet\r\n    helmet_score = helmet_model.predict([x])[0][0]\r\n  File \"/home/samjith/anaconda3/envs/keras-retina/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1078, in predict\r\n    callbacks=callbacks)\r\n  File \"/home/samjith/anaconda3/envs/keras-retina/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 363, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"/home/samjith/anaconda3/envs/keras-retina/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\", line 3292, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"/home/samjith/anaconda3/envs/keras-retina/lib/python3.7/site-packages/tensorflow/python/client/session.py\", line 1458, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.\r\n  (0) Failed precondition: Error while reading resource variable block3b_expand_conv/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block3b_expand_conv/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node block3b_expand_conv/Conv2D/ReadVariableOp}}]]\r\n\t [[dense_1/Softmax/_1903]]\r\n  (1) Failed precondition: Error while reading resource variable block3b_expand_conv/kernel from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/block3b_expand_conv/kernel/N10tensorflow3VarE does not exist.\r\n\t [[{{node block3b_expand_conv/Conv2D/ReadVariableOp}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n`\r\nI have gone through [this github  link](https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-495005162). But the error still remains.\r\n", "comments": ["@Samjith888, Can you share complete standalone code and Tensorflow version. Thanks", "> @Samjith888, Can you share complete standalone code and Tensorflow version. Thanks\r\n\r\nHere i have three .py files.\r\n\r\n```\r\n*****main.py****\r\n\r\nimport efficientnet.tfkeras\r\nfrom tensorflow.keras.models import load_model\r\nimport efficientnet.keras as efn\r\nimport cv2\r\nfrom first_predict import first_prediction\r\nfrom second_predict import second_prediction\r\n\r\nimage = cv2.imread('abc.jpg')\r\n\r\n#download pretrained weights \r\nsecond_mld = efn.EfficientNetB4(weights='imagenet', include_top=False)\r\n\r\nfirst_model_path = 'first_model.h5'\r\nsecond_model_path = path to 'second_mdl'\r\n\r\nfirst_prediction(image,first_model_path)\r\n\r\n#second model loading\r\nsecond_model = load_model(second_model_path)\r\nsecond_prediction(image,second_model)\r\n\r\n#second model loading\r\nsecond_model = load_model(second_model_path)\r\nsecond_prediction(output_img,second_model)\r\n\r\n```\r\n```\r\n****first_predict.py *****\r\n\r\nimport keras\r\nfrom keras_retinanet import models\r\nfrom keras_retinanet.utils.image import  preprocess_image, resize_image\r\nimport tensorflow as tf\r\n\r\ndef get_session():\r\n    config = tf.compat.v1.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    return tf.compat.v1.Session(config=config)\r\n\r\ndef first_prediction(img, first_model_path):\r\n        #Load model for detection     \r\n        keras.backend.tensorflow_backend.set_session(get_session()) \r\n        model = models.load_model(first_model_path, backbone_name='resnet50')\r\n\r\n        #preprocess the image for inference    \r\n        image = preprocess_image(img)\r\n        image, scale = resize_image(image)\r\n\r\n        boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\r\n\r\n```\r\n\r\n```\r\n***second_predict.py*****\r\n                                                   \r\nimport efficientnet.tfkeras\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.preprocessing import image\r\n\r\ndef second_prediction(img,second_model):\r\n    ##some image  preprocessing code\r\n    x = image.img_to_array(img)\r\n    # Reshape\r\n    x = x.reshape((1,) + x.shape)\r\n    x /= 255.\r\n  \r\n    probabilities =second_model.predict([x])\r\n\r\n```\r\ndownload pretrained model and place the path in following fields \r\n```\r\nfirst_model_path = 'first_model.h5'\r\nsecond_model_path = 'second_model.hdf5'\r\n```\r\n\r\nI'm getting the error in `'probabilities =second_model.predict([x])'` in second_predict.py file\r\n\r\n**Environment**\r\nPython = 3.7\r\nTensorflow = 1.14.0\r\nkeras =2.3.1\r\n\r\n\r\nNote : This error is not on the trained models, i have already tested successfully by executing these models individually. ", "@gadagashwini  Any update on this issue ?", "@Samjith888,\r\nSorry for the delayed response. On running the code, I am facing an error stating `ImportError: cannot import name 'first_prediction'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d3a8b4bafc0fdeee3238eab50c100e65/38571.ipynb).\r\n\r\nLooks like the code for `first_predict.py` file is incomplete. Could you please share the complete code to reproduce the issue reported here. Thanks!", "> @Samjith888,\r\n> Sorry for the delayed response. On running the code, I am facing an error stating `ImportError: cannot import name 'first_prediction'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d3a8b4bafc0fdeee3238eab50c100e65/38571.ipynb).\r\n> \r\n> Looks like the code for `first_predict.py` file is incomplete. Could you please share the complete code to reproduce the issue reported here. Thanks!\r\n\r\nPlease have try with the updated code. I have corrected the error which u mentioned above", "@Samjith888,\r\nI am unable to reproduce the error because of the missing files `first_model.h5` and `second_model.hdf5`. \r\n\r\nCould you please share all the supporting files required to run the code. Thanks!", "> @Samjith888,\r\n> I am unable to reproduce the error because of the missing files `first_model.h5` and `second_model.hdf5`.\r\n> \r\n> Could you please share all the supporting files required to run the code. Thanks!\r\n\r\n\r\n\r\n> > @Samjith888, Can you share complete standalone code and Tensorflow version. Thanks\r\n> \r\n> Here i have three .py files.\r\n> \r\n> ```\r\n> *****main.py****\r\n> \r\n> import efficientnet.tfkeras\r\n> from tensorflow.keras.models import load_model\r\n> import cv2\r\n> from first_predict import first_prediction\r\n> from second_predict import second_prediction\r\n> \r\n> image = cv2.imread('abc.jpg')\r\n> \r\n> first_model_path = 'first_model.h5'\r\n> second_model_path = 'second_model.hdf5'\r\n> \r\n> first_prediction(image,first_model_path)\r\n> \r\n> #second model loading\r\n> second_model = load_model(second_model_path)\r\n> second_prediction(image,second_model)\r\n> \r\n> #second model loading\r\n> second_model = load_model(second_model_path)\r\n> second_prediction(output_img,second_model)\r\n> ```\r\n> \r\n> ```\r\n> ****first_predict.py *****\r\n> \r\n> import keras\r\n> from keras_retinanet import models\r\n> from keras_retinanet.utils.image import  preprocess_image, resize_image\r\n> import tensorflow as tf\r\n> \r\n> def get_session():\r\n>     config = tf.compat.v1.ConfigProto()\r\n>     config.gpu_options.allow_growth = True\r\n>     return tf.compat.v1.Session(config=config)\r\n> \r\n> def first_prediction(img, first_model_path):\r\n>         #Load model for detection     \r\n>         keras.backend.tensorflow_backend.set_session(get_session()) \r\n>         model = models.load_model(first_model_path, backbone_name='resnet50')\r\n> \r\n>         #preprocess the image for inference    \r\n>         image = preprocess_image(img)\r\n>         image, scale = resize_image(image)\r\n> \r\n>         boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\r\n> ```\r\n> \r\n> ```\r\n> ***second_predict.py*****\r\n>                                                    \r\n> import efficientnet.tfkeras\r\n> from tensorflow.keras.models import load_model\r\n> from tensorflow.keras.preprocessing import image\r\n> \r\n> def second_prediction(img,second_model):\r\n>     ##some image  preprocessing code\r\n>     x = image.img_to_array(img)\r\n>     # Reshape\r\n>     x = x.reshape((1,) + x.shape)\r\n>     x /= 255.\r\n>   \r\n>     probabilities =second_model.predict([x])\r\n> ```\r\n> \r\n> download pretrained model and place the path in following fields\r\n> \r\n> ```\r\n> first_model_path = 'first_model.h5'\r\n> second_model_path = 'second_model.hdf5'\r\n> ```\r\n> \r\n> I'm getting the error in `'probabilities =second_model.predict([x])'` in second_predict.py file\r\n> \r\n> **Environment**\r\n> Python = 3.7\r\n> Tensorflow = 1.14.0\r\n> keras =2.3.1\r\n> \r\n> Note : This error is not on the trained models, i have already tested successfully by executing these models individually.\r\n\r\n\r\n\r\n> @Samjith888,\r\n> I am unable to reproduce the error because of the missing files `first_model.h5` and `second_model.hdf5`.\r\n> \r\n> Could you please share all the supporting files required to run the code. Thanks!\r\n\r\nHello ,\r\n The custom trained weighst are very large and can't upload it here.  Now i have updated the code for downloading the second_model . \r\n\r\nPlease download the weight of first model   [by clicking here ](https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5) and modify the below line in main.py by this model path\r\n\r\n`first_model_path = 'first_model.h5'`\r\n\r\nAlso place the replace the path of second model once it downloaded \r\n\r\n`second_model_path = path to 'second_mdl'`\r\n\r\nI couldn't simplify the code and this issue more than this \r\n\r\nNote : This is a common error, not depend on the trained weights which i generated. So please try to help me based on the pretrained weights which i have mentioned above", "Each model will have its own session and graph. The default session and graph within this context cannot load the new weights to the already-existing graph. This could be solved in TF 1 with session, but I don't see how to solve this in TF 2 yet without disabling eager execution.\r\n\r\nTry:\r\n```\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\nfirst_graph = tf.Graph()\r\nfirst_session = tf.compat.v1.Session(config=config)\r\n\r\nsecond_graph = tf.Graph()\r\nsecond_session = tf.compat.v1.Session(config=config)\r\n\r\nfirst_model_path = 'first_model.h5'\r\nsecond_model_path = 'second_model.hdf5'\r\n\r\nwith first_graph.as_default(), first_session.as_default():\r\n    first_prediction(image,first_model_path)\r\n\r\n#second model loading\r\nwith second_graph.as_default(), second_session.as_default():\r\n    second_model = load_model(second_model_path)\r\n    second_prediction(image,second_model)\r\n```", "@Samjith888,\r\nOn running the `main.py` file I am facing a different error stating `AttributeError: 'NoneType' object has no attribute 'astype'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/580c32f3f0a31327709b363bd446474d/38571.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38571\">No</a>\n", "I have fixed this by changing the `load` function. If you are loading your model from `.h5` or `.pb`\r\n\r\nchange it from this:\r\n\r\n`model = tf.saved_model.load(self.filename, [tf.saved_model.SERVING])`\r\n\r\nto this:\r\n\r\n```\r\nfrom tensorflow.python.keras.models import load_model\r\nmodel = load_model(self.filename)\r\n```", "> Each model will have its own session and graph. The default session and graph within this context cannot load the new weights to the already-existing graph. This could be solved in TF 1 with session, but I don't see how to solve this in TF 2 yet without disabling eager execution.\r\n> \r\n> Try:\r\n> \r\n> ```\r\n> config = tf.compat.v1.ConfigProto()\r\n> config.gpu_options.allow_growth = True\r\n> \r\n> first_graph = tf.Graph()\r\n> first_session = tf.compat.v1.Session(config=config)\r\n> \r\n> second_graph = tf.Graph()\r\n> second_session = tf.compat.v1.Session(config=config)\r\n> \r\n> first_model_path = 'first_model.h5'\r\n> second_model_path = 'second_model.hdf5'\r\n> \r\n> with first_graph.as_default(), first_session.as_default():\r\n>     first_prediction(image,first_model_path)\r\n> \r\n> #second model loading\r\n> with second_graph.as_default(), second_session.as_default():\r\n>     second_model = load_model(second_model_path)\r\n>     second_prediction(image,second_model)\r\n> ```\r\n\r\nHi, did you find the solution to this issue with tensorflow 2? If not, how did you disable the eager execution?", "Same problem here, any one solved this issue please help me!!\r\nThank guys !"]}, {"number": 38570, "title": "Add new hyperparameter k in tf.compat.v1.train.polynomial_decay and other decay fuction", "body": "Hey, according to [this paper](https://arxiv.org/abs/2004.05909) should be adding new hyperparameter k in the API of tf.compat.v1.train.polynomial_decay and other decay functions to simply improve performance. Thank you very much.\r\n", "comments": ["We are not making ongoing changes to compat.v1 functions. You can consider making a contribution to TF Addons if you have related functionality you want to add for TF v2. https://github.com/tensorflow/addons"]}, {"number": 38569, "title": "Not coverting pb to tflite for using with microinterpreter for x86_64 platform", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (using pip):\r\n- TensorFlow version (1.14.0):\r\n\r\n\r\n**1) when I convert my model to pb using export_inference_graph.py using following command:\r\n\r\n> python export_inference_graph.py --input_type image_tensor --pipeline_config_path ../training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix ../training/model.ckpt-25 --output_directory ../inference_graph\r\n\r\n**2) convert the above generated pb to tflite using tflite converter by below command:**\r\n\r\n> tflite_convert   --output_file=\"/home/ashwini/Object_detection_general/models/research/logs_text_detection/frozen_pb_using_export_inference_graph/aaa.tflite\"   --graph_def_file=\"/home/ashwini/Object_detection_general/models/research/logs_text_detection/frozen_pb_using_export_inference_graph/frozen_inference_graph.pb\"   --inference_type=QUANTIZED_UINT8   --input_arrays=\"image_tensor\"   --output_arrays=\"detection_boxes,detection_scores,detection_classes,num_detections\"   --mean_values=128   --std_dev_values=128   --input_shapes=1,300,300,3   --change_concat_input_ranges=false --default_ranges_min=0 and --default_ranges_max=6\r\n**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**2020-04-15 16:09:59.924340: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\n2020-04-15 16:09:59.924359: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArraySizeV3\r\n2020-04-15 16:09:59.924385: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TensorArrayGatherV3\r\n2020-04-15 16:09:59.982759: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1442 operators, 2553 arrays (0 quantized)\r\n2020-04-15 16:10:00.074646: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 1367 operators, 2399 arrays (0 quantized)\r\n2020-04-15 16:10:00.186402: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1367 operators, 2399 arrays (0 quantized)\r\n2020-04-15 16:10:00.268372: F tensorflow/lite/toco/graph_transformations/resolve_constant_slice.cc:59] Check failed: dim_size >= 1 (0 vs. 1)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007fb5e3e9e740 (most recent call first):\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33 in execute\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59 in main\r\n  File \"/home/ashwini/Object_detection_general/environment/obj_det/bin/toco_from_protos\", line 11 in <module>\r\nAborted (core dumped)**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you try using [export_tflite_ssd_graph](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py) instead to convert your model in to `.pb` and try converting to tflite?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Ashwini-prkt \r\n\r\nAny update on this issue please. Thanks!", "> Can you try using [export_tflite_ssd_graph](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py) instead to convert your model in to `.pb` and try converting to tflite?\r\n\r\nI tried with export_tflite_ssd_graph.py, it is generating the pb file.\r\nThen I tried to convert to tflite using tflite_convert and with --allow_custom_ops = True\r\nThen I converted the tflite generated to C-byte array using xxd tool to implement model for microcontrollers (tflite micro)\r\nBut as **mobilenet ssd v1** has some postprocessing operations which are unsupported with **tflite micro** , I am unable to use the model's C-byte generated and giving me errors for custom operations such as TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 while excuting it with TFLite microinterpreter.", "Perhaps you can raise a new issue thread for unsupported tflite micro operations. We want to keep each issue thread specific to discuss initial problem. \r\nThanks!"]}, {"number": 38568, "title": "Error occurs when bazel building the example", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.1.0\r\n- Python version:3.7.7\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):2.0.0\r\n\r\n\r\n\r\n**Describe the problem**\r\nRecently I'm studying tensorflow-lite. After cloning the repo and install Bazel2.0.0, I'm trying to build the tf-lite hello-world example. \r\n**Here's the command**\r\n```shell\r\ncd E:\\tensorflow\\tensorflow\\lite\\micro\\examples\\hello_world\r\nbazel build :sine_model_data\r\n```\r\n**Here's the error message:**\r\n```shell\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Anaconda3/envs/tf2.0/python.exe\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file e:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file e:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file e:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Call stack for the definition of repository 'flatbuffers' which is a third_party_http_archive (rule definition at E:/tensorflow/third_party/repo.bzl:219:28):\r\n - E:/tensorflow/third_party/flatbuffers/workspace.bzl:6:5\r\n - E:/tensorflow/tensorflow/workspace.bzl:53:5\r\n - E:/tensorflow/tensorflow/workspace.bzl:100:5\r\n - E:/tensorflow/WORKSPACE:19:1\r\nINFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.\r\n * Hash '62f2223fb9181d1d6338451375628975775f7522185266cd5296571ac152bc45' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz\r\nIf the definition of 'flatbuffers' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'flatbuffers':\r\n   java.io.IOException: Could not create symlink from E:/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl: C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl (File exists)\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - E:/tensorflow/WORKSPACE:37:1\r\nERROR: Skipping ':sine_model_data': no such package '@flatbuffers//': java.io.IOException: Could not create symlink from E:/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl: C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl (File exists)\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@flatbuffers//': java.io.IOException: Could not create symlink from E:/tensorflow/third_party/flatbuffers/build_defs.bzl to C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl: C:/users/chenhao/_bazel_chenhao/3rb265pl/external/flatbuffers/build_defs.bzl (File exists)\r\nINFO: Elapsed time: 2.267s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/lite/micro/examples/hello_world\r\n```\r\n\r\nAccording to the Error, I try to build the flatbuffer first(I don't know whether it'll work, just have a try)\r\nHere's the command\r\n```shell\r\ncd E:\\tensorflow\\third_party\\flatbuffers\r\nbazel build :flatbuffers\r\n```\r\nHere's the output\r\n```shell\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/Anaconda3/envs/tf2.0/python.exe\r\nINFO: Reading rc options for 'build' from e:\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Found applicable config definition build:v2 in file e:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file e:\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file e:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n - C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - E:/tensorflow/WORKSPACE:37:1\r\nINFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at E:/tensorflow/third_party/py/python_configure.bzl:280:20):\r\n - E:/tensorflow/tensorflow/workspace.bzl:96:5\r\n - E:/tensorflow/WORKSPACE:19:1\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n                _create_local_python_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 209, in _create_local_python_repository\r\n                _check_python_bin(<2 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 145, in _check_python_bin\r\n                auto_config_fail(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/remote_config/common.bzl\", line 12, in auto_config_fail\r\n                fail(<1 more arguments>)\r\nConfiguration Error: --define PYTHON_BIN_PATH='D:\\Anaconda3\\envs\\tf2.0\\python.exe\r\nD:\\Anaconda3\\python.exe\r\nC:\\Users\\ChenHao\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe' is not executable. Is it the python binary?\r\nINFO: Call stack for the definition of repository 'rules_java' which is a http_archive (rule definition at C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - C:/users/chenhao/_bazel_chenhao/3rb265pl/external/bazel_tools/tools/build_defs/repo/utils.bzl:205:9\r\n - /DEFAULT.WORKSPACE.SUFFIX:290:1\r\nERROR: While resolving toolchains for target //third_party/flatbuffers:flatbuffers: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n                _create_local_python_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 209, in _create_local_python_repository\r\n                _check_python_bin(<2 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 145, in _check_python_bin\r\n                auto_config_fail(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/remote_config/common.bzl\", line 12, in auto_config_fail\r\n                fail(<1 more arguments>)\r\nConfiguration Error: --define PYTHON_BIN_PATH='D:\\Anaconda3\\envs\\tf2.0\\python.exe\r\nD:\\Anaconda3\\python.exe\r\nC:\\Users\\ChenHao\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe' is not executable. Is it the python binary?\r\nERROR: Analysis of target '//third_party/flatbuffers:flatbuffers' failed; build aborted: invalid registered toolchain '@local_config_python//:py_toolchain': no such package '@local_config_python//': Traceback (most recent call last):\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 263\r\n                _create_local_python_repository(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 209, in _create_local_python_repository\r\n                _check_python_bin(<2 more arguments>)\r\n        File \"E:/tensorflow/third_party/py/python_configure.bzl\", line 145, in _check_python_bin\r\n                auto_config_fail(<1 more arguments>)\r\n        File \"E:/tensorflow/third_party/remote_config/common.bzl\", line 12, in auto_config_fail\r\n                fail(<1 more arguments>)\r\nConfiguration Error: --define PYTHON_BIN_PATH='D:\\Anaconda3\\envs\\tf2.0\\python.exe\r\nD:\\Anaconda3\\python.exe\r\nC:\\Users\\ChenHao\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe' is not executable. Is it the python binary?\r\nINFO: Elapsed time: 20.577s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n    currently loading: @bazel_tools//tools/jdk\r\n\r\n```\r\nWhy is there an Error about \"**PYTHON_BIN_PATH** which is the right path in the output. \r\n\r\nHow can I build the example right?\r\n\r\n", "comments": ["@ChenHaoHere\r\nThe tflite micro is designed to run on micro controller.\r\nYou are building it for PC which is not the appropriate way to use that example.\r\nCould you refer to the README.md file inside tensorflow\\lite\\micro\\examples\\hello_world to try it?", "Could you try again? I'm not sure if this issue is specific to the hello_world example.\r\n\r\nThe following commands worked for me on linux from the root of the tensorflow tree:\r\n```\r\nbazel clean\r\nbazel test tensorflow/lite/micro/examples/hello_world:hello_world_test\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38568\">No</a>\n"]}, {"number": 38567, "title": "Implement int8 requantization in TFLu.", "body": "Change-Id: I078ca4306074359d225b09a2a4c8ddb9832e9e7d", "comments": ["@patriklaurell Can you please check @jenselofsson comments and keep us posted. Thanks!", "@jenselofsson comment has been resolved", "@patriklaurell Can you please resolve conflicts? Thanks!"]}, {"number": 38566, "title": "Reset best weights in EarlyStopping.on_train_begin", "body": "EarlyStopping callback instances can be reused in different training loops, but the `on_train_begin` hook forgets to reset the `self.best_weights` state.", "comments": ["There's also `self.wait = 0` and `self.stopped_epoch = 0` that could be removed from `__init__` in that case. Personally, I'd keep it there --- some IDEs (PyCharm / IntelliJ) prefer to have all attributes defined in `__init__` .", "> There's also `self.wait = 0` and `self.stopped_epoch = 0` that could be removed from `__init__` in that case. Personally, I'd keep it there --- some IDEs (PyCharm / IntelliJ) prefer to have all attributes defined in `__init__` .\r\n\r\nSounds good. Thanks!"]}, {"number": 38565, "title": "Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED ", "body": "**System information**\r\n- windows 10 64\r\n- TensorFlow installed from anaconda\r\n- TensorFlow version: 2.1\r\n- Python version: 3.7.6\r\n- Installed using conda\r\n- CUDA/cuDNN version: \r\nnot sure what cuda version i use: conda list cudatoolkit gives 10.1.243 but nvidia-smi gives cuda 11\r\ncudnn version: 7.6.5\r\n- GPU model and memory:\r\nnvidia gtx 1660Ti 6GB turing\r\nkeras version: 2.3.1, keras base 2.3.1, keras-application 1.0.8, keras prepossessing 1.1 \r\n\r\n\r\n\r\n\r\nwhen I an trying to use the layer conv2Dtranspose in a keras model I get the error:\r\n```\r\n2020-04-15 11:26:57.432157: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-04-15 11:26:57.438997: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-04-15 11:26:57.443860: F tensorflow/core/kernels/conv_grad_input_ops.cc:1163] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()), &algorithms)\r\n```\r\nmy guess is that there is a compatablity issue with my cuda or cudnn version, that I dont know how to change in anaconda. changing the version of anything else does not seem to work. \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem:\r\n\r\n```\r\nimport os\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nfrom keras.models import Model\r\nfrom keras.optimizers import Adam\r\nfrom keras.layers.advanced_activations import LeakyReLU\r\nfrom keras.layers import Conv2D, Input,BatchNormalization, Flatten, Dense, Activation, Conv2DTranspose\r\nfrom keras import backend as K\r\nfrom sklearn.model_selection import train_test_split\r\nfrom keras.callbacks.callbacks import EarlyStopping,Callback\r\nfrom keras.backend.tensorflow_backend import set_session\r\nimport tensorflow as tf\r\ngpus= tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n\r\ndef adam_optimizer():\r\n    return Adam(lr=0.000002, beta_1=0.5)\r\n\r\ndef build_generator(inputs, image_size):\r\n    # network parameters\r\n    layer_filters = [256, 128, 64, 32, 3]\r\n    strides = 2\r\n    x = inputs\r\n    for filters in layer_filters:\r\n        x = Conv2DTranspose(filters=filters,kernel_size=(3,3),strides=strides,padding='same')(x)\r\n        shape = K.int_shape(x)\r\n        x = Activation('relu')(x)\r\n        x = BatchNormalization()(x)\r\n    x = Activation('sigmoid')(x)\r\n    generator = Model(inputs, x, name='generator')\r\n    generator.compile(loss='binary_crossentropy',optimizer=adam_optimizer(),metrics=['accuracy'])\r\n    return generator\r\n\r\ninputs = Input(shape=(2,2,480))\r\ng = build_generator(inputs, (64,64,3))\r\n\r\nfalse_faces = g.predict(np.random.normal(size=(2,2,2,480)))\r\n```\r\nAny other info / logs:\r\nlogs from the notebook:\r\n```\r\n(base) C:\\Users\\Moran>jupyter notebook\r\n[I 11:26:40.436 NotebookApp] Serving notebooks from local directory: C:\\Users\\Moran\r\n[I 11:26:40.436 NotebookApp] The Jupyter Notebook is running at:\r\n[I 11:26:40.436 NotebookApp] http://localhost:8888/?token=eeabd801c4c1b83aa79d704bf93c4465f305c688ebb846dc\r\n[I 11:26:40.436 NotebookApp]  or http://127.0.0.1:8888/?token=eeabd801c4c1b83aa79d704bf93c4465f305c688ebb846dc\r\n[I 11:26:40.436 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 11:26:40.508 NotebookApp]\r\n\r\n    To access the notebook, open this file in a browser:\r\n        file:///C:/Users/Moran/AppData/Roaming/jupyter/runtime/nbserver-14384-open.html\r\n    Or copy and paste one of these URLs:\r\n        http://localhost:8888/?token=eeabd801c4c1b83aa79d704bf93c4465f305c688ebb846dc\r\n     or http://127.0.0.1:8888/?token=eeabd801c4c1b83aa79d704bf93c4465f305c688ebb846dc\r\n[I 11:26:45.246 NotebookApp] Kernel started: 14241611-5379-485d-b99b-c51ef765ca55\r\n2020-04-15 11:26:47.464197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-15 11:26:50.655541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-15 11:26:50.703563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2020-04-15 11:26:50.715262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-15 11:26:50.726496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-15 11:26:50.737190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-15 11:26:50.747595: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-15 11:26:50.759230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-15 11:26:50.768199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-15 11:26:50.786405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-15 11:26:50.794014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-15 11:26:50.798597: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-04-15 11:26:50.811901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2020-04-15 11:26:50.823790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-15 11:26:50.830518: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-15 11:26:50.837440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-15 11:26:50.844229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-15 11:26:50.851215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-15 11:26:50.857471: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-15 11:26:50.864682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-15 11:26:50.870630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-15 11:26:51.694569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-15 11:26:51.700712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-04-15 11:26:51.705257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-04-15 11:26:51.710677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 4625 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-04-15 11:26:52.683708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2020-04-15 11:26:52.696926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-15 11:26:52.703966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-15 11:26:52.710870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-15 11:26:52.717898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-15 11:26:52.726352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-15 11:26:52.732807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-15 11:26:52.738251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-15 11:26:52.746009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-15 11:26:52.750168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-15 11:26:52.756798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0\r\n2020-04-15 11:26:52.760471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-04-15 11:26:52.764642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4625 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-04-15 11:26:55.502574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-15 11:26:57.432157: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-04-15 11:26:57.438997: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-04-15 11:26:57.443860: F tensorflow/core/kernels/conv_grad_input_ops.cc:1163] Check failed: stream->parent()->GetConvolveBackwardDataAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(stream->parent()), &algorithms)\r\n```\r\ni am trying to solve this for days (!!) now - I looked at all similar problems but for no use  - the solutions there does not work or not apply to my case (for example, since I am using anaconda)", "comments": ["@MoranReznik, Please provide the details about cuDNN version and also add the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environmental variable.Please refer this [doc](https://www.tensorflow.org/install/gpu#windows_setup). Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@MoranReznik,\r\nplease update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38565\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38565\">No</a>\n", "I have the exact same problem!", "me too, this issue shouldn't be closed", "same here, nvidia 1050ti on Dell Notebook", "I have same problem , GeForce GTX 1660 SUPER,cudnn-10.0-windows10-x64-v7.6.4.38"]}, {"number": 38564, "title": "Wrong function in example for tensor_diag", "body": "The examples in the documentations of `tf.linalg.tensor_diag_part` and `tf.linalg.tensor_diag`\r\nare showing the non-tensor version of these functions, e.g.\r\n```\r\n# 'diagonal' is [1, 2, 3, 4]\r\ntf.diag(diagonal) ==> [[1, 0, 0, 0]\r\n                       [0, 2, 0, 0]\r\n                       [0, 0, 3, 0]\r\n                       [0, 0, 0, 4]]\r\n```\r\n\r\nSee \r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag\r\nhttps://www.tensorflow.org/api_docs/python/tf/linalg/tensor_diag_part", "comments": ["I'll start working on this, Thanks!\r\n", "Is this issue not closed yet?", "I think the documentation seems to be fine and is indicating what the resulting Tensor would look like. Closing issue for now.", "sorry, but I have to disagree on this being resolved and the documentation being fine. The documentation shows an example which uses the `tf.diag_part` function, but is supposed to document usage of the `tf.tensor_diag_part` function (which is not an alias but another function). The choice of example seems also unfortunate in the sense that, in this particular case, both functions would give the same result, and as such the example does not help the reader to better understand the difference between the two. "]}, {"number": 38563, "title": "error: 'arm_convolve_1_x_n_s8_get_buffer_size' was not declared in this scope", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04 under WSL\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source):  2ddbc3572cd10002cf4e74a9c7b4dbf3d5995d7b\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):   sparkfun_edge\r\n\r\nwhen trying to build person detection app using the cmsis-nn libraries using this command:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=sparkfun_edge person_detection_int8_bin`\r\n\r\nI run into the following compile error:\r\n\r\n`arm-none-eabi-g++ -std=c++11 -DTF_LITE_STATIC_MEMORY -O3 -DPART_apollo3 -DAM_PACKAGE_BGA -DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D __FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 -fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections -fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb -mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra -Wsign-compare -Wdouble-promotion -Wunused-variable -Wshadow -Wmissing-field-initializers -Wno-unused-parameter -Wno-write-strings -fno-delete-null-pointer-checks -fno-threadsafe-statics -fomit-frame-pointer -fpermissive -fno-use-cxa-atexit -nostdlib -ggdb -O3 -I. -Itensorflow/lite/micro/tools/make/downloads/ -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/third_party/hm01b0 -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ -isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ -Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/mcu/apollo3/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/CMSIS/AmbiqMicro/Include/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/edge/bsp -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/devices/ -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/utils/ -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/Core/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/NN/Include -Itensorflow/lite/micro/tools/make/downloads/cmsis//CMSIS/DSP/Include -Itensorflow/lite/micro/tools/make/downloads/kissfft -Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/boards_sfe/common/third_party/lis2dh12/ -c tensorflow/lite/micro/kernels/cmsis-nn/conv.cc -o tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/kernels/cmsis-nn/conv.o\r\ntensorflow/lite/micro/kernels/cmsis-nn/conv.cc: In function 'TfLiteStatus tflite::ops::micro::conv::EvalQuantizedPerChannel(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::micro::conv::OpData*, const TfLiteTensor*, const TfLiteTensor*, const TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':\r\ntensorflow/lite/micro/kernels/cmsis-nn/conv.cc:269:30: error: 'arm_convolve_1_x_n_s8_get_buffer_size' was not declared in this scope\r\n     const int32_t buf_size = arm_convolve_1_x_n_s8_get_buffer_size(\r\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/micro/kernels/cmsis-nn/conv.cc:269:30: note: suggested alternative: 'arm_convolve_s8_get_buffer_size'\r\n     const int32_t buf_size = arm_convolve_1_x_n_s8_get_buffer_size(\r\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                              arm_convolve_s8_get_buffer_size\r\ntensorflow/lite/micro/kernels/cmsis-nn/conv.cc:271:9: error: 'get_cmsis_scratch_buffer' was not declared in this scope\r\n     if (get_cmsis_scratch_buffer(context, &buf, buf_size) != kTfLiteOk) {\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/micro/tools/make/Makefile:288: recipe for target 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/kernels/cmsis-nn/conv.o' failed\r\nmake: *** [tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/kernels/cmsis-nn/conv.o] Error 1`\r\n\r\n\r\nwithout the TAGS=cmsis-nn flags, the code builds without trouble ...\r\n\r\nBest Regards\r\n\r\n\r\n\r\n\r\n", "comments": ["Hello,\r\n\r\nThanks for reporting this issue! It should be fixed by this PR: https://github.com/tensorflow/tensorflow/pull/38123 (got merged just recently). Remember to clean the downloads folder before re-testing by typing:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile TAGS=cmsis-nn TARGET=sparkfun_edge clean_downloads`\r\n\r\nCheers!", "allready fixed indeed. \r\n\r\nThanks for pointing out."]}, {"number": 38562, "title": "Fix only pass RunOptions to keras will trigger core", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nRight now, keras will trigger segmentation fault when only pass `RunOptions` but no `RunMetadata`. This bug can be replicated in tf-nightly in this [gist](https://colab.research.google.com/gist/zhuzilin/a6f71f2240a60769e1a6e054e8733145/untitled3.ipynb)\r\n\r\nIf you cannot open the gist, the code to replicate the bug is\r\n```python\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nrun_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'],\r\n              options=run_options)\r\n\r\nmodel.fit(x_train,\r\n          y_train,\r\n          epochs=2)\r\n```\r\n\r\nThe reason for this problem is that keras use the `TF_SessionRunCallable` api instead of `TF_Run`. While the former was using an `unique_ptr` to represent the `run_metadata` and will only new one when `RunMetadata` was passed. However, the `RunOption` will need an `RunMetadata` for tracing. This PR modified the implementation of `TF_SessionRunCallable` to that of `TF_Run` and fix this bug.\r\n\r\nThank you for your time on reviewing this PR.", "comments": ["@omalleyt12 Could you have a look at this PR? Thank you!", "@tanzhenyu \r\nCould you have a look at this pr?", "@tanzhenyu Can you please review this PR ? Thanks!", "@tanzhenyu, Can you please review this PR ? Thanks!"]}, {"number": 38561, "title": "Unwanted tf.function retracing when using variable-length inputs", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0rc2\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n\r\nA lot of warnings saying that there is a tf.function retracing are happening when using a keras model in a loop with variable length inputs.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like not to have retracing if there is no need (for example a fully convolutionnal model).\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```python\r\nfrom random import randint\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv1D\r\nfrom tensorflow.keras.models import Sequential\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv1D(8, 3))\r\nmodel.build([None, 12, 1])\r\n\r\npredict_tensors = [\r\n    tf.random.normal([randint(1, 8), randint(4, 40), 1])\r\n    for _ in range(10)\r\n]\r\nfor t in predict_tensors:\r\n    _ = model.predict(t)\r\n```\r\n\r\n**Other info / logs** \r\n\r\nLogs:\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0406 09:22:52.525994 139643050075904 def_function.py:598] 5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a7fc1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW0406 09:22:52.615050 139643050075904 def_function.py:598] 6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a7fc1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW0406 09:22:52.653312 139643050075904 def_function.py:598] 7 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a7fc1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nW0406 09:22:52.706550 139643050075904 def_function.py:598] 8 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f00a7fc1268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n```\r\n\r\nThis issue was originally described [here](https://github.com/tensorflow/tensorflow/issues/34025#issuecomment-609612284), and some other people have had trouble with [training as well](https://github.com/tensorflow/tensorflow/issues/34025#issuecomment-609186763).\r\n\r\nWhen switching back to 2.1, the problem is gone.", "comments": ["As per my understanding, if the input tensor's shape or dtype changes(if it is not constant) then the function would get retraced again.\r\nYou may refer this https://www.tensorflow.org/api_docs/python/tf/function ", "Yes this is totally true, but I am not using `tf.function` myself directly. Maybe keras is under the hood, but in any case they should handle inputs with varying shapes (but same rank and \"compatible\" shapes) better by for example specifying a dynamic input signature (see *Inputs signatures* in the doc).\r\n\r\nMoreover, the behaviour I am describing is for version 2.2.0rc2, and the doc is still for 2.1 where there is no issue.", "You can see the current doc here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/function?version=nightly\r\nI think the option you need should be `experimental_relax_shapes`.\r\n\r\nAs a workaround, you could try to wrap the keras model in an explicit tf.function call, like this\r\n```\r\n@tf.function(experimental_relax_shapes=True)\r\ndef predict(x):\r\n     return model.predict(x)\r\n```", "> Yes this is totally true, but I am not using `tf.function` myself directly. Maybe keras is under the hood, but in any case they should handle inputs with varying shapes (but same rank and \"compatible\" shapes) better by for example specifying a dynamic input signature (see _Inputs signatures_ in the doc).\r\n> \r\n> Moreover, the behaviour I am describing is for version 2.2.0rc2, and the doc is still for 2.1 where there is no issue.\r\n\r\nFollowing is the ouput of tf 2.1.0, seems output is the same\r\n```\r\n/usr/local/bin/python3.7 /Users/gurushant/PycharmProjects/MTCNN/test6.py\r\n2020-04-15 14:12:33.527382: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-04-15 14:12:33.545554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa56ad8c050 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-15 14:12:33.545588: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:5 out of the last 5 calls to <function _make_execution_function.<locals>.distributed_function at 0x134d83290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:6 out of the last 6 calls to <function _make_execution_function.<locals>.distributed_function at 0x134d83290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:7 out of the last 7 calls to <function _make_execution_function.<locals>.distributed_function at 0x134d83290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:8 out of the last 8 calls to <function _make_execution_function.<locals>.distributed_function at 0x134d83290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:9 out of the last 9 calls to <function _make_execution_function.<locals>.distributed_function at 0x134d83290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n```", "@gurushantj yes you are right. I don't know why I thought this was in 2.1, it's actually in 2.0 that the problem is gone.\r\n\r\nStill, the documentation regarding the re-tracing is about the same.\r\n\r\n@ngc92 I tried your workaround but got the following error:\r\n```\r\nValueError: When using data tensors as input to a model, you should specify the `steps` argument.\r\n```", "> @gurushantj yes you are right. I don't know why I thought this was in 2.1, it's actually in 2.0 that the problem is gone.\r\n> \r\n> Still, the documentation regarding the re-tracing is about the same.\r\n> \r\n> @ngc92 I tried your workaround but got the following error:\r\n> \r\n> ```\r\n> ValueError: When using data tensors as input to a model, you should specify the `steps` argument.\r\n> ```\r\n\r\nCould you please validate following and let me know :\r\n\r\nDisable eager execution setting\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\n```\r\nand pass ```steps=1 ``` in model.predict and validate", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/bacd69c72e28f03ebfc6221d8d8fe1fc/38561.ipynb), [TF v2.2.0rc3](https://colab.research.google.com/gist/amahendrakar/cda0a10a833f90ebffe9f5f42e2220aa/38561-2-2.ipynb), [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bac1fe12525a0a1511adc75c308695d7/38561-tf-nightly.ipynb#scrollTo=gx2su05ORucx). Please find the attached gist. Thanks!", "@zaccharieramzi,\r\nCould you please check [this comment](https://github.com/tensorflow/tensorflow/issues/34907#issuecomment-564312261) from a similar issue and let us know if it works? Thanks!", "@amahendrakar I am not sure what I am supposed to see in that comment. The issue you linked suggests that this should be dealt with.", "@ngc92 still got an error: `AttributeError: 'Tensor' object has no attribute '_numpy'`.", "Is this what you want to do?\r\n```\r\n@tf.function(experimental_relax_shapes=True)\r\ndef predict(t):\r\n    return model(t)\r\n\r\nfor t in predict_tensors:\r\n    _ = predict(t)\r\n```\r\n\r\nNote that you are no longer using any features of the `model.predict` function, but since you seem to be looping over examples by hand that might be OK.\r\n\r\nAlso, in tf 2.2 there is support for custom `model.predict_function`, i.e. you might be able to do something like\r\n```\r\nmodel.predict_function = tf.function(experimental_relax_shapes=True)(model.predict_function)\r\n```\r\ni.e. just wrapping the default provided function in something that relaxes shapes.\r\nI haven't tried 2.2 yet, so I'm not very sure about the second suggestion.  ", "@ngc92 yes this is a fair workaround. However there are cases where you would want to use `predict` for the callbacks or the batch size.\r\n\r\nThe second option you provided didn't work straight out of the box, but you can try things in tf 2.2 in colab: https://colab.research.google.com/drive/1MfRPQyRhjrF7he7fymoIEG7k64YCd0Da\r\n\r\nYou will notice that in the case of `evaluate` and I guess `train` if you feed the variable-length input through a tf dataset, it doesn't retrace the function, suggesting a bug somewhere. ", "We're investigating - it seems that a newly-added warning about function retracing seems to fire more than expected.", "The error message is not new one so this seems from the existing retracing detection logic.  I think the warning is WAI as it's tracing many times here.  Perhaps Keras using `experimental_relax_shapes` is an option?", "@omalleyt12 @fchollet ", "@mdanatg do you have any news on this?", "@zaccharieramzi No fix yet. According to the [code](https://github.com/tensorflow/tensorflow/blob/2f2d1a3a2f4e154017c4ab1aff0a18473ecaa494/tensorflow/python/keras/engine/training.py#L1213) the function that the warning talks about should be cached and only traced once.\r\n\r\n@omalleyt12 any thoughts why the tracing happens so many times?", "@mdanatg ok too bad, I just have one question though maybe you have the answer.\r\nDo you know if the [fix provided](https://github.com/tensorflow/tensorflow/issues/38561#issuecomment-614225366) by @ngc92 , i.e.:\r\n\r\n```python\r\n@tf.function(experimental_relax_shapes=True)\r\ndef predict(t):\r\n    return model(t)\r\n\r\nfor t in predict_tensors:\r\n    _ = predict(t)\r\n```\r\n\r\nstill allows `predict` to benefit from a distribution strategy (typically `MirroredStrategy`)? My guess is that not but I am not sure, and not sure how to test this on a single GPU (2 logical GPUs).", "@guptapriya ", "@zaccharieramzi Thanks for the issue! This should be fixed in the latest nightly", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38561\">No</a>\n", "@omalleyt12 thanks ! One question I didn't ask though is: was this bug slowing down anything or I am just getting annoyed with an unwanted warning?", "TF 2.3 still have this issue. ", "The problem still persists with version 2.4.1 and the workaround doesn't work on predict_on_batch:\r\n```\r\n    @tf.function(experimental_relax_shapes=True)\r\n    def predict_on_batch(self,states):\r\n        return self.model.predict_on_batch(states)\r\n```\r\ntf reports error:\r\n`RuntimeError: Detected a call to `Model.predict_on_batch` inside a `tf.function`. `Model.predict_on_batch is a high-level endpoint that manages its own `tf.function`. Please move the call to `Model.predict_on_batch` outside of all enclosing `tf.function`s. Note that you can call a `Model` directly on `Tensor`s inside a `tf.function` like: `model(x)`.`", "OK, I found my problem is because of multi-thread calling the predict_on_batch function. I added an empty predict before launching the threads and the warning was gone.", "Have this issue with using model.predict inside a loop of 5 different models . The warning also leads to :\r\n\r\nnvidia-smi\r\nFailed to initialize NVML: Driver/library version mismatch", "My observation - In multiprocessing setting, invoking predict() causes this warning and when processing large amounts of data it errors out eventually(may be memory leakage). Setting experimental_relax_shape=True for the function being invoked by multiple processors resolves the issue. Also using model(input) instead of model.predict(input) resolves the issue. So key issue seems to be due to retracing even when input shape changes. Issue persists even on using tensors as input instead of python object"]}, {"number": 38560, "title": "Memory leak when try to use Metal delegate with \u201cTFLGpuDelegateWaitTypeAggressive\u201d option", "body": "**System information** \r\n- Mobile device \uff1aiPhone XR\r\n- OS version\uff1a13.3.1\r\n\r\n**Describe the current behavior**\r\nI encountered a memory leak problem when trying  to deploy metal delegate backend on iPhone.\r\n\r\n**Describe the expected behavior**\r\nThere is no memory leak in tflite lib\r\n\r\n**Standalone code to reproduce the issue** \r\n1)\tModify the source code in metal_delegate.mm as follows\r\nAt line 109, add \u201calarm_thread_ = 0; \u201c during the init of \u201calarm_thread_\u201d;\r\n### \r\n      device_ = [command_queue_ device];\r\n      total_alarms_ = 1;\r\n      alarm_thread_ = 0;\r\n      NSString* error;\r\n\r\n> Note:\r\nThis change is to allow the \u201calarm_thread\u201d thread to run normally when metal delegate option is set to \u201cTFLGpuDelegateWaitTypeAggressive\u201d. \r\n\r\n2)\tRecompile the modified source code, generate tflite framework lib and metal delegate library\r\n3)\tUse the IOS project located in lite/examples/ios/camera to test the new lib generated by step 2)\r\nModify the source code in CameraExampleViewController.mm as follows\r\n> TFLGpuDelegateOptions options; \r\n  options.allow_precision_loss = true; \r\n  // options.wait_type = TFLGpuDelegateWaitTypeActive; \r\n  options.wait_type = TFLGpuDelegateWaitTypeAggressive; \r\n  delegate = TFLGpuDelegateCreate(&options); \r\n  interpreter->ModifyGraphWithDelegate(delegate);\r\n\r\n4)\tUse Xcode IDE to recompile the camera project, it can be seen that when the APP is running on iPhone, the memory keeps growing.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n", "comments": ["Should have been fixed by \r\nhttps://github.com/tensorflow/tensorflow/commit/2a72ad4071b3b5612492f85ee6bd50e35c90128b", "@wangshuai-github Could you please have a look at this[ commit ](https://github.com/tensorflow/tensorflow/commit/2a72ad4071b3b5612492f85ee6bd50e35c90128b) probably it is a fixed for this issue . Please let us know if it helps? Thank you! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38560\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38560\">No</a>\n"]}, {"number": 38559, "title": "Failed to use vectorizing mapping for tf.data", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  ubuntu18.04\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  pip, tf2.1.0\r\n- CUDA/cuDNN version: - GPU model and memory: cuda10.1, cudnn7.6.5\r\n\r\n**Describe the current behavior**\r\nFailed to apply vectorizing mapping for tf.data\r\n\r\n**Describe the expected behavior**\r\nApply vectorizing mapping for tf.data and speed up as described in https://www.tensorflow.org/guide/data_performance#vectorizing_mapping\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n    import tensorflow as tf\r\n\r\n    data = tf.data.TFRecordDataset(['images.tfrecord'])\r\n\r\n    image_feature_description = {\r\n        'height': tf.io.FixedLenFeature([], tf.int64),\r\n        'width': tf.io.FixedLenFeature([], tf.int64),\r\n        'depth': tf.io.FixedLenFeature([], tf.int64),\r\n        'bboxes': tf.io.VarLenFeature(tf.int64),\r\n        'image_raw': tf.io.FixedLenFeature([], tf.string),\r\n    }\r\n\r\n    def parse_example(example):\r\n        data = tf.io.parse_single_example(example, image_feature_description)\r\n        \r\n        img = tf.io.decode_jpeg(data['image_raw'])\r\n        \r\n        img = tf.image.resize(img, (416, 416))\r\n\r\n        bboxes = data['bboxes']\r\n        bboxes = tf.sparse.to_dense(bboxes)\r\n        bboxes = tf.reshape(bboxes, [-1, 5])\r\n\r\n        return img, bboxes\r\n\r\n\r\n    #data = data.map(parse_example).batch(1)  # this works\r\n    data = data.batch(1).map(parse_example)  # I tried to apply vectorizing mapping but errors raised\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n    Traceback (most recent call last):\r\n    File \"test_tfrecord.py\", line 28, in <module>\r\n        data = data.batch(1).map(parse_example)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 1588, in map\r\n        return MapDataset(self, map_func, preserve_cardinality=True)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3888, in __init__\r\n        use_legacy_function=use_legacy_function)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3147, in __init__\r\n        self._function = wrapper_fn._get_concrete_function_internal()\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2395, in _get_concrete_function_internal\r\n        *args, **kwargs)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n        graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n        graph_function = self._create_graph_function(args, kwargs)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n        capture_by_value=self._capture_by_value),\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3140, in wrapper_fn\r\n        ret = _wrapper_helper(*args)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3082, in _wrapper_helper\r\n        ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n    File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 237, in wrapper\r\n        raise e.ag_error_metadata.to_exception(e)\r\n    ValueError: in converted code:\r\n\r\n        test_tfrecord.py:14 parse_example  *\r\n            data = tf.io.parse_single_example(example, image_feature_description)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/parsing_ops.py:472 parse_single_example_v2_unoptimized\r\n            serialized = _assert_scalar(serialized, \"serialized\")\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/parsing_ops.py:1319 _assert_scalar\r\n            raise ValueError(\"Input %s must be a scalar\" % name)\r\n\r\n        ValueError: Input serialized must be a scalar\r\n\r\nHow should I fix it, thanks", "comments": ["i am able to replicate this, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/30416fd71b2065ef43b6069de4223370/38559.ipynb)", "you are using `parse_single_example`, but passing in a batch of examples. This does not seem to be a bug but the intended behaviour of the function.\r\n\r\nFrom the documentation:\r\n```\r\nOne might see performance advantages by batching Example protos with parse_example instead of using this function directly.\r\n```\r\nhttps://www.tensorflow.org/api_docs/python/tf/io/parse_example", "I have replaced parse_single_example with parse_example, but it raised another error\r\n\r\n    import tensorflow as tf\r\n\r\n    data = tf.data.TFRecordDataset(['images.tfrecord'])\r\n\r\n    image_feature_description = {\r\n        'height': tf.io.FixedLenFeature([], tf.int64),\r\n        'width': tf.io.FixedLenFeature([], tf.int64),\r\n        'depth': tf.io.FixedLenFeature([], tf.int64),\r\n        'bboxes': tf.io.VarLenFeature(tf.int64),\r\n        'image_raw': tf.io.FixedLenFeature([], tf.string),\r\n    }\r\n\r\n    def parse_example(example):\r\n        data = tf.io.parse_example(example, image_feature_description)\r\n\r\n        img = tf.io.decode_jpeg(data['image_raw'])\r\n        \r\n        img = tf.image.resize(img, (416, 416))\r\n\r\n        bboxes = data['bboxes']\r\n        bboxes = tf.sparse.to_dense(bboxes)\r\n        bboxes = tf.reshape(bboxes, [-1, 5])\r\n\r\n        return img, bboxes\r\n\r\n    data = data.batch(1).map(parse_example)\r\n\r\nerrors:\r\n\r\n    Traceback (most recent call last):\r\n      File \"test_tfrecord.py\", line 28, in <module>\r\n        data = data.batch(1).map(parse_example)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 1588, in map\r\n        return MapDataset(self, map_func, preserve_cardinality=True)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3888, in __init__\r\n        use_legacy_function=use_legacy_function)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3147, in __init__\r\n        self._function = wrapper_fn._get_concrete_function_internal()\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2395, in _get_concrete_function_internal\r\n        *args, **kwargs)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n        graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n        graph_function = self._create_graph_function(args, kwargs)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n        capture_by_value=self._capture_by_value),\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n        func_outputs = python_func(*func_args, **func_kwargs)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3140, in wrapper_fn\r\n        ret = _wrapper_helper(*args)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3082, in _wrapper_helper\r\n        ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n      File \"/home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 237, in wrapper\r\n        raise e.ag_error_metadata.to_exception(e)\r\n    ValueError: in converted code:\r\n\r\n        test_tfrecord.py:16 parse_example  *\r\n            img = tf.io.decode_jpeg(data['image_raw'])\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_image_ops.py:1092 decode_jpeg\r\n            dct_method=dct_method, name=name)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\r\n            attrs=attr_protos, op_def=op_def)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\r\n            compute_device)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\r\n            op_def=op_def)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1786 __init__\r\n            control_input_ops)\r\n        /home/wilson/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1622 _create_c_op\r\n            raise ValueError(str(e))\r\n\r\n        ValueError: Shape must be rank 0 but is rank 1 for 'DecodeJpeg' (op: 'DecodeJpeg') with input shapes: [?].\r\n\r\n\r\nHow should I fix it, thanks", "@hgffly Are you still facing the issue?", "> @hgffly Are you still facing the issue?\r\n\r\nNo, I have switched to PyTorch, thanks", "@hgffly I am sorry to hear this. But the issue here is that you have to convert `data['image_raw']` from  rank 1 to rank 0 i.e., scalar as `tf.io.decode_jpeg()` only expects scalar as input. So you can use this to reshape it into a scalar\r\n`img = tf.io.decode_jpeg(tf.reshape(data['image_raw'],[]))` and everything should work as normal.\r\n\r\nAnyways, I am closing this issue now. If you face anymore issues, please reopen it. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38559\">No</a>\n", "https://stackoverflow.com/questions/61245158/failed-to-apply-vectorizing-mapping-for-tf-data-in-tensorflow-2-1-0"]}, {"number": 38558, "title": "ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install \r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ntflite_convert --saved_model_dir=/path/to/automl/efficientdet/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/saved_model/ --output_file=/path/to/automl/efficientdet/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/hulining/.local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 503, in main\r\n\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 499, in run_main\r\n\r\n    _convert_tf1_model(tflite_flags)\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 193, in _convert_tf1_model\r\n\r\n    output_data = converter.convert()\r\n\r\n  File \"/home/hulining/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 811, in convert\r\n\r\n    _get_tensor_name(tensor), shape_list))\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://github.com/google/automl/tree/master/efficientdet\r\n\r\nhttp://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz\r\n\r\nhttp://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz\r\n```\r\n\r\n**Failure details**\r\nI tried  several ways like Python API and command line and several models above to convert a .pb file to a tflite file. Both of them failed with the error above.\r\ndon't know how to slove the error.\r\n\r\n\r\n", "comments": ["@Ringhu Can you please share a standalone code to reproduce the issue? Thanks!", "Of course. @jvishnuvardhan \r\nwith the command line mode, I used:\r\n```\r\n tflite_convert --saved_model_dir=/path/to/automl/efficientdet/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/saved_model/ --output_file=/path/to/automl/efficientdet/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/\r\n```\r\nwith the python API code, the code is:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model',signature_keys=['serving_default'])\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"saved_model/converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nThe transfered model may not be same (all from the link I shared above), but the error is the same.\r\n\r\nBTW, my GPU is RTX 2080 Ti as another reference, don't know if it will affect.", "Same issue here, one more example program and stack trace for reference:\r\n\r\n<details>\r\n<summary>\r\nconvert_to_tflite.py\r\n</summary>\r\n\r\n```python\r\nimport sys\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef usage():\r\n    proggie = os.path.basename(sys.argv[0])\r\n    print(f\"\"\"\\\r\nUsage: {proggie} <input_saved_model_dir> <output>\r\n\r\nExample: {proggie} output_inference_graph_v1.pb/saved_model converted_model.tflite\"\"\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) != 3 or '-h' in sys.argv or '--help' in sys.argv:\r\n        usage()\r\n        sys.exit(1)\r\n\r\n    saved_model_dir, outfile = sys.argv[1:]\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    tflite_model = converter.convert()\r\n    with open(outfile, \"wb\") as f:\r\n        f.write(tflite_model)\r\n    print(f\"{outfile} written\")\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>output</summary>\r\n\r\n```\r\n$ docker exec tf python /tf/notebooks/training/routesv1/convert_to_tflite.py /tf/notebooks/training/routesv1/trained-inference-graphs/output_inference_graph_v1.pb/saved_model /tf/notebooks/training/routesv1/trained-inference-graphs/converted_model.tflite\r\n2020-04-16 05:41:49.534053: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-04-16 05:41:49.565995: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3301490000 Hz\r\n2020-04-16 05:41:49.567745: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x545a200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-04-16 05:41:49.567786: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\n2020-04-16 05:41:51.050535: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-04-16 05:41:51.050690: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-16 05:41:51.178923: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-04-16 05:41:51.179140: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.354ms.\r\n2020-04-16 05:41:51.179144: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\nTraceback (most recent call last):\r\n  File \"/tf/notebooks/training/routesv1/convert_to_tflite.py\", line 22, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 896, in convert\r\n    _get_tensor_name(tensor), shape_list))\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>nvidia-smi</summary>\r\n\r\n```\r\n$ nvidia-smi\r\nWed Apr 15 22:43:06 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 00000000:65:00.0  On |                  N/A |\r\n|  0%   48C    P8    14W / 166W |    477MiB /  8118MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0   1332156      G   /usr/lib/Xorg                                212MiB |\r\n|    0   1332206      G   /usr/bin/gnome-shell                         172MiB |\r\n|    0   1332729      G   alacritty                                     11MiB |\r\n|    0   1857817      G   ...gs/.local/share/Steam/ubuntu12_32/steam    20MiB |\r\n|    0   1857987      G   ./steamwebhelper                               3MiB |\r\n|    0   1858032      G   ...vice-request-channel-token=355490097884    49MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n</details>\r\n\r\n(guessing the gpu isn't used for the conversion, but there ya go)\r\n\r\n<details>\r\n<summary>pip show tensorflow</summary>\r\n\r\n```\r\n$ docker exec tf pip show tensorflow\r\nWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nName: tensorflow\r\nVersion: 1.15.2\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.6/dist-packages\r\nRequires: grpcio, opt-einsum, astor, absl-py, keras-applications, gast, keras-preprocessing, termcolor, six, tensorboard, tensorflow-estimator, numpy, protobuf, wheel, wrapt, google-pasta\r\nRequired-by:\r\n```\r\n</details>", "I download yolov3 model , then covert to keras h5 file \r\nwhen to covert h5 to tflite ,has the same error \r\n\r\n''None is only supported in the 1st dimension\" ", "I am getting the same error converting from a Keras model to tflite. I followed this tutorial\r\nhttps://stackoverflow.com/a/60136583/11575257\r\nWhen importing my saved model I printed a summary:\r\n\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, None, None, 3)]   0         \r\n_________________________________________________________________\r\nconv1_pad                        (None, None, None, 3)     0         \r\n_________________________________________________________________\r\nconv1                                (None, None, None, 32)    864      \r\n...\r\n\r\nHowever, before training if I print a summary, I get:\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \r\n_________________________________________________________________\r\nconv1_pad                       (None, 225, 225, 3)       0         \r\n_________________________________________________________________\r\nconv1                               (None, 112, 112, 32)      864       \r\n...\r\n\r\nWhich has correct output shapes.\r\n\r\nI believe that this shows that the model did not save properly and output shapes were not saved in the model.", "@Ringhu Sorry for the delay in my response. I tried converting the `mobilenet` model and was successful. Can you please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/23066f1a722fc566b437a5210d1b97b0/ssd_saved_model_tflite_conversion.ipynb) here.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks", "@bgy-proprietor I converted yolo-v3 as shown in [this](https://github.com/jvishnuvardhan/TF_Lite/blob/master/TFlite_Yolov3_conversion.ipynb) gist. Try it , if it is not working for you then please create a new issue (with a standalone code) so that it will be easy for the community to follow. Thanks", "@rgbarishian @mgalgs Can you please try `tf-nightly` and if it is not working then can you please create a separate issue with a standalone code to reproduce the issue? It will be easy for the community to follow. Thanks", "I found the error in my code. @Ringhu  may have the same error. \r\nWhen importing MobileNet, I did not give input_shape after setting include_top to False. After adding that kwarg, error no longer occurs", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @rgbarishian @mgalgs Can you please try `tf-nightly` and if it is not working then can you please create a separate issue with a standalone code to reproduce the issue? It will be easy for the community to follow. Thanks\r\n\r\nOk, just tested again and I'm not seeing this error with `tf-nightly` :+1: ", "Thanks for the confirmation. I will close this issue as this was resolved. Thanks!", "@jvishnuvardhan so it turns out that I can't use `tf-nightly` to convert a model which was created using Tensorflow 1.15 (it dumps a huge error which also appears to dump the tensors themselves, 216MB worth of logs so not sharing...).  I'm guessing you have to do the tflite conversion using the same version of TF as you use to create the model?\r\n\r\nUnfortunately, I can't generate my model using `tf-nightly` because I depend on on the [`object_detection` API](https://github.com/tensorflow/models/tree/master/research/object_detection), which isn't yet supported on Tensorflow 2...\r\n\r\nSo I'm stuck between TF 1 and 2... :sob: Any suggestions?", "> @Ringhu Sorry for the delay in my response. I tried converting the `mobilenet` model and was successful. Can you please check the [gist](https://colab.research.google.com/gist/jvishnuvardhan/23066f1a722fc566b437a5210d1b97b0/ssd_saved_model_tflite_conversion.ipynb) here.\r\n> \r\n> Please verify once and close the issue if this was resolved for you. Thanks\r\n\r\n\r\nHi @jvishnuvardhan \r\nI tried to convert the checkpoint (saved_model) using this [solution](https://colab.research.google.com/gist/jvishnuvardhan/23066f1a722fc566b437a5210d1b97b0/ssd_saved_model_tflite_conversion.ipynb#scrollTo=X5s5D5ufCcKb), it is successfully converted but I think it is incorrect since I checked the input for the model I got it as [1,1,1,3] instead of [1,300,300,3]\r\n\r\nthe saved model directory has: \r\n-\r\n![image](https://user-images.githubusercontent.com/68266028/89431979-40533200-d749-11ea-9cee-d492b567758b.png)\r\n\r\n"]}, {"number": 38557, "title": "TF 1.13 with TRT 6.0", "body": "Is it possible to modify tensorrt submodule from tensorflow 1.13 to support it with TRT 6.0? Can it support TRT 6.0?\r\nI have seen performance drop for only tensorflow models from TF 1.13 to  TF 1.15. So I want to stick to 1.13 with TRT 6.0.", "comments": ["@jvishnuvardhan @sanjoy Could you please update on this. Thanks.!", "We no longer support versions of TF below 1.15. As such, we no longer patch those versions.\r\n\r\nYou could try compiling from source, but we won't be able to provide guidance in case of compile errors either.\r\n\r\nTF 2.2 which will be released this week/early next week (RC releases are already out) is performance focused and should have significant performance gains.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38557\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38557\">No</a>\n"]}, {"number": 38556, "title": "Can not create control inputs with tf.control_dependencies ", "body": "I was expecting to get some control inputs with `tf.control_dependencies()`, but the following test code did not give back what I expected.\r\n    \r\n    import tensorflow as tf\r\n\r\n    a = tf.get_variable('a', shape = [2, 3])\r\n    b = tf.get_variable('b', shape = [2, 3])\r\n    c = tf.scalar_mul(2, a)\r\n    d = tf.scalar_mul(3, b)\r\n\r\n    with tf.control_dependencies([d, c]):\r\n      f = d-c\r\n\r\n    print (f.op.control_inputs)\r\n\r\nIt returned `[]`. If I did the other way\r\n\r\n    f = d-c\r\n    f.op._add_control_inputs([c.op, d.op])\r\n    print (f.op.control_inputs)\r\n\r\nI got `[<tf.Operation 'Mul' type=Mul>, <tf.Operation 'Mul_1' type=Mul>]`, which seems good. \r\n\r\nSo does `tf.control_dependencies()` create control dependencies? Or does `op.control_inputs` reflect all the control inputs? Could this be a bug?", "comments": ["Was able to reproduce the issue with Tf1.15.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/481a920d74fafb5e1ed971e1dfd34231/untitled510.ipynb#scrollTo=0NrsY94pUuqc) and [gist](https://colab.sandbox.google.com/gist/gadagashwini/b590bda50bd92e63b3e28380c9b98718/untitled511.ipynb) without `tf.control_dependencies`. Thanks", "You have to use `tf.Variable` function instead of `tf.get_Variable`. I made a toy example to show `tf.control_dependecies` behavior. \r\n```python\r\nimport tensorflow as tf\r\ntf.reset_default_graph()\r\na = tf.Variable(name='a', initial_value=2)\r\nb = tf.Variable(name='b', initial_value=1) \r\nc = tf.multiply(2, a)\r\nd = tf.multiply(3, b)\r\n\r\nwith tf.control_dependencies([c,d]):\r\n  f = tf.subtract(a,b)\r\n\r\nprint(f.op.control_inputs)\r\n```\r\noutput:\r\n```python\r\n[<tf.Operation 'Mul' type=Mul>, <tf.Operation 'Mul_1' type=Mul>]\r\n```", "@ymodak Thanks, it solved my problem. I know there are some differences between `tf.get_variable()` and `tf.Variable()` about variable sharing, but should control dependencies be one of them?", "@ymodak I just found another problem with `control_flow_ops.with_dependencies`. \r\n\r\n    import tensorflow as tf\r\n    from tensorflow.python.ops import control_flow_ops\r\n    tf.reset_default_graph()\r\n    a = tf.constant([2,3], dtype = tf.float32)\r\n    b = tf.constant([4,5], dtype = tf.float32)\r\n    c = tf.multiply(2., a)\r\n    d = tf.multiply(3., b)\r\n\r\n    f = tf.subtract(a, b)\r\n    control_flow_ops.with_dependencies([c, d], f)\r\n\r\n    print(f.op.control_inputs)\r\n\r\nIt returned `[]`.", "`control_flow_ops.with_dependencies` is deprecated long time ago. We recommend using  [`tf.control_dependencies`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Graph#control_dependencies) with the `with` keyword which does the same things. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38556\">No</a>\n"]}, {"number": 38555, "title": "[tf.function] tf.Variable converted to tf.Tensor automatically in second loop", "body": "\r\n**System information** \r\n- Have I written custom code :  Yes\r\n- OS Platform and Distribution (e.g.,Linux Ubuntu 16.04):   MacOS Mojave 10.14.5\r\n- TensorFlow installed from (source or binary): - TensorFlow version (use command below):  pip\r\n- Python version: - 3.6.5\r\n\r\n**Describe the current behavior**\r\ntf.Variable converted to tf.Tensor automatically after second loop in function decorated by tf.function.\r\n\r\n**Describe the expected behavior**\r\nIt should not convert automatically.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```python\r\n@tf.function\r\ndef foo(a):\r\n    print(a)\r\n    for i in range(10):\r\n        if a[0] > 3:\r\n            print(f'True: a_{i}: {a}')\r\n            a = a[0].assign(1)\r\n        else:\r\n            print(f'False: a_{i}: {a}')\r\n            a = a[0].assign(2)\r\na = tf.Variable(np.array([1,2,3]))\r\nfoo(a)\r\n```\r\n\r\n**Other info / logs** \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-25-9c589be6e6c6> in <module>\r\n----> 1 foo(a)\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    613       # This is the first call of __call__, so we have to initialize.\r\n    614       initializers = []\r\n--> 615       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    616     finally:\r\n    617       # At this point we know that the initialization is complete (or less\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    495     self._concrete_stateful_fn = (\r\n    496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 497             *args, **kwds))\r\n    498 \r\n    499     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2387       args, kwargs = None, None\r\n   2388     with self._lock:\r\n-> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2390     return graph_function\r\n   2391 \r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    437         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    438         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    441 \r\n\r\n~/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in converted code:\r\n\r\n    <ipython-input-23-3a2d4adc4a09>:7 foo  *\r\n        a = a[0].assign(1)\r\n    /Users/mac/.pyenv/versions/3.6.5/envs/tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1074 assign\r\n        raise ValueError(\"Sliced assignment is only supported for variables\")\r\n\r\n    ValueError: Sliced assignment is only supported for variables\r\n```", "comments": ["@AngelPone \r\nplease share the tensorflow version where the error is faced", "> @AngelPone\r\n> please share the tensorflow version where the error is faced\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.__version__\r\nOut[3]: '2.1.0'\r\n```", "@AngelPone I tried your issue in tf2.0\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function\r\ndef foo(a):\r\n    print(a)\r\n    for i in range(10):\r\n    \tprint(\"----\")\r\n    \t#print(tf.type(a))\r\n    \tif a[0] > 3:\r\n    \t\tprint(f'True: a_{i}: {a}')\r\n    \t\ta = a[0].assign(1)\r\n    \t\tprint(\"here in first condition\")\r\n    \telse:\r\n    \t\tprint(f'False: a_{i}: {a}')\r\n    \t\ta = a[0].assign(2)\r\n    \t\tprint(\"here in second condition\")\r\na = tf.Variable(np.array([1,2,3]))\r\nfoo(a)\r\n```\r\n\r\nTensorflow is unable to recognize condition since it written in python and it will just ignore that line and will execute both if and else block , you can see that both print statements will be printed. Also when next `a[0].assign(2)` happens it  no longer remains a variable \r\n\r\nThe right way to do it is to use tf.condition https://www.tensorflow.org/api_docs/python/tf/cond and use TF API ", "@Saduf2019  Yeah, it works after I used tf.cond, but will it(recognize if-else) be fixed  in the future ? ", "@AngelPone  This lets you write platform-independent code, but the caveat is that it's a (small) subset of Python.  Hopefully in future .", "@Saduf2019  is there any way i can be assigned few tickets ? ", "@17patelumang Thanks for your help, let me close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38555\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38555\">No</a>\n"]}, {"number": 38554, "title": "[XLA] Use EmitWriteArrayElement that add LLVM annotation. ", "body": "This could help LLVM to vectorize.\r\n\r\nSee one commit in https://github.com/tensorflow/tensorflow/pull/38136\r\n\r\n@cheshire \r\nI looked at all \"Store(\" and \"CreateStore(\" under xla/service/gpu to fix all missed annotation cases.", "comments": ["Some tests are failing. But most of them also fail upstream... So I can't do any debuging.\r\n\r\nIs the Linux build passing upstream?", "As I can't debut normally, I remove one of the change. The CI should be able to test it.", "Sorry for the delay. It seems our tooling pulled the changes into the previous changelist, but we had already removed ourselves as reviewer from that changelist because that one was not ready to land. Therefore we did not get another notification that this change is ready to be approved.", "Changes have been merged into master by commit 79504b1. So closing the PR."]}, {"number": 38553, "title": "[INTEL MKL]  Enabling tf_cuda_cc_test to execute in CPU only builds also.", "body": "Currently, tests using tf_cuda_cc_test do not run in CPU only builds (ex. when using MKL), unless the test is invoked manually with full test name (i.e //tensorflow/core/grappler/optimizers:remapper_test) \r\n\r\nChanged tf_gpu_cc_test to always execute the test in CPU only builds also.\r\nThis change exposed a bug that caused two tests -  CAPI.LibraryLoadFunctions and TestApiDef.TestCreateApiDef to fail in //tensorflow/c:c_api_test. Fix for the test is also included in this PR.", "comments": ["Oops. Just realized this is not a grappler PR. Un-requesting review from @ezhulenev then. Sorry!", "It is intentional to have tf_gpu_cc_test to only run on GPUs.\r\nIs there any tests you see that should run on both cpu and GPU?\r\nWe should change them instead.", "> It is intentional to have tf_gpu_cc_test to only run on GPUs.\r\n> Is there any tests you see that should run on both cpu and GPU?\r\n> We should change them instead.\r\n\r\n@gunan  thanks.  //tensorflow/core/grappler/optimizers:remapper_test is an example of a test the needs to run on CPU and GPU.  I had a PR (now closed)  https://github.com/tensorflow/tensorflow/pull/38464 @ezhulenev  pointed out that changing the call will skip GPU tests.\r\n\r\nFrom the code for tf_gpu_cc_test we see that it executes on CPU and GPU, but CPU part gets triggered only when the full name //tensorflow/core/grappler/optimizers:remapper_test is given and is not invoked when we use  wild cards //tensorflow/core/grappler/..  or //tensorflow/core/..  because of the manual tag.\r\nAFAIK  a different call  tf_cuda_only_cc_test  has to be used to run test on GPUs only.", "argh, you are right.\r\nIn that case, we may accept this change. However ,I expect the migration of this change a painful, and particularly long one. We will need to test this globally internally, which may take up to a week.\r\n@penpornk do you know who may be able to import this change manually, with a global presubmit?", "@gunan I can do that. :)", "@penpornk ran the tests, and all looks in order.\r\nWe can merge this PR.", "> @penpornk ran the tests, and all looks in order.\r\n> We can merge this PR.\r\n\r\nThanks @gunan  and @penpornk "]}, {"number": 38552, "title": "Failed to load the Native tensorflow routine", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS \r\n- TensorFlow installed from (source or binary):  https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.5.0-py3-none-any.whl\r\n- TensorFlow version: Tensorflow 1.5.0\r\n- Python version: Python 2.7.12\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried running : \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.reduce_sum(tf.random.normal([1000, 1000])))\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nThe following is the error I get.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"wtf.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/sarva/.local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\n**additional info**\r\n Can someone please guide on how to properly install tensorflow for my system. I am running ubuntu in VirtualBox on Windows 10 if that helps.\r\n\r\nOn typing uname -a: the system info returned is Linux ubuntu 4.4.0-177-generic #207-Ubuntu SMP Mon Mar 16 01:15:50 UTC 2020 i686 i686 i686 GNU/Linux\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "@Sarvagya2009 \r\nJust to verify did you follow the instructions in [tensorflow website](https://www.tensorflow.org/install/source?hl=en) . Please, see tested build configurations from [here.](https://www.tensorflow.org/install/source?hl=en#tested_build_configurations). Tensorlow 1.5.0 is very older version .Request you to go for latest versions of Tensorflow for better support.Thanks!", "@ravikyram \r\nOn following the website's instructions and installing bazelisk, I get the following error:\r\n**Exception: Unsupported machine architecture \"i686\". Bazel currently only supports x86_64.**\r\n\r\nSo bazel is not compatible with my architecture. Any other method of installing tensorflow?", "Just to verify, did you use TensorFlow 1.5.0? That is very old and not supported\r\n\r\nDo you need to use that version? Or is switching to 1.15 or later possible?", "@mihaimaruseac \r\nI changed it to a later version but still same thing and on trying to build from source and installing bazel, I got the aforementioned error. ", "On trying on a different machine with x86_64, tensor-flow was installed successfully. So I think this problem was because of i686 architecture.", "@Sarvagya2009 \r\n\r\nGlad to know it worked.Please close this thread if it solves your question. Thanks!", "Yes, we only build for 64 bits architectures. Our code is too complex and building also for 32 bits will require at least 2-3 years of work (since we have so many numerics code assuming 64bits architectures which will result in a lot of security issues due to overflows if not handled properly). Since most of the processors in use are now on 64 bits, we are not prioritizing the work to support 32 bits (see also #32315).\r\n\r\nYou can use codelab if you don't have access to a 64-bits CPU.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38552\">No</a>\n"]}, {"number": 38551, "title": "Linking of tensorflowlite_c.so throws multiple undefined reference errors", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): GNU/Linux aarch64\r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from\r\nsource): gcc version 7.4.0 (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1) \r\n\r\n**Describe the current behavior**\r\nBuilt the extended TF lite runtime with enable tf ops (complete process found in issue #38077). Compilation of the minimal.cc example succeeds, but the linker throws mulitple errors afterwards.\r\n\r\n**Describe the expected behavior**\r\nBuild should complete, as the shared object is found.\r\n\r\n**Standalone code to reproduce the issue** \r\nmake executes the commands:\r\n```\r\ng++ -Iinclude -I/workspace/tensorflow -I/workspace/abseil-cpp -Wall  -c src/minimal.c -o obj/minimal.o\r\ng++ -Llib obj/minimal.o -ltensorflowlite_c -lflatbuffers -o test\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nobj/minimal.o: In function `main':\r\nminimal.c:(.text+0x6c): undefined reference to `tflite::DefaultErrorReporter()'\r\nminimal.c:(.text+0x80): undefined reference to `tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'\r\nminimal.c:(.text+0xe0): undefined reference to `tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\nminimal.c:(.text+0x100): undefined reference to `tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\nminimal.c:(.text+0x110): undefined reference to `tflite::impl::InterpreterBuilder::operator()(std::unique_ptr<tflite::impl::Interpreter, std::default_delete<tflite::impl::Interpreter> >*)'\r\nminimal.c:(.text+0x174): undefined reference to `tflite::impl::Interpreter::AllocateTensors()'\r\nminimal.c:(.text+0x1d8): undefined reference to `tflite::PrintInterpreterState(tflite::impl::Interpreter*)'\r\nminimal.c:(.text+0x1e4): undefined reference to `tflite::impl::Interpreter::Invoke()'\r\nminimal.c:(.text+0x248): undefined reference to `tflite::PrintInterpreterState(tflite::impl::Interpreter*)'\r\nminimal.c:(.text+0x25c): undefined reference to `tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\nminimal.c:(.text+0x2a4): undefined reference to `tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\nobj/minimal.o: In function `std::default_delete<tflite::FlatBufferModel>::operator()(tflite::FlatBufferModel*) const':\r\nminimal.c:(.text._ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_[_ZNKSt14default_deleteIN6tflite15FlatBufferModelEEclEPS1_]+0x24): undefined reference to `tflite::FlatBufferModel::~FlatBufferModel()'\r\nobj/minimal.o: In function `std::default_delete<tflite::impl::Interpreter>::operator()(tflite::impl::Interpreter*) const':\r\nminimal.c:(.text._ZNKSt14default_deleteIN6tflite4impl11InterpreterEEclEPS2_[_ZNKSt14default_deleteIN6tflite4impl11InterpreterEEclEPS2_]+0x24): undefined reference to `tflite::impl::Interpreter::~Interpreter()'\r\nobj/minimal.o:(.data.rel.ro._ZTVN6tflite3ops7builtin17BuiltinOpResolverE[_ZTVN6tflite3ops7builtin17BuiltinOpResolverE]+0x10): undefined reference to `tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'\r\nobj/minimal.o:(.data.rel.ro._ZTVN6tflite3ops7builtin17BuiltinOpResolverE[_ZTVN6tflite3ops7builtin17BuiltinOpResolverE]+0x18): undefined reference to `tflite::MutableOpResolver::FindOp(char const*, int) const'\r\nobj/minimal.o:(.data.rel.ro._ZTIN6tflite3ops7builtin17BuiltinOpResolverE[_ZTIN6tflite3ops7builtin17BuiltinOpResolverE]+0x10): undefined reference to `typeinfo for tflite::MutableOpResolver'\r\nobj/minimal.o: In function `tflite::MutableOpResolver::~MutableOpResolver()':\r\nminimal.c:(.text._ZN6tflite17MutableOpResolverD2Ev[_ZN6tflite17MutableOpResolverD5Ev]+0xc): undefined reference to `vtable for tflite::MutableOpResolver'\r\nminimal.c:(.text._ZN6tflite17MutableOpResolverD2Ev[_ZN6tflite17MutableOpResolverD5Ev]+0x10): undefined reference to `vtable for tflite::MutableOpResolver'\r\ncollect2: error: ld returned 1 exit status\r\nmakefile:20: recipe for target 'test' failed\r\nmake: *** [test] Error 1\r\n```\r\n", "comments": ["I have tried to fix this issue with the suggestions of #36661, but unfortunately it did NOT link correctly. What confuses me is that the build command there uses an additional tensorflow binary which I do not have:\r\n```\r\ng++ -I../tensorflow -Llib -ltensorflow_cc -Llib -ltensorflowlite test.cpp -o exec\r\n```\r\n`libtensorflow_cc` AND `libtensorflowlite` seem to be to be required. When I build the tensorflow lite binary, I do not get these. Instead, I only find the following files in the directory `/tensorflow/bazel-bin/tensorflow/lite/c/` :\r\n```\r\nlibtensorflowlite_c.so\r\nlibtensorflowlite_c.so-2.params\r\n```\r\nCan somebody explain which approach is correct? @ymodak ", "You can build libtensorflowlite.so with the following target\r\n```\r\n//tensorflow/lite:libtensorflowlite.so\r\n```", "I built libtensorflowlite as follows and am seeing the same issue using CMake\r\n\r\ncd to tensorflow folder\r\n ./configure\r\nbazel build //tensorflow/lite:libtensorflowlite.so", "@terryheo \r\nThank you for your response. The addition of libtensorflow.so removes the linker errors. BUT: even though I have compiled the libtensorflowlite_c.so (notice the \"_c\") with the flex delegate, the operations that need that flex delegate are not supported. I get an error upon loading my model:\r\n\r\n```\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 3 (FlexRandomStandardNormal) failed to prepare.\r\n\r\nError at src/minimal.cc:92\r\n```\r\nFurther inspection shows that the needed symbols are not provided by libtensorflowlite_c.so. The output of `nm -D` shows me this:\r\n\r\n```\r\n000000000529d2f0 T TfLiteDelegateCreate\r\n000000000529cee8 T TfLiteFloatArrayCreate\r\n000000000529cf18 T TfLiteFloatArrayFree\r\n000000000529ced8 T TfLiteFloatArrayGetSizeInBytes\r\n000000000529ce60 T TfLiteIntArrayCopy\r\n000000000529ce30 T TfLiteIntArrayCreate\r\n000000000529cdf0 T TfLiteIntArrayEqual\r\n000000000529cd70 T TfLiteIntArrayEqualsArray\r\n000000000529ced0 T TfLiteIntArrayFree\r\n000000000529cd60 T TfLiteIntArrayGetSizeInBytes\r\n0000000000730780 T TfLiteInterpreterAllocateTensors\r\n0000000000730220 T TfLiteInterpreterCreate\r\n0000000000730548 T TfLiteInterpreterDelete\r\n0000000000730668 T TfLiteInterpreterGetInputTensor\r\n0000000000730648 T TfLiteInterpreterGetInputTensorCount\r\n00000000007307b0 T TfLiteInterpreterGetOutputTensor\r\n0000000000730790 T TfLiteInterpreterGetOutputTensorCount\r\n0000000000730788 T TfLiteInterpreterInvoke\r\n000000000072fb58 T TfLiteInterpreterOptionsAddBuiltinOp\r\n000000000072fb60 T TfLiteInterpreterOptionsAddCustomOp\r\n00000000007300e8 T TfLiteInterpreterOptionsAddDelegate\r\n000000000072ff50 T TfLiteInterpreterOptionsCreate\r\n000000000072ffd0 T TfLiteInterpreterOptionsDelete\r\n0000000000730218 T TfLiteInterpreterOptionsSetErrorReporter\r\n00000000007300e0 T TfLiteInterpreterOptionsSetNumThreads\r\n000000000072fb68 T TfLiteInterpreterOptionsSetUseNNAPI\r\n000000000072fb50 T TfLiteInterpreterResetVariableTensors\r\n00000000007306a8 T TfLiteInterpreterResizeInputTensor\r\n000000000072fc28 T TfLiteModelCreate\r\n000000000072fd58 T TfLiteModelCreateFromFile\r\n000000000072fe80 T TfLiteModelDelete\r\n000000000529cf68 T TfLiteQuantizationFree\r\n000000000529cfe0 T TfLiteSparsityFree\r\n0000000000730818 T TfLiteTensorByteSize\r\n0000000000730838 T TfLiteTensorCopyFromBuffer\r\n0000000000730868 T TfLiteTensorCopyToBuffer\r\n0000000000730820 T TfLiteTensorData\r\n000000000529cf20 T TfLiteTensorDataFree\r\n0000000000730808 T TfLiteTensorDim\r\n000000000529d0a0 T TfLiteTensorFree\r\n0000000000730828 T TfLiteTensorName\r\n00000000007307f8 T TfLiteTensorNumDims\r\n0000000000730830 T TfLiteTensorQuantizationParams\r\n000000000529d188 T TfLiteTensorRealloc\r\n000000000529d0f8 T TfLiteTensorReset\r\n00000000007307f0 T TfLiteTensorType\r\n000000000529d200 T TfLiteTypeGetName\r\n000000000072fc18 T TfLiteVersion\r\n```\r\nThat means that either the build of the c variant is somewhere corrupted and does not provide the necessary functions OR the two shared objects do need to exist in parallel and need some special compile options.\r\n\r\nEither way, it is very unclear where to go from here. Can somebody clarify how these two libs interact and which lib has to be built with which bazel build option?", "@DocDriven what's you target device?\r\nI'm not sure if the current TFLite supports Flex for non Android system.\r\n\r\n@abattery Could you confirm this?", "I think we do not have a shared object build target with flex right now like libtensorflowlite.so.", "@terryheo \r\nThanks for your efforts, I was able to resolve the issue. After a lot of searching, I found that the documentation is within in the `/tensorflow/lite/c/c_api.h` header. The minimal example is written in C++ and thus all functions have to be replaced. My confusion was due to the fact that C and C++ code was mixed up.\r\n\r\nIt might be worth documenting the C API outside of said header.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38551\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38551\">No</a>\n"]}, {"number": 38550, "title": "Keras Model in SavedModel format Errors on Loading -ValueError('Model inputs are already set.')", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): ProductName:\tMac OS X, ProductVersion:\t10.15.2, BuildVersion:\t19C57\r\nTensorFlow installed from (source or binary): pip\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.6.8\r\nCUDA/cuDNN version: None\r\nGPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nWhen trying to load one of my SavedModel format models (saved using 1.15.0) using` tf.keras.models.load_model `an error is thrown at the following location:\r\n\r\n```\r\n  File \"/Users/saurabh/.pyenv/versions/emotion-python/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2671, in _set_input_attrs\r\n    raise ValueError('Model inputs are already set.')\r\n```\r\n**I can successfully load and run this model using TensorFlow versions 2.0.0, 1.15.0 and 1.14.0.**\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nCan successfully load a model from a SMB(SavedModel format) file.\r\n\r\n**Code to reproduce the issue:**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel_smb = tf.keras.models.load_model('smbnew', compile=False)\r\n```\r\n\r\n**Other info / logs:**\r\n\r\n**_I am also attaching a dummy SavedModel model below which can be used to test.**\r\n\r\n\r\n\r\nComplete Stacktrace of the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 9, in <module>\r\n    model_smb = tf.keras.models.load_model('smbnew', compile=False)\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 150, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 89, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 552, in load_internal\r\n    export_dir)\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 119, in __init__\r\n    self._finalize()\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 165, in _finalize\r\n    node._set_inputs(inputs)\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2647, in _set_inputs\r\n    inputs = self._set_input_attrs(inputs)\r\n  File \"python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 2671, in _set_input_attrs\r\n    raise ValueError('Model inputs are already set.')\r\nValueError: Model inputs are already set.\r\n\r\n```\r\nWhen loaded with tf.keras in v2.0.0 the layers, model config, inputs, outputs, summary etc. are all parsed correctly, as well as being able to run data through the model.", "comments": ["[smbnew.zip](https://github.com/tensorflow/tensorflow/files/4478031/smbnew.zip)\r\n", "Was able to replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/f90b3abd21f828db1212f4dab2a1c5d7/untitled512.ipynb). Thanks!", "@tripathysa I think this was resolved in recent `tf-nightly`. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/75bb1d6f47dace0e45c272dfb5428ff0/untitled512.ipynb). If you want to use a stable version, `TF2.2` will be released in near future and this update will reflect in `TF2.2`. \r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks! ", "@jvishnuvardhan : Will it be patched in 2.1 as well?", "@tripathysa I don't think so. But I guess `TF2.2` may be released soon as the last release candidate (rc3) was a week back. Thanks!", "I am closing the issue as this was resolved. I think stable `TF2.2` will be released in near future. Please feel free to reopen if it persists again later. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38550\">No</a>\n"]}, {"number": 38549, "title": "Fix issue in tf.image.extract_glimpse", "body": "This PR is to re-apply PR #12829. While #12829 was merged before, it was reverted at one point. Not sue about the reason though my guess is that there are some internal testing that caused the revert. This PR will try to submit again, and will fix any internal tests that fails.\r\n\r\nThis PR fixes #38545.\r\n\r\nThis fix tries to fix the issue raised in #2134 where\r\n`tf.image.extract_glimpse` does not work as expected\r\nwhen `centered=False` and `normalized=False`\r\n\r\nThis fix fixes #2134.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["For tensorflow/api-owners\r\n\r\nIs it the case that these examples/tests were just wrong before? We are concerned that these changes are not backwards compatible if even our examples have to change. Since this is a significant change in behavior, we should have a v2 version of the op rather than a change to the existing op.\r\n\r\nThat said, it's unclear based on https://github.com/sb2nov/tensorflow/commit/63b599bcd5443366e0f6c65bc6a349d3da25c5a4 whether this is universally considered a bug versus expected behavior. Maybe what we need here is a clarification of behavior in docstrings? ", "Thanks @karmel @tensorflow/api-owners  for the comment. The previous behavior of `tf.image.extract_glimpse` was incorrectly implemented in two issues:\r\n1) both `centered=True` and `centered=False` will behave like `centered=False` (sort of, see combination effect of 2)\r\n2) there is also another issue where the `centered=False` will take positive value incorrectly.\r\n\r\nBelow is an example:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nimg = tf.constant(np.arange(25).reshape((1, 5, 5, 1)), dtype=tf.float32)\r\n\r\n# Image:\r\n# [  0.   1.   2.   3.   4.]\r\n# [  5.   6.   7.   8.   9.]\r\n# [ 10.  11.  12.  13.  14.]\r\n# [ 15.  16.  17.  18.  19.]\r\n# [ 20.  21.  22.  23.  24.]\r\n\r\ntf.image.extract_glimpse(\r\n    img, [3, 3], [[-2, 2]], centered=False, normalized=False, noise='zero')\r\n\r\n# [ 0.  0.  0.]\r\n# [ 0.  0.  0.]\r\n# [ 0.  0.  0.]\r\n```\r\n\r\nIn the above example, since `size = [3, 3]`, `offset = [[-2, 2]]`, and `centered=False`, the obtained region should take upper-left corner as `[0, 0]`, thus the correct one should be:\r\n```\r\n# [ 0.  0.  0.]\r\n# [ 0.  0.  0.]\r\n# [ 2.  3.  4.]\r\n```\r\n\r\nGiven this implementation had been in place for a really long time. I don't know the best way to address it without breaking the API. There might be several considerations:\r\n1) We could consider this as a bug that must be fixed, so change the underlying implementation directly.\r\n2) We could implement the correct one in C++ with a new kernel (`ExtractGlipmpseV2`), and leave the old C++ kernel alone. In this way, when people use python API, they will be rerouted to the new kernel, however, for saved models poeple will get the old result.\r\n3) We could implement the correct one with a new python API `extract_glimpse_v2`, and co-exist with both `extract_glimpse` and `extract_glimpse_v2`. It might cause some confusion though, as user will see two exactly APIs with different behavior.", "I prefer option (3), coupled with a deprecation of tf.image.extract_glimpse\nwith instructions in the message for how to get the \"broken\" results using\nextract_glimpse_v2.\n\nAny other option can change the properties of code which existing works now\nand depends on the buggy behavior, and because TF promises backwards\ncompatibility of its python API and graphdef we cannot do that.\n\nOn Tue, Apr 21, 2020 at 8:15 AM Yong Tang <notifications@github.com> wrote:\n\n> Thanks @karmel <https://github.com/karmel> @tensorflow/api-owners\n> <https://github.com/orgs/tensorflow/teams/api-owners> for the comment.\n> The previous behavior of tf.image.extract_glimpse was incorrectly\n> implemented in two issues:\n>\n>    1. both centered=True and centered=False will behave like\n>    centered=False (sort of, see combination effect of 2)\n>    2. there is also another issue where the centered=False will take\n>    positive value incorrectly.\n>\n> Below is an example:\n>\n> import tensorflow as tf\n> import numpy as np\n>\n> img = tf.constant(np.arange(25).reshape((1, 5, 5, 1)), dtype=tf.float32)\n>\n> # Image:\n> # [  0.   1.   2.   3.   4.]\n> # [  5.   6.   7.   8.   9.]\n> # [ 10.  11.  12.  13.  14.]\n> # [ 15.  16.  17.  18.  19.]\n> # [ 20.  21.  22.  23.  24.]\n>\n> tf.image.extract_glimpse(\n>     img, [3, 3], [[-2, 2]], centered=False, normalized=False, noise='zero')\n>\n> # [ 0.  0.  0.]\n> # [ 0.  0.  0.]\n> # [ 0.  0.  0.]\n>\n> In the above example, since size = [3, 3], offset = [[-2, 2]], and\n> centered=False, the obtained region should take upper-left corner as [0,\n> 0], thus the correct one should be:\n>\n> # [ 0.  0.  0.]\n> # [ 0.  0.  0.]\n> # [ 2.  3.  4.]\n>\n> Given this implementation had been in place for a really long time. I\n> don't know the best way to address it without breaking the API. There might\n> be several considerations:\n>\n>    1. We could consider this as a bug that must be fixed, so change the\n>    underlying implementation directly.\n>    2. We could implement the correct one in C++ with a new kernel (\n>    ExtractGlipmpseV2), and leave the old C++ kernel alone. In this way,\n>    when people use python API, they will be rerouted to the new kernel,\n>    however, for saved models poeple will get the old result.\n>    3. We could implement the correct one with a new python API\n>    extract_glimpse_v2, and co-exist with both extract_glimpse and\n>    extract_glimpse_v2. It might cause some confusion though, as user will\n>    see two exactly APIs with different behavior.\n>\n> \u2014\n> You are receiving this because you are on a team that was mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/38549#issuecomment-617244215>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLEBMICJOFHBTLZ5J3RNW2A7ANCNFSM4MICQYMA>\n> .\n>\n\n\n-- \n - Alex\n", "I think we should be a little more aggressive here. This is clearly a bug,\nand it is also inconsistent with the documentation. We don't have to\nslavishly preserve the broken behavior. I think therefore, option 2 is\nbest. This preserves the behavior of existing graphs, but we can fix the\nbehavior that is generated for new programs. Because this is a behavior\nchange, we have to make sure to note this in the relnotes.\n", "Thanks all for the feedback! I have updated the PR with the following changes:\r\n- A new C++ kernel `ExtractGlimpseV2` coexist with old `ExtractGlimpse` kernel.\r\n- `tf.compat.v1.image.extract_glimpse` is routed to `ExtractGlimpse`\r\n- `tf.compat.v2.image.extract_glimpse` is routed to `ExtractGlimpseV2`\r\n- Existing tests are not modified but routed to old `ExtractGlimpse` (so that it makes sure it does not break).\r\n- Added new tests that is different from old tests (routed to new `ExtractGlimpseV2`, cover the new behavior)\r\n\r\nPlease take a look.", "Thank you! Can you add a bullet to relnotes explaining this (in the breaking changes section for the next release?", "Thanks @martinwicke! The PR has been updated with the following added in RELEASE.md:\r\n\r\n Breaking Changes\r\n* `tf.image.extract_glimpse` has been updated to correctly process the case where  `centered=False` and `normalized=False`. This is a breaking change as the output is different from (incorrect) previous versions. Note this breaking change only impacts `tf.image.extract_glimpse` and `tf.compat.v2.image.extract_glimpse` API endpoints. The behavior of `tf.compat.v1.image.extract_glimpse` does not change. The behavior of exsiting C++ kernel `ExtractGlimpse` does not change as well, so saved models will not be impacted.\r\n", "Sorry, you do change the endpoint, but the docs are inconsistent.", "Thanks @martinwicke. That was caused by doc example of v1 also use `tf.image.extract_glimpse`  in docstring. So the example has to be updated in order to pass the `doctest` check.\r\n\r\nI think we can change the example  in v1 doc to `tf.compat.v1.image.extract_glimpse`, then the example will pass the `doctest` without behavior change.\r\n\r\nLet me update the PR shortly.", "@martinwicke The PR has been updated. The doc example in v1 now maintain the old behavior, though the API usage changed to `tf.compat.v1.image.extract_glimpse` so that python doctest could pass. Please take a look.", "@yongtang Can you please resolve conflicts? Thanks!", "@gbaned  The PR has been rebased and pushed. Please take a look.", "@yongtang  Can you please resolve conflicts? Thanks!"]}, {"number": 38548, "title": "Beam search for Transformer example", "body": "Hi! I have been following the Transformer tutorial here https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb and I am wondering how can beam search be implemented to work with the current code? Thanks!", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 38547, "title": "TimeDistributed fails with GlobalAveragePooling1D layer. ", "body": "I am trying to use Bert to encode chunks of text. I get the 3D output from Bert, and trying to apply GlobalAveragePooling1D. I get list index out of range. Problem seems to be when applying timedistributed layer to globalaveragepooling1d layer. See below.\r\n\r\n`   def bert_model(self, max_seq_len, number_of_labels=130, adapter_size=64, bert_config_file=None, bert_ckpt_file=None, bert_model_name=None):\r\n\r\n        with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\r\n            bc = StockBertConfig.from_json_string(reader.read())\r\n            bert_params = map_stock_config_to_params(bc)\r\n            bert_params.adapter_size = adapter_size\r\n            bert = BertModelLayer.from_params(bert_params, name=\"bert\")\r\n\r\n        sentence_input = Input(shape=(max_seq_len,), dtype='float32', name=\"sentence_input_ids\")\r\n        subtitles_input = Input(shape=(self.max_shape[1], max_seq_len), dtype='int32', name=\"SubtitlesInput\")\r\n\r\n        bert_output = bert(sentence_input)\r\n        bert_output = GlobalAveragePooling1D()(bert_output)\r\n\r\n        self.bert_sentence_model = Model(sentence_input, bert_output)\r\n\r\n        segment_time_distributed = TimeDistributed(self.bert_sentence_model, name=\"TimeDistributedSegment\")\r\n        segment_cnn = Conv1D(256, 2, padding=\"same\", strides=1, activation=\"relu\", name=\"Segment2Conv1D\")\r\n        segment_max_pool_2 = MaxPooling1D(pool_size=3, name=\"Segment2MaxPool1D\")\r\n\r\n        subtitles_timedistributed = segment_time_distributed(subtitles_input)\r\n        subtitles_cnn = segment_cnn(subtitles_timedistributed)\r\n        subtitles_maxpool = segment_max_pool_2(subtitles_cnn)\r\n\r\n        subtitles_dropout = SpatialDropout1D(0.30, name=\"SubtitlesDropout\")(subtitles_maxpool)\r\n        subtitles_pre_attention_output = Dense(256, name=\"SubtitlesPreAttnOutput\")(subtitles_dropout)\r\n\r\n        attention_subtitles = Attention(name=\"SubtitlesAttention\")([subtitles_pre_attention_output, subtitles_maxpool])\r\n\r\n        subtitles_max_output = GlobalMaxPool1D(name=\"GlobalMaxPoolSubitles\")(attention_subtitles)\r\n        subtitles_avg_output = GlobalAveragePooling1D(name=\"GlobalAvgPoolSubitles\")(attention_subtitles)\r\n\r\n        concat_output = Concatenate(axis=-1, name=\"OutputConcatenate\")([subtitles_max_output, subtitles_avg_output])\r\n        dropput = Dropout(0.40)(concat_output)\r\n        output = Dense(number_of_labels, activation=\"sigmoid\", name=\"Output\")(dropput)\r\n\r\n        # model = keras.Model(inputs=[sentence_input_ids, token_type_ids], outputs=logits)\r\n        # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\r\n        self.model = Model(inputs=subtitles_input, outputs=output)\r\n        #self.model.build(input_shape=(None, max_seq_len))\r\n\r\n        # load the pre-trained model weights\r\n        load_stock_weights(bert, bert_ckpt_file)\r\n\r\n        # freeze weights if adapter-BERT is used\r\n        if adapter_size is not None:\r\n            self.freeze_bert_layers(bert)\r\n\r\n        self.model.compile(optimizer=\"adam\",\r\n                      loss=\"binary_crossentropy\")\r\n\r\n        self.bert_sentence_model.summary()\r\n        self.model.summary()`\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Development/Projects/GenrePrediction/GenrePredictionModel.py\", line 765, in <module>\r\n    validation_labels=genre_prediction.validation_labels, epochs=1000, batch_size=1)\r\n  File \"C:/Development/Projects/GenrePrediction/GenrePredictionModel.py\", line 468, in fit_bert\r\n    self.sentence_model, self.model = self.bert_model(max_sentence_length, bert_ckpt_file=\"D:/Development/Projects/bert_models/\"+self.bert_model_name+\"/bert_model.ckpt\", bert_config_file=\"D:/Development/Projects/bert_models/\"+self.bert_model_name+\"/bert_config.json\", number_of_labels=len(self.genres))\r\n  **File \"C:/Development/Projects/GenrePrediction/GenrePredictionModel.py\", line 336, in bert_model\r\n    subtitles_timedistributed = segment_time_distributed(subtitles_input)**\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\", line 270, in call\r\n    output_shape = self.compute_output_shape(input_shape).as_list()\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\", line 212, in compute_output_shape\r\n    child_output_shape = self.layer.compute_output_shape(child_input_shape)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 768, in compute_output_shape\r\n    layer_output_shapes = layer.compute_output_shape(layer_input_shapes)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\pooling.py\", line 591, in compute_output_shape\r\n    return tensor_shape.TensorShape([input_shape[0], input_shape[2]])\r\nIndexError: list index out of range\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: 10.1, 7,65\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Please, let us know TensorFlow version you are using?.Looks like code is incomplete. Request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@nectario \r\n\r\nAny update on this issue please. Thanks!\r\n", "Issue has been resolved. The issue was fixed in the library bert-for-tf2.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38547\">No</a>\n"]}]