[{"number": 23025, "title": "Keras saved_model.simple_save model bigger and bigger", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 19 Tara\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: python script.py\r\n\r\n\r\n### Describe the problem\r\nWhen saving successively a keras model using `tf.get_default_graph()`, the size of `saved_model.pb` increase.\r\n\r\n![image](https://user-images.githubusercontent.com/7363034/47020409-982a3e80-d159-11e8-80db-e39f47278f03.png)\r\n\r\n\r\n### Source code / logs\r\nscript.py\r\n```\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import *\r\n\r\nif __name__ == '__main__':\r\n\r\n    sess = tf.keras.backend.get_session()\r\n\r\n    model = Sequential()\r\n\r\n    model.add(Dense(units=64, activation='relu', input_dim=4))\r\n    model.add(Dense(units=4, activation='linear'))\r\n\r\n    opt = RMSprop(lr=0.1)\r\n    model.compile(loss='mse', optimizer=opt)\r\n\r\n    graph = tf.get_default_graph()  # Use the same graph as global thread\r\n\r\n    model.predict(np.array([[1, 1, 1, 1], [1, 1, 1, 1]]))\r\n\r\n    for _ in range(100):\r\n        time.sleep(1)\r\n        temp_export_path = '/tmp/models/' + str(time.time()).split(\".\")[0]\r\n\r\n        with graph.as_default():\r\n            tf.saved_model.simple_save(\r\n                sess,\r\n                temp_export_path,\r\n                inputs={'state': model.input},\r\n                outputs={t.name: t for t in model.outputs})\r\n\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "Updated", "@Coac You would need to reset the graph before initiating the model. Also, add longer sleep time between graphs. Here is the code snippet I modified, from which you will get exactly the same file size for all the .pb files.\r\n\r\n```\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import *\r\n\r\nif __name__ == '__main__':\r\n    \r\n    for _ in range(100):\r\n        tf.reset_default_graph()\r\n        sess = tf.keras.backend.get_session()\r\n\r\n        model = Sequential()\r\n        model.add(Dense(units=64, activation='relu', input_dim=4))\r\n        model.add(Dense(units=4, activation='linear'))\r\n\r\n        opt = RMSprop(lr=0.1)\r\n        model.compile(loss='mse', optimizer=opt)\r\n\r\n        graph = tf.get_default_graph()  # Use the same graph as global thread\r\n\r\n        model.predict(np.array([[1, 1, 1, 1], [1, 1, 1, 1]]))\r\n        time.sleep(10)\r\n        temp_export_path = '/tmp/models/' + str(time.time()).split(\".\")[0]\r\n\r\n        with graph.as_default():\r\n            tf.saved_model.simple_save(\r\n                sess,\r\n                temp_export_path,\r\n                inputs={'state': model.input},\r\n                outputs={t.name: t for t in model.outputs})\r\n```", "Thanks for your answer, this is working, the size does not increase when resetting the graph each time. \r\nBut we get a new graph and we **lose the trained weights**. \r\nCan't we get the latest version of the graph exported each time without having the size of the .pb file explodes?", "Glad it worked. To save the latest version of the graph exported every time, you probably want to use `saved_model.builder.SavedModelBuilder()` and define different tags then you can have multiple .pb. ", "I tried using `saved_model.builder.SavedModelBuilder()`, however I also get the exported .pb bigger after each call :(\r\n\r\nWhat I want to do is training the model, for some time then export it to serve it, then retraining.\r\nTraining -> Export -> Training -> Export -> ...\r\n\r\nThe version using a builder:\r\n```python\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import SGD\r\nfrom tensorflow.python.saved_model import tag_constants\r\nfrom tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\r\n\r\nif __name__ == '__main__':\r\n\r\n    sess = tf.keras.backend.get_session()\r\n\r\n    model = Sequential()\r\n\r\n    model.add(Dense(units=64, activation='relu', input_dim=4))\r\n    model.add(Dense(units=4, activation='linear'))\r\n\r\n    opt = SGD(lr=0.01)\r\n    model.compile(loss='mse', optimizer=opt)\r\n\r\n    graph = tf.get_default_graph()\r\n\r\n    model.predict(np.array([[1, 1, 1, 1], [1, 1, 1, 1]]))\r\n\r\n    for _ in range(100):\r\n        with graph.as_default():\r\n            time.sleep(1)\r\n            model.fit(np.array([[1, 1, 1, 1], [1, 1, 1, 1]]), np.array([[1, 1, 1, 1], [1, 1, 1, 1]]))\r\n            temp_export_path = '/tmp/models/' + str(time.time()).split(\".\")[0]\r\n\r\n            # Saving\r\n            builder = tf.saved_model.builder.SavedModelBuilder(temp_export_path)\r\n            signature = predict_signature_def(inputs={'state': model.input},\r\n                                              outputs={t.name: t for t in model.outputs})\r\n            builder.add_meta_graph_and_variables(sess, [tag_constants.SERVING],\r\n                                                 signature_def_map={\r\n                                                     tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\r\n            builder.save()\r\n\r\n```\r\nThe builder instantiation is inside the loop as I would like to save the model to a separate model each time.\r\n"]}, {"number": 22977, "title": "gradients of tf.fake_quant_with_min_max_vars function.", "body": "### System information\r\n- **OS Platform and Distribution : Linux Ubuntu 16.04**\r\n- **TensorFlow version : 1.6.0**\r\n- **Python version : 3.6**\r\n- **CUDA/cuDNN version : CUDA Version 9.0.176/ CUDNN 7.0.5**:\r\n- **GPU model and memory : TITAN Xp/12196MiB**\r\n\r\n### Describe the problem\r\nI assume that **tf.fake_quant_with_min_max_vars function** can not be differentiable due to the fact that quantization should be working based on threshold, such as round, or sign function. And it means that we can not get the gradient of variables due to the nature of chain rule.\r\n\r\nIn these days, several tricks are researched, and one of the most popular method is called **'straight-through-estimator'**, literally passing through the gradient itself.\r\n\r\nSo I tested **tf.fake_quant_with_min_max_vars** to apply **'straight-through-estimator'** functionality. Most results get a gradient 1. This means it works well. Sometimes, however, the result is incorrect. More detailed errors are described below.\r\n\r\n### Source code / logs\r\nHere's some snippet code\r\n`x = tf.cast(np.random.normal(0, 1, (10), tf.float32)`\r\n`x_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)`\r\n`grad = tf.gradients(x_q, x)`\r\n\r\nIn that case, sometimes **grad** get weird results:\r\n`[array([1., 1., 1., 0., 1., 1., 2., 1., 1., 1.], dtype=float32)]`\r\nAnd non-zero gradient values have a common quantization value. In this case, the **x_q** is:\r\n`array([0., 0., 0., -2.289673, 2.289673, 0., -2.289673, 0., 4.570346, 0.], dtype=float32)`", "comments": ["I forgot to mention that this error or bug is important because it has a lot of impact on convergence in the training process when using **tf.fake_quant_with_min_max_vars function**.", "@sicnarf1a I ran the above code snippet and received consistent output every time, i.e. a gradient of 1. `tf.fake_quant_with_min_max_vars` works as expected, not producing the error you are getting. \r\nPlease describe under what condition you would get different results.\r\n\r\n", "I can reproduce it using tf.fake_quant_with_min_max_args function on Ubuntu 16.04 with python 2.7.12, numpy 1.14.0, and TF 1.11.0 (CPU only):\r\n\r\n```\r\nmichael@Pascal:~$ cat tf_quant.py\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.set_printoptions(precision=3, linewidth=120, suppress=True)\r\n\r\nsess = tf.Session()\r\n\r\nfor i in range(3):\r\n    sample = np.random.normal(0,1,(8,8))\r\n    x = tf.cast(sample, tf.float32)\r\n\r\n    #min_ = tf.reduce_min(x)\r\n    #max_ = tf.reduce_max(x)\r\n    min_ = np.min(sample)\r\n    max_ = np.max(sample)\r\n    x_q = tf.fake_quant_with_min_max_args(x, min=min_, max=max_, num_bits=2)\r\n\r\n    grad = tf.gradients(x_q, x)\r\n    print sess.run([x, x_q, grad])\r\n```\r\n```\r\nmichael@Pascal:~$ python2 tf_quant.py\r\n2018-10-22 09:03:10.722455: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n       [array([\r\n       [-0.418, -0.105,  0.994,  0.022, -1.241, -0.324,  0.128,  0.001],\r\n       [ 0.399, -1.085,  0.361,  2.381,  0.095, -0.23 , -0.892, -0.864],\r\n       [ 2.527,  0.456, -1.682,  0.049,  0.508, -0.515,  0.379, -0.514],\r\n       [-1.474, -2.128,  1.136, -0.21 , -1.141, -0.059,  0.609,  1.454],\r\n       [ 2.644, -0.136,  1.267,  0.397, -0.688, -1.37 , -0.625, -0.144],\r\n       [-0.887, -1.912, -1.888,  0.066,  0.07 ,  0.166, -0.582, -1.693],\r\n       [-0.032, -1.16 , -0.344, -0.31 ,  0.72 , -0.713,  0.756,  1.   ],\r\n       [ 0.599, -2.627,  0.643,  2.024, -0.593,  0.528,  0.62 , -0.893]], dtype=float32), \r\n       array([\r\n       [ 0.   ,  0.   ,  1.757,  0.   , -1.757,  0.   ,  0.   ,  0.   ],\r\n       [ 0.   , -1.757,  0.   ,  1.757,  0.   ,  0.   , -1.757,  0.   ],\r\n       [ 1.757,  0.   , -1.757,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\r\n       [-1.757, -1.757,  1.757,  0.   , -1.757,  0.   ,  0.   ,  1.757],\r\n       [ 3.514,  0.   ,  1.757,  0.   ,  0.   , -1.757,  0.   ,  0.   ],\r\n       [-1.757, -1.757, -1.757,  0.   ,  0.   ,  0.   ,  0.   , -1.757],\r\n       [ 0.   , -1.757,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.757],\r\n       [ 0.   , -1.757,  0.   ,  1.757,  0.   ,  0.   ,  0.   , -1.757]], dtype=float32), \r\n       [array([\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 0., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 0., 0., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 0., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n       [array([\r\n       [-0.367, -0.653,  0.494,  0.604,  3.324,  0.023,  2.17 ,  0.203],\r\n       [-0.639,  0.214,  3.579, -0.498,  0.859,  2.193, -1.086, -0.746],\r\n       [ 0.066,  1.084,  0.795,  1.373,  0.489,  0.945,  0.157,  0.013],\r\n       [ 0.713,  0.306,  0.762,  0.298,  0.482, -1.181, -1.783, -0.756],\r\n       [-0.658, -1.185,  0.671,  1.643,  0.282,  0.879, -0.78 ,  1.086],\r\n       [ 0.031,  0.602, -0.686, -0.006, -1.228, -1.294, -1.03 ,  0.959],\r\n       [-0.603,  0.039,  0.034, -0.62 ,  0.174,  0.121, -1.015,  0.202],\r\n       [-0.544, -2.412, -1.444, -1.354, -0.576,  0.108,  1.627,  0.868]], dtype=float32), \r\n       array([\r\n       [ 0.   ,  0.   ,  0.   ,  0.   ,  3.994,  0.   ,  1.997,  0.   ],\r\n       [ 0.   ,  0.   ,  3.994,  0.   ,  0.   ,  1.997, -1.997,  0.   ],\r\n       [ 0.   ,  1.997,  0.   ,  1.997,  0.   ,  0.   ,  0.   ,  0.   ],\r\n       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   , -1.997, -1.997,  0.   ],\r\n       [ 0.   , -1.997,  0.   ,  1.997,  0.   ,  0.   ,  0.   ,  1.997],\r\n       [ 0.   ,  0.   ,  0.   ,  0.   , -1.997, -1.997, -1.997,  0.   ],\r\n       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   , -1.997,  0.   ],\r\n       [ 0.   , -1.997, -1.997, -1.997,  0.   ,  0.   ,  1.997,  0.   ]], dtype=float32), \r\n       [array([\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 0., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n       [array([\r\n       [-1.025,  0.599, -1.482,  0.203,  0.043,  0.098,  0.331, -2.39 ],\r\n       [-0.097,  1.226, -0.622,  0.668,  0.386,  1.166, -1.397,  0.325],\r\n       [ 1.481,  0.71 , -0.901,  0.784, -1.656,  0.436, -1.389, -1.009],\r\n       [ 1.175,  0.308,  0.249, -0.82 , -2.894, -0.62 ,  0.989,  0.968],\r\n       [ 0.775,  1.358,  0.506, -2.297, -0.183, -0.976, -0.743,  0.661],\r\n       [ 0.09 ,  0.324, -1.645, -0.832,  1.265,  0.257,  0.999, -0.491],\r\n       [ 2.886,  0.406,  0.393, -0.557, -0.68 , -0.243,  0.547, -1.332],\r\n       [ 0.49 ,  0.412,  0.036, -0.434,  0.022,  0.679, -0.229,  1.001]], dtype=float32), \r\n       array([\r\n       [-1.927,  0.   , -1.927,  0.   ,  0.   ,  0.   ,  0.   , -1.927],\r\n       [ 0.   ,  1.927,  0.   ,  0.   ,  0.   ,  1.927, -1.927,  0.   ],\r\n       [ 1.927,  0.   ,  0.   ,  0.   , -1.927,  0.   , -1.927, -1.927],\r\n       [ 1.927,  0.   ,  0.   ,  0.   , -3.853,  0.   ,  1.927,  1.927],\r\n       [ 0.   ,  1.927,  0.   , -1.927,  0.   , -1.927,  0.   ,  0.   ],\r\n       [ 0.   ,  0.   , -1.927,  0.   ,  1.927,  0.   ,  1.927,  0.   ],\r\n       [ 1.927,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   , -1.927],\r\n       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.927]], dtype=float32), \r\n       [array([\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [0., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n```\r\n\r\n\r\n\r\n", "@michaelklachko I run the code in your environment and results are normal. Note that you need to use `tf.reduce_max` and `tf.reduce_min` in `tf.fake_quant_with_min_max_vars()`\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfor i in range(3):\r\n    tf.reset_default_graph()\r\n    x = tf.cast(np.random.normal(0, 1, (8,8)), tf.float32)\r\n    x_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)\r\n    grad = tf.gradients(x_q, x)\r\n    sess = tf.Session()\r\n    print(sess.run([grad]))\r\n```\r\n```\r\n[[array([[1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n[[array([[1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n[[array([[1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)]]\r\n```", "@wt-huang not sure I understand: have you ran my code exactly as shown? If yes, did you not see any zero gradients? ", "@michaelklachko You are not calling the same function used in the code snippet from @sicnarf1a. I fixed your code so it would produce results as expected. Best to open a new issue so we can focus on your scenario.  ", "@wt-huang I think it's pretty obvious that the issue that caused the OP to open this ticket is the same one reproduced with my code. Now you have two people reporting the same issue, using different functions. ", "@michaelklachko `tf.fake_quant_with_min_max_vars` and `tf.fake_quant_with_min_max_args` are related but they carry different names so at least the title needs to be updated if we were to include them in one ticket. \r\n\r\nIn your code snippet, need to use `tf.reduce_max` and `tf.reduce_min` for max and min respectively. Then you will get 1's all the way through. This is also what @sicnarf1a did.\r\n\r\nBoth `tf.fake_quant_with_min_max_vars` and `tf.fake_quant_with_min_max_args` can work if using my code snippet above. In the future we will certainly do our best to include more test cases and make the functions more user friendly.", "@wt-huang Now I'm really confused: are you saying the arguments min/max to `tf.fake_quant_with_min_max_args` should NOT be floats, as specified here: https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_args ? Then what are the default values for that function? ", "The arguments min/max to `tf.fake_quant_with_min_max_args` should be floats, which is why you need to cast them to floats. ", "@wt-huang \uff0cHi wt-huang\uff1a\r\n    Is there a doc that can elaborate how to add fake node manually\uff1fSince in tensorflow 1.11,some op like transpose conv can not add fake node automatic.\r\nThanks,if you can help me,I search the internet for several days,but counld not find any doc."]}, {"number": 22975, "title": "Intermittent very long latency in XRT operations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: mater\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nOn occasion, I've seen XRT operations take significantly longer than I'd expect and then they usually do (into the 10s of seconds). Attaching GDB while this is happening reveals that it is spending most of its time in TF Graph level optimization passes, particularly EncapsulateXlaComputations (which don't really make any sense in conjunction with XRT ops). I don't have an exact reproducer for when this happens, but it appears that it's more likely to happen upon the first XRTAllocate after reconnecting after a client crashed. I believe, I managed to capture a log  at VLOG level 2 while this was happening (see gist at https://gist.github.com/Keno/3a5e0dc86d3829f5712c5e1a5f65161b). Hopefully that should aid in figuring out what's going on. At the end of the gist, it tried to dump the serialized representation of the ~500MB sized model into the log, so I interrupted it there. I can try to reproduce it again, letting it finish dumping the serialized model if that would be helpful.\r\n\r\ncc @michaelisard", "comments": ["I have also seen this behavior. Profiling shows that (at least one major) bottleneck is `Encapsulator::GetFunctionNameAttr` inside `EncapsulateXlaComputationsPass`, which itself is mostly `AttrSlice::Find`\u2014this call seems to take >10s cumulatively.", "The profile was from before https://github.com/tensorflow/tensorflow/commit/24333d8e55bdd995089e93122750340bf8d1ddba, which fixes the specific issue I was seeing. I'm looking into whether there are other bottlenecks.", "It looks like some Grappler passes still take a lot more time than I might expect (the graph is only a small handful of XRT ops) and it looks like this is due to large constant tensors that are repeatedly copied or checked for equality. In particular `TensorProto::merge` and `FastAttrValueHash` are each responsible for several seconds of CPU time in a single gRPC call for a graph composed of a handful of XRT ops, if those ops include large tensors.", "Is there a need to pass large constants in XRT? Does the problem go away if\nyou use placeholders instead?\n\nOn Tue, Oct 16, 2018 at 1:34 PM James Bradbury <notifications@github.com>\nwrote:\n\n> It looks like some Grappler passes still take a lot more time than I might\n> expect (the graph is only a small handful of XRT ops) and it looks like\n> this is due to large constant tensors that are repeatedly copied or checked\n> for equality. In particular TensorProto::merge and FastAttrValueHash are\n> each responsible for several seconds of CPU time in a single gRPC call for\n> a graph composed of a handful of XRT ops, if those ops include large\n> tensors.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22975#issuecomment-430389461>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFIK9QOxtOykMMIGGNn1U4tz2fEhszjTks5ulkLNgaJpZM4XbaNO>\n> .\n>\n", "Let me try that change. I may have taken the API examples a bit too seriously ;)", "@azaks2  Hi, could you please look into this issue ?", "Also, slightly related in that it's caused by a grappler optimizer, the logs get spewed with a bunch of xrt related messages from pin_to_host_optimizer:\r\n```\r\n2018-10-25 15:36:35.278265: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.279518: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.280739: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.284193: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.285282: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.286265: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTExecute\r\n2018-10-25 15:36:35.315119: I tensorflow/core/grappler/optimizers/pin_to_host_optimizer.cc:194] Could not find KernelDef for: XRTReadLiteral\r\n```\r\nDoesn't seem to be causing a problem, but is a bit annoying. Let me know if I should open a separate issue."]}, {"number": 22926, "title": "Feature Request: GPUOptions for Go binding", "body": "Current implementation of Go binding can not specify options.\r\n\r\nGPUOptions struct is in internal package. And `go generate` doesn't work for protobuf directory. So we can't specify GPUOptions for `NewSession`.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This problem appears to be broader than just specifying `GPUOptions` for TensorFlow sessions. There is no native Go API for passing *any* options when creating a session. The user must create the binary representation of a `ConfigProto` protocol buffer outside of the TensorFlow Go API.\r\n\r\nSee, for example, the test case `TestSessionConfig` in [session_test.go](https://github.com/tensorflow/tensorflow/blob/7e5b561df1f1dbcba9216e662c345eccb46b3048/tensorflow/go/session_test.go#L252):\r\n```go\r\nfunc TestSessionConfig(t *testing.T) {\r\n\t// Exercise SessionOptions.\r\n\t// Arguably, a better API would be for SessionOptions.Config to be the\r\n\t// type generated by the protocol buffer compiler. But for now, the\r\n\t// tensorflow package continues to be independent of protocol buffers\r\n\t// and this test exercises the option since the implementation has a\r\n\t// nuanced conversion to C types.\r\n\t//\r\n\t// Till then, the []byte form of Config here was generated using a toy\r\n\t// tensorflow Python program:\r\n\t/*\r\n\t import tensorflow\r\n\t c = tensorflow.ConfigProto()\r\n\t c.intra_op_parallelism_threads = 1\r\n\t print c.SerializeToString()\r\n\t*/\r\n\tgraph := NewGraph()\r\n\tc, err := Const(graph, \"Const\", int32(14))\r\n\tif err != nil {\r\n\t\tt.Fatal(err)\r\n\t}\r\n\topts := SessionOptions{Config: []byte(\"(\\x01\")}\r\n[...]\r\n```\r\n\r\nWould the TensorFlow maintainers accept a non-Google contribution that added the ability to specify session options using pure Go code? This functionality would require generating instances of the `ConfigProto` protocol buffer defined in `config.proto`. I can see two ways to generate these protocol buffers: Either add a build target to generate Go bindings for the files in `tensorflow/core/protobuf`; or add Go wrappers for the generated C++ code in `tensorflow/core/protobuf/config.pb.h`", "@asimshankar Can you please take a look? Thanks!", "@frreiss we'll definitely accept a Go-only API to configure tensorflow.", "I am looking into this.", "Created PR #26682 with a Go API to create ConfigOptions protocol buffer messages. I made the changes as narrow in scope as I could.", "Closed by https://github.com/tensorflow/tensorflow/pull/26682\r\n\r\n@frreiss Thank you", "#26682 is reverted https://github.com/tensorflow/tensorflow/commit/6e9cb400d17f60e50cefe59fc099fc5b6e4074ce#diff-dcd28ad951bd17e9c512d3b564640bab\r\n", "is this issue open? Can i work on it? @mattn ", "Yes please.", "May I know how to reproduce the issue?", "Sorry, I don't know why the changes was reverted.\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/26682#issuecomment-480350306", "ok no problem I will try some other issues\r\n", "Hi,Adesh  I am beginner i don't where start Can anybody help me?????\r\n", "Hello!\r\nI am newbie to open source contribution and really interested to contribute!! . But I have very very less knowledge about it . Can anyone help me please?", "can someone guide me what to do? i will be really thankful :+1: ", "Kindly close this issue. It's confusing for first time contributors who come here for solving their first Issue.", "I don't mind to close this but no one explain why https://github.com/tensorflow/tensorflow/pull/26682 was reverted.", "is this still open?", "is it still open ?\r\n\r\n", "@mattn is this issue closed?\r\nif not\r\nis it open for beginners to help?\r\nif not then why does it have the `good first issue`??\r\n", "Assign me", "relax people! if u are here to solve these tasks, u are a beginner anyway. \r\njust do it, regardless if someone already started working on the PR or not.\r\nu will learn a bunch of stuff!!\r\ndon't focus too much on whether someone already started working on it. in that way, u will never start..", "can any one guide me to this issue", "Hello Gays ", "Ok ill start on the issue.", "name =input(\"enter your name :  \")\r\npassword = input(\"enter your password :  \")\r\npassword_length = len(password)\r\nif password_length <= 8:\r\n    print(\"Short paas troop word!\")\r\nelif 8 <password_length <= 12:\r\n    print(\"Medium paas word! \")\r\nelse:\r\n    print(\"Perfect paas word! \")\r\nprint(password_length)", "@RedaAitBabaaziz ?", "@plopd Thanks. I really look forward to lots of learning. So how to get started?\r\n\r\n", "Note from the past to fellow coders and beginners: \r\nAhem.\r\n\r\nSo this pops up in the good first issue page and a lot of \"good first issue finder\" websites. You might've came from a link, or other way.\r\n\r\nSo here is the deal:\r\n\r\n_I am a TensorFlow Developer_ - god level: Try it out, if you can understand what these names mean and can spot them in in the \"TensorFlow\".\r\n\r\n_I am a TensorFlow Developer_: Try to solve it/ or learn the issue.\r\n\r\n_I am a huge developer of [X language used in TensorFlow]_: First learn the issue(more like, learn how the issue relates to the \"TensorFlow\".\r\n\r\n_I am a developer of [X language used in TensorFlow]_: try it out; learn the issue.\r\n\r\n_I am a learner in TensorFlow_: you can start by learning the code of TensorFlow, then the issue, then the solutions + comments( <- do this before solving it!)\r\n\r\n_I am a learner of [X language used in TensorFlow]_: you can explore but try this issue when you become a _I am a learner in TensorFlow_ or _I am a developer of [X language used in TensorFlow]_...\r\n\r\n_I am a learner of programming_: first be a _I am a learner of [X language used in TensorFlow]_ or a _I am a developer of [X language used in TensorFlow]_....\r\n\r\n_I am a explorer of GitHub/TensorFlow/[X language used in TensorFlow]_: you can start by learning the code of TensorFlow, then the issue, then the solutions + comments( <- do this before solving it!)\r\n\r\nAfter choosing what to do from the above; do these:\r\n\r\nUnderstand _what is going on here_; what's the comments, what's the feature, what are the questions here, how all these PRs and versions align to the issue, Can you make a timeline of the improvements.\r\n\r\nSo you are fixing the issue: First understand what is  \"GPUOptions for Go binding\" then look the PRs already in progress, then chat with everyone(feel free to chat in other places like Reddit, Stack overflow.\r\n\r\n**Thanks for the hearts!**", "#22926", "Hello-World", "@tensorflowbutler This comment section is just 1 mile away from getting the normal-land spam, isn't there some body to moderate this and take a look at the PRs and like disable commenting if a PR does good?\r\n\r\nExamples of **not**-spam but _worthless_ comments:\r\n\r\n* >Hello Gays\r\n\r\n* >#22926\r\n\r\n* >Hello-World\r\n\r\n* >```\r\n  >name =input(\"enter your name : \")\r\n  >password = input(\"enter your password : \")\r\n  >password_length = len(password)\r\n  >if password_length <= 8:\r\n  >print(\"Short paas troop word!\")\r\n  >elif 8 <password_length <= 12:\r\n  >print(\"Medium paas word! \")\r\n  >else:\r\n  >print(\"Perfect paas word! \")\r\n  >print(password_length)\r\n  \r\n\r\n* >52193\r\n*  >#22926\r\n* >Request\r\n* > [OpenCV VS Tensorflow](https://drasticcode.com/face-recognition-opencv-or-tensorflow/)\r\n   > \r\n   > The main difference between OpenCV is the computer vision library and TensorFlow is Machine Learning Tool. definition, differences, application, languages they support\r\n* >Open CV\r\n\r\nI am sorry if I got a comment wrong but sure this needs attention from someone (even not for my request/specific-thing but just...)", "52193 ", "Is this issue open?\r\n", "> 52193\r\n\r\nWhat are you expecting by commenting this number?", "> Is this issue open?\r\n\r\nYes it is.", "Request", "\r\n[OpenCV VS Tensorflow](https://drasticcode.com/face-recognition-opencv-or-tensorflow/\r\n)\r\n\r\nThe main difference between OpenCV is the computer vision library and TensorFlow is Machine Learning Tool. definition, differences, application, languages they support\r\n\r\n", "Open CV", "Is this  issue still open?\nAnd if anyone is not working on it pleaee assign it to me", "Is this issue still open? I would like to take a stab at it", "@Tasfia-Ara are you new to this open source stuff", "@Wansh619 Yes this is my first project.\r\n", "For me too but there is no response that is this  issue stiil open or not", "I hope someone responds - it also seems that many people tried to solve this, but couldn't", "Are you a student?", "yeah - I'm a comp sci major", "From which country?\nI am from India ", "@Tasfia-Ara i was thinking that if you too are new in this so let work together as a tem to solve this issue \nBy the way do you have any idea how to work on this issue or you too need resources to learn. ", "@Wansh619 I will probably need more resources to learn more about this issue.", "Hello, I am new here and want to contribute in your project I would like to ask what i should learn to contribute here and please also tell me some resources ....\r\n", "Leave a comment ", "Hey all! I'm looking to contribute to this project in some way, and have coding skills that include C, C++, Python, and Java. Is there anything that needs to be worked on, as a first-time contributor? If not code, perhaps some changes to documentation? Thanks!", "Anyone can help me in my project .....\r\nproject is social media integrated search engine"]}, {"number": 22894, "title": "Can't use CTCBeamSearchDecoder in c++, LINK ERROR occur,BUG in CTCBeamSearchDecoder 's source code", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro 1803\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**: from source\r\n- **TensorFlow version (use command below)**: r1.4  and r1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: cmake 3.12.2\r\n- **GCC/Compiler version (if compiling from source)**: VS 2015\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\n I want to use [CTCBeamSearchDecoder ](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/util/ctc/ctc_beam_search.h), but there is a link error when I call CTCBeamSearchDecoder  constructor  function(`CTCBeamSearchDecoder<> decoder(num_classes, 10 * top_paths, &default_scorer);`).  And I comment the constructor  function, the error disappear.  I test the r1.4 tensorflow.lib complied by myself with cmake and r1.10 [download from github](https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.10.0/cpp) . **So I think there might be a bug in CTCBeamSearchDecoder  's source code.**\t\r\n### Source code / logs\r\nThis is the test code which is from [ctc_beam_search_test.cc](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/core/util/ctc/ctc_beam_search_test.cc). The code is very simple.\r\n```\r\n#include \"third_party/eigen3/Eigen/Core\"\r\n//#include \"tensorflow/core/platform/env.h\"\r\n#include \"tensorflow/core/util/ctc/ctc_beam_search.h\"\r\n#include <vector>\r\nvoid test()\r\n{\r\n\tusing tensorflow::ctc::CTCBeamSearchDecoder;\r\n\tusing tensorflow::ctc::CTCDecoder;\r\n\tconst int batch_size = 1;\r\n\tconst int timesteps = 5;\r\n\tconst int top_paths = 3;\r\n\tconst int num_classes = 6;\r\n\r\n\t// Plain decoder using hibernating beam search algorithm.\r\n\tCTCBeamSearchDecoder<>::DefaultBeamScorer default_scorer;\r\n        //COMMENT AND NO ERROR\r\n\tCTCBeamSearchDecoder<> decoder(num_classes, 10 * top_paths, &default_scorer);\r\n\r\n\t// Dictionary decoder, allowing only two dictionary words : {3}, {3, 1}.\r\n\t//DictionaryBeamScorer dictionary_scorer;\r\n\t//CTCBeamSearchDecoder<HistoryBeamState> dictionary_decoder(num_classes, top_paths, &dictionary_scorer);\r\n\r\n\t// Raw data containers (arrays of floats, ints, etc.).\r\n\tint sequence_lengths[batch_size] = { timesteps };\r\n\tfloat input_data_mat[timesteps][batch_size][num_classes] = {\r\n\t\t{ { 0, 0.6, 0, 0.4, 0, 0 } },\r\n\t\t{ { 0, 0.5, 0, 0.5, 0, 0 } },\r\n\t\t{ { 0, 0.4, 0, 0.6, 0, 0 } },\r\n\t\t{ { 0, 0.4, 0, 0.6, 0, 0 } },\r\n\t\t{ { 0, 0.4, 0, 0.6, 0, 0 } } };\r\n\r\n\t// The CTCDecoder works with log-probs.\r\n\tfor (int t = 0; t < timesteps; ++t) {\r\n\t\tfor (int b = 0; b < batch_size; ++b) {\r\n\t\t\tfor (int c = 0; c < num_classes; ++c) {\r\n\t\t\t\tinput_data_mat[t][b][c] = std::log(input_data_mat[t][b][c]);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Plain output, without any additional scoring.\r\n\tstd::vector<CTCDecoder::Output> expected_output = {\r\n\t\t{ { 1, 3 },{ 1, 3, 1 },{ 3, 1, 3 } },\r\n\t};\r\n\r\n\t// Dictionary outputs: preference for dictionary candidates. The\r\n\t// second-candidate is there, despite it not being a dictionary word, due to\r\n\t// stronger probability in the input to the decoder.\r\n\tstd::vector<CTCDecoder::Output> expected_dict_output = {\r\n\t\t{ { 3 },{ 1, 3 },{ 3, 1 } },\r\n\t};\r\n\r\n\t// Convert data containers to the format accepted by the decoder, simply\r\n\t// mapping the memory from the container to an Eigen::ArrayXi,::MatrixXf,\r\n\t// using Eigen::Map.\r\n\tEigen::Map<const Eigen::ArrayXi> seq_len(&sequence_lengths[0], batch_size);\r\n\tstd::vector<Eigen::Map<const Eigen::MatrixXf>> inputs;\r\n\tinputs.reserve(timesteps);\r\n\tfor (int t = 0; t < timesteps; ++t) {\r\n\t\tinputs.emplace_back(&input_data_mat[t][0][0], batch_size, num_classes);\r\n\t}\r\n\r\n\t// Prepare containers for output and scores.\r\n\tstd::vector<CTCDecoder::Output> outputs(top_paths);\r\n\tfor (CTCDecoder::Output& output : outputs) {\r\n\t\toutput.resize(batch_size);\r\n\t}\r\n\tfloat score[batch_size][top_paths] = { { 0.0 } };\r\n\tEigen::Map<Eigen::MatrixXf> scores(&score[0][0], batch_size, top_paths);\r\n}\r\nint main()\r\n{\r\n\ttest();\r\n\treturn 0;\r\n}\r\n\r\n```\r\nThis is complier 's error log.\r\n```\r\nerror LNK2001 unresolved external symbol  \"void __cdecl tensorflow::internal::MakeCheckOpValueString<unsigned char>(class std::basic_ostream<char,struct std::char_traits<char> > *,unsigned char const &)\" (??$MakeCheckOpValueString@E@internal@tensorflow@@YAXPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEBE@Z)\ttest1\tC:\\Users\\46099\\Desktop\\testtf\\test1\\test1\\main.obj\t1\r\n```\r\n", "comments": ["Is this still an issue?\n\nOn Sat, Nov 10, 2018, 10:48 AM Alfred Sorten Wolf <notifications@github.com\nwrote:\n\n> Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 29\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22894#issuecomment-437608991>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyMckf_pCXq8fjajAdDS2dENYJ5sks5utx9ygaJpZM4XXZTK>\n> .\n>\n", "> Is this still an issue?\r\n> [\u2026](#)\r\n> On Sat, Nov 10, 2018, 10:48 AM Alfred Sorten Wolf ***@***.*** wrote: Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly. \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#22894 (comment)](https://github.com/tensorflow/tensorflow/issues/22894#issuecomment-437608991)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABtimyMckf_pCXq8fjajAdDS2dENYJ5sks5utx9ygaJpZM4XXZTK> .\r\n\r\nYes, it is still an issue. But I  manually  copy  `MakeCheckOpValueString` function from [logging.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/logging.cc)\r\nand it works.\r\n ```\r\nnamespace tensorflow {\r\n\tnamespace internal {\r\n\t\ttemplate <>\r\n\t\tvoid MakeCheckOpValueString(std::ostream* os, const unsigned char& v) {\r\n\t\t\tif (v >= 32 && v <= 126) {\r\n\t\t\t\t(*os) << \"'\" << v << \"'\";\r\n\t\t\t}\r\n\t\t\telse {\r\n\t\t\t\t(*os) << \"unsigned char value \" << static_cast<unsigned short>(v);\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```"]}, {"number": 22869, "title": "Tensorflow C API: SessionRun batch size (how to properly set)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.3.0\r\n- **CUDA/cuDNN version**: 9.2 / 7.3\r\n- **GPU model and memory**: Nvidia GTX 1070ti\r\n\r\n\r\nDear sirs,\r\nI am struggling to find the correct way to properly set the batch size for a SessionRun call in the C API. \r\nWhat I am trying to do is passing multiple images in a single SessionRun to a CNN (VGG16).\r\nI am wondering which is the correct way to do so and to retrieve the appropriate correlated output values.\r\nI have also come to believe that a single SessionRun call can only accept a single image as an input, so a proper batched input could only be achieved by having multiple SessionRun calls (for an example inside a \"for\" loop) with a single image each. Am I correct? Is there not a way to batch multiple images inside a single SessionRun call for a single-input CNN?\r\n\r\nI have searched thoroughly your documentation but I could not find any evidence of a proper indication on how to do it.\r\n\r\nBest regards", "comments": ["The `session.run` executes a graph. A graph may have any number of input images as part of it's input pipeline.  Typically you will use the `tf.data.Dataset`API to define your data input layer as well as decide on appropriate batch sizes for your model.\r\n\r\nFolks will probably ask you to post this on something like stackoverflow as this is probably not best asked here :)", "Personally, it is also my belief that the documentation is lacking an indication on how to correctly perform a batched inference step.\r\n@sabhiram , you pointed out that a model must have an input pipeline that allows multiple inputs, but i am working with a single image input layer (since I am using the same exact model on Keras) and I believe that I should be able to perform a batched inference pass on that network.\r\n\r\nIf I use a SessionRun inside a loop (on the same graph) am I not introducing an overhead caused by the overhead time needed to load an image and a graph on gpu each time?\r\n\r\nThank you all for your kind attention.\r\n\r\nP.s. Since I personally believe that the documentation is lacking some information, this place should be the right place in which to ask for an answer :) ", "I really need help with this question, because it is blocking for my work (and I did not find any advice in the documentation).\r\n\r\nI have tried these pseudo-code: (calling these code SISO = Single Input Single Output, and MIMO = Multiple Input Multiple Output, the input is batch_size * 50 * 50 * 3, in SISO batch_size = 1, in MIMO batch_size = 3; remember that I am using VGG16 graph)\r\n\r\nSISO:\r\n```\r\nloadGraph()\r\nTF_NewSession()\r\nTF_AllocateTensor( 1 * 50 * 50 * 3)\r\n// copy data into tensor\r\nnum_bytes = rows() * cols() * channels() * sizeof(float);\r\nmemcpy(tensor_data, img_data, num_bytes)\r\nTF_SessionRun()\r\ndata = static_cast<float*>(TF_TensorData(output_tensor));\r\n// data[0] = frame number\r\n// data[1] = probabily of prediction\r\nout = (data[1] > 0.5f) ? true : false;\r\n```\r\n\r\nfor SISO, I have always the right output.\r\n\r\nMIMO:\r\n```\r\nloadGraph()\r\nTF_NewSession()\r\nTF_AllocateTensor( 3 * 50 * 50 * 3)\r\n// copy data into tensor\r\nnum_bytes = rows() * cols() * channels() * sizeof(float);\r\noffset = 0;\r\nfor(size_t i = 0; i < 3; ++i)\r\n{\r\n   memcpy(tensor_data + offset, imgs_data[i], num_bytes);\r\n   offset += num_bytes;\r\n}\r\nTF_SessionRun()\r\ndata = static_cast<float*>(TF_TensorData(output_tensor));\r\noffset = 0;\r\nnum_bytes = 2 * sizeof(float);\r\n// data[i] = frame number\r\n// data[i+1] = probabily of prediction\r\nfor(size_t i = 0; i < 3; ++i)\r\n    {\r\n        out[i] = (*(data + offset + 1) > 0.5f) ? true : false;\r\n        offset += num_bytes;\r\n    }\r\n```\r\n\r\nfor MIMO, only the first classification output value is right, the other ones are random numbers.\r\n\r\nwhat is wrong with this code?\r\n", "@EnricoGiordano1992 shouldn't you use `data[i+1]` in this line:\r\n```\r\nout[i] = (*(data + offset + 1) > 0.5f) ? true : false;\r\n```", "@wt-huang it seems that something changed, but I have wrong outputs.\r\n\r\nWhat about input tensor setting? What is wrong?\r\n\r\n```\r\n    uint8_t * tensor_data = static_cast<uint8_t *>(TF_TensorData(input_tensor));\r\n    const size_t num_bytes = rows() * cols() * channels() * sizeof(float); // 50 * 50 * 3 * 4\r\n    size_t offset = 0;\r\n    for(size_t i = 0; i < batch_sz; ++i)\r\n    {\r\n        std::memcpy((tensor_data + offset), actual_in.at(i).data, num_bytes);\r\n        offset += num_bytes;\r\n    }\r\n```", "any suggestions?", "any suggestions?", "any update on this?", "any update on this? it is also pending for my work \r\n"]}, {"number": 22843, "title": "Error with transform_graph tool:  \"Failed to parse --transform argument\"", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X High Sierra\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: - \r\n- **TensorFlow installed from (source or binary)**: PIP INSTALLED\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: - \r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: - \r\n- **Exact command to reproduce**: \r\nbazel run transform_graph -- --in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb --out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb --inputs='image_tensor' --outputs='detection_boxes,detection_scores,detection_classes,num_detections' --transforms='\r\nstrip_unused_nodes(type=float,shape=\u201c1,299,299,3\u201d)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms'\r\n\r\n### Describe the problem\r\n\r\nI have used the Object Detection API to create a transfer learning model based upon MobileNet-V2-Coco. I have used \"export_for_inference.py\" on my checkpoint files to create a frozen model.\r\n\r\nI am now trying to optimise the frozen model as described here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#introduction\r\n\r\nHowever, I am receiving an error:\r\n\r\ntensorflow/tools/graph_transforms/transform_graph.cc:241] **Failed to parse --transform argument, error was Looking for parameter name, but found \u201c1,299,299,3\u201d)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms**\r\n\r\n\r\n### Source code / logs\r\n\r\nTRACEBACK\r\n\r\nMac-Air:graph_transforms mac$ bazel run transform_graph -- --in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb --out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb --inputs='image_tensor' --outputs='detection_boxes,detection_scores,detection_classes,num_detections' --transforms='\r\nstrip_unused_nodes(type=float,shape=\u201c1,299,299,3\u201d)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms'\r\nINFO: Analysed target //tensorflow/tools/graph_transforms:transform_graph (0 packages loaded).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/graph_transforms:transform_graph up-to-date:\r\n  bazel-bin/tensorflow/tools/graph_transforms/transform_graph\r\nINFO: Elapsed time: 0.554s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/transform_graph '--in_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/frozen_inference_graph.pb' '--out_graph=/Users/Documents/modelsmaster/research/object_detection/inference_graphX/out_graph.pb' '--inputs=image_tensor' '--outputs=detection_boxes,detection_scores,detection_classes,num_detections' '--transforms=\r\nstrip_unused_nodes(type=float,shape=\u201c1,299,299,3\u201d)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nINFO: Build completed successfully, 1 total action\r\n2018-10-09 22:25:41.764013: E tensorflow/tools/graph_transforms/transform_graph.cc:241] Failed to parse --transform argument, error was Looking for parameter name, but found \u201c1,299,299,3\u201d)\r\nfold_constants(ignore_errors=true)\r\nfold_batch_norms\r\nfold_old_batch_norms\r\n\r\n\r\nPlease help me in sorting out this issue.\r\n", "comments": ["Please look into this issue ..", "Facing the same issue here. My Os is Ubuntu 18.04.\r\n\r\nI'm trying to convert the xception model with graph_transform.\r\n\r\n```\r\n//The command\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n> --in_graph=xception.pb \\\r\n> --out_graph=optimized_xception_graph.pb --inputs='ImageTensor' --outputs='SemanticPredictions' \\\r\n> --transforms=' \\\r\n>   strip_unused_nodes(name=ImageTensor,type=uint8, shape=\"1,-1,-1,3\") \\\r\n>   remove_nodes(op=Identity, op=CheckNumerics) \\\r\n>   fold_constants(ignore_errors=true) \\\r\n>   fold_batch_norms \\\r\n>   fold_old_batch_norms'\r\n\r\n//The Error\r\n2018-10-22 13:03:42.975779: E tensorflow/tools/graph_transforms/transform_graph.cc:241] Failed to parse --transform argument, error was Looking for transform name, but found \\\r\n  strip_unused_nodes(name=ImageTensor,type=float, shape=\"1,-1,-1,3\") \\\r\n  remove_nodes(op=Identity, op=CheckNumerics) \\\r\n  fold_constants(ignore_errors=true) \\\r\n  fold_batch_norms \\\r\n  fold_old_batch_norms\r\n```\r\n\r\nThe output of graph_summarize is\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=xception.pb\r\nFound 1 possible inputs: (name=ImageTensor, type=uint8(4), shape=[1,?,?,3]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=SemanticPredictions, op=Identity) \r\nFound 41258392 (41.26M) const parameters, 0 (0) variable parameters, and 4 control_edges\r\nOp types used: 821 Const, 734 Identity, 146 FusedBatchNorm, 82 Relu, 79 Conv2D, 68 DepthwiseConv2dNative, 23 Add, 9 StridedSlice, 8 Sub, 6 BatchToSpaceND, 6 SpaceToBatchND, 5 ResizeBilinear, 5 Pad, 4 GreaterEqual, 4 Pack, 4 Shape, 4 Assert, 2 ExpandDims, 2 ConcatV2, 2 Cast, 2 Maximum, 2 Squeeze, 1 Slice, 1 ResizeNearestNeighbor, 1 LogicalAnd, 1 Reshape, 1 Placeholder, 1 Mul, 1 Equal, 1 BiasAdd, 1 AvgPool, 1 ArgMax\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=xception.pb --show_flops --input_layer=ImageTensor --input_layer_type=uint8 --input_layer_shape=1,-1,-1,3 --output_layer=SemanticPredictions\r\n```\r\n\r\n", "Does anyone fix this problem?", "Hoping someone to solve this problem!", "I am also facing the same issue. TF version: 1.13.1 &, bazel is 0.19.2. Platform: Windows 10. Any suggestions. It seems that it is getting confused with the double quotes . "]}, {"number": 22825, "title": "Second order derivative not supported for LRN (tf.nn.lrn)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: binary \r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  CUDA 9.0, cuDNN - 7\r\n- **GPU model and memory**: Titan X\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a network architecture which has a LRN layer (tf.nn.lrn). I am using the same network definition from the CIFAR 10 tutorial: https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10.py .\r\nI have a loss term which involves the gradient of the cross entropy loss with respect to the input. This throws an error during graph construction:\r\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\r\n\r\n\r\n### Source code\r\nI have reused the same code from the CIFAR10 tutorial with minimal change to the loss function.\r\nI have modified the loss function to have an additional term which penalizes the L2 norm of the gradient of the cross entropy loss with respect to the input.\r\nFollowing is the change to the loss function made to the cifar10.py from the tutorial.\r\n```python\r\ndef loss(images, logits, labels):\r\n  \"\"\"Please note that the caller needs to feed the input images as well.\r\n  Args:\r\n    images: Input images to be used for gradient computation\r\n    logits: Logits from inference().\r\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\r\n            of shape [batch_size]\r\n  Returns:\r\n    Loss tensor of type float.\r\n  \"\"\"\r\n  labels_onehot = tf.one_hot(labels, depth=10, off_value=0.0, on_value=1.0, dtype=tf.float32)\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\r\n      labels=labels_onehot, logits=logits, name='cross_entropy_per_example')\r\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\r\n  \r\n  cross_entropy_grads, = tf.gradients(cross_entropy_mean, images)\r\n  xent_grad_norm = tf.nn.l2_loss(cross_entropy_grads)\r\n\r\n  tf.add_to_collection('losses', cross_entropy_mean)\r\n  tf.add_to_collection('losses', xent_grad_norm)\r\n\r\n  return tf.add_n(tf.get_collection('losses'), name='total_loss')\r\n```\r\n### Logs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 730, in _GradientsHelper\r\n    grad_fn = ops.get_gradient_function(op)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2460, in get_gradient_function\r\n    return _gradient_registry.lookup(op_type)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\", line 93, in lookup\r\n    \"%s registry has no entry for: %s\" % (self._name, name))\r\nLookupError: gradient registry has no entry for: LRNGrad\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n..............................................\r\n..............................................\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n  File \"/home/vipin/anaconda3/envs/py36-tf1.11.0-test/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 734, in _GradientsHelper\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'gradients/norm1_grad/LRNGrad' (op type: LRNGrad)\r\n```\r\n\r\n", "comments": ["@vipinpillai Second order derivative should be supported for LRN (tf.nn.lrn). I ran the above code snippet without running into any errors. ", "@wt-huang Could you please share the full code snippet that you had used? \r\nI have verified this on multiple versions of tensorflow including 1.11.0 and it gave the error that I has posted. Which TF version did you use to check this issue?", "@vipinpillai The code I used is the same as above with modification to cross entropy. I used TensorFlow nightly and 1.12.0-rc0, both worked. ", "@wt-huang I was able to reproduce the bug on tf-nightly and python 3.6.\r\nI have attached the 2 files which contain the minimal changes to reproduce the issue. \r\nTo summarize the changes, I have edited the function loss in cifar10_lrn.py to take an argument for input images and added an additional loss term which is the L2 norm of the gradients of the cross entropy with respect to the input images. \r\nCould you please use the attached code to reproduce the issue at your end?\r\n\r\n[cifar10_lrn_bug.zip](https://github.com/tensorflow/tensorflow/files/2480487/cifar10_lrn_bug.zip)\r\n\r\n\r\n\r\n", "@vipinpillai The issue lies in `tf.gradients(cross_entropy_mean, images)`. Images used here is a 4D tensor that needs to be preprocessed before feeding into `tf.gradients`. You can probably leverage tf.image.image_gradients(image) to some extent.\r\n\r\n\r\n\r\n", "@wt-huang I don't think you have quite understood the issue here. `tf.image.image_gradients(image)` will merely give gradients for the image itself, whereas I am trying to compute the gradient of the cross entropy loss with respect to the image.\r\nCould you please clarify how you were able to execute the code snippet previously without errors since the snippet included `tf.gradients(cross_entropy_mean, images)`. Images used here is 4D tensor as is the standard practice and I have used the same preprocessing which is used in the CIFAR10 tutorial. I have made extremely minimal changes in the loss function to just add an additional term for L2 norm of the gradients of the cross entropy loss with respect to the input image.", "@vipinpillai Yes, understood the issue, which is why I said to leverage  `tf.image.image_gradients` only to get some implementation ideas. There are other alternatives as well. Previously executed the code you provided inside the issue without running into error, which is different from the zip file. \r\nAbsolutely, images used as 4D tensor is standard practice, but feeding into tf.gradients directly is a different story. Please refer to [`tf.gradients`.](https://www.tensorflow.org/api_docs/python/tf/gradients). ", "@wt-huang `tf.gradients` only mentions that the xs should be a tensor. I was expecting the `tf.gradients` to be working across any nodes in the graph as long as they were tensors and were connected. \r\nCould you please clarify as to why `tf.nn.lrn` is not working as expected since automatic differentiation should allow computation of higher order gradients?", "@vipinpillai `tf.gradients(cross_entropy_mean, arg1)` returns the gradient of cross_entropy_mean with respect to each tensor in arg1. You would need to define the shape for arg1 such that its elements are included in `cross_entropy_mean`. Also, the LHS should match with the arg1 shape as well.\r\n\r\nAfter fixing `tf.gradients(cross_entropy_mean, arg1)`, you should be able to use `tf.nn.lrn` to get the gradient for lrn as per PR [13987](https://github.com/tensorflow/tensorflow/pull/13987). ", "@wt-huang In the API `tf.gradients(cross_entropy_mean, arg1)` , arg1 can be a list of tensors or simply a tensor. There is no restriction on the dimensionality of the _arg1_ tensor as long as the tensor is connected to _cross_entropy_mean_ in the graph. Could you please clarify as to what exactly do you mean by:\r\n> You would need to define the shape for arg1 such that its elements are included in cross_entropy_mean.\r\n\r\nThere is no restriction anywhere in the `tf.gradients` method as to the LHS shape to match with arg1. \r\n\r\nAlso, I don't exactly understand as to what you mean by \r\n> After  fixing `tf.gradients(cross_entropy_mean, arg1)`.\r\n\r\nWhat should be fixed here? Is it not possible to take a simple gradient of the loss with respect to the input? The important point to note here is that there is no error if we don't use `tf.nn.lrn` suggesting that the problem lies in the underlying implementation of `tf.nn.lrn` itself. ", "@skye @aselle Could you please take a quick look at this issue?", "I'm not very familiar with tf.nn.lrn, but it appears that the LRN op has a custom kernel for its gradient computation, LRNGrad. The LRNGrad op doesn't have a gradient defined, which is why you can't take the second derivative.\r\n\r\nI'm gonna mark this contributions welcome for now, since someone needs to implement the LRNGrad gradient function to address this. @ebrevdo @martinwicke do you know if anyone on the TF team owns/knows about LRN + LRNGrad?", "@rryan might.", "Thanks @skye . \r\nI would be happy to contribute if given some hints to proceed with the required changes.\r\nDo I need to add an additional GradGrad op?", "Yes exactly. You would register an LRNGradGrad op here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L684\r\n\r\nAnd then implement the kernel here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/lrn_op.cc\r\n\r\nAnd then register a Python gradient function here: http://go/gh/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L579\r\n\r\nI'm not familiar with the specifics of LRN, so I can't really give more context than that. It looks like the current LRN kernels are pretty fancy so this might be tricky, although it's possible you can make a simpler GradGrad kernel that's not as performant but still works.\r\n\r\nAlternatively, you can possibly implement the gradient entirely in the Python gradient function using lower-level ops (i.e. create the grad grad computation using the standard TF API). This would have the advantage that it could in turn be differentiated (assuming you use ops that have gradients defined), and might be easier to implement.\r\n\r\nPlease let me know if you have any questions!", "I've run into the same problem. @vipinpillai Can you please share if you were able to solve the issue? ", "@ashutosh96 No, I moved to PyTorch more than a year ago and higher order gradients are fully supported in PyTorch."]}, {"number": 22815, "title": "Object detection api training issue", "body": "### System information\r\nOS WIN 10\r\nProcessor : i5-4440 3.10GHz\r\nRAM 8\r\n\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: 3.6.6\r\n\r\n### Describe the problem\r\nI am trying to train object detection api using (faster_rcnn_inception_v2_coco_2018_01_28)\r\n\r\nconfiguration file\r\n[config.docx](https://github.com/tensorflow/tensorflow/files/2455380/config.docx)\r\n - \r\nI am facing with issue with object detection training\r\nafter 400~ steps the training is stop because a path problem while the script create the path\r\nthe path - C:\\Users\\z\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages\\tensorflow\\models\\research\\object_detection\\training_dataturk\\models\\model\\export\\Servo\\temp-b'1538897129'\r\n----the error screen shot that happend twice before stopped\r\n![1](https://user-images.githubusercontent.com/38852155/46601079-f6dc2200-caa0-11e8-8c11-e3af22ff602d.PNG)\r\n\r\n\r\n\r\n", "comments": []}, {"number": 22810, "title": "Segfault when loading libtensorflow_cc.so for a second time", "body": "\r\n\r\nSee\r\n\r\nhttps://stackoverflow.com/questions/52683649/libtensorflow-cc-so-initialised-a-second-time-causes-segfault\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["@samhodge Installing and executing multiple plugins may incur complexity down the road such as memory and resources issues, to name a few. Currently this is not supported by TensorFlow. ", "It seems like a perfectly valid use case", "If it a true Singleton pattern there should be methods for making sure you are using the one and only class.", "I do not think loading tensorflow libraries twice is a supported usecase from our end. To the best of my knowledge, we have not verified if this works.\r\n\r\ncc @skye @asimshankar @allenlavoie ", "Why not link both plugins against `libtensorflow_cc.so` normally, so that they share the same instance?", "What do you mean?\r\n\r\nThe plugin architecture defines how the dynamic libraries are loaded.\r\n\r\nhttp://openeffects.org/documentation/guide/\r\n\r\n\r\nCurrently I have made it work by putting each of the plugins into one binary so they share the same instance. But if i want to ship two products seperately or if another vendor want to make use of tensorflow under the OpenFX framework we will end up that the first plugin to load will work, but after that it will result in a segfault.", "I'm not familiar with OpenFX, but by \"normal\" dynamic linking I mean several shared objects (/plugins) each reference a \"libtensorflow_cc.so\" by name. When the second plugin shared object is loaded it sees that \"libtensorflow_cc.so\" is already loaded and re-uses it rather than running its static initializers again. Trying to load a shared object twice in the same program seems very strange to me.\r\n\r\nThe other option is to statically link TensorFlow into each plugin, which should be doable.\r\n\r\nI don't see why an intermediate option between fully dynamic/shared and fully static would be useful, but please let me know if you disagree.", "I would be happy with static linking, in fact this is what I am doing under windows, I just do not have that working under OSX and Linux.\r\n\r\nThanks for your help, I will see what i can do, but I thought it was worth reporting if anybody else has  a similar issue.", "Hi this is preventing me to get my Plugin working in Autodesk Flame.\r\n\r\nThe error message I get is as follows:\r\n2019-04-23 21:52:58.662294: F tensorflow/stream_executor/host/host_platform.cc:94] Non-OK-status: MultiPlatformManager::RegisterPlatform(std::move(platform)) status: Internal: platform is already registered with name: \"Host\"\r\n\r\nAnd a segmentation fault, they are linked against libtensorflow.so and libtensorflow_framework.so\r\n\r\nI am not sure the versions they are using but I have been using v1.12.0\r\n\r\nsam", "@allenlavoie \r\n\r\n> I'm not familiar with OpenFX, but by \"normal\" dynamic linking I mean several shared objects (/plugins) each reference a \"libtensorflow_cc.so\" by name. When the second plugin shared object is loaded it sees that \"libtensorflow_cc.so\" is already loaded and re-uses it rather than running its static initializers again. Trying to load a shared object twice in the same program seems very strange to me.\r\n> \r\n> The other option is to statically link TensorFlow into each plugin, which should be doable.\r\n> \r\n> I don't see why an intermediate option between fully dynamic/shared and fully static would be useful, but please let me know if you disagree.\r\n\r\nHere is a stacktrace of the problem I am having\r\n\r\n```\r\nfound non-cached binary /Library/OFX/Plugins/rotobot.ofx.bundle/Contents/MacOS-x86-64/rotobot.ofx\r\n2019-04-24 17:22:40.792677: F tensorflow/stream_executor/host/host_platform.cc:94] Non-OK-status: MultiPlatformManager::RegisterPlatform(std::move(platform)) status: Internal: platform is already registered with name: \"Host\"\r\nProcess 73411 stopped\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT\r\n    frame #0: 0x00007fff5b8a7b66 libsystem_kernel.dylib`__pthread_kill + 10\r\nlibsystem_kernel.dylib`__pthread_kill:\r\n->  0x7fff5b8a7b66 <+10>: jae    0x7fff5b8a7b70            ; <+20>\r\n    0x7fff5b8a7b68 <+12>: movq   %rax, %rdi\r\n    0x7fff5b8a7b6b <+15>: jmp    0x7fff5b89eae9            ; cerror_nocancel\r\n    0x7fff5b8a7b70 <+20>: retq   \r\nTarget 0: (flame) stopped.\r\n(lldb) bt\r\n* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT\r\n  * frame #0: 0x00007fff5b8a7b66 libsystem_kernel.dylib`__pthread_kill + 10\r\n    frame #1: 0x00007fff5ba72080 libsystem_pthread.dylib`pthread_kill + 333\r\n    frame #2: 0x00007fff5b8031ae libsystem_c.dylib`abort + 127\r\n    frame #3: 0x00000002ebb3fb30 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32\r\n    frame #4: 0x00000002ebb3fb40 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16\r\n    frame #5: 0x00000002ebdc780a libtensorflow_framework.so`_GLOBAL__sub_I_host_platform.cc + 314\r\n    frame #6: 0x0000000109fb0ac6 dyld`ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 420\r\n    frame #7: 0x0000000109fb0cf6 dyld`ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\r\n    frame #8: 0x0000000109fac218 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 330\r\n    frame #9: 0x0000000109fac1ab dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\r\n    frame #10: 0x0000000109fac1ab dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\r\n    frame #11: 0x0000000109fab34e dyld`ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 134\r\n    frame #12: 0x0000000109fab3e2 dyld`ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 74\r\n    frame #13: 0x0000000109f9f3e5 dyld`dyld::runInitializers(ImageLoader*) + 82\r\n    frame #14: 0x0000000109fa80a8 dyld`dlopen + 527\r\n    frame #15: 0x00007fff5b758d86 libdyld.dylib`dlopen + 86\r\n    frame #16: 0x000000010619d730 flame`___lldb_unnamed_symbol224514$$flame + 48\r\n    frame #17: 0x000000010619d8b8 flame`___lldb_unnamed_symbol224517$$flame + 24\r\n    frame #18: 0x00000001061b20b1 flame`___lldb_unnamed_symbol224758$$flame + 81\r\n    frame #19: 0x00000001061b6503 flame`___lldb_unnamed_symbol224790$$flame + 131\r\n    frame #20: 0x00000001061b39e0 flame`___lldb_unnamed_symbol224775$$flame + 1712\r\n    frame #21: 0x00000001061b3faa flame`___lldb_unnamed_symbol224777$$flame + 138\r\n    frame #22: 0x0000000101987288 flame`___lldb_unnamed_symbol89425$$flame + 488\r\n    frame #23: 0x0000000101987080 flame`___lldb_unnamed_symbol89424$$flame + 288\r\n    frame #24: 0x00000001006d6696 flame`___lldb_unnamed_symbol23438$$flame + 2678\r\n    frame #25: 0x0000000100d6a73c flame`___lldb_unnamed_symbol47787$$flame + 60\r\n    frame #26: 0x0000000100d6bf68 flame`___lldb_unnamed_symbol47838$$flame + 56\r\n    frame #27: 0x00000001000233a5 flame`___lldb_unnamed_symbol97$$flame + 1189\r\n    frame #28: 0x000000010017770c flame`___lldb_unnamed_symbol3661$$flame + 13244\r\n    frame #29: 0x00007fff5b757015 libdyld.dylib`start + 1\r\n```\r\n\r\nsee:\r\nhttps://opensource.apple.com/source/dyld/dyld-43/src/ImageLoader.h.auto.html\r\n\r\nSo what were you suggesting?\r\n\r\nsam", "Here is me requesting static linking\r\n\r\nhttps://github.com/bazelbuild/bazel/issues/1920#issuecomment-381360017"]}, {"number": 22770, "title": "crash via tf_should_use format_stack", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary (pip)\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 680 (will not be used)\r\n- **Exact command to reproduce**: -\r\n\r\n\r\n### Describe the problem\r\nWhen `__repr__` is called on some TF objects at the wrong time, this can lead to a crash (seg fault; see below). There can be various reasons why this can happen, e.g. when a debugger shows the locals of all threads. My case was this, but I think this doesn't matter:\r\n\r\n- Via [better_exchook](https://github.com/albertz/py_better_exchook), I extended the output of `sys.excepthook` and some `traceback` functions to print out some local vars and their `__repr__` output. There is something similar for IPython.\r\n- I created some `tf.TensorArray` and called `unstack` and I did not use the result value. That `unstack` method is wrapped via `should_use_result`.\r\n- The Python GC called the `_TFShouldUseHelper.__del__` function at some random point, and this triggered the stack formating and then the call some some `__repr__` of some TF objects.\r\n\r\nOriginally, this happened at exit, and I thought that probably it's just not safe at exit to touch any existing TF objects. So I fixed that case in better_exchook: It will not print any vars at exit. A test case to reproduce exactly that case is [here](https://github.com/albertz/playground/blob/master/test-tf111-tfshoulduse-crash.py).\r\n\r\nHowever, now I get the same crash also not at exit but at another random point (see stack below). It will be hard to come up with a test case for this, as it is very non-deterministic when exactly the GC runs and calls the `__del__` function.\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\nCurrent thread 0x00007f14209e8700 (most recent call first):\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1897 in name\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 352 in name\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 614 in __repr__\r\n  File \"/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py\", line 250 in pretty_print\r\n  File \"/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py\", line 487 in format_py_obj\r\n  File \"/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py\", line 571 in <lambda>\r\n  File \"/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py\", line 522 in _trySet\r\n  File \"/u/zeyer/setups/librispeech/2018-02-26--att/returnn/tests/../better_exchook.py\", line 571 in format_tb\r\n  File \"/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/traceback.py\", line 37 in format_list\r\n  File \"/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/traceback.py\", line 193 in format_stack\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 60 in __del__\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 81 in __init__\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4181 in _add_device_to_stack\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4243 in device\r\n  File \"/u/zeyer/.linuxbrew/opt/python3/lib/python3.6/contextlib.py\", line 81 in __enter__\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3366 in _GroupControlDeps\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3415 in group\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 3486 in tuple\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 791 in _GradientsHelper\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 596 in gradients\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 517 in compute_gradients\r\n  File \"/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 401 in minimize\r\n  File \"tests/test_TFNetworkRecLayer.py\", line 219 in test_rhn_nan\r\n  File \"tests/test_TFNetworkRecLayer.py\", line 2175 in <module>\r\n```\r\n\r\n\"ops.py\", line 1897 in name, that is this code:\r\n\r\n```\r\n  @property\r\n  def name(self):\r\n    \"\"\"The full name of this operation.\"\"\"\r\n    return c_api.TF_OperationName(self._c_op)\r\n```\r\n\r\nI often also see this just before the crash:\r\n\r\n    pure virtual method called\r\n\r\nA Travis log with this crash can also be seen [here](https://travis-ci.org/rwth-i6/returnn/jobs/437693971), or [here](https://travis-ci.org/rwth-i6/returnn/jobs/437716636).\r\n\r\nThe C backtrace is this:\r\n\r\n```\r\n/lib/x86_64-linux-gnu/libpthread.so.0(raise+0x29)[0x7f7e8df1a269]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f7e8df1a390]\r\n/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TF_OperationName+0xa)[0x7f7e5ccc0eca]\r\n/u/zeyer/py-envs/py36-tf111/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x1982264)[0x7f7e5ca78264]\r\n/u/zeyer/.linuxbrew/Cellar/python3/3.6.3/lib/libpython3.6m.so.1.0(_PyCFunction_FastCallDict+0x209)[0x7f7e8e1f61c9]\r\n...\r\n```\r\n\r\n", "comments": ["@ebrevdo any idea what could be causing this?", "My guess: Some Swig internals, which do not expect a thread change in certain context (which is triggered here by the Python GC calling `__del__` in some unexpected context).", "@allenlavoie may have insight.", "Nothing jumps out to me as an obvious cause. Sounds like this needs debugging, and without a more concrete reproduction I'm not sure there's much to be done.\r\n\r\nIs there a loop you can construct which eventually results in this bug being triggered?", "I'm having a hard time replicating the issue.  I ran:\r\n\r\n$ python3 --version\r\nPython 3.5.3\r\n\r\n$ python3 -c 'import tensorflow; print(tensorflow.__version__)'\r\n1.13.0-dev20181121\r\n\r\n$ python3 test-tf111-tfshoulduse-crash.py\r\n\r\n2018-11-21 15:59:19.821636: I\r\ntensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\r\ninstructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\ncreate graph\r\nWARNING:tensorflow:From\r\n/home/ebrevdo/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263:\r\ncolocate_with (from tensorflow.python.framework.ops) is deprecated and will\r\nbe removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nvariables:\r\n[<tf.Variable 'b:0' shape=(10, 1, 6) dtype=float32_ref>]\r\ninit vars\r\ngraph size: 8668\r\ntrain\r\nstep 0, loss: 1.596843\r\nEXCEPTION\r\nTraceback (most recent call last):\r\n  File \"test-tf111-tfshoulduse-crash.py\", line 217, in test\r\n    line: raise Exception(\"foo\")\r\n    locals:\r\n      Exception = <builtin> <class 'Exception'>\r\nException: foo\r\nExit.\r\natexit handler\r\nEXCEPTION\r\nTraceback (most recent call last):\r\n(Exclude vars because we are exiting.)\r\n  File \"test-tf111-tfshoulduse-crash.py\", line 229, in at_exit_handler\r\n    line: raise Exception(\"foo\")\r\nException: foo\r\nDummy Goodbye\r\nERROR:tensorflow:==================================\r\nObject was never used (type <class\r\n'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\r\n<tensorflow.python.ops.tensor_array_ops.TensorArray object at\r\n0x7f2390b5b6d8>\r\nIf you want to mark it as used call its \"mark_used()\" method.\r\nIt was originally created here:\r\n  File \"test-tf111-tfshoulduse-crash.py\", line 240, in <module>    line:\r\nprint(\"Exit.\")  File \"test-tf111-tfshoulduse-crash.py\", line 219, in test\r\n  line: sys.excepthook(*sys.exc_info())  File\r\n\"/home/ebrevdo/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py\",\r\nline 189, in wrapped    line: return _add_should_use_warning(fn(*args,\r\n**kwargs))\r\n==================================\r\n...\r\n\r\nI had a similar successful run with TF nightly from september.\r\n", "You used the better_exchook version which includes the workaround for this\ncase. Can you try an older version?\n\n\nAm Do., 22. Nov. 2018, 08:03 hat ebrevdo <notifications@github.com>\ngeschrieben:\n\n> I'm having a hard time replicating the issue. I ran:\n>\n> ```\n> $ python3 --version\n> Python 3.5.3\n>\n> $ python3 -c 'import tensorflow; print(tensorflow.__version__)'\n> 1.13.0-dev20181121\n>\n> $ python3 test-tf111-tfshoulduse-crash.py\n>\n> 2018-11-21 15:59:19.821636: I\n> tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\n> instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n> create graph\n> WARNING:tensorflow:From\n>\n> /home/ebrevdo/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263:\n> colocate_with (from tensorflow.python.framework.ops) is deprecated and will\n> be removed in a future version.\n> Instructions for updating:\n> Colocations handled automatically by placer.\n> variables:\n> [<tf.Variable 'b:0' shape=(10, 1, 6) dtype=float32_ref>]\n> init vars\n> graph size: 8668\n> train\n> step 0, loss: 1.596843\n> EXCEPTION\n> Traceback (most recent call last):\n> File \"test-tf111-tfshoulduse-crash.py\", line 217, in test\n> line: raise Exception(\"foo\")\n> locals:\n> Exception = <builtin> <class 'Exception'>\n> Exception: foo\n> Exit.\n> atexit handler\n> EXCEPTION\n> Traceback (most recent call last):\n> (Exclude vars because we are exiting.)\n> File \"test-tf111-tfshoulduse-crash.py\", line 229, in at_exit_handler\n> line: raise Exception(\"foo\")\n> Exception: foo\n> Dummy Goodbye\n> ERROR:tensorflow:==================================\n> Object was never used (type <class\n> 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n> <tensorflow.python.ops.tensor_array_ops.TensorArray object at\n> 0x7f2390b5b6d8>\n> If you want to mark it as used call its \"mark_used()\" method.\n> It was originally created here:\n> File \"test-tf111-tfshoulduse-crash.py\", line 240, in <module> line:\n> print(\"Exit.\") File \"test-tf111-tfshoulduse-crash.py\", line 219, in test\n> line: sys.excepthook(*sys.exc_info()) File\n>\n> \"/home/ebrevdo/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py\",\n> line 189, in wrapped line: return _add_should_use_warning(fn(*args,\n> **kwargs))\n> ==================================\n> ...\n> ```\n>\n> I had a similar successful run with TF nightly from september.\n>\n> On Wed, Nov 21, 2018 at 10:57 AM, Alfred Sorten Wolf <\n> notifications@github.com> wrote:\n>\n> > Nagging Assignee @ebrevdo <https://github.com/ebrevdo>: It has been 29\n> > days with no activity and this issue has an assignee. Please update the\n> > label and/or status accordingly.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/22770#issuecomment-440774695\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/ABtim9IkzbUaa0mtj5s1WZtMyuWB79Mjks5uxaIxgaJpZM4XKiRX\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22770#issuecomment-440855291>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADm_Og_cTU0HLyNrYC9i4JFTfYNy37nks5uxenKgaJpZM4XKiRX>\n> .\n>\n", "I removed better_exchook (removed the import, and the two commands in main) and still am no able to replicate in py3.5", "See my earlier explanation. Only with better_exchook you can trigger this\ncrash.\n\n\nAm Do., 22. Nov. 2018, 08:34 hat ebrevdo <notifications@github.com>\ngeschrieben:\n\n> I removed better_exchook (removed the import, and the two commands in\n> main) and still am no able to replicate in py3.5\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22770#issuecomment-440864313>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AADm_IWoiR4eA4WIOBZ5tbeIXC5NLRDSks5uxfEFgaJpZM4XKiRX>\n> .\n>\n", "oh i see; an older version of better_exchook.  checking...\n\nOn Wed, Nov 21, 2018 at 4:36 PM, Albert Zeyer <notifications@github.com>\nwrote:\n\n> See my earlier explanation. Only with better_exchook you can trigger this\n> crash.\n>\n>\n> Am Do., 22. Nov. 2018, 08:34 hat ebrevdo <notifications@github.com>\n> geschrieben:\n>\n> > I removed better_exchook (removed the import, and the two commands in\n> > main) and still am no able to replicate in py3.5\n> >\n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 22770#issuecomment-440864313>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AADm_\n> IWoiR4eA4WIOBZ5tbeIXC5NLRDSks5uxfEFgaJpZM4XKiRX>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22770#issuecomment-440865763>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1vtZRsHzzizd3uF2jUcJkLmD023ks5uxfGJgaJpZM4XKiRX>\n> .\n>\n", "ok i was able to replicate the issue.  gonna see if i can run this under address sanitizer...", "OK; asan picked something up:\r\n\r\n```\r\nColocations handled automatically by placer.\r\nvariables:\r\n[<tf.Variable 'b:0' shape=(10, 1, 6) dtype=float32_ref>]\r\ninit vars\r\ngraph size: 8668\r\ntrain\r\nstep 0, loss: 1.596843\r\nEXCEPTION\r\nTraceback (most recent call last):\r\n  File \"test-tf111-tfshoulduse-crash.py\", line 75, in test\r\n    line: raise Exception(\"foo\")\r\n    locals:\r\n      Exception = <builtin> <class 'Exception'>\r\nException: foo\r\nExit blah.\r\natexit handler\r\nEXCEPTION\r\nTraceback (most recent call last):\r\n  File \"test-tf111-tfshoulduse-crash.py\", line 87, in at_exit_handler\r\n    line: raise Exception(\"foo\")\r\n    locals:\r\n      Exception = <builtin> <class 'Exception'>\r\nException: foo\r\nDummy Goodbye\r\n=================================================================\r\n==229269==ERROR: AddressSanitizer: heap-use-after-free on address 0x62500077e338 at pc 0x55e64a6b258e bp 0x7ffffdcce0c0 sp 0x7ffffdcce0b8\r\nREAD of size 8 at 0x62500077e338 thread T0\r\n    #0 0x55e64a6b258d in std::__shared_ptr<tensorflow::NodeProperties, (__gnu_cxx::_Lock_policy)2>::operator->() const crosstool/stable/toolchain/bin/../lib/gcc/x86_64-linux-gnu/version/../../../../x86_64-linux-gnu/include/c++/version/bits/shared_ptr_base.h:1046:9\r\n    #1 0x55e64a6a58df in tensorflow::Node::name() const tensorflow/core/graph/graph.cc:159:43\r\n    #2 0x55e63bd73458 in TF_OperationName tensorflow/c/c_api.cc:1418:21\r\n    #3 0x7fc5b33dfa94 in _wrap_TF_OperationName(_object*, _object*) bazel-out/asan-py3-dbg/genfiles/tensorflow/python/_third__party_tensorflow_python_pywrap__tensorflow__internal.cc:12098:22\r\n    #4 0x55e64d31f265 in _PyCFunction_FastCallDict python_runtime/v3_6/Objects/methodobject.c:234:22\r\n    #5 0x55e64d3a407d in call_function python_runtime/v3_6/Python/ceval.c:4837:9\r\n    #6 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #7 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #8 0x55e64d39bd63 in PyEval_EvalCodeEx python_runtime/v3_6/Python/ceval.c:4187:12\r\n    #9 0x55e64d307a85 in function_call python_runtime/v3_6/Objects/funcobject.c:604:14\r\n    #10 0x55e64d2d5e2a in PyObject_Call python_runtime/v3_6/Objects/abstract.c:2261:14\r\n    #11 0x55e64d2f27d2 in property_descr_get python_runtime/v3_6/Objects/descrobject.c:1384:11\r\n    #12 0x55e64d322ba8 in _PyObject_GenericGetAttrWithDict python_runtime/v3_6/Objects/object.c:1066:19\r\n    #13 0x55e64d39ff59 in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:2872:29\r\n    #14 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #15 0x55e64d39bd63 in PyEval_EvalCodeEx python_runtime/v3_6/Python/ceval.c:4187:12\r\n    #16 0x55e64d307a85 in function_call python_runtime/v3_6/Objects/funcobject.c:604:14\r\n    #17 0x55e64d2d5e2a in PyObject_Call python_runtime/v3_6/Objects/abstract.c:2261:14\r\n    #18 0x55e64d2f27d2 in property_descr_get python_runtime/v3_6/Objects/descrobject.c:1384:11\r\n    #19 0x55e64d322ba8 in _PyObject_GenericGetAttrWithDict python_runtime/v3_6/Objects/object.c:1066:19\r\n    #20 0x55e64d39ff59 in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:2872:29\r\n    #21 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #22 0x55e64d39bd63 in PyEval_EvalCodeEx python_runtime/v3_6/Python/ceval.c:4187:12\r\n    #23 0x55e64d307a85 in function_call python_runtime/v3_6/Objects/funcobject.c:604:14\r\n    #24 0x55e64d2d5e2a in PyObject_Call python_runtime/v3_6/Objects/abstract.c:2261:14\r\n    #25 0x55e64d2f27d2 in property_descr_get python_runtime/v3_6/Objects/descrobject.c:1384:11\r\n    #26 0x55e64d322ba8 in _PyObject_GenericGetAttrWithDict python_runtime/v3_6/Objects/object.c:1066:19\r\n    #27 0x55e64d39ff59 in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:2872:29\r\n    #28 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #29 0x55e64d2d60a1 in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2310:18\r\n    #30 0x55e64d2d6295 in _PyObject_Call_Prepend python_runtime/v3_6/Objects/abstract.c:2373:14\r\n    #31 0x55e64d2d605b in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2331:18\r\n    #32 0x55e64d335e89 in slot_tp_repr python_runtime/v3_6/Objects/typeobject.c:6127:15\r\n    #33 0x55e64d321d26 in PyObject_Repr python_runtime/v3_6/Objects/object.c:490:11\r\n    #34 0x55e64d32edca in tuplerepr python_runtime/v3_6/Objects/tupleobject.c:303:13\r\n    #35 0x55e64d321d26 in PyObject_Repr python_runtime/v3_6/Objects/object.c:490:11\r\n    #36 0x55e64d31f3b2 in _PyCFunction_FastCallDict python_runtime/v3_6/Objects/methodobject.c\r\n    #37 0x55e64d3a407d in call_function python_runtime/v3_6/Python/ceval.c:4837:9\r\n    #38 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #39 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #40 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #41 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #42 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #43 0x55e64d3a519b in fast_function python_runtime/v3_6/Python/ceval.c:4978:12\r\n    #44 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #45 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #46 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #47 0x55e64d3a519b in fast_function python_runtime/v3_6/Python/ceval.c:4978:12\r\n    #48 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #49 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #50 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #51 0x55e64d3a519b in fast_function python_runtime/v3_6/Python/ceval.c:4978:12\r\n    #52 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #53 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #54 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #55 0x55e64d3a519b in fast_function python_runtime/v3_6/Python/ceval.c:4978:12\r\n    #56 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #57 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #58 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #59 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #60 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #61 0x55e64d3a4a82 in _PyEval_EvalCodeWithName python_runtime/v3_6/Python/ceval.c:4166:14\r\n    #62 0x55e64d3a519b in fast_function python_runtime/v3_6/Python/ceval.c:4978:12\r\n    #63 0x55e64d3a405a in call_function python_runtime/v3_6/Python/ceval.c:4858:17\r\n    #64 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #65 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #66 0x55e64d2d60a1 in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2310:18\r\n    #67 0x55e64d2d6295 in _PyObject_Call_Prepend python_runtime/v3_6/Objects/abstract.c:2373:14\r\n    #68 0x55e64d2d605b in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2331:18\r\n    #69 0x55e64d336a4f in slot_tp_finalize python_runtime/v3_6/Objects/typeobject.c:6463:15\r\n    #70 0x55e64cd7e172 in finalize_garbage python_runtime/v3_6/Modules/gcmodule.c:806:13\r\n    #71 0x55e64cd7b1b7 in collect python_runtime/v3_6/Modules/gcmodule.c:1005:5\r\n    #72 0x55e64cd7ad19 in collect_with_callback python_runtime/v3_6/Modules/gcmodule.c:1128:14\r\n    #73 0x55e64cd7ab60 in PyGC_Collect python_runtime/v3_6/Modules/gcmodule.c:1594:13\r\n    #74 0x55e64d3cb556 in Py_FinalizeEx python_runtime/v3_6/Python/pylifecycle.c:601:5\r\n\r\n0x62500077e338 is located 2616 bytes inside of 8192-byte region [0x62500077d900,0x62500077f900)\r\nfreed by thread T0 here:\r\n    #0 0x55e62bb55ac2 in __interceptor_free llvm/llvm/projects/compiler-rt/lib/asan/asan_malloc_linux.cc:124:3\r\n    #1 0x55e64aa37578 in tensorflow::core::Arena::~Arena() tensorflow/core/lib/core/arena.cc:66:5\r\n    #2 0x55e64a6a928b in tensorflow::Graph::~Graph() tensorflow/core/graph/graph.cc:372:1\r\n    #3 0x55e63bd873e5 in TF_Graph::~TF_Graph() tensorflow/c/c_api_internal.h:75:8\r\n    #4 0x55e63bd7fb9d in TF_DeleteSession tensorflow/c/c_api.cc:2588:14\r\n    #5 0x7fc5b33f65dc in _wrap_TF_DeleteSession(_object*, _object*) bazel-out/asan-py3-dbg/genfiles/tensorflow/python/_third__party_tensorflow_python_pywrap__tensorflow__internal.cc:16303:5\r\n    #6 0x55e64d31f265 in _PyCFunction_FastCallDict python_runtime/v3_6/Objects/methodobject.c:234:22\r\n    #7 0x55e64d3a407d in call_function python_runtime/v3_6/Python/ceval.c:4837:9\r\n    #8 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #9 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #10 0x55e64d2d60a1 in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2310:18\r\n    #11 0x55e64d2d6295 in _PyObject_Call_Prepend python_runtime/v3_6/Objects/abstract.c:2373:14\r\n    #12 0x55e64d2d605b in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2331:18\r\n    #13 0x55e64d336a4f in slot_tp_finalize python_runtime/v3_6/Objects/typeobject.c:6463:15\r\n    #14 0x55e64cd7e172 in finalize_garbage python_runtime/v3_6/Modules/gcmodule.c:806:13\r\n    #15 0x55e64cd7b1b7 in collect python_runtime/v3_6/Modules/gcmodule.c:1005:5\r\n    #16 0x55e64cd7ad19 in collect_with_callback python_runtime/v3_6/Modules/gcmodule.c:1128:14\r\n    #17 0x55e64cd7ab60 in PyGC_Collect python_runtime/v3_6/Modules/gcmodule.c:1594:13\r\n    #18 0x55e64d3cb556 in Py_FinalizeEx python_runtime/v3_6/Python/pylifecycle.c:601:5\r\n\r\npreviously allocated by thread T0 here:\r\n    #0 0x55e62bb56ac9 in __interceptor_posix_memalign llvm/llvm/projects/compiler-rt/lib/asan/asan_malloc_linux.cc:219:3\r\n    #1 0x55e62ebbd292 in aligned_malloc(unsigned long, unsigned long) base/port.h:897:7\r\n    #2 0x55e64aa37200 in tensorflow::core::Arena::Arena(unsigned long) tensorflow/core/lib/core/arena.cc:54:31\r\n    #3 0x55e64a6a7d4c in tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) tensorflow/core/graph/graph.cc:323:7\r\n    #4 0x55e63bd792bc in TF_Graph::TF_Graph() tensorflow/c/c_api.cc:1854:7\r\n    #5 0x55e63bd7942a in TF_NewGraph tensorflow/c/c_api.cc:1860:38\r\n    #6 0x7fc5b33d3f88 in _wrap_TF_NewGraph(_object*, _object*) bazel-out/asan-py3-dbg/genfiles/tensorflow/python/_third__party_tensorflow_python_pywrap__tensorflow__internal.cc:10165:26\r\n    #7 0x55e64d31f265 in _PyCFunction_FastCallDict python_runtime/v3_6/Objects/methodobject.c:234:22\r\n    #8 0x55e64d3a407d in call_function python_runtime/v3_6/Python/ceval.c:4837:9\r\n    #9 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #10 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #11 0x55e64d2d60a1 in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2310:18\r\n    #12 0x55e64d2d6295 in _PyObject_Call_Prepend python_runtime/v3_6/Objects/abstract.c:2373:14\r\n    #13 0x55e64d2d5e2a in PyObject_Call python_runtime/v3_6/Objects/abstract.c:2261:14\r\n    #14 0x55e64d336908 in slot_tp_init python_runtime/v3_6/Objects/typeobject.c:6420:11\r\n    #15 0x55e64d331fc8 in type_call python_runtime/v3_6/Objects/typeobject.c:915:19\r\n    #16 0x55e64d2d605b in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2331:18\r\n    #17 0x55e64d3a4053 in call_function python_runtime/v3_6/Python/ceval.c:4861:17\r\n    #18 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #19 0x55e64d3a5606 in _PyFunction_FastCall python_runtime/v3_6/Python/ceval.c:4919:14\r\n    #20 0x55e64d2d60a1 in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2310:18\r\n    #21 0x55e64d2d6295 in _PyObject_Call_Prepend python_runtime/v3_6/Objects/abstract.c:2373:14\r\n    #22 0x55e64d2d5e2a in PyObject_Call python_runtime/v3_6/Objects/abstract.c:2261:14\r\n    #23 0x55e64d336908 in slot_tp_init python_runtime/v3_6/Objects/typeobject.c:6420:11\r\n    #24 0x55e64d331fc8 in type_call python_runtime/v3_6/Objects/typeobject.c:915:19\r\n    #25 0x55e64d2d605b in _PyObject_FastCallDict python_runtime/v3_6/Objects/abstract.c:2331:18\r\n    #26 0x55e64d3a4053 in call_function python_runtime/v3_6/Python/ceval.c:4861:17\r\n    #27 0x55e64d3a0fed in _PyEval_EvalFrameDefault python_runtime/v3_6/Python/ceval.c:3335:19\r\n    #28 0x55e64d308a38 in gen_send_ex python_runtime/v3_6/Objects/genobject.c:189:14\r\n    #29 0x55e64d398cf8 in builtin_next python_runtime/v3_6/Python/bltinmodule.c:1330:11\r\n\r\nSUMMARY: AddressSanitizer: heap-use-after-free crosstool/stable/toolchain/bin/../lib/gcc/x86_64-linux-gnu/version/../../../../x86_64-linux-gnu/include/c++/version/bits/shared_ptr_base.h:1046:9 in std::__shared_ptr<tensorflow::NodeProperties, (__gnu_cxx::_Lock_policy)2>::operator->() const\r\nShadow bytes around the buggy address:\r\n  0x0c4a800e7c10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n=>0x0c4a800e7c60: fa fa fa fa fa fa fa[fa]fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7c90: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7ca0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c4a800e7cb0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07\r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==229269==ABORTING\r\n```", "@allenlavoie looks like in this case (`test-tf111-tfshoulduse-crash.py` in python3 with `better_exchook == 20171121.105512`) we attempt to access graph data after it has been deleted, presumably this is caused by an interaction with `tf_should_use` `format_stack`.", "Perhaps we can be more careful about when we call [format_stack](https://github.com/tensorflow/tensorflow/blob/b05573e268d9bedfb6f7bbc20aea570f1800ec64/tensorflow/python/util/tf_should_use.py#L60)?  We do this lazily to avoid the cost of formatting, but is there a way to check that the graph in the stack still exists?", "We could also consider sanitizing the stack before formatting.", "So, to make it clear: There is a Python object which corresponds to a graph in C++ which does not exist anymore, or has become invalid? How is this possible? This is via Swig, right? I thought that Swig does some sort of reference counting.\r\n\r\nOr does the C++ graph object itself still exists, but accessing it becomes invalid? Is there a flag or so that marks that the object is invalid now? Maybe there should just be a check for this flag and if the object is invalid, any related functions should return some sane value (None or so) or throw a Python exception, instead of this crash?\r\n\r\nI feel like cleaning/sanitizing the stack trace to try to avoid any possible access to such objects is just a workaround to the problem.", "I tried to write some simpler test case. See the commit I just referenced. That code sometimes crashes in various different way.\r\n", "Oh interesting, good find. So maybe we just need to set some Python properties to `None` when the destructor for the C Graph object runs? https://github.com/tensorflow/tensorflow/blob/73f193aa4b9999e0a5bf7d29b1838e2a662e9507/tensorflow/python/framework/c_api_util.py#L52"]}, {"number": 22710, "title": "[Cloud TPU] Intermittent freezes requiring reset of the TPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWe are seeing intermittent freezes of the Cloud TPU when running VGG19 inference from the Julia frontend via xrt (do note that we're also seeing incorrect answers, which is filed as #22709 and may or may not be related). If the Cloud TPU freezes, the `TF_SessionRun` call never returns. An aggravating, but non-essential factor is the concurrent execution of the TPU profiler via `capture_tpu_profile`. In that situation both the main program and the `capture_tpu_profile` invocation never exit. If the profiler is running, we usually see only 2-3 successful runs until things start hanging. If I manually disconnect and forcefully kill the session (by severing the socket) and then reconnect, I see either continued freezes or errors of the form\r\n\r\n```\r\nERROR: Tensorflow error: Status: Unable to enqueue when not opened, queue: [0000:00:04.0 PE0 C0 MC0 TN0 Queue HBM_WRITE]. State is: CLOSED\r\n\t [[{{node XRTAllocate}} = XRTAllocate[_device=\"/job:tpu_worker/replica:0/task:0/device:TPU:0\"](XRTAllocate/Const_G1)]]\r\n```\r\n\r\nmy standard protocol to recover from this has been the following:\r\n- Reconnect the session\r\n- Run the `ShutdownDistributedTPU` op (Remote session closes)\r\n- Reconnect the session\r\n- Run the `ShutdownDistributedTPU` op (Causes a non-fatal error, but the next step hangs if not done)\r\n- Run the `ConfigureDistributedTPU` op (works)\r\n\r\nafterwards I can usually use the TPU again.\r\n\r\n### Source code / logs\r\nXLA dump (batch size N=1, but the same happens for at least N=10 and N=100): https://gist.github.com/Keno/b54e4be146096daf4d464c1319639404\r\n", "comments": []}, {"number": 22655, "title": "C++ low level API documentation", "body": "I am unable to find the C++ low level API documentation, please advise.", "comments": ["https://www.tensorflow.org/api_docs/cc/ and the source code. Also, this is a question more appropriate for [stack overflow ](https://stackoverflow.com/)", "https://www.tensorflow.org/api_docs/cc/ points to the high level API (ClientSession vs. Session) for example.", "The test scripts of C++  APIs may be helpful to learn the C++ low-level API.", "https://www.tensorflow.org/api_docs/cc/ points to the high level API docs. I am looking for the low level API docs that includes Session (low-level API) vs (ClientSession high level API)"]}, {"number": 22598, "title": "Empty tfrecord in gcs would fail the TFRecordDataset.read", "body": "The behaviour of gcs(google cloud file storage) file system is different than local file system. \r\n\r\nas `tf.gfile.Open('local_empty.tfrecord').read(100000000) == ''`\r\nwhile `tf.gfile.Open('gcs_empty_file.tfrecord').read(1000000)=='Out[11]: \"<?xml version='1.0' encoding='UTF-8'?><Error><Code>InvalidRange</Code><Message>The requested range cannot be satisfied.</Message><Details>bytes=some number</Details></Error>\"'`\r\n\r\nso when we use `tf.data.TFRecordDataset` to process the tfrecord in the gcs, there would be \r\n`DataLossError`\r\n\r\nsuch as here https://github.com/tensorflow/tensorflow/issues/13463\r\n---\r\n\r\n", "comments": ["@saeta This pairs well with #22599.", "@martinwicke  so it's from the gcs fs", "@saeta are you looking into this, or do we need to reassign?", "Thanks for the ping @martinwicke ! Unfortunately, I do not expect to have bandwidth to look into this. @rxsang might be able to help out? CC @jhseu ", "@henrytansetiawan Would you be able to take a look at this issue?"]}, {"number": 22541, "title": "Java process crashes during model loading", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: AWS c5.xlarge, Amazon Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: java binary\r\n- **TensorFlow version (use command below)**: java org.tensorflow:tensorflow:1.11.0-rc2\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI run the following program on `c5.xlarge` (CPU only, no GPU) with 8GB RAM and 12GB swap memory.\r\n```java\r\npublic class TryOut {\r\n    public static void main(String[] args) {\r\n        SavedModelBundle[] loadedModels = new SavedModelBundle[15];\r\n        for (int i = 0; i < 15; i++) {\r\n            System.out.println(\"loading model \" + i);\r\n            loadedModels[i] = SavedModelBundle.load(\"models/0\", \"serve\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n`models/0` contains the model [faster_rcnn_resnet101_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz) from the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\r\nHere is the directory structure:\r\n```sh\r\nmodels/0/\r\n\u2514\u2500\u2500 saved_model.pb\r\n```\r\n\r\nI run the program as `java -jar mem-test.jar`.\r\n\r\n\r\n#### Problem\r\nProcess crashes during model loading when the free memory becomes low (see `output.txt`).\r\nThe process does not try to utilize available swap memory.\r\n\r\nThis behavior is reproducible with other models from the model zoo.\r\n\r\nOn the other hand, `tensorflow-serving` can download multiple models into memory, utilizes swap and runs inferences without crashing as long as there is swap memory available.\r\n\r\n### Source code / logs\r\n\r\nSee also the attached files:\r\n* [output.txt](https://github.com/tensorflow/tensorflow/files/2420367/output.txt)\r\n* [hs_err_pid31150.log](https://github.com/tensorflow/tensorflow/files/2420368/hs_err_pid31150.log)\r\n\r\n", "comments": []}, {"number": 22531, "title": "Android tfLite Shared STL support ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nN/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nAndroid\r\n- **TensorFlow installed from (source or binary)**:\r\nN/A\r\n- **TensorFlow version (use command below)**:\r\nAll version of TFlite\r\n- **Python version**:\r\nN/A\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\nCurrently the final .so files from the android build are produced linking against the libc++_static.a library, this has the effect of compiling a large amount of code into the tfLite shared library.\r\n\r\nAndroid also supports linking against a libc++_shared.so version of the STL (as of NDK 18 libc++ is the ONLY supported version of the stl). \r\n\r\nThe main reason for this is that we also have a few other native modules that currently link against libc++_shared.so so it would be handy to reduce the size of the tflite.so library we have to package into the final APK.\r\n\r\nIt would be VERY handy to have two versions of the AAR tfLite package produced, the current version with a static version of libc++ and another version linking against libc++_shared.so \r\n\r\nNDK Docs for libc++:\r\nhttps://developer.android.com/ndk/guides/cpp-support#libc\r\n\r\nCurrent tfLite AAR package: \r\nhttps://bintray.com/google/tensorflow/tensorflow-lite/1.10.0\r\n\r\nmaybe a new package could be something like: \r\norg.tensorflow:tensorflow-lite-shared:1.10.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "updated with the info ", "Hi @curtpm, is this feature request still of interest?", "Not 100% the same thing, but I would be very interested in having a separate target for the shared `libtensorflowlite.so` (//tensorflow/lite:tensorflowlite) that links against libc++_shared.so provided by the NDK. Might be as simple as changing a flag somewhere but it's hard for me to understand what and where. \r\n\r\nAs far as I can see, other useful targets like the GPU delegate also have a static libc++. If we could link against libc++_shared instead we'd save a lot on the total binary size.", "This is on our radar, but I haven't yet found a way to customize this behavior w/ bazel.", "Can I follow this issue and assume you'll post updates here, or is there another ticket? I understand that my request is a bit different than the OP.", "Yes, will update this issue.\r\n\r\nThanks"]}, {"number": 22498, "title": "Writing a SavedModel on Windows with long file path fails", "body": "### System information\r\n- **Have I written custom code**: Yes, see below.\r\n- **OS Platform and Distribution**: Windows 10 Enterprise 64Bit (10.0.16299)\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from**: Binary\r\n- **TensorFlow version**: b'v1.10.0-rc1-19-g656e7a2b34' 1.10.0\r\n- **Python version**: 3.5.5\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Run the provided script on Windows\r\n\r\n\r\n### Describe the problem\r\nWhen exporting a SavedModel to a directory with a rather long path the maximum file path length of Windows (260 chars) gets exceeded while creating temporary variable files and the process fails because it can't find the file (See attachments for the full log):\r\n```\r\nNotFoundError (see above for traceback): Failed to create a NewWriteableFile: C:\\reallylongpath\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoo\\variables\\variables_temp_cbd5fe79f07a4790a08e7c2adf1ed95b/part-00000-of-00001.data-00000-of-00001.tempstate12448200021531902965 : The system cannot find the path specified.\r\n```\r\nThe usual workaround to prepend `'\\\\\\\\?\\\\'` to the path (like described [here](https://stackoverflow.com/a/3557977)) doesn't work (See attachments).\r\n\r\n### Source code / logs\r\n\r\n__Code to reproduce:__\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Long export path (should be at least 140 chars long)\r\nexport_dir = r'C:\\reallylongpath\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoobarfoobarfoobarfoobarfoo\\barfoobarfoo'\r\nprint('Export dir lenght: {}'.format(len(export_dir)))\r\n\r\n# Build an easy graph with variables\r\nx = tf.constant(2)\r\nw = tf.Variable(3)\r\ny = tf.multiply(x, w)\r\n\r\nwith tf.Session() as sess:\r\n    # Initialize the variable\r\n    sess.run(w.initializer)\r\n\r\n    # Save as a SavedModel\r\n    tf.saved_model.simple_save(sess,\r\n                               export_dir,\r\n                               inputs={'x': x },\r\n                               outputs={'y': y })\r\n```\r\n\r\n__Logs:__\r\nFull log:\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/2414582/log.txt)\r\n\r\nFull log with `'\\\\\\\\?\\\\'` prefix:\r\n[log_prefix_0.txt](https://github.com/tensorflow/tensorflow/files/2414583/log_prefix_0.txt)\r\n\r\nFull log with `'\\\\?\\'` prefix:\r\n[log_prefix_1.txt](https://github.com/tensorflow/tensorflow/files/2414584/log_prefix_1.txt)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "@HedgehogCode In Windows 10 you can enable file path longer than 260 [here](https://betanews.com/2016/05/29/long-paths-windows-10/)", "Hi @wt-huang,\r\nthank you for your reply!\r\nSadly, this setting doesn't help. Note that the description of the setting says:\r\n> Enabling Win32 long paths will allow manifested win32 applications and Windows Store applications to access paths beyond the normal 260 character limit [...]\r\n\r\nThis suggests that it has to be enabled on application level. Maybe [this](https://social.msdn.microsoft.com/Forums/en-US/50ecad58-b8cc-411a-af50-08a67dde5fca/long-name-support-is-not-enabled-on-windows-10?forum=windowssdk) helps (I have to admit that I have no idea about development on windows so I can't say if this is applicable).", "@HedgehogCode This works for NFS in Windows. Make sure you have group policy enabled or just download `gpedit.msc`. Check if `Enable Long File Path` is indeed enabled in `LocalGroupPolicyFolder > Computer Configuration > Administrative Templates > System > Filesystem > NTFS`. Then restart your system. Go to Windows terminal and test it out. ", "> This works for NFS in Windows.\r\n\r\nWhat do you mean by \"for NFS\"? I want to use a local path.\r\n\r\n>  Check if `Enable Long File Path` is indeed enabled in `LocalGroupPolicyFolder > Computer Configuration > Administrative Templates > System > Filesystem > NTFS`\r\n\r\n`Enable Long File Path` is not located in the NTFS folder anymore but in  `LocalGroupPolicyFolder > Computer Configuration > Administrative Templates > System > Filesystem` ([Source](https://superuser.com/a/1119980))\r\n\r\nI checked that the configuration was enabled and restarted the computer again (to be sure) but it still doesn't work. Did you try it?\r\n\r\n(I installed tensorflow via pip in an anaconda environment if this matters)", "@HedgehogCode Yes, I tried and was able to make file path of ~1000 characters long. Here is the link to [NFS](https://docs.microsoft.com/en-us/windows-server/storage/nfs/nfs-overview). You would need to go to the command line to test it out. Please post the error you observed.", "Hi @wt-huang,\r\nusing NFS is not a valid solution for me. I want to save the `SavedModel` to a local NTFS partition mounted at `C:`.\r\nI tested my script as described above from the command line multiple times and still observe the same error as attached to my original post.", "@HedgehogCode You can first try to save the model to a local directory with long file path. If successful, depending on the system setup, you can enable read/write access or write a simple script to automatically transfer files to NTFS partition mounted disk. ", "Hi @wt-huang,\r\n\r\n> You can first try to save the model to a local directory with long file path.\r\n\r\nWith 'local NTFS partition mounted at `C:`' I mean a local directory. `C:` is in my case (as usual) the local partition where Windows is installed.\r\n\r\nI tried it on a freshly installed virtual machine and it still fails with the same error message. New system information:\r\n- **OS Platform and Distribution**: Windows 10 Pro 64Bit (Version: 1803, Build: 17134.286)\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: b'1.11.0-rc2-4-gc19e29306c' 1.11.0\r\n- **Python version**: 3.5.6\r\n\r\n(I enabled the `Enable Long File Path` config and restarted the virtual machine)", "Hi @HedgehogCode, I would try to set this up without virtual machine first. You can use TensorFlow 1.11.0 which is available now. ", "Closing this issue for now, feel free to reopen if problem persists.", "@wt-huang,\r\nSorry for the late reply. I don't use a Windows machine as my daily driver which makes testing this on the Windows machine in our office time-consuming. (That's why I changed to the virtual machine)\r\n\r\nThe problem still persists with TensorFlow 1.11.0 on a natively installed Windows (I get the same error as attached in my original post).\r\n\r\nI think me trying the same thing over and over again doesn't help. Can you provide details about the setup you used when it worked for you? Did you use the script as I provided it or did you modify it slightly? How did you install TensorFlow? Which Windows version did you use?\r\n\r\nIt seems like I don't have the permissions to reopen the issue. Can you reopen it for me?\r\n\r\nThank you!", "@wt-huang,\r\nSorry for the late reply. It is very time-consuming for me to test this because I don't use a Windows machine as my daily driver and need to use the Windows workstation in our office. (This is why I changed to a virtual machine)\r\n\r\nI tested it now with TensorFlow 1.11.0 on the natively installed Windows 10 (like in my original post) and the problem still persists. I don't have the access rights to reopen the issue. Could you please reopen it?\r\n\r\nI think me trying the same thing over and over again doesn't help. Could you provide details on the setup where you tried it successfully? Did you modify the script? Which version of Windows did you use? How did you install TensorFlow?\r\n\r\nThank you!\r\n", "Any news on this?", "I was having this problem with Windows 10 1909, Python 3.6, Tensorflow 2.3.0\r\n\r\nIn my case, enabling the local policy solved the problem\r\n\r\n", "Same issue with Windows 10 2004 & TensorFlow 2.1.0.\r\n\r\nEnabling the Long Path option in Registry Editor solves the problem.\r\n\r\nAnyway is there a solution on Python side to avoid modifying Windows parameters?\r\n\r\nAs mentioned in the original post, adding the UNC prefix `\\\\\\\\?\\\\` or `\\\\?\\` to the \"too long path\" doesn't work."]}, {"number": 22465, "title": "[Feature Request]: tf.data.Dataset.map parallelism autotune enhancement", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n\r\n### Describe the problem\r\nRecently this [PR](https://github.com/tensorflow/tensorflow/commit/c8a0dfc741736a59f8fd1776b71f38619d66da56#diff-df634c8243713c0afd2e05c1689412e2) which leverages the underlying  performance model to find the optimal values for the parallelism knobs, this seems intuitive and really useful for me. While after carefully going through the relevant codebase, there are two problems that have been bothering me.\r\n\r\n(1) Currently dataset.map's num_parallel_calls autotune (as well as [dataset.prefetch buffer size autotune](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/prefetch_autotuner.cc#L36-L55)) only works in the scenario when `kAutotune` guard is set and then start a one-way increment. On the one hand, in actual production environment, careless setting of parameters like `map.num_parllel_calls` or `prefetch.buffer_size` is somewhat common, which may result in inefficient  or even wore resource utilization.  On the other hand, even a carefully-tuned parameter which leverages underlying system info and works good in environment may turn out to be incompatible when we transfer into another environment. Therefore it's nature that our AUTOTUNE logic can support our auto_tune params's autotune based on runtime system info **as well as user's initial params passed in**, in which case the autotune_params' adjustment should be two-way.\r\n\r\n(2) At the specific code implementation level, this PR leverages `port::NumSchedulableCPUs()` to get the number of cores available in the process, and calculate the idealized acceleration brought by parallelism's increment(e.g. [PARALLEL_MAP](https://github.com/tensorflow/tensorflow/blob/c8a0dfc741736a59f8fd1776b71f38619d66da56/tensorflow/core/framework/model.cc#L214-L221)). This should be tuned carefully as in complicated distributed environment the increase of map parallelism perhaps results in invalid scheduling of resources, and eventually **brings in negative optimization**.\r\n\r\nFor the above two points I have figured out some workaround, and the first draft of the code is also being developed. Any comments from you side is hight welcome and much appreciated. @jsimsa \r\n\r\nThanks\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nMobile device", "Agree. The auto tuning of `map.num_parallel_calls` should consider the factors within different environment. The CPU is scarce resource in distributed environment so that we should carefully tune the `num_parallel_calls`  instead of paying attention only to the `map_fn` processing time. @fanshiqing is preparing code in recent days. Any comments is appreciated.", "@fanshiqing \r\n\r\nRe (1): `tf.data.Dataset.prefetch()` buffer size auto-tuning is indeed one-way but this autotuning is currently not related to the performance model related autotuning of parallelism. `tf.data.Dataset.interleave()` and `tf.data.Dataset.map()` parallelism auto-tuning is not one-way. As you pointed out, this mechanism is meant to help users create pipelines that are expected to work across different environments (and data patterns). Adding support for default value provided by the user is certainly something that we could consider but at the moment we do not see need for it.\r\n\r\nRe (2): We plan to soon introduce a mechanism to allow users to configure the CPU budget to be used by the performance modeling for the parallelism auto-tuning.\r\n\r\nIn other words, we are aware of the limitations you have pointed out and are working on addressing some of them. Since your request does not describe a concrete issue and steps how to reproduce it, I am going to close this issue.\r\n", "@jsimsa \r\n\r\nHi Jiri,\r\n\r\nCould please kindly wait for a while to close this issue? Or at least wait for our confirmation?\r\n\r\nAs mentioned in this post, we are already working on some PR preparation based on our observation. \r\nIf it is possible, any deeper interaction would be highly appreciated. Otherwise, we may do some duplicated work. That's why we raise what we observe as early as possible. \r\n\r\nBased on your closing this issue so quickly without any specific input, frankly speaking, we are not sure whether we should go on with our work about the auto-tuning enhancement or not. \r\n\r\nI think if Google wants to get more input from the community, maybe the interaction could be deeper.\r\n\r\nThanks ", "@yangjunpro issues should be created for bugs or feature requests.\r\n\r\nBugs should be accompanied by instructions on how to reproduce and feature requests should identify the desired functionality to be developed.\r\n\r\nAs far as I can tell, this issue is neither a bug nor a feature request. You are welcome to reopen this issue if you encounter a (performance) bug and can share steps on how to reproduce it. If instead you prefer to address what you believe is the limitation yourself, you can create a PR with your contribution when it is ready.", "Please see https://www.tensorflow.org/community/ for details.", "@jsimsa Thanks for your comments. According to your reply, will the consumer speed (namely subsequent training threads) be considered in your performance model in development, or the performance model is just kept unchanged like this now. As feasible negative optimization will actually slow down the consumer's speed in my opinion.", "@jsimsa \r\n\r\nWe have already delivered several PRs to the community so we are familiar with the basic flow.\r\n\r\nFrom my perspective, I think Shiqing already mentioned that he thought this is a feature and he is working on it. I think the title of this issue could be adjusted a little bit to avoid misleading. \r\n\r\nAs to the \"reproduce it\" mentioned by you, the limitation is observed in our in-house cluster environment, I think it is not easy to reproduce it in Google' cluster environment. \r\nThe logic regarding this issue is straightforward when we allocate CPU resources for the IO threads, it may consume too much to block the execution of the execution/communication threads. \r\nWe already started to work on this improvement.  If Google is also already working on it, it's fine for us to move to another target to save RD resource. But I think before making any decision, deeper and more effective communication to ensure the community and Google are at the same page. \r\n\r\nThanks.", "If I understand correctly, you are both pointing out a (performance) bug and requesting a feature (on which you have already started working).\r\n\r\nThe bug is that if the input pipeline threads may allocate CPU too eagerly, which can slow down other computation that is happening on the CPU (e.g. the model computation or data transfer to/from accelerator). You have observed this in your environment but do not know how to reproduce it outside of your environment. TensorFlow is an open-source project and its goal is to address the needs of its community (of which Google is only a part of). The reason we ask for steps to reproduce a problem is to help us determine whether the wider community (not just you or Google) might experience the problem. If your contribution does not solve a general problem, then the project might not benefit much from it.\r\n\r\nThe feature request is that you would like the auto-tuning mechanism to take into account the speed of the consumer. This sounds reasonable and since you have already started working on this I will mark this issue as \"contributions welcome\" and wait for your PR.", "@jsimsa Thanks for your detailed reply, initial PR will be proposed within the next 3-4 days.", "@jsimsa I'm very sorry to inform you that the initial PR will be delayed compared to the plan due to other high priority tasks and Chinese National Day holiday. And I will be back as soon as this PR is ready.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you."]}, {"number": 22464, "title": "memory leak cased by function tf.dynamic_partition", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:_no_\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  _Linux Ubuntu 16.04.4 LTS_\u3002\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:_no_\r\n- **TensorFlow installed from (source or binary)**:_binary_\r\n- **TensorFlow version (use command below)**:_1.8.0_\r\n- **Python version**:_Python 3.5.2_\r\n- **Bazel version (if compiling from source)**:_None_\r\n- **GCC/Compiler version (if compiling from source)**:_None_\r\n- **CUDA/cuDNN version**:_CUDA-9.0/cuDNN-7.0.5_\r\n- **GPU model and memory**: _GTX 1080Ti 11GB, 2 same cards_\r\n- **Exact command to reproduce**:_below_\r\n\r\n### Supplementary material\r\nI just pulled the tensorflow's offical docker image and RUN some `pip install ...` instructions.\r\nThe tag of the iamge is `1.8.0-devel-gpu-py3`.\r\n\r\n### Describe the problem\r\nI want to calculate a focal loss. \r\n![image](https://user-images.githubusercontent.com/22476764/45917573-bb403600-beac-11e8-8262-56be63acd1c3.png)\r\n\r\nIt just added weight factor to the original cross entropy loss.\r\n\r\n#### `tf_leak` code (shown in Source code / logs) \r\n\r\nWhen I used the `tf_leak` code as shown below, there will be memory leak.\r\nI used `top` command to see the process's memory usage.\r\n\r\n- **pic01 occupy about 3G memory after about 1min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917603-212cbd80-bead-11e8-8b2b-e784196405b2.png)\r\n\r\n- **pic02 occupy about 11G memory after about 33min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917609-3ace0500-bead-11e8-9d6e-b58d748f6fbc.png)\r\n\r\n- **pic03 occupy about 14G memory after about 191min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917619-45889a00-bead-11e8-8370-082b341c4907.png)\r\n\r\n**I use the `tf.estimator` and the graph is finazied. So I do not add ops to the graph when the seesion is running.**\r\n\r\n- **pic04 graph is finazied**\r\n![image](https://user-images.githubusercontent.com/22476764/45917706-16bef380-beae-11e8-8af9-a0a8fd46c52a.png)\r\n\r\n\r\n#### `tf_no_leak` code (shown in Source code / logs) \r\n \r\nI replace the ` tf.dynamic_partition` function with `tf.boolean_mask` and the memory leak disappered.\r\n\r\n- **pic05 occupy about 3G memory after about 9min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917758-ec216a80-beae-11e8-9d49-115f2598fa50.png)\r\n\r\n- **pic06 occupy about 3G memory after about 133min**\r\n![image](https://user-images.githubusercontent.com/22476764/45917782-2be85200-beaf-11e8-9427-021f2f5fcd51.png)\r\n\r\n#### more information\r\n\r\nI tested the `tf_leak` and `tf_no_leak` code with a small dataset, when I use a bigger dataset, the process will occupy a large memory and the system will kill the process. This is why I found the problem.\r\n\r\n\r\n### Source code / logs\r\n```\r\nparams:\r\n\t\tvalid_labels\t:\tthe labels, a Tensor with shape [N * w * h, ]\r\n\t\tvalid_logits\t:\tthe logits, a Tensor with shape [N * w * h, num_classes]\r\n\t\tnum_classes \t: \tthe number of classes, a scalar\r\n                ohem_prob_g  :       a hyperparameter, a scalar, default=1.5\r\n```\r\n\r\n1. `tf_leak` code\r\n\r\n```\t\r\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\r\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0\r\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\r\nweight_matrix = ohem_prob_g - tf.dynamic_partition(valid_probs, valid_one_hot, num_partitions=2)[1]\r\n## calculate the final cross-entropy\r\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \r\n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\r\nreturn cross_entropy\r\n```\r\n\r\n2. `tf_no_leak` code\r\n\r\nsame as the `tf_leak` code except I replace the `tf.dynamic_partition` with `tf.boolean_mask`.\r\n\r\n```\r\nvalid_probs = tf.clip_by_value(tf.nn.softmax(valid_logits, axis=-1), \\\r\n\t\t\t\t\tclip_value_min=1e-6, clip_value_max=1.0)\r\nvalid_one_hot = tf.one_hot(valid_labels, depth=num_classes, dtype=tf.int32)\r\n## calculate the final cross-entropy\r\nweight_matrix = ohem_prob_g - tf.boolean_mask(valid_probs, valid_one_hot)\r\ncross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=valid_labels, \r\n\t\t\t\t\t\tlogits=valid_logits, weights=weight_matrix)\r\nreturn cross_entropy\r\n```\r\n\r\n3. input code used for the `tf.estimator`\r\n\r\n```\r\ndef input_fn(is_training, data_dir, batch_size, num_epochs=1):\r\n  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\r\n  Args:\r\n    is_training: A boolean denoting whether the input is for training.\r\n    data_dir: The directory containing the input data.\r\n    batch_size: The number of samples per batch.\r\n    num_epochs: The number of epochs to repeat the dataset.\r\n  Returns:\r\n    A tuple of images and labels.\r\n  \"\"\"\r\n  dataset = tf.data.Dataset.from_tensor_slices(get_filenames(is_training, data_dir))\r\n  dataset = dataset.flat_map(tf.data.TFRecordDataset)\r\n  if is_training:\r\n    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])\r\n\r\n  dataset = dataset.map(parse_record)\r\n  dataset = dataset.map(\r\n      lambda image, label, idx: preprocess_image(image, label, idx, is_training),\r\n      num_parallel_calls=1)\r\n\r\n  # We call repeat after shuffling, rather than before, to prevent separate\r\n  # epochs from blending together.\r\n  dataset = dataset.repeat(num_epochs)\r\n  dataset = dataset.batch(batch_size)\r\n  dataset = dataset.prefetch(buffer_size=FLAGS.prefetch_buffer_size)\r\n\r\n  iterator = dataset.make_one_shot_iterator()\r\n  images, labels, idx = iterator.get_next()\r\n\r\n  return images, labels\r\n\r\n```\r\n", "comments": ["#### Supplementary information\r\nWhen I use the results of the `tf.dynamic_partition` to do comparison operations(`>,<,==`) with a scalar, it is totally ok.\r\n", "I meet similar question, under watching.", "Did anybody else meet this question?", "@rmlarsen Hi, could you please look into this issue ?", "@mzhaoshuai Is this still an issue?\r\nWe see that you are using TF v1.x which is not actively supported and we recommend you to upgrade to latest TF versions(2.4 or later).Please refer to the [migration](https://www.tensorflow.org/guide/migrate) doc .\r\nEstimators are not recommended for new code. Estimators  can behave unexpectedly, especially when combined with TF 2 code. Could you please refer to this [link](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) for more details  and let us know if it helps?\r\nThanks!"]}, {"number": 22410, "title": "GAN train will hand if not use default GANTrainSteps(1, 1)", "body": "When I test WGAN on tensorflow,  I set the generator_train_steps = 1 and discriminator_train_steps = 5 following the paper, like this:\r\ntensorflow/contrib/gan/python/train.py:815\r\n`def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 5))`\r\nThe train process always hand after the first step. \r\nAnd the default value works well.\r\n`def get_sequential_train_hooks(train_steps=namedtuples.GANTrainSteps(1, 1))`\r\n\r\nTest:\r\njust run \r\n`https://github.com/tensorflow/models/blob/master/research/gan/cifar/train.py `\r\nwith cifar10 dataset.\r\n\r\n", "comments": ["@joel-shor , do you have some comments about this? Thanks.", "@joel-shor Hi, did you get a chance to look into this ?", "> @joel-shor Hi, did you get a chance to look into this ?\r\n\r\nAny update ?"]}, {"number": 22396, "title": "[Feature Request]:Assign the name to SaprseTensor when build_tensor_info of it", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N.A.\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: Tesla P100 16GB\r\n- **Exact command to reproduce**:\r\nA minimum reproduce example:\r\n```\r\na = tf.constant([[[1.0]]])\r\nb = tf.constant([1])\r\noutput,prob = tf.nn.ctc_beam_search_decoder(a,b)\r\na_tensor_proto = tf.saved_model.utils.build_tensor_info(a)\r\noutput_tensor_proto = tf.saved_model.utils.build_tensor_info(output[0]) \r\n```\r\n### Describe the problem\r\nA normal Tensor has a name:\r\n```\r\na_tensor_proto\r\nname: \"Const_15:0\"\r\ndtype: DT_FLOAT\r\ntensor_shape {\r\n  dim {\r\n    size: 1\r\n  }\r\n  dim {\r\n    size: 1\r\n  }\r\n  dim {\r\n    size: 1\r\n  }\r\n}\r\n\r\n```\r\nwhile the SparseTensor from the ctc Beam_Search_Decoder does not has a name:\r\n```\r\noutput_tensor_proto\r\ndtype: DT_INT64\r\ntensor_shape {\r\n  dim {\r\n    size: -1\r\n  }\r\n  dim {\r\n    size: -1\r\n  }\r\n}\r\ncoo_sparse {\r\n  values_tensor_name: \"CTCBeamSearchDecoder_12:1\"\r\n  indices_tensor_name: \"CTCBeamSearchDecoder_12:0\"\r\n  dense_shape_tensor_name: \"CTCBeamSearchDecoder_12:2\"\r\n}\r\n```\r\nWhich cause the error when predict:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import predictor\r\nfrom tensorflow.contrib.saved_model.python.saved_model import reader\r\nfrom tensorflow.contrib.saved_model.python.saved_model import signature_def_utils\r\nfrom tensorflow.python.tools import saved_model_utils\r\n\r\nexp_dir = MY_EXPORT_DIRECTORY\r\nmetagraph_def = saved_model_utils.get_meta_graph_def(exp_dir,'serve')\r\nsignature_def = signature_def_utils.get_signature_def_by_key(\r\n        metagraph_def,\r\n        'predicted_sequences')\r\noutput_names = {k: v.name for k, v in signature_def.outputs.items()}\r\nprint(output_names)\r\npredict_fn = predictor.from_saved_model(exp_dir)\r\nseq_len = np.asarray([[4]])\r\ncombined_input = np.concatenate((batch_x,seq_len),axis = 1)\r\npredictions = predict_fn({'inputs':combined_input})\r\n```\r\nValueError: The name '' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\r\nand  also the above error, when export it into a saved_model and request it.\r\n\r\nThe reason is when building a TensorProto for the SaprseTensor, no name is assigned to it:\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/saved_model/utils_impl.py#L46\r\n\r\n", "comments": [" tensorflow/serving#1096", "Hi, does tf-serving support SparseTensor? I'm afraid that SparseTensor is not a Tensor actually.\r\n\r\n[ tf.saved_model.utils.build_tensor_info ](https://www.tensorflow.org/api_docs/python/tf/saved_model/utils/build_tensor_info)\r\n> For SparseTensors, the names of the three constitutent Tensors are used.", "SparseTensor is a group of three dense tensors: indices, values, and dense_shape, what do you mean by saying it's not actually a tensor?\r\nDo you want to imply that one should not feed the SparseTensor into the build_tensor_info?", "Sorry, I'm not familiar with tf serving. Yes, as you said, SparseTensor is a group of 3 Tensors. I guess that ValueError might be raised by codes where expect a Tensor (but it receives a SparseTensor? ). Could you post the whole exception stack? ", "@facaiy  Please check tensorflow/serving#1096 and tensorflow/serving#1100 .", "Hi, I didn't find the stack trace of your exception ValueError in those two issues. It should be useful for analysis.\r\n\r\n@karmel  @annarev Hi, are you a good person to look at this, or can you redirect as necessary? Thanks!", "tf-serving should support sparse tensors, as they can be encoded as you show above in the metagraph. Can you provide the stack trace? It's hard to tell from the above what the precise issue is, but my guess is you need to specify options from the set of three names to properly find the tensor:\r\n\r\n```\r\n  values_tensor_name: \"CTCBeamSearchDecoder_12:1\"\r\n  indices_tensor_name: \"CTCBeamSearchDecoder_12:0\"\r\n  dense_shape_tensor_name: \"CTCBeamSearchDecoder_12:2\"\r\n```", "Here is the trace:\r\nErrorReport_tf_SparseTensor_Name/error_reproduce_client.py', wdir='/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name')\r\n{'output': ''}\r\nINFO:tensorflow:Restoring parameters from /home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/TEST_MODEL/export/1/variables/variables\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-6-a4394582e9d4>\", line 1, in <module>\r\n    runfile('/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/error_reproduce_client.py', wdir='/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name')\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 705, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py\", line 102, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/error_reproduce_client.py\", line 23, in <module>\r\n    predict_fn = predictor.from_saved_model(exp_dir)\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/contrib/predictor/predictor_factories.py\", line 143, in from_saved_model\r\n    config=config)\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/contrib/predictor/saved_model_predictor.py\", line 169, in __init__\r\n    for k, v in output_names.items()}\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/contrib/predictor/saved_model_predictor.py\", line 169, in <dictcomp>\r\n    for k, v in output_names.items()}\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3515, in get_tensor_by_name\r\n    return self.as_graph_element(name, allow_tensor=True, allow_operation=False)\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3339, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n\r\n  File \"/home/heavens/anaconda3/envs/tensorflow-1.10.1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3413, in _as_graph_element_locked\r\n    raise ValueError(err_msg)\r\n\r\nValueError: The name '' looks like an (invalid) Operation name, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".", "And here is a reproducing example I try to build by deleting unrelevant code from my own project:\r\nExport script:\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.saved_model import signature_constants as sig_constants\r\nfrom chiron.chiron_model import inference,read_config\r\nfrom chiron.chiron_eval import path_prob\r\n\r\nmodel_path = \"/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/TEST_MODEL\"\r\nOUTPUT = \"/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/TEST_MODEL/export\"\r\ndef output_list(x,seq_length):\r\n    training = tf.constant(False,dtype = tf.bool,name = 'Training')\r\n    config_path = os.path.join(model_path,'model.json')\r\n    model_configure = read_config(config_path)\r\n    logits, ratio = inference(x, \r\n                              seq_length, \r\n                              training=training,\r\n                              full_sequence_len = 4,\r\n                              configure = model_configure)\r\n    ratio = tf.constant(ratio,dtype = tf.float32,shape = [])\r\n    seq_length_r = tf.cast(tf.round(tf.cast(seq_length,dtype = tf.float32)/ratio),tf.int32)\r\n    prob_logits = path_prob(logits)\r\n    predict,log_prob = tf.nn.ctc_beam_search_decoder(tf.transpose(logits, perm=[1, 0, 2]), \r\n                                            seq_length_r, \r\n                                            merge_repeated=True,\r\n                                            beam_width = 30)\r\n    return predict[0],logits,prob_logits,log_prob\r\n\r\ndef build_and_run_exports():\r\n    \"\"\"Given the latest checkpoint file export the saved model.\r\n    \"\"\"\r\n\r\n    prediction_graph = tf.Graph()\r\n    \r\n    with prediction_graph.as_default():\r\n        x = tf.placeholder(tf.float32, shape=[None, 4])\r\n        seq_len = tf.placeholder(tf.int32, shape = [None])\r\n        #Inference\r\n        predict,logits,prob_logits,log_prob = output_list(x,seq_len)\r\n        values, indices = tf.nn.top_k(logits,k=1)\r\n        saver = tf.train.Saver()\r\n\r\n        with tf.Session(graph=prediction_graph) as sess:\r\n            ckpt = tf.train.get_checkpoint_state(model_path)\r\n            if ckpt and ckpt.model_checkpoint_path:\r\n                saver.restore(sess,ckpt.model_checkpoint_path)\r\n            output_path = os.path.join(\r\n                    tf.compat.as_bytes(OUTPUT),\r\n                    tf.compat.as_bytes(str(1)))\r\n            exporter = tf.saved_model.builder.SavedModelBuilder(output_path)\r\n            x_tensor_info = tf.saved_model.utils.build_tensor_info(x)\r\n            seq_len_tensor_info = tf.saved_model.utils.build_tensor_info(seq_len)\r\n            sparse_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n                    predict)\r\n            \r\n            prediction_signature = (tf.saved_model.signature_def_utils.build_signature_def(\r\n                inputs = {'x':x_tensor_info,\r\n                          'seq_len':seq_len_tensor_info},\r\n                outputs = {'output':sparse_output_tensor_info},\r\n                method_name=sig_constants.PREDICT_METHOD_NAME))\r\n            \r\n            exporter.add_meta_graph_and_variables(\r\n                sess,\r\n                tags=[tf.saved_model.tag_constants.SERVING],\r\n                signature_def_map={\r\n                        tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:prediction_signature\r\n                },\r\n                main_op=tf.tables_initializer(),\r\n                strip_default_attrs=True)\r\n            exporter.save()\r\n\r\n\r\nif __name__ == '__main__':\r\n    build_and_run_exports()\r\n```\r\nPredict Script:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import predictor\r\nfrom tensorflow.contrib.saved_model.python.saved_model import reader\r\nfrom tensorflow.contrib.saved_model.python.saved_model import signature_def_utils\r\nfrom tensorflow.python.tools import saved_model_utils\r\nimport numpy as np\r\n\r\nexp_dir = \"/home/heavens/Documents/ErrorReport_tf_SparseTensor_Name/TEST_MODEL/export/1\"\r\nmetagraph_def = saved_model_utils.get_meta_graph_def(exp_dir,'serve')\r\nsignature_def = signature_def_utils.get_signature_def_by_key(\r\n        metagraph_def,\r\n         tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)\r\noutput_names = {k: v.name for k, v in signature_def.outputs.items()}\r\nprint(output_names)\r\npredict_fn = predictor.from_saved_model(exp_dir)\r\nbatch_x = np.asarray([[[1,1,1,1]]])\r\nseq_len = np.asarray([[4]])\r\npredictions = predict_fn({'x':batch_x,'seq_len':seq_len})\r\n```\r\n[TEST_MODEL.tar.gz](https://github.com/tensorflow/tensorflow/files/2413795/TEST_MODEL.tar.gz)\r\n", "The problem can be solved by exporting the three dense tensor of the Sparse Tensor instead of exporting the Sparse Tensor itself, e.g.:\r\nChange\r\n```\r\nsparse_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n                    predict)\r\noutputs = {'output':sparse_output_tensor_info}\r\n```\r\n\r\nto\r\n\r\n```\r\nindices_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n                    predict.indices)\r\nvalues_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n                    predict.values)\r\ndense_shape_output_tensor_info = tf.saved_model.utils.build_tensor_info(\r\n                    predict.dense_shape)\r\n outputs = {'indices':indices_output_tensor_info,\r\n                           'values':values_output_tensor_info,\r\n                           'dense_shape':dense_shape_output_tensor_info}\r\n```\r\nWell I guess it could be an easy fix by redirecting the correct name of SparseTensor when predicting, but I am not sure.", "I am experiencing this same issue when attempting to call the predict REST endpoint for a served model:\r\n```\r\n\"error\": \"Tensor :0, specified in either feed_devices or fetch_devices was not found in the Graph\"\r\n```\r\nThe output tensor in question is a sparse tensor resulting from a \r\n[tf.string_split()](https://www.tensorflow.org/api_docs/python/tf/string_split) call.", "@mckeown12 did you find the solution?", "I ended up not calling `tf.string_split()` and kept my output as a single string.  Then I made the people using the endpoint parse it after the fact.  Not ideal.", "@mckeown12 Yes, please share whatever you have done it might be useful "]}, {"number": 22369, "title": "when use tensorflow to predict images with different resolution,the forward's cost  unstable", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:1.10.1\r\n- **Python version**:3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:9.0/7.1\r\n- **GPU model and memory**:Tesla P4 ,8GB\r\n- **Exact command to reproduce**:no\r\n\r\n### Describe the problem\r\nwhen use tensorflow to predict images with different resolution,the forward's cost  unstable. my image long side is less than 1280 , the resize code below:\r\n```\r\n def resize_im(self, im, scale=600, max_scale=1280): \r\n      f = 1.0\r\n      if scale > min(im.shape[0], im.shape[1]): \r\n        f = float(scale) / min(im.shape[0], im.shape[1]) \r\n      if max_scale != None and f * max(im.shape[0], im.shape[1]) > max_scale: \r\n        f = float(max_scale) / max(im.shape[0], im.shape[1]) \r\n      return cv2.resize(im, None, None, fx=f, fy=f, interpolation=cv2.INTER_LINEAR), f \r\n```\r\n\r\n[use different resolution.txt](https://github.com/tensorflow/tensorflow/files/2395362/use.different.resolution.txt)\r\nsee the net_fw\r\n\r\nwhen padding image to 1280*1280, the forward's cost stable,between 0.25 and 0.3,but the small image will cost more.\r\n\r\nI think it is BFC\u2018s problem.\r\nHow can I solve this problem?I think if tensorflow can lock the chunk's memory (use input image with resolution 1280*1280) for all layers , this problem could be sovled\r\n", "comments": ["anyone can response me?", "@reedwm Reed, do you have any idea?", "@kasyoukin, what do you mean by \"the forward's cost\"? I am not quite sure what the issue is.", "@reedwm neural network forward calculation time"]}, {"number": 22359, "title": "Bazel Build all_protos proto_library Support", "body": "I would like to build a `proto_library` for all the protobuf files in (core) tensorflow.\r\n\r\nSpecifically, I need to build the GraphDef protobuf message (and later target it for Golang) and include it in a custom proto definition. However, it seems only cc and py are visible/supported. It does not seem like the general `proto_library` is exposed by the tensorflow repo. Is there a simple way to do this? Ideally there would be a `proto_library` definition that is available in the [core/BUILD file](https://github.com/tensorflow/tensorflow/blob/89e172f6ae5a52f8ceca6b4690331e1dce89d456/tensorflow/core/BUILD).\r\n\r\nWhen I try to execute \"tf_additional_all_protos\" after successfully loading it from build_config.bzl, I get an error saying `no such package 'tensorflow/core'`. \r\n\r\nAny suggestions?\r\n\r\nHave I written custom code\r\n### Bazel Build\r\n```\r\n# bazel build :my_tf_proto\r\nERROR: /Users/zane/dev0/piran/test/BUILD:4:1: no such package 'tensorflow/core': BUILD file not found on package path and referenced by '//:_tensorflow_all_protos'\r\nERROR: Analysis of target '//:my_tf_proto' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 0.346s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n```\r\n\r\n### Environment\r\nOS Platform and Distribution\r\n```\r\n# bazel version\r\nBuild label: 0.15.2\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 17 12:22:40 2018 (1531830160)\r\nBuild timestamp: 1531830160\r\nBuild timestamp as int: 1531830160\r\n#\r\n# lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 16.04.5 LTS\r\nRelease:\t16.04\r\nCodename:\txenial\r\n```\r\n\r\ntf.proto\r\n```\r\nsyntax = \"proto3\";\r\n\r\npackage mytf;\r\n\r\nimport \"tensorflow/core/framework/graph.proto\";\r\n\r\nmessage MyMessage {\r\n  tensorflow.GraphDef myGraph = 1;\r\n}\r\n```\r\n\r\nWORKSPACE\r\n```\r\n\r\ngit_repository(\r\n    name = \"io_bazel_rules_closure\",\r\n    remote = \"https://github.com/bazelbuild/rules_closure.git\",\r\n    tag = \"0.8.0\",\r\n)\r\n\r\ngit_repository(\r\n    name = \"com_github_tensorflow_tensorflow\",\r\n    remote = \"https://github.com/tensorflow/tensorflow.git\",\r\n    commit = \"0c8a8289da120ee353c4fba5decb0bea9014e0a7\"  # Sep 18, 2018\r\n)\r\nload(\"@com_github_tensorflow_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\r\ntf_workspace()\r\n\r\n```\r\n\r\nBUILD\r\n```\r\npackage(default_visibility = [\"//visibility:public\"])\r\nload(\"@com_github_tensorflow_tensorflow//tensorflow/core:platform/default/build_config.bzl\", \"tf_additional_all_protos\")\r\n\r\nproto_library(\r\n  name = '_tensorflow_all_protos',\r\n  srcs = tf_additional_all_protos(),\r\n)\r\n\r\nproto_library(\r\n\tname = 'my_tf_proto',\r\n\tsrcs = ['tf.proto'],\r\n\tdeps = [':_tensorflow_all_protos'],\r\n)\r\n```\r\n\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: `bazel build :my_tf_proto`\r\nMobile device: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Any suggestions or feasible workarounds?", "@yongtang Any inputs ?", "Really not familiar with bazel setup with respect to protobuf. Wondering if @gunan has any insight with this issue?", "what the hell is going on"]}, {"number": 22358, "title": "Document how to train with large batch sizes", "body": "This request was a result of the TensorFlow Fall Symposium. Document tips and methods to use for large batch scaling with TensorFlow.  Possibly mention where this is proven and unproven to work.  This would include adding optimizers or wrapper to simplify usage.  Researchers are large labs want to scale but do not have the knowledge to do it quickly.\r\n", "comments": ["Randomly assigned, feel free to reassign.  "]}, {"number": 22304, "title": "C++   set_allow_soft_placement does not work", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution: Mac  CPU\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\n\r\nI have set set_allow_soft_placement(true), but it does not work.\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n\r\nc++ source code\r\n```c++\r\n    const string pathToGraph = \u201ctf/checkpoint/model-1.meta\";\r\n\r\n    tensorflow::SessionOptions options;\r\n    options.config.set_allow_soft_placement(true);  // why this does not work?\r\n    auto session = NewSession(options);\r\n\r\n    Status status = session->Create(graph_def.graph_def());\r\n\r\n```\r\n\r\nIt get errors in my mac(cpu),  no error in GPU.\r\n\r\n```sh\r\n./eval\r\nlibc++abi.dylib: terminating with uncaught exception of type std::runtime_error: \r\nError creating graph: Invalid argument: Cannot assign a device for \r\noperation 'ExponentialMovingAverage/AssignMovingAvg_5/sub': \r\nOperation was explicitly assigned to /device:GPU:0 but available devices are \r\n[ /job:localhost/replica:0/task:0/device:CPU:0 ]. \r\nMake sure the device specification refers to a valid device.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Can you give a self-contained example to reproduce? I cannot reproduce at 0e1efc3d9129c740a16081fdc53bdc482f8f0c11 with the following:\r\n\r\n```\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n\r\nint main() {\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  GraphDef graph_def;\r\n  {\r\n    Scope root = Scope::NewRootScope().WithDevice(\"/gpu:0\");\r\n    auto A = Const(root, 1.0f);\r\n    Graph* graph = root.graph();\r\n    graph->ToGraphDef(&graph_def);\r\n  }\r\n  SessionOptions options;\r\n  options.config.set_allow_soft_placement(true);\r\n  Session* sess = NewSession(options);\r\n  Status status = sess->Create(graph_def);\r\n  LOG(INFO) << status;\r\n}\r\n```", "@xu-song Hi, can you share an example to reproduce these errors ?", "ok, i will prepare one example soon. Thanks for your reply", "# simple example\r\n\r\nthis is a simple example, which get `result = a*b+1`\r\n\r\n## python - save model\r\n```py\r\n# demo.py\r\nimport tensorflow as tf\r\nimport os\r\n\r\nif __name__ == '__main__':\r\n    train_dir = os.path.join('demo_model/', \"demo\")\r\n\r\n    with tf.device('/gpu:0'):\r\n        a = tf.placeholder(dtype=tf.int32, shape=None, name='a')\r\n        b = tf.placeholder(dtype=tf.int32, shape=None, name='b')\r\n        y = tf.Variable(tf.ones(shape=[1], dtype=tf.int32), dtype=tf.int32, name='y')\r\n        res = tf.add(tf.multiply(a, b), y, name='res')\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    with tf.Session(config=config) as sess:\r\n        feed_dict = {a:2, b:3}\r\n        fetch_list = [res]\r\n        sess.run(tf.global_variables_initializer())\r\n        saver = tf.train.Saver()\r\n\r\n        res = sess.run(feed_dict=feed_dict, fetches=fetch_list)\r\n        saver.save(sess, train_dir)\r\n\r\n        print(\"result: \", res[0])\r\n```\r\n\r\n```sh\r\n$ python demo.py \r\n2018-09-29 09:35:28.855859: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n('result: ', array([7], dtype=int32))\r\n```\r\n\r\n## python - restore\r\n```py\r\n# demo-restore.py\r\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\n\r\nif __name__ == '__main__':\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    with tf.Session(config=config) as sess:\r\n        saver = tf.train.import_meta_graph('demo_model/demo.meta')\r\n        saver.restore(sess, tf.train.latest_checkpoint('demo_model/'))\r\n        graph = tf.get_default_graph()\r\n        a = graph.get_tensor_by_name(\"a:0\")\r\n        b = graph.get_tensor_by_name(\"b:0\")\r\n        feed_dict = {a: 2, b: 3}\r\n\r\n        op_to_restore = graph.get_tensor_by_name(\"res:0\")\r\n        print(sess.run(fetches=op_to_restore, feed_dict=feed_dict))\r\n```\r\n\r\n```\r\n$ python demo-restore.py\r\n[7]\r\n```\r\n\r\n## C++ restore model\r\n```c\r\n// demo.cc\r\n#include <iostream>\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/protobuf/meta_graph.pb.h\"\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\nint main()\r\n{\r\n    const string pathToGraph = \"demo_model/demo.meta\";\r\n    const string checkpointPath = \"demo_model/demo\";\r\n    SessionOptions options;\r\n    options.config.set_allow_soft_placement(true);   // this line does not work\r\n    auto session = NewSession(options); \r\n\r\n//    auto session = NewSession(SessionOptions());\r\n    if (session == nullptr)\r\n    {\r\n        throw runtime_error(\"Could not create Tensorflow session.\");\r\n    }\r\n\r\n    Status status;\r\n    MetaGraphDef graph_def;\r\n    status = ReadBinaryProto(Env::Default(), pathToGraph, &graph_def);\r\n    if (!status.ok())\r\n    {\r\n        throw runtime_error(\"Error reading graph definition from \" + pathToGraph + \": \" + status.ToString());\r\n    }\r\n\r\n   // restore graph\r\n    status = session->Create(graph_def.graph_def());\r\n    if (!status.ok())\r\n    {\r\n        throw runtime_error(\"Error creating graph: \" + status.ToString());\r\n    }\r\n\r\n    //  restore checkpoint \r\n    Tensor checkpointPathTensor(DT_STRING, TensorShape());\r\n    checkpointPathTensor.scalar<std::string>()() = checkpointPath;\r\n    status = session->Run(\r\n            {{ graph_def.saver_def().filename_tensor_name(), checkpointPathTensor },},\r\n            {},\r\n            {graph_def.saver_def().restore_op_name()},\r\n            nullptr);\r\n    if (!status.ok())\r\n    {\r\n        throw runtime_error(\"Error loading checkpoint from \" + checkpointPath + \": \" + status.ToString());\r\n    }\r\n\r\n    std::vector<std::pair<string, Tensor>> input;\r\n    Tensor a(tensorflow::DT_INT32, tensorflow::TensorShape({1}));\r\n    Tensor b(tensorflow::DT_INT32, tensorflow::TensorShape({1}));\r\n\r\n    auto a_map = a.tensor<int,1>();\r\n    a_map(0) = 2;\r\n    auto b_map = b.tensor<int,1>();\r\n    b_map(0) = 3;\r\n    input.emplace_back(std::string(\"a\"), a);\r\n    input.emplace_back(std::string(\"b\"), b);\r\n\r\n   //   run session\r\n    std::vector<tensorflow::Tensor> answer;\r\n    status = session->Run(input, {\"res\"}, {}, &answer);\r\n\r\n    Tensor result = answer[0];\r\n    auto result_map = result.tensor<int,1>();\r\n    cout<<\"result: \"<<result_map(0)<<endl;\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\n## error - in cpu\r\n\r\n```\r\n./demo\r\nlibc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Error creating graph: \r\nInvalid argument: Cannot assign a device for operation 'y': Operation was explicitly assigned to \r\n/device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the \r\ndevice specification refers to a valid device.\r\n\t [[Node: y = VariableV2[_output_shapes=[[1]], container=\"\", dtype=DT_INT32, shape=[1], \r\nshared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\nAbort trap: 6\r\n```\r\n\r\n## no error  - in gpu\r\n", "> # simple example\r\n> this is a simple example, which get `result = a*b+1`\r\n> \r\n> ## python - save model\r\n> ```python\r\n> # demo.py\r\n> import tensorflow as tf\r\n> import os\r\n> \r\n> if __name__ == '__main__':\r\n>     train_dir = os.path.join('demo_model/', \"demo\")\r\n> \r\n>     with tf.device('/gpu:0'):\r\n>         a = tf.placeholder(dtype=tf.int32, shape=None, name='a')\r\n>         b = tf.placeholder(dtype=tf.int32, shape=None, name='b')\r\n>         y = tf.Variable(tf.ones(shape=[1], dtype=tf.int32), dtype=tf.int32, name='y')\r\n>         res = tf.add(tf.multiply(a, b), y, name='res')\r\n> \r\n>     config = tf.ConfigProto(allow_soft_placement=True)\r\n>     with tf.Session(config=config) as sess:\r\n>         feed_dict = {a:2, b:3}\r\n>         fetch_list = [res]\r\n>         sess.run(tf.global_variables_initializer())\r\n>         saver = tf.train.Saver()\r\n> \r\n>         res = sess.run(feed_dict=feed_dict, fetches=fetch_list)\r\n>         saver.save(sess, train_dir)\r\n> \r\n>         print(\"result: \", res[0])\r\n> ```\r\n> \r\n> ```shell\r\n> $ python demo.py \r\n> 2018-09-29 09:35:28.855859: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> ('result: ', array([7], dtype=int32))\r\n> ```\r\n> \r\n> ## python - restore\r\n> ```python\r\n> # demo-restore.py\r\n> import tensorflow as tf\r\n> import os\r\n> import numpy as np\r\n> \r\n> if __name__ == '__main__':\r\n>     config = tf.ConfigProto(allow_soft_placement=True)\r\n>     with tf.Session(config=config) as sess:\r\n>         saver = tf.train.import_meta_graph('demo_model/demo.meta')\r\n>         saver.restore(sess, tf.train.latest_checkpoint('demo_model/'))\r\n>         graph = tf.get_default_graph()\r\n>         a = graph.get_tensor_by_name(\"a:0\")\r\n>         b = graph.get_tensor_by_name(\"b:0\")\r\n>         feed_dict = {a: 2, b: 3}\r\n> \r\n>         op_to_restore = graph.get_tensor_by_name(\"res:0\")\r\n>         print(sess.run(fetches=op_to_restore, feed_dict=feed_dict))\r\n> ```\r\n> \r\n> ```\r\n> $ python demo-restore.py\r\n> [7]\r\n> ```\r\n> \r\n> ## C++ restore model\r\n> ```c\r\n> // demo.cc\r\n> #include <iostream>\r\n> #include \"tensorflow/core/public/session.h\"\r\n> #include \"tensorflow/core/protobuf/meta_graph.pb.h\"\r\n> #include \"tensorflow/cc/client/client_session.h\"\r\n> #include \"tensorflow/cc/ops/standard_ops.h\"\r\n> #include \"tensorflow/core/framework/tensor.h\"\r\n> \r\n> using namespace std;\r\n> using namespace tensorflow;\r\n> \r\n> int main()\r\n> {\r\n>     const string pathToGraph = \"demo_model/demo.meta\";\r\n>     const string checkpointPath = \"demo_model/demo\";\r\n>     SessionOptions options;\r\n>     options.config.set_allow_soft_placement(true);   // this line does not work\r\n>     auto session = NewSession(options); \r\n> \r\n> //    auto session = NewSession(SessionOptions());\r\n>     if (session == nullptr)\r\n>     {\r\n>         throw runtime_error(\"Could not create Tensorflow session.\");\r\n>     }\r\n> \r\n>     Status status;\r\n>     MetaGraphDef graph_def;\r\n>     status = ReadBinaryProto(Env::Default(), pathToGraph, &graph_def);\r\n>     if (!status.ok())\r\n>     {\r\n>         throw runtime_error(\"Error reading graph definition from \" + pathToGraph + \": \" + status.ToString());\r\n>     }\r\n> \r\n>    // restore graph\r\n>     status = session->Create(graph_def.graph_def());\r\n>     if (!status.ok())\r\n>     {\r\n>         throw runtime_error(\"Error creating graph: \" + status.ToString());\r\n>     }\r\n> \r\n>     //  restore checkpoint \r\n>     Tensor checkpointPathTensor(DT_STRING, TensorShape());\r\n>     checkpointPathTensor.scalar<std::string>()() = checkpointPath;\r\n>     status = session->Run(\r\n>             {{ graph_def.saver_def().filename_tensor_name(), checkpointPathTensor },},\r\n>             {},\r\n>             {graph_def.saver_def().restore_op_name()},\r\n>             nullptr);\r\n>     if (!status.ok())\r\n>     {\r\n>         throw runtime_error(\"Error loading checkpoint from \" + checkpointPath + \": \" + status.ToString());\r\n>     }\r\n> \r\n>     std::vector<std::pair<string, Tensor>> input;\r\n>     Tensor a(tensorflow::DT_INT32, tensorflow::TensorShape({1}));\r\n>     Tensor b(tensorflow::DT_INT32, tensorflow::TensorShape({1}));\r\n> \r\n>     auto a_map = a.tensor<int,1>();\r\n>     a_map(0) = 2;\r\n>     auto b_map = b.tensor<int,1>();\r\n>     b_map(0) = 3;\r\n>     input.emplace_back(std::string(\"a\"), a);\r\n>     input.emplace_back(std::string(\"b\"), b);\r\n> \r\n>    //   run session\r\n>     std::vector<tensorflow::Tensor> answer;\r\n>     status = session->Run(input, {\"res\"}, {}, &answer);\r\n> \r\n>     Tensor result = answer[0];\r\n>     auto result_map = result.tensor<int,1>();\r\n>     cout<<\"result: \"<<result_map(0)<<endl;\r\n> \r\n>     return 0;\r\n> }\r\n> ```\r\n> \r\n> ## error - in cpu\r\n> ```\r\n> ./demo\r\n> libc++abi.dylib: terminating with uncaught exception of type std::runtime_error: Error creating graph: \r\n> Invalid argument: Cannot assign a device for operation 'y': Operation was explicitly assigned to \r\n> /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the \r\n> device specification refers to a valid device.\r\n> \t [[Node: y = VariableV2[_output_shapes=[[1]], container=\"\", dtype=DT_INT32, shape=[1], \r\n> shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n> Abort trap: 6\r\n> ```\r\n> \r\n> ## no error - in gpu\r\nbecause you train your model with gpu,you can try load your model just cpu type"]}, {"number": 22298, "title": "Can't initialize keras.Model based network with tf.train.init_from_chekpoint", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: git master\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nWe are trying use `tf.keras.Model` as a base class for our neural network. So far, with `slim` based network we have used `tf.train.init_from_checkpoint` to warm start the training.\r\nBut, it seems that the current version of `tf.train.init_from_checkpoint` doesn't play well with `keras.Model`. Probably because under the hood, `tf.train.init_from_checkpoint` calls `variable_scope._get_defautl_variable_store()` which doesn't know variables created with keras objects.\r\n\r\nBelow a snippet of code to illustrate this problem\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.ops import variable_scope as vs\r\n\r\nnet = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(10, name='fc1')\r\n])\r\n\r\nx = tf.random_uniform([10, 3])\r\ny = net(x)\r\n\r\n# all variable are created \r\nvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\nfor v in vars:\r\n    print(v.name)\r\n\r\n# but variable scope doesn't see them\r\n# then no variable to initialize for tf.train.init_from_checkpoint\r\nvar_store = vs._get_default_variable_store()._vars\r\nprint(var_store)\r\n```", "comments": ["@jrabary Hi, are you getting any error or does the system not respond without any error? Could you post the error message if you get it.", "@harshini-gadige Hi, no there is no error. But the variable are not initialized. I know this because I set the logging to DEBUG in order to see which variables are initialized in the logs.\r\nThe good news is that I can initialize my network tf.estimator.WarmStartSettings.", "@fchollet  Any suggestions or explanation on why the network cannot be initialized with tf.train.init_from_checkpoint ", "@jrabary  How about using the native methods of [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#variables), say `load`, `load_weights` or `set_weights`?", "@facaiy Need to figure out how to use `load_weights` since we are using tf.Estimator to optimize the model. This is because we also have `old` models coded with slim so we just don't want to change everything.", "I'm afraid that the graph built by tf.Estimator is quite different from the one built by keras, so you have to resolve those details by yourself.  Even if tf.train.init_from_checkpoint could support tf.keras, it still might not work as you expected.\r\n\r\nIf you really need it, I think you can load all variables from checkpoint file with ` tf.train.load_variable`, and then assign them one by one with `set_weigths` method of your keras model. By the way, you can collect all variables by `get_weights` method.", "It is a big problem\r\nMNIST tutorial with tf.Estimator says us to use keras Model for architecture creation task\r\nBut if we wanna restore and fine tune tf Estimator, the best way to do it is tf.train.init_from_checkpoint\r\nAnd it is troubling situation, when you train model and waste time to restore", "I always check that error is within tensorflow\r\ntf.python.ops.variable_scope has a function `variable()`\r\nWhich has a strange initializer after creation ", "https://github.com/tensorflow/tensorflow/pull/23110\r\nFixed this problem as commit\r\nIt requires testing all framework with this updater\r\nMy local tests has been completed"]}, {"number": 22242, "title": "Using TF-TRT doubles the size of frozen protobuf file", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master @ 3e137b24b06a81772402b86392dbd158653d487b\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0/7.1\r\n- **GPU model and memory**: 1080ti/11gb dual\r\n- **Exact command to reproduce**: \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\nimport numpy as np\r\n\r\nfrom keras import backend as K\r\ncfg = K.tf.ConfigProto()\r\ncfg.gpu_options.allow_growth = True\r\nK.set_session(K.tf.Session(config=cfg))\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the\r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we can use again a convenient built-in function to import a graph_def into the\r\n    # current default Graph\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(\r\n            graph_def,\r\n            name='', #DEBUG\r\n        )\r\n    return graph\r\n\r\nfid = \"model.pb\"\r\noutput_nodenames = 'output1,output2,output3'\r\noutput_node = list(output_nodenames.split(\",\"))\r\ng = load_graph(fid)\r\nwith tf.Session(graph=g) as sess:\r\n    nodes = [n.name for n in tf.get_default_graph().as_graph_def().node]\r\n    with open(\"original_graph.txt\", \"w\") as fid:\r\n        for item in nodes:\r\n            fid.write(\"%s\\n\" % item)\r\n    writer = tf.summary.FileWriter(\"logs_viz_orig\", tf.get_default_graph().as_graph_def())\r\n    trt_graph = trt.create_inference_graph(\r\n    input_graph_def=tf.get_default_graph().as_graph_def(),\r\n    outputs=output_node,\r\n    max_batch_size=1,\r\n    max_workspace_size_bytes=1 << 25,\r\n    precision_mode=\"FP32\",  # TRT Engine precision \"FP32\",\"FP16\" or \"INT8\"\r\n    minimum_segment_size=2  # minimum number of nodes in an engine\r\n    )\r\n    f = open(\"trt.pb\", 'w')\r\n    f.write(trt_graph.SerializeToString())\r\n    f.close()\r\n```\r\n  \r\nThe original graph as ~1100 ops in total, and trt graph has ~900, even then the original model was ~60mb, whereas the exported trt graph is ~120 mb. \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["What exactly is your issue? ", "Using TF-TRT doubles the size of the protobuf file. This kind of optimizations are meant for mobile devices at inference time, and hence this may be an issue."]}, {"number": 22226, "title": "Orthogonal initialization gives inaccurate result when running on GPU", "body": "### System information\r\n- **Have I written custom code**: see example below\r\n- **OS Platform and Distribution**:Google Colabratory (also in compiled code on Linux)\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Google Colabratory\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2.7.14 \r\n- **Bazel version (if compiling from source)**:  N/A\r\n- **GCC/Compiler version (if compiling from source)**:  N/A\r\n- **CUDA/cuDNN version**:  N/A\r\n- **GPU model and memory**: Issue occurred both on Telsa V100 and Tesla K80 with at least 11GB of available memory (but not when using CPU runtime)\r\n- **Exact command to reproduce**: see example below\r\n\r\n### Describe the problem\r\nWhen generating an orthogonal matrix and running on a GPU, if the matrix is 1000x1000 or larger the resulting matrix is not orthogonal. The deviations are several orders of magnitude larger than expected from finite precision errors.\r\nThe issue doesn't occur for smaller n (say 100) or when running on a CPU.\r\n\r\nIn the example below, the final result has entry-wise deviations from the identity matrix of order 1e-2.\r\n\r\n### Source code / logs\r\n \r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\ntf.set_random_seed(0)\r\ninit = tf.orthogonal_initializer(1,seed=0)\r\na = init([1000,1000])\r\ntf.matmul(a,tf.transpose(a))\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "sure, I updated the post", "@dargilboa  Hi, could you please post the output which you get when running on a GPU for a 1000x1000 matrix. Need to check if you are getting any error or just the deviation.", "This is for a 1000x1000\r\n<tf.Tensor: id=24, shape=(1000, 1000), dtype=float32, numpy=\r\narray([[ 1.0023266 ,  0.02684135, -0.01610221, ..., -0.00190309,\r\n         0.01394506, -0.00587076],\r\n       [ 0.02684135,  1.0824999 , -0.0193597 , ...,  0.0389607 ,\r\n        -0.00889984, -0.05881837],\r\n       [-0.01610221, -0.0193597 ,  1.0005746 , ..., -0.02111715,\r\n        -0.00168062,  0.01316965],\r\n       ...,\r\n       [-0.00190309,  0.0389607 , -0.02111715, ...,  1.0288322 ,\r\n        -0.01475003, -0.02641175],\r\n       [ 0.01394506, -0.00889984, -0.00168062, ..., -0.01475003,\r\n         1.0051492 ,  0.01033276],\r\n       [-0.00587076, -0.05881837,  0.01316965, ..., -0.02641175,\r\n         0.01033276,  1.0294992 ]], dtype=float32)>\r\n\r\nwhile on a CPU I get \r\n\r\n<tf.Tensor: id=24, shape=(1000, 1000), dtype=float32, numpy=\r\narray([[ 9.9999940e-01, -1.8626451e-09, -2.7939677e-08, ...,\r\n         1.5075784e-08, -8.3819032e-09,  3.7252903e-09],\r\n       [-1.8626451e-09,  1.0000001e+00, -4.7730282e-08, ...,\r\n         3.1664968e-08, -6.5192580e-08,  9.3132257e-10],\r\n       [-2.7939677e-08, -4.7730282e-08,  1.0000000e+00, ...,\r\n        -5.0291419e-08, -5.8207661e-09,  9.3132257e-09],\r\n       ...,\r\n       [ 1.5075784e-08,  3.1664968e-08, -5.0291419e-08, ...,\r\n         1.0000000e+00, -3.0733645e-08, -1.8742867e-08],\r\n       [-8.3819032e-09, -6.5192580e-08, -5.8207661e-09, ...,\r\n        -3.0733645e-08,  9.9999988e-01, -2.7939677e-08],\r\n       [ 3.7252903e-09,  9.3132257e-10,  9.3132257e-09, ...,\r\n        -1.8742867e-08, -2.7939677e-08,  9.9999994e-01]], dtype=float32)>\r\n\r\nalso when comparing the matrix a in both cases, although most entries are identical between the CPU and GPU (up to finite precision errors) there are larger deviations in a few rows", "/CC @rmlarsen, can you take a look?", "Any update on this?", "Hi @dargilboa ! Have you checked this issue using [tf.keras.initializers.Orthogonal](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Orthogonal) yet ?Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/86d35b123facf588ac30288267eb3258/git_22226.ipynb#scrollTo=q3zITfhJFaUM) for reference."]}, {"number": 22214, "title": "C++ API: tensorflow::ops::Mul causes read access violation when second attribute is int constant", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: C++ API compiled from source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: Compiled via CMake\r\n- **GCC/Compiler version (if compiling from source)**: VS 2015\r\n- **CUDA/cuDNN version**: 9/7.0.5\r\n- **GPU model and memory**: GTX 1080, 8GBs\r\n- **Exact command to reproduce**: run the script\r\n\r\n### Describe the problem\r\nIt seems `tensorflow::ops::Mul` accepts arguments of different type ad then crashes during `Run()`.\r\n\r\nTake this sample program:\r\n\r\n```\r\n// dunno if all are needed, some may be redundant for this small sample\r\n#include <tensorflow/cc/ops/const_op.h>\r\n#include <tensorflow/cc/ops/image_ops.h>\r\n#include <tensorflow/cc/ops/standard_ops.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/platform/init_main.h>\r\n#include <tensorflow/cc/client/client_session.h>\r\n\r\nint main(int argc, char* argv[])\r\n{\r\n    tensorflow::port::InitMain(argv[0], &argc, &argv);\r\n    auto root = tensorflow::Scope::NewRootScope();\r\n    auto input = tensorflow::ops::Placeholder(root, tensorflow::DataType::DT_UINT8);\r\n    auto id = tensorflow::ops::Identity(root, input); // just to check we get this far before the error occurs\r\n    auto to_float = tensorflow::ops::Cast(root, id, tensorflow::DataType::DT_FLOAT);\r\n    auto mul = tensorflow::ops::Mul(root, to_float, { -1 }); // <<<<<<<<<<<<<<<<<\r\n\r\n    tensorflow::ClientSession session(root);\r\n\r\n    // Run net\r\n    std::vector<tensorflow::Tensor> outputs;\r\n    tensorflow::Status run_status = session.Run({ {input, {(uint8_t)42, (uint8_t)35}} }, { id, mul }, &outputs);\r\n    std::cout << \"Run status: \" << run_status << std::endl;\r\n    std::cout << outputs[0].DebugString() << std::endl;\r\n    std::cout << outputs[1].DebugString() << std::endl;\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\nIt compiles fine but it will throw an \"Access Violation reading location 0x0000000000000050\" when executing `Run()`.\r\n\r\nChanging the marked line to\r\n`auto mul = tensorflow::ops::Mul(root, to_float, { -1.0f });`\r\n(i.e., just changing the numerical constant in the second argument from int to float) fixes the error and produces the proper output.\r\n\r\nI'm not sure whether this is a bug or my error for passing two tensors with different types, but either way IMHO this should probably emit a warning at graph construction time (or prevent compilation altogether, which would be the better solution).\r\n", "comments": ["Also, the same issue happens with `tensorflow::ops::Add` (and I assume more ops, these are just those I tested)", "tensorflow does not have an implicit cast in this case. The -1 will be interpreted as an int. Therefore you are trying to multiplicate an int with an float. Your second approach (-1.0f) is the the right one. Use that. It is not an error in tensorflow (in my opinion)\r\n\r\nWhile debugging your application you can check, if your operation works, if _mul != nullptr_ (or anything like NULL)", "Just a simple question in my own interest: How did you build tensorflow on your machine (cmake or bazel)? How did you integrate tensorflow into your application?", "@PinkySan yes, I thought the implicit cast missing was the case. The problem is, the program compiles just fine and raises no log messages until it dies abruptly. To be fair, I haven't checked warning messages in the compiler, maybe something's written there, but it's still a bit too easy a mistake to make to rely simply on a warning IMO.\r\n\r\nAbout the build, I used Cmake with VS2015 x64 as the generator. I built only the shared library and once you build the \"install\" project you get all you need in the install directory. Then it's just a matter of linking to tensorflow.lib and including the headers"]}, {"number": 22213, "title": "how to fix  \"tf.nightly-gpu\" caused \"nan\" problem", "body": "\r\n### System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version (use command below): 1.11.0 (use tf-night-gpu)\r\nPython version:2.7\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: 9.0 /7.1\r\nGPU model and memory:\r\nExact command to reproduce:\r\n\r\n\r\n### Describe the problem\r\nI used `tf-night-gpu` in my project code, but it will be made \"nan\" at the next batch train.\r\nit only the  `tf-night-gpu` will happen, it is can work in tensorflow1.10.\r\n\r\n\r\n### Source code / logs\r\nthe minimal code at here , it is just part of my project code. the input is word_index.\r\nthe batch size = 64\r\nuse tf-nightly-gpu.\r\nplatform = jupyter notebook.\r\nin first batch size was worked, but in next batch train will be \"nan\". but this problem only happen in \"tf-nightly-gpu\"\r\n```\r\n\r\nclass  RNN_Decoder(tf.keras.Model):\r\n    def __init__(self, embedding_dim, units, vocab_size):\r\n        super(RNN_Decoder, self).__init__()\r\n        self.units = units\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n    def call(self, x, features, hidden):\r\n        print('embedding_input:  ', x)\r\n        x = self.embedding(x)\r\n        print('embedding_output:  ', x)\r\n\r\ndecoder = RNN_Decoder(embedding_dim, units, vocab_size)\r\n\r\nfor epoch in range(20):\r\n    hidden = decoder.reset_state(batch_size=64)\r\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\r\n        with tf.GradientTape() as tape:\r\n            features = encoder(img_tensor)\r\n            for i in range(1, target.shape[1])\r\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\r\n```\r\n\r\n---- first batch train input and output------\r\n```\r\n('embedding_input ', <tf.Tensor: id=4586198, shape=(64, 1), dtype=int32, numpy=\r\narray([[3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3]], dtype=int32)>)\r\n('Embedding_output:  ', <tf.Tensor: id=4586379, shape=(64, 1, 256), dtype=float32, numpy=\r\narray([[[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]],\r\n\r\n       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]],\r\n\r\n       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]],\r\n\r\n       ...,\r\n\r\n       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]],\r\n\r\n       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]],\r\n\r\n       [[ 0.01097491, -0.02041122,  0.04823625, ..., -0.0155656 ,\r\n          0.01521399, -0.0496642 ]]], dtype=float32)>)\r\n```\r\n\r\nthe next batch train , have same keras.layers.Embedding_input:\r\n\r\n```\r\n`Epoch 1 Batch 0 Loss 2.0415\r\n(''keras.layers.Embedding_input:: ', <tf.Tensor: id=4607289, shape=(64, 1), dtype=int32, numpy=\r\narray([[3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3],\r\n       [3]], dtype=int32)>)\r\n(''keras.layers.Embedding_output::  ', <tf.Tensor: id=4607374, shape=(64, 1, 256), dtype=float32, numpy=\r\narray([[[nan, nan, nan, ..., nan, nan, nan]],\r\n\r\n       [[nan, nan, nan, ..., nan, nan, nan]],\r\n\r\n       [[nan, nan, nan, ..., nan, nan, nan]],\r\n\r\n\r\n       [[nan, nan, nan, ..., nan, nan, nan]],\r\n\r\n       [[nan, nan, nan, ..., nan, nan, nan]],\r\n\r\n       [[nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)>)`\r\n```", "comments": ["Sorry, but I am not able to run or replicate with the code snippet above; can you produce a minimal example that demonstrates the issue?", "I'm copying from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb) you just use tf-nightly-gpu running this code. \r\nand output it  `embedding`", "@MarkDaoust -- an issue with an Eager example; can you look into it?", "It looks like the notebook was relying on undefined behavior for Out of Vocabulary values, and this undefined behavior changed in r 1.11.\r\n\r\nI'll see what I can find for a fix.\r\n", "Hi @Q82822 ! 1.x versions are not supported any more. You can use our [migration document](https://www.tensorflow.org/guide/migrate) for using  1.x codes in 2.8 version. Thanks!"]}]