[{"number": 10193, "title": "When could it be upgraded to 1.2 from pip? ", "body": "I noticed that 1.2 has released in official website. I tried to upgrade to 1.2 via pip, but it is still 1.1.\r\nHow could I upgrade to 1.2?", "comments": ["I think you can upgrade via pip with `pip install --upgrade 'tensorflow==1.2.0rc0'`.\r\n1.2.0 is still a rc version.", "Once the 1.2 release is final, you can use `pip install --upgrade tensorflow` command to upgrade to 1.2.\r\nUntil then, you can do as @czy941030 described."]}, {"number": 10192, "title": "How to know what commands and types are supported in iOS build?", "body": "According to this [comment](https://github.com/tensorflow/tensorflow/issues/9934#issuecomment-302817142)  and other related issues currently there is some commands and types that iOS users can't load from `frozen.pb` graph. So we could use any TF API in python but not in iOS. It is hard to guess what python API will not been supported in iOS. So is there any documentation or instructions of how to write solution using python API and what functions and types could be used to make `frozen.pb` graph be fully supported by iOS API?\r\n\r\n", "comments": ["@petewarden, could you comment on this.", "Try this:\r\nin the (your tensorflow root)/tensorflow/contrib/makefile/Makefile\r\nremove the line \"-D__ANDROID_TYPES_SLIM__ \" under \"# Settings for iOS.\" for all \"$(IOS_ARCH)\"\r\nthen recompile the libtensorflow-core.a according to the modified Makefile.\r\n\r\nThe reason why this happened is because:\r\nin tensorflow/core/kernels/cwise_ops_common.h\r\n#if defined(ANDROID_TYPES_SLIM)\r\n// Note that ANDROID_TYPES_SLIM is also checked in the cwise_ops*.cc files.\r\n// Normally Android TensorFlow is built with a reduced number of types (float).\r\n// Override on the command-line \"--define ANDROID_TYPES=ANDROID_TYPES_FULL\"\r\n// to generate a library with full type support with a consequent increase in\r\n// code size.\r\n#define REGISTER2(OP, D, N, F, T0, T1) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER3(OP, D, N, F, T0, T1, T2) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER4(OP, D, N, F, T0, T1, T2, T3) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER6(OP, D, N, F, T0, T1, T2, T3, T4, T5) REGISTER(OP, D, N, F, T0)\r\n#define REGISTER7(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6) \r\nREGISTER(OP, D, N, F, T0)\r\n#define REGISTER8(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7) \r\nREGISTER(OP, D, N, F, T0)\r\n#define REGISTER9(OP, D, N, F, T0, T1, T2, T3, T4, T5, T6, T7, T8) \r\nREGISTER(OP, D, N, F, T0)\r\n#else // !defined(ANDROID_TYPES_SLIM)\r\n\r\n(ps: I also wrote a iOS_SSDMobilenet example base on the python ssd_mobilenet and ios_camera_example, and here is the github link: @https://github.com/JieHe96/ios_SSDMobilenet_tensorflow_example)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to lack of activity. Please reopen if it's still a problem."]}, {"number": 10191, "title": "[XLA] Ensure constants conform to the specified index type", "body": "The strided slice update introduces constants into the HLO graph which are not of the type specified by the core op.\r\n\r\nThis adds methods to the computation builder to allow the creation of literals with types determined at runtime, and makes the strided slice update op use them.\r\n\r\nit depends on https://github.com/tensorflow/tensorflow/pull/10164\r\n", "comments": ["Can one of the admins verify this patch?", "I left this line in the elemental_ir_emitter:\r\n\r\n```\r\nllvm_ir::IrArray::Index dim_index(1, ir_builder_->getInt64(i));\r\n```\r\nI believe it is an index into the vector of indices, and that the CreateZExtOrBitCast function will successfully extend the int32 to whatever the native size of the target IR is.\r\n\r\n", "Actually - despite the potential for more type-specific constants in the future, I'm abandoning this one.\r\n"]}, {"number": 10190, "title": "tf.contrib.keras Conv2DTranspose output shape undefined", "body": "The output shape for Conv2DTranspose is not fixed even when the input shape is fixed. The output shape for simple convolutioncomes to be correct.\r\n\r\n```\r\nimport tensorflow as tf\r\nL = tf.contrib.keras.layers.Conv2DTranspose( 512, (4,4), strides=(4, 4), padding='valid', data_format='channels_first' )\r\nL2 = tf.contrib.keras.layers.Conv2D( 512, (4,4), strides=(4, 4), padding='valid', data_format='channels_first' )\r\n\r\nimg = tf.placeholder(tf.float32, shape=( 16 , 3 , 64 , 64 ))\r\n\r\nx1 = L(img)\r\nx2 = L2(img)\r\n\r\nprint x1.get_shape() # why ???\r\nprint x2.get_shape() # this is fine\r\n```\r\nOutputs : \r\n```\r\n(?, 512, ?, ?)\r\n(16, 512, 16, 16)\r\n```", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 10189, "title": "[Tensorboard] AssertionError: Cannot find .runfiles directory for /usr/bin/tensorboard", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 25\r\n- **PIP Version**: 9.0.1\r\n- **Tensorboard Version**: 1.0.0a7\r\n\r\n\r\n### Describe the problem\r\nWhen installing Tensorboard with pip, it is not executeable and producing following error.\r\nI could reproduce this on my laptop and Tower both running Fedora 25.\r\nIs there some issue with the tensorboard whl?\r\n\r\n### Install\r\n\r\n```\r\n$ sudo pip3 install tensorboard\r\nCollecting tensorboard\r\n  Using cached tensorboard-1.0.0a7-cp35-cp35m-manylinux1_x86_64.whl\r\nRequirement already satisfied: protobuf>=3.1.0 in /usr/lib64/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: Pillow>=4.0.0 in /usr/lib64/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: numpy>=1.11.0 in /usr/lib64/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: wheel>=0.26 in /usr/lib/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: werkzeug>=0.11.10 in /usr/lib64/python3.5/site-packages (from tensorboard)\r\nRequirement already satisfied: setuptools in /usr/lib/python3.5/site-packages (from protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: olefile in /usr/lib/python3.5/site-packages (from Pillow>=4.0.0->tensorboard)\r\nRequirement already satisfied: packaging>=16.8 in /usr/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: appdirs>=1.4.0 in /usr/lib/python3.5/site-packages (from setuptools->protobuf>=3.1.0->tensorboard)\r\nRequirement already satisfied: pyparsing in /usr/lib/python3.5/site-packages (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorboard)\r\nInstalling collected packages: tensorboard\r\nSuccessfully installed tensorboard-1.0.0a7\r\n```\r\n\r\n### Starting tensorboard\r\n\r\nStarting by just running ```tesorboard``` in terminal:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/bin/tensorboard\", line 160, in <module>\r\n    Main()\r\n  File \"/usr/bin/tensorboard\", line 110, in Main\r\n    module_space = FindModuleSpace()\r\n  File \"/usr/bin/tensorboard\", line 91, in FindModuleSpace\r\n    sys.argv[0])\r\nAssertionError: Cannot find .runfiles directory for /usr/bin/tensorboard\r\n```\r\n", "comments": ["Same issue with pip version 9.0.1, with the same Tensorboard Version .", "Please do not `pip install tensorboard`. We didn't create that package. TensorBoard comes included with `pip install tensorflow`.", "@jart But how could I update/reinstall the tensorboard. I have uninstall tensorboard just now. ", "You can `pip install tensorflow-tensorboard` or `pip install tb-nightly`."]}, {"number": 10188, "title": "Update mnist.py", "body": "I am reading/learning the code, Read by Refactoring. For clean and clear.\r\n\ud83e\udd42 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins test this please."]}, {"number": 10187, "title": "inter_op_parallelism_threads on Window for GPU models", "body": "I need to limit CPU usage of tensorflow on Windows by setting inter_op_parallelism_threads = 1 , ( intra_op_parallelism_threads = 1  also for CPU)\r\n\r\nWhen I run a model on CPU, inter_op_parallelism_threads = 1  works perfectly, and only one logical core is used.\r\n\r\nBut when I run a model on GPU,  inter_op_parallelism_threads =1 doesn't work,  tensorflow still uses all the available logical cores. ", "comments": ["This might be a duplicate of https://github.com/tensorflow/tensorflow/issues/4455. See mrry's comment. Maybe what you want to do is pass that option to the server."]}, {"number": 10186, "title": "Bug in r1.2rc", "body": "[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/kernels/image_ops.h#L69](url)\r\n\r\nstatic will cause a compile error(Its a TODO:) )", "comments": ["Do you upgrade via pip? I want to upgrade to 1.2 while it's still 1.1 in pip.", "Our CI does not indicate any failure.\r\nCould you include more information?\r\nWhat OS do you use, what is your compiler version?\r\nIf you can provide all the information requested in the issue template, we may be able to help.\r\nOtherwise, as I said our CI does not catch a failure here, so I will have to close the issue.", "Linux 3.10.0-327.36.3.el7.x86_64 #1 SMP Mon Oct 24 16:09:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3.10.0-327.36.3.el7.x86_64 #1 SMP Mon Oct 24 16:09:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.0rc0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0-rc0\r\ntf.GIT_VERSION = unknown\r\ntf.COMPILER_VERSION = unknown\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /opt/intel/mkl/lib/intel64/:/usr/local/cuda-7.5/lib64:/usr/local/compiled_program/lib:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri May 26 10:17:12 2017       \r\n+------------------------------------------------------+                       \r\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |                       \r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K40m          Off  | 0000:02:00.0     Off |                    0 |\r\n| N/A   29C    P0    61W / 235W |     22MiB / 11519MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K40m          Off  | 0000:03:00.0     Off |                    0 |\r\n| N/A   31C    P0    62W / 235W |     22MiB / 11519MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla K40m          Off  | 0000:83:00.0     Off |                    0 |\r\n| N/A   30C    P0    62W / 235W |     22MiB / 11519MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla K40m          Off  | 0000:84:00.0     Off |                    0 |\r\n| N/A   29C    P0    66W / 235W |     22MiB / 11519MiB |     97%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib64/libcudart_static.a\r\n/usr/local/cuda-7.5/lib/libcudart.so.7.5.18\r\n/usr/local/cuda-7.5/lib/libcudart_static.a\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-7.5/doc/man/man7/libcudart.7", "I see that you are using cuda 7.5.\r\nWe have support for cuda 8, it is possible build with cuda 7.5 is broken.\r\nI recommend upgrading CUDA installed in your system.", "Ok, tks, I post it here just to remind others if they encounter the same problem, i think it will be fixed in the future."]}, {"number": 10185, "title": "Unable to create so file ", "body": "I have got the build file in my` tensorflow-master/tensorflow` directory. Following is the build file contents\r\n\r\n```\r\ncc_binary(\r\n    name = \"libtensorflow.so\",\r\n    copts = tf_copts(),\r\n    linkshared = 1,\r\n    linkopts = [\r\n        \"-lpthread\",\r\n        \"-lm\",\r\n    ],\r\n    deps = [\r\n        \":cc_ops\",\r\n        \"//tensorflow/core:kernels\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n```\r\n\r\nExecuting this build file, throws the following error\r\n```\r\n/Users/Johnny/Downloads/tensorflow-master/tensorflow/BUILD: line 1: syntax error near unexpected token `newline'\r\n/Users/Johnny/Downloads/tensorflow-master/tensorflow/BUILD: line 1: `cc_binary('\r\nlogout\r\n```", "comments": ["BUILD files are run with the `bazel` command, rather than bash."]}, {"number": 10184, "title": "the performance is Unexpectedly in iOS", "body": "today i test Tensorflow(TF) iOS example with my iPhone 6S , according to the introduction in TF Website and source code , i know it use Apple's Accelerate framework , i build the protobuf , and TF's source code in my Mac , then run iOS example , i record the time with the code\r\n```\r\ntensorflow::Status run_status = tf_session->Run(\r\n        {{input_layer_name, image_tensor}}, {output_layer_name}, {}, &outputs);\r\n```\r\nand the time is fast, only 90ms, i know TF's iOS example use the Google Inception V1 Model , and i test Apple's example which use Google Inception V3 Model , the time is 120ms, metal is more slow than Accelerate framework ? i can not understand . i do not think there is too much different feature that affect performance between inception V1 and V3... so how to explain it ?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, there is a performance difference between InceptionV1 and InceptionV3 - the architectures are sufficiently different that there will be a significant performance difference between them."]}, {"number": 10183, "title": "tf.nn.dynamic_rnn seems not working when given sequence_length", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.1\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 1.2.0rc0\r\n- **CUDA/cuDNN version**: CUDA: 8.0 cuDNN: 5.1\r\n- **GPU model and memory**: GTX 1080 8GB\r\n\r\n### Describe the problem\r\nI tried to migrate my code from tensorflow r1.1.0 to tensorflow r1.2.0rc0.\r\nWhen using tf.nn.dynamic_rnn, if sequence_length is not None, It got an error(traceback pasted below).\r\nHowever, if I set sequence_length to None, it works properly as before.\r\nIs this a bug or I got something wrong?\r\n\r\n### Source code / logs\r\nSource code:\r\n``` python\r\ntf.nn.dynamic_rnn(\r\n    cell,\r\n    inputs,\r\n    dtype=tf.float32,\r\n    sequence_length=lens)\r\n```\r\nwhere lens is a Tensor:\r\n```\r\nTensor(\"Gather_DequeueMany:0\", shape=(256,), dtype=int32)\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\", line 671, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shapes must be equal rank, but are 0 and 1 for 'rnn/while/Select_4' (op: 'Select') with input shapes: [256], [], [].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 74, in <module>\r\n    tf.app.run()\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 70, in main\r\n    trainer = ModelTrainer(config)\r\n  File \"/home/carbon/Codes/seq2seqmapmatching/model_trainer.py\", line 25, in __init__\r\n    self.model.construct(self.train_reader.queue, self.valid_reader.queue, self.global_step)\r\n  File \"/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py\", line 35, in construct\r\n    global_step=global_step)\r\n  File \"/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py\", line 118, in build_model\r\n    outputs, decoder_state = self.build_decoder(encoder_outputs, inputs, per_h, lens, pts_lens)\r\n  File \"/home/carbon/Codes/seq2seqmapmatching/seq2seq_model.py\", line 185, in build_decoder\r\n    sequence_length=lens)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 566, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 729, in _dynamic_rnn_loop\r\n    swap_memory=swap_memory)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 712, in _time_step\r\n    skip_conditionals=True)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 202, in _rnn_step\r\n    final_output_and_state = _copy_some_through(new_output, new_state)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 174, in _copy_some_through\r\n    for state, new_state in zip(flat_state, flat_new_state)]\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 174, in <listcomp>\r\n    for state, new_state in zip(flat_state, flat_new_state)]\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 163, in _copy_one_through\r\n    return array_ops.where(copy_cond, output, new_output)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 2328, in where\r\n    return gen_math_ops._select(condition=condition, t=x, e=y, name=name)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2145, in _select\r\n    name=name)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2508, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1873, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1823, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/home/carbon/py3-tensorflow-1.2/lib/python3.5/site-packages/tensorflow/python/framework/common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Shapes must be equal rank, but are 0 and 1 for 'rnn/while/Select_4' (op: 'Select') with input shapes: [256], [], [].\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10182, "title": "tensorflow: How to access and reuse feature column values?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution Linux Ubuntu 16.04**:\r\n- **TensorFlow installed from pip**:\r\n- **TensorFlow version 1.1.0**:\r\n\r\n### problem\r\n\r\nIn tensorflow's wide & deep model tutorial, it constructed some feature columns like:\r\n\r\n```\r\nwide_columns = [gender, native_country, education, occupation, workclass,\r\n              relationship, age_buckets,\r\n              tf.contrib.layers.crossed_column([education, occupation],\r\n                                               hash_bucket_size=int(1e4)),\r\n              tf.contrib.layers.crossed_column(\r\n                  [age_buckets, education, occupation],\r\n                  hash_bucket_size=int(1e6)),\r\n              tf.contrib.layers.crossed_column([native_country, occupation],\r\n                                               hash_bucket_size=int(1e4))]\r\n```\r\nI want to directly reuse his columns result/values to construct my wide & deep model with another deep learning library, but how can I access these values after feed inputs ? or how to save into file ? \uff08I didn't found any method that export true data\uff09", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10181, "title": "Fix RNN tests broken in contrib move.", "body": "", "comments": []}, {"number": 10180, "title": "MKL_INSTALL_PATH should not be ignore when given", "body": "I'm  trying to build tensorflow from source, just found configure will clear my MKL_INSTALL_PATH", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jhseu I means this script overwrite the MKL_INSTALL_PATH out side configure, which is expected to working inside.\r\n\r\nI'm trying to build a auto compile script like:\r\n\r\n```bash\r\n... \\\r\nMKL_INSTALL_PATH=$DEST \\\r\n... \\\r\n./configure\r\n```", "@jhseu You misunderstood my intention. I'm trying to make the MKL_INSTALL_PATH configurable outside configure file. ", "@jhseu Thank you for the review. That's my mistake.\r\nI think it works now :)", "@jhseu Is `$fromuser` set right?  I'm a little confused about it.", "@jhseu Any status on this?", "Jenkins, test this please", "```bash\r\n//tensorflow/python:rmsprop_test                                         PASSED in 30.6s\r\n//tensorflow/python:saver_test                                           PASSED in 33.5s\r\n//tensorflow/python:session_benchmark                                    PASSED in 20.0s\r\n//tensorflow/python:session_manager_test                                 PASSED in 91.6s\r\n//tensorflow/python:slot_creator_test                                    PASSED in 24.9s\r\n//tensorflow/python:special_math_ops_test                                PASSED in 25.5s\r\n//tensorflow/python:split_benchmark                                      PASSED in 21.3s\r\n//tensorflow/python:tensorboard_logging_test                             PASSED in 19.3s\r\n//tensorflow/python:timeline_test                                        PASSED in 26.6s\r\n//tensorflow/python:training_ops_test                                    PASSED in 26.8s\r\n//tensorflow/python:transpose_benchmark                                  PASSED in 18.5s\r\n//tensorflow/python/kernel_tests:stage_op_test                          TIMEOUT in 1 out of 2 in 329.1s\r\n  Stats over 2 runs: max = 329.1s, min = 35.4s, avg = 182.2s, dev = 146.8s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-gpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local_linux-opt/testlogs/tensorflow/python/kernel_tests/stage_op_test/shard_1_of_2/test.log\r\n\r\nExecuted 311 out of 311 tests: 310 tests pass and 1 fails locally.\r\n\r\nParameterized build ends with FAILURE at: Mon Jun  5 22:01:34 UTC 2017 (Elapsed time: 2508 s)\r\nBuild step 'Execute shell' marked build as failure\r\n[Set GitHub commit status (universal)] ERROR on repos [] (sha:2dc13c0) with context:tensorflow-pull-requests-gpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of c6e36aa11089ab7283a8a421016c86d191f1220c to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5116/ and message: 'FAILURE\r\n '\r\nUsing context: Linux GPU\r\nFinished: FAILURE\r\n```\r\n\r\nI have no idea about the failure", "Jenkins, test this please", "@jhseu \r\n", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 10179, "title": "Support python3 on Docker image tensorflow/tensorflow:latest", "body": "### System information\r\n\r\nI'm running the `tensorflow/tensorflow:latest` Docker image\r\n\r\n```bash\r\n$ docker run -it --rm tensorflow/tensorflow python3\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named 'tensorflow'\r\n```\r\n\r\nNote that the same works perfectly for Python 2:\r\n\r\n```bash\r\n\u2717 docker run -it --rm tensorflow/tensorflow python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>>\r\n```\r\n\r\n### Describe the problem\r\n\r\nImporting tensorflow from python3 fails for the tensorflow/tensorflow Docker image\r\nThis is surprising because python3 itself is installed, so the solution is:\r\n\r\na) make tensorflow be importable from python3\r\nb) remote python3 so it's obvious you need to use a different tag (point at it, maybe)", "comments": ["We do support Python 3000 on Docker. It's just released under different [tags](https://hub.docker.com/r/tensorflow/tensorflow/tags/).\r\n\r\nTry `tensorflow/tensorflow:latest-py3`.", "I understand there is a different tag, but this is confusing no matter how much you document it.\r\n\r\nWhy isn't tensorflow supported for Python3 on `latest`?\r\nIf it's for image size concerns, shouldn't we also remove `python3` completely from `latest`?\r\n\r\nThanks", "@gunan our friend is curious as to why the `python3` command exists on our docker tag `latest` if you need to install `latest-py3` for it to work.", "The reason not having both py3 and py2 TF installed on latest is exactly image size.\r\nI think having python3 installed on latest is simply an oversight.\r\n@caisq maybe we missed some of the image size optimizations?", "Aqui esta como instalar python, docker, tensorflow, virtualbox\r\nhttps://youtu.be/OiPjSnKAmSI", "Hey I've got a problem relating to this. When I run `tensorflow/tensorflow` and invoke `python`, the tensorflow python module appears to not be installed. Am I missing something here?\r\n\r\n```\r\n$ docker image ls --no-trunc | grep tensorflow/tensorflow\r\nREPOSITORY                         TAG                 IMAGE ID                                                                  CREATED             SIZE\r\ntensorflow/tensorflow              latest              sha256:a2d1671e8a9373d617402e49124b78bf4c73ad9171d4cd4aaf272a035c4993d6   9 days ago          1.25GB\r\n\r\n$ docker run --rm -ti tensorflow/tensorflow python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n```", "@developius could you try:\r\n```\r\ndocker pull tensorflow/tensorflow\r\ndocker run --rm -ti tensorflow/tensorflow python\r\n```\r\nThe image works ok for me.", "That is really weird. The image checksums are the same yet the new one works.", "One more data point.\r\nIt looks  like no matter which python version you try to use, bazel requires us to have python to be able to run any python program, because it inserts the shebang line \"/usr/bin/env python\".\r\n\r\nThis means on python2 image, we do not need python3, but on python3 image we need both python2 and python 3.", "Reassigning to @angersson as he will work on restructuring some of our docker files.", "This should go away when https://github.com/tensorflow/community/pull/8 gets implemented.", "A related question is: why are the images based on python`3.5.2` ? Is it just an inherent lib from Ubuntu 16 ? If yes, any concerns to upgrade the Ubuntu version ?", "Nagging Assignee @angersson: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I don't think we're going to be changing anything in the current Dockerfiles for this, but anyone is welcome to make further changes to the Dockerfiles to improve documentation, etc.\r\n\r\n@ddragosd We probably are limited to this version. See https://github.com/tensorflow/tensorflow/pull/23066."]}, {"number": 10178, "title": "Setter for Tensor.shape?", "body": "`tensor.shape` is equivalent to `tensor.get_shape()` in TF 1.0 (see https://github.com/tensorflow/tensorflow/issues/586).\r\n\r\nShould we also make `tensor.shape = new_shape` equivalent to `tensor.set_shape(new_shape)`? This feels natural and results in slightly more idiomatic Python.\r\n\r\nThis would be straightforwardly done with a setter method, e.g.,\r\n```\r\n@shape.setter\r\ndef shape(self, new_shape):\r\n    self.set_shape(new_shape)\r\n```", "comments": ["From the document: \"set_shape(shape) Updates the shape of this tensor.\"\r\n`set_shape` doesn't really set the shape to something else. It updates the shape, i.e. you cannot set a tensor of shape [10,100] to [10, None].\r\n\r\nSo I feel it would be even more confusing if assignment statements like `tensor.shape = new_shape` is allowed. `tensor.shape |= new_shape` matches the semantics better.", "I agree that `tensor.shape |= new_shape` looks better, but I don't think we can make it work with Python syntax, unless `TensorShape` contains a reference back to the containing `Tensor` (which would be a back ideal). Syntactically, this is equivalent to:\r\n```python\r\nshape = tensor.shape\r\nshape |= new_shape\r\n```\r\n\r\nIt's not unheard of for properties to have constraints on how you can set them. For example, you can't set `numpy_array.shape = new_shape` if `new_shape` has a different size.\r\n\r\nI guess my main argument is that a method name like `set_shape` really sounds like it should be a setter method instead. If we called it `update_shape` I would not be arguing for this :).", "I'd argue to keeping this set_shape() and in the future renaming it. I am concerned about portraying the idea that this is equivalent to the idea of numpy shape... i.e.\r\nfoo = np.array([[1,2,3],[4,5,6]])\r\nfoo.shpae = (6,)\r\nis equivalent to  foo = np.reshape(foo, (6,))\r\nThough in TensorFlow this doesn't work, and probably would require returning a new op rather than changing all instances in place.\r\n@yifeif and @martinwicke might have views as well.", "I agree -- it's not quite a numpy shape, and so it's reasonable to make operations explicit.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Ok -- I will close this. Unless you want to keep it open for eventually renaming to update_shape?"]}, {"number": 10177, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 10176, "title": "Make PYTHON_BIN_PATH error go away", "body": "This also makes the logging a little less aggressive.\r\n\r\nFixes #9436\r\n\r\nCC: @nlopezgi", "comments": ["So if I understand correctly, this defaults to /usr/bin/python when we can't find it from explicit configuration. But we don't check whether this actually points to a good python. So this seems more like a bandaid fix rather than getting at the root issue. Am I correct in that analysis? If so, should we at least put a comment in the code saying that this is a hack? I notice in the linked issue you said the root cause may be an upstream issue in bazel.", "Yes this is a bandaid fix, because our Bazel build is currently kind of broken.\r\n\r\n@nlopezgi wrote this Python configuration tool, so I hope he will take ownership of creating a more optimal solution.", "Can we open an issue against bazel and have a link in our code saying essentially \"this is a hack, remove when bazelbuild/issues/4242 is resolved\"?", "Done.", "Looking into a permanent fix for the issue. Apologies for the late reply.", "Tried to repo this error by using standard Dockerfile.devel in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel (comment on https://github.com/tensorflow/tensorflow/issues/9436 indicates they could repro the issue with this setup, iiuc?). Was able to run the build successfully even after removing the default value set in https://github.com/tensorflow/tensorflow/blob/master/third_party/py/python_configure.bzl#L246. Ran build successfuly with both bazel 0.4.5 and 0.5.1. Do you have any pointers as to how to reliably repro this error so I can diagnose better. Thanks."]}, {"number": 10175, "title": "Add is_closed() method to Queue", "body": "to show if a Queue of any type is closed or not, mainly for debugging purposes. Fixes #7355.", "comments": ["Can one of the admins verify this patch?", "Jenkins test this please.", "@girving, you may also be interested to take a look since you discussed this on #7355", "Jenkins, test this please.", "@cwhipkey Looks like the tests failed for tool reasons.  Should I ping you or someone else?", "Both failures seem to be issues on infra side.\r\nI took the bad machines offline. now should be better.\r\n\r\nJenkins, test this please.", "@martinwicke Who merges these days?", "This week, @andrewharp.", "Should get API review.", "This is fine for API review assuming that one change is made.", "Jenkins, test this please.", "Jenkins, test this please.", "The two tests failing are the api-compatibility-test and sparse_ops_test.py.\r\nThey have also failed other PRs, as seen in #10671 and #10673 6 hours ago.", "The API test is failing because you changed the API.  Please follow the instructions to make it pass:\r\n\r\n    If this test fails, it means a change has been made to the public API. Backwards\r\n    incompatible changes are not allowed. You can run the test as follows to update\r\n    test goldens and package them with your change.\r\n\r\n        $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n        $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n              --update_goldens True", "@girving Done. Could you please confirm that this is what is needed? ", "@a-lattas Yep, that's perfect.\r\n\r\nJenkins, test this please.", "@martinwicke The API compatibility test still fails, but it looks a lot like the update script was correctly run.  The update script only changed the `QueueBase` API, though, the errors are for the derived queue classes:\r\n\r\n    ERROR:tensorflow:5 differences found between API and golden.\r\n    ERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.PaddingFIFOQueue.\r\n    ERROR:tensorflow:Issue 2\t: Change detected in python object: tensorflow.PriorityQueue.\r\n    ERROR:tensorflow:Issue 3\t: Change detected in python object: tensorflow.RandomShuffleQueue.\r\n    ERROR:tensorflow:Issue 4\t: Change detected in python object: tensorflow.QueueBase.\r\n    ERROR:tensorflow:Issue 5\t: Change detected in python object: tensorflow.FIFOQueue.\r\n\r\nIs this a bug in the update script or test?", "Hmm, sorry, I'm wrong: the failure is also for `QueueBase`.  Not sure what's happening then.", "@gunan do you know what could be wrong here?", "Jenkins, test this please.", "Is it possible the PR authors local version is out of sync with the master?\r\nMaybe a pull is needed first?", "@gunan Done. ", "Jenkins, test this please.", "@gunan, no luck. ", "If I have understood the `api-compatibility-test` correctly, it will fail at any change on the API, except when it is told to update the goldens. Following the instructions in its `README`, I ran the command locally and that should allow only the next `api-compatibility-test` to succeed with API changes, by creating a temporary text file in the same folder. My question is, is the CI aware of this? \r\n\r\nAlso note that I added `is_closed` to `Queue_Base` goldens myself. It was not updated automatically. Any thoughts? ", "The pbtxt files that it generated should be included in the change --\nthat's how the CI gets to know about it.\n", "I updated (hardcodded) myself the pbtxt file for QueueBase only. Running \r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\ndid not update them. Should I update it for all Queue Types or all should be updated automatically?", "If they are not updates that is a sign that there is either something wrong\nwith the test, or your changes don't actually result in API changes (which\nin your case, would be troubling).\n", "Thank you for the feedback. I will look into it and come up with an answer shortly.", "I think The problem is still related to syncing.\r\nI checked out your branch, and the test is failing the same way for me.\r\nI think what you needed to do was, first fetch and merge all changes from tensorflow/tensorflow:master. You did this. But you needed to rerun api compatibility test and update goldens again. I can confirm that, when I run api compatibility test on your branch, I see this failure:\r\n```\r\nERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.PaddingFIFOQueue.\r\nERROR:tensorflow:Issue 2\t: Change detected in python object: tensorflow.PriorityQueue.\r\nERROR:tensorflow:Issue 3\t: Change detected in python object: tensorflow.RandomShuffleQueue.\r\nERROR:tensorflow:Issue 4\t: Change detected in python object: tensorflow.QueueBase.\r\nERROR:tensorflow:Issue 5\t: Change detected in python object: tensorflow.FIFOQueue.\r\n```\r\nThen I ran update goldens script. Which made the following modifications:\r\n```\r\n\tmodified:   tensorflow/tools/api/golden/tensorflow.-f-i-f-o-queue.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/tensorflow.-padding-f-i-f-o-queue.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/tensorflow.-priority-queue.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/tensorflow.-queue-base.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/tensorflow.-random-shuffle-queue.pbtxt\r\n```\r\nThen rerunning api compatibility test returns success.\r\n```\r\nRan 2 tests in 1.080s\r\n\r\nOK\r\n```", "@a-lattas any luck with that?", "I apologise for the delay, I am away from home for a few days.\r\n\r\nI can see on @gunan 's post that the `api-compatibility-test` is failing and running it with `update-goldens` let's it progress.\r\n\r\nHowever, when running \r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\nI get something like\r\n```\r\nRan 2 tests in 0.080s (skipped 1)\r\nOK\r\n```\r\nwith no failure message and \r\n```\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\ndoes not make any chaninges to the goldens.\r\n\r\nI have not found the reason why. `skipped 1` could mean that it is skipping the test? Also, I am not using Docker, as it is not described as necessary. Any ideas? ", "It is certainly possible.\r\nApi compatibility test runs on Linux and python 2 only, as different versions of dependencies can trigger failures in the api object names. Is it possible you are on a macbook, or using python3?", "I suppose this is the issue then. I am using Ubuntu, but with python 3.\r\n\r\nI will run the test soon with python 2 and inform you about its result.\r\n\r\nThank you.", "@gunan Thanks! \r\nI did with python2 . Everything worked as expected and goldens got automatically updated. It should be ready for testing.", "Jenkins, test this please.", "The only failure looks unrelated.  @drpngx: Ready for merge!"]}, {"number": 10174, "title": "Register GPU RefExit kernel", "body": "It looks like the code was all set up to support a GPU kernel for `RefExit` (as was done for similar kernels, e.g. `RefEnter`), but `RefExit` was never actually registered for the GPU.  I'm not sure if it was intentionally left out for some reason, or if this was just an oversight.  All I did was add the registration calls and it appears to work.", "comments": ["Can one of the admins verify this patch?", "The more general motivation for this change is that currently all loop Variables are pinned to the CPU, because they are colocated with the CPU-only `RefExit`.  This means that the Variable data gets transferred on and off the GPU every loop iteration.  Adding a GPU kernel for `RefExit` allows the whole loop to run on the GPU, which significantly improves performance.", "Any updates on the review?  Normally I wouldn't push, but it'd be really nice to get this into the 1.2 release if possible, since it's a pretty significant performance bottleneck.  @ebrevdo might be able to quickly comment, since he wrote the GPU RefExit code.", "I believe the GPU_HOST_REF_KERNEL is already being registered [here](https://github.com/drasmuss/tensorflow/blob/51a7b7be792139f0f35877f464b88c50f11190c8/tensorflow/core/kernels/control_flow_ops.cc#L445)", "Are there any other changes you'd suggest @ebrevdo?", "No, I'm away this week so will ask someone else to review.\n\nOn Jun 13, 2017 7:29 AM, \"Daniel Rasmussen\" <notifications@github.com>\nwrote:\n\nAre there any other changes you'd suggest @ebrevdo\n<https://github.com/ebrevdo>?\n\n\u2014\nYou are receiving this because you were mentioned.\n\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/10174#issuecomment-308134751>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/ABtim-skvKejOuztUaxgo_AJ5IQE4UiOks5sDpzigaJpZM4NlpiR>\n.\n", "@keveman can you take a look?", "@tensorflow-jenkins test this please", "I have just ran all internal tests, and looks like this change does not cause any breakages internally.\r\nIt is safe to merge this PR."]}, {"number": 10173, "title": "about cudnn version", "body": "which version of cud should I use?\r\nI saw cudnn has developed to v8, but the tensorflow recommend https://www.tensorflow.org/install/install_mac (V5.1)\r\nshould I use a new one", "comments": ["You are confusing CUDA with cuDNN.\r\nOur website says we support cuDNN 5.1 for prebuilt binaries, so anything else wont work.\r\n\r\nMoreover, with TensorFlow 1.2, we are dropping support for GPUs on MacOS.\r\nI recomment switching to linux if you need to run TF on GPUs."]}, {"number": 10172, "title": "Merge from internal.", "body": "", "comments": ["Jenkins test this please."]}, {"number": 10171, "title": "tf.contrib.rnn.decoder does not require explicitly build encoder?", "body": "As a beginner to tensorflow, I need to build a LSTM encoder decoder framework for images.\r\nThe tf.contrib.seq2seq.decoder does not require explicitly build encoder? Does it take the output from previous step as input? Thus the decoder_inputs are actually the input for encoder?", "comments": ["+1. There is no full usage example available that I'm aware of. The closest thing to a complete example that I have found is in the unit tests; see [test for attention wrapper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/kernel_tests/attention_wrapper_test.py) that builds the decoder.\r\n\r\nThe test makes the point that @leesunfreshing made valid. There's a difference between the encoder outputs and decoder inputs, but it's unclear what the decoder inputs even are (they're random numbers in the test).", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there.\r\n\r\n@danielwatson6 We would be happy to welcome contributions for more example code, if it has tests."]}, {"number": 10170, "title": "re", "body": "", "comments": []}, {"number": 10169, "title": "tf.resize_images return image full of noise but work on resize_image_with_crop_or_pad", "body": "- **OS OSX 10.12**:\r\n- **TensorFlow installed from: source**:\r\n- **TensorFlow version 1.2.0rc**:\r\n- **Bazel version: 0.45**:\r\n- **CUDA: CPU-only**:\r\n- **GPU: CPU-only**:\r\n```\r\nimg = tf.image.decode_jpeg(tf.read_file(path), channels=3)\r\nimg = tf.image.resize_images(img, [200, 200])\r\nsess.run(img)\r\n```\r\n![damaged!](https://cloud.githubusercontent.com/assets/18662395/26419617/de547202-40f2-11e7-82c4-3640990d625c.png)\r\n\r\n\r\n```\r\nimg = tf.image.decode_jpeg(tf.read_file(path), channels=3)\r\nimg = tf.image.resize_image_with_crop_or_pad(img, 200, 200)\r\nsess.run(img)\r\n```\r\n\r\n![worked](https://cloud.githubusercontent.com/assets/18662395/26419629/ec09f1a6-40f2-11e7-9f44-0807633f60da.png)\r\n", "comments": ["This sounds really bad, but I'm not able to reproduce it with 1.2.0rc0.\r\n\r\nIt would help if you could provide more information on how you got the above.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/49262/26479226/866525a2-4186-11e7-8154-407086ecc17f.png)", "please checkout this\r\n@jart \r\n[here](https://github.com/wqj97/tf_issue)", "I believe this happens because:\r\n\r\n- `tf.image.decode_jpeg` returns a `tf.uint8` tensor\r\n- `tf.image.resize_image_with_crop_or_pad` doesn't do any numerical operations on the pixel values, it just crops or pads to change the shape of the input tensor but returns a tensor of the same type. In this case, it's a `tf.uint8` tensor.\r\n- [`tf.image.resize_images`](https://www.tensorflow.org/api_docs/python/tf/image/resize_images) returns a `tf.float32` tensor\r\n- `plt.imshow` expects either a uint8 array or a float array with values between 0 and 1.\r\n\r\nSince the resized tensor is a float tensor but pixel values are not between 0 and 1, it's probably confusing `plt.imgshow`. Something like the following should fix things up:\r\n\r\n```python\r\nimg = tf.image.decode_jpeg(tf.read_file(path), channels=3)\r\nimg = tf.cast(tf.image.resize_images(img, [200, 200]), tf.uint8)\r\nsess.run(img)\r\n```\r\n\r\nLet us know if that works out.", "Yeah~ \r\nIt works\r\nThx!", "I came across this issue and the solution worked for me but in my case I was saving the image back.\r\nSo without casting like this the image has only noise. After casting it is the same image.\r\n\r\nSave after resizing :\r\n \r\n   def resize(image):\r\n    resized_image = tf.cast(tf.image.resize_images(image, [299, 299]), tf.uint8)\r\n    return resized_image\r\n\r\n   document_tensor = Image.fromarray(document_tensor, \"RGB\")\r\n", "@asimshankar Thank you for the response.\r\nBut why does tf.image.convert_image_dtype not able to do the intended dtype conversion and tf.cast instead?", "@ankit1997 \r\nThis might be a late response, but I'd like to help anyone who is searching for the solution.\r\n\r\nThe document for tf.image.convert_image_dtype says that:\r\n\"Images that are represented using floating point values are expected to have values in the range [0,1)\"\r\nHowever, after tf.image.resize_images, the value is float32 in range [0, 255). I am not sure how convert_image_dtype behaves in such case.\r\n\r\nOn the other hand, tf.cast directly casts the dtype without changing the value (well, with rounding of course) , so the output is a normal uint8 image.\r\n"]}, {"number": 10168, "title": "Branch 155393864", "body": "", "comments": []}, {"number": 10167, "title": "Ability to completelly reset tensorflow state (Session.close() does not do that)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: parallels ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**: 0.4.5-jdk7\r\n- **CUDA/cuDNN version**: no cuda\r\n- **GPU model and memory**: no gpu\r\n- **Exact command to reproduce**: see the code below\r\n\r\n### Describe the problem\r\n\r\nBug:\r\nsession.close() seems to imply that all resources would be freed and tensorflow state would be reset.\r\nor Feature request:\r\nIf I'm wrong about session.close(), then there should be a way to reset tensorflow state. \r\n\r\nWhen I first time create a session, tensorflow seem to initialize certain variables that I see as a printout like \"platform Host present with 8 visible devices\". However after I close the session I expect these resources to be freed. At least there would be nice to maybe have an additional command to close the device. After running this \"complete_device_close()\" command, I expect that a new session will initialize device again with the same printout. \r\n\r\nThe lack of this behavior currently does not allow to run tensorflow sessions after a fork even in the case where I know that the parent process is never going to use tensorflow again. The following code hangs:\r\n```python\r\n   import multiprocessing\r\n   import tensorflow as tf\r\n\r\n    def _runner():\r\n        sess = tf.Session()\r\n        sess.run(tf.Variable(0.).initializer)  # this hangs after fork\r\n        sess.close()  # here all resources should be cleared\r\n\r\n    _runner()\r\n\r\n    p = multiprocessing.Process(target=_runner)\r\n    p.start()\r\n    p.join()\r\n```\r\nThere are multiple issues on \"hanging after fork\" topic:\r\nhttps://github.com/tensorflow/tensorflow/issues/5448\r\nhttps://github.com/tensorflow/tensorflow/issues/2448\r\nwith a solution to create a server or not to fork at all. However in my case parent process does not want to use tensorflow anymore so I expect to close it forever and the parent process and reopen it in a child. Suggested workarounds do not work for me because parent tensorflow use and a use in a child after fork happens in totally unrelated parts of the system and at different times and creating any coupling between them would be really ugly. We would better establish a practice to \"properly close tensorflow after you done\".\r\n\r\nNotice that initializing devices in multiple children after fork works fine if parent is not involved:\r\n```python\r\n   import multiprocessing\r\n   import tensorflow as tf\r\n\r\n    def _runner_2(index):\r\n        sess = tf.Session()\r\n        for i in range(10):\r\n            import time\r\n            time.sleep(0.1)\r\n            print('running %d' % index)\r\n            sess.run(tf.Variable(0.).initializer)\r\n        sess.close()\r\n\r\n    for j in range(10):\r\n        p = multiprocessing.Process(target=_runner_2, args=(j,))\r\n        p.start()\r\n```\r\nThis code prints \"platform Host present with 8 visible devices\" 10 times.\r\nThis suggests that there is no underlying reason to prevent sharing of CPU or any other resources in this case (I do not have GPU).\r\n\r\nAlternative phrasing of the feature request would be:\r\n\"Make tensorflow fork-safe after certain full deinitialization command\"\r\n\r\n### Source code / logs\r\nsee above\r\n", "comments": ["TensorFlow is not fork safe. However, it will probably work if you move `import tensorflow as tf` into your `_runner_2` function.", "@jart \r\nThank you for the response. As mentioned in the description, I do know that tensorflow is not fork safe. In fact, I did some research into possible solutions with some references in the issue.  Also, my `_runner_2` works fine already. \r\n\r\nI might have written the description unclear but this is a feature request:\r\n\"Make tensorflow fork-safe after certain full deinitialization command\"\r\n\r\nI might be mistaken but I thought here is the right place to make such a request. If its not, would you be so kind to point me where I can post it?\r\n\r\n\r\n"]}, {"number": 10166, "title": "OOM error when initializing float tensors 1/2 the size of the VRAM (in bytes)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04 (64 bit)\r\n- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu\r\n- **TensorFlow version (use command below)**:  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0/5.1.10-1\r\n- **GPU model and memory**: K40 / 12 GB (also GTX 980 / 4 GB)\r\n- **Exact command to reproduce**:\r\n\r\n`time CUDA_VISIBLE_DEVICES=1 python memory_usage.py`\r\n\r\nTF encounters OOM when I try to initialize an array half the size of the VRAM, but only when it's a FLOAT type (float16, float32, float64). This happens even when `trainable=False`.\r\n\r\nI checked via `nvidia-smi` that no other process is using the GPU.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nvram = 12 * 1024 ** 3 # 12 GB\r\n\r\ndef use_half_vram(dtype):\r\n    n = vram // dtype.size // 2\r\n    x = tf.get_variable('x', shape=[n], dtype=dtype, \\\r\n        initializer=tf.constant_initializer(7), trainable=False)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n# use_half_vram(tf.uint8) # OK\r\n# use_half_vram(tf.int32) # OK\r\n# use_half_vram(tf.int64) # OK\r\n# use_half_vram(tf.float16) # OOM\r\nuse_half_vram(tf.float32) # OOM\r\n# use_half_vram(tf.float64) # OOM\r\n```\r\n\r\n", "comments": ["This is somewhat expected as the current design goes. TensorFlow allocates 95% of all the memory upon initialization. Also the initializer copies a const value to GPU, in addition to the actual variable storage, for the actual assignment to happen. That's why can only have slightly less than 1/2 of the GPU memory for a single variable.\r\n", "@zheng-xq \r\n\r\n> That's why can only have slightly less than 1/2 of the GPU memory for a single variable.\r\n\r\nThis also happens with multiple variables, but not with `int` types or (at least some) other initializers.\r\n\r\nOne would think that constants would be the easiest to initialize in-place rather than copying them from the CPU and via a GPU temporary.\r\n\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing this one unless someone comes up with a better design. Technically, tf.assign could be smart and detect the input is the only copy, and simply take that copy, instead of doing an actual copy. If someone is interested in adding this feature, feel free to reopen or make an contribution."]}, {"number": 10165, "title": "R1.2", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 10164, "title": "[XLA] Move some useful Literal conversion code into LiteralUtil", "body": "This code looks like it should be in LiteralUtil.  It is useful to have it there for our backend, and probably also for allowing literals to be created with explicit types to the constant data type.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Passing this back to @hawkinsp to merge.", "@tensorflow-jenkins test this please", "I notice dnovillo has made a significant change to the LiteralUtil class recently.  he/she should probably be aware of this change.", "ok - this one seems good to merge\r\n", "it would be really useful to merge this one today.  this evening i will try to fix the F16 support in LiteralUtil, and we might end up with two different merges which both modify the same files.", "@hawkinsp this looks good to merge. I've pinged dnovillo to take a look as well.\r\n", "So, this change is actually going in the opposite direction it should've gone.  LiteralUtil is on the way out.  I'm working on a patch that is getting rid of it.\r\n\r\nIt's not a big deal that this change is in, but note that I will be moving the new code inside xla::Literal.", "awesome, thanks guys.  will submit the f16 fix as soon as I can (later tonight as i have to be a taxi driver now)\r\n", "I am sorry -- I did merge this. I made a revert but it sounds like that's probably not worth it, so I'll get rid of that.", "@martinwicke no worries -- @dnovillo mentioned he will be merging it once it's synced downstream. Thanks for the merge!", "Yup, no worry about this one.  I'll roll it up with my patch to get rid of\nLiteralUtil.\n\nOn Tue, Jun 13, 2017 at 12:52 PM, Kay Zhu <notifications@github.com> wrote:\n\n> @martinwicke <https://github.com/martinwicke> no worries -- @dnovillo\n> <https://github.com/dnovillo> mentioned he will be merging it once it's\n> synced downstream. Thanks for the merge!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10164#issuecomment-308228151>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AG9RZ1fjTNY2e0yiQer0C0OlwUd37gKJks5sDuhkgaJpZM4NlVXA>\n> .\n>\n"]}]