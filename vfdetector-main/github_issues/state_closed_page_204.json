[{"number": 48562, "title": "Building the latest 'tflite_runtime' from source for Windows", "body": "**System information**\r\n- Windows 10\r\n- tf-nightly\r\n- Python version: 3.7\r\n- Bazel version:3.7.2\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build from source the latest `tflite_runtime-2.6.0` for windows. using the following command:\r\n\r\n`$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows`\r\nand it fails, the output is below. I took this command from the guide https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow\r\n```\r\n\r\n```\r\n$ bash tensorflow/lite/tools/make/download_dependencies.sh\r\ndownload_dependencies.sh completed successfully.\r\n```\r\n\r\nHere are a list of extra commands that I took just to make sure that everything is in place:\r\n\r\n```\r\n$ bazel --version\r\nbazel 3.7.2\r\n```\r\n\r\n```\r\n$ mintty --version\r\nmintty 3.0.1 (x86_64-pc-msys)\r\n```\r\n\r\n```\r\n$ which python\r\n/e/ProgramData/Anaconda3/envs/python37_april2021_tf24/python\r\n```\r\n\r\n```\r\n$ git --version\r\ngit version 2.22.0.windows.1\r\n```\r\n\r\n```\r\n$ which pip\r\n/e/ProgramData/Anaconda3/envs/python37_april2021_tf24/Scripts/pip\r\n```\r\n\r\n```\r\n$ bash --version\r\nGNU bash, version 4.4.23(1)-release (x86_64-pc-msys)\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nSo when i run the command `$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows`\r\nThis is the output:\r\n\r\n```\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-108808-g1fe211938d1'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --action_env PYTHON_LIB_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/lib/site-packages --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Build options --action_env, --copt, and --host_copt have changed, discarding analysis cache.\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (1 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (14 packages loaded, 15 targets configured)\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 214, column 51, in _create_local_python_repository\r\n                python_include_rule = _symlink_genrule_for_dir(\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 66, column 35, in _symlink_genrule_for_dir\r\n                files = \"\\n\".join(read_dir(repository_ctx, src_dir))\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 113, column 30, in read_dir\r\n                find_result = execute(\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Repository go_sdk instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 1.534s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (14 packages loaded, 15 targets configured)\r\nFAILED: Build did NOT complete successfully (14 packages loaded, 15 targets configured)\r\n\r\n```\r\n\r\n", "comments": ["@yyoon @terryheo could you take a look?", "Could you try the followings?\r\n```\r\n$ set CI_BUILD_PYTHON=python\r\n$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n```", "> Could you try the followings?\r\n> \r\n> ```\r\n> $ set CI_BUILD_PYTHON=python\r\n> $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n> ```\r\n\r\n\r\nIt still doesn't build, below you see the output. \r\nIf you look carefully, the error says `The system cannot find the path specified.` Now, I am a little unsure but I think it has something to do with the fact that Windows uses Backslash instead of Forward slash in the path naming. \r\n\r\n```\r\n ~/Downloads $ git clone https://github.com/tensorflow/tensorflow.git\r\n ~/Downloads $ cd tensorflow\r\n ~/Downloads/tensorflow (master) $ set CI_BUILD_PYTHON=python\r\n ~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-109346-g946a3db8047'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/90fc2c296c157e0055013be313925f504d6fed47.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (0 packages loaded, 0 targets configured)\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Repository go_sdk instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 0.600s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n\r\n```", "Could you confirm if you can follow steps in the doc?\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\n```\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```", "> Could you confirm if you can follow steps in the doc?\r\n> https://www.tensorflow.org/install/source_windows\r\n> \r\n> ```\r\n> bazel build //tensorflow/tools/pip_package:build_pip_package\r\n> ```\r\n\r\nI followed all steps from (https://www.tensorflow.org/install/source_windows)\r\nHere is an image of the ENV var\r\n\r\n![image](https://user-images.githubusercontent.com/20945569/116048661-ac7a1b00-a675-11eb-8b10-75560e290c39.png)\r\n\r\n\r\nThe command `bazel build //tensorflow/tools/pip_package:build_pip_package` also fails and i guess it bacuase of `The system cannot find the path specified` but it doesn't say which file it cannot find\r\n\r\n```\r\n$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/90fc2c296c157e0055013be313925f504d6fed47.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Build options --copt and --define have changed, discarding analysis cache.\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (48 packages loaded, 14 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (49 packages loaded, 30 targets configured)\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 267, column 40, in _python_autoconf_impl\r\n                _create_local_python_repository(repository_ctx)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Repository astunparse_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:396:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository termcolor_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:443:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository go_sdk instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/toolchain/toolchains.bzl:379:28: in go_register_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:65:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/io_bazel_rules_go/go/private/sdk.bzl:53:35: in <toplevel>\r\nINFO: Repository wrapt instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1071:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository functools32_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:419:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository opt_einsum_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:478:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository rules_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:964:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository gast_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:431:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository dill_archive instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1109:21: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:530:20: in _tf_repositories\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:112:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/repo.bzl:65:35: in <toplevel>\r\nINFO: Repository rules_proto instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:120:20: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:29:18: in grpc_extra_deps\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/com_google_protobuf/protobuf_deps.bzl:42:21: in protobuf_deps\r\nRepository rule http_archive defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 3.660s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (158 packages loaded, 1905 targets configured)\r\nFAILED: Build did NOT complete successfully (158 packages loaded, 1905 targets configured)\r\n\r\n\r\n```", "I could see some differences with my configuration.\r\n\r\n- I'm using Bazel 4.0.0\r\n- I'm using Python from https://www.python.org/downloads/windows/\r\n\r\nIt could be worth to try these.", "> I could see some differences with my configuration.\r\n> \r\n> * I'm using Bazel 4.0.0\r\n> * I'm using Python from https://www.python.org/downloads/windows/\r\n> \r\n> It could be worth to try these.\r\n\r\nOk, now i installed Bazel 4.0.0 and python 3.7.9 from https://www.python.org/downloads/windows/ below again is the list of command just to check that everything is in place. Then i run the command `$ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows` this time the error was something else ```\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n) ``` you can see below\r\n\r\n\r\n\r\n```\r\n~ $ bazel --version\r\nbazel 4.0.0\r\n\r\n```\r\n\r\n\r\n```\r\n~ $ bash --version\r\nGNU bash, version 4.4.23(1)-release (x86_64-pc-msys)\r\n```\r\n\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ python ./configure.py\r\nYou have bazel 4.0.0 installed.\r\nPlease downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ which python\r\n/c/Users/radus/AppData/Local/Programs/Python/Python37/python\r\n```\r\n\r\n\r\n```\r\n\r\n~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-109346-g946a3db8047'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --action_env PYTHON_LIB_PATH=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/lib/site-packages --python_path=E:/ProgramData/Anaconda3/envs/python37_april2021_tf24/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (0 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (0 packages loaded, 0 targets configured)\r\nINFO: Repository local_config_cc instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:106:24: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/repositories.bzl:28:17: in rules_cc_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl:180:16: in cc_configure\r\nRepository rule cc_autoconf defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl:143:30: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl\", line 120, column 36, in cc_autoconf_impl\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 694, column 31, in configure_windows_toolchain\r\n                msvc_vars = _get_msvc_vars(repository_ctx, paths)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 494, column 28, in _get_msvc_vars\r\n                vc_path = _find_vc_path(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 165, column 25, in _find_vc_path\r\n                vc_dir = execute(repository_ctx, [\"./get_vc_dir.bat\"], environment = env)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 203, column 32, in execute\r\n                auto_configure_fail(\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 112, column 9, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s %s\\n\" % (red, no_color, msg))\r\nError in fail:\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n)\r\nERROR: Error fetching repository: Traceback (most recent call last):\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl\", line 120, column 36, in cc_autoconf_impl\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 694, column 31, in configure_windows_toolchain\r\n                msvc_vars = _get_msvc_vars(repository_ctx, paths)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 494, column 28, in _get_msvc_vars\r\n                vc_path = _find_vc_path(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 165, column 25, in _find_vc_path\r\n                vc_dir = execute(repository_ctx, [\"./get_vc_dir.bat\"], environment = env)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 203, column 32, in execute\r\n                auto_configure_fail(\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 112, column 9, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s %s\\n\" % (red, no_color, msg))\r\nError in fail:\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n)\r\nERROR: C:/users/radus/downloads/tensorflow/tensorflow/lite/python/interpreter_wrapper/BUILD:80:17: //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper.so depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//':\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n)\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 1.677s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n```\r\n", "After refresh of all ENV variables and a system restart as well as pulling the latest tensorflow from github i got back the same error as before `The system cannot find the path specified.` This is using Bazel 4.0.0 and Python 3.7 from https://www.python.org/downloads/windows/\r\n\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-109346-g946a3db8047'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Build options --action_env, --copt, --define, and 2 more have changed, discarding analysis cache.\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (1 packages loaded, 0 targets configured)\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nERROR: Error fetching repository: Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 5.873s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (16 packages loaded, 15 targets configured)\r\nFAILED: Build did NOT complete successfully (16 packages loaded, 15 targets configured)\r\n\r\n```", "> SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n> PYTHON=python3\r\n\r\nDid you run `set CI_BUILD_PYTHON=python` ? PYTHON should be just `python`", "> > SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n> > PYTHON=python3\r\n> \r\n> Did you run `set CI_BUILD_PYTHON=python` ? PYTHON should be just `python`\r\n\r\nusing `set CI_BUILD_PYTHON=python` did not change to `PYTHON=python` but when i manually set then `CI_BUILD_PYTHON` in the env list and restarted the system, then it got updated as you can see below.  But still the built process fails. Below is the output.\r\n\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-109346-g946a3db8047'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nStarting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nLoading: 0 packages loaded\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (1 packages loaded, 0 targets configured)\r\nINFO: Repository local_execution_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:85:27: in _tf_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/configs.bzl:6:28: in initialize_rbe_configs\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/tf_toolchains/toolchains/remote_config/rbe_config.bzl:158:27: in _tensorflow_local_config\r\nRepository rule local_python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:275:41: in <toplevel>\r\nINFO: Repository local_config_python instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:1099:19: in workspace\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace2.bzl:95:21: in _tf_toolchains\r\nRepository rule python_configure defined at:\r\n  C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl:294:35: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_execution_config_python':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nERROR: Error fetching repository: Traceback (most recent call last):\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 210, column 33, in _create_local_python_repository\r\n                python_lib = _get_python_lib(repository_ctx, python_bin)\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/py/python_configure.bzl\", line 130, column 21, in _get_python_lib\r\n                result = execute(repository_ctx, [python_bin, \"-c\", cmd])\r\n        File \"C:/users/radus/downloads/tensorflow/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nThe system cannot find the path specified.\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Repository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 9.874s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (16 packages loaded, 15 targets configured)\r\nFAILED: Build did NOT complete successfully (16 packages loaded, 15 targets configured)\r\n\r\n```\r\n", "Did you run `configure` script?\r\nI wonder if `PYTHON_BIN_PATH` is configured correctly.\r\n", "> Did you run `configure` script?\r\n> I wonder if `PYTHON_BIN_PATH` is configured correctly.\r\n\r\nWhen i run the configure with bazel 4.0.0  i get `Please downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow`\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ python ./configure.py\r\nYou have bazel 4.0.0 installed.\r\nPlease downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n\r\n```\r\n\r\nwhen i run it with bazel 3.7.0 then it runs ok, below is the output\r\n\r\n```\r\n~/Downloads/tensorflow (master) $ python ./configure.py\r\nYou have bazel 3.7.2 installed.\r\nFound possible Python library paths:\r\n  C:\\Users\\radus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\radus\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n```\r\n\r\nI also ran `download_dependencies.sh` as you can see below:\r\n\r\n```\r\n ~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/make/download_dependencies.sh\r\n..\r\ndownload_dependencies.sh completed successfully.\r\n\r\n```\r\n\r\n\r\nSince the `configure` script doesn't run with bazel 4.0, i downgraded to bazel 3.7, and after i ran `build_pip_package_with_bazel.sh` command, here is the new output\r\n\r\n```\r\n\r\n ~/Downloads/tensorflow (master) $ bash tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh windows\r\n+++ dirname tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh\r\n++ cd tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ grep '_VERSION = ' /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ cut -d= -f2\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.6.0\r\n+ export PACKAGE_VERSION=2.6.0\r\n+ PACKAGE_VERSION=2.6.0\r\n+ BUILD_DIR=/c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ TENSORFLOW_TARGET=windows\r\n+ '[' '!' -z '' ']'\r\n+ rm -rf /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ mkdir -p /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python/tflite_runtime\r\n+ cp -r /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup_with_binary.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ cp /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python/tflite_runtime\r\n+ echo '__version__ = '\\''2.6.0'\\'''\r\n++ git -C /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\n+ echo '__git_version__ = '\\''0.6.0-109346-g946a3db8047'\\'''\r\n+ cd /c/Users/radus/Downloads/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ BAZEL_FLAGS=--copt=-O3\r\n+ export CROSSTOOL_PYTHON_INCLUDE_PATH\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ LIBRARY_EXTENSION=.pyd\r\n+ bazel build -c opt -s --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl --copt=-O3 //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper\r\nWARNING: The following configs were expanded more than once: [monolithic]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\radus\\downloads\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/radus/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/radus/AppData/Local/Programs/Python/Python37/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:noaws in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_aws_support=true\r\nINFO: Found applicable config definition build:nogcp in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_gcp_support=true\r\nINFO: Found applicable config definition build:nohdfs in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:nonccl in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:windows in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\radus\\downloads\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nLoading:\r\nLoading: 0 packages loaded\r\nDEBUG: C:/users/radus/_bazel_radus/tfciaawb/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (0 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper (0 packages loaded, 0 targets configured)\r\nINFO: Repository local_config_cc instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:106:24: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/repositories.bzl:28:17: in rules_cc_toolchains\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl:180:16: in cc_configure\r\nRepository rule cc_autoconf defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl:143:30: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/cc_configure.bzl\", line 120, column 36, in cc_autoconf_impl\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 694, column 31, in configure_windows_toolchain\r\n                msvc_vars = _get_msvc_vars(repository_ctx, paths)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 494, column 28, in _get_msvc_vars\r\n                vc_path = _find_vc_path(repository_ctx)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/windows_cc_configure.bzl\", line 165, column 25, in _find_vc_path\r\n                vc_dir = execute(repository_ctx, [\"./get_vc_dir.bat\"], environment = env)\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 203, column 32, in execute\r\n                auto_configure_fail(\r\n        File \"C:/users/radus/_bazel_radus/tfciaawb/external/rules_cc/cc/private/toolchain/lib_cc_configure.bzl\", line 112, column 9, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s %s\\n\" % (red, no_color, msg))\r\nError in fail:\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n)\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/users/radus/downloads/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/radus/downloads/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/radus/_bazel_radus/tfciaawb/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: C:/users/radus/downloads/tensorflow/tensorflow/lite/python/interpreter_wrapper/BUILD:80:17: //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper.so depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//':\r\nAuto-Configuration Error: non-zero exit code: 1, command [\"./get_vc_dir.bat\"], stderr: (The system cannot find the path specified.\r\n)\r\nERROR: Analysis of target '//tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 1.629s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n```\r\n\r\n\r\n\r\n\r\n", "Ok. Let's forget about Bazel-4.0.0\r\nAlso you don't need to run `tensorflow/lite/tools/make/download_dependencies.sh`. It's only needed for Makefile build.\r\n\r\nIt looks like you're facing a C compiler configuration issue.\r\n\r\nCould you double confirm that you followed all steps in the guide\r\nspecially, https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2019 ?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48562\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48562\">No</a>\n"]}, {"number": 48560, "title": "RTX 3070 gpu usage is low in CNN model training using tensorflow.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below): tensorflow 2.5.0rc1\r\n- Python version: python 3.6.9\r\n- CUDA/cuDNN version: 11.2/8.1\r\n- GPU model and memory: RTX 3070\r\n\r\n**Describe the current behavior**\r\n\r\nI trained the CNN model below using Docker.\r\nIt took 10 minutes per epoch on the GTX 1060, but over 20 minutes per epoch on the RTX 3070. When I checked the gpu usage through the nvidia-smi command, it was mostly 0%.\r\nWhy does the RTX 3070 have a slower training time than the GTX 1060? How do I fix it?\r\n\r\ntensorflow recognizes RTX 3070 and uses vram as well.\r\ndocker used tensorflow/tensorflow:2.5.0rc1-gpu-jupyter\r\nThe cpu is using the intel i7-10700k\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\neffnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)`\r\ndef create_model():\r\n    model = keras.Sequential()\r\n    model.add(effnet)\r\n    model.add(GlobalAveragePooling2D())\r\n    \r\n    model.add(Dense(512))\r\n    model.add(BatchNormalization())\r\n    model.add(ReLU())\r\n    \r\n    model.add(Dense(128))\r\n    model.add(BatchNormalization())\r\n    model.add(ReLU())\r\n    \r\n    model.add(Dense(32))\r\n    model.add(BatchNormalization())\r\n    model.add(ReLU())\r\n    model.add(Dense(num_classes, activation=\"sigmoid\"))\r\n    return model\r\n```\r\n```\r\ndef scheduler(epoch, lr):\r\n    if epoch == 0:\r\n        lr = init_learning_rate\r\n    else:\r\n        lr = lr * 0.9\r\n    \r\n    print('learning rate: {:.10f}'.format(lr))\r\n    tf.summary.scalar('learning_rate', data=lr, step=epoch)\r\n    return lr\r\n\r\nlearning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\n```\r\n```\r\nmodel = create_model()\r\n\r\nf1_score = tfa.metrics.F1Score(\r\n    num_classes=num_classes,\r\n    average='macro')\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\r\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer,\r\n              metrics=['accuracy',f1_score])\r\n```\r\n```\r\nlogdir = \"/tf/notebooks/compete/Plant_Pathology_2021/logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nfile_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\r\nfile_writer.set_as_default()\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\r\n\r\nhistory = model.fit(\r\n        train_generator,\r\n        steps_per_epoch=len(train_generator),\r\n        epochs=num_epochs,\r\n        validation_data=validation_generator,\r\n        validation_steps=len(validation_generator),\r\n        callbacks=[tensorboard_callback,learning_rate_scheduler],\r\n        verbose=1)\r\n```", "comments": ["@hogbal,\r\nCould you please run the below code snippet and share the results with us\r\n```\r\nimport tensorflow as tf\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n```\r\n\r\nAlso, take a look at the TensorFlow profiling tool [link #1](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras), [link #2](https://www.tensorflow.org/guide/profiler#gpu_kernel_stats) and let us know if it helps. Thanks!", "@amahendrakar\r\nThe result of that code is shown below.\r\n<pre>\r\nINFO:tensorflow:Enabling eager execution\r\nINFO:tensorflow:Enabling v2 tensorshape\r\nINFO:tensorflow:Enabling resource variables\r\nINFO:tensorflow:Enabling tensor equality\r\nINFO:tensorflow:Enabling control flow v2\r\nNum GPUs Available:  1\r\n</pre>\r\n<pre>\r\n2021-04-21 03:38:45.461567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-21 03:38:46.314021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-21 03:38:46.369702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-21 03:38:46.370220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3070 computeCapability: 8.6\r\ncoreClock: 1.755GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-21 03:38:46.370280: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-04-21 03:38:46.372175: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-04-21 03:38:46.372202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-04-21 03:38:46.372774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-04-21 03:38:46.372938: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-04-21 03:38:46.373533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11\r\n2021-04-21 03:38:46.374051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-04-21 03:38:46.374173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-04-21 03:38:46.374236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-21 03:38:46.374610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-21 03:38:46.374908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n</pre>\r\n\r\nAfter using the TensorFlow profiling tool, I will share the results :)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank! As a result of using the TensorFlow profiling tool, it was a problem of input time.\r\nI put an image using a data generator, but it was solved when I changed it to TFRecord.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48560\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48560\">No</a>\n"]}, {"number": 48558, "title": "tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: cloud server\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2..0.0\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: CUDA 10.1 & cuDNN 7.6.5\r\n- GPU model and memory: \r\n00:07.0 VGA compatible controller: NVIDIA Corporation Device 1eb8 (rev a1) (prog-if 00 [VGA controller])\r\n\tSubsystem: NVIDIA Corporation Device 130e\r\n\tPhysical Slot: 7\r\n\tFlags: bus master, fast devsel, latency 0, IRQ 37\r\n\tMemory at fc000000 (32-bit, non-prefetchable) [size=16M]\r\n\tMemory at e0000000 (64-bit, prefetchable) [size=256M]\r\n\tMemory at fa000000 (64-bit, non-prefetchable) [size=32M]\r\n\tI/O ports at c500 [size=128]\r\n\tCapabilities: [68] MSI: Enable+ Count=1/1 Maskable- 64bit+\r\n\tKernel driver in use: nvidia\r\n\tKernel modules: nvidiafb, nouveau, nvidia_drm, nvidia\r\n\r\n![Screen Shot 2021-04-16 at 6 59 05 PM](https://user-images.githubusercontent.com/11975415/115015147-d26d2600-9ee5-11eb-91db-2fb3c863fa4f.png)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHere is how i got the error\r\n\r\n``Python 3.7.7 (default, May  7 2020, 21:25:33) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n2021-04-16 16:46:25.884063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2021-04-16 16:46:25.888707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.889366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:00:07.0 name: GRID T4-4Q computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 3.97GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-04-16 16:46:25.889562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-04-16 16:46:25.891366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-04-16 16:46:25.893293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-04-16 16:46:25.893593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-04-16 16:46:25.895611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-04-16 16:46:25.896780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-04-16 16:46:25.901308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-04-16 16:46:25.901433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.902118: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.902701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-04-16 16:46:25.903031: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2021-04-16 16:46:25.910663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2500000000 Hz\r\n2021-04-16 16:46:25.910940: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa728000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-04-16 16:46:25.910965: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2021-04-16 16:46:25.967935: W tensorflow/compiler/xla/service/platform_util.cc:210] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN: unknown error\r\n2021-04-16 16:46:25.968035: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA\r\n2021-04-16 16:46:25.968258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.968912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:00:07.0 name: GRID T4-4Q computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 3.97GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-04-16 16:46:25.968993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2021-04-16 16:46:25.969017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2021-04-16 16:46:25.969042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2021-04-16 16:46:25.969066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2021-04-16 16:46:25.969089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2021-04-16 16:46:25.969113: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2021-04-16 16:46:25.969140: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2021-04-16 16:46:25.969237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.969898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 16:46:25.970481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2021-04-16 16:46:25.970546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 270, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 95, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"/home/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 515, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIndeed, before this error, I had another error:\r\n\r\nCould not load dynamic library 'libcublas.so.10'; dlerror: libcublasLt.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64:\r\n\r\nI didn't know why, but libcublas.so.10 and libcublasLt.so.10 were in /usr/local/cuda-10.2/targets/x86_64-linux/lib instead of /usr/local/cuda-10.1/lib64, so I copied them both to /usr/local/cuda-10.1/lib64 and it solved the problem. Would there be any connections between these two errors? If no, any help would be appreciated !\r\n", "comments": ["@Leiga \r\nthis seems Nvidia related error, please refer to these links and let us know if it helps:\r\n[Link](https://forums.developer.nvidia.com/t/failure-to-set-vgpu-computing-mode-from-prohibited-to-default/107067/2), [link1](https://stackoverflow.com/questions/41965187/nvidia-device-error-in-tensorflow),[link2](https://github.com/NVIDIA/nvidia-docker/issues/262),[link3](https://stackoverflow.com/questions/6966496/nvidia-cuda-error-all-cuda-capable-devices-are-busy-or-unavailable-on-osx)\r\n\r\nAlso please confirm if you have referred to these [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\nYou may also try:\r\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n\r\n\r\n ", "@Leiga Let's try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and see if it helps. Before that please make sure you close all python sessions and exit python interpreter, we want to make sure your gpu is not utilized any simulations.\r\n```python\r\n# on the top of your script\r\nimport tensorflow as tf\r\ngpus = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n# Paste rest of your code here\r\n```", "> @Leiga Let's try [limiting gpu memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and see if it helps. Before that please make sure you close all python sessions and exit python interpreter, we want to make sure your gpu is not utilized any simulations.\r\n> \r\n> ```python\r\n> # on the top of your script\r\n> import tensorflow as tf\r\n> gpus = tf.config.list_physical_devices('GPU')\r\n> tf.config.experimental.set_memory_growth(gpus[0], True)\r\n> # Paste rest of your code here\r\n> ```\r\n\r\nI have tried putting those on top of my test script and run it. Unfortunately, It produced the same error as mentioned above.  ", "> @Leiga\r\n> this seems Nvidia related error, please refer to these links and let us know if it helps:\r\n> [Link](https://forums.developer.nvidia.com/t/failure-to-set-vgpu-computing-mode-from-prohibited-to-default/107067/2), [link1](https://stackoverflow.com/questions/41965187/nvidia-device-error-in-tensorflow),[link2](https://github.com/NVIDIA/nvidia-docker/issues/262),[link3](https://stackoverflow.com/questions/6966496/nvidia-cuda-error-all-cuda-capable-devices-are-busy-or-unavailable-on-osx)\r\n> \r\n> Also please confirm if you have referred to these [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n> You may also try:\r\n> Restart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\r\n\r\nI have tried using `os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'`  but it did not solve the problem.\r\n\r\nAfter browsing this [link](https://forums.developer.nvidia.com/t/how-to-run-devicequery/54624) you recommended, I found that I had an issue with the Recommended [Actions](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#recommended-post) under Post-Installation Actions from Nvidia docs\r\n\r\nI tried running `/usr/bin/nvidia-persistenced --verbose`\r\n\r\nbut it raised an unexpected error:\r\n\r\n`nvidia-persistenced failed to initialize. Check syslog for more details.`\r\n\r\nAnd I had a trouble running ./deviceQuery. Would those cause the problem?", "Perhaps this [thread ](https://forums.developer.nvidia.com/t/nvidia-persistenced-failed-to-initialize-check-syslog-for-more-details/74052/5)can help you fix \r\n> nvidia-persistenced failed to initialize. Check syslog for more details.", "@Leiga \r\nSame error is reported and solved at Nvidia webiste shared below, please follow it and move this issue to closed status as it is not a tensorflow related issue, in case of any further questions related to same issue you may open a ticket with Nvidia.\r\n\r\nhttps://forums.developer.nvidia.com/t/nvidia-persistenced-failed-to-initialize-check-syslog-for-more-details/74052", "> @Leiga\r\n> Same error is reported and solved at Nvidia webiste shared below, please follow it and move this issue to closed status as it is not a tensorflow related issue, in case of any further questions related to same issue you may open a ticket with Nvidia.\r\n> \r\n> https://forums.developer.nvidia.com/t/nvidia-persistenced-failed-to-initialize-check-syslog-for-more-details/74052\r\n\r\nAre you sure the error `nvidia-persistenced failed to initialize. Check syslog for more details.` is the main cause of `tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable` ? I have checked my nvidia-persistenced status with `systemctl status nvidia-persistenced` and it was active. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48558\">No</a>\n"]}, {"number": 48557, "title": "Why there is no DepthwiseConv1D function in TensorFlow?", "body": "The function DepthwiseConv2D\u3001SeparableConv1D and SeparableConv2D are all supported in TensorFlow, however, the function DepthwiseConv1D is still not supported yet. But in fact, the function of SeparableConv1D is conduct by a sequence of a DepthwiseConv and a PointWiseConv, so it seems that it's very convient to add DepthwiseConv1D function to the TensorFlow.", "comments": ["Sure, that seems reasonable. Would you like to open a PR?", "@fchollet I can work on it.", "@around-star assigning this issue to you. Please link the PR to this issue when you're ready. Thanks!", "@around-star \r\nWould like to work on this with you.\r\nAlso, is it possible to generalize this, like `Conv` and `SeperableConv` layers?", "I think that we could assign two contributors but you need to collaborate on the same external fork cause we expect a single PR. Nikita what do you think?", "I would like to request support for `causal` padding for the new DepthwiseConv1D layer.\r\nThanks :)", "@bhack \r\nMyself and @around-star have some code ready. Unfortunately, we are not able to build it multiple times due to limited resources. Thus, it is has become quite difficult to debug. Can you suggest some solution?\r\n", "We are working on this at https://github.com/tensorflow/tensorflow/pull/48421 \r\n\r\nIn the meantime If you can complete the first build with this param it will easier with the next builds if you don't re-sync too much you branch with master: https://docs.bazel.build/versions/master/remote-caching.html#disk-cache", "@AdityaKane2001 Are you able to complete the first build with your local machine?", "@bhack \r\nI was not able to complete the build, I am getting following error message:\r\n```\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29910\\include\\complex(674): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.28.29910\\include\\complex(674): error C3861: 'copysign': identifier not found```", "It was fixed at https://github.com/microsoft/vcpkg/pull/16315", "@bhack \r\nI updated python to 3.9.2 and reinstalled the MS Build Tools but the error still persists.\r\n", "It was closed as stale on our side  https://github.com/tensorflow/tensorflow/issues/46902 /cc @mihaimaruseac \n\nIn the meantime can you build with out Docker devel image at https://www.tensorflow.org/install/source#docker_linux_builds?\n\n", "Sure, I'll give it a try. Will I be able to use the bazel cache somehow? The previous build had compiled around 11k objects.", "No it will be invalidated as the platform is totally different. Are you sure that you are compiling on updated Build tools with python 3.9.2 cause the `copysign` MACRO was removed in Python 3.9.2 so it seems strange that you still have these conflicts.", "Yes, it seemed a bit weird to me too. \r\nI have installed docker and pulled the image, I'll try to build.", "@bhack \r\nI tried to build using docker. It was going fine for a while, and then it got stuck at\r\n` Compiling tensorflow/core/kernels/bias_op.cc [for host]; 2086s local, remote-cache` for 30 mins or so, thus \r\nI cancelled the build. When I restarted it, it started doing all over again from scratch. ", "Have you compiled with --disk_cache= ?", "Yes I did\r\n", "> Yes I did\n> \n\nHave you restarted the container?", "```cmd\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --disk_cache=../docker_build_cache --local_cpu_resources=4 \r\n --local_ram_resources=HOST_RAM*.5\r\n```\r\nwas the command I ran", ">Have you restarted the container?\r\n\r\n\r\nNot yet", "Remember that the containers are ephimeral. You need to pass the disk cache as volume so It will be persistent across containers destroy/creation instances.", "> You need to pass the disk cache as volume so It will be persistent across containers destroy/creation instances.\r\n\r\nCould you please tell a bit more about that?", "Read https://docs.bazel.build/versions/master/bazel-container.html#build-abseil-project-from-your-host-machine-with-directory-mounting\n\nAnd you need to do the same also for the volume/path that you want to use for the `--disk_cache`", "Thanks a lot, I'll take a look at it.", "@bhack \r\nPlease give me some time to do this. I am quite new to Docker, but I'll try my best. Thanks for your support and patience.", "Ok you have two approaches in that Doc: \n- with a Git checkout on the host\n- with the checkout in the container  (in our image we also already have a Tensorflow checkout).\n\nSo as you like but remember to set a volume mount also for `--disk_cache` directory.", "@bhack \r\nFacing the following error:\r\n```\r\n /tensorflow_src/tensorflow/lite/python/BUILD:55:10 C++ compilation of rule '@llvm-project//llvm:Passes' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 50 argument(s) skipped)\r\n```\r\nI used git checkout in the container, and my fork is about a week old, not more than that.\r\nAs per your instructions, I set up a volume and used it in the build commands. Following are the relevant commands that were executed:\r\n\r\n```\r\n> docker run -it -w /tensorflow_src -v tfcache:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:devel bash  //tfcache is the name of my volume\r\n\r\n# git pull https://github.com/AdityaKane2001/tensorflow.git\r\n# ./configure\r\n#  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --disk_cache /mnt --local_ram_resources=HOST_RAM*.5 --local_cpu_resources=6\r\n\r\n```", "So you have used the first solution (to use the checkout inside the cotaienr).\nBut with the commands I see are you sure that you are building in the right folder?\n\nI see that you have `-w /tensorflow_src` so your already inside the tensorflow master `git checkout` that is available in the image.\n\nIf you want to work with your remote in this  checkout directory I suggest you to just [add a new remote](https://docs.github.com/en/github/getting-started-with-github/managing-remote-repositories#adding-a-remote-repository) with your fork and don't make a complete checkout in that directory.", "> a complete checkout in that directory.\r\n\r\n\r\nDo you mean I should pull my fork in a new directory and work in that directory?", "As you want. You have 3 main solutions:\n\n- You could mount your checkout from your host with an extra `-v`\n- You can add your remote in /tensorflow_src\n- You could checkout in new  different ephimeral directory.\n\nAll these will work with its own pro and cons.", "> You could mount your checkout from your host with an extra -v\r\n\r\nIf I use this method, each time I use `docker run`  with the volumes attached, I'll be ready to go, right? Because right now I committed the container to a new local image. Yet, I'm not sure whether I will be able to access the changes made during the session.", "Yes, in that case you could use a volume or a local host path.\r\n", "Ok. Could you please tell how can I resolve the error?\r\n\r\n```\r\n /tensorflow_src/tensorflow/lite/python/BUILD:55:10 C++ compilation of rule '@llvm-project//llvm:Passes' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 50 argument(s) skipped)```", "I don't think that it is related to your commits, you could rebase or merge the current master on your branch.\r\nAlso you could try to compile only your target (this example is for convolutional you can change the string with your own new layer):\r\n\r\n```\r\nbazel bazel build --config=opt //tensorflow/python/keras/layers:convolutional --disk_cache /mnt --local_ram_resources=HOST_RAM*.5 --local_cpu_resources=6  \r\n```\r\n", "Ok, so as you said earlier, I'll mount two volumes and try to build again. ", "Yes It was basically the first of the two solutions already mentioned at https://docs.bazel.build/versions/master/bazel-container.html#build-abseil-project-from-your-host-machine-with-directory-mounting\r\n\r\nIn that example was `-v /src/workspace:/src/workspace`\r\n\r\nIf you still want to have the same access from your host remember to use \r\n`  -e USER=\"$(id -u)\"   -u=\"$(id -u)\" ` as in the example", "Got it \ud83d\udc4d ", "@bhack \r\nUsed docker compose this time. As discussed, I used 2 volumes: \r\n```\r\n#docker-compose.yaml\r\n\r\nversion: \"3\"\r\n\r\nservices: \r\n    tf:\r\n        image: tensorflow/tensorflow:devel\r\n        volumes: \r\n            - ./cache:/cache\r\n            - ./repo:/repo\r\n    \r\n\r\n```\r\nUsed the following command:\r\n```\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package --disk_cache ../cache --local_ram_resources=HOST_RAM*.5 --local_cpu_resources=4\r\n```\r\n\r\nEncountered the following errors on two successive builds : \r\n```\r\nERROR: /repo/tensorflow/compiler/mlir/tensorflow/BUILD:411:15: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_n_z' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 111 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /repo/tensorflow/lite/python/BUILD:55:10 C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_n_z' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 111 argument(s) skipped)\r\nINFO: Elapsed time: 2011.170s, Critical Path: 631.06s\r\nINFO: 1083 processes: 155 internal, 928 local.\r\n```\r\n\r\nand \r\n\r\n```\r\nERROR: /repo/tensorflow/core/kernels/BUILD:5287:18: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 86 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /repo/tensorflow/lite/python/BUILD:55:10 C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 86 argument(s) skipped)\r\nINFO: Elapsed time: 5235.750s, Critical Path: 444.55s\r\n``` \r\n\r\nCould you please tell how can I resolve this error? \r\nThanks", "Have you rabased your branch? ", "Yes", "Probably you could try to reduce your Resources (especially RAM). See https://github.com/tensorflow/tensorflow/issues/35051#issuecomment-623751066", "I'll try", "If you still have the same issue after tuning the resources please post a comment at https://github.com/tensorflow/tensorflow/issues/39947", "Sure", "@bhack  @nikitamaia \r\n\r\nRequesting to close this, as it is now implemented in [#14863](https://github.com/keras-team/keras/pull/14863) of keras-team/keras.", "@AdityaKane2001 Would it be possible to support `causal` padding?\r\nThis is something I need for a streaming model.", "@andreselizondo-adestech \r\nSorry, but I wasn't the one who contributed this code. Nor am I aware of its internal working. Please open an issue in keras-team/keras for this.", "I'have any permission on this repository. @mihaimaruseac can you close this?", "@bhack \r\nThanks for all the support. Learnt a lot in this one."]}, {"number": 48556, "title": "Create image_classifier in colab have an error. \"unknown image file format. one of jpeg png gif bmp required\"", "body": "![image](https://user-images.githubusercontent.com/44739285/115004251-d04c9c80-9ed0-11eb-924c-7dcf0060dae8.png)\r\nThe images datasets [here](https://babycuatoi.vn/tree.zip)\r\nHelp me please!", "comments": ["@Duynguyen0897 \r\nCan you please share the colab notebook or a minimal standalone reproducible gist?", "> \r\n> \r\n> @Duynguyen0897\r\n> Can you please share the colab notebook or a minimal standalone reproducible gist?\r\n\r\nThe colab notebook [here](https://colab.research.google.com/drive/1hl7WPnzNOEH_MvchRqMGw6omRmy-5Z3y?usp=sharing)\r\nThank you!", "@Duynguyen0897 \r\nThe images in /tree/cam folder are of RGBA type, thus they are not being read properly by the model. Please convert these images to RGB. [This](https://stackoverflow.com/questions/9166400/convert-rgba-png-to-rgb-with-pil) may be helpful.\r\nHope this helps.", "@AdityaKane2001\r\nThank for your help!\r\nI converted to .jpeg format and everything was fine\r\n", "Please close the issue if the query is resolved.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48556\">No</a>\n"]}, {"number": 48554, "title": "RuntimeError: MetaGraphDef associated with tags {''serve'} could not be found in SavedModel.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installation (pip package or built from source):anaconda installation\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):2.3.0\r\n\r\n### 2. Code\r\n I am using, the official code from the tensorflow website:\r\nREFERENCE : https://www.tensorflow.org/lite/convert\r\n\r\n`import tensorflow as tf `                                                                            \r\n                                                                                                                 \r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)`\r\n`tflite_model = converter.convert()                                                            `\r\n`open(\"converted_model.tflite\", \"wb\").write(tflite_model)                         `\r\n\r\n\r\n### 3. ERROR  \r\n\r\nAccording above code,here is an error.\r\n\r\n**RuntimeError :  MetaGraphDef associated with tags {'serve'} could not be found in SavedModel. \r\nTo inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`\r\navailable_tags: [{'train', 'serve'}]**\r\n \r\n\r\n### 4. modify code\r\nI modeify my code:\r\n\r\n`import tensorflow as tf `                                                                            \r\n                                                                                                                \r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir,tags=[{'serve', 'train'}])`\r\n`tflite_model = converter.convert()                                                            `\r\n`open(\"converted_model.tflite\", \"wb\").write(tflite_model)                         `\r\n\r\n\r\nHave an other error:\r\n\r\n **if set(meta_graph_def.meta_info_def.tags) == set(tags):\r\nTypeError: unhashable type: 'set'**\r\n\r\n\r\n### Any other info / logs\r\n\r\nI don't know this modify is true.\r\n\r\nWhen I try to convert this is the error I get, so I cannot successfully convert to Lite.\r\n\r\n\r\n\r\n\r\n", "comments": ["Could you use the `saved_model_cli` tool to inspect the available tags in the given saved model?\r\n\r\nIf the available tags are \"serve\" and \"train\", you need to provide them as a set, e.g.,\r\n\r\ntf.lite.TFLiteConverter.from_saved_model(\r\n    saved_model_dir, tags=set())  # if the tags are empty.\r\n\r\ntf.lite.TFLiteConverter.from_saved_model(\r\n    saved_model_dir, tags=set([\"serve\", \"train\"]))  # if the tags are \"serve\" and \"train\".", "> Could you use the `saved_model_cli` tool to inspect the available tags in the given saved model?\r\n> \r\n> If the available tags are \"serve\" and \"train\", you need to provide them as a set, e.g.,\r\n> \r\n> tf.lite.TFLiteConverter.from_saved_model(\r\n> saved_model_dir, tags=set()) # if the tags are empty.\r\n> \r\n> tf.lite.TFLiteConverter.from_saved_model(\r\n> saved_model_dir, tags=set([\"serve\", \"train\"])) # if the tags are \"serve\" and \"train\".\r\n\r\n\r\nThanks for your reply!\r\n\r\nI use  `saved_model_cli`\r\n\r\nIt report: `MetaGraphDef with tag-set: 'train, serve' contains the following SignatureDefs:`\r\n\r\nAnd I modify my code\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set([\"serve\", \"train\"]))`\r\n\r\nIt report : **ValueError: Only support a single signature key.**\r\n\r\n,I modify my code\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set([\"train\"]))`\r\n\r\nIt report :  **RuntimeError: MetaGraphDef associated with tags {'train'} could not be found in SavedModel. \r\n                 To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`**\r\n\r\n,then I modify my code\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set(['train, serve']))`\r\n\r\nIt report :  **RuntimeError: MetaGraphDef associated with tags {'train, serve'} could not be found in SavedModel. \r\n                 To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`**\r\n\r\n\r\n\r\n**I'm not sure what I should write in the 'set'**\r\n\r\n", "Looks like your model has tags, {'train, serve'}. So, you need to specify tags with ['train', 'serve'].\r\n\r\nThe signatures are different. A signature represents an entry point of the given saved model. You can also get the list of the signatures by the following Python code:\r\n\r\n```\r\nloaded = tf.saved_model.load(\"/tmp/mobilenet/1/\")\r\nprint(list(loaded.signatures.keys()))\r\n```\r\n\r\nAmong the signature keys, you need to pick up the one signature key and provide it through the saved model converter API like the following example:\r\n\r\n```\r\ntf.lite.TFLiteConverter.from_saved_model(\r\nsaved_model_dir, tags=set([\"serve\", \"train\"]), signature_keys=[\"serve_default\"]) \r\n```", "> Looks like your model has tags, {'train, serve'}. So, you need to specify tags with ['train', 'serve'].\r\n> \r\n> The signatures are different. A signature represents an entry point of the given saved model. You can also get the list of the signatures by the following Python code:\r\n> \r\n> ```\r\n> loaded = tf.saved_model.load(\"/tmp/mobilenet/1/\")\r\n> print(list(loaded.signatures.keys()))\r\n> ```\r\n> \r\n> Among the signature keys, you need to pick up the one signature key and provide it through the saved model converter API like the following example:\r\n> \r\n> ```\r\n> tf.lite.TFLiteConverter.from_saved_model(\r\n> saved_model_dir, tags=set([\"serve\", \"train\"]), signature_keys=[\"serve_default\"]) \r\n> ```\r\n\r\nThanks again for your reply!\r\n\r\nI try the python you gave\r\nIt shows this:\r\n`[]`\r\n\r\n`[]`  Is this None?\r\n\r\nSo I modify my code\r\n ### 1\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set([\"serve\", \"train\"]), signature_keys=[\"\"])`\r\nShow this error:\r\n**ValueError: Invalid signature key '' found. Valid keys are ''.**\r\n\r\n### 2   \r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set([\"serve\", \"train\"]), signature_keys=None)`\r\nShow this error:\r\n**ValueError: Only support a single signature key.**  (None is default)\r\n\r\n\r\n\r\n\r\n", "I think your saved model does not any signatures. TFLite converter API needs at least one sigature availabe. In such cases, you can re export your TF graph with explicitly defining a signature. Or, the saved model object will be a callable so you can create a simple concrete function to wrap the callable saved model and then, you can convert it through the concrete function converter API.\r\n\r\nI recommend re-exporting a new saved model with the signaturee information.", "> I think your saved model does not any signatures. TFLite converter API needs at least one sigature availabe. In such cases, you can re export your TF graph with explicitly defining a signature. Or, the saved model object will be a callable so you can create a simple concrete function to wrap the callable saved model and then, you can convert it through the concrete function converter API.\r\n> \r\n> I recommend re-exporting a new saved model with the signaturee information.\r\n\r\n\r\nThank for you replay!\r\n \r\nI re-exporting a new saved model.\r\n\r\nIt has singnaturee ['serving_default']\r\n\r\n\r\nThen,i modify code\r\n\r\n`converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags=set([\"train\", \"serve\"]), signature_keys=['serving_default'])`\r\n\r\n`tflite_model = converter.convert()`\r\n\r\nIt reported (when it executes to that \"`tflite_model = converter.convert()`\"):\r\n\r\n**WARNING:tensorflow:Issue encountered when serializing variables.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. \r\nNote this is a warning and probably safe to ignore.**\r\n\r\n**error: 'tf_executor.graph' op is not any of a builtin TFLite op, a flex TensorFlow op or a custom TensorFlow op\r\nerror: failed while converting: 'main':**\r\n\r\n\r\n\r\n\r\n\r\n", "Please share simple stand alone code such that we can replicate the issue faced or if possible share a colab gist. Or, you can provide the saved model to us if possible.", "\r\nThank for you replay!\r\n\r\n\r\nThis is my saved model:\r\n\r\nhttps://drive.google.com/drive/folders/1zbQ_tR2QlsZP-doHnfA_BXkgGczYwLBD?usp=sharing\r\n\r\nAnd this is from : https://github.com/tegg89/SRCNN-Tensorflow  (I upgrade it to tf 2.3)\r\n\r\nChange the checkpoint in the file and save it as a saved model. \r\n\r\n\r\n**Below is the code I used to convert**\r\n\r\n\r\n    import os   \r\n    import tensorflow as tf\r\n\r\n    trained_checkpoint_prefix = ' ' # ckeckpoint dir\r\n    export_dir = os.path.join('export_dir', '1')\r\n\r\n    graph = tf.Graph()\r\n    with tf.compat.v1.Session(graph=graph) as sess:\r\n   \r\n        loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\r\n        loader.restore(sess, trained_checkpoint_prefix)\r\n\r\n        signature = tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\r\n            method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME)\r\n        signature_map = {tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:signature}\r\n\r\n        builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\r\n        builder.add_meta_graph_and_variables(sess,\r\n                                         [tf.compat.v1.saved_model.SERVING],\r\n                                         signature_def_map=signature_map,\r\n                                         strip_default_attrs=True)\r\n        builder.save()\r\n\r\n\r\n**I want to practice the process of converting saved model to tflite.**", "Looks like the signature in the attached saved model does not have any outputs. Could you create a saved model with the signature, which has valid output signature information as well and try again?", "> Looks like the signature in the attached saved model does not have any outputs. Could you create a saved model with the signature, which has valid output signature information as well and try again?\r\n\r\n\r\n\r\nThank  for you replay!\r\n\r\nSorry, it will take me a long time.\r\n\r\nI am figuring out the archive type of tensorflow.\r\n\r\nAnd i tried successfully.\r\n\r\nI haven't understood the meaning of the tag and signature of saved_model before, but now I understand it roughly.\r\n\r\n**My modify code:**\r\n`      builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\"./saved_model/1\")`\r\n`     signature = tf.compat.v1.saved_model.predict_signature_def(inputs={'Input_images': self.images},\r\n                                                                  outputs={'myOutput': self.pred})`\r\n`      builder.add_meta_graph_and_variables(sess=sess,\r\n                                           tags=[tf.compat.v1.saved_model.tag_constants.SERVING],\r\n                                           signature_def_map={'predict': signature})`\r\n`      builder.save() `\r\n\r\n\r\n\r\n**This is my saved_model(by use saved_model_cli):**\r\n![image](https://user-images.githubusercontent.com/82639664/115984187-53a87500-a5d8-11eb-89d0-7492a4e94adf.png)\r\n\r\n\r\n\r\n\r\nThen I have a question to ask\r\n\r\nIf I have a saved checkpoint and GraphDef.Can I use these two to convert to saved_model?\r\nI wanted to go through these two before.But the signature has not been specified.\r\nThis time I directly changed the source code.\r\n\r\n", "> If I have a saved checkpoint and GraphDef.Can I use these two to convert to saved_model?\r\n\r\nFor the above question, it would be better to ask the new question as a separate issue and invite the TF saved model experts.", "> > If I have a saved checkpoint and GraphDef.Can I use these two to convert to saved_model?\r\n> \r\n> For the above question, it would be better to ask the new question as a separate issue and invite the TF saved model experts.\r\n\r\nOK\r\n\r\nAfter sorting it out, I am asking this question.\r\n\r\nThank you for your reply these few days.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48554\">No</a>\n"]}, {"number": 48553, "title": "Best methods to debug and identify dynamic-sized tensors to work around: TensorFlow Lite Error: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 12 pro\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): v1.12.1-53831-ga8b6d5ff93a 2.5.0-rc0\r\n- Python version: 3.8\r\n\r\n\r\n**Describe the current behavior**\r\nConverted Google's Repnet Model to a tflite model to run on ios. I appear to be using too much ram to be able to run with the cpu, so I am trying to run on gpu. Except `TensorFlow Lite Error: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.` is being thrown at runtime on ios.\r\n\r\n**Describe the expected behavior**\r\nI understand dynamic-sized tensors cannot be used with tflite (edit: Tflite GPU), but how can I debug to identity the use of dynamic-sized tensors?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\r\n\r\n```\r\n# converting to tflite with\r\nmodel = get_repnet_model(PATH_TO_CKPT)\r\n\r\ntf.keras.models.save_model( model,\r\n    \"repnet_savedmodel\"\r\n)\r\n\r\n# Convert the model using TFLiteConverter\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"repnet_savedmodel\")\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE] # .OPTIMIZE_FOR_SIZE]\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\ntflite_model = converter.convert()\r\nwith open(\"repnet.tflite\", 'wb') as f:\r\n  f.write(tflite_model)\r\n\r\n```\r\n\r\n", "comments": ["Actually, TFLite supports dynamic dimension tensors but GPU acceleration is not supported for dynamic dimension tensors.\r\n\r\nIf the CPU based graph execution does satisfy your case, you can consider running the graph without enabling GPU delegate.\r\n\r\nIf the graph does not have any If or While ops and all the tensors can have static shapes (you can determine them with the Netron visualizer), the graph can be accelerated with GPU acceleration.\r\n\r\nSetting the dynamic dimension input tensors with the certain values might fix some or all of the tensors to have static shapes.\r\n\r\nV1 converter APIs can have an easy way to override the input shapes. Please refer to [this](https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_saved_model) and take a look at the input_shape argument.", "@abattery \r\n\r\nYeah as I said,  the model appears to be using too much ram to be able to run on ios with the cpu, so I am trying to run on gpu. But thank you for the quick reply, that's really helpful, I'll check those methods to debug.", "@abattery so after visualizing with Netron I see there are a few while loops in the graph. Are there any good ways to actually determine which parts of the model's code introduce these while loops? As there are no actual while loops in the model's code. Are for loops converted to be represented as while loops?\r\n\r\n\r\n# For example:\r\n<img width=\"727\" alt=\"Screen Shot 2021-04-16 at 10 46 39 AM\" src=\"https://user-images.githubusercontent.com/4157455/115042928-ee4ed880-9ea1-11eb-83be-c31dddd0ab47.png\">\r\n\r\n\r\n\r\n\r\n", "There are node names or tensor names that can be found in the Netron. From those names, you can get hints to locate the original TF graph spot.", "@abattery So I was able to remove the while loops from the graph by changing `tf.map_fn` to `tf.vectorized_map`, and I fixed the input sizes. There are no if statements in the graph. When I look at the graph in Netron there is no node with dynamically sized tensors now. Dynamically sized tensors would have a -1 in its shape, correct?\r\n\r\nBut, I'm still getting the same `TensorFlow Lite Error: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.` error on iOS. What is there anything else I should look for?", "String, resource, and variant type tensors always are categorizded as dynamic tensors. Dynamically sized tensors would have a -1 or None in its shape.\r\n\r\nIs it possible to share your TFLite model?", "@abattery yeah, this is what I have now after slightly modifying the model from here: https://colab.research.google.com/github/google-research/google-research/blob/master/repnet/repnet_colab.ipynb\r\n\r\n```\r\nimport base64\r\nimport io\r\nimport os\r\nimport sys\r\nimport time\r\n\r\nimport cv2\r\n\r\nfrom IPython.display import display\r\nfrom IPython.display import HTML\r\nfrom IPython.display import Javascript\r\n\r\nimport matplotlib\r\nfrom matplotlib.animation import FuncAnimation\r\nimport matplotlib.pyplot as plt\r\n\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom scipy.signal import medfilt\r\n\r\nimport tensorflow as tf\r\n# import tensorflow_addons as tfa\r\nfrom tensorflow import keras\r\n\r\nprint(tf.__version__)\r\n\r\n# Model definition\r\nlayers = tf.keras.layers\r\nregularizers = tf.keras.regularizers\r\n\r\n\r\nclass ResnetPeriodEstimator(tf.keras.Model):\r\n    \"\"\"RepNet model.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            num_frames=64,\r\n            image_size=112,\r\n            base_model_layer_name='conv4_block3_out',\r\n            temperature=13.544,\r\n            dropout_rate=0.25,\r\n            l2_reg_weight=1e-6,\r\n            temporal_conv_channels=512,\r\n            temporal_conv_kernel_size=3,\r\n            temporal_conv_dilation_rate=3,\r\n            conv_channels=32,\r\n            conv_kernel_size=3,\r\n            transformer_layers_config=((512, 4, 512),),\r\n            transformer_dropout_rate=0.0,\r\n            transformer_reorder_ln=True,\r\n            period_fc_channels=(512, 512),\r\n            within_period_fc_channels=(512, 512)):\r\n        super(ResnetPeriodEstimator, self).__init__()\r\n\r\n        # Model params.\r\n        self.num_frames = num_frames\r\n        self.image_size = image_size\r\n\r\n        self.base_model_layer_name = base_model_layer_name\r\n\r\n        self.temperature = temperature\r\n\r\n        self.dropout_rate = dropout_rate\r\n        self.l2_reg_weight = l2_reg_weight\r\n\r\n        self.temporal_conv_channels = temporal_conv_channels\r\n        self.temporal_conv_kernel_size = temporal_conv_kernel_size\r\n        self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\r\n\r\n        self.conv_channels = conv_channels\r\n        self.conv_kernel_size = conv_kernel_size\r\n        # Transformer config.py in form of (channels, heads, bottleneck channels).\r\n        self.transformer_layers_config = transformer_layers_config\r\n        self.transformer_dropout_rate = transformer_dropout_rate\r\n        self.transformer_reorder_ln = transformer_reorder_ln\r\n\r\n        self.period_fc_channels = period_fc_channels\r\n        self.within_period_fc_channels = within_period_fc_channels\r\n\r\n        # Base ResNet50 Model.\r\n        base_model = tf.keras.applications.ResNet50V2(\r\n            include_top=False, weights=None, pooling='max')\r\n        self.base_model = tf.keras.models.Model(\r\n            inputs=base_model.input,\r\n            outputs=base_model.get_layer(self.base_model_layer_name).output)\r\n\r\n        # 3D Conv on k Frames\r\n        self.temporal_conv_layers = [\r\n            layers.Conv3D(self.temporal_conv_channels,\r\n                          self.temporal_conv_kernel_size,\r\n                          padding='same',\r\n                          dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\r\n                          kernel_regularizer=regularizers.l2(self.l2_reg_weight),\r\n                          kernel_initializer='he_normal')]\r\n        self.temporal_bn_layers = [layers.BatchNormalization()\r\n                                   for _ in self.temporal_conv_layers]\r\n\r\n        # Counting Module (Self-sim > Conv > Transformer > Classifier)\r\n        self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\r\n                                            self.conv_kernel_size,\r\n                                            padding='same',\r\n                                            activation=tf.nn.relu)\r\n\r\n        channels = self.transformer_layers_config[0][0]\r\n        self.input_projection = layers.Dense(\r\n            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\r\n            activation=None)\r\n        self.input_projection2 = layers.Dense(\r\n            channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\r\n            activation=None)\r\n\r\n        length = self.num_frames\r\n        self.pos_encoding = tf.compat.v1.get_variable(\r\n            name='resnet_period_estimator/pos_encoding',\r\n            shape=[1, length, 1],\r\n            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\r\n        self.pos_encoding2 = tf.compat.v1.get_variable(\r\n            name='resnet_period_estimator/pos_encoding2',\r\n            shape=[1, length, 1],\r\n            initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\r\n\r\n        self.transformer_layers = []\r\n        for d_model, num_heads, dff in self.transformer_layers_config:\r\n            self.transformer_layers.append(\r\n                TransformerLayer(d_model, num_heads, dff,\r\n                                 self.transformer_dropout_rate,\r\n                                 self.transformer_reorder_ln))\r\n\r\n        self.transformer_layers2 = []\r\n        for d_model, num_heads, dff in self.transformer_layers_config:\r\n            self.transformer_layers2.append(\r\n                TransformerLayer(d_model, num_heads, dff,\r\n                                 self.transformer_dropout_rate,\r\n                                 self.transformer_reorder_ln))\r\n\r\n        # Period Prediction Module.\r\n        self.dropout_layer = layers.Dropout(self.dropout_rate)\r\n        num_preds = self.num_frames // 2\r\n        self.fc_layers = []\r\n        for channels in self.period_fc_channels:\r\n            self.fc_layers.append(layers.Dense(\r\n                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\r\n                activation=tf.nn.relu))\r\n        self.fc_layers.append(layers.Dense(\r\n            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\r\n\r\n        # Within Period Module\r\n        num_preds = 1\r\n        self.within_period_fc_layers = []\r\n        for channels in self.within_period_fc_channels:\r\n            self.within_period_fc_layers.append(layers.Dense(\r\n                channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\r\n                activation=tf.nn.relu))\r\n        self.within_period_fc_layers.append(layers.Dense(\r\n            num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\r\n\r\n    def call(self, x):\r\n        # 1, 64, 112, 112, 3\r\n        # Ensures we are always using the right batch_size during train/eval.\r\n        tf.print(tf.shape(x))\r\n        batch_size = 1 # tf.shape(x)[0]\r\n        # Conv Feature Extractor.\r\n        x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\r\n        x = self.base_model(x)\r\n        tf.print(tf.shape(x))\r\n\r\n        h = 7 # tf.shape(x)[1]\r\n        w = 7 # tf.shape(x)[2]\r\n        c = 1024 # tf.shape(x)[3]\r\n        x = tf.reshape(x, [batch_size, -1, h, w, c])\r\n\r\n        # 3D Conv to give temporal context to per-frame embeddings.\r\n        for bn_layer, conv_layer in zip(self.temporal_bn_layers,\r\n                                        self.temporal_conv_layers):\r\n            tf.print(tf.shape(x))\r\n            x = conv_layer(x)\r\n            x = bn_layer(x)\r\n            x = tf.nn.relu(x)\r\n\r\n        x = tf.reduce_max(x, [2, 3])\r\n\r\n        # Reshape and prepare embs for output.\r\n        final_embs = x\r\n\r\n        # Get self-similarity matrix.\r\n        x = get_sims(x, self.temperature)\r\n\r\n        # 3x3 conv layer on self-similarity matrix.\r\n        x = self.conv_3x3_layer(x)\r\n        x = tf.reshape(x, [batch_size, self.num_frames, -1])\r\n        within_period_x = x\r\n\r\n        # Period prediction.\r\n        x = self.input_projection(x)\r\n        x += self.pos_encoding\r\n        for transformer_layer in self.transformer_layers:\r\n            x = transformer_layer(x)\r\n        x = flatten_sequential_feats(x, batch_size, self.num_frames)\r\n        for fc_layer in self.fc_layers:\r\n            x = self.dropout_layer(x)\r\n            x = fc_layer(x)\r\n\r\n        # Within period prediction.\r\n        within_period_x = self.input_projection2(within_period_x)\r\n        within_period_x += self.pos_encoding2\r\n        for transformer_layer in self.transformer_layers2:\r\n            within_period_x = transformer_layer(within_period_x)\r\n        within_period_x = flatten_sequential_feats(within_period_x,\r\n                                                   batch_size,\r\n                                                   self.num_frames)\r\n        for fc_layer in self.within_period_fc_layers:\r\n            within_period_x = self.dropout_layer(within_period_x)\r\n            within_period_x = fc_layer(within_period_x)\r\n\r\n        return x, within_period_x, final_embs\r\n   \r\n\r\n    # @tf.function(input_signature=[tf.TensorSpec(shape=[None, 32], dtype=tf.float32),\r\n    #                               tf.TensorSpec(shape=[None, 1], dtype=tf.float32)])\r\n    def get_score(self, period_score, within_period_score):\r\n        # tf.print(f\"period_score {period_score} {within_period_score}\", output_stream=sys.stderr)\r\n        \"\"\"Combine the period and periodicity scores.\"\"\"\r\n        within_period_score = tf.nn.sigmoid(within_period_score)[:, 0]\r\n        per_frame_periods = tf.argmax(period_score, axis=-1) + 1\r\n        pred_period_conf = tf.reduce_max(\r\n            tf.nn.softmax(period_score, axis=-1), axis=-1)\r\n        pred_period_conf = tf.where(\r\n            tf.math.less(per_frame_periods, 3), 0.0, pred_period_conf)\r\n        within_period_score *= pred_period_conf\r\n        within_period_score = tf.sqrt(within_period_score)\r\n        pred_score = tf.reduce_mean(within_period_score)\r\n        return pred_score, within_period_score\r\n\r\n\r\ndef get_sims(embs, temperature):\r\n    \"\"\"Calculates self-similarity between batch of sequence of embeddings.\"\"\"\r\n    batch_size = tf.shape(embs)[0]\r\n    seq_len = tf.shape(embs)[1]\r\n    embs = tf.reshape(embs, [batch_size, seq_len, -1])\r\n\r\n    def _get_sims(embs):\r\n        \"\"\"Calculates self-similarity between sequence of embeddings.\"\"\"\r\n        dist = pairwise_l2_distance(embs, embs)\r\n        sims = -1.0 * dist\r\n        return sims\r\n\r\n    sims = tf.vectorized_map(_get_sims, embs, fallback_to_while_loop=False)#, fn_output_signature=tf.TensorSpec((64, 64), dtype=tf.float32))\r\n    # print(\"\\n \\n shape: {} \\n\\n\".format(sims.shape))\r\n    # images = tf.map_fn(pre_input, inputs, dtype=tf.float32, fn_output_signature=tf.TensorSpec((64, 64, 3), dtype=tf.float32))\r\n    sims /= temperature\r\n    sims = tf.nn.softmax(sims, axis=-1)\r\n    # sims = tf.expand_dims(sims, -1)\r\n    #tf.print(sims.shape)\r\n    sims = tf.reshape(sims, (1,64,64, 1))\r\n    return sims\r\n\r\n\r\ndef flatten_sequential_feats(x, batch_size, seq_len):\r\n    \"\"\"Flattens sequential features with known batch size and seq_len.\"\"\"\r\n    x = tf.reshape(x, [batch_size, seq_len, -1])\r\n    return x\r\n\r\n\r\n# Transformer from https://www.tensorflow.org/tutorials/text/transformer .\r\ndef scaled_dot_product_attention(q, k, v, mask):\r\n    \"\"\"Calculate the attention weights.\r\n\r\n    q, k, v must have matching leading dimensions.\r\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\r\n    The mask has different shapes depending on its type(padding or look ahead)\r\n    but it must be broadcastable for addition.\r\n\r\n    Args:\r\n      q: query shape == (..., seq_len_q, depth)\r\n      k: key shape == (..., seq_len_k, depth)\r\n      v: value shape == (..., seq_len_v, depth_v)\r\n      mask: Float tensor with shape broadcastable\r\n            to (..., seq_len_q, seq_len_k). Defaults to None.\r\n\r\n    Returns:\r\n      outputs: shape == (..., seq_len_q, depth_v)\r\n      attention_weights: shape == (..., seq_len_q, seq_len_k)\r\n    \"\"\"\r\n\r\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\r\n\r\n    # scale matmul_qk.\r\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\r\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\r\n\r\n    # add the mask to the scaled tensor.\r\n    if mask is not None:\r\n        scaled_attention_logits += (mask * -1e9)\r\n\r\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\r\n    # add up to 1.\r\n    # (..., seq_len_q, seq_len_k)\r\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\r\n\r\n    outputs = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\r\n\r\n    return outputs, attention_weights\r\n\r\n\r\ndef point_wise_feed_forward_network(d_model, dff):\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Dense(dff, activation='relu'),\r\n        tf.keras.layers.Dense(d_model)\r\n    ])\r\n\r\n\r\nclass MultiHeadAttention(tf.keras.layers.Layer):\r\n    \"\"\"Multi-headed attention layer.\"\"\"\r\n\r\n    def __init__(self, d_model, num_heads):\r\n        super(MultiHeadAttention, self).__init__()\r\n        self.num_heads = num_heads\r\n        self.d_model = d_model\r\n\r\n        assert d_model % self.num_heads == 0\r\n\r\n        self.depth = d_model // self.num_heads\r\n\r\n        self.wq = tf.keras.layers.Dense(d_model)\r\n        self.wk = tf.keras.layers.Dense(d_model)\r\n        self.wv = tf.keras.layers.Dense(d_model)\r\n\r\n        self.dense = tf.keras.layers.Dense(d_model)\r\n\r\n    def split_heads(self, x, batch_size):\r\n        \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\r\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, v, k, q, mask):\r\n        batch_size = tf.shape(q)[0]\r\n\r\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\r\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\r\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\r\n\r\n        q = self.split_heads(\r\n            q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\r\n        k = self.split_heads(\r\n            k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\r\n        v = self.split_heads(\r\n            v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\r\n\r\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\r\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\r\n        scaled_attention, attention_weights = scaled_dot_product_attention(\r\n            q, k, v, mask)\r\n\r\n        scaled_attention = tf.transpose(\r\n            scaled_attention,\r\n            perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\r\n\r\n        concat_attention = tf.reshape(\r\n            scaled_attention,\r\n            (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\r\n\r\n        outputs = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\r\n\r\n        return outputs, attention_weights\r\n\r\n\r\nclass TransformerLayer(tf.keras.layers.Layer):\r\n    \"\"\"Implements a single transformer layer (https://arxiv.org/abs/1706.03762).\r\n    \"\"\"\r\n\r\n    def __init__(self, d_model, num_heads, dff,\r\n                 dropout_rate=0.1,\r\n                 reorder_ln=False):\r\n        super(TransformerLayer, self).__init__()\r\n\r\n        self.mha = MultiHeadAttention(d_model, num_heads)\r\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\r\n\r\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\r\n\r\n        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\r\n        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\r\n\r\n        self.reorder_ln = reorder_ln\r\n\r\n    def call(self, x):\r\n        inp_x = x\r\n\r\n        if self.reorder_ln:\r\n            x = self.layernorm1(x)\r\n\r\n        # (batch_size, input_seq_len, d_model)\r\n        attn_output, _ = self.mha(x, x, x, mask=None)\r\n        attn_output = self.dropout1(attn_output)\r\n\r\n        if self.reorder_ln:\r\n            out1 = inp_x + attn_output\r\n            x = out1\r\n        else:\r\n            # (batch_size, input_seq_len, d_model)\r\n            out1 = self.layernorm1(x + attn_output)\r\n            x = out1\r\n\r\n        if self.reorder_ln:\r\n            x = self.layernorm2(x)\r\n\r\n        # (batch_size, input_seq_len, d_model)\r\n        ffn_output = self.ffn(x)\r\n        ffn_output = self.dropout2(ffn_output)\r\n\r\n        if self.reorder_ln:\r\n            out2 = out1 + ffn_output\r\n        else:\r\n            # (batch_size, input_seq_len, d_model)\r\n            out2 = self.layernorm2(out1 + ffn_output)\r\n\r\n        return out2\r\n\r\n\r\ndef pairwise_l2_distance(a, b):\r\n    \"\"\"Computes pairwise distances between all rows of a and all rows of b.\"\"\"\r\n    norm_a = tf.reduce_sum(tf.square(a), 1)\r\n    norm_a = tf.reshape(norm_a, [-1, 1])\r\n    norm_b = tf.reduce_sum(tf.square(b), 1)\r\n    norm_b = tf.reshape(norm_b, [1, -1])\r\n    dist = tf.maximum(norm_a - 2.0 * tf.matmul(a, b, False, True) + norm_b, 0.0)\r\n    return dist\r\n\r\n\r\ndef get_repnet_model(logdir):\r\n    \"\"\"Returns a trained RepNet model.\r\n\r\n    Args:\r\n      logdir (string): Path to directory where checkpoint will be downloaded.\r\n\r\n    Returns:\r\n      model (Keras model): Trained RepNet model.\r\n    \"\"\"\r\n    # Check if we are in eager mode.\r\n    assert tf.executing_eagerly()\r\n\r\n    # Models will be called in eval mode.\r\n    tf.keras.backend.set_learning_phase(0)\r\n\r\n    # Define RepNet model.\r\n    model = ResnetPeriodEstimator()\r\n    # tf.function for speed.\r\n    model.call = tf.function(model.call)\r\n\r\n    # Define checkpoint and checkpoint manager.\r\n    ckpt = tf.train.Checkpoint(model=model)\r\n    ckpt_manager = tf.train.CheckpointManager(\r\n        ckpt, directory=logdir, max_to_keep=10)\r\n    latest_ckpt = ckpt_manager.latest_checkpoint\r\n    print('Loading from: ', latest_ckpt)\r\n    if not latest_ckpt:\r\n        raise ValueError('Path does not have a checkpoint to load.')\r\n    # Restore weights.\r\n    ckpt.restore(latest_ckpt).expect_partial()\r\n\r\n    # Pass dummy frames to build graph.\r\n    model(tf.random.uniform((1, 64, 112, 112, 3)))\r\n    return model\r\n", "Is it possible to share the converted TFLite model file?", "@abattery yeah sorry, here you go: https://drive.google.com/file/d/1-LcGSsb4PbPDsGe1Cy1cwR-W_7Hu-kyP/view?usp=sharing\r\n\r\nedit: This is after removing tf.print ", "So I looked into it some more. It looks like `vectorized_map` also is not supported on GPU? Since it uses `gather` and that is [not supported on GPU](https://github.com/tensorflow/tensorflow/issues/42180)? Is that correct? ", "> So I looked into it some more. It looks like `vectorized_map` also is not supported on GPU? Since it uses `gather` and that is [not supported on GPU](https://github.com/tensorflow/tensorflow/issues/42180)? Is that correct?\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48553\">No</a>\n", "> So I looked into it some more. It looks like `vectorized_map` also is not supported on GPU? Since it uses `gather` and that is [not supported on GPU](https://github.com/tensorflow/tensorflow/issues/42180)? Is that correct?\r\n\r\nYou might use the tflite's benchmark model tools ([pre-built binaries](https://www.tensorflow.org/lite/performance/implementing_delegate#download_links_for_nightly_pre-built_tflite_tooling_binaries)) and use \"--use_gpu=true --dry_run=true --print_preinvoke_state=true --graph=TFLITE-MODEL-PATH\" to get a sense of what parts of the graph are delegated or not.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48553\">No</a>\n"]}, {"number": 48551, "title": "tf.strings.lower() does not lower-case non-ASCII characters", "body": "**System information**\r\nThis bug happens in Colab notebook. It happens on other platforms as well, but that's beside the point.\r\n\r\n**Describe the current behavior**\r\n\r\ntf.strings.lower() lower cases A-Z to a-z but it does not lower case other unicode characters, e.g. LATIN CAPITAL LETTER E WITH ACUTE, LATIN CAPITAL LETTER Z WITH CARON, and many others, etc. See the code below. \r\n\r\nIt seems that tf.strings.lower() keeps all upper-case non-ASCII characters as is. I was unable to find an example of any character outside ASCII that is lower cased correctly. But I did only spot checks.\r\n\r\n**Describe the expected behavior**\r\n\r\ntf.strings.lower() should lower case all accented characters. E.g. \u00c9 should become \u00e9, \u017d should become \u017e, same as in Python. My understanding of Unicode standard is that there is a language-independent mapping from upper case to lower case for every unicode character. This Unicode web page explains the details: https://unicode.org/faq/casemap_charprop.html \r\nTensorflow's tf.strings.lower() should implement this mapping.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is a link to a Colab notebook: https://colab.research.google.com/drive/1LyF0FZp9uMsqsyUGaWEdalhLql5B69k5\r\n\r\nHere is the copy of the code from the Colab botebook:\r\n\r\n```\r\n# This piece of code demonstrates a bug in tf.strings.lower().\r\n\r\nimport tensorflow as tf\r\n\r\n# Text in Slovak language. (It is first half of a panagram.)\r\n# The text is in 3 different casings: Upper case, lower case, and\r\n# mixed case. In the mixed case, all letters with diacritics \r\n# are upper case and standard English ASCII letters are lower case.\r\nUPPER_CASE = u\"STAR\u00dd K\u00d4\u0147 NA H\u0154BE KN\u00cdH \u017dUJE T\u00cd\u0160KO POV\u00c4DNUT\u00c9 RU\u017dE\"\r\nLOWER_CASE = u\"star\u00fd k\u00f4\u0148 na h\u0155be kn\u00edh \u017euje t\u00ed\u0161ko pov\u00e4dnut\u00e9 ru\u017ee\"\r\nMIXED_CASE = u\"star\u00dd k\u00d4\u0147 na h\u0154be kn\u00cdh \u017duje t\u00cd\u0160ko pov\u00c4dnut\u00c9 ru\u017de\"\r\n\r\n\r\nclass TestLowerCase(tf.test.TestCase):\r\n\r\n    # This test case passes.\r\n    # It verifies that, in Python, all three texts\r\n    # are the same after lower casing.\r\n    def test_python(self):\r\n        self.assertEqual(UPPER_CASE.lower(), LOWER_CASE)\r\n        self.assertEqual(MIXED_CASE.lower(), LOWER_CASE)\r\n        self.assertEqual(LOWER_CASE.lower(), LOWER_CASE)\r\n\r\n    # This test case passes.\r\n    # It demonstrates the current tensorflow behaviour,\r\n    # which is arguably incorrect.\r\n    def test_tensorflow_current(self):\r\n        tf_upper_case = tf.constant(UPPER_CASE, dtype=tf.string)\r\n        tf_mixed_case = tf.constant(MIXED_CASE, dtype=tf.string)\r\n        self.assertAllEqual(tf.strings.lower(tf_upper_case), tf_mixed_case)\r\n\r\n    # This test case fails!\r\n    # It demonstrates the desired/expected behavior.\r\n    def test_tensorflow_desired(self):\r\n        tf_upper_case = tf.constant(UPPER_CASE, dtype=tf.string)\r\n        tf_lower_case = tf.constant(LOWER_CASE, dtype=tf.string)\r\n        self.assertAllEqual(tf.strings.lower(tf_upper_case), tf_lower_case)\r\n\r\n\r\n# Run all three test cases. The first two pass. The third one fails.\r\nTestLowerCase().test_python()\r\nTestLowerCase().test_tensorflow_current()\r\nTestLowerCase().test_tensorflow_desired()\r\n```", "comments": ["@ymodak ,\r\n\r\nI was able to reproduce the issue in TF v2.4,v2.5.0rc0,nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/0b612d4464d60ee8713c30c134373054/48551.ipynb) here\r\n\r\n", "tf.strings.upper() has a similar bug. It does not correctly handle non-ASCII characters.\r\n", "Thanks for the minimal reproduction! If you pass in encoding='utf-8' as an argument to tf.strings.lower, the test seems to pass. It looks like by default if no encoding is provided then we default to ASCII. We'll look into maybe making that the default but there might be some backwards compatibility considerations we need to think through.", "Thanks a lot for the explanation. This makes sense. In retrospect, I feel stupid and I should have guessed it. However, it would nice to update both the online documentation on the web and the functions' docstrings, and list at all these places explicitly the set of allowed encodings, so that people don't trip on this like I did.\r\n\r\nAs for backward compatibility/default behavior, I don't have any strict opinion. Status quo is fine for now. Long term, defaulting to \"utf-8\" encoding would be probably more user-friendly.\r\n\r\nI am not sure what is the process for submitting a PR against TensorFlow. Once I figure it out, I will try to send to a PR with the documentation and/or docstring update.", "The reason why tf.strings.lower() and its default behavior matters (to me at least) is because of tf.keras.layers.experimental.preprocessing.TextVectorization layer, which is responsible for creating words from a piece of text. The layer, by default, lower cases the text. It calls tf.strings.lower() internally without any encoding parameter and does not handle non-ASCII characters as one would naively expect. Fortunately, the \"standardization\" logic in TextVectorization layer can be specified by the user by passing a custom standardize argument.", "Thanks David for confirming! I agree we should document this clearly so that users don't trip on it. Please send us a PR with the documentation updates - it'll be super useful! https://www.tensorflow.org/community/contribute/code#contribution_workflow has some instructions that might be helpful. It might be helpful to even add documentation to Keras Preprocessing Layers to indicate this as well.\r\n\r\nI'm assigning the issue back to you for documentation updates :) ", "Here is the PR with the documentation fix: https://github.com/tensorflow/tensorflow/pull/48911\r\n\r\nI am guessing that the online documentation is automatically generated from the files I modify in the PR.\r\n", "My PR https://github.com/tensorflow/tensorflow/pull/48911 was merged.\r\nWe can close this issue.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48551\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48551\">No</a>\n"]}, {"number": 48550, "title": "Missing libtensorflow builds for 2.5.0-rc0 and 2.5.0-rc1", "body": "There don't appear to be prebuilt binaries for libtensorflow (https://www.tensorflow.org/install/lang_c) for `2.5.0-rc0` and `2.5.0-rc1`.\r\n\r\nThese binaries exist for 2.3.0 and 2.4.0 (and the corresponding RCs) under https://storage.googleapis.com/tensorflow/libtensorflow/, but none exist for the 2.5.0 RCs.\r\n\r\nFor example, https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0-rc0.tar.gz is one of the missing files.\r\n\r\nWould it be possible to push those binaries/trigger CI builds to generate them?\r\n\r\nThanks!\r\n", "comments": ["We're trying to get them for final release but we had some CI troubles for RC0 and RC1.", "Can you check if they are in RC2 release?", "@mihaimaruseac Thanks! Looks like Linux CPU is available, but Linux GPU and Mac CPU are missing.\r\n\r\n* Linux CPU: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0rc2.tar.gz\r\n* Linux GPU (missing): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.5.0rc2.tar.gz\r\n* Mac CPU (missing): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-2.5.0rc2.tar.gz\r\n\r\n(I haven't done any validation yet apart from checking if the files exist)", "Also, not sure if it matters, but the previous releases used the format `-x.y.z-rc2` and it seems like 2.5.0 uses `-x.y.zrc2`", "@av8ramit can you take a look?", "I've fixed the issue that caused this and done a manual re-upload. I can now see the previously missing files. Can you please confirm this is resolved @VivekPanyam ?", "I've tested your uploads, and the following are now present:\r\n- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0rc3.tar.gz\r\n- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.5.0rc3.tar.gz\r\n\r\nThis is still missing:\r\n- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-2.5.0rc3.tar.gz\r\n\r\nSome of these are missing for earlier release candidates, but I assume people don't care about that.\r\n\r\nI'm having the same issue with 2.3.2, as I've reported at #48955, if you could take a look at that too I'd be very grateful.", "@av8ramit @mihaimaruseac I confirmed that the files exist (and they match the previous convention of `version-rcX` instead of just `versionrcX`)\r\n\r\nRC2:\r\n* Linux CPU: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0-rc2.tar.gz\r\n* Linux GPU: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.5.0-rc2.tar.gz\r\n* Mac CPU: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-2.5.0-rc2.tar.gz\r\n\r\n(Again, I haven't done any validation yet apart from checking if the files exist)\r\n\r\nThe corresponding files for RC3 are missing though:\r\n* Linux CPU (missing): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.5.0-rc3.tar.gz\r\n* Linux GPU (missing): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.5.0-rc3.tar.gz\r\n* Mac CPU (missing): https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-darwin-x86_64-2.5.0-rc3.tar.gz", "Okay so I did a manual upload again and all three files that @VivekPanyam lists with the proper convention of `version-rcX` are up. This may be a problem in the release automation upload process. Assigning to @angerson who may know more. ", "Hi @VivekPanyam and @JustinWick, thanks for your reports. There are a few oddities about the system that uploads our binaries, and I'd like to clear them up with your help. I found two causes for the inconsistencies you're seeing:\r\n\r\n1. Our auto-uploader uses a static list of renaming instructions and was misconfigured to upload `linux` as `darwin` (this caused https://github.com/tensorflow/tensorflow/issues/48955).\r\n1. Our auto-uploader uses `2.X.0rc0` instead of `2.X.0-rc0`, like our earlier uploads did. For example, in the links you pointed out as missing in https://github.com/tensorflow/tensorflow/issues/48550#issuecomment-834686425, the files are there if you change `2.5.0-rc3` to `2.5.0rc3`.\r\n\r\nFor (1), I fixed the misconfiguration and added a checker to make sure future uploads can't be misconfigured in the same way. Next we'll re-upload the botched binaries. Have you noticed any other TensorFlow versions with this problem? I'll repeat this in https://github.com/tensorflow/tensorflow/issues/48955.\r\n\r\nFor (2), this looks like an oversight in the uploader. For consistency with previous releases, I'll update the uploader to use a hyphen. For reference, is this expectation documented somewhere? How were you determining the download links for the release candidates?", "I've made the changes to the uploader and re-uploaded the -rc3 binaries to make sure they're all present. \r\n\r\nWe don't need to re-upload `rc0` and `rc1`, however, as they're soon to be deprecated by the packages for the full release (which were unaffected by this issue, anyway).", "I think this is resolved, right?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48550\">No</a>\n"]}, {"number": 48549, "title": "TF Squeezing dimension off of sequential model inputs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab notebook provided below\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v2.4.1-0-g85c8b2a817f\r\n\r\n**Describe the current behavior**\r\n\r\nCreating a simple model where the final axis dimension is 1 causes data called by the model to fail with the following error: `ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape (5, 160)`\r\nFor the output above, the input was defined as `(160, 1)`, but running an array with the shape `(5, 160, 1)` caused the error. Running in TF 2.2 everything works as expected.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should successfully be called on data where the final axis is 1.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://colab.research.google.com/drive/1owEhfGDRt3lrRfpu5PzRE9v1I6SWVzxc\r\n\r\n**Other info / logs**\r\n\r\nThe problem seems to be due to [this statement](https://github.com/tensorflow/tensorflow/blob/5f808811084219533fd58238b6e97577218808e6/tensorflow/python/keras/engine/functional.py#L615). There's a function, `_conform_to_reference_input` which will squeeze the final axis \"if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\"\r\n\r\nThis behaviour seems quite counter intuitive to me. The data I'm working with is a time series that can have variable numbers of input channels. Removing the axis with a single input channel breaks many of my models when I have a single input channel. Even a basic model, such as a 2D CNN on MNIST, this would seem to make the inputs incompatible unless that axis is added again in the model before the CNN layer.\r\n\r\nIs there a different way we're supposed to handle inputs with a single channel?\r\n", "comments": ["It was introduced by @omalleyt12 with https://github.com/tensorflow/tensorflow/commit/2b58bb4025df1afe47cd9b523a988d4b75f3f89f. Let see what was the origin of this change.", "This works either using the Functional API, or setting the input shape of the layer with Sequential API. It seems to only be a problem using `model.build(...)`", "@tetelestia \r\nPlease confirm if the issue still persist", "There is a simple workaround to use Functional API, but my example still fails in `tf-nightly-2.6.0.dev20210428`", "```\r\n        # Should squeeze last dimension.\r\n        # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\r\n```\r\nIt was introduce by @omalleyt12 I don't know if he could comment on this.\r\n", "I was able to reproduce the issue on colab using `TF2.6` and `TF-nightly(2.8.0-dev20211003)`.Please find the gist [here](https://colab.research.google.com/gist/chunduriv/ecb18bfa8cad4c955dd3107774674759/48549.ipynb) for reference.Thanks!", "@qlzh727 Can you help to route this? Why it was introduced?", "I think this is expected.\r\n\r\nThe core issue here is that the invocation of `model.build(input_length, input_channels)`.\r\n\r\nNote that the input_shape is expected to contain batch dimension. The passed param here doesn't contain the batch dim, which make the model record the input shape to be a 2D tensor (rather than 3D). When the model sees the real input, the will try to compare the expected shape against the real input. In the case that the last dim is 1 and the expected rank is 1 less, it will try to squeeze the last dim of the put, and pass it to the layer.\r\n\r\nTo fix this issue on client side, you need to update the param for the build method to include batch dim as well. If you don't know the batch dim at the moment, you can use None instead.", "Can we close this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48549\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48549\">No</a>\n"]}, {"number": 48548, "title": "TFlite INT8 conversion error- Quantization not supported for op: 'DIV'", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation (pip package or built from source): 2.4.1\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): pip\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n(just the part where I get the error, of the whole script)\r\n```python\r\n# tflite experimentation\r\noptimize_lite_model = True\r\nnum_calibration_examples = 20 \r\nrepresentative_dataset = None\r\nif optimize_lite_model and num_calibration_examples:\r\n    # Use a bounded number of training examples without labels for calibration.\r\n    # TFLiteConverter expects a list of input tensors, each with batch size 1.\r\n    representative_dataset = lambda: itertools.islice(\r\n      ([image[None, ...]] for batch, _ in train_dataset for image in batch),\r\n      num_calibration_examples)\r\n\r\nif optimize_lite_model:\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    if representative_dataset:  # This is optional, see above.\r\n        converter.representative_dataset = representative_dataset\r\n\r\ntflite_model = converter.convert()\r\ntflite_model_name = 'TFlites/m0/int8_optimize.tflite'\r\nopen(tflite_model_name, \"wb\").write(tflite_model)\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-34-5b618f18cb5b> in <module>\r\n     43         converter.representative_dataset = representative_dataset\r\n     44 \r\n---> 45 tflite_model = converter.convert()\r\n     46 tflite_model_name = 'TFlites/m0/int8_optimize.tflite'\r\n     47 open(tflite_model_name, \"wb\").write(tflite_model)\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    872 \r\n    873     return super(TFLiteKerasModelConverterV2,\r\n--> 874                  self).convert(graph_def, input_tensors, output_tensors)\r\n    875 \r\n    876 \r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n    631     if calibrate_and_quantize:\r\n--> 632       result = self._calibrate_quantize_model(result, **flags)\r\n    633 \r\n    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\r\n    459       return calibrate_quantize.calibrate_and_quantize(\r\n    460           self.representative_dataset.input_gen, inference_input_type,\r\n--> 461           inference_output_type, allow_float, activations_type)\r\n    462 \r\n    463   def _is_unknown_shapes_allowed(self):\r\n\r\n~/.local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input)\r\n    102         np.dtype(input_type.as_numpy_dtype()).num,\r\n    103         np.dtype(output_type.as_numpy_dtype()).num, allow_float,\r\n--> 104         np.dtype(activations_type.as_numpy_dtype()).num)\r\n    105 \r\n    106   def calibrate_and_quantize_single(self,\r\n\r\nRuntimeError: Quantization not yet supported for op: 'DIV'.\r\n```", "comments": ["@teijeong could you triage this?", "Can you try the tf-nightly and using the following flags:\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\nconverter.experimental_new_quantizer = True\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48548\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48548\">No</a>\n"]}, {"number": 48547, "title": "WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.2.3 and Ubuntu 18 (google colab)\r\n- TensorFlow installed from (source or binary): pip (binary)\r\n- TensorFlow versions (use command below):\r\n\r\n  - colab    --> v2.4.1-0-g85c8b2a817f 2.4.1\r\n  - macOS --> v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\n- Python version: 3.8.8 (macOS) and 3.7 (colab)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using the following actor critic model:\r\n\r\n    def create_a2c(input_shape, actor_units):\r\n        initializer = Orthogonal(tf.math.sqrt(2.0))\r\n        x0 = Input(input_shape)\r\n        x = Dense(64, activation='tanh', kernel_initializer=initializer)(x0)\r\n        x = Dense(64, activation='tanh', kernel_initializer=initializer)(x)\r\n        actor_out = Dense(actor_units, kernel_initializer=Orthogonal(0.01))(x)\r\n        critic_out = Dense(1, kernel_initializer=Orthogonal(1.0))(x)\r\n        return Model(x0, [actor_out, critic_out])\r\n\r\nI keep getting:\r\n\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['dense_2/kernel:0', 'dense_2/bias:0'] when minimizing the loss.\r\n\r\n**Describe the expected behavior**\r\nI expect the gradients to exist.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere's a colab [notebook](https://colab.research.google.com/drive/1gqkXUBiulwZ4XSiWg4eVmfcKFQ3Ce1G_?usp=sharing) \r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue in TF 2.4 and nightly versions. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/ec58aa8fffe616dc5546be7f20550b69/a2c-cont-issue.ipynb).Thanks!", "I fixed the problem by changing the implementation. This doesn't necessarily mean that the current implementation is wrong. It works only for discrete environments, otherwise I get this ghost gradient warning. Based on this, I'm closing this issue and if anyone encounters the similar issue, feel free to re-open it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48547\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48547\">No</a>\n"]}, {"number": 48546, "title": "Fix GPU delegate flatbuffer target when built from external projects.", "body": "### Problem\r\n\r\nBuilding the target `//tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs` currently fails if doing so from an external project that has TensorFlow as a dependency.\r\n\r\nWithin the TF repo `bazel build //tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs` works as expected. However, from an external project running `bazel build @org_tensorflow//tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs` causes a build error:\r\n\r\n```\r\nerror: unable to load include file: tensorflow/lite/delegates/gpu/common/task/serialization_base.fbs\r\n```\r\n\r\n<details>\r\n<summary>Full error logs (building with `--verbose_failures`)</summary>\r\n<br>\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_adamwork/c7d230be9f03353c15a455e1a87ecf41/external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/BUILD:488:22: Generating flatbuffer files for serialization_cc_fbs_srcs: @org_tensorflow//tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs failed (Exit 1): bash failed: error executing command \r\n  (cd /private/var/tmp/_bazel_adamwork/c7d230be9f03353c15a455e1a87ecf41/execroot/larq_compute_engine && \\\r\n  exec env - \\\r\n    <PATH ENV VAR REDACTED>\r\n    PYTHON_BIN_PATH=/usr/local/opt/python@3.8/bin/python3.8 \\\r\n    PYTHON_LIB_PATH=/usr/local/Cellar/python@3.8/3.8.9/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; for f in external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/serialization.fbs; do bazel-out/host/bin/external/flatbuffers/flatc --scoped-enums -I ./  -c -o bazel-out/darwin-opt/bin/external/org_tensorflow/tensorflow/lite/delegates/gpu/cl $f; done')\r\nExecution platform: @local_execution_config_platform//:platform\r\nerror: external/org_tensorflow/tensorflow/lite/delegates/gpu/cl/serialization.fbs:15: 75: error: unable to load include file: tensorflow/lite/delegates/gpu/common/task/serialization_base.fbs\r\nTarget @org_tensorflow//tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs failed to build\r\n```\r\n</details>\r\n\r\nThe issue comes from this line (recently introduced by cfd834264dbd82a2): https://github.com/tensorflow/tensorflow/blob/d98f22e0e769bfa6177f73799b19266bc7382398/tensorflow/lite/delegates/gpu/cl/BUILD#L521\r\n\r\nThis include argument passed to `flatc` hard-codes the (relative) workspace root as `./`, which is indeed true when building the target from within the TensorFlow repo. However, if building from an external project the TF workspace root is instead `external/org_tensorflow`; hence the `unable to load include file` build error.\r\n\r\n### Proposed solution\r\n\r\nThis PR fixes the issue by defining a `workspace_root` variable inside `tensorflow.bzl` that will always resolve to the correct root. I am not wedded to this solution and welcome any alternate suggestions for how to fix the problem, but it does work and is (relatively) clean.\r\n\r\n### Context\r\n\r\nI ran into this issue because I work on [a project](https://github.com/larq/compute-engine) that has TensorFlow as an external dependency. I am in the process of updating our TF dependency from 2.4 to the recent 2.5 release candidates, which are the first to have the problem. Inside our project we build a target that depends on `@org_tensorflow//tensorflow/lite/tools/benchmark:benchmark_tflite_model_lib`, which in turn depends on this flatbuffer target.", "comments": ["@impjdi could you review this PR?", "It currently fails at line\r\n\r\n```\r\n    workspace_root = Label(\"//:WORKSPACE\").workspace_root or \"./\"\r\n```\r\n\r\nwith the assertion:\r\n\r\n```\r\nAssertionError: Must use absolute label in constructor.\r\n```\r\n\r\nUnfortunately, I'm not familiar enough with Bazel to suggest a fix :( ", "Hi @impjdi, thanks for looking into this :)\r\n\r\nDo you know what command triggers this error (/and maybe what environment too)? For me, on this branch, running `bazel build //tensorflow/lite/delegates/gpu/cl:serialization_cc_fbs_srcs` works just fine. I'm on MacOS if that makes any difference.", "Yeah, your PR is fine in the open source world, but isn't compatible with Google's internal repository.  I'm checking with several people how we can integrate this fix of yours.  I'll keep you posted.", "Just letting you know that I bumped up the review internally.", "That assertion looks for a well-formed label, e.g. `\"@repo//foo/bar:baz\"`, so `./` is probably breaking it. I'm not certain of how it works myself. Will it work to put `workspace_root` in the `flatc_args` instead, like the way they were before? String interpolation should work there, right?", "> That assertion looks for a well-formed label, e.g. `\"@repo//foo/bar:baz\"`, so `./` is probably breaking it. I'm not certain of how it works myself. Will it work to put `workspace_root` in the `flatc_args` instead, like the way they were before? String interpolation should work there, right?\r\n\r\nI'm not super familiar with Bazel/Starlark so I'm not sure what you mean by string interpolation - could you elaborate with the precise change you think might work?", "`\"-I \" + workspace_root`, for instance. There are a few more examples of functions that build strings throughout the BUILD file.", "> `\"-I \" + workspace_root`, for instance. There are a few more examples of functions that build strings throughout the BUILD file.\r\n\r\nAh cool, I misunderstood. I had assumed that the internal issue was with the definition of `workspace_root` in `tensorflow.bzl`, so I thought you were suggesting some different way of defining `workspace_root` inside the `BUILD` file directly. Thanks, I've pushed that change now :)", "@angerson I guess `feedback/copybara - Google internal checks FAILED` means that solution didn't work?", "@angerson @impjdi sorry to ping you both, but this is blocking us updating [our project's](https://github.com/larq/compute-engine) TF dependency, so I'd really like to be able to get this in (and cherry-picked onto the `r2.5` branch) if at all possible before the 2.5 release.\r\n\r\nIs there anything else I can try from my end to help with this?", "One proposal I had (and asked @angerson but haven't heard back yet) was\r\nthat your PR just sets workspace_root = \".\", we wait for it to get to the\r\nGoogle repo, I follow up and swap it out with\r\nLabel('//:WORKSPACE').workspace_root or \".\" before it is sent back to TF\r\nGitHub repo.", "> One proposal I had (and asked @angerson but haven't heard back yet) was that your PR just sets workspace_root = \".\", we wait for it to get to the Google repo, I follow up and swap it out with Label('//:WORKSPACE').workspace_root or \".\" before it is sent back to TF GitHub repo.\r\n\r\nAh okay great, thanks :)\r\n\r\nI've pushed that change, but of course can revert it if that's not the approach you end up deciding to take.", "I don't think the Windows build failure is related?", "I think it's unrelated too.  I'm not too familiar with github PRs.  Is\nthere a way to trigger the tests again to see whether it went away?\n\nOn Wed, May 5, 2021 at 4:22 AM Adam Hillier ***@***.***>\nwrote:\n\n> I don't think the Windows build failure is related?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/48546#issuecomment-832610979>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACKKUTZ7YXJZRDIFU7LO5HTTMES7BANCNFSM4272ZYTQ>\n> .\n>\n", "For now I've just merged in the most recent `master` changes to re-trigger.", "@gbaned Adam has been trying to submit this PR for ages, but things are failing for totally unrelated reasons.  Is there a way to expedite this?", "Aite.  It's in; thank you for the patience!\r\n\r\nNow I'm following up and doing the change from the inside.  I'll update this thread when that is submitted and goes live.", "Fantastic, thanks very much for your help @impjdi", "Okay.  This is now submitted and exported at commit: 59fa037eec3bf526bf8399aae900453e151a05f3"]}, {"number": 48543, "title": "[r2.5] Update of oneDNN version for mkl_aarch64 build target", "body": "This PR noticeably improves the performance of CNNs on AArch64 (`--config=mkl_aarch64`). oneDNN version upgrade follows the PR #48035 ", "comments": ["@penpornk @agramesh1 please have a look.", "@alenik01 Oh, I thought you are going to upgrade this in master and not 2.5. \r\n\r\nFor 2.5, I'm not sure if we can cherry-pick this, since cherry-picks are for security / bug fixes. Upgrading from a v2.1 release to v2.2 release might count more towards adding features rather than fixes. (The upgrade we did [here](https://github.com/tensorflow/tensorflow/pull/48345) was from a random commit after `v2.2-rc` to `v2.2` for more stability and fixes.)\r\n\r\n@mihaimaruseac What do you think?", "@penpornk if it is possible to cherry-pick this for v2.5 release, then it would be nice (the problem was that oneDNN v2.2 had been released after TF feature freeze deadline). If not, then this PR can be merged in the master.", "@alenik01 Could you please share how much performance improvements you observe from this update? Is this update for performance improvements alone or is it also fixing some issues?", "@alenik01 Also, could you please create another PR for the `master` branch? We usually only merge a cherry-pick after seeing the same changes successfully merged (and passed nightly tests) in `master`. \r\n\r\nI will check with the team if this PR is eligible for cherry-picking when I hear back about the speedups and whether this fixes any issues as well. Thank you!", "@penpornk\r\n> Could you please share how much performance improvements you observe from this update?\r\n\r\nThe speedup is noticeable, x1.7 (was measure for ResNet50 inference).\r\n\r\n> Is this update for performance improvements alone or is it also fixing some issues?\r\n\r\nPerformance alone.\r\n\r\n> Also, could you please create another PR for the master branch?\r\n\r\nOK, I will create one.\r\n\r\n> since cherry-picks are for security / bug fixes\r\n\r\nMany thanks for clarifying, now I understand that the version update for `mkl_dnn_v1` was one-off case.", "@mihaimaruseac - this is the PR we briefly discussed at the SIG build meeting the other week.\r\n", "@penpornk\r\n>Also, could you please create another PR for the master branch? \r\n\r\nHere it is: https://github.com/tensorflow/tensorflow/pull/48574\r\n"]}, {"number": 48542, "title": "Fixed MobilenetV3 from keras/application", "body": "relates to #48504", "comments": ["@l-bat Nice to see you also here ", "We have a shape mismatch in tests now E.g. `AssertionError: Shapes differ: (None, 1, 1, 1024) vs (None, None, None, 1024)`.\r\nCheck Ubuntu CPU logs.", "Can you run the test locally with `bazel test //tensorflow/python/keras/applications:applications_test`? I think iterating over the CI is quite slow ", "@nikitamaia We need to connect this PR  to the related issues, eventually assign these to @l-bat,  and put them in progress: \r\nhttps://github.com/tensorflow/tensorflow/issues/48504\r\nhttps://github.com/tensorflow/tensorflow/issues/48226\r\nhttps://github.com/tensorflow/tensorflow/issues/48066\r\n", "Can I modify tests to fix them? Because after my changes for MobilenetV3 without top shapes are `(None, 1, 1, 1024)` instead of `(None, None, None, 1024)`. I checked the original TF model from slim. The first GlobalAveragePooling2D  is included in the base part of the mobilenet.\r\n", "I think you can change the test and we can run the CI but @fchollet  or @qlzh727 will need to evaluate the impact on the existing users code base.", "Thanks for sending the fix, let me take a closer look for the issue.", "Thanks for the fix. I think your fix is correct.\r\n\r\nTo provide more context here, I made the original implementation in https://github.com/tensorflow/tensorflow/commit/ef88a7aad44756a99aa57deddc59dc756ac2b469. One of the delta it stated is that:\r\n\"4. [Major] Changed the include_top implementation. The Conv2D layer with name \"Conv_2\" and its activation is moved to be base model structure, which means they are in the model even the include_top is False. This is based on comparing the implementation detail in original slim implementation in https://github.com/tensorflow/models/blob/a811a3b7e640722318ad868c99feddf3f3063e36/research/slim/nets/mobilenet/mobilenet_v3.py. If we can confirm this change is correct, then we should also fix it on the OSS keras_application as well.\"\r\n\r\nI think I was overlook the conv layer between conv_2 and last conv layer for logits, and the change above is probably NOT correct.\r\n\r\nTo this PR, the correctness is justified in following points:\r\n1. In the original slim implementation, the global pooling layer is between conv_1 and conv_2. https://github.com/tensorflow/models/blob/a811a3b7e640722318ad868c99feddf3f3063e36/research/slim/nets/mobilenet/mobilenet_v3.py#L132\r\n2. In original paper (page 5 table 1), there is a \"pool, 7x7 \" between two conv layers.\r\n", "@qlzh727 Thanks can you connect these tickets: https://github.com/tensorflow/tensorflow/pull/48542#issuecomment-821171546"]}, {"number": 48541, "title": "ValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Variable 'UnreadVariable' ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI am using Python 3.6 with Tensorflow 2.3.0 and Keras 2.3.0 on Windows 10.  NVIDIA-SMI 452.56       Driver Version: 452.56       CUDA Version: 11.0\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I run training of a Color-CNN model (https://github.com/beerboaa/Color-Classification-CNN/blob/master/color_net_training.py) I get \r\n\r\nFound 1260 images belonging to 3 classes.\r\nFound 420 images belonging to 3 classes.\r\nTraceback (most recent call last):\r\n  File \"C:/workspace/color_model/color_classification_cnn.py\", line 162, in <module>\r\n    model.fit_generator(\r\n  File \"C:\\Python3\\lib\\site-packages\\keras\\legacy\\interfaces.py\", line 91, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1718, in fit_generator\r\n    return training_generator.fit_generator(\r\n  File \"C:\\Python3\\lib\\site-packages\\keras\\engine\\training_generator.py\", line 42, in fit_generator\r\n    model._make_train_function()\r\n  File \"C:\\Python3\\lib\\site-packages\\keras\\engine\\training.py\", line 328, in _make_train_function\r\n    self.train_function = K.function(\r\n  File \"C:\\Python3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3007, in function\r\n    return tf_keras_backend.function(inputs, outputs,\r\n  File \"C:\\Python3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3932, in function\r\n    raise ValueError('`updates` argument is not supported during '\r\nValueError: `updates` argument is not supported during eager execution. You passed: [<tf.Variable 'UnreadVariable' shape=(48,) dtype=float32, numpy=\r\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n      dtype=float32)>, <tf.Variable 'UnreadVariable' shape=(48,) dtype=float32, numpy=\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThis script should train the color-CNN model\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://github.com/beerboaa/Color-Classification-CNN/blob/master/color_net_training.py\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["That code is 4 years old. I suggest you to open an issue there if the maintainer is still active. It will need for sure some changes to run in modern TF versions.\r\n@saikumarchalla I think that we can close this.", "@SixtyTrees   The code which you have provided using keras  instead of tf.keras and it is very old. Please check the suggestion mentioned above by @bhack .\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48541\">No</a>\n"]}, {"number": 48540, "title": "How can I change dtype of TFLITE model Weights ?", "body": "**System information**\r\n- OS Platform and Distribution (Windows 10):\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.4.1\r\n\r\n\r\nI have a TFLITE model which was made from KERAS Functional API. Unfortunately I have deleted the all files related to that model like weights.h5 and model structure file. Now All I have is my TFLITE model. Its weights are in float 32 but I have to run it on mobile. So I need to convert weights to int8 (Quantization mode) or float16. Is there anyway So I can cast my TFLITE model weights ?  \r\n[Model.zip](https://github.com/tensorflow/tensorflow/files/6318011/Model.zip)\r\n\r\nI have Attached the Model below\r\n", "comments": ["I am afraid there is no way to do this without saved model since all model optimization techniques ([integer-only quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant#convert_using_integer-only_quantization), [float16 quantization](https://www.tensorflow.org/lite/performance/post_training_float16_quant#convert_to_a_tensorflow_lite_model)) are declared before converting `SavedModel/Keras` model into `tflite` format.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48540\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48540\">No</a>\n"]}, {"number": 48539, "title": "Add Xtensa Vision platform support for person detection usecase. ", "body": "Vision P6 processor is supported for now.\r\n\r\nFor build information, please follow tensorflow/lite/micro/tools/make/target/xtensa_vision/README.md\r\n\r\n@advaitjain Please review.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@kpraving  Can you please resolve conflicts? Thanks!", "yes @advaitjain , I will close this and open another one as per your comments. Thanks."]}, {"number": 48538, "title": "build_all_linux.sh failed: threadpool.cc:94:29: error: no matching function for call to \u2018Eigen::NonBlockingThreadPoolTempl", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):  https://github.com/tensorflow/tensorflow.git\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6.13\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.19.2\r\n- GCC/Compiler version (if compiling from source): gcc version 7.5.0\r\n- CUDA/cuDNN version: No CUDA\r\n- GPU model and memory: No GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen I checkout tensorflow source code r1.13 and run the build_all_linux.sh, the downloading of eigen is failed as the eigen resource URL https://bitbucket.org/eigen/eigen/get/9f48e814419e.tar.gz writen in download_dependencies.sh did not exist. It seems that the version is eigen 3.3.7. Then I downloaded this eigen version from github.\r\nThen I run build_all_linux.sh again. There were some errors during compiling: Please find the compiling log below. \r\nIt seems that the eigen version 3.3.7 did not match the tensorflow r1.13 source code: the tensorflow/core/lib/core/threadpool.cc requires a different input parameter of NonBlockingThreadPoolTempl format that Eigen::NonBlockingThreadPoolTempl provided. However after I downloaded all the eigen 3.3.x and checked their NonBlockingThreadPoolTempl definition there were none of them match the tensorflow's requirements. What's the matter?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ntensorflow/tensorflow/contrib/makefile/build_all_linux.sh\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\ntensorflow/core/lib/core/threadpool.cc: In constructor \u2018tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int, bool, Eigen::Allocator*)\u2019:\r\ntensorflow/core/lib/core/threadpool.cc:94:29: error: no matching function for call to \u2018Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int&, bool&, tensorflow::thread::EigenEnvironment)\u2019\r\n         allocator_(allocator) {}\r\n                             ^\r\nIn file included from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:58:0,\r\n                 from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:74,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from tensorflow/core/lib/core/threadpool.cc:19:\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note: candidate: Eigen::NonBlockingThreadPoolTempl<Environment>::NonBlockingThreadPoolTempl(int, Environment) [with Environment = tensorflow::thread::EigenEnvironment]\r\n   NonBlockingThreadPoolTempl(int num_threads, Environment env = Environment())\r\n   ^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:22:3: note:   candidate expects 2 arguments, 3 provided\r\ntensorflow/core/lib/core/threadpool.cc: In member function \u2018void tensorflow::thread::ThreadPool::Impl::ParallelFor(tensorflow::int64, tensorflow::int64, std::function<void(long long int, long long int)>)\u2019:\r\ntensorflow/core/lib/core/threadpool.cc:100:72: error: no matching function for call to \u2018Eigen::ThreadPoolDevice::ThreadPoolDevice(tensorflow::thread::ThreadPool::Impl*, int, Eigen::Allocator*&)\u2019\r\n     Eigen::ThreadPoolDevice device(this, this->NumThreads(), allocator_);\r\n                                                                        ^\r\nIn file included from /home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/Tensor:92:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from tensorflow/core/lib/core/threadpool.cc:19:\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:109:3: note: candidate: Eigen::ThreadPoolDevice::ThreadPoolDevice(Eigen::ThreadPoolInterface*, int)\r\n   ThreadPoolDevice(ThreadPoolInterface* pool, int num_cores) : pool_(pool), num_threads_(num_cores) { }\r\n   ^~~~~~~~~~~~~~~~\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:109:3: note:   candidate expects 2 arguments, 3 provided\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note: candidate: constexpr Eigen::ThreadPoolDevice::ThreadPoolDevice(const Eigen::ThreadPoolDevice&)\r\n struct ThreadPoolDevice {\r\n        ^~~~~~~~~~~~~~~~\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note:   candidate expects 1 argument, 3 provided\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note: candidate: constexpr Eigen::ThreadPoolDevice::ThreadPoolDevice(Eigen::ThreadPoolDevice&&)\r\n/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceThreadPool.h:107:8: note:   candidate expects 1 argument, 3 provided\r\ntensorflow/core/lib/core/threadpool.cc: In member function \u2018void tensorflow::thread::ThreadPool::ScheduleWithHint(std::function<void()>, int, int)\u2019:\r\ntensorflow/core/lib/core/threadpool.cc:204:10: error: \u2018struct tensorflow::thread::ThreadPool::Impl\u2019 has no member named \u2018ScheduleWithHint\u2019; did you mean \u2018Schedule\u2019?\r\n   impl_->ScheduleWithHint(std::move(fn), start, limit);\r\n          ^~~~~~~~~~~~~~~~\r\n          Schedule\r\ntensorflow/core/lib/core/threadpool.cc: In member function \u2018void tensorflow::thread::ThreadPool::SetStealPartitions(const std::vector<std::pair<unsigned int, unsigned int> >&)\u2019:\r\ntensorflow/core/lib/core/threadpool.cc:209:10: error: \u2018struct tensorflow::thread::ThreadPool::Impl\u2019 has no member named \u2018SetStealPartitions\u2019\r\n   impl_->SetStealPartitions(partitions);\r\n          ^~~~~~~~~~~~~~~~~~\r\ngcc --std=c++11 -march=native -I. -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/../../../ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/eigen -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/fft2d -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/double_conversion -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/downloads/absl -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/ -I/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/protobuf-host/include -I/usr/local/include -c tensorflow/core/lib/io/inputstream_interface.cc -o /home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/io/inputstream_interface.o\r\ntensorflow/contrib/makefile/Makefile:885: recipe for target '/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/core/threadpool.o' failed\r\nmake: *** [/home/chendan/code/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/core/threadpool.o] Error 1\r\n", "comments": ["@atptour2017 ,\r\n\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!\r\n", "> \r\n> \r\n> @atptour2017 ,\r\n> \r\n> TensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!\r\n\r\nThanks!\r\nSo I checkout r2.4 but could not find build_all_linux.sh in this branch. The similar folder is tensorflow/tensorflow/lite/tools/make. This folder has download_dependencies.sh inside. But I do not know how to build the code. I need to build out protobuf header files and c++ source files and others as I want to use tensorflow in C++.\r\nWhen I checkout r2.0 the tensorflow\\tensorflow\\contrib\\makefile\\build_all_linux.sh exists. It's the same structure as version1.x.\r\nHow to build the v2.4 source code?", "@atptour2017 \r\nPlease share the error logs faced on  tf 2.4.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48538\">No</a>\n"]}, {"number": 48537, "title": "micro: Port RESIZE_BILINEAR from lite to micro", "body": "@tensorflow/micro\r\n\r\n@advaitjain This issue tracks my work on porting the RESIZE_BILINEAR kernel for int8 and float to TFL micro. It will be delivered in four PRs.\r\n* PR1 (merged): Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function. #43427\r\n* PR2 (merged): Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header. #48616\r\n* PR3 (merged): Port the RESIZE_BILINEAR kernel, port the tests and add it to the micro build as three separate commits. #48617\r\n\r\n\r\n\r\n\r\n", "comments": ["Thanks @patriklaurell!\r\n\r\nNote that we have realized that it's better to combine PR3 and 4 into a single pull request with separate commits for those steps.", "This issue was resolved by the merge of #48617"]}, {"number": 48536, "title": "Update api_def_TensorScatterAdd.pbtxt", "body": "Replacing \"tf.scatter_nd_add\" with \"tf.compat.v1.scatter_nd_add\" \r\n\r\nFixes issue #48487 ", "comments": []}, {"number": 48535, "title": "Wrong warning message about \"Variables were used a Lambda layer's call\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): TF 2.4.1 and TF nightly\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI want to create a trainable variable. I use a subclassed layer and make this variable a weight of the layer, instead of creating a variable directly and use it in a Lambda layer, as recommended by https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda.\r\n\r\nHowever, there is a warning message says\r\n```\r\nWARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (tf.linalg.matmul_7), but\r\nare not present in its tracked objects:\r\n  <tf.Variable '...'>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.\r\n```\r\n\r\nThe model builds, compiles, and trains successfully, and the variable is recognized as a trainable weight. \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nExpect no warning.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI have tested the code on Colab in both tensorflow 2.4.1 and tf-nightly, and received the same warning.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass VariableLayer(tf.keras.layers.Layer):\r\n    def __init__(self, shape, initializer, trainable, v_name):\r\n        super(VariableLayer, self).__init__()\r\n        self.shape = shape\r\n        self.initializer = initializer\r\n        self.trainable = trainable\r\n        self.v_name = v_name\r\n\r\n    def build(self, input_shape):\r\n        self.v = self.add_weight(shape=self.shape,\r\n                                 initializer=self.initializer,\r\n                                 trainable=self.trainable,\r\n                                 name=self.v_name)\r\n\r\n    def call(self, inputs):\r\n        return self.v\r\n\r\n\r\ndef build_subclass_functional():\r\n    x = tf.keras.Input(shape=(8,))\r\n    layer = VariableLayer(shape=(8, 3), initializer='glorot_normal', trainable=True, v_name='v')\r\n    z = layer(x)\r\n    o = tf.matmul(x, z)\r\n    model = tf.keras.Model(inputs=x, outputs=o)\r\n    return model\r\n\r\n\r\nif __name__=='__main__':\r\n    data = np.random.rand(2, 8)\r\n    model = build_subclass_functional()\r\n    print(model(data))\r\n    x = np.random.rand(5, 8)\r\n    y = np.random.rand(5, 3)\r\n    model.compile(optimizer='Adam', loss=\"mse\")\r\n    model.fit(x, y)\r\n    print(model.trainable_weights)\r\n```\r\n\r\n**Output**\r\n```\r\nWARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (tf.linalg.matmul_7), but\r\nare not present in its tracked objects:\r\n  <tf.Variable 'variable_layer/v:0' shape=(8, 3) dtype=float32, numpy=\r\narray([[...]], dtype=float32)>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.\r\n```\r\n\r\n", "comments": ["I figured out that the problem is that I didn't connect the inputs and output of the layer. Will close this issue. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48535\">No</a>\n"]}, {"number": 48534, "title": "Training a keras model on pretrained weights using load_weights()", "body": "\r\nI am using a custom keras model in Databricks environment. For a custom keras model, `model.save(model.h5)` does not work, because custom model is not serializable. Instead it is recommended to use` model.save_weights(path)` as an alternate.\r\n\r\nmodel.save_weights(pathDirectory) works. This yields 3 files c`heckpoint`,`.data-00000-of-00001,``.index` in the pathDirectory\r\n\r\nFor loading weights, Following mechanism is working fine.\r\n\r\n```\r\nmodel = Model()\r\n\r\nmodel.load_weights(path)\r\n```\r\n\r\nBut I want to train my model on pretrained weights I just saved. Like I saved model weights, and continue training on these saved weights afterwards. \r\n\r\nSo, when I load model weights and apply training loop, I get this error, TypeError: 'CheckpointLoadStatus' object is not callable\r\n\r\nCode is [here](https://gist.github.com/irfanumar1994/f38c3f80ac399717af5bce9904daa042)", "comments": ["@irfanumar1994 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nAlso please, share colab link or simple standalone code to reproduce the issue in our environment.\r\n\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48533, "title": "'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'FastGFile'", "body": "I'm using TensorRT in Docker 20.12 with Tensorflow-gpu 2.4 in Ubuntu 18\r\n", "comments": ["@za13  I tried to reproduce the issue but facing multiple errors and  you can see it in the colab  [here](https://colab.research.google.com/gist/saikumarchalla/12d0fc1a5be4381dfa953ab06ebe3f91/untitled60.ipynb#scrollTo=P5Se6akmSEAw).  If possible, Could you please provide the colab link  by adding all dependecies and necessary dataset that are required.\r\n\r\nAlso, check this SO [link](https://stackoverflow.com/questions/60730544/tensorboard-colab-tensorflow-api-v1-io-gfile-has-no-attribute-get-filesystem) with similar error. Hope it helps your query.Thanks!", "the code should work as long as you use any 1024x1024 image and rename it as owlResized.bmp. Was your image 1024x1024?\r\n\r\nI already saw that SO link and it doesn't address my error and I already said in my post that I added:\r\n\r\n    import tensorboard as tb\r\n    tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\r\n\r\nAnd got\r\n\r\n    with tf.io.gfile.FastGFile('tfmodel.pb', mode='wb') as f:\r\n    AttributeError: module 'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'FastGFile'", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "can anyone help?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "`FastGFile` is deprectaed in favor of just `GFile`.  It was a breaking change introduced in the move from TensorFlow 1.X to TF2.\r\n\r\nIt is possible to continue to use the API by replacing your line:\r\n`    with tf.io.gfile.FastGFile('tfmodel.pb', mode='wb') as f:`\r\nwith\r\n`    with tf.compat.v1.gfile.FastGFile('tfmodel.pb', mode='wb') as f:`\r\n(docs here https://www.tensorflow.org/api_docs/python/tf/compat/v1/gfile/FastGFile)\r\n\r\nhowever, you may be better off just using the TF2 GFile api for this:\r\n`    with tf.gfile.GFile('tfmodel.pb', mode='wb') as f:`\r\n(docs here https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile)\r\n\r\n@za13  Do either of these suggestions solve your issue?\r\n", "if I try \r\n\r\n    with tf.compat.v1.gfile.FastGFile('simpleTrain.pb', mode='wb') as f:`\r\n        f.write(constant_graph.SerializeToString())\r\n    os.system(\"python3 -m tf2onnx.convert --opset 11 --input simpleTrain.pb --inputs input:0 --outputs output:0 --output simpleTrain.onnx\")\r\n\r\nI then get\r\n\r\n    AssertionError: input is not a graph\r\n\r\nIf I try `with tf.io.gfile.GFile('simpleTrain.pb', mode='wb') as f:` instead, then I get the same error\r\n\r\n\r\nHowever, I do notice that the code outputs the `simpleTrain.pb` and `simpleTrain.onnx` when I use\r\n\r\n    --input tfmodel.pb\r\n\r\ninstead of \r\n\r\n    --input simpleTrain.pb\r\n\r\nin the `os.system(\"python3 -m tf2onnx...`\r\n\r\nif `tfmodel.pb` already pre-exists when previously generated from this code: https://github.com/ttyio/github-issues/blob/main/964/test.py", "can anyone help?", "it looks like  `tf.compat.v1.gfile.FastGFile('simpleTrain.pb', mode='wb')` has solved the issue as originally stated.  Is it possible your problem now is with tf2onnx.convert?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48533\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48533\">No</a>\n"]}, {"number": 48532, "title": "Select-tf-ops Is Not Working in Android Kitkat 4.4 (API 19)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Tab 3 (Android KitKat with API 19)\r\n- TensorFlow installed from (source or binary): Installed using pip\r\n- TensorFlow version (use command below): 2.4.3\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.4\r\n- GPU model and memory: Tesla T4 16GB\r\n\r\n**Describe the problem**\r\nI'm trying to implement YoloV3 on android application based on this [implementation](https://github.com/zzh8829/yolov3-tf2). To convert the model I have to enable the custom operation by adding this snippet code while converting the model\r\n```\r\nconverter.target_spec.supported_ops = [\r\n   tf.lite.OpsSet.TFLITE_BUILTINS,\r\n   tf.lite.OpsSet.SELECT_TF_OPS ]\r\n```\r\nthe model conversion was successful, and to implement the model on android application, I added the select-tf-ops dependency \r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n```\r\nThe application working correctly on my Android 5.0 (API 21) and Android 10 (API 29), but it gives an error when I run the application on Android 4.4.2 (API 19)\r\n```\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Node number 65 (FlexSize) failed to prepare.\r\n04-14 09:18:33.912 8903-8903/com.example.posm W/System.err: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:204)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:378)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.yolo.Yolo.predict(Yolo.java:71)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.POSM.predict(POSM.java:45)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.run_posm(MainActivity.java:110)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.MainActivity.lambda$onCreate$1$MainActivity(MainActivity.java:75)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.example.posm.-$$Lambda$MainActivity$w2a9VvlIKLLPHcyHZBUHuFlJhQA.onClick(lambda)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at android.view.View.performClick(View.java:4640)\r\n04-14 09:18:33.942 8903-8903/com.example.posm W/System.err:     at com.google.android.material.button.MaterialButton.performClick(MaterialButton.java:1119)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.view.View$PerformClick.run(View.java:19431)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.handleCallback(Handler.java:733)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Handler.dispatchMessage(Handler.java:95)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.os.Looper.loop(Looper.java:146)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at android.app.ActivityThread.main(ActivityThread.java:5598)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invokeNative(Native Method)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at java.lang.reflect.Method.invoke(Method.java:515)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)\r\n04-14 09:18:33.952 8903-8903/com.example.posm W/System.err:     at dalvik.system.NativeStart.main(Native Method)\r\n04-14 09:18:33.952 8903-8903/com.example.posm E/ERROR: Exception java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 65 (FlexSize) failed to prepare.\r\n    \r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 65 (FlexSize) failed to prepare.\r\n    \r\n    Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before infer\r\n```\r\nBased on my experience, this happened when I run the Android project without select-tf-ops dependency, then after adding the select-tf-ops dependency in my project, the application working correctly in Android 5.0 and Android 10. Is it a compatibility issue with older android APIs?\r\n\r\nI have tested by adding the ```FlexDelegate.initTensorFlowForTesting();``` before running the model, and it gives another error\r\n```\r\n04-15 10:53:58.399 358-358/com.example.posm E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.posm, PID: 358\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"wcsnrtombs\" referenced by \"libtensorflowlite_flex_jni.so\"...\r\n        at java.lang.Runtime.loadLibrary(Runtime.java:364)\r\n        at java.lang.System.loadLibrary(System.java:526)\r\n        at org.tensorflow.lite.flex.FlexDelegate.<clinit>(FlexDelegate.java:61)\r\n        at com.example.posm.MainActivity.onCreate(MainActivity.java:71)\r\n        at android.app.Activity.performCreate(Activity.java:5459)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1093)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2364)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2458)\r\n        at android.app.ActivityThread.access$900(ActivityThread.java:172)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1305)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:146)\r\n        at android.app.ActivityThread.main(ActivityThread.java:5598)\r\n        at java.lang.reflect.Method.invokeNative(Native Method)\r\n        at java.lang.reflect.Method.invoke(Method.java:515)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)\r\n        at dalvik.system.NativeStart.main(Native Method)\r\n04-15 10:54:01.952 358-358/com.example.posm I/Process: Sending signal. PID: 358 SIG: 9\r\n```\r\n\r\n**Describe the current behavior**\r\nThe select-tf-ops dependency is not working on Android 4.4.2 \r\n\r\n**Describe the expected behavior**\r\nThe select-tf-ops dependency working correctly on any devices and the model running well\r\n\r\n**Standalone code to reproduce the issue**\r\nHere's the link to the [model](https://drive.google.com/file/d/1yW1KxUZuNIbWXhOzEh3zFBAC5Cb7Ztpa/view?usp=sharing) that cannot be run in Android 4.4.2 even tough I've added the select-tf-ops dependency on it.", "comments": ["Hopefully I can get some advice from @thaink ", "@thaink could you take a look?", "In order to reproduce the issue on the Android, here is my build.gradle file\r\n```\r\nplugins {\r\n    id 'com.android.application'\r\n}\r\napply plugin: 'kotlin-android'\r\n\r\nandroid {\r\n    compileSdkVersion 29\r\n    buildToolsVersion \"29.0.3\"\r\n\r\n    defaultConfig {\r\n        applicationId \"com.example.posm\"\r\n        minSdkVersion 16\r\n        targetSdkVersion 29\r\n        versionCode 1\r\n        versionName \"1.0\"\r\n\r\n        testInstrumentationRunner \"androidx.test.runner.AndroidJUnitRunner\"\r\n        ndk {\r\n            abiFilters 'armeabi-v7a', 'arm64-v8a'\r\n        }\r\n    }\r\n\r\n    buildTypes {\r\n        release {\r\n            minifyEnabled false\r\n            proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'\r\n        }\r\n    }\r\n    compileOptions {\r\n        sourceCompatibility JavaVersion.VERSION_1_8\r\n        targetCompatibility JavaVersion.VERSION_1_8\r\n    }\r\n    aaptOptions {\r\n        noCompress \"tflite\"\r\n        noCompress \"lite\"\r\n    }\r\n}\r\n\r\ndependencies {\r\n\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:2.0.4'\r\n    testImplementation 'junit:junit:4.+'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.3.0'\r\n\r\n    // Tensorflow-lite\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n\r\n    implementation \"androidx.core:core-ktx:+\"\r\n    implementation \"org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version\"\r\n}\r\nrepositories {\r\n    maven { url 'https://dl.bintray.com/kotlin/kotlin-eap' }\r\n    mavenCentral()\r\n}\r\n```", " Did it happen in with the stable version?\r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.4.0'", "It gives another error\r\n```\r\n04-16 09:07:25.726 10132-10132/com.example.posm E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.posm, PID: 10132\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"fegetround\" referenced by \"libtensorflowlite_flex_jni.so\"...\r\n        at java.lang.Runtime.loadLibrary(Runtime.java:364)\r\n        at java.lang.System.loadLibrary(System.java:526)\r\n        at org.tensorflow.lite.flex.FlexDelegate.<clinit>(FlexDelegate.java:61)\r\n        at com.example.posm.MainActivity.onCreate(MainActivity.java:71)\r\n        at android.app.Activity.performCreate(Activity.java:5459)\r\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1093)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2364)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2458)\r\n        at android.app.ActivityThread.access$900(ActivityThread.java:172)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1305)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:146)\r\n        at android.app.ActivityThread.main(ActivityThread.java:5598)\r\n        at java.lang.reflect.Method.invokeNative(Native Method)\r\n        at java.lang.reflect.Method.invoke(Method.java:515)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1283)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1099)\r\n        at dalvik.system.NativeStart.main(Native Method)\r\n```\r\nseems like the select-tf-ops dependency incompatible with the Android 4.4.2", "I actually found fenv.h in Android 4.4.2\r\nhttps://android.googlesource.com/platform/prebuilts/gcc/linux-x86/host/i686-linux-glibc2.7-4.4.3/+/refs/tags/android-4.4.2_r1/sysroot/usr/include/fenv.h\r\nNot sure why this happens.", "@malik-anhar, Does [this](https://github.com/tensorflow/tensorflow/issues/48532#issuecomment-820896677) reference helps you? We see that you are using old version of tensorflow(2.4) , Could you please try to execute your code using latest stable version of TF 2.6.0 and let us know if the issue still persists in newer versions .Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48532\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48532\">No</a>\n"]}, {"number": 48531, "title": "Computing gradients with Tensorflow 2.4.1: 'KerasTensor' object has no attribute '_id'", "body": "I am trying to manipulate gradients within a layer in Tensorflow 2.4.1.  There seems to be a problem leading to the error 'KerasTensor' object has no attribute '_id'.  I am using a custom loss function.\r\n\r\nI have linked standalone code below that reproduces the issue.  This is the bare minimum necessary to generate the problem.  My actual working code is part of a large RL pipeline so I am looking for a solution without modifying the general structure of the linked example.  I suspect it is a bug.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\n\r\nAttributeError: 'KerasTensor' object has no attribute '_id' when computing gradients using custom loss function in a lambda layer.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model to compile without error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps://drive.google.com/file/d/11jAwZM2cq-4ng9Spo03dWxg_H5iaMbVN/view?usp=sharing\r\n\r\nThank you in advance for the assistance.", "comments": ["@jwallbridge \r\nThe correct usage for gradient tape is:\r\n```python\r\nwith tf.GradientTape() as tape:\r\n\t\t# make a prediction using the model and then calculate the\r\n\t\t# loss\r\n\t\tpred = model(X)\r\n\t\tloss = custom_loss_fn(y, pred)\r\n\t# calculate the gradients using our tape and then update the\r\n\t# model weights\r\n\tgrads = tape.gradient(loss, model.trainable_variables)\r\n\topt.apply_gradients(zip(grads, model.trainable_variables))\r\n```\r\n\r\nAlso, you need not make a Lambda layer for the loss function. The loss function `customLoss` is good enough. You can pass `loss = customLoss` in the model.", "@jwallbridge,\r\n\r\nPlease take a look at @AdityaKane2001's comment and let us know if you are still facing the same issue?  Thanks!", "Thank you @AdityaKane2001 and @tilakrayal.  I am still facing the same issue.  The problem with @AdityaKane2001's comment is the `X` in `model(X)`.  Let's say I want to rescale the gradients in a layer by 1/3.  In Tensorflow 1 you could simply do \r\n```python\r\noptimizer = Adam(learning_rate=0.01)\r\nvar_list = model.get_layer(name='mid_layer').trainable_variables\r\ngradients = optimizer.compute_gradients(objective, var_list=var_list)\r\nfor i, (grad, var) in enumerate(gradients):\r\n    if grad is not None:\r\n        scale_val = 1.0 / 3\r\n        grad *= scale_val\r\noptimizer.apply_gradients(gradients)\r\n```\r\nwithout an explicit input.  It seems `compute_gradients` is not available in Tensorflow 2.4.1.  When you replace `model(X)` by `model.output` you get \r\n```\r\nTypeError: Cannot convert function customLoss to a TensorFlow DType.\r\n```\r\nThis is due to `pred=model.output` in GradientTape.  \r\n\r\nThere must be a simple way to do this in Tensorflow 2.4.1?  \r\n\r\nFor more context, I would like to scale particular layer gradients within an RL pipeline consisting of a compile module (associated to an abstract base class) of the form\r\n```python\r\ndef compile(self, optimizer, metrics=[]):\r\n    ...\r\n    y_pred = self.model.output\r\n    y_true = Input(name='y_true', shape=(self.nb_actions,))\r\n    ....\r\n    SCALE GRADIENTS HERE\r\n    var_list = model.get_layer(name='mid_layer').trainable_variables\r\n    optimizer.apply_gradients(zip(grads, var_list))\r\n    ....\r\n    model.compile(optimizer=optimizer, loss=customLoss, metrics=combined_metrics)\r\n````\r\nAs you can see, there is no explicit `X`.", "@jwallbridge \r\nI am a bit confused what you are trying to do here. If I'm not mistaken, you want to write a custom training loop which will scale the gradients by some constant and then apply them, right? Or is it something else that you are trying to achieve?\r\n\r\nIf you intend to do the first thing, I would recommend you go through [this](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch). Hope this helps.", "@AdityaKane2001 \r\nThank you for your assistance.  Indeed, I would like to scale (and clip) gradients in a chosen layer of my network.  \r\n\r\nFor complete context, I am modifying the compile method of existing code from the well known Keras-RL:\r\nhttps://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py#L167\r\nThe training loop with a fit method already exists.  I just want to manipulate gradients within the compile method.  \r\n\r\nWith the old `compute_gradients` this is simple.  So what replaces `compute_gradients` in TF 2.4.1 or could this be a bug with GradientTape in this context?  My `y_pred` is not of the form `model(X)` but simply `model.output` in the compile method.", "@jvishnuvardhan ,\r\nI was able to reproduce the issue in TF v2.4,v2.5.0rc0,nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/7b6386e1bf710ea29e5e1f9e4657c0f3/48531.ipynb) here", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you @tilakrayal for reproducing the issue.\r\n@jvishnuvardhan ,\r\ndo you have an update on the issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jwallbridge Can you please check this [issue](https://github.com/tensorflow/tensorflow/issues/46194#issuecomment-827036146) which is similar to your issue. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48531\">No</a>\n"]}, {"number": 48530, "title": "[ROCm] Fixes for ROCm CSB breakage - 210414", "body": "copy-pasting individual commit messages here for description\r\n\r\n## 1 \r\nAdding \"no_rocm\" tag to the test `//tensorflow/compiler/xla/service/gpu:nvptx_compiler_test`\r\n\r\nThis is a CUDA specific test, and should not be enabled on the ROCm platform\r\n\r\n## 2\r\nROCM specific workaround for an MLIR unittest that results in a call to `ThenBlasGemmWithAlgorithm`\r\n\r\nROCm platform does not yet have autotuning support for rocBLAS GEMM API. The `GemmAlgorithmPicker` pass is not called on the ROCm platform ( `amdgpu_compiler.cc` ), so the algorithm field does not get populated, and hence the `ThenBlassGemmWithAlgorithm` routine does not get called.\r\n\r\nHowever the MLIR unit-tests introduced by the following commit ( 15e1036) has the algorithm field pre-populated leading to failure on the ROCm platform\r\n\r\n```\r\n...\r\n2021-04-14 23:22:14.718495: I tensorflow/compiler/xla/service/gpu/gemm_thunk.cc:63] Running GEMM thunk\r\n2021-04-14 23:22:14.718504: I tensorflow/compiler/xla/service/gpu/gemm_thunk.cc:179] Executing a GemmThunk\r\n2021-04-14 23:22:14.718541: I tensorflow/stream_executor/stream.cc:3487] [stream=0x5597e86929e0,impl=0x5597e868b170] Called Stream::ThenBlasGemmWithAlgorithm(transa=NoTranspose, transb=NoTranspose, m=2, n=2, k=2, alpha=1, a=0x7fe5d4201000, lda=2, b=0x7fe5d4200000, ldb=2, beta=0, c=0x7fe5d420d000, ldc=2, computation_type=f32, algorithm=7)\r\n2021-04-14 23:22:14.718555: I tensorflow/stream_executor/plugin_registry.cc:246] Selecting default BLAS plugin, rocBLAS\r\n2021-04-14 23:22:14.747078: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library librocblas.so\r\n2021-04-14 23:22:14.747200: E tensorflow/stream_executor/rocm/rocm_blas.cc:1883] rocBLAS does not currently support the GEMMwithAlgorithm operation for the \"float\" datatype\r\n...\r\n\r\n```\r\n\r\nThis commit updates the code in `gemm_thunk.cc` to always skip the path that calls `ThenBlasGemmWithAlgorithm` routine on the ROCm platform.\r\n\r\n\r\n## 3\r\n\r\nSkipping unit-tests (within `segment_reduction_ops_deterministic_test_gpu`) that test complex types.\r\n\r\nComplex type support has not yet been enabled for segment_reduction_ops on the ROCm platform.\r\n\r\n\r\n------------------------------------\r\n\r\n/cc @cheshire @chsigg @sanjoy \r\n\r\n\r\n", "comments": []}, {"number": 48529, "title": "Run bluepill tests with and without -Os", "body": "For the resize_bilinear port from https://github.com/tensorflow/tensorflow/pull/43426, all the CI checks passed.\r\n\r\nHowever, as noted in https://github.com/tensorflow/tensorflow/issues/48516 it broke the Xtensa build.\r\n\r\nAdditionally, I found out that #43426 broke the bluepill unit tests as well, but only if we build without -Os.\r\n\r\nThis change is running the bluepill unit tests with and without -Os to try and catch more errors prior to a PR being merged.\r\n\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48528, "title": "AttributeError: 'ConvolutionalBoxPredictor' object has no attribute 'inputs'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nColab\r\n- Python version:3.7\r\n_ TPU\r\n_ Tensorflow version 2.4.1\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ngetting error below during running model_tpu_main.py:\r\n`!python model_tpu_main.py --model_dir=model --pipeline_config_path=model/pipeline_tpu.config  --job-dir=model`\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nRefer to Colab [notebook](https://colab.research.google.com/drive/1XojAvhpsM4BaauXLKI1ObJw4-Sn8rfZ8?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n**error Msg:**\r\n _File \"/usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/faster_rcnn_meta_arch.py\", line 2947, in updates\r\n    self._first_stage_box_predictor.inputs))\r\nAttributeError: 'ConvolutionalBoxPredictor' object has no attribute 'inputs'_\r\n", "comments": ["@halhwadi The issue is related to models repo. Could you please submit a new issue [here](https://github.com/tensorflow/models/issues/new?assignees=&labels=type%3Abug%2Cmodels%3Aresearch&template=30-research-bug-report-issue.md).Thanks!", "@halhwadi Closing this issue for now as I can see you have created a new issue on models repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48528\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48528\">No</a>\n"]}, {"number": 48527, "title": "[XLA] Enable vectorization of row broadcasting", "body": "Reland https://github.com/tensorflow/tensorflow/pull/48381 with a fix\r\n\r\nMostly the refactoring done in the commit https://github.com/tensorflow/tensorflow/pull/48381/commits/247b1d3bc60d80662541a0aff7423025cbe8ba05\r\n\r\nintroduced the bug. Sometimes, the new simpler indexing was triggered even when we didn't try to vectorize the row broadcasting. The inputs of the fusion wasn't c contiguous in this case.\r\n\r\nNow, I pass directly if we enable the row vectorization or not. So the simpler indexing can only be triggered when we try to vectorize row broadcasting.\r\n\r\n@cheshire ", "comments": ["Do we have the regression test case?", "> Do we have the regression test case?\r\n\r\nI have it. But it is ~40MB due to the big contant. So I can't reuse it.\r\nI was able to make one test, but it wasn't trivial. I amended the last commit to include it.\r\n\r\nI had 2 main problems:\r\n- I changed the constant to be parameter inputs but this doesn't reproduce the bug.\r\n- I tried to use a FileCheck to check the indexing, but I have this error:\r\n```\r\n%tmp_12 = f32[32,7,7,352]{3,2,1,0} convolution(f32[32,7,7,1024]{3,2,1,0} %tmp_10, f32[1,1,1024,352]{3,2,1,0} %tmp_11), window={size=1x1}, dim_labels=b01f_01io->b01f2021-04-15 15:40:54.001613: F tensorflow/compiler/xla/service/gpu/tests/hlo_to_llvm_ir.cc:135] Non-OK-status: CompileAndPrintLlvmIrFromFile(argv[1], ptx, sm) status: Internal: LHLO opcode convolution is not supported.\r\n```\r\n\r\nSo finally, I added the optimized graph, but remove the convolution.", "> Internally there were a lot of failing tests with this PR.\r\n> Can you please check whether you can reproduce the failures for these tests:\r\n> tensorflow/compiler/xla/service/gpu/tests:gemm_rewrite_test\r\n> tensorflow/compiler/xla/tests:array_elementwise_ops_test_gpu\r\n> tensorflow/compiler/tests:fused_batchnorm_test_gpu\r\n> tensorflow/compiler/tests:binary_ops_test_gpu\r\n> \r\n> (there were more failures, but I hope at least one of those tests can be used to reproduce the problem).\r\n\r\nThe first 3 are failing here. So I can repro it and work on it. Thanks.\r\n\r\nI amended the last commit to remove the not used variable.", "I was able to repro tests failure. But I also had those same tests on the commits my PR was based on.\r\nSo I rebased and now only one tests fail. But it also fail on the commits my PR is based on.\r\nIt is a link failure. So this could be an environment issues.\r\n\r\nCan you run again the CI?", "> I was able to repro tests failure. But I also had those same tests on the commits my PR was based on.\r\n> So I rebased and now only one tests fail. But it also fail on the commits my PR is based on.\r\n> It is a link failure. So this could be an environment issues.\r\n> \r\n> Can you run again the CI?\r\n\r\nI approved the PR which made the tests run again. Unfortunately I still see a few failing tests, e.g. //tensorflow/compiler/tests:fused_batchnorm_test_gpu\r\nCan you also see the results in the Details section of \"Linux GPU\" ? If not, I can give you the full list of failing tests.", "I fixed the failures that I can repro. Can you review the new commit and approuve?\r\nI'm not able to run all tests locally. See bellow.\r\n\r\nI'm able to see the public CI failures.\r\n\r\nI ran on my PR this tests locally and it fails: //tensorflow/compiler/xla/tests:broadcast_test_gpu\r\nBut if I takes the base TF commit, it pass. So now I have a real failure from this PR.\r\n\r\nWith it, I was able to do one fix.\r\n\r\nI tried to run all the tests, but I have tons of failures similar to this one:\r\n\r\n```\r\nImportError: /root/.cache/bazel/_bazel_root/45a7ca8b5c99c684c2d5c22cdd8175f0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/jit_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor12UnaryFunctorIN5Eigen9GpuDeviceENS0_3negINS2_4halfEEEEclERKS3_NS2_9TensorMapINS2_6TensorIS5_Li1ELi1ElEELi16ENS2_11MakePointerEEENSA_INSB_IKS5_Li1ELi1ElEELi16ESD_EE\r\n```\r\n\r\nSo I can't test those and will need to rely on the CI...\r\n\r\nI tried to use the google tensorflow/tensorflow:devel-gpu container. But I'm not able to even run the tensorflow configure scripts. It fail with this error:\r\n\r\n```\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 1396, column 38, in _cuda_autoconf_impl\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 977, column 35, in _create_local_cuda_repository\r\n                cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 666, column 30, in _get_cuda_config\r\n                config = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 643, column 41, in find_cuda_config\r\n                exec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 637, column 19, in _exec_find_cuda_config\r\n                return execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n        File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n                fail(\r\nError in fail: Repository command failed\r\nCould not find any cublas_api.h matching version '' in any subdirectory:\r\n        ''\r\n        'include'\r\n        'include/cuda'\r\n        'include/*-linux-gnu'\r\n        'extras/CUPTI/include'\r\n        'include/cuda/CUPTI'\r\nof:\r\n        '/lib/x86_64-linux-gnu'\r\n        '/usr'\r\n        '/usr/lib/x86_64-linux-gnu'\r\n        '/usr/local/cuda'\r\n        '/usr/local/cuda-11.0/targets/x86_64-linux/lib'\r\n        '/usr/local/cuda/lib64/stubs'\r\n```\r\n\r\nI wasn't able to make manually a working .tf_configure.bazelrc. CUDA and CUBLAS detection doesn't work anymore in that container. So I can't even run what the CI is running.\r\n\r\nIf you have any idea how to fix one or both of those problem, it would be great.", "I rebased as there was a conflict.", "> I fixed the failures that I can repro. Can you review the new commit and approuve?\r\n> I'm not able to run all tests locally. See bellow.\r\n> \r\n> I'm able to see the public CI failures.\r\n> \r\n> I ran on my PR this tests locally and it fails: //tensorflow/compiler/xla/tests:broadcast_test_gpu\r\n> But if I takes the base TF commit, it pass. So now I have a real failure from this PR.\r\n> \r\n> With it, I was able to do one fix.\r\n> \r\n> I tried to run all the tests, but I have tons of failures similar to this one:\r\n> \r\n> ```\r\n> ImportError: /root/.cache/bazel/_bazel_root/45a7ca8b5c99c684c2d5c22cdd8175f0/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/compiler/tests/jit_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow7functor12UnaryFunctorIN5Eigen9GpuDeviceENS0_3negINS2_4halfEEEEclERKS3_NS2_9TensorMapINS2_6TensorIS5_Li1ELi1ElEELi16ENS2_11MakePointerEEENSA_INSB_IKS5_Li1ELi1ElEELi16ESD_EE\r\n> ```\r\n> \r\n> So I can't test those and will need to rely on the CI...\r\n> \r\n> I tried to use the google tensorflow/tensorflow:devel-gpu container. But I'm not able to even run the tensorflow configure scripts. It fail with this error:\r\n> \r\n> ```\r\n> ERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n>    Traceback (most recent call last):\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 1396, column 38, in _cuda_autoconf_impl\r\n>                 _create_local_cuda_repository(repository_ctx)\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 977, column 35, in _create_local_cuda_repository\r\n>                 cuda_config = _get_cuda_config(repository_ctx, find_cuda_config_script)\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 666, column 30, in _get_cuda_config\r\n>                 config = find_cuda_config(repository_ctx, find_cuda_config_script, [\"cuda\", \"cudnn\"])\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 643, column 41, in find_cuda_config\r\n>                 exec_result = _exec_find_cuda_config(repository_ctx, script_path, cuda_libraries)\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/gpus/cuda_configure.bzl\", line 637, column 19, in _exec_find_cuda_config\r\n>                 return execute(repository_ctx, [python_bin, \"-c\", decompress_and_execute_cmd])\r\n>         File \"/home/fbastien/github/tensorflow-tf2-upstream2.cp/third_party/remote_config/common.bzl\", line 230, column 13, in execute\r\n>                 fail(\r\n> Error in fail: Repository command failed\r\n> Could not find any cublas_api.h matching version '' in any subdirectory:\r\n>         ''\r\n>         'include'\r\n>         'include/cuda'\r\n>         'include/*-linux-gnu'\r\n>         'extras/CUPTI/include'\r\n>         'include/cuda/CUPTI'\r\n> of:\r\n>         '/lib/x86_64-linux-gnu'\r\n>         '/usr'\r\n>         '/usr/lib/x86_64-linux-gnu'\r\n>         '/usr/local/cuda'\r\n>         '/usr/local/cuda-11.0/targets/x86_64-linux/lib'\r\n>         '/usr/local/cuda/lib64/stubs'\r\n> ```\r\n> \r\n> I wasn't able to make manually a working .tf_configure.bazelrc. CUDA and CUBLAS detection doesn't work anymore in that container. So I can't even run what the CI is running.\r\n> \r\n> If you have any idea how to fix one or both of those problem, it would be great.\r\n\r\nSorry to hear about the trouble with the open source build. Unfortunately I am not so familiar with the infrastructure stuff. I think I have heard that find_cuda_config is deprecated, and possibly the tensorflow/tensorflow:devel-gpu docker image needs to be updated to reflect that.\r\n@chsigg Does that error look familiar?", "This has been merged now.", "Thanks.", "About the container problem. I used latest-gpu by mistake instead of devel-gpu. When I use devel-gpu, I do not have the error I reported. So this wasn't a real problem.\r\nI still have the undefined symbol in my own container. But this is something else.", "Unfortunately the PR was automatically rolled back in d73d688687e48a29f06c84404f9817876e4b6325 due to two test failures:\r\n//tensorflow/compiler/xla/tests:call_test_gpu\r\n//tensorflow/compiler/xla/service:dynamic_padder_test_gpu\r\nBut the fix seems simple, ThreadsPerBlockRowVectorized should return -1 if the shape is empty. I will try to roll forward with the fix now.", "Thanks for looking at it. Why those tests fail, but we do not see any failure in the \"Linux GPU\" CI?", "> Thanks for looking at it. Why those tests fail, but we do not see any failure in the \"Linux GPU\" CI?\r\n\r\nThey only fail with --copt=-UNDEBUG (essentially an assert was triggered, but asserts are disabled by default with -c opt). But I think technically it could result in a segfault if unlucky.", "Thanks"]}]