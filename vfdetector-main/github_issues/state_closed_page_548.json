[{"number": 37277, "title": "[BugFix] - Prefetching on GPU is actually executed on CPU", "body": "When applying the following:\r\n\r\n```python\r\ndataset = dataset.apply(\r\n    tf.data.experimental.prefetch_to_device(\r\n        '/gpu:0',\r\n        buffer_size=1,\r\n    )\r\n)\r\n```\r\n\r\nTensorflow behind the hood does two additional things:\r\n\r\n1. Create an _OptionsDataset (everything is fine since the underlying `_variant_tensor` is not touched)\r\n  - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/prefetching_ops.py#L76-L78\r\n  - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L4323\r\n\r\n2. Prefetch a number of batches on GPU depending on buffer_size => **Issue is Here**\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/prefetching_ops.py#L54\r\n\r\nThe problem is that `gen_dataset_ops.prefetch_dataset` defaults on CPU and basically re-copy the dataset back on the CPU. Effectively killing the purpose and adding some overhead.\r\n\r\nThe fix is simple, just adding `with ops.device(input_dataset._variant_tensor.device):` depending on where the `_variant_tensor` is already located\r\n\r\nCC: @sanjoy \r\n\r\n**To Be Noted:** This issue will re-appear if the user does not prefetch to device as the \"last step\" of the TF Data Pipeline. Maybe a warning should be executed in case the user does not respect this heuristic\r\n\r\n**Eventual Question:** Could someone explain the rationale behind these lines, why prefetching to device should have optimizations and autotune deactivated? \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/prefetching_ops.py#L73-L75\r\n\r\n```python\r\noptions = dataset_ops.Options()\r\noptions.experimental_optimization.apply_default_optimizations = False\r\noptions.experimental_optimization.autotune = False\r\nreturn _CopyToDeviceDataset(\r\n  dataset, target_device=target_device,\r\n  source_device=source_device).with_options(options)\r\n```", "comments": ["CC @jsimsa @mrry @rohan100jain ", "@DEKHTIARJonathan thank you for the detailed information. Would you be interested in submitting a fix for this issue? If so, please also include a unit test that fails prior to your bug fix and passes afterwards.\r\n\r\nAs for your eventual question, the reason for those settings is to avoid chaining the `ModelDataset` and `OptimizeDataset` transformations to the end of the input pipeline because the transformations only have CPU kernels.", "@jsimsa It's a PR so you just have to merge and we are good ;)\r\n\r\nI'm adding a unittest immediately ;)\r\n\r\n> As for your eventual question, the reason for those settings is to avoid chaining the ModelDataset and OptimizeDataset transformations to the end of the input pipeline because the transformations only have CPU kernels.\r\n\r\nPlease stop me if the following doesn't make any sense\r\n\r\nWhy instead of doing:\r\n1. CopyToDevice\r\n2. Deactivate Optimization & Autotune\r\n3. Prefetch\r\n\r\nWe don't do:\r\n1. Activate Default Opt & Autotune => Which will still run on CPU at this point\r\n2. CopyToDevice\r\n3. Prefetch\r\n\r\n@sanjoy As this PR is really tiny (1 line fix). And as the bug potentially leaves a fair amount of perf on the table, could we have this cherrypicked for 2.2 ?", "Sorry, I thought this is an issue (as opposed to a PR) :-).\r\n\r\nThe options need to be set because otherwise tf.data will chain the two datasets to the end of the input pipeline when creating an iterator for it. However, you have a good point in that the host dataset should have optimizations and autotuning applied to it. This can be done [as follows](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/kernel_tests/scan_test.py#L278-L279) and you are welcome to add this functionality to this PR. Thanks!\r\n\r\nEDIT: I can help with cherrypicking this into TF 2.2", "@jsimsa I have added two lines in the unittests, that should be fine and sufficient to show pass/fail.\r\n\r\nBtw. I just found that this code lead again to the same issue:\r\n```python\r\niterator = dataset_ops.make_initializable_iterator(device_dataset)\r\nnext_element = iterator.get_next()\r\nself.assertEqual(next_element.device, '/device:GPU:0')  # Fails\r\n```\r\n", "@jsimsa I pushed the unittest showing both issues and fixed the problem in the same way for prefetch and iterator.get_next()\r\n\r\nPlease let me know if I can do anything else", "@jsimsa Exploring a little I find a bunch of couple identical bugs. I have updated the PR, however numerous changes needs to be done before/after merging this PR:\r\n\r\n1. Relying on the TensorFlow placer implementation as you suggested instead of `ops.colocate_with()`.\r\n\r\n2. Most TF Data Options are in fact incompatible with GPU placements. I removed the `.with_options()` from the `prefetch_to_device` implementation and directly bypassed the options in case we are running on GPU. This has two advantages:\r\n    2.1 Users can still set the options ( even if they are not used ) and later benefits from any GPU implementation we provide with updating their code.\r\n    2.2 This is more generic as most (all ?) of the optimizations are in fact failing if the `dataset._variant_tensor` is on the GPU.\r\n\r\n@sanjoy Could we discuss a little the strategy to adress this issue next Wednesday meeting ? @jsimsa feel free to join ;)\r\n\r\nCC: @cliffwoolley @nluehr ", "tf.device is much better, thanks. I am still unsure what happens if the\nuser places the iterator creation in a device scope; now we'll ignore their\naction and always place it on the dataset device. I think the same number\nof copies happens in either case but if the iterator gets put on the user's\ndevice (as opposed to the device we choose) we might be able to a better\njob of prefetching the tensors there.\n\nI'm not sure; I need to think a little more about this.\n\nOn Tue, Apr 7, 2020 at 4:32 PM Jonathan DEKHTIAR <notifications@github.com>\nwrote:\n\n> *@DEKHTIARJonathan* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/data/ops/dataset_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/37277#discussion_r405173524>\n> :\n>\n> > @@ -2194,14 +2194,17 @@ def _make_initializable_iterator(self, shared_name=None):  # pylint: disable=mis\n>      dataset = self._apply_options()\n>      if shared_name is None:\n>        shared_name = \"\"\n> -    iterator_resource = gen_dataset_ops.iterator_v2(\n> +\n> +    with ops.colocate_with(self._variant_tensor):\n>\n> @alextp <https://github.com/alextp> thanks you so much for you answer :)\n> I wasn't aware of this issue with colocate_with.\n>\n> I did something really naive and replaced with ops.colocate_with() to with\n> ops.device().\n> That actually perfectly works and even pass my unittest easy.\n>\n> bazel test --verbose_failures //tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test\n>\n> Could this be acceptable ?\n>\n> it has the advantage to completely fix the colocation issue you raise and\n> fully address point 1) and 2) without any additional change. And most\n> likely will be good for Horovod (I gotta try).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37277#discussion_r405173524>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHROKTYE5Y32ZHOJXNA3RLOZ2TANCNFSM4LAYRPHA>\n> .\n>\n\n\n-- \n - Alex\n", "Let's discuss tomorrow then. I'm sure any concern can be addressed. We seem on good track", "I agree that `tf.device` is better.  I am still not convinced that doing this in Python is better than doing this in placer but assuming the PR addresses my comments below I support the change.\r\n\r\nThe current version of the PR only fixes the iterator placement issue for TF 1 graph-mode only. \r\n\r\nPlease extend this PR to also fix the iterator placement issue for TF 2 eager mode and tf.function graph-mode. This will require additional `tf.device` scopes and I will create a test that will help us validate that your change works in TF 2 eager mode and tf.function graph-mode. Furthermore, this PR should also use the device scope for `reduce`-based consumption of tf.data elements.\r\n\r\nEDIT: I realized that `reduce` op does not have GPU kernel so you can disregard that part.\r\n", "@jsimsa do you want write access to my fork ? That way you can directly push your unittest in the PR.\r\nWhen do you think we could have it ?\r\n\r\nGreat, sounds like a plan to me.", "I already created the test internally and will ping this PR when it is submitted.", "My change has been submitted and you will need to resolve merge conflicts.\r\n\r\nOn a second thought, I suggest that instead of updating existing unit tests, you write new unit tests that check that the `prefetch_with_device` and `iterator` op placement is fixed by your change. \r\n\r\nIn other words, please add the following tests:\r\n\r\n1) `prefetch_to_device_test.py`: eager mode and graph mode test that checks that the device of the prefetch dataset variant tensor matches the target device \r\n\r\n2) `iterator_test.py`: \r\n- eager mode test that checks that the device of the eagerly created iterator (i.e. `iter(dataset)` matches the device of the input dataset\r\n- graph mode test that checks that the device of iterator created by `make_one_shot_iterator` and `make_initializable_iterator` matches the device of the input dataset", "@jsimsa @alextp I applied the requested changes, let me know if that looks good with you.\r\n\r\n```bash\r\n$ bazel test //tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test\r\n\r\nTarget //tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test up-to-date:\r\n  bazel-bin/tensorflow/python/data/experimental/kernel_tests/prefetch_to_device_test\r\nINFO: Elapsed time: 20.261s, Critical Path: 9.85s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\n//tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test (cached) PASSED in 4.4s\r\n\r\nINFO: Build completed successfully, 1 total action\r\n```\r\n\r\n```bash\r\n$ bazel test //tensorflow/python/data/kernel_tests:iterator_test\r\n\r\nINFO: Analyzed target //tensorflow/python/data/kernel_tests:iterator_test (328 packages loaded, 24880 targets configured).\r\nINFO: Found 1 test target...\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/sandbox\r\nTarget //tensorflow/python/data/kernel_tests:iterator_test up-to-date:\r\n  bazel-bin/tensorflow/python/data/kernel_tests/iterator_test\r\nINFO: Elapsed time: 27.329s, Critical Path: 17.28s\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed successfully, 2 total actions\r\n//tensorflow/python/data/kernel_tests:iterator_test                      PASSED in 7.1s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\nINFO: Build completed successfully, 2 total actions\r\n```", "@jsimsa @alextp any update? ", "@DEKHTIARJonathan Alex left a comment for you on Thursday that (AFAICT) you have not addressed:\r\n\r\n> The Tensor.device attribute is present in tf2 graph mode. Or are we not getting the device of a tensor? Can you print the object which doesn't have the device attribute on a failure and paste the result here or in a gist?\r\n\r\n", "@jsimsa Hmm ... I did, anyway probably lost in the comment thread or dismissed by pushing an new commit.\r\nBy the way, here the issue is the iterator (not a tensor) missing `_device` attribute (not `device`):\r\n\r\n----------------\r\n\r\nSo by just commenting `if not tf2.enabled() or context.executing_eagerly():`\r\n\r\nI obtain the following error:\r\n\r\n```python\r\n======================================================================\r\nERROR: testIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\r\ntestIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\r\ntestIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/absl_py/absl/testing/parameterized.py\", line 263, in bound_param_test\r\n    test_method(self, **testcase_params)\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 314, in decorated\r\n    execute_test_method()\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 297, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1098, in testIteratorOnDeviceGraphModeInitializableIterator\r\n    device_dataset, device_iterator, device_tensor\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1037, in assert_dataset_placement\r\n    \"cpu:0\" in host_iterator._device.lower() or host_iterator._device == \"\"\r\nAttributeError: 'Iterator' object has no attribute '_device'\r\n\r\n======================================================================\r\nERROR: testIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\r\ntestIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\r\ntestIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/absl_py/absl/testing/parameterized.py\", line 263, in bound_param_test\r\n    test_method(self, **testcase_params)\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 314, in decorated\r\n    execute_test_method()\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 297, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1078, in testIteratorOnDeviceGraphModeOneShotIterator\r\n    device_dataset, device_iterator, device_tensor\r\n  File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1037, in assert_dataset_placement\r\n    \"cpu:0\" in host_iterator._device.lower() or host_iterator._device == \"\"\r\nAttributeError: 'Iterator' object has no attribute '_device'\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\n```python\r\n>>> dir(host_iterator)\r\n\r\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_variable_with_custom_getter', '_checkpoint_dependencies', '_deferred_dependencies', '_element_spec', '_flat_tensor_shapes', '_flat_tensor_types', '_gather_saveables_for_checkpoint', '_get_next_call_count', '_handle_deferred_dependencies', '_initializer', '_iterator_resource', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_preload_simple_restoration', '_restore_from_checkpoint_position', '_setattr_tracking', '_single_restoration_from_checkpoint_position', '_string_handle', '_tf_api_names', '_tf_api_names_v1', '_track_trackable', '_tracking_metadata', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', 'element_spec', 'from_string_handle', 'from_structure', 'get_next', 'initializer', 'make_initializer', 'output_classes', 'output_shapes', 'output_types', 'string_handle']\r\n```\r\n\r\nAs you can see `_device` does not exist\r\n\r\n```python\r\n>>> import pprint\r\n>>> pprint.pprint(host_iterator.__dict__, indent=4)\r\n\r\n{  \r\n    '_element_spec': TensorSpec(shape=(), dtype=tf.int64, name=None),\r\n    '_flat_tensor_shapes': [TensorShape([])],\r\n    '_flat_tensor_types': [tf.int64],\r\n    '_get_next_call_count': 1, \r\n    '_initializer': <tf.Operation 'MakeIterator' type=MakeIterator>,\r\n    '_iterator_resource': <tf.Tensor 'IteratorV2:0' shape=() dtype=resource>,\r\n    '_string_handle': <tf.Tensor 'IteratorToStringHandle:0' shape=() dtype=string>\r\n}\r\n```", "It needs to be iterator._iterator_resource.device, it seems\n\nOn Mon, Apr 13, 2020 at 9:45 AM Jonathan DEKHTIAR <notifications@github.com>\nwrote:\n\n> Hmm ... I did, anyway probably lost in the comment thread or dismissed by\n> pushing an new commit.\n> By the way, here the issue is the iterator (not a tensor) missing _device\n> attribute (not device):\n> ------------------------------\n>\n> So by just commenting if not tf2.enabled() or context.executing_eagerly():\n>\n> I obtain the following error:\n>\n> ======================================================================ERROR: testIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\n> testIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\n> testIteratorOnDeviceGraphModeInitializableIterator_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)----------------------------------------------------------------------\n> Traceback (most recent call last):\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/absl_py/absl/testing/parameterized.py\", line 263, in bound_param_test\n>     test_method(self, **testcase_params)\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 314, in decorated\n>     execute_test_method()\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 297, in execute_test_method\n>     test_method(**kwargs_to_pass)\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1098, in testIteratorOnDeviceGraphModeInitializableIterator\n>     device_dataset, device_iterator, device_tensor\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1037, in assert_dataset_placement\n>     \"cpu:0\" in host_iterator._device.lower() or host_iterator._device == \"\"AttributeError: 'Iterator' object has no attribute '_device'\n> ======================================================================ERROR: testIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\n> testIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2 (__main__.IteratorTest)\n> testIteratorOnDeviceGraphModeOneShotIterator_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)----------------------------------------------------------------------\n> Traceback (most recent call last):\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/absl_py/absl/testing/parameterized.py\", line 263, in bound_param_test\n>     test_method(self, **testcase_params)\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 314, in decorated\n>     execute_test_method()\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py\", line 297, in execute_test_method\n>     test_method(**kwargs_to_pass)\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1078, in testIteratorOnDeviceGraphModeOneShotIterator\n>     device_dataset, device_iterator, device_tensor\n>   File \"/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/kernel_tests/iterator_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/iterator_test.py\", line 1037, in assert_dataset_placement\n>     \"cpu:0\" in host_iterator._device.lower() or host_iterator._device == \"\"AttributeError: 'Iterator' object has no attribute '_device'\n> ----------------------------------------------------------------------\n>\n> >>> dir(host_iterator)\n>\n> ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_variable_with_custom_getter', '_checkpoint_dependencies', '_deferred_dependencies', '_element_spec', '_flat_tensor_shapes', '_flat_tensor_types', '_gather_saveables_for_checkpoint', '_get_next_call_count', '_handle_deferred_dependencies', '_initializer', '_iterator_resource', '_list_extra_dependencies_for_serialization', '_list_functions_for_serialization', '_lookup_dependency', '_maybe_initialize_trackable', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_preload_simple_restoration', '_restore_from_checkpoint_position', '_setattr_tracking', '_single_restoration_from_checkpoint_position', '_string_handle', '_tf_api_names', '_tf_api_names_v1', '_track_trackable', '_tracking_metadata', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', 'element_spec', 'from_string_handle', 'from_structure', 'get_next', 'initializer', 'make_initializer', 'output_classes', 'output_shapes', 'output_types', 'string_handle']\n>\n> As you can see _device does not exist\n>\n> >>> import pprint>>> pprint.pprint(host_iterator.__dict__, indent=4)\n>\n> {\n>     '_element_spec': TensorSpec(shape=(), dtype=tf.int64, name=None),\n>     '_flat_tensor_shapes': [TensorShape([])],\n>     '_flat_tensor_types': [tf.int64],\n>     '_get_next_call_count': 1,\n>     '_initializer': <tf.Operation 'MakeIterator' type=MakeIterator>,\n>     '_iterator_resource': <tf.Tensor 'IteratorV2:0' shape=() dtype=resource>,\n>     '_string_handle': <tf.Tensor 'IteratorToStringHandle:0' shape=() dtype=string>\n> }\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37277#issuecomment-612982080>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJSPIMZZDRWNQ5OPYTRMM6TLANCNFSM4LAYRPHA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp Funny enough, what I showed is working perfectly in TF1 or TF2 Eager.\r\nShall I just update with what you propose ?", "@alextp are you sure replacing with `iterator._iterator_resource.device` is the proper way ?\r\nAs stated above: `host_iterator._device` is working perfectly in TF1 or TF2 Eager.\r\n\r\nTo me it looks like `iterator` and `iterator._iterator_resource` are different objects and I should check the placement for both objects", "Oh if it works in tf1 and tf2 then it's fine.\n\nOn Wed, Apr 22, 2020 at 1:53 PM Jonathan DEKHTIAR <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> are you sure replacing with\n> iterator._iterator_resource.device is the proper way ?\n> As stated above: host_iterator._device is working perfectly in TF1 or TF2\n> Eager.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37277#issuecomment-618034617>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRI52P6LDXL7U3WWHIDRN5KNFANCNFSM4LAYRPHA>\n> .\n>\n\n\n-- \n - Alex\n", "It just doesn't work in TF2 graph mode. The attribute does not exist. Hence I only check this for TF1 and TF2 eager. If that's good with you then we can merge", "It also needs to work in tf2 graph mode.\n\nOn Wed, Apr 22, 2020 at 2:02 PM Jonathan DEKHTIAR <notifications@github.com>\nwrote:\n\n> It just doesn't work in TF2 graph mode. The attribute does not exist.\n> Hence I only check this for TF1 and TF2 eager. If that's good with you then\n> we can merge\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37277#issuecomment-618038688>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRKZEX733EEU4AMCD5DRN5LOFANCNFSM4LAYRPHA>\n> .\n>\n\n\n-- \n - Alex\n", "I can't fix it in this PR. It looks to be an issue in the Iterator in TF2 graph mode. A complete different issue than this PR tries to solve.", "Can't you do the fix I suggested if in tf2 graph mode? Something like if\nnot executing_eagerly() and executing_eagerly_outside_functions()?\n\nOn Wed, Apr 22, 2020 at 2:05 PM Jonathan DEKHTIAR <notifications@github.com>\nwrote:\n\n> I can't fix it in this PR. It looks to be an issue in the Iterator in TF2\n> graph mode. A complete different issue than this PR tries to solve.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/37277#issuecomment-618040158>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPWA4Y7XGHCHXGDOOTRN5L2JANCNFSM4LAYRPHA>\n> .\n>\n\n\n-- \n - Alex\n", "That's exactly what I did: https://github.com/tensorflow/tensorflow/blob/65b1a97ab98f09b197ee7b06a1ac6290349be816/tensorflow/python/data/kernel_tests/iterator_test.py#L1036-L1038\r\n\r\n@jsimsa was complaining about this:\r\n> Why are are we not checking the iterator device placement in TF2 graph mode?\r\n> This is somewhat worrisome because if the test code cannot access .device that neither will non-test code and since the code you have modified will run in TF2 graph mode, I suspect that this is an issue.\r\n> \r\n> @alextp does it make sense that the .device attribute is no present in TF2 graph mode?\r\n\r\nFixing the missing `iterator._device` in TF2 graph mode is outside the scope of this PR. I would prefer not mixing objectives", "The reason you should use `_iterator_resource` instead of `_device` is because there are two different implementations of tf.data.Iterator -- [one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/iterator_ops.py#L546) for eager mode + tf.function and [one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/iterator_ops.py#L77) for graph mode. Only the former has the `_device` property, while both of them have the `_iterator_resource` property.", "Perfect. Just wanted to make sure this was expected and we understood correctly the issue ;).\r\nJust pushed the fix. Let me know if there's any other blocking issue to merge this PR.\r\n\r\n```bash\r\nTarget //tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test up-to-date:\r\n  bazel-bin/tensorflow/python/data/experimental/kernel_tests/prefetch_to_device_test\r\nINFO: Elapsed time: 19.913s, Critical Path: 6.06s\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed successfully, 5 total actions\r\n//tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test PASSED in 3.8s\r\n```\r\n\r\n```bash\r\nTarget //tensorflow/python/data/kernel_tests:iterator_test up-to-date:\r\n  bazel-bin/tensorflow/python/data/kernel_tests/iterator_test\r\nINFO: Elapsed time: 19.905s, Critical Path: 6.81s\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed successfully, 2 total actions\r\n//tensorflow/python/data/kernel_tests:iterator_test                      PASSED in 5.0s\r\n```", "@jsimsa https://github.com/tensorflow/tensorflow/pull/37277#discussion_r413351911\r\n\r\nI can't see the issue with the indent here. Looks good to me ...", "@jsimsa linting issues fixed: 0f26cfa90c2477392da1b1409f690c70cdf050f7\r\nCan you re-approve the PR ? Thanks", "There is still a couple of lint issues:\r\n\r\n```\r\nFAIL: Found 3 non-whitelisted pylint errors:\r\ntensorflow/python/data/ops/dataset_ops.py:2277: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/data/ops/dataset_ops.py:2285: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n\r\ntensorflow/python/data/ops/dataset_ops.py:2286: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n```\r\n\r\nAFAICT, the windows failures are unrelated.", "@jsimsa I did not modify these lines\r\nhttps://github.com/tensorflow/tensorflow/blob/0f26cfa90c2477392da1b1409f690c70cdf050f7/tensorflow/python/data/ops/dataset_ops.py#L2275-L2287\r\n\r\nAnd honestly I'm not sure how to fix them, if I apply the suggested fix, i'll get indent errors.", "> @jsimsa I did not modify these lines\r\n> https://github.com/tensorflow/tensorflow/blob/0f26cfa90c2477392da1b1409f690c70cdf050f7/tensorflow/python/data/ops/dataset_ops.py#L2275-L2287\r\n> \r\n> And honestly I'm not sure how to fix them, if I apply the suggested fix, i'll get indent errors.\r\n\r\nThe line numbers are wrong (not sure why) but the indent issues exists. You could have just carefully review the PR diff. To make it easier for you, I commented on the lines that have the wrong indent in your PR.", "@jsimsa linting issues fixed in 356a3dd \r\nPlease approve again ;)\r\n\r\nAny way I can run the linter by myself ?\r\nI tried running: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh without much luck ...", "FYI, this got rolled back automatically because of internal test failures. I will take a look, time permitting. Please ping me if you do not see a response in a week.", "@jsimsa *ping*", "@jsimsa any chance we could get an update on when do you plan to re-merge this PR ?  It's a quite important PR that impact performance. Let me know if I can help.", "This should be fixed now by https://github.com/tensorflow/tensorflow/commit/8be4d61574f29568c8699708d88945b441bfd317", "@aaudiber : the commit you linked is far from addressing all concerns. If you look at this PR diff, you will see that it modifies many more things, namely placing the Iterator on the correct device.\r\n\r\nThe commit you linked is not sufficient (and I checked master, it's not here too). Consequently, doing `x = iterator.get_next()` triggers a copy back on CPU. Unfortunately, this makes 8be4d61 totally useless since we bump in the same issue a bit further down the input pipeline\r\n\r\nOne example (out of many):\r\n- should be: https://github.com/tensorflow/tensorflow/pull/37277/files#diff-90fceb4429d0c75477037971f7191558L2255-R2273\r\n- current state in master: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L2353-L2362\r\n\r\nCC: @nluehr @jsimsa ", "@DEKHTIARJonathan The linked commit fixes a bug where the prefetch dataset would be placed on CPU. As demonstrated by the included unit test, the prefetch dataset is now placed on GPU.\r\n\r\nThere is a separate issue that iterators are not automatically colocated with their inputs. We originally tried to address both issues with this PR, but automatic colocation broke internal tests, so we had to roll this PR back. It will take some time to figure out the root issue. It should be possible to work around the issue by explicitly putting the `get_next` inside a device scope. Could you open a separate issue for iterator colocation? Please include details about why it is important, to help us prioritize the issue."]}, {"number": 37276, "title": "Allow missing labels when there is no loss / metric associated with t\u2026", "body": "\u2026hem.\r\n\r\nAlso cleans up the LossesContainer and MetricsContainer class to use shared\r\nlogic.\r\n\r\nPiperOrigin-RevId: 298674749\r\nChange-Id: Iaac04d1038c41b1098c0af711c696350a8d7906f", "comments": []}, {"number": 37275, "title": "Undo changes to the input spec when RNN.unroll is True.", "body": "\r\nThe changes from fc7116dd0d087e707adacf6d1f4c6f3e76d83b8c that made the input spec more stringent do not apply to all unrolled recurrent layers. I'll have to revisit b/148491963 later.\r\n\r\n\r\nPiperOrigin-RevId: 298492355\r\nChange-Id: I442ad2a23576cae8fa2fad02a27a199f13b159c1", "comments": []}, {"number": 37274, "title": "General discussion of tf.image", "body": "Hi,\r\n\r\nI was thinking about applying for the GSoC for the topic of expandin tf.image (Adding and standardizing image processing operations in tf.image). It is said that it involves adding operations present in other image processing libraries. I am thinking operations that for example OpenCV has. \r\n\r\nI was looking for example at rotating an image, which is right now just limited to rotate by multiples of 90 degrees (tf.image.rot90). More generally one would like to for example get a rotation matrix and do an affine transform there to get any rotation angle. It seems though that tf.keras.precprocessing.image also has some image processing operations like tf.keras.precprocessing.image.apply_affine_transform that would do that. It seems to me redundant though for having tf.keras.preprocessing.image and tf.image in the end.\r\n\r\nAny thoughts on that? \r\n\r\nAnyway if I need a mentor for that from the TF community for the GSoC project, who should I contact you think?", "comments": ["Hello @czabo!\r\n\r\nI am also looking forward to applying at GSoC 2020. Replacing tf.keras.preprocessing.image.ImageDataGenerator for data augmentation op(s) using vanilla tf.image instead of SciPy based affine transformations have been extensively discussed. (https://docs.google.com/document/d/1yz2kwWsZ3wx-cObodb84a-LgHuxbTbxo53VI5yFqms8/edit) Also, there were discussions regarding complete rewrite of tf.keras.preprocessing submodule using tf.data.Dataset pipelines exclusively. The meeting notes indeed mention that some of these changes are to brought up in TF2.2 (this March). Any leads?\r\n\r\nAny information from the community in this regard would be greatly appreciated as that'd help in supporting information for GSoC proposals.\r\n\r\nThank you.", "Thank you for your update @swghosh :) Yes it would be indeed helpful if there would be any further information regarding all this.", "Closing this issue since we have an active thread tagged above covering this topic. \r\nFeel free to reopen if necessary. Thanks!", "@swghosh I applied for the same problem statement but unfortunately could not get through it. Can you please tell me from where you got the above link(is there any site or place where the outcomes of such meetings are published). I am asking it just for information so that it can be helpful for me if I apply to tensorflow in future.", "Hi @ghosalsattam, probably you can go through the following to get information about community meetings and discussion about API changes.\r\n\r\nA good point to start would be to refer:\r\n- https://www.tensorflow.org/community/contribute\r\n- https://github.com/tensorflow/community/ (new RFCs for TensorFlow ecosystem goes here)\r\n- https://github.com/keras-team/governance (new RFCs that subclass Keras APIs or changes to actual keras itself)"]}, {"number": 37273, "title": "Can't restore checkpoint! - ValueError: Tensor's shape is not compatible with supplied shape", "body": "**System information**\r\n- OS Platform and Distribution: Google AI Platform\r\n- TensorFlow version: v2.0\r\n- Python version: v3.*\r\n- Machine type: 16 vCPUs, 30 GB RAM\r\n- GPU: 1x NVIDIA Tesla K80\r\n\r\nI am training a model to caption images and have mostly used this tutorial: https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\r\n\r\nThe only thing that is different from this notebook is that I am using a different dataset.\r\n\r\nI set up the checkpoints\r\n```\r\ncheckpoint_path = \"./checkpoints\"\r\nckpt = tf.train.Checkpoint(encoder=encoder,\r\n                           decoder=decoder,\r\n                           optimizer = optimizer)\r\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n```\r\n\r\nAnd then I use ```ckpt_manager.save()``` every 5 epochs.\r\n\r\nI restart my kernel and try to restore using:\r\n```\r\nckpt.restore(ckpt_manager.latest_checkpoint)\r\n\r\n>> <tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1e76d12ba8>\r\n```\r\n\r\nBut when I try and make a prediction I get this error:\r\n```\r\nValueError: Tensor's shape (18859, 256) is not compatible with supplied shape (17128, 256)\r\n```\r\n\r\nCan anyone help out?\r\n\r\nThanks", "comments": ["@anthonyatp \r\n\r\nCould you please share a simple standalone indented code[with all dependencies] for me to replicate the issue faced by you in my local.", "Hmm, it's difficult to reproduce simply without having trained the model with the checkpoints.\r\n\r\nI'd suggest running the notebook from the tutorial: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\r\n\r\nAnd once the model has been trained and checkpoints saved, restart the kernel to reset the `encoder` and `decoder` variables and try to run:\r\n\r\n``` \r\nckpt.restore(ckpt_manager.latest_checkpoint)\r\n```\r\n\r\nThen try to make a prediction, which is where I get the error.\r\n\r\nLet me know if this is not sufficient.\r\n\r\nThanks!", "@anthonyatp \r\nAs per steps shared by you,  \"And once the model has been trained and checkpoints saved, **restart the kernel** to reset the encoder and decoder variables and try to run:\" restarting kernel would lose all the stored information.\r\nCould you please share a colab link with the same if possible.", "Apologies, I forgot colab storage gets deleted when runtime refreshes - I am using a notebook on google ai platform.\r\n\r\nOk, so I have put together a colab notebook with instructions to reproduce the problem. I have added a link to download my checkpoints so you don't have to train the model again:\r\n\r\nhttps://colab.research.google.com/drive/1P322lnNI_S9L2dP1ZcsJhU_izemgNcMg\r\n\r\nYou should get the error when you try and run the last cell", "What I'm really trying to solve is how do you save this type of model architecture (encoder, decoder, attention) and deploy it for serving - checkpoints may be the wrong method entirely.", "@anthonyatp \r\nCould you please let us know if your goal is to deploy model using serving, in that case you will have to save the model using tf.saved_model.save", "I would like to understand why I can't restore a checkpoint and make a prediction but yes, the main goal is to deploy model using serving.\r\n\r\nAlthough, I have tried this before. I've used `model.save()` and` tf.saved_model.save` both give this error when trying to save the decoder:\r\n\r\n```\r\nTypeError: call() missing 2 required positional arguments: 'features' and 'hidden'\r\n```\r\n\r\nI am using `decoder.save('decoder_model')` and  `tf.saved_model.save(decoder, 'decoder_model')`\r\n\r\nAm I doing something wrong? What is the correct way to save this type of model?", "@anthonyatp I ran the tutorial n colab successfully and was able to save and restore checkpoints successfully. As your issue is not a bug, Please post your issue on stack overflow as there is a wider community to respond. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> **System information**\r\n> \r\n> * OS Platform and Distribution: Google AI Platform\r\n> * TensorFlow version: v2.0\r\n> * Python version: v3.*\r\n> * Machine type: 16 vCPUs, 30 GB RAM\r\n> * GPU: 1x NVIDIA Tesla K80\r\n> \r\n> I am training a model to caption images and have mostly used this tutorial: https://www.tensorflow.org/tutorials/text/image_captioning#download_and_prepare_the_ms-coco_dataset\r\n> \r\n> The only thing that is different from this notebook is that I am using a different dataset.\r\n> \r\n> I set up the checkpoints\r\n> \r\n> ```\r\n> checkpoint_path = \"./checkpoints\"\r\n> ckpt = tf.train.Checkpoint(encoder=encoder,\r\n>                            decoder=decoder,\r\n>                            optimizer = optimizer)\r\n> ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n> ```\r\n> \r\n> And then I use `ckpt_manager.save()` every 5 epochs.\r\n> \r\n> I restart my kernel and try to restore using:\r\n> \r\n> ```\r\n> ckpt.restore(ckpt_manager.latest_checkpoint)\r\n> \r\n> >> <tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1e76d12ba8>\r\n> ```\r\n> \r\n> But when I try and make a prediction I get this error:\r\n> \r\n> ```\r\n> ValueError: Tensor's shape (18859, 256) is not compatible with supplied shape (17128, 256)\r\n> ```\r\n> \r\n> Can anyone help out?\r\n> \r\n> Thanks\r\n\r\n@anthonyatp Hi, were you able to find a solution for this? I'm currently facing this exact problem and I don't know how to fix it\ud83d\ude2d"]}, {"number": 37272, "title": "tf.estimator.SummarySaverHook should support native V2 symbols.", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAt present when specifying `tf.estimator.SummarySaverHook` in `tf.estimator.EstimatorSpec` when writing a custom `model_fn`, one is bound to use symbols from `tf.compat.v1`. \r\n\r\nThis is because, `tf.estimator.SummarySaverHook` needs  `summary_op` as an argument which should be a  tensor or list of tensors.\r\n\r\nOn the other hand the native V2 api in the form of `tf.summary...` symbols  return a boolean value. \r\n\r\nTherefore it is not possible to use `tf.summary` functions when using `tf.estimator.SummarySaverHook`.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will change the API as it will require either a basic change in `tf.summary` or `tf.estimator`\r\n\r\n**Who will benefit with this feature?**\r\nIt will help people in migrating their codes to native V2 symbols.\r\n", "comments": ["#36516 is also a related issue I think.", "I have also stumbled upon this issue. Is somebody working on a resolution?", "@ujjwal-ai ,\r\nSorry for the delayed response. In the **`Tensorflow Version 2.x`**, since we use [TF Keras](https://www.tensorflow.org/api_docs/python/tf/keras/) and [tf.data](https://www.tensorflow.org/guide/data) predominantly and don't use [Estimators](https://www.tensorflow.org/guide/estimator) much, can you please let us know if this Feature is still relevant? \r\n\r\nThanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37271, "title": "[r2.2:Cherrypick] Reject code containing the for/else pattern, which is not being faithfully converted at the moment. This replaces silently incorrect behavior with a warning. Also whitelist known urllib module.", "body": "PiperOrigin-RevId: 298586574\nChange-Id: Idd9113ccc699eb6b13cd5ec76c563a79b20b7aef", "comments": []}, {"number": 37270, "title": "[r2.2:Cherrypick] Change Keras batch normalization layer to use the running mean and average computation in fused_batch_norm.", "body": "PiperOrigin-RevId: 298446605\nChange-Id: I87466c847e6c3c54a5f16d9588c19a72e24b15eb", "comments": []}, {"number": 37269, "title": "tf.keras.utils.get_file extraction of zip not working when fname to a full path is used on Windows", "body": "I am following the tensorflow image classification tutorial: https://www.tensorflow.org/tutorials/images/classification\r\n\r\nI am unable to unzip the downloaded folder when I use a full path to a location as opposed to using just a fname as shown in the example on Windows. Have not tried on Linux, so don't know if it exists on that platform.\r\n\r\nThis is my code:\r\n``` \r\ndownload_data_loc = os.path.join(os.path.abspath(os.path.pardir),'data\\cats_and_dogs.zip')\r\n\r\npath_to_zip = tf.keras.utils.get_file(download_data_loc,origin=_URL,extract=True, archive_format='zip')\r\n\r\n```\r\nI see the cats_and_dogs.zip folder in following location: C:\\Users\\abhat\\research\\Tensorflow_2.0_tutorials\\data\\cats_and_dogs.zip\r\n\r\nBut there isn't an unzipped folder in this location.\r\n\r\nHowever, if I use,\r\n``` \r\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\r\n\r\n```\r\nI see the zipped folder as well as the unzipped version in the right location\r\nC:\\Users\\abhat\\.keras\\datasets\\cats_and_dogs.zip\r\n\r\n\r\n**System Info:**  \r\nWindows 10 Pro, 64 bit OS.\r\n\r\n**Tensorflow version:**\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0", "comments": ["I have exactly the same issue, any idea of how can we handle it?", "@rudraxx \r\n\r\nCan you please check unzipped folder of `cats_and_dogs.zip` will be saving in `.keras/datasets` folder. Thanks!", "@ravikyram : The .keras/datasets does have the cats_and_dogs.zip and within it the cats_and_dogs_filtered folder. I also see a cats_and_dogs_filtered folder that was automatically unzipped in the .keras/datasets folder. Is this what you wanted me to check?\r\n\r\nI want my data to be in a particular folder. When I specify the full path name to where I want the download, I don't see the unzipped version. \r\n\r\n\r\n", "@vguerrero87:  Haven't found a great solution as yet.\r\n\r\nHere is what I have as of now: \r\n1) Specify the log dir using os library,  which will give windows style path. For eg:\r\n`logdir = os.path.join(model_folder_name, \"tensorboard\", \"my_logs\")\r\n`\r\n(This results in a logdir like:  ..\\cnn_dog_cat_classifier\\tensorboard\\my_logs)\r\n\r\n2) Launch TensorBoard from command window, NOT for the Jupyter notebook. Specify the path as a UNIX style path:\r\n` >> tensorboard --logdir=./cnn_dog_cat_classifier/tensorboard --port=6006\r\n`\r\n\r\nWith this, I can see the multiple runs, but for some reason I can see only the validation loss. The training loss isn't getting logged.\r\n\r\nPlease let me know if you see the same behavior.\r\n\r\nThanks\r\n\r\n", "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\r\nreplace :\r\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs', origin=_URL, extract=True)", "@rudraxx  closing this issue since it was the intended behavior. Please feel free to reopen the issue if necessary.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37269\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37269\">No</a>\n", "I am following the tensorflow text classification and I need to load a zip archive in Jupyter Notes in Windows. When I use the tf.keras.utils.get_file,, I am unable to unzip the downloaded folder, it is url as \"http://...../file.zip\" . \r\nCan you help me??\r\nThanks ", "setting the `fname=None` created the correct file with the following arguments\r\n\r\n`data_dir = tf.keras.utils.get_file(fname=None, origin=f'file://absolute/path/to/file.zip', extract=True)`\r\n"]}, {"number": 37268, "title": "Increased Inference time after doing optimization.", "body": "Hello,\r\n\r\nI am trying to get an optimized graph by doing quantization and removing redundant nodes from frozen_graph.pb.  os features are\r\n\r\nos features:\r\nubuntu 16.0.4 LTS\r\nnvidia driver version = Driver Version: 440.33.01\r\ncuda version = 10\r\ncuDNN version = 7.4\r\ntensorflow = 1.14.1 (built from source with cuda and tensorRT support)\r\nGPU = NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB]\r\nPython version: 3.6.8 (pyenv)\r\n- Bazel version: 0.25\r\n\r\nI ran the command which has to build the new frozen model for me.\r\nwhen I tested it in production, Though the size is reduced to around 20MB but inference time is increased from  average 15ms to 26 ms.\r\n\r\nthe command to produced new optimized graph: \r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=\"/home/ubuntu/test/resnetV150_frozen.pb\" \\\r\n--out_graph=\"/home/ubuntu/test/optimized_inception_graph.pb\" \\\r\n--inputs='image_tensor' \\\r\n--outputs='resnet_v1_50/predictions/Reshape_1' \\\r\n--transforms='  \r\n add_default_attributes  \r\n strip_unused_nodes(type=float, shape=\"1,1280,720,3\")  \r\n remove_nodes(op=Identity, op=CheckNumerics)  \r\n fold_constants(ignore_errors=true)  \r\n fold_batch_norms  \r\n fold_old_batch_norms  \r\n quantize_weights  \r\n quantize_nodes  \r\n strip_unused_nodes \r\n sort_by_execution_order'\r\n\r\n\r\n\r\n", "comments": ["@purvang-ai,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Any update regarding the code to reproduce this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37267, "title": "[r2.2:Cherrypick] Add experimental_make_distributed_values_from_function method to distribution strategy.", "body": "PiperOrigin-RevId: 298445422\nChange-Id: I907cf7808e5bedaf45adccf3b6355ccf219e4116", "comments": []}, {"number": 37266, "title": "Dataset is not cached after model.predict() call (issue present in TF2.0.0 but not in TF2.1.0)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **NO**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **issue present on Linux and Windows**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: NA\r\n- TensorFlow installed from (source or\r\nbinary): **binary**\r\n- TensorFlow version (use command below): **2.0.0  (no issue with 2.1.0)**\r\n- Python version: Python 3.7.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from\r\nsource): NA\r\n- CUDA/cuDNN version: CUDA 10.1 / CuDNN 7.6\r\n- GPU model and memory: issue happens with/without GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe dataset is not correctly put in cache after the first call to predict as it should.\r\nSo the 2nd call to predict returns different predictions than the 1st call despite we have used .cache() on the given dataset.\r\n**Describe the expected behavior**\r\nThe 2 calls to predict in the code below should return the same predictions and thus the assert should pass.\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\ninput_data = tf.keras.layers.Input(name='input_data', shape=(1,), dtype='float32')\r\ndummy_model = tf.keras.models.Model(inputs=input_data, outputs=input_data*2)\r\nds = tf.data.Dataset.range(10).shuffle(10).batch(1).cache()\r\n#len(list(ds))  # next assert passes if we uncomment this line because it forces the caching of the dataset\r\nassert (dummy_model.predict(ds) == dummy_model.predict(ds)).all()\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Issue is replicating on colab with Tf 2.0 and tf-nightly.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/eabd1801363a1f8eb64950264d046da1/untitled423.ipynb). Thanks!", "@gadagashwini you can you reproduce the issue with TF 2.1?", "@jsimsa, Issue is not replicating with Tf 2.1. Please find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/40c8a38201342c07c15f4f05d453a6e7/untitled429.ipynb?authuser=2). Thanks", "And it is replicating with tf-nightly?", "@jsimsa, \r\nWas able to replicate the issue with Tf-nightly. \r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/3498ef44d88ff2cc5fd6640e58753489/untitled434.ipynb). Thanks! ", "@rachellim could you please take a look? thank you", "This is fixed with tf-nightly version '2.2.0-dev20200402'. Thanks!", "@ismael-elatifi  Closing this issue as it  is resolved. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37266\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37266\">No</a>\n"]}, {"number": 37265, "title": "Tensorflow import issue after new install ImportError: DLL load failed: The specified module could not be found.", "body": "Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tF\r\nTraceback (most recent call last):\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python37\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@sandeepmcr \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you. \r\n\r\nPlease, fill the issue template  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).So we will know details about what platform you are using (operating system, architecture),TensorFlow version, which  in turn helps us in finding root cause of the issue.Thanks!", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37265\">No</a>\n", "> @sandeepmcr\r\n> \r\n> What is make/model of your cpu?\r\n> I suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\n> Make sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads)\r\n> .Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install).\r\n> \r\n> Please, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.\r\n> \r\n> Please, fill the issue template [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).So we will know details about what platform you are using (operating system, architecture),TensorFlow version, which in turn helps us in finding root cause of the issue.Thanks!\r\n\r\nApologize for late resonse ravi:\r\n\r\nI have Thinkpad mobile workstation with Intel Core i7 2.8 Ghz, 64-bit,  64GB RAM , Windows 10\r\nNIVDIA Quadro M620 \r\ni installed Python 64bit both 3.7 and 3.8.   \r\n\r\n", "I updated VC++ distributable as per link provided by you.  Now when import tensorflow i get this specific error. Should ignore it because at least tensorflow seems to load.\r\n\r\n```\r\n2020-03-08 10:40:29.002371: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-03-08 10:40:29.006704: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```", "@sandeepmcr that is just a warning that cuda couldn't be loaded on your system. As the second line says, you can ignore that if you don't need GPU.\r\n\r\nIf you do, then you have to make sura CUDA 10.1 libs are available.", "> @sandeepmcr that is just a warning that cuda couldn't be loaded on your system. As the second line says, you can ignore that if you don't need GPU.\r\n> \r\n> If you do, then you have to make sura CUDA 10.1 libs are available.\r\n\r\nThis has got resolved now after I installed CUDA 10.2 toolkit . Tensorflow is getting imported without error . "]}, {"number": 37264, "title": "tf.name_scope does not obey its own documented rules", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (GNU/Linux 4.14.137+ x86_64)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.1.0-0-ge5bf8de410\r\n- Python version: 3.6.9 (default, Nov  7 2019, 10:44:02)\r\n\r\n\\\r\nAccording to the https://www.tensorflow.org/api_docs/python/tf/name_scope:\r\n\r\n> If the scope name already exists, the name will be made unique by appending _n. For example, calling my_op the second time will generate MyOp_1/a, etc.\r\n\r\nHowever, it turned out to be not. Nor did I manage to find any code (inside `tf.name_scope`) that might generate the uniqueness.\r\n\r\n``` python\r\nimport tensorflow as tf\r\nlayer = tf.keras.Input(shape = [None])\r\ndef get_shape(layer):\r\n  with tf.name_scope('scope') as scope:\r\n    return tf.shape(layer, name=scope)\r\nget_shape(layer)\r\nget_shape(layer)\r\n```\r\nThe first `get_shape(layer)`:\r\n```\r\n<tf.Tensor 'scope:0' shape=(2,) dtype=int32>\r\n```\r\nThe second `get_shape(layer)`:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\", line 1619, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'scope'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<stdin>\", line 3, in get_shape\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py\", line 519, in shape_v2\r\n    return shape(input, name, out_type)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py\", line 545, in shape\r\n    return shape_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/array_ops.py\", line 573, in shape_internal\r\n    return gen_array_ops.shape(input, name=name, out_type=out_type)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_array_ops.py\", line 8234, in shape\r\n    \"Shape\", input=input, out_type=out_type, name=name)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/func_graph.py\", line 595, in _create_op_internal\r\n    compute_device)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\", line 3322, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\", line 1786, in __init__\r\n    control_input_ops)\r\n  File \"/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\", line 1622, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Duplicate node name in graph: 'scope'\r\n```", "comments": ["@failure-to-thrive\r\nI have tried to replicate your issue and observe that the error is reproducible even for single use of get_shape,please refer to the [gist](https://colab.sandbox.google.com/gist/Saduf2019/9096209a8e0752e0bb96d08a727635e7/37264.ipynb) and let us know if that helps.\r\nplease refer to this link for [usage of shape](https://www.tensorflow.org/api_docs/python/tf/shape?hl=sv)\r\n\r\n", "> I have tried to replicate your issue and observe that the error is reproducible even for single use of get_shape,please refer to the [gist](https://colab.sandbox.google.com/gist/Saduf2019/9096209a8e0752e0bb96d08a727635e7/37264.ipynb) and let us know if that helps.\r\n\r\nThe reason why you may observe the error even after the first use - you already launched the script and haven't noticed that. You should reset the notebook. The 'scope' is in the graph and will not go away on its own.\r\n\r\n> please refer to this link for [usage of shape](https://www.tensorflow.org/api_docs/python/tf/shape?hl=sv)\r\n\r\nThanks, but my point is to demonstrate that the `tf.name_scope` doesn't work as documented. `tf.shape` has nothing to do with that. It's just for an example. You may try `print(scope.name)`", "This is fixed with tf-nightly version '2.2.0-dev20200309'. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37264\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37264\">No</a>\n"]}, {"number": 37263, "title": "LOW GPU HIGH CPU :(", "body": "Hello!\r\nI am using Tensorflow on Docker, Ubuntu.\r\nI have tried many tensorflow docker versions, tried different drivers, tried everything.:\r\n\r\n## Sys info\r\n-  Linux 5.3.0-40-generic 18.04.1 Ubuntu x86_64\r\n- Docker 19.03.6, nvdia-smi works, I have all packagages needed\r\n- Tensorflow Docker versions I tried (in some of the the GPU didn't even get recognized..): \r\n  - latest-gpu-jupyter (Py2, this works okay, but low gpu usage)\r\n  - latest-gpu-py3-jupyter (sometimes bugs out)\r\n  - 2.0.1-gpu-py3-jupyter (sometimes bugs out regarding GPU too)\r\n- NVIDIA-SMI: 440.59, GeForce MX150, 2002MiB\r\n\r\n## So what I am trying to DO:\r\n I load the MINTS dataset and do a **nearest neighbor** algorithm on them. Pretty simple right, I just started using tenforflow, I would love it if it was eating 100% of my GPU not CPU (if that is even possible). This is my code concisely:\r\n\r\n```\r\n(Xtr, Ytr), (Xte, Yte) = keras.datasets.cifar10.load_data()\r\n# I convert them to tensors\r\n# I save the whole training set into Xtr (50 000 rows of 3072 int32) to later do the nearest neighbor with them\r\n# Upon \"prediction\" I find the index of Xtr row with lowest L1 distance from X for each X test row (10000 rows of 3072 ints):\r\nwith tf.device('/gpu:0'):\r\n    min_distances_index = tf.map_fn(self.reduce_distance, Xte)\r\n# which calls this tf function:\r\n    @tf.function\r\n    def reduce_distance(self, X):\r\n        # X is of shape (3072,1)\r\n        return tf.math.argmin(tf.math.reduce_sum(tf.math.abs(tf.math.subtract(self.Xtr, X)), axis=1), output_type=tf.int32)\r\n# Quite simple and readable I hope!\r\n# Then when I have min distance indices I simply return the Y values from them\r\n```\r\nI use int32 beacause as you saw there is subtraction and then sum, so.. \r\n\r\n## Now\r\n**This takes 30 minutes!**\r\nAnd this is my system statistic during running:\r\nFrom htop:\r\n![image](https://user-images.githubusercontent.com/11639734/75797157-05ea0100-5d75-11ea-9c7b-66df992270ab.png)\r\nnvidia-smi:\r\n![image](https://user-images.githubusercontent.com/11639734/75797912-1e0e5000-5d76-11ea-9b23-245cd648f9d3.png)\r\nJumping between 0% and 20% but on average below 10%\r\n\r\nWhy? Is this normal? \r\nThanks in advance.\r\n", "comments": ["@najibghadri \r\n\r\nThanks for reporting the issue. Looks like code is incomplete. Request you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "https://colab.research.google.com/drive/1QyVMNO-paFJQ2Ie9zkIva0FA9wbxAYT2", "@najibghadri I think this is because most int32 kernels are placed on host memory. You can refer to the comment [here](https://github.com/tensorflow/tensorflow/issues/33835#issuecomment-548661718). ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing this issue as it has been inactive for a long time. Please add additional comments for us to open this issue again. Thanks!"]}, {"number": 37262, "title": "tensorflow:Saver not created because there are no variables in the graph to restore", "body": "I am a new bee trying to learn Tensorflow from this[ site ](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#test-tf-models). \r\nAfter all installation when I run:\r\n`(tensorflow1) C:\\tensorflow1\\models\\research\\object_detection> jupyter notebook object_detection_tutorial.ipynb`\r\nI got this message: \"tensorflow:Saver not created because there are no variables in the graph to restore\"\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: NO\r\n- **OS Platform and Distribution: Windows 10_64\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**: conda install \r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: 3.7.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**: cudatoolkit =v10.1.243   cudnn  =v7.6.5\r\n- **GPU model and memory**: GeForce 920MX  2GB\r\n\r\n\r\n\r\nProblem:\r\nI get the message \r\n\r\n> \r\nmodel_name = 'ssd_mobilenet_v1_coco_2017_11_17'\r\ndetection_model = load_model(model_name)\r\n\r\n\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n\r\ntensorflow:Saver not created because there are no variables in the graph to restore\r\nand \r\n![image](https://user-images.githubusercontent.com/61747889/75795865-ddcac400-5dad-11ea-9fc6-c5e3f545c3d1.png)\r\n\r\nfinally end up with \r\n![image](https://user-images.githubusercontent.com/61747889/75796019-0fdc2600-5dae-11ea-8730-e19665de8371.png)\r\n\r\n###Final error logs:\r\n---------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<ipython-input-21-25f0f9a75087> in <module>\r\n      1 for image_path in TEST_IMAGE_PATHS:\r\n----> 2   show_inference(masking_model, image_path)\r\n\r\n<ipython-input-17-e474e557b383> in show_inference(model, image_path)\r\n      4   image_np = np.array(Image.open(image_path))\r\n      5   # Actual detection.\r\n----> 6   output_dict = run_inference_for_single_image(model, image_np)\r\n      7   # Visualization of the results of a detection.\r\n      8   vis_util.visualize_boxes_and_labels_on_image_array(\r\n\r\n<ipython-input-16-4110867dcb70> in run_inference_for_single_image(model, image)\r\n      7 \r\n      8   # Run inference\r\n----> 9   output_dict = model(input_tensor)\r\n     10 \r\n     11   # All outputs are batches tensors.\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   1549       TypeError: For invalid positional/keyword argument combinations.\r\n   1550     \"\"\"\r\n-> 1551     return self._call_impl(args, kwargs)\r\n   1552 \r\n   1553   def _call_impl(self, args, kwargs, cancellation_manager=None):\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_impl(self, args, kwargs, cancellation_manager)\r\n   1589       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\r\n   1590           list(kwargs.keys()), list(self._arg_keywords)))\r\n-> 1591     return self._call_flat(args, self.captured_inputs, cancellation_manager)\r\n   1592 \r\n   1593   def _filtered_call(self, args, kwargs):\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\Anaconda3\\envs\\tensorflow1\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted:  OOM when allocating tensor with shape[1166541600] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/NonMaxSuppressionV2 (defined at <ipython-input-9-f8a3c92a04a4>:11) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[BatchMultiClassNonMaxSuppression/map/while/strided_slice/_170]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted:  OOM when allocating tensor with shape[1166541600] and type int32 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[node BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/NonMaxSuppressionV2 (defined at <ipython-input-9-f8a3c92a04a4>:11) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_pruned_65677]\r\n\r\nFunction call stack:\r\npruned -> pruned\r\n", "comments": ["@masumkmu,\r\n\r\nThe link you have provided does not have the code \r\n```\r\nmodel_name = 'ssd_mobilenet_v1_coco_2017_11_17'\r\ndetection_model = load_model(model_name)\r\n```\r\nanywhere in the tutorial.\r\n\r\nAlso, I see that the tutorial is using TensorFlow 1.9, whereas you have TensorFlow 2.1 installed.\r\n\r\nCould you please check out [this](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) official tutorial from TensorFlow, which uses TensorFlow 2.x for object detection. Thanks!", "I think thats more of the memory problem can you show us the output of nvidia-smi before runnnig the code and also the cudnn version", "@masumkmu,\r\nAny updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nI don`t know why? who can tell me where the problem has been occurred..Thanks\r\n@masumkmu  Hi,guy! Have you solved this problem?", "Same issue here on \"ssd_mobilenet_v1_coco_2017_11_17\" and \"ssd_mobilenet_v2_coco_2018_03_29\" Does anyone find the solution? Does anyone knows the reason why it is not working? Using the frozen graphs is the best option for now? ", "I'm having the same issue as op from \"research/object_detection/object_detection_tutorial.ipynb\".", "> I'm having the same issue as op from \"research/object_detection/object_detection_tutorial.ipynb\".\r\n\r\n@marued,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/models/issues/new/choose) and fill in the issue template, so that we can track it there. Thanks!", "I'm having the same issue as op from \"research/object_detection/object_detection_tutorial.ipynb\".\r\nThere is no answer about \"ssd_mobilenet_v2_coco_2018_03_29\" or other saved model.", "I use ML.NET and have the same issue: \r\nSaver not created because there are no variables in the graph to restore", "@kwijik,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the issue template, so that we can track it there. Thanks!", "sorry, \r\nhttps://github.com/tensorflow/tensorflow/issues/37262#issuecomment-594477332\r\n> \r\n> Could you please check out [this](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) official tutorial from TensorFlow, which uses TensorFlow 2.x for object detection. Thanks!\r\n\r\nat 2020-06-26 the link seems broken...where is the 2.x correct ipython notebook for objectdetection?", "@ozett, \r\nLooks like the file has been removed. Could you please submit a new issue from [this link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "new issue here: https://github.com/tensorflow/models/issues/8736", "any update ?", "Hi guys, here are my two cents about those people facing this kind  issues like `Saver not created because there are no variables in the graph to restore`\r\n1. I believe you are trying to import frozen model that trained with tensorflow 1 by using tensorflow 2.\r\n- check the version of the tensorflow you have installed in your system. \r\n2. if the model is trained with tensorflow 2, under the folder **saved_model** u will find while called variable and assets. try to load the model using tensorflow 2 as well. \r\n3. try to import your full path full path i mean you have to import **absolute path** to the file that you want to import. \r\n\r\nHope this will help.", "I am using \r\nbert-tensorflow==1.0.1\r\ntensorflow version = 2.4.1\r\nkeras - 2.4.3\r\n\r\n\r\n# Code\r\nBERT_MODEL_HUB = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\r\n\r\ndef create_tokenizer_from_hub_module():\r\n  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\r\n  with tf.Graph().as_default():\r\n    bert_module = hub.Module(BERT_MODEL_HUB)\r\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\r\n    with tf.Session() as sess:\r\n      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\r\n                                            tokenization_info[\"do_lower_case\"]])\r\n      \r\n  return bert.tokenization.FullTokenizer(\r\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\r\n\r\ntokenizer = create_tokenizer_from_hub_module()\r\n\r\nAfter running above cell in Colab I'm getting the following msg\r\n\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n", "@knightperfectionist Have you solved this problem?"]}, {"number": 37261, "title": "Giving example on website has no model.predict examples", "body": "First of all, you have a great website to understand the Tensorflow library. I am a newbie. And want to understand the point is: I go through the examples. Except for basic image classification, there is no example : how to feed my own data wich has no label. And I want to get predictions of my own data. example :   https://stackoverflow.com/questions/60389558/how-to-feed-my-own-data-and-evaluate-at-tf-text-classification ", "comments": ["@cappittall, Please go through this [link](https://www.tensorflow.org/tutorials/load_data/text) to load your own text data. Thanks!", "@gadagashwini Thank you for the reply. I am aware of this. I study and trained my own data. Built the model. Now, I need only the prediction of my new data. How to feed new data to the model.  ", "@cappittall, To predict your own data with trained model, you use the following code\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimag\r\n\r\nimg = mpimage.imread(\"test_image.jpg\")\r\nimplot = plt.imshow(img)\r\nprob = loaded_model.predict(img)  # load your model \r\n\r\n```\r\nLet us know if this helps. Thanks!", "Yea, there at basic image classification all are very clear.\u00a0 I am working on text classification. I am sorry I must describe this at the beginning. (Detail was at StackOverflow link)  However, I work on \r\n\r\n> https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\r\n\r\nI come to the end with success (with my data.) Build the model. Now Should I use the same like image detection as:\r\n\r\n```\r\nprobability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\r\npredictions = probability_model.predict(test_text)\r\n```\r\nI tried this with no success.\u00a0Gives: \r\n `Error when checking input: expected sequential_input to have 1 dimensions, but got array with shape ()`\r\n\r\nSo, at the bottom of the text classification, I need to feed a text or text array to get predictions. I tried some ways with no success. ", "@cappittall, Can you provide the complete standalone code to analyze the issue. Thanks", "Here my code so far: \r\nHere I tried to re-product Tensorflow's [own example](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub) \r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport matplotlib.pyplot as plt\r\nfrom absl import logging\r\nimport numpy as np\r\nimport os\r\nimport pandas as pd\r\nimport seaborn as sns\r\n\r\n\r\ndir = os.path.dirname(os.path.realpath(__file__))\r\nprint(dir)\r\nfile_name=\"traindata.txt\"\r\n\r\ntrain_df = {}\r\ntest_df = {}\r\nremain = {}\r\n\r\nwith open (file_name, 'r', encoding=\"utf8\") as l:\r\n  lines = l.readlines()\r\n\r\n#my predict data \r\nwith open ('remain.txt', 'r', encoding=\"utf8\" ) as l:\r\n    remain_data = l.readlines()\r\n\r\nremain[\"html\"] = [i.lower() for i in remain_data]\r\n\r\n#Split data into train and test \r\ntrain_df[\"html\"] = [i.lower() for i in lines[:1100]]\r\ntest_df[\"html\"] = [i.lower() for i in lines[1100:]]\r\n\r\n#Split labels into train and test\r\nwith open ('trainlabel.txt', 'r') as l:\r\n  labels =  l.readlines()\r\n  labels = [int(i) for i in labels]\r\n  train_df[\"polarity\"] = labels[:1100]\r\n  test_df[\"polarity\"] = labels[1100:]\r\n\r\n\r\ntrain_df = pd.DataFrame.from_dict(train_df)\r\ntest_df = pd.DataFrame.from_dict(test_df)\r\nremain_df = pd.DataFrame.from_dict(remain)\r\n\r\n# Reduce logging output.\r\nlogging.set_verbosity(logging.ERROR)\r\n\r\n\r\n# Training input on the whole training set with no limit on training epochs.\r\ntrain_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(train_df, train_df[\"polarity\"], num_epochs=None, shuffle=True)\r\n\r\n# Prediction on the whole training set.\r\npredict_train_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(train_df, train_df[\"polarity\"], shuffle=False)\r\n\r\n# Prediction on the test set.\r\npredict_test_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(test_df, test_df[\"polarity\"], shuffle=False)\r\n\r\n# Prediction on the remain  set.\r\npredict_remain_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(test_df, test_df[\"polarity\"], shuffle=False)\r\n\r\nembedded_text_feature_column = hub.text_embedding_column(\r\n    key=\"html\",\r\n    module_spec=\"https://tfhub.dev/google/random-nnlm-en-dim128/1\")\r\n\r\nestimator = tf.estimator.DNNClassifier(\r\n    hidden_units=[64, 32],\r\n    feature_columns=[embedded_text_feature_column],\r\n    n_classes=2,\r\n    optimizer=tf.keras.optimizers.Adagrad(lr=0.003))\r\n\r\n\r\nestimator.train(input_fn=train_input_fn, steps=5000)\r\n\r\ntrain_eval_result = estimator.evaluate(input_fn=predict_train_input_fn)\r\ntest_eval_result = estimator.evaluate(input_fn=predict_test_input_fn)\r\n\r\n\r\nprint(\"Training set accuracy: {accuracy}\".format(**train_eval_result))  ## Ressult %99,\r\nprint(\"Test set accuracy: {accuracy}\".format(**test_eval_result))       ## Result ~ %79, \r\n```\r\n\r\n\r\nNow I need to make predictions.  I have a text array ( scraped site text) that need to get predict whether each text is an e e-commerce site or not.   (ie myArray = [[a sitetext],[ another site text], [ another site text]]", "@cappittall, Save your model and load it. Do the pre-processing steps and feed the input to prediction. Below is the sample code, try this\r\n```\r\nPrediction = estimator.predict(input)\r\n\r\nPrediction_Classes = tf.argmax(Prediction, axis=1)\r\n\r\nfor Input_File_Name, Pred_Class in zip(Input_Files, Prediction_Classes):\r\n    print('Prediction for {} is {}'.format(Input_File_Name, Labels_List[(Pred_Class.numpy()-1)]))\r\n\r\n```\r\nLet us know if it helps. Thanks", "@cappittall, As I gone through given stackoverflow link, I guess that you want to predict on remain data. For this first you need to change \r\n`# Prediction on the remain  set.\r\npredict_remain_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(test_df, test_df[\"polarity\"], shuffle=False)`\r\nto \r\n`# Prediction on the remain  set.\r\npredict_remain_input_fn = tf.compat.v1.estimator.inputs.pandas_input_fn(remain_df, remain_df[\"polarity\"], shuffle=False)`\r\nand then to predict result\r\n`remain_eval_result = estimator.evaluate(input_fn=predict_remain_input_fn)`\r\n`print(\"Remain set accuracy: {accuracy}\".format(**remain_eval_result))`\r\n", "@cappittall, Please confirm if issue is resolved we can move to closure. Thanks!"]}, {"number": 37260, "title": "[XLA] vectorize row reduction for even row size", "body": "This depend on LLVM commit: https://reviews.llvm.org/D74444 that is recently merged in XLA.\r\n\r\nThis PR vectorize row reduction for even row size.\r\nFor odd row size, every other row is unaligned. So would need a more complicated fix.\r\nOn a reduction of size 32x131070, I have no speed up for a max reduction, but I have 1.21x speed on the sum+pw reduction from the softmax in float16.\r\n\r\nThis was benchmarked on the row major \"Softmax\" TF operations.\r\nThe optimized graph contain 3 main operations: reduce_8 (the max reduction), fusion_1 (the exponential and sum reduction), fusion (the post normalization pw operations, ignored).\r\nThis was benchmarked with a matrix of input shape (32x131070) with float16 dtype:\r\n\r\n| Kernel     | Before PR | After PR | Speed up |\r\n| ---------- | ---------- | ---------- | ---------- |\r\n| fusion_1  | 39.827us  | 33.843us | 1.18x |\r\n| reduce_8 | 22.112us  | 22.047us | 1x |\r\n\r\nHere is the not optimized HLO:\r\n```\r\nHloModule cluster_0\r\n%max_ {\r\n  %x = f16[] parameter(0)\r\n  %y = f16[] parameter(1)\r\n  ROOT %maximum = f16[] maximum(%x, %y)\r\n}\r\n\r\n%add_ {\r\n  %x = f32[] parameter(0)\r\n  %y = f32[] parameter(1)\r\n  ROOT %add = f32[] add(%x, %y)\r\n}\r\n\r\nENTRY %main {\r\n  %arg0.1 = f16[32,131070]{1,0} parameter(0)\r\n  %constant.3 = f16[] constant(-inf)\r\n  %pad1 = f16[32,131070] pad(%arg0.1, %constant.3), padding=0_0_0x0_0_0\r\n  %reduce.8 = f16[32]{0} reduce(%pad1, %constant.3), dimensions={1}, to_apply=%max_\r\n  %broadcast.9 = f16[32,131070]{1,0} broadcast(%reduce.8), dimensions={0}\r\n  %subtract.131070 = f16[32,131070]{1,0} subtract(%arg0.1, %broadcast.9)\r\n  %exponential.11 = f16[32,131070]{1,0} exponential(%subtract.131070)\r\n  %convert.12 = f32[32,131070]{1,0} convert(%exponential.11)\r\n  %constant.13 = f32[] constant(0)\r\n  %pad2 = f32[32,131070] pad(%convert.12, %constant.13), padding=0_0_0x0_0_0\r\n  %reduce.18 = f32[32]{0} reduce(%pad2, %constant.13), dimensions={1}, to_apply=%add_\r\n  %convert.19 = f16[32]{0} convert(%reduce.18)\r\n  %broadcast.20 = f16[32,131070]{1,0} broadcast(%convert.19), dimensions={0}\r\n  ROOT %divide.21 = f16[32,131070]{1,0} divide(%exponential.11, f16[32,131070]{1,0} %broadcast.20)\r\n}\r\n```\r\n\r\n\r\n\r\n", "comments": ["I did the change or answer all comments.\r\n\r\nJust a note that for odd row size, this is also complicated. There is memory order optimization problem in LLVM. I already fixed one, but the problem left is probably more complicated. I'm still trying a few different options before trying to change LLVM optimization passes.", "Can you share one HLO graph that regress with this?\r\nAlso, on which GPU do you saw regression? I didn't benchmark on many GPUs or on older GPU generations. Here is an HLO that give me speed up the fused reduction in the optimized graph:\r\n\r\n```\r\nHloModule cluster_0__XlaCompiledKernel_true__XlaNumConstantArgs_0__XlaNumResourceArgs_0_.25\r\n\r\n%max_ {\r\n  %x = f16[] parameter(0)\r\n  %y = f16[] parameter(1)\r\n  ROOT %maximum = f16[] maximum(%x, %y)\r\n}\r\n\r\n%add_ {\r\n  %x = f32[] parameter(0)\r\n  %y = f32[] parameter(1)\r\n  ROOT %add = f32[] add(%x, %y)\r\n}\r\n\r\nENTRY %main {\r\n  %arg0.1 = f16[32,131070]{1,0} parameter(0)\r\n  %constant.3 = f16[] constant(-inf)\r\n  %pad1 = f16[32,131070] pad(%arg0.1, %constant.3), padding=0_0_0x0_0_0\r\n  %reduce.8 = f16[32]{0} reduce(%pad1, %constant.3), dimensions={1}, to_apply=%max_\r\n  %broadcast.9 = f16[32,131070]{1,0} broadcast(%reduce.8), dimensions={0}\r\n  %subtract.131070 = f16[32,131070]{1,0} subtract(%arg0.1, %broadcast.9)\r\n  %exponential.11 = f16[32,131070]{1,0} exponential(%subtract.131070)\r\n  %convert.12 = f32[32,131070]{1,0} convert(%exponential.11)\r\n  %constant.13 = f32[] constant(0)\r\n  %pad2 = f32[32,131070] pad(%convert.12, %constant.13), padding=0_0_0x0_0_0\r\n  %reduce.18 = f32[32]{0} reduce(%pad2, %constant.13), dimensions={1}, to_apply=%add_\r\n  %convert.19 = f16[32]{0} convert(%reduce.18)\r\n  %broadcast.20 = f16[32,131070]{1,0} broadcast(%convert.19), dimensions={0}\r\n  ROOT %divide.21 = f16[32,131070]{1,0} divide(%exponential.11, f16[32,131070]{1,0} %broadcast.20)\r\n}\r\n```\r\n\r\nHere is the nvprof output:\r\n\r\n```\r\n==> OUT.softmaxf16_32x131070_131070._nvprof_TFV=old_master_xla=1.txt <==\r\n2020-03-12 13:04:20.388311: E tensorflow/compiler/xla/tools/replay_computation.cc:476] iteration complete.\r\n==9320== Profiling application: .../tensorflow-source.copy//bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/replay_computation_gpu softmax_template.txt.tmp --print_result=false --use_fake_data --num_runs=5\r\n==9320== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   90.81%  4.8267ms         1  4.8267ms  4.8267ms  4.8267ms  [CUDA memcpy DtoH]\r\n                    3.75%  199.14us         5  39.827us  39.328us  40.607us  fusion_1\r\n                    2.77%  147.36us         5  29.472us  29.024us  29.792us  fusion\r\n                    2.08%  110.56us         5  22.112us  21.600us  22.432us  reduce_8\r\n                    0.34%  17.856us         1  17.856us  17.856us  17.856us  broadcast_2\r\n                    0.25%  13.536us        10  1.3530us  1.0880us  1.8560us  [CUDA memset]\r\n\r\n==> OUT.softmaxf16_32x131070_131070._nvprof_TFV=rb2_xla=1.txt <==\r\n2020-03-12 12:57:39.328761: E tensorflow/compiler/xla/tools/replay_computation.cc:476] iteration complete.\r\n==8924== Profiling application: .../tensorflow-source.copy//bazel-out/k8-opt/bin/tensorflow/compiler/xla/tools/replay_computation_gpu softmax_template.txt.tmp --print_result=false --use_fake_data --num_runs=5\r\n==8924== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   94.13%  7.3283ms         1  7.3283ms  7.3283ms  7.3283ms  [CUDA memcpy DtoH]\r\n                    2.17%  169.22us         5  33.843us  32.608us  35.040us  fusion_1\r\n                    1.88%  146.72us         5  29.343us  28.896us  29.791us  fusion\r\n                    1.42%  110.24us         5  22.047us  21.760us  22.176us  reduce_8\r\n                    0.23%  17.792us         1  17.792us  17.792us  17.792us  broadcast_2\r\n                    0.17%  13.312us        10  1.3310us  1.0560us  1.8880us  [CUDA memset]\r\n```\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37260) for more info**.\n\n<!-- need_author_cla -->", "I added 4 commits:\r\n- The first one trigger some of the missing vectorization by LLVM. ptxas was vectorizing those already, but at least, now they are tested here.\r\n- The 3rd one fix a regression. We where using the columns reduction indexing in some cases.\r\n- The 2nd and 4th limit the vectorization on P100 GPUs. We now only vectorize when the tile fit exactly and for dtype of up to 32 bits. \r\n\r\nOn P100, with float16, I see 1.23x and 2.33x speed up. In float32, I see 1.08x and 1.10x speed up.\r\n\r\nThe PR can be benchmarked. But I still need to update the tests to have expect the right vectorization depending of the hardware. I need to find how to detect the current GPU in the test.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37260) for more info**.\n\n<!-- ok -->", "I updated the tests to expect the right PTX depending of the GPU arch.\r\nI also fixed crashed that I introduced today by an earlier in d9ae69f04ba944384d117662b888c43cb7e0bf72\r\n\r\nI did a small refactoring to help that kind of error to not pop up again here.", "@nouiz I'm a bit confused by your benchmark, do you mean the runtime reduction in `fusion_1` from `39us` to `33us`?", "exact", "Their is a CI failure. I didn't see it before. I'm able to reproduce it. I'll look into it .", "Fixed. I needed to check that the layout is row major for one of the simplification.", "New PR with the CHECK fix and the other small code changes:\r\nhttps://github.com/tensorflow/tensorflow/pull/38136"]}, {"number": 37259, "title": "Implementation of 8-bit quantized mean in TFLu", "body": "", "comments": ["Here is the internal error please check \r\n\r\n`In file included from third_party/tensorflow/lite/micro/kernels/reduce.cc:16:\r\n./third_party/tensorflow/lite/kernels/internal/reference/reduce.h:360:17: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int' [-Werror,-Wsign-compare]\r\n    if (current > (std::numeric_limits<U>::max() / num_elements_in_axis)) {\r\n        ~~~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/lite/micro/kernels/reduce.cc:115:26: note: in instantiation of function template specialization 'tflite::reference_ops::QuantizedMeanOrSum<signed char, int>' requested here\r\n          reference_ops::QuantizedMeanOrSum(\r\n                         ^\r\nIn file included from third_party/tensorflow/lite/micro/kernels/reduce.cc:16:\r\n./third_party/tensorflow/lite/kernels/internal/reference/reduce.h:360:17: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int' [-Werror,-Wsign-compare]\r\n    if (current > (std::numeric_limits<U>::max() / num_elements_in_axis)) {\r\n        ~~~~~~~ ^  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/lite/micro/kernels/reduce.cc:124:22: note: in instantiation of function template specialization 'tflite::reference_ops::QuantizedMeanOrSum<unsigned char, int>' requested here\r\n      reference_ops::QuantizedMeanOrSum(`", "@patriklaurell Can you please check @rthadur's comments and keep us posted. Thanks!", "On hold. We found an issue with this that needs to be fixed.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37259) for more info**.\n\n<!-- need_author_consent -->", "accidentally rebased. will create new PR with appropriate changes. my apologies "]}, {"number": 37258, "title": "Got \"KeyError: 'stack/values_0' \" when use custom network layer by tf.keras.layers.Layer", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution: Linux Ubuntu 16.04 \r\n- TensorFlow installed from : Anaconda\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0/7.0\r\n- GPU model and memory: 6GB\r\n- Tensorfflow version: 2.0.0\r\n\r\n**custom network layer**\r\n``` python\r\nimport tensorflow as tf\r\n\r\n\r\nclass ROIPoolingLayer(tf.keras.layers.Layer):\r\n    def __init__(self, pooled_height, pooled_width, **kwargs):\r\n        self.pooled_height = pooled_height\r\n        self.pooled_width = pooled_width\r\n        super(ROIPoolingLayer, self).__init__(**kwargs)\r\n\r\n    @staticmethod\r\n    def _pool_roi(feature_map, roi, pooled_height, pooled_width):\r\n        \"\"\"\r\n        Applies ROI Pooling to a single image and a single ROI\r\n        \"\"\"\r\n        # Compute the region of interest\r\n        feature_map_height = int(feature_map.shape[0])\r\n        feature_map_width = int(feature_map.shape[1])\r\n\r\n        h_start = tf.cast(feature_map_height * roi[0], dtype=tf.int32)\r\n        w_start = tf.cast(feature_map_width * roi[1], dtype=tf.int32)\r\n        h_end = tf.cast(feature_map_height * roi[2], dtype=tf.int32)\r\n        w_end = tf.cast(feature_map_width * roi[3], dtype=tf.int32)\r\n\r\n        region = feature_map[h_start:h_end, w_start:w_end, :]\r\n\r\n        # Divide the region into non overlapping areas\r\n        region_height = h_end - h_start\r\n        region_width = w_end - w_start\r\n        h_step = tf.cast(region_height / pooled_height, dtype=tf.int32)\r\n        w_step = tf.cast(region_width / pooled_width, dtype=tf.int32)\r\n\r\n        areas = [\r\n            [\r\n                (\r\n                    i * h_step,\r\n                    j * w_step,\r\n                    (i + 1) * h_step if i + 1 < pooled_height else region_height,\r\n                    (j + 1) * w_step if j + 1 < pooled_width else region_width\r\n                )\r\n                for j in range(pooled_width)\r\n            ]\r\n            for i in range(pooled_height)\r\n        ]\r\n\r\n        # Take the maximum of each area and stack th result\r\n        def pool_area(x):\r\n            return tf.reduce_max(region[x[0]:x[2], x[1]:x[3], :], axis=[0, 1])\r\n\r\n        pooled_features = tf.stack([[pool_area(x) for x in row] for row in areas])\r\n\r\n        return pooled_features\r\n\r\n    @staticmethod\r\n    def _pool_rois(feature_map, rois, pooled_height, pooled_width):\r\n        \"\"\"\r\n        Applies ROI pooling for a single image and varios ROIs\r\n        \"\"\"\r\n        def curried_pool_roi(roi):\r\n            return ROIPoolingLayer._pool_roi(feature_map, roi, pooled_height, pooled_width)\r\n\r\n        pooled_areas = tf.map_fn(curried_pool_roi, rois, dtype=tf.float32)\r\n\r\n        return pooled_areas\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        \"\"\"\r\n        Returns the shape of the ROI Pooling Layer output\r\n        \"\"\"\r\n        feature_map_shape, rois_shape = input_shape\r\n        assert feature_map_shape[0] == rois_shape[0]\r\n        batch_size = feature_map_shape[0]\r\n        n_rois = rois_shape[1]\r\n        n_channels = feature_map_shape[0]\r\n        return tuple((batch_size, n_rois, self.pooled_height, self.pooled_width, n_channels))\r\n\r\n    def get_config(self):\r\n        config = {\r\n            \"pooled_height\": self.pooled_height,\r\n            \"pooled_width\": self.pooled_width\r\n        }\r\n        base_config = super(ROIPoolingLayer, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n\r\n    def call(self, inputs, **kwargs):\r\n        \"\"\"\r\n        Maps the input tensor of the ROI layer to its output\r\n        \"\"\"\r\n        def curried_pool_rois(x):\r\n            return ROIPoolingLayer._pool_rois(x[0], x[1], self.pooled_height, self.pooled_width)\r\n\r\n        pooled_areas = tf.map_fn(curried_pool_rois, inputs, dtype=tf.float32)\r\n        if pooled_areas.shape[1] == 1:\r\n            return tf.squeeze(pooled_areas, axis=1)\r\n        return pooled_areas\r\n```\r\n\r\n**Test code**\r\n``` python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom net.layer.roi_pooling import ROIPoolingLayer\r\n\r\nddef test_for_tf2():\r\n    input_img = tf.keras.Input(shape=(200, 100, 1), batch_size=1, name=\"input_img\")\r\n    roi_region = ROIPoolingLayer(3, 7)([input_img, np.asarray([[[0.5, 0.2, 0.7, 0.4], [0.0, 0.0, 1.0, 1.0]]], dtype=np.float32)])\r\n    fc0 = tf.keras.layers.Flatten()(roi_region)\r\n    fc1 = tf.keras.layers.Dense(30, activation=None, name=\"pose_ren_outputs\")(fc0)\r\n    model = tf.keras.Model(inputs=input_img, outputs=fc1, name=\"Test_model\")\r\n    model.summary()\r\n\r\ndef main():\r\n    test_for_tf2()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\n**Error info**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/test/roi_pooling_test.py\", line 59, in <module>\r\n    main()\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/test/roi_pooling_test.py\", line 55, in main\r\n    test_for_tf2()\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/test/roi_pooling_test.py\", line 48, in test_for_tf2\r\n    roi_region = ROIPoolingLayer(3, 7)([input_img, np.asarray([[[0.5, 0.2, 0.7, 0.4], [0.0, 0.0, 1.0, 1.0]]], dtype=np.float32)])\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/net/layer/roi_pooling.py\", line 95, in call\r\n    pooled_areas = tf.map_fn(curried_pool_rois, inputs, dtype=tf.float32)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2714, in while_loop\r\n    loop_vars = body(*loop_vars)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2705, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\", line 257, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/net/layer/roi_pooling.py\", line 93, in curried_pool_rois\r\n    return ROIPoolingLayer._pool_rois(x[0], x[1], self.pooled_height, self.pooled_width)\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/net/layer/roi_pooling.py\", line 61, in _pool_rois\r\n    pooled_areas = tf.map_fn(curried_pool_roi, rois, dtype=tf.float32)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2714, in while_loop\r\n    loop_vars = body(*loop_vars)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2705, in <lambda>\r\n    body = lambda i, lv: (i + 1, orig_body(*lv))\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py\", line 257, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/net/layer/roi_pooling.py\", line 59, in curried_pool_roi\r\n    return ROIPoolingLayer._pool_roi(feature_map, roi, pooled_height, pooled_width)\r\n  File \"/home/lxz/PycharmProjects/pose-ren-tf2/src/net/layer/roi_pooling.py\", line 49, in _pool_roi\r\n    pooled_features = tf.stack([[pool_area(x) for x in row] for row in areas])\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 1165, in stack\r\n    return gen_array_ops.pack(values, axis=axis, name=name)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6304, in pack\r\n    \"Pack\", values=values, axis=axis, name=name)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3360, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1792, in __init__\r\n    self._control_flow_post_processing()\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1800, in _control_flow_post_processing\r\n    for input_tensor in self.inputs:\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2167, in inputs\r\n    for tf_output in tf_outputs\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 2167, in <listcomp>\r\n    for tf_output in tf_outputs\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3801, in _get_tensor_by_tf_output\r\n    op = self._get_operation_by_tf_operation(tf_output.oper)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3765, in _get_operation_by_tf_operation\r\n    return self._get_operation_by_name_unsafe(op_name)\r\n  File \"/home/lxz/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3761, in _get_operation_by_name_unsafe\r\n    return self._nodes_by_name[name]\r\nKeyError: 'stack/values_0'\r\n```\r\n\r\n\r\n\r\n", "comments": ["Can anyone help me?", "I am a bit familiar with this code...\r\nAre you try to build Fast/Faster R-CNN?\r\n[ROI Pool Layer](https://medium.com/xplore-ai/implementing-attention-in-tensorflow-keras-using-roi-pooling-992508b6592b)\r\n", "> I am a bit familiar with this code...\r\n> Are you try to build Fast/Faster R-CNN?\r\n> [ROI Pool Layer](https://medium.com/xplore-ai/implementing-attention-in-tensorflow-keras-using-roi-pooling-992508b6592b)\r\n\r\nYes", "> > I am a bit familiar with this code...\r\n> > Are you try to build Fast/Faster R-CNN?\r\n> > [ROI Pool Layer](https://medium.com/xplore-ai/implementing-attention-in-tensorflow-keras-using-roi-pooling-992508b6592b)\r\n> \r\n> Yes\r\n\r\ntry this:\r\n[Layer Class](https://github.com/AlucardNosferatu/RCNN/blob/Fast/ROI_Pooling.py)\r\nIn line 71:\r\n[Called in Model](https://github.com/AlucardNosferatu/RCNN/blob/Fast/FastRCNN.py)\r\n\r\nPS.I am not the author.", "> > > I am a bit familiar with this code...\r\n> > > Are you try to build Fast/Faster R-CNN?\r\n> > > [ROI Pool Layer](https://medium.com/xplore-ai/implementing-attention-in-tensorflow-keras-using-roi-pooling-992508b6592b)\r\n> > \r\n> > \r\n> > Yes\r\n> \r\n> try this:\r\n> [Layer Class](https://github.com/AlucardNosferatu/RCNN/blob/Fast/ROI_Pooling.py)\r\n> In line 71:\r\n> [Called in Model](https://github.com/AlucardNosferatu/RCNN/blob/Fast/FastRCNN.py)\r\n> \r\n> PS.I am not the author.\r\n\r\nThanks\uff01It works", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37258\">No</a>\n", "> > > > I am a bit familiar with this code...\r\n> > > > Are you try to build Fast/Faster R-CNN?\r\n> > > > [ROI Pool Layer](https://medium.com/xplore-ai/implementing-attention-in-tensorflow-keras-using-roi-pooling-992508b6592b)\r\n> > > \r\n> > > \r\n> > > Yes\r\n> > \r\n> > \r\n> > try this:\r\n> > [Layer Class](https://github.com/AlucardNosferatu/RCNN/blob/Fast/ROI_Pooling.py)\r\n> > In line 71:\r\n> > [Called in Model](https://github.com/AlucardNosferatu/RCNN/blob/Fast/FastRCNN.py)\r\n> > PS.I am not the author.\r\n> \r\n> Thanks\uff01It works\r\n\r\n\u5927\u4f6c\uff0c\u90a3\u4e2aROI\u6c60\u5316\u5c42\u8c8c\u4f3c\u6709\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u6d6e\u70b9\u5750\u6807\u505a\u7c7b\u578b\u8f6c\u5316\u65f6tf.cast\u4f1a\u76f4\u63a5\u628a\u5c0f\u6570\u90e8\u5206\u780d\u6389\u5bfc\u81f4\u4e00\u4e9bROI\u7279\u5f81\u56fe\u9762\u79ef\u4e3a\u96f6\uff0c\u5982\u679c\u4f60\u51c6\u5907\u6539\u5199ROI Align\u5f53\u6211\u5565\u4e5f\u6ca1\u8bf4\u597d\u4e86\u3002\u3002\u3002\r\n"]}, {"number": 37257, "title": " Blas GEMM launch failed;failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): windows10\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): tensorflow2.0.0\r\n- Python version: python3.7.3\r\n- CUDA/cuDNN version: cuda10.2/cudnn7.6.5\r\n- GPU model and memory:GeForce GTX 1060 6G\r\n\r\nYou can collect some of this information using our environment capture\r\nerror log as blow:\r\n`2020-03-03 20:36:53.881317: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\n2020-03-03 20:36:53.886872: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 69, in <module>\r\n    train()\r\n  File \"main.py\", line 61, in train\r\n    train_epoch(epoch)\r\n  File \"main.py\", line 43, in train_epoch\r\n    out = model(x)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\", line 270, in call\r\n    outputs = layer(inputs, **kwargs)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 891, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\", line 1056, in call\r\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\r\n  File \"D:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 6126, in mat_mul\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(200, 784), b.shape=(784, 512), m=200, n=512, k=784 [Op:MatMul]`\r\n**Describe the current behavior**\r\ntwo errors:1.failed to create cublas handle. 2.Blas GEMM launch failed\r\nI don't know where there is a problem. this is my first time using a gpu to code. when I thought the installation these cuda and tensorflow well, everyting is ok ,the problem occurs as below.\r\n\r\n", "comments": ["@Questdream \r\nCould you please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/36781#issuecomment-593611624) on similar issue faced and let us know if that helps you resolve the issue.", "@Questdream\r\nPlease update us on the above comment", "@Questdream\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37257\">No</a>\n", "I have meet the same problem like you when i update tensorflow2.0.0 -> 2.3.0\r\n\r\n**Environment:**\r\n\r\nRTX1080Ti + cuda10.0 + cudnn7.6.0 + ubuntu 16.04 + tensorflow-gpu 2.3.0\r\n\r\n**The code is very simple.**\r\n```\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\npredictions = model(x_train[:1]).numpy()\r\nprint(predictions)\r\n```\r\n\r\n**Error Msg:**\r\n```\r\n2020-09-01 12:48:46.703511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-01 12:48:47.713105: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-09-01 12:48:47.740276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.740779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-01 12:48:47.740798: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-01 12:48:47.741669: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-01 12:48:47.742320: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-01 12:48:47.742489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-01 12:48:47.743454: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-01 12:48:47.744218: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-01 12:48:47.746263: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-01 12:48:47.746327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.746794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.747208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-01 12:48:47.747393: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-09-01 12:48:47.772100: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3192000000 Hz\r\n2020-09-01 12:48:47.772975: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e3db1aba90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-01 12:48:47.772994: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-01 12:48:47.852506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.852956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e3db217d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-01 12:48:47.852971: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-09-01 12:48:47.853082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.853437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-01 12:48:47.853459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-01 12:48:47.853472: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-01 12:48:47.853480: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-01 12:48:47.853489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-01 12:48:47.853497: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-01 12:48:47.853504: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-01 12:48:47.853512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-01 12:48:47.853544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.853904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:47.854236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-09-01 12:48:47.854253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-01 12:48:48.306067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-01 12:48:48.306095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-09-01 12:48:48.306101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-09-01 12:48:48.306254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:48.306654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-01 12:48:48.307018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9554 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-09-01 12:48:48.420102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-09-01 12:48:48.420404: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-09-01 12:48:48.420442: E tensorflow/stream_executor/cuda/cuda_blas.cc:225] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\r\n2020-09-01 12:48:48.420448: W tensorflow/stream_executor/stream.cc:2055] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\nTraceback (most recent call last):\r\n  File \"tf2_test.py\", line 16, in <module>\r\n    predictions = model(x_train[:1]).numpy()\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 372, in call\r\n    return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\", line 386, in call\r\n    inputs, training=training, mask=mask)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\", line 508, in _run_internal_graph\r\n    outputs = node.layer(*args, **kwargs)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 985, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\", line 1198, in call\r\n    dtype=self._compute_dtype_object)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/keras/layers/ops/core.py\", line 53, in dense\r\n    outputs = gen_math_ops.mat_mul(inputs, kernel)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 5624, in mat_mul\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/ibrain/anaconda3/envs/nlp/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6843, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(1, 784), b.shape=(784, 128), m=1, n=128, k=784 [Op:MatMul]\r\n```\r\n\r\nEverything goed will in tensorflow2.0.0, but went wrong in tensorflow2.3.0.\r\n\r\n**This comment(https://github.com/tensorflow/tensorflow/issues/36781#issuecomment-593611624) is not suited for me, my cuda version is 10.0**\r\n\r\nOtherwise, when i run: sudo apt-cache policy libcublas10\r\n```\r\nN: Unable to locate package libcublas10\r\n```"]}, {"number": 37256, "title": "Tensorflow 2.1: How does the metric 'tf.keras.metrics.PrecisionAtRecall' works with multiclass-classification? ", "body": "I am wondering how this metrics works in case of multiclass classification. Does it compute the average between values of precision belonging to each class?\r\n\r\nI have a multiclass-classification problem, with three classes. I am interested in calculate the PrecisionAtRecall when the recall value is equal to 0.76, only for a specific class . What I have done was just setting 0.76 in brackets :\r\n'' model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.PrecisionAtRecall(0.76)], sample_weight_mode='temporal') ''\r\nBut I am sure is not the right way.\r\nHow can I adjust the metric to reach my goal?\r\nThanks", "comments": ["As you can see\r\n`tf.keras.metrics.PrecisionAtRecall(recall, num_thresholds=200, name=None, dtype=None)`\r\nhas 3 arguments\r\nThe argument here to notice is `num_thresholds` which is optional and Defaults to 200. The number of thresholds to use for matching the given recall.\r\n\r\nHere's how precision is calculated:\r\n1. Calculate recall at all the thresholds (200 thresholds by default).\r\n2. Find the index of the threshold where the recall is closest to the requested value.\r\n3. Compute precision at that index.\r\n\r\nFor more info you can refer to the [source code](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/metrics.py#L1626).\r\n\r\nThreshold : A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is `true`, below is `false`). One metric value is generated for each threshold value.\r\n\r\nIn setting Recall value in this case `tf.keras.metrics.PrecisionAtRecall` will consider recall value over all the classes not a specific class i.e., (True Positive over all the classes/Actual Positives of all the classes).\r\n\r\nYou need to write your own function if you want to calculate recall for a specific class or use binary classification where you have 2 class - the class you are interested in setting the recall value and rest of the classes binned as a single class.", "@FrancescaAlf Did the above comment help you in resolving your problem?", "yes, thanks :) ", "In case anyone else stumbles upon this, I adapted the existing metrics to work in a multiclass setting using a subclass. Essentially, I just transform my `y_true` and `y_pred` into the binary equivalent before passing them.\r\n\r\n```python\r\nclass RecallAtPrecisionByClass(tf.keras.metrics.RecallAtPrecision):\r\n    def __init__(self, class_id, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.class_id = class_id\r\n\r\n    def update_state(self, y_true, y_pred, sample_weight=None):\r\n        y_true = y_true[:, self.class_id]\r\n        y_pred = tf.where(\r\n            tf.equal(tf.argmax(y_pred, axis=-1), self.class_id),\r\n            y_pred[:, self.class_id],\r\n            tf.zeros_like(y_pred[:, self.class_id]),\r\n        )\r\n        return super().update_state(\r\n            y_true=y_true, y_pred=y_pred, sample_weight=None\r\n        )\r\n```\r\n\r\nHere's a quick example of this metric on some dummy data.\r\n\r\n```python\r\ny_true = np.array([\r\n    [1, 0],\r\n    [0, 1],\r\n    [0, 1],\r\n    [0, 1]\r\n])\r\ny_pred = np.array([\r\n    [0.7, 0.3],\r\n    [0.7, 0.3],\r\n    [0.7, 0.3],\r\n    [0.7, 0.3]\r\n])\r\n\r\n# Precision is only 25% but our minimum is 33%.\r\nmetric = RecallAtPrecisionByClass(class_id=0, precision=0.3333)\r\nmetric.update_state(y_true=y_true, y_pred=y_pred)\r\nmetric.result().numpy() # 0\r\n\r\n# Here we exclude the final prediction so that the precision is 33%,\r\n# which meets our minimum.\r\nmetric = RecallAtPrecisionByClass(class_id=0, precision=0.3333)\r\nmetric.update_state(y_true=y_true[:-1], y_pred=y_pred[:-1])\r\nmetric.result().numpy() # 1.0\r\n```"]}, {"number": 37255, "title": "Fixed inline code markdown for  in docstring", "body": "Very minor fix from 'antialias' to `antialias` in docstring for proper inline code markdown notation. ", "comments": ["As this is a very small fix, IMHO it should not queued for long in order to avoid burdening the CI."]}, {"number": 37254, "title": "InvalidArgumentError:  indices[0,4] = 10 is not in [0, 10) \t [[node model/embedding_2/embedding_lookup (defined at <ipython-input-54-57c88bb5c904>:38) ]] [Op:__inference_distributed_function_20003]  Errors may have originated from an input operation. Input Source operations connected to node model/embedding_2/embedding_lookup:  model/embedding_2/embedding_lookup/19444 (defined at c:\\users\\naik9\\appdata\\local\\programs\\python\\python37\\lib\\contextlib.py:112)  Function call stack: distributed_function", "body": "Train on 10240 samples, validate on 1284 samples\r\nEpoch 1/30\r\n   40/10240 [..............................] - ETA: 4:26WARNING:tensorflow:Can save best model only with val_categorical_accuracy available, skipping.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-55-6d913c48a8e6> in <module>\r\n----> 1 train(model_cnn,'cnn', use_pos, use_meta, use_dep)\r\n\r\n<ipython-input-54-57c88bb5c904> in train(model, name, use_pos, use_meta, use_dep)\r\n     36             {'main_input': X_val, 'aux_input': X_val_meta, 'dep_input': X_val_dep},\r\n     37             {'main_output': Y_val}\r\n---> 38         ), callbacks=[tb,csv_logger,checkpoint])\r\n     39     else:\r\n     40       model.fit(\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\nc:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError:  indices[0,4] = 10 is not in [0, 10)\r\n\t [[node model/embedding_2/embedding_lookup (defined at <ipython-input-54-57c88bb5c904>:38) ]] [Op:__inference_distributed_function_20003]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model/embedding_2/embedding_lookup:\r\n model/embedding_2/embedding_lookup/19444 (defined at c:\\users\\n\\appdata\\local\\programs\\python\\python37\\lib\\contextlib.py:112)\r\n\r\nFunction call stack:\r\ndistributed_function", "comments": ["https://github.com/Hackerdash/nlp-news/blob/master/project.ipynb\r\n\r\nsource code", "@Hackerdash \r\nplease refer to this [link](https://github.com/tensorflow/tensorflow/issues/33163), let us know if it helps.\r\n\r\nplease provide with simple standalone code for us to replicate the issue in our environment with correct indentation and all dependencies along with the tensorflow version to replicate in.Thanks!", "same issue", "Could you please update us on the above comment on standalone code to help us resolve your issue", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37254\">No</a>\n"]}, {"number": 37253, "title": "tf2.1 with gpu can't open cuda 10.2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10 (profession)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de410\r\n- Python version:  3.68\r\n- Installed using virtualenv? pip? conda?:  pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.2 cuda / 7.6 cudnn\r\n- GPU model and memory:  2G\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nERROR: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] cannot opened dynamic library cudart64_101.dll\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nMy computer has windows 10 professional vesion os, and GPU-GTX1050. Then Install the cuda 10.2 and cudnn 7.6 version.  I use Pycharm professional version create a virtual env. Wait a moment, use pip install the tensorflow-2.1-gpu version, successfully. When I test the tensorflow2.1-gpu version, it arises the error \"cannot opened dynamic library cudart64_101.dll\".\r\n\r\nHow do I resolve? \r\nMaybe It is something wrong with cuda10.2, therefore ,I search the dir where i install the cuda. I find the directory-bin which has cudart64_102.dll , not cudart64_101.dll. For safely, i copy cudart64_102.dll, and rename , push into the bin directory. \r\n\r\n\r\nThen i rerun the tf, it can work...\r\n2020-03-03 16:38:19.699659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n\r\n\r\n**Any other info / logs**\r\n\r\nMaybe it is something wrong with cuda10.2 and tensorflow-2.1-gpu.\r\nTensorflow can't find the cudart64_101.dll. The cuda10.2 has not cudart64_101.dll, but has cudart64_102.dll. Ok, it should be the api.\r\n", "comments": ["@jacketchan, Tensorflow-gpu==2.1 supports `CUDA 10.1` which will have `cudart64_101.dll` file. \r\nSince, `CUDA 10.2` will have `cudart64_102.dll` file which is not supported by `Tensorflow 2.1`. \r\nPlease downgrade the CUDA version to 10.1. Let us know how it progresses. Thanks!", "You can try [this](https://github.com/fo40225/tensorflow-windows-wheel)", "@jacketchan, Issue still persists?", "Since the title of the issue is resolved.Closing this issue. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37253\">No</a>\n"]}, {"number": 37251, "title": "tensorflow:AutoGraph could not transform ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):    windows 10 (profession)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device:  No\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  v2.1.0-rc2-17-ge5bf8de410\r\n- Python version: - Bazel\r\nversion (if compiling from source):  3.68\r\n- GCC/Compiler version (if compiling from\r\nsource):  \r\n- CUDA/cuDNN version:  - GPU model and memory:  10.2cuda/ 7.6cudnn \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is some code:\r\n \r\nclass MultiHeadAttention(keras.layers.Layer):\r\n    def __init__(self, d_model, num_heads):\r\n        super(MultiHeadAttention, self).__init__()\r\n        self.num_heads = num_heads\r\n        self.d_model = d_model\r\n        assert self.d_model%self.num_heads == 0\r\n        self.depth = self.d_model // self.num_heads\r\n        self.WQ = keras.layers.Dense(self.d_model)\r\n        self.WK = keras.layers.Dense(self.d_model)\r\n        self.WV = keras.layers.Dense(self.d_model)\r\n        self.dense = keras.layers.Dense(self.d_model)\r\n    \r\n    def call(self,q,k,v,mask):\r\n        batch_size = tf.shape(q)[0]\r\n.......\r\n\r\n\r\n**Other info / logs** \r\nThe main warning: Maybe it is a bug...\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AFC2668>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96E9D978>, <gast.gast.Return object at 0x000001DD96E9D9E8>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AFC2668>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96E9D978>, <gast.gast.Return object at 0x000001DD96E9D9E8>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2CCBD5C0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96F0FCF8>, <gast.gast.Return object at 0x000001DD96F0FD68>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2CCBD5C0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96F0FCF8>, <gast.gast.Return object at 0x000001DD96F0FD68>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BFDB2E8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96F7E0F0>, <gast.gast.Return object at 0x000001DD96F7E160>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BFDB2E8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96F7E0F0>, <gast.gast.Return object at 0x000001DD96F7E160>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AA7A208>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96FEB470>, <gast.gast.Return object at 0x000001DD96FEB4E0>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AA7A208>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD96FEB470>, <gast.gast.Return object at 0x000001DD96FEB4E0>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1A34A8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD970746A0>, <gast.gast.Return object at 0x000001DD97074710>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1A34A8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD970746A0>, <gast.gast.Return object at 0x000001DD97074710>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BA9EBE0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD970C3B38>, <gast.gast.Return object at 0x000001DD970C3BA8>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BA9EBE0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD970C3B38>, <gast.gast.Return object at 0x000001DD970C3BA8>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2CD6BC18>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B074EF0>, <gast.gast.Return object at 0x000001DD9B074F60>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2CD6BC18>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B074EF0>, <gast.gast.Return object at 0x000001DD9B074F60>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1E4CF8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B0C0390>, <gast.gast.Return object at 0x000001DD9B0C0400>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1E4CF8>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B0C0390>, <gast.gast.Return object at 0x000001DD9B0C0400>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AC4AB70>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B130710>, <gast.gast.Return object at 0x000001DD9B130780>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AC4AB70>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B130710>, <gast.gast.Return object at 0x000001DD9B130780>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1959B0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B137BA8>, <gast.gast.Return object at 0x000001DD9B137C18>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2B1959B0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B137BA8>, <gast.gast.Return object at 0x000001DD9B137C18>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AF24080>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B1EFF28>, <gast.gast.Return object at 0x000001DD9B1EFF98>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2AF24080>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B1EFF28>, <gast.gast.Return object at 0x000001DD9B1EFF98>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BF92048>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B23A400>, <gast.gast.Return object at 0x000001DD9B23A470>]\r\nWARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x000001DC2BF92048>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x000001DD9B23A400>, <gast.gast.Return object at 0x000001DD9B23A470>]", "comments": ["@jacketchan \r\ncould you please check the gast version. Tensorflow 2.x supports gast ==0.2.2. \r\nlet us know if that helps,Thanks!", "@jacketchan \r\nCould you please update on the above comment.", "@jacketchan\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37251\">No</a>\n"]}, {"number": 37250, "title": "Saving model with tf.keras.layers.RNN and stateful=True with save_format='tf' fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device: -\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.2.0-dev20200228\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nSaving a `tf.keras.Sequential` model with `tf.keras.layers.RNN` and `stateful=True` with `save_format=tf` fails.\r\n\r\n**Describe the expected behavior**\r\nSaving should succeed.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nnumber_of_cells = 2\r\n\r\nmodel = tf.keras.Sequential()\r\n\r\nmodel.add(tf.keras.layers.Input(batch_input_shape=(1, 1, 1)))\r\n\r\ncells = []\r\n\r\nfor _ in range(number_of_cells):\r\n    cells.append(tf.keras.layers.GRUCell(10))\r\n\r\nmodel.add(tf.keras.layers.RNN(cells, stateful=True))\r\n\r\nmodel.compile()\r\n\r\nmodel.save('rnn.tf', save_format='tf')\r\n\r\nmodel2 = tf.keras.models.load_model('rnn.tf')\r\n\r\n```\r\n\r\n**Other info / logs**\r\nSaving succeeds with `save_format='h5'`.\r\n\r\nTraceback in case of failure:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    model.save('rnn.tf', save_format='tf')\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1044, in save\r\n    signatures, options)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 138, in save_model\r\n    signatures, options)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 78, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 951, in save\r\n    obj, export_dir, signatures, options, meta_graph_def)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1027, in _build_meta_graph\r\n    options.namespace_whitelist)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 629, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 497, in _generate_signatures\r\n    function, mapped_inputs, resource_map)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 449, in _call_function_with_mapped_captures\r\n    resource_map)\r\n  File \"/home/test/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 372, in _map_captures_to_created_tensors\r\n    ).format(interior))\r\nAssertionError: Tried to export a function which references untracked object Tensor(\"2164:0\", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n```\r\n", "comments": ["I have tried on colab with TF Nightly version and was able to reproduce the issue. Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e38aae55dc2b68e5852c2524e2d2b02c/untitled700.ipynb). Thanks!", "@padoremu I was able to reproduce the issue with `tf-nightly==2.2.0-dev20200228`. However, it looks like this was resolved recently. With the recently `tf-nightly` (!pip install tf-nightly==2.2.0-dev20200303), I was not able to reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/31e9ae78106e740000deae010a8c4b30/37250.ipynb). Thanks!\r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if there is any related issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37250\">No</a>\n"]}, {"number": 37249, "title": "Fix typo: delete redundant quote in tf_executor dialect", "body": "I deleted the unnecessary quote for better highlighting. Otherwise, most text editors will regard the rest of the code as string.", "comments": []}, {"number": 37248, "title": "Fixed remote handles to be ready when they are poisoned", "body": "", "comments": []}, {"number": 37247, "title": "Does BatchMatMul is still not supported in TF1.15?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.14\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):TF1.15\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOGISTIC, MATRIX_DIAG, MEAN, MUL, PACK, POW, RESHAPE, SELECT, SHAPE, SLICE, SOFTMAX, SPLIT, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul.\r\nTraceback (most recent call last):\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/tianhongzxy/Documents/sakt_tf1.15/venv_sakt_tf/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EQUAL, EXPAND_DIMS, FILL, FLOOR_DIV, FULLY_CONNECTED, GATHER, LOGISTIC, MATRIX_DIAG, MEAN, MUL, PACK, POW, RESHAPE, SELECT, SHAPE, SLICE, SOFTMAX, SPLIT, SQUARED_DIFFERENCE, SQUEEZE, STRIDED_SLICE, SUB, TILE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BatchMatMul.\r\n\r\n```", "comments": ["@TianHongZXY \r\nCan you please share a standalone code to reproduce the issue. Did you try adding converter.allow_custom_ops = True to enable custom ops in the tf.lite? Thanks!", "@Saduf2019 If I set converter.allow_custom_ops = True, the model can be converted, but converted model still can't be used because I didn't implement the custom_op BatchMatMul, but I found the supported ops includes BatchMatMul in `tensorflow/tensorflow/lite/delegates/flex/whitelisted_flex_ops.cc`", "@Saduf2019  Here's the code you can reproduce the issue, if you set `converter.allow_custom_ops = True`, the tflite file can be converted successfully. When you `run test_infer_model` you may encounter `reshape op error`, you can solve this by switch to `tf-nightly` version. But the unimplemented op BatchMatMul is the problem I still can't solve yet.", "https://colab.research.google.com/drive/1shJbg5iK7i6-F_eAJnexN9_Aj8PO9tAL", "@TianHongZXY \r\nplease refer to this [issue](https://github.com/tensorflow/tensorflow/issues/35449#issuecomment-570318723) and let us know if it helps", "@Saduf2019 I'm using the TF1.15 and my model can't be run with tf-nightly, so I found it not helpful and `converter.experimental_new_converter = True` doesn't work too.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37247\">No</a>\n"]}]