[{"number": 41588, "title": "need to add SparseEmbedding layer support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHi, can we add a SparseEmbedding layer into tf.keras.layers or make tf.keras.layers.Embedding support sparse embedding? \r\nThe main idea is that in many cases like recommendation or CTR prediction models, there are a huge number of items to be embeded, but in each batch we only use a very small portion of it. Therefore, the SparseEmbedding layer should bring just the parameters that are actually used in this batch into the model parameters to be optimized. This is will provide a huge performance improvement, especially in Multiworker Distributed training as the network cost will be significantly decreased.\r\nBTW, pytorch already have something similar like torch.nn.Embedding(sparse=True) https://pytorch.org/docs/master/generated/torch.nn.Embedding.html\r\n**Will this change the current api? How?**\r\nsmall refactor only. By adding a new Class or adding a construction parameter\r\n**Who will benefit with this feature?**\r\nin many cases like recommendation or CTR prediction models, there are a huge number of items to be embeded, but in each batch we only use a very small portion of it.\r\n**Any Other info.**\r\n", "comments": ["Hi, may I know if there are any thoughts regarding this?", "My bad.. it looks like tf.gather already provides sparse gradient, so no bother needed.", "So it's not a bad idea to have Embedding layer to support sparse inputs, I can help on this if you need. Though you're right if you use simple gather ops it will do the same thing"]}, {"number": 41587, "title": "pyspark + keras as pandas_udf does not work properly", "body": "In my scenario I wish to train a keras model per each partition on a Spark cluster with Python enabled.\r\nFor this I am using Spark PandasUDFs.\r\nInside the PandasUDF code I have the following command:\r\n\r\ncurr_input = keras.Input((1,))\r\n\r\nThis already causes the error below to trigger.\r\nI do not understand why this happens as I use PandasUDFs a lot and I have never seen this error when just creating some small object like this on a worker node.\r\nAny ideas would be highly appreciated.\r\n\r\nThanks,\r\nRoy.\r\n\r\n\r\n_answer = 'xro123'\r\ngateway_client = <py4j.java_gateway.GatewayClient object at 0x0000027AB538F988>\r\ntarget_id = 'o122', name = 'count'\r\n\r\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\r\n        \"\"\"Converts an answer received from the Java gateway into a Python object.\r\n    \r\n        For example, string representation of integers are converted to Python\r\n        integer, string representation of objects are converted to JavaObject\r\n        instances, etc.\r\n    \r\n        :param answer: the string returned by the Java gateway\r\n        :param gateway_client: the gateway client used to communicate with the Java\r\n            Gateway. Only necessary if the answer is a reference (e.g., object,\r\n            list, map)\r\n        :param target_id: the name of the object from which the answer comes from\r\n            (e.g., *object1* in `object1.hello()`). Optional.\r\n        :param name: the name of the member from which the answer comes from\r\n            (e.g., *hello* in `object1.hello()`). Optional.\r\n        \"\"\"\r\n        if is_error(answer)[0]:\r\n            if len(answer) > 1:\r\n                type = answer[1]\r\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n                if answer[1] == REFERENCE_TYPE:\r\n                    raise Py4JJavaError(\r\n                        \"An error occurred while calling {0}{1}{2}.\\n\".\r\n>                       format(target_id, \".\", name), value)\r\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling o122.count.\r\nE                   : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.net.SocketException: Connection reset\r\nE                   \tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\nE                   \tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\nE                   \tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\nE                   \tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\nE                   \tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\nE                   \tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:159)\r\nE                   \tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\nE                   \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\nE                   \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_1$(Unknown Source)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\nE                   \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\nE                   \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\nE                   \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\nE                   \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\nE                   \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\nE                   \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nE                   \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nE                   \tat java.lang.Thread.run(Thread.java:748)\r\nE                   \r\nE                   Driver stacktrace:\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\nE                   \tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\nE                   \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\nE                   \tat scala.Option.foreach(Option.scala:257)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\nE                   \tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\nE                   \tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nE                   \tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\nE                   \tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\nE                   \tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\nE                   \tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\nE                   \tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\nE                   \tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\nE                   \tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\r\nE                   \tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\r\nE                   \tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\r\nE                   \tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\nE                   \tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\nE                   \tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\nE                   \tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\nE                   \tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nE                   \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nE                   \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nE                   \tat java.lang.reflect.Method.invoke(Method.java:498)\r\nE                   \tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nE                   \tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nE                   \tat py4j.Gateway.invoke(Gateway.java:282)\r\nE                   \tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nE                   \tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nE                   \tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\nE                   \tat java.lang.Thread.run(Thread.java:748)\r\nE                   Caused by: java.net.SocketException: Connection reset\r\nE                   \tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\nE                   \tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\nE                   \tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\nE                   \tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\nE                   \tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\nE                   \tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:159)\r\nE                   \tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\nE                   \tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\nE                   \tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\nE                   \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\r\nE                   \tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_1$(Unknown Source)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\r\nE                   \tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\nE                   \tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\nE                   \tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\nE                   \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\nE                   \tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\nE                   \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\nE                   \tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\nE                   \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\nE                   \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\nE                   \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\nE                   \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\nE                   \tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\nE                   \tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\nE                   \tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\nE                   \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\nE                   \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nE                   \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nE                   \t... 1 more\r\n\r\n..\\..\\.env\\lib\\site-packages\\py4j\\protocol.py:328: Py4JJavaError_\r\n", "comments": ["@rolevin This issue is related to pyspark not related to Tensorflow or keras in anyway. Please go through a similar discussion [here](https://stackoverflow.com/questions/51952535/pyspark-error-py4jjavaerror-an-error-occurred-while-calling-o655-count-when). \r\n\r\nAlso, please post this issue in stack overflow or pyspark related communities where you should get some help. Thanks!", "I suspect this may have to do with code in Keras that assumes keras objects are created in the main thread, which is not the case when running in spark udfs. If this is the case (or something along these lines) the solution must come from keras/tensorflow and cannot come from spark.", "Could you please confirm if the reported issue is still exists. \r\nIf you still face the issue please open this issue on keras-team/keras repo. \r\n \r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41586, "title": "Support TF logs in TerminateOnNaN callback", "body": "This PR adds support for TF logs in the `TerminateOnNaN` callback. This callback is the only one that implements batch hooks and doesn't yet support TF logs by default. With this change only the `loss` needs to be converted to numpy instead of the entire `logs` dictionary.\r\n\r\nThis change is covered by the existing unittests so I didn't adapt them.", "comments": ["@omalleyt12 thanks for the review. Could you also take a look at #41742?"]}, {"number": 41585, "title": "When use MirroredStrategy for multi-gpu and fit with multi-workers there is a error --\"task_done() called too many times\"", "body": "**System information**\r\n\r\nOS: Ubuntu 18.04\r\nTensorflow 2.20 from pip install\r\nPython version: 3.7.7\r\nCUDA Version: 10.2\r\ncuDNN Version: release 10.2, V10.2.89\r\nGPU : 2070 x 2\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\ncode:\r\n```python\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor g in gpus:\r\n    tf.config.experimental.set_virtual_device_configuration(g, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=7000)])\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\r\n\r\ntrain_image_generator = ImageDataGenerator(rescale=1./255, horizontal_flip=True, zoom_range=0.1, rotation_range=45) # Generator for our training data\r\nvalidation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\r\n\r\ntrain_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\r\n                                                           directory=train_dir,\r\n                                                           shuffle=True,\r\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\r\n                                                           class_mode='binary')\r\n\r\nval_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\r\n                                                              directory=validation_dir,\r\n                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\r\n                                                              class_mode='binary')\r\n\r\nsample_training_images, _ = next(train_data_gen)\r\n\r\n\r\nwith mirrored_strategy.scope():\r\n    tinydarknet = keras.Sequential([\r\n        keras.layers.Conv2D(16, (3, 3), strides=[1, 1], padding=\"same\", input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)),\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.LeakyReLU(alpha=0.1),\r\n        keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\r\n        ..........\r\n        keras.layers.BatchNormalization(),\r\n        keras.layers.AveragePooling2D(),\r\n        keras.layers.Flatten(),\r\n        keras.layers.Dense(1)\r\n    ])\r\n\r\n\r\n    # In[8]:\r\n\r\n\r\n    tinydarknet.compile(optimizer=\"adam\",\r\n                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n                 metrics=[\"accuracy\"])\r\n\r\n\r\n# In[9]:\r\n\r\nhistory = tinydarknet.fit(#_generator(\r\n    train_data_gen,\r\n    steps_per_epoch=total_train // batch_size,\r\n    epochs=epochs,\r\n    validation_data=val_data_gen,\r\n    validation_steps=total_val // batch_size,\r\n    workers=NUM_WORKERS\r\n)\r\n\r\nacc = history.history['accuracy']\r\nval_acc = history.history['val_accuracy']\r\n\r\nloss=history.history['loss']\r\nval_loss=history.history['val_loss']\r\n\r\nepochs_range = range(epochs)\r\n\r\ntinydarknet.save(\"keras_model\")\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen I set workers=1 in fit(), it is normal work, but when I workers more than one , it is got a error:\r\n\r\ntensorflow/core/framework/op_kernel.cc:1741] Invalid argument: ValueError: task_done() called too many times\r\nTraceback (most recent call last):\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 785, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 801, in wrapped_generator\r\n    for data in generator_fn():\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 880, in get\r\n    six.reraise(*sys.exc_info())\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 875, in get\r\n    self.queue.task_done()\r\n\r\n  File \"/data_ssd/anaconda3/envs/tf2/lib/python3.7/queue.py\", line 74, in task_done\r\n    raise ValueError('task_done() called too many times')\r\n\r\nValueError: task_done() called too many times\r\n\r\n\r\n", "comments": ["@smallworld-network-wupeng \r\nI ran the code shared and its unindented, please share simple stand alone indented code such that we can replicate the error faced or if possible please share a colab gist with the error.", "> @smallworld-network-wupeng\r\n> I ran the code shared and its unindented, please share simple stand alone indented code such that we can replicate the error faced or if possible please share a colab gist with the error.\r\n\r\nfixed", "@smallworld-network-wupeng \r\nKindly provide all [details to replicate the issue](https://colab.research.google.com/gist/Saduf2019/00626c9969693c68618cec46623dd509/untitled291.ipynb) or share a colab gist with the error faced.", "> @smallworld-network-wupeng\r\n> Kindly provide all [details to replicate the issue](https://colab.research.google.com/gist/Saduf2019/00626c9969693c68618cec46623dd509/untitled291.ipynb) or share a colab gist with the error faced.\r\n\r\nI put all code to this colab. https://colab.research.google.com/drive/18zlBNCw-hNRiW2An2dC_zHzK8uyhJz9l  But how to upload the train images ?", "@smallworld-network-wupeng \r\nI do not have access to the link shared, please attach the images in a  zipped folder. ", "I have the same error with my code, I can't share it but could try to replicate it with the code and images of @smallworld-network-wupeng once fully provided.\r\n\r\n```\r\nOS: Ubuntu 20.04\r\nTensorflow 2.2.0 from pip install\r\nPython version: 3.8.2\r\nCUDA Version: 10.1, V10.1.243 (nvcc --version)\r\nGPU: Titan RTX x 2\r\n```\r\n\r\n#39933 might fix the issue, is it included in the current release candidates for 2.3.0?", "\r\n\r\n> I have the same error with my code, I can't share it but could try to replicate it with the code and images of @smallworld-network-wupeng once fully provided.\r\n> \r\n> ```\r\n> OS: Ubuntu 20.04\r\n> Tensorflow 2.2.0 from pip install\r\n> Python version: 3.8.2\r\n> CUDA Version: 10.1, V10.1.243 (nvcc --version)\r\n> GPU: Titan RTX x 2\r\n> ```\r\n> \r\n> #39933 might fix the issue, is it included in the current release candidates for 2.3.0?\r\n\r\nYes , it is same bug , I fixed it.\r\n\r\nBut I don't understand it slow than one GPU\r\n\r\n**2GPU:**\r\n37/37 [==============================] - 36s 969ms/step - accuracy: 0.6183 - loss: 1.2452 - val_accuracy: 0.4935 - val_loss: 0.6968\r\n\r\n**1GPU:**\r\n37/37 [==============================] - 29s 794ms/step - loss: 1.1153 - accuracy: 0.6241 - val_loss: 0.6943 - val_accuracy: 0.4518", "Hi @smallworld-network-wupeng, glad to hear you were able to fix the issue and get your model training. \r\nThis slowdown can happen if your program is input bound and your GPUs aren't being used as efficiently as possible (there are other possibilities too, but this one is common). I would suggest taking a look at this tutorial for the [Tensorboard Profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) which can help to discover what inefficiencies you might have. If you can share fully reproducible code, I'd be happy to help diagnose further.\r\n\r\nAlso since @quassy had the same original issue, do you might explaining how you fixed it? Just in case someone else runs into the same problem in the future.", "@smallworld-network-wupeng Could you share what you have changed?\r\n\r\nI personally tested my code with tensorflow==2.3.0rc2 just now and it seems to run without this error now. Will report back if the error comes up again.", "> @smallworld-network-wupeng Could you share what you have changed?\r\n> \r\n> I personally tested my code with tensorflow==2.3.0rc2 just now and it seems to run without this error now. Will report back if the error comes up again.\r\n\r\nyes, it is looks fixed in 2.3.0rc2 . You can fixed it as [#39933 ](https://github.com/tensorflow/tensorflow/pull/39933) if you use 2.2.0", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41585\">No</a>\n"]}, {"number": 41584, "title": "Build did not complete successfully.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r1.15\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.5\r\n- GPU model and memory: Tesla P40 , 22919MiB (nvidia-smi)\r\n\r\n\r\n\r\n**Describe the problem**\r\nI tried to build from source for the branch r1.15 but continuously failed to build from source.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`bazel build --verbose_failures --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` and\r\n\r\n`bazel build --verbose_failures --config=opt --config=cuda --config=monolithic //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nI got an error message like below by setting the `--verbose_failures` flags.\r\n```bash\r\nERROR: /root/tensorflow/tensorflow/python/BUILD:329:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-0.26.0-linux-x86_64/bin:/root/bin:/usr/local/cuda-10.0/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/root/go/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL '-DGRPC_ARES=0' '-DPB_FIELD_32BIT=1' -DSQLITE_OMIT_DEPRECATED -iquote . -iquote bazel-out/host/bin -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/hwloc -iquote bazel-out/host/bin/external/hwloc -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm -iquote bazel-out/host/bin/external/llvm -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/grpc -iquote bazel-out/host/bin/external/grpc -iquote external/com_github_nanopb_nanopb -iquote bazel-out/host/bin/external/com_github_nanopb_nanopb -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/png_archive -iquote bazel-out/host/bin/external/png_archive -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive -isystem bazel-out/host/bin/external/gif_archive -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm/include -isystem bazel-out/host/bin/external/llvm/include -isystem external/llvm/lib/IR -isystem bazel-out/host/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm/include/llvm/IR -isystem external/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm/lib/Target/X86 -isystem external/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm/lib/Transforms/InstCombine -isystem external/llvm/lib/Target/AArch64 -isystem bazel-out/host/bin/external/llvm/lib/Target/AArch64 -isystem external/llvm/lib/Target/ARM -isystem bazel-out/host/bin/external/llvm/lib/Target/ARM -isystem external/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm/lib/Target/AMDGPU -isystem external/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm/lib/Target/NVPTX -isystem external/grpc/include -isystem bazel-out/host/bin/external/grpc/include -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/grpc/third_party/address_sorting/include -isystem external/png_archive -isystem bazel-out/host/bin/external/png_archive -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n\r\n\r\n\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:638:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:641:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:645:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:649:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:653:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:608:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 5310.456s, Critical Path: 1039.95s\r\nINFO: 19074 processes: 19074 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["I found similar issue (#41086) and trying suggested workaround (https://github.com/tensorflow/tensorflow/issues/41086#issuecomment-656833081).\r\nI will let you know when I fail again.", "The workaround not worked, (add const* for input arguments)\r\nbut following another suggestion (https://github.com/tensorflow/tensorflow/issues/41061#issuecomment-662222308)\r\n(downgrade `numpy<1.19.0`) works! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41584\">No</a>\n", "Update, according to this [issue](https://github.com/tensorflow/tensorflow/issues/41061#issuecomment-662222308), I workaround the error in tf-1.15.2 by modifying the code in _tensorflow/python/lib/core/bfloat16.cc_ from\r\n```\r\ntemplate <typename InType, typename OutType, typename Functor>\r\nvoid BinaryUFunc(char** args, npy_intp* dimensions, npy_intp* steps,\r\n                 void* data){...}\r\ntemplate <typename Functor>\r\nvoid CompareUFunc(char** args, npy_intp* dimensions, npy_intp* steps,\r\n                  void* data){...}\r\n```\r\nto \r\n```\r\ntemplate <typename InType, typename OutType, typename Functor>\r\nvoid BinaryUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n                 void* data){...}\r\ntemplate <typename Functor>\r\nvoid CompareUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n                  void* data){...}\r\n```", "@ZanZong Thanks a lot. I suffer from this issue even I changed the NumPy version but solved this issue with your comments.\r\nThis method can also work at tf-1.13.1"]}, {"number": 41583, "title": "Setting 'name' parameter in keras.layers results in accuracy degradation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP VM (Debian GNU/Linux 9 Stretch + TF 1-15-3)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): preinstalled\r\n- TensorFlow version (use command below): 1.15.3 (v1.15.2-30-g4386a66)\r\n- Python version: 3.5.3\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version:  -\r\n- GPU model and memory: TPUv2-8\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I set name parameter in keras.layers.Conv2D, the training behavior changes (accuracy goes down consistently)\r\n\r\n**Describe the expected behavior**\r\nThe training behavior should be similar : accuracy have to be similar.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n0. Clone official mnasnet code\r\n```\r\ngit clone https://github.com/tensorflow/tpu.git\r\ncd tpu/models/official/mnasnet\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`/../../:`pwd`/../efficientnet\r\n```\r\n\r\n1. train mnasnet for 50 epochs\r\n```\r\npython mnasnet_main.py --data_dir=IMAGNET_DIR --tpu=TPU_NAME --train_steps=62550 --steps_per_eval=6255 --train_batch_size=1024 --eval_batch_size=1024 --model_dir=SAVE_DIR --model_name=mnasnet-a1\r\n```\r\n\r\n2. add name to keras.layers (can be added using diff below)\r\n```\r\ndiff --git models/official/mnasnet/mnasnet_model.py models/official/mnasnet/mnasnet_model.py\r\nindex 935421e..7909ec2 100644\r\n--- models/official/mnasnet/mnasnet_model.py\r\n+++ models/official/mnasnet/mnasnet_model.py\r\n@@ -117,7 +117,8 @@ def _get_conv2d(filters,\r\n                 padding,\r\n                 use_bias,\r\n                 data_format='channels_last',\r\n-                use_keras=True):\r\n+                use_keras=True,\r\n+                name=None):\r\n   \"\"\"A helper function to create Conv2D layer.\"\"\"\r\n   if use_keras:\r\n     return tf.keras.layers.Conv2D(\r\n@@ -127,7 +128,8 @@ def _get_conv2d(filters,\r\n         kernel_initializer=kernel_initializer,\r\n         padding=padding,\r\n         data_format=data_format,\r\n-        use_bias=use_bias)\r\n+        use_bias=use_bias,\r\n+        name=name)\r\n   else:\r\n     return tf.layers.Conv2D(\r\n         filters=filters,\r\n@@ -136,7 +138,8 @@ def _get_conv2d(filters,\r\n         kernel_initializer=kernel_initializer,\r\n         padding=padding,\r\n         data_format=data_format,\r\n-        use_bias=use_bias)\r\n+        use_bias=use_bias,\r\n+        name=name)\r\n \r\n \r\n class MnasBlock(object):\r\n@@ -190,13 +193,15 @@ class MnasBlock(object):\r\n           padding='same',\r\n           use_bias=False,\r\n           data_format=self._data_format,\r\n-          use_keras=self._use_keras)\r\n+          use_keras=self._use_keras,\r\n+          name='expand_conv')\r\n       # TODO(hongkuny): b/120622234 need to manage update ops directly.\r\n       self._bn0 = tf.layers.BatchNormalization(\r\n           axis=self._channel_axis,\r\n           momentum=self._batch_norm_momentum,\r\n           epsilon=self._batch_norm_epsilon,\r\n-          fused=True)\r\n+          fused=True,\r\n+          name='expand_bn')\r\n \r\n     kernel_size = self._block_args.kernel_size\r\n     # Depth-wise convolution phase:\r\n@@ -207,7 +212,8 @@ class MnasBlock(object):\r\n           depthwise_initializer=conv_kernel_initializer,\r\n           padding='same',\r\n           data_format=self._data_format,\r\n-          use_bias=False)\r\n+          use_bias=False,\r\n+          name='dw_conv')\r\n     else:\r\n       self._depthwise_conv = mnas_utils.DepthwiseConv2D(\r\n           [kernel_size, kernel_size],\r\n@@ -215,12 +221,14 @@ class MnasBlock(object):\r\n           depthwise_initializer=conv_kernel_initializer,\r\n           padding='same',\r\n           data_format=self._data_format,\r\n-          use_bias=False)\r\n+          use_bias=False,\r\n+          name='dw_conv')\r\n     self._bn1 = tf.layers.BatchNormalization(\r\n         axis=self._channel_axis,\r\n         momentum=self._batch_norm_momentum,\r\n         epsilon=self._batch_norm_epsilon,\r\n-        fused=True)\r\n+        fused=True,\r\n+        name='dw_bn')\r\n \r\n     if self.has_se:\r\n       num_reduced_filters = max(\r\n@@ -234,7 +242,8 @@ class MnasBlock(object):\r\n           padding='same',\r\n           use_bias=True,\r\n           data_format=self._data_format,\r\n-          use_keras=self._use_keras)\r\n+          use_keras=self._use_keras,\r\n+          name='se_reduce')\r\n       self._se_expand = _get_conv2d(\r\n           filters,\r\n           kernel_size=[1, 1],\r\n@@ -243,7 +252,8 @@ class MnasBlock(object):\r\n           padding='same',\r\n           use_bias=True,\r\n           data_format=self._data_format,\r\n-          use_keras=self._use_keras)\r\n+          use_keras=self._use_keras,\r\n+          name='se_expand')\r\n \r\n     # Output phase:\r\n     filters = self._block_args.output_filters\r\n@@ -255,12 +265,14 @@ class MnasBlock(object):\r\n         padding='same',\r\n         use_bias=False,\r\n         data_format=self._data_format,\r\n-        use_keras=self._use_keras)\r\n+        use_keras=self._use_keras,\r\n+        name='proj_conv')\r\n     self._bn2 = tf.layers.BatchNormalization(\r\n         axis=self._channel_axis,\r\n         momentum=self._batch_norm_momentum,\r\n         epsilon=self._batch_norm_epsilon,\r\n-        fused=True)\r\n+        fused=True,\r\n+        name='proj_bn')\r\n \r\n   def _call_se(self, input_tensor):\r\n     \"\"\"Call Squeeze and Excitation layer.\r\n\r\n```\r\n\r\n3. use same code to train again\r\n```\r\npython mnasnet_main.py --data_dir=IMAGNET_DIR --tpu=TPU_NAME --train_steps=62550 --steps_per_eval=6255 --train_batch_size=1024 --eval_batch_size=1024 --model_dir=DIFFERENT_SAVE_DIR --model_name=mnasnet-a1\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen I added name, the accuracy consistently degraded.\r\nI repeated 3 times.\r\n\r\n![image](https://user-images.githubusercontent.com/8455454/88022457-42729980-cb6a-11ea-9eee-65653ca9aae9.png)", "comments": ["@TPFRL,\r\nOn running the command to train mnasnet, I am facing an error stating `ModuleNotFoundError: No module named 'utils'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/219d8912e7e0ba5a9ffcf6ada19cfcf6/41583.ipynb#scrollTo=DzjJ80s4SgTM).\r\n\r\nCould you please provide a minimal working example to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41583\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41583\">No</a>\n"]}, {"number": 41582, "title": "tf.keras.datasets.imdb.load_data() where path is a local path", "body": "When I download imdb dataset and save it in my disk, say, the **loacl path** is 'D:/datasets/imdb.npz', when I pass the path into load_data() API, remote download will start rather than load data from local path.", "comments": ["@Hailprob \r\nPlease follow [this comment](https://github.com/tensorflow/tensorflow/issues/32278#issuecomment-531051609) and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41579, "title": "Enable eager test for gradient descent optimizer", "body": "Enable all eager tests for gradient_descent_test.py as float16 GPU scatter ops used for sparse update are available in #41265.", "comments": ["/cc @tanzhenyu in TODO list as tests pass now"]}, {"number": 41578, "title": "Tensorflow lite make error", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10 x64\r\n- TensorFlow installed from (source or binary):source(tensorflow-master) for github\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): keil IDE 5.30\r\n\r\n\r\n\r\n**Describe the problem**\r\nGit Bash 2.270 x64 was used\r\nerror of make the generate_projects=>error1\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n1.start git bash\r\n2. change the file directory to d;\\tensorflow-master\r\n3. make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n4. error1 ocurred, process stopped\r\n![tensorflow lite make error](https://user-images.githubusercontent.com/44084579/88012475-8eaee100-cb4c-11ea-8e9b-a0182d91d3a1.PNG)\r\n\r\n", "comments": ["@jiahueytsao \r\nCan we please have the error message within quotes for better readability, with the tensorflow version .", "> @jiahueytsao\r\n> Can we please have the error message within quotes for better readability, with the tensorflow version .\r\n\r\nDear Saduf,\r\nThe error message in text as following(by using the git bash for windows-x64):\r\n\"$ make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853\r\nd75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7\r\nf98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/flatbuffers/archive/v1.12.0.zip\" \"a1afdbf114dec01a861c1b8c917d0fc7\" tensorfl\r\now/lite/micro/tools/make/downloads/flatbuffers\r\ndownloading https://github.com/google/flatbuffers/archive/v1.12.0.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/ruy/archive/d492ac890d982d7a153a326922f362b10de8d2ad.zip\" \"3a5c19abc60c3d9a8\r\n045ddf6b114067f\" tensorflow/lite/micro/tools/make/downloads/ruy\r\ndownloading https://github.com/google/ruy/archive/d492ac890d982d7a153a326922f362\r\nb10de8d2ad.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_202\r\n0_05_27.zip\" \"1f4607b05ac45b8a6146fb883dbc2d7b\" tensorflow/lite/micro/tools/make\r\n/downloads/image_recognition_model\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/models/tflite\r\n/cifar_image_recognition_model_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://www.cs.toronto\r\n.edu/~kriz/cifar-10-binary.tar.gz\" \"c32a1d4ab5d03f1284b67883e8d87530\" tensorflow\r\n/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset\r\ndownloading https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/mbo\r\nrgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow\r\n/lite/micro/tools/make/downloads/kissfft patch_kissfft\r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_0\r\n5_27.zip\" \"55b85f76e2995153e660391d4a209ef1\" tensorflow/lite/micro/tools/make/do\r\nwnloads/person_model_grayscale\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_\r\nmicro_person_data_grayscale_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2\r\n020_06_23.zip\" \"9b5b6d4677dd0a91b1bb992d1c4c0417\" tensorflow/lite/micro/tools/ma\r\nke/downloads/person_model_int8\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_\r\nmicro_person_data_int8_grayscale_2020_06_23.zip\r\n/usr/bin/sh: -c: line 0: unexpected EOF while looking for matching `\"'\r\n/usr/bin/sh: -c: line 1: syntax error: unexpected end of file\r\nmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:34: tensorflo\r\nw/lite/micro/tools/make/gen/windows_x86_64/prj/hello_world_test/keil/keil_projec\r\nt.uvprojx] Error 1\r\n\"\r\nMeanwhile, the error picture was attached again for your reference.\r\nThe tensorflow version was the newest, just download from the github site of tensorflow on 2020/7/24\r\n![generate_projects make error](https://user-images.githubusercontent.com/44084579/88372873-57993380-cdc9-11ea-9de0-1d835426a74c.png)\r\n", "Dear Saduf,\r\nThe error message in text as following(by using the git bash for windows-x64):\r\n\"$ make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/ruy'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: overriding recipe for ta\r\nrget 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/Makefile:305: warning: ignoring old recipe for\r\ntarget 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853\r\nd75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp\r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7\r\nf98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/flatbuffers/archive/v1.12.0.zip\" \"a1afdbf114dec01a861c1b8c917d0fc7\" tensorfl\r\now/lite/micro/tools/make/downloads/flatbuffers\r\ndownloading https://github.com/google/flatbuffers/archive/v1.12.0.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/goo\r\ngle/ruy/archive/d492ac890d982d7a153a326922f362b10de8d2ad.zip\" \"3a5c19abc60c3d9a8\r\n045ddf6b114067f\" tensorflow/lite/micro/tools/make/downloads/ruy\r\ndownloading https://github.com/google/ruy/archive/d492ac890d982d7a153a326922f362\r\nb10de8d2ad.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/models/tflite/cifar_image_recognition_model_202\r\n0_05_27.zip\" \"1f4607b05ac45b8a6146fb883dbc2d7b\" tensorflow/lite/micro/tools/make\r\n/downloads/image_recognition_model\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/models/tflite\r\n/cifar_image_recognition_model_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://www.cs.toronto\r\n.edu/~kriz/cifar-10-binary.tar.gz\" \"c32a1d4ab5d03f1284b67883e8d87530\" tensorflow\r\n/lite/micro/tools/make/downloads/cifar10 patch_cifar10_dataset\r\ndownloading https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/mbo\r\nrgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow\r\n/lite/micro/tools/make/downloads/kissfft patch_kissfft\r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2020_0\r\n5_27.zip\" \"55b85f76e2995153e660391d4a209ef1\" tensorflow/lite/micro/tools/make/do\r\nwnloads/person_model_grayscale\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_\r\nmicro_person_data_grayscale_2020_05_27.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.google\r\napis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2\r\n020_06_23.zip\" \"9b5b6d4677dd0a91b1bb992d1c4c0417\" tensorflow/lite/micro/tools/ma\r\nke/downloads/person_model_int8\r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_\r\nmicro_person_data_int8_grayscale_2020_06_23.zip\r\n/usr/bin/sh: -c: line 0: unexpected EOF while looking for matching `\"'\r\n/usr/bin/sh: -c: line 1: syntax error: unexpected end of file\r\nmake: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:34: tensorflo\r\nw/lite/micro/tools/make/gen/windows_x86_64/prj/hello_world_test/keil/keil_projec\r\nt.uvprojx] Error 1\r\n\"\r\nMeanwhile, the error picture was attached again for your reference.\r\nThe tensorflow version was the newest, just download from the github site of tensorflow on 2020/7/24\r\ngenerate_projects make error\r\n![generate_projects make error](https://user-images.githubusercontent.com/44084579/88373059-b65ead00-cdc9-11ea-8e6e-67377c2d7a8d.png)\r\n", "@jiahueytsao You need to pass project names individually . See example usage below;\r\nhttps://github.com/tensorflow/tensorflow/blob/a4725be4a74c9acae623231e80d5a8002a54e94a/tensorflow/lite/micro/tools/make/Makefile#L10\r\nAlso see similar [thread](https://github.com/tensorflow/tensorflow/issues/41818)\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41578\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41578\">No</a>\n"]}, {"number": 41577, "title": "Adding an option to tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()", "body": "This PR adds a note on using the `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()` method and also briefly specify where it might be useful. An end-to-end workflow is demonstrated here in [this Colab Notebook](https://colab.research.google.com/github/sayakpaul/Adventures-in-TensorFlow-Lite/blob/master/DeepLabV3/DeepLab_TFLite_COCO.ipynb). \r\n\r\nCc: @khanhlvg ", "comments": []}, {"number": 41576, "title": "Merge pull request #1 from tensorflow/master", "body": "update to release 1.13", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41576) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 41575, "title": "Unable to generate .pbtxt file for Tensorflow 2.0 models.", "body": "**System information**\r\n\r\nRunning Code on Google Colab\r\nUsing Tensorflow 2.2\r\n\r\n**Problem**\r\n\r\nI'm trying to generate a .pbtxt file from a .pb file that I trained using **TFOD 2 api**  but I'm getting a parsing message error. In fact I'm even getting this error when I try to use a trained SSD mobilenet V2 model from tensorflow model Zoo.\r\n\r\n**This is the Code Used to get .pbtxt, you can just paste this code in google colab and the error will be reproduced**\r\n\r\n```\r\n\r\n!wget 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz'\r\n!tar -xf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\r\nmodel_path = 'ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model/saved_model.pb'\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile(model_path, \"rb\") as f:\r\n    graph_def = tf.compat.v1.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\n    for i in reversed(range(len(graph_def.node))):\r\n        if graph_def.node[i].op == 'Const':\r\n            del graph_def.node[i]\r\n\r\n    tf.io.write_graph(graph_def, \"\", 'graph.pbtxt', as_text=True)\r\n```\r\n\r\nFacing this Error while running the code:\r\n```\r\n\r\n----> graph_def.ParseFromString(f.read())\r\nDecodeError: Error parsing message\r\n```\r\n\r\n**How can I generate a .pbtxt file from Tensorflow Object detection api 2.x models. The end goal is to use these models inside OpenCV DNN module**\r\n", "comments": ["I have tried in colab with TF 2.2, 2.3-rc1,nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/958ba7472041d6d10f58229a2808539f/untitled155.ipynb).Thanks!", "Closing as a duplicate of #8923 in tensorflow/models/issues", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41575\">No</a>\n"]}, {"number": 41574, "title": "BroadcastTo", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["@byronlan,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the TensorFlow version you are using. Thanks!\r\n", "Hi,\r\nI am trying to convert a mobilenetv3 model implemented using tensorflow2.2 into a TFLite model using the tf-lite convertor using the following command for LPCV competition:\r\n\r\npython3 tensorflow/lite/python/tflite_convert.py --graph_def_file=~/Projects/save_models/mobilenetv4-1009/saved_model.pb --output_file=mobilenetv4-1009.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --input_shape=\"1,224,224,3\" --input_array=\"input_1\" --output_array=probs --mean_value=\"123.675,116.28,103.53\" --std_dev_value=\"58.395,57.12,57.375\" --saved_model_dir=/Users/z004njq/Projects/save_models/mobilenetv4-1009 --allow_custom_ops\r\n\r\nI am having the following issue:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/z004njq/Library/Python/3.7/lib/python/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, DIV, FULLY_CONNECTED, HARD_SWISH, MAXIMUM, MEAN, MINIMUM, MUL, PACK, RESHAPE, SHAPE, SOFTMAX, STRIDED_SLICE, SUB. Here is a list of operators for which you will need custom implementations: BroadcastTo.\r\n\r\nI tried using --enable_select_tf_ops, but it did not work. Could you please guide me how to solve this issue? It's little time-sensitive.\r\n\r\n", "@dipendra009,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "You will need a custom implementation for `BroadcastTo` or replace it with operators which exist in the TFLite dialect\r\n\r\n```\r\nHere is a list of operators for which you will need custom implementations: BroadcastTo.\r\n```\r\n\r\nSince this is not an error in the TF code, closing the issue."]}, {"number": 41573, "title": "TFDBG doesn't display any tensor ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0rc2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2020-07-21 10-49-27](https://user-images.githubusercontent.com/17592563/88007139-df6c0d00-cb3f-11ea-8309-749d1df03c43.png)\r\n\r\n**Describe the expected behavior**\r\nDisplay record tensors which like tf1.x\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\npython -m tensorflow.python.debug.examples.v2.debug_mnist_v2     --dump_dir /tmp/tfdbg2_logdir --dump_tensor_debug_mode FULL_HEALTH\r\n```\r\n```\r\npython -m tensorflow.python.debug.cli.offline_analyzer --dump_dir=\"/tmp/tfdbg2_logdir\"\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["`tensorflow.python.debug.cli.offline_analyzer` is not compatible with the debugger v2 API in TF 2.x (i.e., the `tf.debugging.experimental.enable_dump_debug_info()` as used in the `debug_mnist_v2` example). Instead, please use tensorboard (2.3+, to be released soon) to look at the dumped debug data. \r\n\r\nPlease take a look at https://www.tensorflow.org/tensorboard/debugger_v2 for more details.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41573\">No</a>\n", "> `tensorflow.python.debug.cli.offline_analyzer` is not compatible with the debugger v2 API in TF 2.x (i.e., the `tf.debugging.experimental.enable_dump_debug_info()` as used in the `debug_mnist_v2` example). Instead, please use tensorboard (2.3+, to be released soon) to look at the dumped debug data.\r\n> \r\n> Please take a look at https://www.tensorflow.org/tensorboard/debugger_v2 for more details.\r\n\r\nAny plan to fix cli?\r\nSometimes, I worried about Tensorboard has worse performance than cli.\r\nBecause Tensorboard uses resources to render web UI."]}, {"number": 41572, "title": "[TF 2.2.0] tf.io.decode_image caught CUDA_ERROR_NOT_INITIALIZED: initialization error when use_multiprocessing=True", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\n- Python version: 3.6.9\r\n- GCC/Compiler version (if compiling from source): 8.4.0\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: 6GB \r\n\r\nI try to decode a dataset in *.tfrecord type using ```tf.io.decode_image```\r\n```\r\ndef get_item(type, index):\r\n    if type == 'trainval':\r\n        item = images_trainval[index]\r\n    else:\r\n        item = images_val[index]\r\n\r\n    image = item['image/encoded']\r\n    image = tf.io.decode_image(image, 3)\r\n    image = image.numpy()\r\n\r\n    mask = item['image/segmentation/class/encoded']\r\n    mask = tf.io.decode_image(mask, 1)\r\n    mask = mask.numpy()\r\n    mask = mask.reshape(mask.shape[:2])\r\n\r\n    return image, mask\r\n\r\nclass DLDataset(Dataset):\r\n    def __init__(self, split, dataset_dir):\r\n        self.split = split\r\n        \r\n        self.transformer = data_transforms[split]\r\n\r\n    def __getitem__(self, i):\r\n        image, mask = get_item(self.split, i)\r\n\r\n        image, mask = safe_crop(image, mask, size=512)\r\n        image = transforms.ToPILImage()(image.copy().astype(np.uint8))\r\n        image = self.transformer(image)\r\n\r\n        mask = torch.from_numpy(mask)\r\n\r\n        return image, mask\r\n\r\n    def __len__(self):\r\n        return get_len(self.split)\r\n```\r\nAnd it caught error:\r\n```\r\n...\r\n2020-07-21 08:04:06.203528: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203533: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203532: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203538: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203537: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203547: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203559: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203561: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203556: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203564: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203568: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203572: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203571: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203568: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203578: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203577: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203585: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-07-21 08:04:06.203585: I tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n...\r\n```\r\nThe error seems to be infinities. I had to press Ctrl + C to stop it. \r\n\r\nBut, when I try a clearly function (```tf.io.decode_jpeg``` and ```tf.io.decode_png``` instead of ```tf.io.decode_image```)\r\n```\r\ndef get_item(type, index):\r\n    if type == 'trainval':\r\n        item = images_trainval[index]\r\n    else:\r\n        item = images_val[index]\r\n\r\n    image = item['image/encoded']\r\n    image = tf.io.decode_jpeg(image, 3)\r\n    image = image.numpy()\r\n\r\n    mask = item['image/segmentation/class/encoded']\r\n    mask = tf.io.decode_png(mask, 1)\r\n    mask = mask.numpy()\r\n    mask = mask.reshape(mask.shape[:2])\r\n\r\n    return image, mask\r\n```\r\nIt work perfectly. \r\n\r\nI noticed this error occurs when use ```workers > 1``` and ```use_multiprocessing = True``` in model fit. If i don't use multi workers, It will work.\r\n\r\n\r\n\r\n", "comments": ["@vietnamican \r\n\r\nLooks like code is incomplete.Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram. This is the simply version in colab:\r\nhttps://colab.research.google.com/drive/1tt3DmJ7CPQXklKFhtPOl7bF8mBVff1ov?usp=sharing\r\n\r\nIn notebook, It seems that the error is thread be locked rather than CUDA error ", "I have tried in colab with TF version 2.2,nightly version(`2.4.0-dev20200726`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/931d530a2a92272556b394654d7572cd/untitled184.ipynb).You are also seeing the same behavior?.Thanks!", "Look like ```tf.io.decode_image``` and ```torch.utils.data.Dataset``` is not compatible. I do not know where the error occurred, it maybe in tensorflow or it may be in pytorch. But now we can still handle it by using altenative function to decode image from TFRecord. \r\nAre you going to investigate it? ", "@vietnamican Were you able to find the cause of the error? Tried to reproduce this issue but was not able to", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41572\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41572\">No</a>\n"]}, {"number": 41571, "title": "keras.Model.fit cannot handle variable epoch sizes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\n`keras.Model.fit` with finite-length dataset inputs can run without `steps_per_epoch`. However, if the number of batches in the epoch changes between epochs, this can lead to:\r\n* truncation of epoch if subsequence epochs are longer than the initial epoch; and\r\n* truncation of training if subsequence epochsa re shorter than the initial epoch.\r\n\r\nWhile not a common occurance, this happens e.g. in graph neural network applications where it makes more sense to batch according to some maximum number of edges/nodes in a graph as opposed to a fixed batch size. A fixed number of graphs in a dataset can result in a variable number of batches when these are shuffled.\r\n\r\n**Describe the expected behavior**\r\nEach epoch of `fit` should iterate over a dataset in it's entirety when `steps_per_epoch` is not provided, and run to the defined number of epochs (unless terminated prior with callbacks), even if the number of steps varies between epochs. Alternatively, a special `steps_per_epoch` value (say, -1) should be able to be provided to specify a dynamic epoch size.\r\n**Standalone code to reproduce the issue**\r\n[Colab](https://colab.research.google.com/drive/1HWPtXa2N_EcSFjhcFDquxBrl8RlZvB9y?usp=sharing)\r\n\r\n**Other info / logs**\r\nFrom colab above, when subsequence epochs are shorter than the first:\r\n```bash\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\r\n```", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/81bba42e7be6d5f5b31523988f902207/41571-tf-nightly.ipynb). Thanks!", "@jackd Thanks for the issue! I'd recommend using a Dataset that contains every batch you want to train on, and setting steps_per_epoch as an arbitrary number, so that epoch boundaries are just a way to check on the progress of training in your case. If that doesn't work, then you can always write a custom training loop for your use case: https://keras.io/guides/writing_a_training_loop_from_scratch/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41571\">No</a>\n"]}, {"number": 41570, "title": "TFLite interpreter.invoke() crashes on GPU despite successful TFLite conversion", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (Google Colab notebook)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.0-dev20200720\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1/7 (Google Colab default GPU)\r\n- GPU model and memory: Google Colab default GPU\r\n\r\n**Describe the current behavior**\r\nI converted a simple 3DCNN keras model to TFLite. Upon creating an interpreter from that TFLite model and calling `interpreter.invoke()`, the Google Colab notebook crashes. This only happens when using a GPU runtime; `interpreter.invoke()` works fine on a CPU. The converted TFLite model uses both `tf.lite.OpsSet.TFLITE_BUILTINS` and `tf.lite.OpsSet.SELECT_TF_OPS`, and I was converting to TFLite in an attempt to apply quantization-aware training and post-training quantization according to [this guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide). \r\n\r\n**Describe the expected behavior**\r\nI expect to be able to call `interpreter.invoke()` successfully on a GPU without any crashing.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n1. Add [this Google Colab notebook](https://colab.research.google.com/drive/19gXOPePKytN-6JptdBqljHfEsLXQHaMj?usp=sharing) and [this dataset](https://drive.google.com/file/d/1F1qxN1HrBbMkkYEQH3Vpa7-LnPF63bEK/view?usp=sharing) to your Google Drive.\r\n2. Open the colab notebook. Set the runtime to GPU (Runtime -> Change Runtime Type -> GPU). \r\n3. Mount your drive and change \"/content/drive/My Drive/full_dataset_vectors.h5\" to wherever you're storing the dataset.\r\n4. Run all cells of the notebook. The actual crash only happens in the last cell, with `interpreter.invoke()`.\r\n\r\n**Other info / logs** \r\n\r\nThe logs in the Google Colab notebook didn't provide any information about the cause for the crash. To get more info, I tried running the code on an identical local environment and managed to obtain a gdb backtrace. It seems likely to be related to [this issue](https://github.com/tensorflow/tensorflow/issues/35987), but the backtrace looks different enough to possibly be a separate bug, so I decided to create a new issue.\r\n\r\n```\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n__memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370\r\n370    ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.\r\n(gdb) bt\r\n#0  __memmove_sse2_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:370\r\n#1  0x00007ff291abd70c in tflite::FlexDelegate::CopyFromBufferHandle(TfLiteContext*, int, TfLiteTensor*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007ff280226f1d in tflite::impl::Subgraph::Invoke() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#3  0x00007ff280229bd0 in tflite::impl::Interpreter::Invoke() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#4  0x00007ff27ffc8a58 in tflite::interpreter_wrapper::InterpreterWrapper::Invoke() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#5  0x00007ff27ffbe6be in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#4}, pybind11::object, tflite::interpreter_wrapper::InterpreterWrapper&, pybind11::name, pybind11::is_method, pybind11::sibling>(pybind11_init__pywrap_tensorflow_interpreter_wrapper(pybind11::module&)::{lambda(tflite::interpreter_wrapper::InterpreterWrapper&)#4}&&, pybind11::object (*)(tflite::interpreter_wrapper::InterpreterWrapper&), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call)\r\n    () from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#6  0x00007ff27ffbfc19 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\n#7  0x00000000005669ac in _PyCFunction_FastCallDict () at ../Objects/methodobject.c:231\r\n#8  0x000000000050a5c3 in call_function.lto_priv () at ../Python/ceval.c:4875\r\n#9  0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#10 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7ff180002e48, for file /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py, line 524, in invoke (self=<Interpreter(_custom_op_registerers=[], _interpreter=<tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110>, _delegates=[]) at remote 0x7ff2779fd710>)) at ../Python/ceval.c:754\r\n#11 _PyFunction_FastCall (globals=<optimized out>, nargs=140675211341384, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#12 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#13 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#14 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#15 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x7ff1b4001df8, for file inference.py, line 80, in tflite_inference (interpreter=<Interpreter(_custom_op_registerers=[], _interpreter=<tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper at remote 0x7ff2779fd110>, _delegates=[]) at remote 0x7ff2779fd710>, test_data=[<tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4938>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f47d8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4678>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4888>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4e08>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4bf8>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4a98>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff26c0f4b48>, <tensorflow.python.framework.ops.EagerTensor at remote 0x7ff21c0ad048>, <tensorflow.python.framework.ops.EagerTens...(truncated)) at ../Python/ceval.c:754\r\n#16 _PyFunction_FastCall (globals=<optimized out>, nargs=140676083752440, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#17 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#18 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#19 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#20 0x0000000000509758 in PyEval_EvalFrameEx (throwflag=0, \r\n    f=Frame 0x543b138, for file inference.py, line 120, in run_inference at ../Python/ceval.c:754\r\n#21 _PyFunction_FastCall (globals=<optimized out>, nargs=88322360, args=<optimized out>, co=<optimized out>) at ../Python/ceval.c:4933\r\n#22 fast_function.lto_priv () at ../Python/ceval.c:4968\r\n#23 0x000000000050a48d in call_function.lto_priv () at ../Python/ceval.c:4872\r\n#24 0x000000000050bfb4 in _PyEval_EvalFrameDefault () at ../Python/ceval.c:3335\r\n#25 0x0000000000507d64 in PyEval_EvalFrameEx (throwflag=0, f=Frame 0x18c6bd8, for file inference.py, line 150, in <module> ()) at ../Python/ceval.c:754\r\n---Type <return> to continue, or q <return> to quit---\r\n#26 _PyEval_EvalCodeWithName.lto_priv.1820 () at ../Python/ceval.c:4166\r\n#27 0x000000000050ae13 in PyEval_EvalCodeEx (closure=0x0, kwdefs=0x0, defcount=0, defs=0x0, kwcount=0, kws=0x0, argcount=0, args=0x0, locals=<optimized out>, \r\n    globals=<optimized out>, _co=<optimized out>) at ../Python/ceval.c:4187\r\n#28 PyEval_EvalCode (co=<optimized out>, globals=<optimized out>, locals=<optimized out>) at ../Python/ceval.c:731\r\n#29 0x0000000000634c82 in run_mod () at ../Python/pythonrun.c:1025\r\n#30 0x0000000000634d37 in PyRun_FileExFlags () at ../Python/pythonrun.c:978\r\n#31 0x00000000006384ef in PyRun_SimpleFileExFlags () at ../Python/pythonrun.c:419\r\n#32 0x00000000006386c5 in PyRun_AnyFileExFlags () at ../Python/pythonrun.c:81\r\n#33 0x0000000000639091 in run_file (p_cf=0x7ffece85e63c, filename=<optimized out>, fp=<optimized out>) at ../Modules/main.c:340\r\n#34 Py_Main () at ../Modules/main.c:810\r\n#35 0x00000000004b0d00 in main (argc=2, argv=0x7ffece85e838) at ../Programs/python.c:69\r\n```\r\n", "comments": ["FYI, TFLite's GPU supports is different with Tensorflow. It requires Mobile GPU which is available for Android and iOS.\r\nhttps://www.tensorflow.org/lite/performance/gpu\r\n\r\nYour GPU runtime is compatible with Tensorflow not Tensorflow Lite. So please use CPU runtime.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41570\">No</a>\n", "I have the problem with CPU and GPU. My model is a converted mask rcnn in tensorflow2 but my session crashes when I invoke the interpreter. The input is coherent with input details so no problem there. \r\n\r\nThe following message is from GPU:\r\n\r\n```\r\nTimestamp | Level | Message\r\n-- | -- | --\r\nNov 2, 2020, 3:08:48 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 3:04:57 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 3:02:14 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 3:02:07 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 3:01:00 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 3:00:59 PM | WARNING | pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\r\nNov 2, 2020, 2:59:12 PM | INFO | http://172.28.0.2:9000/\r\nNov 2, 2020, 2:59:12 PM | INFO | google.colab serverextension initialized.\r\nNov 2, 2020, 3:08:48 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 3:04:57 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 3:02:14 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 3:02:07 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 3:01:00 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 3:00:59 PM | WARNING | coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\r\nNov 2, 2020, 2:59:12 PM | INFO | Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\nNov 2, 2020, 3:10:23 PM | WARNING | WARNING:root:kernel 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36 restarted\r\nNov 2, 2020, 3:05:41 PM | WARNING | WARNING:root:kernel 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36 restarted\r\nNov 2, 2020, 2:59:12 PM | INFO | Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\nNov 2, 2020, 2:59:12 PM | INFO | The Jupyter Notebook is running at:\r\nNov 2, 2020, 2:59:12 PM | INFO | Serving notebooks from local directory: /\r\nNov 2, 2020, 3:10:23 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nNov 2, 2020, 3:05:41 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nNov 2, 2020, 2:59:56 PM | INFO | Kernel started: 8e9b15e5-7c66-4b1e-bcbe-1d36d13f7e36\r\nNov 2, 2020, 3:08:49 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.\r\nNov 2, 2020, 3:04:57 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.\r\n\r\n\r\n```\r\n\r\nThe kernet seems to restart for unkown reason.\r\n\r\nAlso, the input detail contain 3 tensors beside the image : \r\n\r\n```\r\n[{'dtype': numpy.float32,\r\n  'index': 0,\r\n  'name': 'input_image',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n   'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32)},\r\n  'shape': array([   1, 1024, 1024,    3], dtype=int32),\r\n  'shape_signature': array([  -1, 1024, 1024,    3], dtype=int32),\r\n  'sparsity_parameters': {}},\r\n {'dtype': numpy.float32,\r\n  'index': 1,\r\n  'name': 'input_image_meta',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n   'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32)},\r\n  'shape': array([ 1, 93], dtype=int32),\r\n  'shape_signature': array([-1, 93], dtype=int32),\r\n  'sparsity_parameters': {}},\r\n {'dtype': numpy.float32,\r\n  'index': 2,\r\n  'name': 'input_anchors',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n   'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32)},\r\n  'shape': array([     1, 261888,      4], dtype=int32),\r\n  'shape_signature': array([    -1, 261888,      4], dtype=int32),\r\n  'sparsity_parameters': {}}]\r\n```\r\n\r\nHowever the original model needs only one tensor. I tried to give all three inputs (image of size (1,1024,1024,3) and 2 zeros tensors with appropriate size but no success.\r\n\r\nI need your help on this \ud83d\udc4d ", "**Update**\r\n\r\nIn fact for a model with multiple inputs, you have to specify each one of them at a time like this: \r\n\r\n```\r\ninterpreter.set_tensor(input_details[0]['index'], image)\r\ninterpreter.set_tensor(input_details[1]['index'], A)\r\ninterpreter.set_tensor(input_details[2]['index'], B)\r\n```\r\n\r\nBut when I invoke I session crashes again.\r\n\r\nrun on CPU :\r\n\r\n```\r\n\r\nNov 2, 2020, 3:56:23 PM | WARNING | WARNING:root:kernel 6841efe9-d913-4be1-9de2-c068b02ca452 restarted\r\n-- | -- | --\r\nNov 2, 2020, 3:56:23 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nNov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.\r\nNov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nNov 2, 2020, 3:52:45 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.\r\nNov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.\r\nNov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nNov 2, 2020, 3:47:25 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.\r\nNov 2, 2020, 3:46:45 PM | INFO | Adapting to protocol v5.1 for kernel 6841efe9-d913-4be1-9de2-c068b02ca452\r\nNov 2, 2020, 3:40:47 PM | WARNING | TfLiteFlexDelegate delegate: 0 nodes delegated out of 25 nodes with 0 partitions.\r\nNov 2, 2020, 3:40:47 PM | WARNING | INFO:\r\nNov 2, 2020, 3:40:47 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nNov 2, 2020, 3:40:47 PM | WARNING | INFO: TfLiteFlexDelegate delegate: 12 nodes delegated out of 586 nodes with 4 partitions.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894074: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg8 that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894064: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/map/while/strided_slice that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894053: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg7 because it has fewer than 1024 elements (1).\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.894036: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor arg6 because it has fewer than 1024 elements (4).\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.893950: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor arg4 that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430175: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor mask_rcnn/roi_align_mask/concat because it has no allocated buffer.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430166: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/roi_align_mask/strided_slice_182 that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430151: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/ArgMax that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430142: I tensorflow/lite/tools/optimize/quantize_weights.cc:220] Skipping quantization of tensor mask_rcnn/mrcnn_detection/clipped_boxes because it has no allocated buffer.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430133: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor mask_rcnn/mrcnn_detection/GatherNd because it has fewer than 1024 elements (1000).\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430124: I tensorflow/lite/tools/optimize/quantize_weights.cc:203] Skipping quantization of tensor mask_rcnn/mrcnn_detection/strided_slice_28 that is not type float.\r\nNov 2, 2020, 3:38:49 PM | WARNING | 2020-11-02 14:38:49.430114: I tensorflow/lite/tools/optimize/quantize_weights.cc:211] Skipping quantization of tensor mask_rcnn/mrcnn_detection/GatherNd because it has fewer than 1024 elements (1000).\r\n\r\n\r\n```\r\n\r\nYour help is much appreciated :D \r\n", "If I have CUDA enabled with select ops on some models in tflite I get a crash inside a lambda function called from `CopyFromBufferHandle`, never if it's disabled\r\n\r\nI'm testing tflite on desktop before moving to mobile. Have tried it with r2.4 will try to test with r2.7"]}, {"number": 41569, "title": "ValueError on evaluate with generator.", "body": "```\r\noutput = classifier.predict(x=generator)\r\nevaluate = classifier.evaluate(x=generator)\r\n```\r\n\r\nThis code throws ValueError: Shapes (None, None, None) and (100, 4, 1, 200) are incompatible on the evaluate. The prediction works just fine. \r\n\r\nI don't understand. Why does the predict work when the evaluate gives me a error? Anyone have any ideas?", "comments": ["@surGeonGG \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Figured it out. It was because I added another output.\r\n\r\nRemoved this and everything works fine,\r\n\r\n```\r\n    classifier.summary()\r\n    classifier = Model(inputs=classifier.input,\r\n                   outputs=[classifier.output, classifier.get_layer('attention').output])\r\n    classifier.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\r\n```\r\n\r\n", "@surGeonGG \r\n\r\nGlad to know the issue was resolved. Please, close this thread as your issue was resolved.Thanks!"]}, {"number": 41568, "title": "TF-Lite Micro: port Silicon Labs STK3701A to Micro Speech example", "body": "This ports the Micro Speech example to the Silicon Labs STK3701A development board:\r\nhttps://www.silabs.com/development-tools/mcu/32-bit/efm32gg11-starter-kit\r\n", "comments": ["@gbaned All conflicts have been resolved", "Apologies for the long delay in the response here.\r\n\r\nWe recently updated our [contribution guidelines for TensorFLow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md#new-target--platform--ide--examples) and are currently pausing accepting PRs that add support for new targets, platforms and examples.\r\n\r\nAs such, we will not be able to accept this PR at this time."]}, {"number": 41567, "title": "Fix incomplete description of callbacks.ModelCheckpoint", "body": "Fix is based on this issue #41420\r\nTwo parts got fixed:\r\n\r\nmode: \"{auto, min, max}\" should be {auto, min, max}\r\nsave_best_only: add the part that the current model will not be written", "comments": ["@yil532 Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 41566, "title": "Fix incomplete description of callbacks.ModelCheckpoint", "body": "Fix is based on this issue https://github.com/tensorflow/tensorflow/issues/41420\r\nTwo parts got fixed:\r\n- mode: \"{auto, min, max}\" should be `{auto, min, max}`\r\n- save_best_only: add the part that the current model will not be written", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41566) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 41565, "title": "RGB to YUV: Conflicting information between example and description", "body": "https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv \r\n\r\nFix the documentation explainations. [0, 1] input range was misleading.\r\n\r\nIssue is tracked here https://github.com/tensorflow/tensorflow/issues/41496 and https://github.com/tensorflow/tensorflow/issues/41300", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41565) for more info**.\n\n<!-- need_author_cla -->", "I do have CLA signed. My email is yiwen.li@ibm.com or yil532@ucsd.edu\r\n@googlebot I fixed it.", "@yil532 Thank you for your contribution. Can you please sign CLA? Thanks!", "I\u2019m sorry but this change doesn\u2019t really do anything. The documentation still says it must be between 0 and 1 and the example defies that. ", "> @yil532 Thank you for your contribution. Can you please sign CLA? Thanks!\r\n\r\n@gbaned I did sign CLA, for example, on this PR, it shows that I have CLA signed https://github.com/tensorflow/tensorflow/pull/41567", "> I\u2019m sorry but this change doesn\u2019t really do anything. The documentation still says it must be between 0 and 1 and the example defies that.\r\n\r\n@aigagror Yes, the input define range was not wrong, it was just not very clear. Users need to convert from [0, 255] to [0, 1] float range. Should I change the sample input values then?", "Could someone check my CLA please? This is a previous PR got merged https://github.com/tensorflow/tensorflow/pull/41348\r\nNot sure why it says no CLA here. @gbaned ", "> Could someone check my CLA please? This is a previous PR got merged #41348\r\n> Not sure why it says no CLA here. @gbaned\r\n\r\n@yil532 Can you please make sure to use same GitHub username and email-id associated with it.", "Yes, I checked, the username is yil532 and email is yiwen.li@ibm.com Everything seems fine.\r\n@gbaned ", "@yil532 can you please try to sign the cla with both username and email( yiwen.li@ibm.com) together , we see CLA with only email( yiwen.li@ibm.com)\r\n\r\n", "Sure, asking my team about this @rthadur ", "@rthadur Hi, I did sign my CLA with both yil532 and yiwen.li@ibm.com\r\nShould I refresh this PR?", "@yil532 for some reason it is not showing the same, can you please try to submit a new PR ?", "Sure, doing it now @rthadur "]}, {"number": 41564, "title": "\"Introduction to Tensors\" has incorrect example image", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/tensor\r\nhttps://www.tensorflow.org/guide/tensor#multi-axis_indexing\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe left side image in the figure called \"Selecting the last feature across all locations in each example in the batch\" has batches 1 (blue) and 2 (green) swapped. The right side image and the surrounding code example show the correct order.\r\n\r\nThe image that needs an edit is: https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/index1.png\r\n\r\n### Submit a pull request?\r\n\r\nNo. I do not have a way to edit the image.", "comments": ["Thanks for reporting. Fixed. It will take a day for the change to propagate to the site."]}, {"number": 41563, "title": "AutoGraph fails due to an end-of-line between parentheses", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): uname -a:\r\nDarwin Daniels-MacBook-Pro.local 18.7.0 Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64 x86_64\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\n\r\nAutoGraph fails if we introduce an end of line in the definition of a lambda function.  \r\n\r\n**Describe the expected behavior**\r\n\r\nAutoGraph should work the same way as if there is no EOL. An EOL between parentheses is legitimate Python syntax. In my own code, YAPF introduces the EOL that caused the problem. It was difficult to figure out that this was the problem.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[colab](https://colab.research.google.com/drive/1b_REMiUPxOwIQ3spSzqpLr6HDJ-yY4j-?usp=sharing)\r\n\r\n```python\r\nf = tf.function(\r\n    lambda a, \r\n    b: a + b)\r\nprint(f(1, 2))\r\n```\r\nAutoGraph works however if one removes the EOL between `a,` and `b:`.\r\n\r\n**Other info / logs** \r\n\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f1132d6a268> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n    lambda a, \r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f1132d6a268> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n    lambda a, \r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\n", "comments": ["I am pretty sure that multiple line parameter works when complete parameters are on the same line. For example, in this case, a complete parameter of tf.function is the whole lambda function, so it needs to be written on one line. I believe the that if you still want to separate the lambda function on different lines, you can always use \"\\\" to tell the compiler that there is an EOL.", "> it needs to be written on one line\r\n\r\nI do not understand why. Equivalent Python syntax should yield the same behavior. ", "@fachu000 \r\nI ran the code on nightly version and face no warning, please have a look at the [gist here](https://colab.research.google.com/gist/Saduf2019/cacc8cd5c819f58c14b6d8710528e6f4/untitled284.ipynb)", "Yes, it seems it has been fixed in nightly. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41563\">No</a>\n"]}, {"number": 41562, "title": "Build tflite_runtime version 2.1.0.post1 for python3.8", "body": "\r\nHey all!\r\n\r\nI\u00b4m working on a aarch64 system and inside a ros:foxy docker container.\r\nBecause I\u00b4m working with the google coral chip I need to use tflite_runtime version 2.1.0.post1.\r\nAccording to the edge tpu guy I need to build from commit \"d855adfc5a0195788bf5f92c3c7352e638aa1109\"  but after building a python 3.8 wheel file and installing that I still get this error:\r\n`ValueError: Model provided has model identifier ' TPU', should be 'TFL3'`\r\nI\u00b4m not sure if this is a edge tpu or tflite issue (see [this](https://github.com/google-coral/edgetpu/issues/170) issue for reference).\r\nI\u00b4m also not sure whether i build the correct version because tflite_runtime.__version__ displays 2.1.0. \r\n\r\nHere are my two questions:\r\n\r\n- Which branch/tag do I need to pull for version 2.1.0.post1?\r\n- Do you have any idea where this error might come from? (I\u00b4m 100% sure I\u00b4m loading the correct file)", "comments": ["From the error looks like you're trying to load a model in TFLite interpreter that is not a TFLite flatbuffer. Follow EdgeTPU guide for running your converted model.", "I know that's what it looks like but i\u00b4m using the provided examples from the \"Getting started\" tutorial so the models should be fine. And these issues occur with compiled models as well as normal models.\r\nThe paths are fine as well, I tried copy pasting the commands as well as writing them using tab completion.\r\nOn the other hand [these](https://github.com/guichristmann/edge-tpu-tiny-yolo) things work just fine so maybe something about the model conversion changed that breaks the EdgeTPU example models?\r\n\r\nDoes the commit \"d855adfc5a0195788bf5f92c3c7352e638aa1109\" build version 2.1.0.post1? \r\nIf not maybe this version is responsible for this behavior and I just need to know the right commit to build the right version.", "@ItsMeTheBee If it is TFLite that you're using please send reproduce steps to the issue. If it is the EdgeTPU part, then please file issue with EdgeTPU so they can help you.\r\n\r\nCommit \"d855adfc5a0195788bf5f92c3c7352e638aa1109\" is available in v2.2 +", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41562\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41562\">No</a>\n"]}, {"number": 41561, "title": "Allgather gradients in Tensorflow distributed ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAll gathering gradients in distributed Tensorflow would allow us to directly manipulate gradients in a much more efficient way. We can then encode (compress) gradients at each device(GPU) before cross-communication and then decode (or decompress) the gradients after all gathering. This would also allow us to combine gradients in a non-trivial way. This can allow researchers to develop efficient coding (or compression) schemes for reduced communication (something like Huffman coding). Currently, AFAIK only all reduce with SUM, MEAN option is supported.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nResearchers and developers working in coding techniques for distributed learning. \r\n\r\n**Any Other info.**\r\nThere seems to be some TODO mentioned over [here](https://github.com/tensorflow/tensorflow/blob/3b5b3cd58acb7b0192d43db444b8a4db66f5bae3/tensorflow/python/distribute/cross_device_ops.py#L928). But I was wondering if we can do the above in a much simpler way.\r\n1. Encode (or compress) gradients in each device after the gradient calculation [here](https://github.com/tensorflow/tensorflow/blob/2f4444c1cff8ac07ab2c31d1ae23d23c66147126/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434)\r\n2. Let Tensorflow do all the operations across devices till all reduce.\r\n3. Decode (or decompress) gradients just before the all reduce is called in the function [here](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/distribute/distribute_lib.py#L1905).", "comments": ["Hi @vineeths96, thanks for providing this detailed feature request. This is an important feature and it has been requested before. Exposing all_gather at the strategy level is actively being worked on. I can definitely update this thread when there are any changes.", "@nikitamaia Thank you for your reply.\r\n\r\nI went through the latest RC of TF2.3 and did not see any advancements in this direction. I guess it would take quite some time before this rolls out public.\r\n\r\nDo you have any alternative ideas about how this could be achieved (maybe playing with the source codes)?", "Sorry for my late response here, but hopefully the following help\r\n\r\nThis is a lower level function that will be used in the all_gather implementation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/collective_ops.py#L74\r\n\r\nAnd a function for testing purposes in limited situation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/test_util.py#L33", "Updating and closing this thread because [`tf.distribute.Strategy.gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy?version=nightly#gather) and [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext?version=nightly#all_gather) have been added in nightly."]}, {"number": 41560, "title": "[TFLite] Added coverage test for 16x8 quantizaion post-training mode.", "body": "Added a coverage test for 16x8 post-training quantization mode.", "comments": ["@jdduke, please check this change. We discussed it as one of the requirements for the stable release of 16x8-bit quantization.", "Hi @jdduke Thanks for the review. I renamed. Please take a look. ", "@wwwind Can you please address Ubuntu Sanity errors? Thanks!", "Hi @gbaned \r\nThis failure does not look relevant to my changes.\r\n\r\nThe test //tensorflow/core/grappler/costs:op_level_cost_estimator_test is failing, but this failure is due to timeout. \r\n`tensorflow/core/grappler/costs/op_level_cost_estimator_test.cc:984\r\nExpected equality of these values:\r\n  cost.compute_time\r\n    Which is: 5000ns\r\n  Costs::Duration(expected_compute_time)\r\n    Which is: 4300ns\r\nSoftmax`\r\n\r\nI checked that is passes locally.\r\nCould you please re-run Ubuntu Sanity Checks on this PR ?", "@jdduke fixed. Could you please re-approve ? Thanks!"]}, {"number": 41559, "title": "libtensorflowlite.so built with flex delegates too big!", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 x86_64 GNU/Linux Docker (official tensorflow image)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 2.4.0-dev20200712\r\n\r\nModified BUILD to add following dependency:\r\n```\r\n\"//tensorflow/lite/delegates/flex:delegate\",\r\n```\r\n\r\nAnd did a build using the following command:\r\n```\r\nbazel build --config=monolithic --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\n**The output:**\r\n\r\n`libtensorflowlite.so` that is `147104080` bytes (147MB)\r\n\r\nFollowed the above instructions to generate a libtensorflow with interpreter support for flex ops. Is this file size of 147MB on the library expected? Is this because I added the flex:delegate dependency? How can I reduce this to be <10MB like the lib produced if I follow the lite/tools/make shell scripts (i.e build_lib.sh)", "comments": ["Hi aselva-eb,\r\nFor size reduction, I would recommend to use our new selective build rule in tensorflow/lite/delegates/flex/build_def.bzl.\r\nThere are several rule that you could uses. Ex: tflite_flex_cc_library for the cc_library and tflite_flex_android_library for android_library.\r\nNote that, it currently only work when you build it for android. selective build for linux is not supported yet.", "Hi @thaink \r\n\r\nAre there instructions anywhere of how to use the selective build rules?\r\n\r\nBut also, my target is linux. If selective build rule is not supported for Linux, do you have any suggestions for how I can reduce the lib size by modifying source code maybe? Perhaps manually removing unused non-TFLite ops from being built in by flex (from the allowlist or elsewhere) ? \r\n\r\nAny insight would be appreciated. ", "Hi @aselva-eb \r\nI think you could try to remove some unused deps from:\r\n`tensorflow/core:all_kernels`\r\nand\r\n`tensorflow/core:all_kernels_impl`\r\nPlease let me know the size after you do that.", "Ok! Will explore that option and get back to you!\nThanks Thai!\n\nGet Outlook for Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: Thai Nguyen <notifications@github.com>\nSent: Thursday, July 23, 2020 9:33:55 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Anith Selvakumar <anith.s@ecobee.com>; Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] libtensorflowlite.so built with flex delegates too big! (#41559)\n\n\nHi @aselva-eb<https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Faselva-eb&data=01%7C01%7Canith.s%40ecobee.com%7C1a8a1f0265e345c273c208d82f71a16d%7C487e3dd07f654a9bbf912970cfa93390%7C0&sdata=T2Clxq%2BitOfiWQdus3E%2FVzNVkub30zn%2FHR488Y2bvqU%3D&reserved=0>\nI think you could try to remove some unused deps from:\ntensorflow/core:all_kernels\nand\ntensorflow/core:all_kernels_impl\nPlease let me know the size after you do that.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F41559%23issuecomment-663310420&data=01%7C01%7Canith.s%40ecobee.com%7C1a8a1f0265e345c273c208d82f71a16d%7C487e3dd07f654a9bbf912970cfa93390%7C0&sdata=pX4HHgLjIq9rtwMsQ3BtZFGL24Q5nh%2B7Ve5GpKC0ekw%3D&reserved=0>, or unsubscribe<https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAKJHGH5IHVH3SHS4Y6TYLD3R5DQIHANCNFSM4PCJ3YIA&data=01%7C01%7Canith.s%40ecobee.com%7C1a8a1f0265e345c273c208d82f71a16d%7C487e3dd07f654a9bbf912970cfa93390%7C0&sdata=6vL4nD7rd%2FIkNEqARFFBAmbI2hnlmtNL%2FxYeG5hMALo%3D&reserved=0>.\n", "@aselva-eb \r\nPlease update on the issue.", "> @aselva-eb\r\n> Please update on the issue.\r\n\r\nHi. I haven't had the chance to try that. \r\nWill let you know as soon as I get results", "> Hi @aselva-eb\r\n> I think you could try to remove some unused deps from:\r\n> `tensorflow/core:all_kernels`\r\n> and\r\n> `tensorflow/core:all_kernels_impl`\r\n> Please let me know the size after you do that.\r\n\r\nHi Thai. It looks like it does reduce the lib size incrementally when I remove kernels.\r\nLooking at the kernels dir it's about 40MB. Not sure how much this contributes to the library size when compiled. Could be more or less.\r\n\r\nCan you suggest any big-ticket dependencies I can remove? Or other areas that can be trimmed down? Hoping to reduce this size to less than 50MB if possible.. Any suggestions would be appreciated.", "Hi @aselva-eb \r\nIt depends a lot on your models. So my recommendations is to remove them all first. Then gradually add them back and use  `bazel run -c opt -- tensorflow/lite/tools:list_flex_ops_main --graphs=<your tflite model>` to check when the lib contains enough kernels for your models.", "Thank you @thaink - that helped a lot! Was able to shrink the lib to less than half the original", "@aselva-eb \r\nPlease move this issue to closed status if resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41559\">No</a>\n"]}, {"number": 41558, "title": "RuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 1 (FlexRandomStandardNormal) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nColab link here: https://colab.research.google.com/gist/Pyrsos/2e1021981a779b0d41ee51de5b3fd321/bayesiannn_tflite_issue.ipynb\r\n\r\nAnd in case the link does not work here is the code:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_train = x_train.astype('float32')/255.\r\nx_test = x_test.astype('float32')/255.\r\n\r\nkl_divergence_function = (lambda q, p, _: tfd.kl_divergence(q, p) /\r\n                          tf.cast(x_train.shape[0], dtype=tf.float32))\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Flatten(),\r\n    tfp.layers.DenseFlipout(\r\n        10, kernel_divergence_fn=kl_divergence_function,\r\n        activation=tf.nn.softmax\r\n    ),\r\n])\r\n\r\noptimizer = tf.keras.optimizers.Adam(lr=0.001)\r\nmodel.compile(optimizer, loss='sparse_categorical_crossentropy')\r\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3)\r\n\r\ntflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                              tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = tflite_converter.convert()\r\n\r\ntflite_interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ntflite_interpreter.allocate_tensors()\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 1s 0us/step\r\n2020-07-20 12:34:31.727019: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-07-20 12:34:31.727189: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-07-20 12:34:31.727228: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (0effea1c9ee4): /proc/driver/nvidia/version does not exist\r\n2020-07-20 12:34:31.727927: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-20 12:34:31.739524: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2192950000 Hz\r\n2020-07-20 12:34:31.739933: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd2d0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-20 12:34:31.739972: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-20 12:34:31.758131: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 188160000 exceeds 10% of free system memory.\r\nEpoch 1/3\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/layers/util.py:106: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.add_weight` method instead.\r\n1871/1875 [============================>.] - ETA: 0s - loss: 0.80902020-07-20 12:34:36.954654: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.8086 - val_loss: 0.5851\r\nEpoch 2/3\r\n1854/1875 [============================>.] - ETA: 0s - loss: 0.55612020-07-20 12:34:42.216506: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5546 - val_loss: 0.5113\r\nEpoch 3/3\r\n1863/1875 [============================>.] - ETA: 0s - loss: 0.50292020-07-20 12:34:47.170768: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 31360000 exceeds 10% of free system memory.\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.5030 - val_loss: 0.4894\r\n2020-07-20 12:34:47.848009: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-07-20 12:34:47.848232: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-20 12:34:47.852393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-07-20 12:34:47.852454: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.003ms.\r\n2020-07-20 12:34:47.852466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-07-20 12:34:47.946289: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-07-20 12:34:47.946632: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-07-20 12:34:47.962972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-07-20 12:34:47.963044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (-64), 67 edges (-65), time = 8.269ms.\r\n2020-07-20 12:34:47.963056: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 66 nodes (0), 67 edges (0), time = 1.115ms.\r\nTraceback (most recent call last):\r\n  File \"example.py\", line 31, in <module>\r\n    tflite_interpreter.allocate_tensors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 242, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 110, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexRandomUniformInt) failed to prepare.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/4947761/model.zip)\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\nConversion completes successfully but when the interpreter is used to invoke the converted tflite model the following error appears:\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.Node number 0 (FlexRandomUniformInt) failed to prepare.\r\n``` \r\n\r\n**Any other info / logs**\r\n\r\nI opened a previous issue regarding this #40119, and was giving guidelines to surpass it. I have now applied this recommendation but I receive the error described in this post.\r\n", "comments": ["@Pyrsos \r\n\r\nI have not seeing any issue in TF nightly versions.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c67a19c37ff28f02c67e36542555fd75/untitled153.ipynb).Please, verify once and close the issue.Thanks!", "@ravikyram \r\n\r\nIt works!! Thank you for your quick response, I am closing down the issue.", "@Pyrsos  \r\n\r\nHi, I met one similar problem when I installed tf-2.3.0 on MacOS, it said tf lite cannot support \"FlexRandomUniform\". Luckly, when I installed tf-nightly-2.4.0.dev20200926, the bug was fixed. My colleagues did not meet this bug when he only used the same version tf-2.3.0 on Ubuntu. I have no idea why MacOS shows this bug, but Ubuntu doesn't. ", "What if my board/system is 32 bit that tensorflow cannot be installed. "]}, {"number": 41557, "title": "Disable Warning for complex numbers", "body": "When running the following code:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.Variable(tf.complex([2., 2.], [2., 2.]))\r\nwith tf.GradientTape() as tape:\r\n    y = tf.abs(tf.reduce_sum(x))**2\r\ndy_dx = tape.gradient(y, x)\r\n```\r\nI get the warning message:\r\n`WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.complex64`\r\n\r\nAccording to [Introduction to Gradients and Automatic Differentiation](https://www.tensorflow.org/guide/autodiff#3_took_gradients_through_an_integer_or_string). This message is intended when the variable is not derivable. However, it does make sense to use the autodiff for complex numbers and this Warning message impacts my code negatively. I don't want to disable ALL warnings just for this.\r\n\r\nThis issue is related to [#32774](https://github.com/tensorflow/tensorflow/issues/32774) and [#30107 PR](Make tape only watch the tensor with the floating type).\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.00\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere is a warning in my opinion should not exist.\r\n\r\n**Will this change the current api? How?**\r\nComplex gradient will no longer display a Warning\r\n\r\n**Who will benefit with this feature?**\r\nWhoever is working with complex numbers.\r\n", "comments": ["@NEGU93,\r\nOn running the code with [TF v2.0](https://colab.research.google.com/gist/amahendrakar/5ce6af79ad9ac754efeda05a6f7a44f5/41557-2-0.ipynb), I got a similar warning. However, looks like the issue is fixed in TF v2.2.\r\n\r\nRunning the code with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/62c14e9b0f2852d42930bf95466947c7/41557.ipynb) I did not get any warning. Please find the attached gist. Thanks!", "True, I just updated to v2.2 and this problem does not exist anymore. Sorry!"]}]