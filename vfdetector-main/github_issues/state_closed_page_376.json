[{"number": 42729, "title": "Autograph unable to process lambda statements @TF 2.3.0", "body": "- `TF 2.0: python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` : `v2.3.0-rc2-23-gb36436b087 2.3.0`\r\n\r\n## Problem description  \r\n```python\r\nimport tensorflow as tf\r\nassert tf.__version__ == '2.3.0'\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n   [1,2,3.]\r\n).map(lambda x:x)\r\n```\r\nThrows unexpected warning:\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f4ac022e710> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n).map(lambda x:x)\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <function <lambda> at 0x7f4ac022e710> and will run it as-is.\r\nCause: could not parse the source code:\r\n\r\n).map(lambda x:x)\r\n\r\nThis error may be avoided by creating the lambda in a standalone statement.\r\n\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n\r\n## Expected behaviour  \r\nThe autograph converting the lambda statement as usual.\r\n\r\n## Additional info  \r\nThis bug does not seem to occur in earlier version '2.2.0'. \r\nas per:\r\n```python\r\nimport tensorflow as tf\r\nassert tf.__version__ == '2.2.0'\r\ndt = tf.data.Dataset.from_tensor_slices(\r\n    tf.ones((10,))\r\n).map(lambda x:x)\r\n```\r\nrunning without any erroneous output.\r\n* all installations were made using pip package manager\r\n\r\n", "comments": ["This is my very first issue on git and I more than willing to help however I can. :smile: ", "@tornikeo \r\n\r\nI tried in colab with TF versions 2.2,2.3 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d670398149064d89ca7a94158455e6cb/untitled279.ipynb).You are also seeing the same behavior?\r\nThanks!\r\n", "@ravikyram \r\n\r\nYes, in colab see the same behaviour in both, versions 2.2 and 2.3. ", "@ravikyram \r\n\r\nHow do I contribute? Specifically I want to help with updating writing and translating documentation. Can you provide any pointers?", "Hi @tornikeo,\r\n\r\nThe warning here is because your lambda statement is on a separate line. \r\n`tf.data.Dataset.from_tensor_slices([1,2,3.]).map(lambda x:x)` should work fine.\r\n\r\nThe reason you are not seeing the warning in earlier versions is not because the problem wasn't there, but it wasn't discovered until later and the warning was added. Please see issue #39832 for more context on why the multi line lambda statements can cause problems for autograph (it had to do with an underlying issue in Python).\r\n\r\nWe have added more robust coverage for lambdas and you should not see the problem in tf-nightly. Hope this helps!", "Thanks @nikitamaia \r\nThis means you can safely close this issue now, if you wish.\r\n\r\nP.S: Sorry for bothering you but, about my question above, how do I contribute? Are there any easy to understand guides for contributing to tensorflow? This is the first time i am contributing to FOSS. Any pointers would be greatly appreciated. :smile:\r\n Thank you.\r\n", "That's great! The Tensorflow community is always looking for more contributors! You can learn more about getting started in the [Contributors Guide](https://www.tensorflow.org/community/contribute). Also linking to the specific sections on [docs and translations](https://www.tensorflow.org/community/contribute/docs#community_translations) since you mentioned you were interested. ", "Thnaks @nikitamaia ! I'll look into that. Closing this issue now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42729\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42729\">No</a>\n"]}, {"number": 42728, "title": "OP_REQUIRES failed at conv_ops.cc:1115", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no, used code from CNN tutorial\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, kernel 5.8.3-arch1-1\r\n- TensorFlow installed from (source or binary): python-tensorflow-opt-cuda package\r\n- TensorFlow version (use command below): unknown 2.3.0\r\n- Python version: Python 3.8.5\r\n- CUDA/cuDNN version: CUDA 11.0.2-1,  cuDNN 8.0.2.39-2\r\n- GPU model and memory: Nvidia GeForce RTX 2070 8gb\r\n\r\n**Describe the current behavior**\r\n\r\nAny model fitting using Conv2D layers fails with `OP_REQUIRES failed at conv_ops.cc:1115 : Not found: No algorithm worked!` errors. Full trace:\r\n```\r\n2020-08-28 12:42:25.012311: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1115 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 26, in <module>\r\n    history = model.fit(train_images, train_labels, epochs=10, batch_size=64,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1098, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 840, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1843, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1923, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 545, in call\r\n    outputs = execute.execute(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n\t [[node sequential/conv2d/Conv2D (defined at test.py:26) ]] [Op:__inference_train_function_853]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nModel fitting works correctly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI used [Convolutional Neural Network (CNN) Tutorial](https://www.tensorflow.org/tutorials/images/cnn), full code below:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\nimport matplotlib.pyplot as plt\r\n\r\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\n# Normalize pixel values to be between 0 and 1\r\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\r\n\r\nmodel = models.Sequential()\r\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64, activation='relu'))\r\nmodel.add(layers.Dense(10))\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nhistory = model.fit(train_images, train_labels, epochs=10,\r\n                    validation_data=(test_images, test_labels))\r\n```\r\n\r\nI'm attaching full log file (with `TF_CPP_MIN_LOG_LEVEL=0`): [log.txt](https://github.com/tensorflow/tensorflow/files/5141360/log.txt)\r\n\r\nHow can I help debugging this problem?", "comments": ["I tried reverting different packages to older versions, and it seems the problem was introduced in cuDNN between versions 8.0.0.180 and 8.0.2.39:\r\n\r\n```\r\ncudnn 8.0.0.180 works\r\ncudnn 8.0.2.39 doesn't\r\n```\r\nUnfortunately cuDNN is not open-source so I am not able to debug further.", "@Rogach,\r\nAs per the [tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations), TensorFlow v2.3 is compatible with cuDNN 7.6 and CUDA 10.1. \r\nI was able to run the code without any issues using that configuration, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b9f00c660793a08d672576fa97f6670f/42728.ipynb). Thanks!\r\n\r\nCould you please check if you are facing the same error with cuDNN 7.6 and CUDA 10.1? Thanks!", "Yes, cuDNN `7.6` (or even `8.0.0`) works correctly, problems begin only with `8.0.2`. I'll close the issue for now since the configuration is unsupported.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42728\">No</a>\n", "This workaround fixes this issue for me.\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n  tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\nor\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n```"]}, {"number": 42727, "title": "TFlu: Update third party downloads with Ethos-U", "body": "Update Ethos-U driver download version.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review."]}, {"number": 42726, "title": "tensorflow serialize models", "body": "I use multi model ,example bert1,bert2,bert3 for predict,\r\nHow to serialize to achieve modify model \r\nuse below code ,it's not work\r\n```\r\n    if init_checkpoint:\r\n        (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,init_checkpoint)\r\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\r\n```\r\n\r\n```\r\n2020-08-28 16:59:17.748977: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: Checksum does not match: stored 347393357 vs. calculated on the restored bytes 1825803478\r\n2020-08-28 16:59:17.750680: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: Checksum does not match: stored 4097036962 vs. calculated on the restored bytes 3596090645\r\n20\r\n```\r\nDataLossError: Checksum does not match: stored 1057886036 vs. calculated on the restored bytes 1146985826\r\n", "comments": ["@gucasbrg \r\nCan you please provide simple stand alone code for us to replicate the issue reported or if possible share a colab gist with issue reported for us to analyse.\r\nAlso please let us know the tf version and system information.", "I have solved it, it is spark addfile api problem"]}, {"number": 42725, "title": "DLL load failed", "body": "I'm relatively new to programming so please forgive me if some details mentioned before are not detailed enough. \r\nI downloaded tensorflow today and after looking at the documentation, downloaded/updated CUDA and cuDNN as well. But I'm getting this error:\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>()\r\n----> 1 import tensorflow\r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\eager\\context.py in <module>()\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>()\r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n     28 from tensorflow.python import pywrap_tensorflow\r\n---> 29 from tensorflow.python._pywrap_tfe import *\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nI tried the solutions mentioned in older posts having the same error, but none worked.\r\nI'm using:\r\nWindows 10 64-bit\r\nMobile Device: N/A\r\nTensorFlow downloaded through Anaconda prompt -pip install tensorflow\r\nVersion: 2.3.0\r\nPython version 3.5.5\r\nBazel version (if compiling from source) - N/A\r\nGCC/Compiler version (if compiling from source) - N/A\r\nCUDA/cuDNN version - 10.2\r\nGPU model and memory - NVIDIA GeForce 940MX; 4GB dedicated memory\r\nExact command to reproduce - import tensorflow\r\nTensorflow version - 2.3.0", "comments": ["@TheNightBaron \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "AVX is supported in intel i series of CPUs. Mine is Intel i7 7th Generation CPU. \r\nPython is 64 bit.\r\nI just updated visual C++ version in my laptop and it worked! Thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42725\">No</a>\n"]}, {"number": 42724, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version: 2.3.0\r\nPython version: 3.8.3\r\nInstalled using virtualenv? pip? conda?: conda\r\nBazel version (if compiling from source): N/A\r\nGCC/Compiler version (if compiling from source): N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nProvide the exact sequence of commands / steps that you executed before running into the problem : Just ran a file with import tensorflow on python\r\n\r\nC:\\Users\\User\\.conda\\envs\\tensor\\pythonw.exe F:/Tech_With_Tim/tensorEnv/test.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"F:/Tech_With_Tim/tensorEnv/test.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\User\\.conda\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1", "comments": ["@twinkle055,\r\nIn order to expedite the trouble-shooting process, please fill in the below details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n- Provide the exact sequence of commands / steps that you executed before running into the problem\r\n\r\nAlso if you are using Anaconda environment, please submit a new issue from [this link](https://github.com/ContinuumIO/anaconda-issues/issues), as these issues are tracked in the Anaconda repo. Thanks!", "My issue was solved.\r\nI downgraded my tensorflow because my tensorflow version was not compatible with python 3.7.\r\nI used the command - pip install tensorflow==1.2.0\r\nand also, the command - pip install keras==2.2.4\r\nThis resolved my issue.", "i have same issue, \r\npython v 3.8.5\r\npip and conda\r\nwindows 10\r\n\r\nthat's not fixed and pip install tensorflow==1.2.0 is not avalable,\r\ncan you help me ?!", "i tried to use \r\n\r\n> pip install tensorflow==2.2.0\r\n\r\n but that was not useful ...\r\n", "> i have same issue,\r\n> python v 3.8.5\r\n> pip and conda\r\n> windows 10\r\n> \r\n> that's not fixed and pip install tensorflow==1.2.0 is not avalable,\r\n> can you help me ?!\r\n\r\n@aminanderian That is because... python 3.8.5 is not compatible with tensorflow 1.2.0\r\nI would suggest downgrading your python to 3.7.9 or any version in context to 3.7", "> > i have same issue,\r\n> > python v 3.8.5\r\n> > pip and conda\r\n> > windows 10\r\n> > that's not fixed and pip install tensorflow==1.2.0 is not avalable,\r\n> > can you help me ?!\r\n> \r\n> @aminanderian That is because... python 3.8.5 is not compatible with tensorflow 1.2.0\r\n> I would suggest downgrading your python to 3.7.9 or any version in context to 3.7\r\n\r\nthanks, but I dwongrade it to 3.7.9 and tried it again it gave me this error:\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.2.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\nERROR: No matching distribution found for tensorflow==1.2.0", "> > > i have same issue,\r\n> > > python v 3.8.5\r\n> > > pip and conda\r\n> > > windows 10\r\n> > > that's not fixed and pip install tensorflow==1.2.0 is not avalable,\r\n> > > can you help me ?!\r\n> > \r\n> > \r\n> > @aminanderian That is because... python 3.8.5 is not compatible with tensorflow 1.2.0\r\n> > I would suggest downgrading your python to 3.7.9 or any version in context to 3.7\r\n> \r\n> thanks, but I dwongrade it to 3.7.9 and tried it again it gave me this error:\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\n> ERROR: No matching distribution found for tensorflow==1.2.0\r\n\r\nOk, then\r\nInstall tensorflow 1.14.0\r\npip install tensorflow==1.14.0", "> > > > i have same issue,\r\n> > > > python v 3.8.5\r\n> > > > pip and conda\r\n> > > > windows 10\r\n> > > > that's not fixed and pip install tensorflow==1.2.0 is not avalable,\r\n> > > > can you help me ?!\r\n> > > \r\n> > > \r\n> > > @aminanderian That is because... python 3.8.5 is not compatible with tensorflow 1.2.0\r\n> > > I would suggest downgrading your python to 3.7.9 or any version in context to 3.7\r\n> > \r\n> > \r\n> > thanks, but I dwongrade it to 3.7.9 and tried it again it gave me this error:\r\n> > ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 1.15.2, 1.15.3, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0, 2.1.1, 2.2.0rc0, 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\n> > ERROR: No matching distribution found for tensorflow==1.2.0\r\n> \r\n> Ok, then\r\n> Install tensorflow 1.14.0\r\n> pip install tensorflow==1.14.0\r\n\r\nPython 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\n>>> import tensorflow\r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n\r\n\r\nI have it again ...", "I test the consule with import numpy to know if system has any other error that numpy was ok,\r\nyou know what's the main error !? \r\n", "thanks\r\n", "> My issue was solved.\r\n> I downgraded my tensorflow because my tensorflow version was not compatible with python 3.7.\r\n> I used the command - pip install tensorflow==1.2.0\r\n> and also, the command - pip install keras==2.2.4\r\n> This resolved my issue.\r\n\r\n@twinkle055,\r\nPlease feel free to close the issue if resolved. Thanks!", "@aminanderian,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42724\">No</a>\n"]}, {"number": 42723, "title": "TensorFlow Lite Android nightly method \"runForMultipleInputsOutputs\" that loaded model using stft  not work with converted by tf.lite.OpsSet.SELECT_TF_OPS ", "body": "TensorFlow Lite Android nightly method \"runForMultipleInputsOutputs\" that loaded model using stft  not work with converted by tf.lite.OpsSet.SELECT_TF_OPS .\r\nWhat is wrong ?\r\n\r\nwritten code.\r\n> import numpy as np\r\n> import tensorflow as tf\r\n> from tensorflow.keras.layers import Input, Dense, Dropout\r\n> from tensorflow.keras.layers import Conv2D, MaxPool2D,Flatten\r\n> from tensorflow.keras.layers import BatchNormalization,Activation,Add,GlobalAveragePooling2D,Reshape\r\n> from tensorflow.keras.models import Model\r\n> \r\n> _FFT_SIZE=512\r\n> _HOP_SIZE=256\r\n> _N_MEL_BINS=128\r\n> \r\n> class LogMelSpectrogram(tf.keras.layers.Layer):\r\n>     \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\r\n> \r\n>     def __init__(self, sample_rate, fft_size, hop_size, n_mels,\r\n>                  f_min=0.0, f_max=None, **kwargs):\r\n>         super(LogMelSpectrogram, self).__init__(**kwargs)\r\n>         self.sample_rate = sample_rate\r\n>         self.fft_size = fft_size\r\n>         self.hop_size = hop_size\r\n>         self.n_mels = n_mels\r\n>         self.f_min = f_min\r\n>         self.f_max = f_max if f_max else sample_rate / 2\r\n>         self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\r\n>             num_mel_bins=self.n_mels,\r\n>             num_spectrogram_bins=fft_size // 2 + 1,\r\n>             sample_rate=self.sample_rate,\r\n>             lower_edge_hertz=self.f_min,\r\n>             upper_edge_hertz=self.f_max)\r\n> \r\n>     def build(self, input_shape):\r\n>         self.non_trainable_weights.append(self.mel_filterbank)\r\n>         super(LogMelSpectrogram, self).build(input_shape)\r\n> \r\n>     def call(self, waveforms):\r\n>         \"\"\"Forward pass.\r\n>         Parameters\r\n>         ----------\r\n>         waveforms : tf.Tensor, shape = (None, n_samples)\r\n>             A Batch of mono waveforms.\r\n>         Returns\r\n>         -------\r\n>         log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\r\n>             The corresponding batch of log-mel-spectrograms\r\n>         \"\"\"\r\n>         def _tf_log10(x):\r\n>             numerator = tf.math.log(x)\r\n>             denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\r\n>             return numerator / denominator\r\n> \r\n>         def power_to_db(magnitude, amin=1e-16, top_db=80.0):\r\n>             \"\"\"\r\n>             https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\r\n>             \"\"\"\r\n>             ref_value = tf.reduce_max(magnitude)\r\n>             log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\r\n>             log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\r\n>             log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\r\n> \r\n>             return log_spec\r\n> \r\n>         spectrograms = tf.signal.stft(waveforms,\r\n>                                       frame_length=self.fft_size,\r\n>                                       frame_step=self.hop_size,\r\n>                                       pad_end=False)\r\n> \r\n>         magnitude_spectrograms = tf.abs(spectrograms)\r\n> \r\n>         mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\r\n>                                      self.mel_filterbank)\r\n> \r\n>         log_mel_spectrograms = power_to_db(mel_spectrograms)\r\n> \r\n>         # add channel dimension\r\n>         log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\r\n> \r\n>         return log_mel_spectrograms\r\n> \r\n>     def get_config(self):\r\n>         config = {\r\n>             'fft_size': self.fft_size,\r\n>             'hop_size': self.hop_size,\r\n>             'n_mels': self.n_mels,\r\n>             'sample_rate': self.sample_rate,\r\n>             'f_min': self.f_min,\r\n>             'f_max': self.f_max,\r\n>         }\r\n>         config.update(super(LogMelSpectrogram, self).get_config())\r\n> \r\n>         return config\r\n> \r\n> \r\n> \r\n> def GetModel(n_classes, sample_rate=16000, duration=4,\r\n>               fft_size=_FFT_SIZE, hop_size=_HOP_SIZE, n_mels=_N_MEL_BINS):\r\n>     n_samples = sample_rate * duration\r\n>     input_data = Input(shape=(n_samples,), name='input', dtype='float32')\r\n>     freq = Input(name='freq',shape=[1],dtype='int32')\r\n>     x = input_data\r\n>     test_output = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels)(x)\r\n>     x = Dense(n_classes,activation='softmax')(x)\r\n> \r\n>     return Model(inputs=[input_data,freq], outputs=x)\r\n> \r\n\r\nconvert model to tflite with tf.lite.OpsSet.SELECT_TF_OPS\r\n> N_classes = 50\r\n> Duration = 5\r\n> \r\n> model = GetModel(N_classes,duration=Duration)\r\n> opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6, amsgrad=True)\r\n> model.compile(loss='categorical_crossentropy',\r\n>           optimizer=opt,\r\n>           metrics=['accuracy'])\r\n> model.summary()\r\n> \r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n> \r\n> #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n> #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]\r\n> \r\n> tflite_model = converter.convert()\r\n> with open(\"model.tflite\", \"wb\") as f:\r\n>     f.write(tflite_model)\r\n\r\non Centos , tflite invoke from python is work.\r\n>   input_data = np.zeros(input_shape, dtype=np.float32)\r\n>   pprint(\"input_data\")\r\n>   pprint(input_data)\r\n>   input_data[0][0:] = x2\r\n>   npa = np.array([[0]], dtype=np.int32) \r\n>   npa[0] = 16000\r\n> \r\n>   print(f\"input_details[0]:{input_details[0]}\")\r\n>   print(f\"input_details[1]:{input_details[1]}\")\r\n> \r\n>   interpreter.set_tensor(input_details[0]['index'], input_data)\r\n>   interpreter.set_tensor(input_details[1]['index'], npa)\r\n> \r\n>   interpreter.invoke()\r\n>   output_data = interpreter.get_tensor(output_details[0]['index'])\r\n>   print(\"output_data:\")\r\n>   pprint(output_data)\r\n> \r\n\r\nResults:\r\n> array([[0.01259949, 0.01274813, 0.02341373, 0.02667766, 0.01372223,\r\n>         0.02063654, 0.01939304, 0.01231323, 0.02360482, 0.01985866,\r\n>         0.01616379, 0.02790864, 0.01614609, 0.03300844, 0.02038008,\r\n>         0.02689514, 0.01205852, 0.02373471, 0.03396689, 0.0159423 ,\r\n>         0.01418541, 0.01825977, 0.01506583, 0.01811898, 0.0167273 ,\r\n>         0.01219947, 0.03590814, 0.01607339, 0.0213427 , 0.02323929,\r\n>         0.01976353, 0.01719474, 0.02329947, 0.02231019, 0.02448872,\r\n>         0.01822161, 0.01806694, 0.02725129, 0.0257786 , 0.02799631,\r\n>         0.02095988, 0.01663861, 0.01729359, 0.0102047 , 0.01404527,\r\n>         0.01819064, 0.0125629 , 0.01387509, 0.01704353, 0.032522  ]],\r\n>       dtype=float32)\r\n>\r\n\r\non Android , Kotlin\r\n>tfLite!!.runForMultipleInputsOutputs(inputArray, outputs) \r\n\r\nexample 1\r\n>Internal error: Failed to run on the given Interpreter: Matrix size-incompatible: In[0]: [80000,1], In[1]: [80000,50]\r\n>    \t (while executing 'MatMul' via Eager)\r\n\r\nexample 2\r\n>Cannot copy from a TensorFlowLite tensor (Identity) with shape [80000, 50] to a Java object with shape [1, 50].\r\n\r\n", "comments": ["@tk-okumura \r\nCan you please provide indented complete code, or share a colab gist with the error reported.", "Hi Saduf2019.\r\nthanks your reply.\r\n\r\nafter to post issue, I tryed to write minimum code.\r\n\r\nI needed to use stft.\r\nIt need to convert with \"tf.lite.OpsSet.SELECT_TF_OPS\".\r\nand so converted model rise error.\r\n\r\n**error (written by kotlin)**\r\n\r\n>    I/System.out: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Matrix size-incompatible: \r\n>   In[0]: [80000,1], In[1]: [80000,50]\r\n>        \t (while executing 'MatMul' via Eager)\r\n>        Node number 6 (TfLiteFlexDelegate) failed to invoke.\r\n> \r\n\r\nPython code \"check for runnable\" isn't return any error.\r\nI comment out OPS option \"(*1)\", Kotlin isn't return any error.\r\n(test code is not contain stft then can convert without SELECT_TF_OPS.)\r\n\r\nKotlin code\r\nbuild.gradle\r\n\r\n>     implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n>     implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n> \r\n\r\nPython\r\nModel\r\n\r\n>   import numpy as np\r\n>   import tensorflow as tf\r\n>   from tensorflow.keras.layers import Input, Dense, Dropout\r\n>   from tensorflow.keras.layers import Conv2D, MaxPool2D,Flatten\r\n>   from tensorflow.keras.layers import BatchNormalization,Activation,Add,GlobalAveragePooling2D,Reshape\r\n>   from tensorflow.keras.models import Model\r\n>   \r\n>   _FFT_SIZE=512\r\n>   _HOP_SIZE=256\r\n>   _N_MEL_BINS=128\r\n>   \r\n>   def GetModel(n_classes, sample_rate=16000, duration=4,\r\n>                 fft_size=_FFT_SIZE, hop_size=_HOP_SIZE, n_mels=_N_MEL_BINS):\r\n>       n_samples = sample_rate * duration\r\n>       input_data = Input(shape=(n_samples,), name='input', dtype='float32')\r\n>       x = input_data\r\n>       x = Dense(n_classes,activation='softmax')(x)\r\n>       return Model(inputs=input_data, outputs=x)\r\n> \r\n> Convert\r\n>   import os\r\n>   import tensorflow as tf\r\n>   from declareReportSimple import GetModel\r\n>   \r\n>   N_classes = 50\r\n>   Duration = 5\r\n>   \r\n>   model = GetModel(N_classes,duration=Duration)\r\n>   #opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6, amsgrad=True)\r\n>   opt = tf.keras.optimizers.Adam()\r\n>   model.compile(loss='categorical_crossentropy',\r\n>             optimizer=opt,\r\n>             metrics=['accuracy'])\r\n>   model.summary()\r\n>   \r\n>   converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n>   \r\n>   #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n>   #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n>   converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS] # (*1)\r\n>   \r\n>   tflite_model = converter.convert()\r\n>   with open(\"for_report_simple.tflite\", \"wb\") as f:\r\n>       f.write(tflite_model)\r\n> \r\n> \r\n\r\ncheck for runnable\r\n\r\n>   import os\r\n>   import pandas as pd\r\n>   import numpy as np\r\n>   import tensorflow as tf\r\n>   from tensorflow.keras.utils import to_categorical\r\n>   from pprint import pprint\r\n>   \r\n>   cur_dir = os.getcwd()\r\n>   tflitemodel = 'for_report_simple.tflite'\r\n>   \r\n>   N_classes = 50\r\n>   Duration = 5\r\n>   \r\n>   interpreter = tf.lite.Interpreter(model_path=cur_dir+'/'+tflitemodel)\r\n>   interpreter.allocate_tensors()\r\n>   input_details = interpreter.get_input_details()\r\n>   output_details = interpreter.get_output_details()\r\n>   \r\n>   input_shape = input_details[0]['shape']\r\n>   \r\n>   pprint(f\"input_details[0]:{input_details[0]}\")\r\n>   pprint(f\"output_details[0]:{output_details[0]}\")\r\n>   \r\n>   x2 = np.zeros((16000 * Duration))\r\n>   \r\n>   input_data = np.zeros(input_shape, dtype=np.float32)\r\n>   input_data[0][0:] = x2\r\n>   \r\n>   pprint(f\"input_data.shape:{input_data.shape}\")\r\n>   interpreter.set_tensor(input_details[0]['index'], input_data)\r\n>   \r\n>   interpreter.invoke()\r\n>   output_data = interpreter.get_tensor(output_details[0]['index'])\r\n>   print(f\"output_data:{output_data}\")\r\n", "resolved.\r\n\r\nin case not use tf.lite.OpsSet.SELECT_TF_OPS,\r\ninput buffer shape [16000,1] has been processed.\r\n\r\nin case use tf.lite.OpsSet.SELECT_TF_OPS,\r\ninput buffer shape [16000,1] has been processed  ,batch size was16000.\r\nI changed input shape  to [1,16000]  from [16000,1].\r\n\r\nthank you.\r\n\r\n\r\n", "Thanks for the update. Closing since shape resizing resolves it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42723\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42723\">No</a>\n"]}, {"number": 42721, "title": "fatal error: tensorflow/core/protobuf/error_codes.pb.h: No such file or directory", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Linux Drax 5.4.0-42-generic #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n- TensorFlow source: v2.3.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\n\r\n\r\n\r\ncompile any program that includes tensorflow/core/public/session.h\r\n\r\n\r\ngcc -I/home/amascaro/Programming/src/tensorflow -I/home/amascaro/.local/lib/python3.6/site-packages/tensorflow_core/include/ loader.cc \r\nIn file included from /home/amascaro/Programming/src/tensorflow/tensorflow/core/platform/errors.h:22:0,\r\n                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/lib/core/errors.h:19,\r\n                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/framework/tensor_shape.h:23,\r\n                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/framework/tensor.h:24,\r\n                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/public/session.h:24,\r\n                 from loader.cc:1:\r\n/home/amascaro/Programming/src/tensorflow/tensorflow/core/platform/status.h:28:10: fatal error: tensorflow/core/protobuf/error_codes.pb.h: No such file or directory\r\n #include \"tensorflow/core/protobuf/error_codes.pb.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\nThe missing file is under another path: /home/amascaro/.local/lib/python3.6/site-packages/tensorflow_core/include/tensorflow/core/lib/core/error_codes.pb.h", "comments": ["@AngeloMascaro,\r\nCould you please provide the complete code or the commands you have executed before running into this issue. \r\n\r\nAlso, take a look at [this comment](https://github.com/FloopCZ/tensorflow_cc/issues/16#issuecomment-325797963) from a similar issue and let us know if it helps. Thanks!", "I experience a similar problem. While running the following command on Ubuntu 18.04 after compiling TensorFlow 2.3 C++ API (using Bazel). g++ -std=c++11 -I/usr/local/include/tf -I/usr/local/include/tf/third_party/eigen3 -L/usr/local/lib hello.cc -o hello\r\nI searched for the file \"error_codes.pb.h\" on my computer but its not there. Although, I have installed protoc 3.5.\r\n ```\r\n#include \"tensorflow/cc/client/client_session.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n\r\nint main() {\r\n  using namespace tensorflow;\r\n  using namespace tensorflow::ops;\r\n  Scope root = Scope::NewRootScope();\r\n  // Matrix A = [3 2; -1 0]\r\n  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });\r\n  // Vector b = [3 5]\r\n  auto b = Const(root, { {3.f, 5.f} });\r\n  // v = Ab^T\r\n  auto v = MatMul(root.WithOpName(\"v\"), A, b, MatMul::TransposeB(true));\r\n  std::vector<Tensor> outputs;\r\n  ClientSession session(root);\r\n  // Run and fetch v\r\n  TF_CHECK_OK(session.Run({v}, &outputs));\r\n  // Expect outputs[0] == [19; -3]\r\n  LOG(INFO) << outputs[0].matrix<float>();\r\n  return 0;\r\n}\r\n```", "> Also, take a look at [this comment](https://github.com/FloopCZ/tensorflow_cc/issues/16#issuecomment-325797963) from a similar issue and let us know if it helps. Thanks!\r\n\r\n@AngeloMascaro,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "> I experience a similar problem. \r\n\r\n@fisakhan,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "I check it today afternoon or tomorrow morning and I let you know. I suppose yes: it happens with any code that includes that header file. Even an empty main \n\nInviato da iPhone\n\n> Il giorno 8 set 2020, alle ore 10:06, Abhilash Mahendrakar <notifications@github.com> ha scritto:\n> \n> Also, take a look at this comment from a similar issue and let us know if it helps. Thanks!\n> \n> @AngeloMascaro,\n> Is this still an issue? Please feel free to close the issue if resolved. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "> I check it today afternoon or tomorrow morning and I let you know.\r\n\r\n@AngeloMascaro,\r\nAny updates regarding this issue? Did you get a chance to look into this? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42721\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42721\">No</a>\n"]}, {"number": 42720, "title": "I have no idea how to initialize the tf.feature_column.embedding_column from txt or numpy? ", "body": "I have no idea how to initialize the tf.feature_column.embedding_column from txt or numpy? thanks", "comments": ["@ChaosCY \r\n\r\nPlease, take a look at this usage example from [here](https://www.tensorflow.org/tutorials/structured_data/feature_columns) for tf.feature_column.embedding_column from text.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "> @ChaosCY\r\n> \r\n> Please, take a look at this usage example from [here](https://www.tensorflow.org/tutorials/structured_data/feature_columns) for tf.feature_column.embedding_column from text.\r\n> \r\n> This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n\r\nI cannot find the example loading from a text, sorry. "]}, {"number": 42719, "title": "How to incrementally training the embedding with tf.feature_column.embedding_column?", "body": "I will train a embedding model incrementally through the Estimator and feature_column, but I have no idea how to expand the embedding column when some new id appears, or load the embedding from a numpy or file.\r\nAny help would be appreciated.", "comments": []}, {"number": 42718, "title": "Use a ternary if for checking main_op is None in Saved Model Builder", "body": "Fixes a minor bug in saved_model which raises an\r\nexception since evaluating ops as booleans are\r\nforbidden in TF 2.x\r\n\r\nFixes #38041 \r\n\r\nI haven't added a test case yet, but the fix is quite simple.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42718) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42718) for more info**.\n\n<!-- ok -->"]}, {"number": 42717, "title": "XLA Parallel reduce", "body": "Extend the XLA codegen to generate parallel reductions when there are multiple\r\nreduce instructions in a fusion computation. This paves a way for horizontal input\r\nfusion in the future.\r\n\r\n### Performance Numbers\r\nNow we see ~3% e2e gain for NVIDIA JoC BERT.\r\n\r\nFor `ManyParallelReductions` with 128 reduce instructions in the unittest, the\r\nexecution time is reduced from 325us to 3.9us (83X), reported by nvprof as below.\r\n\r\nBefore:\r\n\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n\r\n                   32.50%  325.54us         1  325.54us  325.54us  325.54us  fusion\r\n\r\nAfter:\r\n\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n\r\n                    0.59%  3.9030us         1  3.9030us  3.9030us  3.9030us  fusion\r\n\r\n\r\n### High-level descriptions about the Algorithm\r\nThe overall idea is to divide the reduce instructions in a fusion into groups for parallelization, as implemented by `DivideOutputInstructionsIntoGroups()`. Different groups will be executed in parallel, while reduce instructions in the same group are run sequentially. Generally speaking, we'd like to run the reduce instructions in parallel without incurring too much re-computation overhead. The current heuristic is to place reduce instructions who share nothing or only\r\n (broadcasted) scalars/constants into different groups; otherwise, they are placed in the same group. Non-reduce instructions always go with the reduce instructions into the same group so long as they share any predecessors.\r\n\r\n\r\nTo avoid complicating the (already complicated) index calculation in reduce code generation, we use raw (CUDA) block_id_y to select the reduce groups (instead of thread_id_x/block_id_x). That is, a block_id_y is assigned to a group and so different groups can be run in parallel. Separation of the reduce codegen index calculation (using raw dim x) and parallelization (using raw dim y) should also provide better future maintainability.\r\n", "comments": ["Thanks! I'm fine with this as long as nitpicks are addressed.", "> Thanks! I'm fine with this as long as nitpicks are addressed.\r\n\r\n@cheshire just to make sure that you know I have updated the codes to address your previous comments.\r\n\r\nWould you be the main reviewer or @thomasjoerg will take charge?\r\n", "@trentlo I'm fine with merging this.", "(provided our internal benchmarks don't regress)", "> @trentlo I'm fine with merging this.\r\n\r\nGreat! I do not see regressions from the NV-internal benchmarks but we will see.", "George, could you launch the CI again? Thanks!\r\n\r\nThere were build errors due to narrowing type conversion (In MacOS CPU Python3 and Bazel Windows GPU). I fixed them by pushing a new commit. An error in Bazel Windows seem not to related to the PR. Other tests were clean.\r\n\r\n", "@trentlo Could you fix the following warnings?\r\n\r\n```\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3007:26: error: non-constant-expression cannot be narrowed from type 'size_t' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]\r\n      return ShapeIndex({i});\r\n                         ^\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3007:26: note: insert an explicit cast to silence this issue\r\n      return ShapeIndex({i});\r\n                         ^\r\n                         static_cast<long long>( )\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3834:50: error: '&&' within '||' [-Werror,-Wlogical-op-parentheses]\r\n         HloOpcode::kBroadcast == instr.opcode() &&\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3834:50: note: place parentheses around the '&&' expression to silence this warning\r\n         HloOpcode::kBroadcast == instr.opcode() &&\r\n                                                 ^\r\n         (\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3957:7: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n      EmitIRForReduction(unnested_hlo, instr_groups[i], &reduction_info,\r\n      ^~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3979:56: error: non-constant-expression cannot be narrowed from type 'std::__u::vector<std::__u::vector<xla::HloInstruction *, std::__u::allocator<xla::HloInstruction *>>, std::__u::allocator<std::__u::vector<xla::HloInstruction *, std::__u::allocator<xla::HloInstruction *>>>>::size_type' (aka 'unsigned long') to 'tensorflow::int64' (aka 'long long') in initializer list [-Wc++11-narrowing]\r\n      {/*x=*/mapping_scheme.GetNumberOfBlocks(), /*y=*/instr_groups.size(),\r\n                                                       ^~~~~~~~~~~~~~~~~~~\r\nthird_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3979:56: note: insert an explicit cast to silence this issue\r\n      {/*x=*/mapping_scheme.GetNumberOfBlocks(), /*y=*/instr_groups.size(),\r\n                                                       ^~~~~~~~~~~~~~~~~~~\r\n                                                       static_cast<int64>()\r\n\r\n```", "(we have a long-standing bug to enable these warnings in OSS, but so far it hasn't been done =/)", "> @trentlo Could you fix the following warnings?\r\n> \r\n> ```\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3007:26: error: non-constant-expression cannot be narrowed from type 'size_t' (aka 'unsigned long') to 'long long' in initializer list [-Wc++11-narrowing]\r\n>       return ShapeIndex({i});\r\n>                          ^\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3007:26: note: insert an explicit cast to silence this issue\r\n>       return ShapeIndex({i});\r\n>                          ^\r\n>                          static_cast<long long>( )\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3834:50: error: '&&' within '||' [-Werror,-Wlogical-op-parentheses]\r\n>          HloOpcode::kBroadcast == instr.opcode() &&\r\n>          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3834:50: note: place parentheses around the '&&' expression to silence this warning\r\n>          HloOpcode::kBroadcast == instr.opcode() &&\r\n>                                                  ^\r\n>          (\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3957:7: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]\r\n>       EmitIRForReduction(unnested_hlo, instr_groups[i], &reduction_info,\r\n>       ^~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3979:56: error: non-constant-expression cannot be narrowed from type 'std::__u::vector<std::__u::vector<xla::HloInstruction *, std::__u::allocator<xla::HloInstruction *>>, std::__u::allocator<std::__u::vector<xla::HloInstruction *, std::__u::allocator<xla::HloInstruction *>>>>::size_type' (aka 'unsigned long') to 'tensorflow::int64' (aka 'long long') in initializer list [-Wc++11-narrowing]\r\n>       {/*x=*/mapping_scheme.GetNumberOfBlocks(), /*y=*/instr_groups.size(),\r\n>                                                        ^~~~~~~~~~~~~~~~~~~\r\n> third_party/tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc:3979:56: note: insert an explicit cast to silence this issue\r\n>       {/*x=*/mapping_scheme.GetNumberOfBlocks(), /*y=*/instr_groups.size(),\r\n>                                                        ^~~~~~~~~~~~~~~~~~~\r\n>                                                        static_cast<int64>()\r\n> ```\r\n\r\nPR updated. Please help to launch the CI again. Thanks~\r\n", "CC @timshen91 "]}, {"number": 42716, "title": "labels for each object prior", "body": "I'm building ssd(object detection) with pytorch-> tensorflow<br/>\r\n\r\nlet say I have \r\n\r\n    object_for_each_prior\r\n    <tf.Tensor: shape=(8,), dtype=int64, numpy=array([2, 2, 0, 1, 0, 0, 1, 1])>\r\n    \r\n    labels\r\n    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 20,  10, 5], dtype=int32)>\r\n\r\nand want to represent the \r\n\r\n    label_for_each_prior = labels[object_for_each_prior]  # (8732)\r\n\r\nexpected output should be \r\n\r\n    tensor([ 5,  5,  0, 10,  0,  0, 10, 10], device='cuda:0')\r\n\r\n", "comments": ["@SlowMonk \r\nPlease fill in [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) for us to analyse the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42716\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42716\">No</a>\n"]}, {"number": 42715, "title": "TFLM: Added VexRISCV optimized DepthwiseConv2d kernel (int8/uint8)", "body": "This PR adds an optimized DepthwiseConv2D `int8`/`uint8` kernel for VexRISCV as discussed with @advaitjain earlier, see below for details. \r\n\r\n## TL;DR\r\nThe optimization\r\n1. Reduces the penalty introduces by excessive memory misses compared to the straightforward implementation\r\n2. No regression on model precision\r\n3. Speedup person_detection (`int8`/`uint8`) by ~5% during inference\r\n4. Requires no special hardware intrinsic instructions or compiler support, just pure C++\r\n\r\n## Overview\r\nThe kernel is optimized based on the reference kernel in Tensorflow Lite: different from the straightforward implementation, this implementation takes memory layout in TF Lite (`NHWC`) into account, which leverages memory hierarchy to reduce memory miss count, to be more specific, it performs depthwise convolution for every channel in a fixed spatial position (iterate `C`-axis first, then `W`-axis, `H`-axis, and `N`-axis).\r\n\r\nThis design re-uses memory blocks that have already been cached in faster memory (e.g. L1 cache or L2 cache) and uses a small fixed size buffer of size `k` to store the intermediate values (`int32`) to perform the exact same operation (in this kernel: 2-D depthwise convolution,) thus, **this design has no regression in terms of model precision**.\r\n\r\n## Cost of the design\r\nThe cost consists of two parts:\r\n1. Uses a small fixed size buffer: memory usage will increase in this layer compared to the original implementation, but this probably will not be an issue (see below for details of how to determine the buffer size `k`)\r\n2. Potentially more memory load when the accumulation is completed: bring buffer for quantization, activation, etc.\r\n\r\n## Results\r\nThe following results are measured with Digilent Arty A7 with VexRISCV soft-CPU running two versions of person_detection available in the repo, this is not officially supported but basically the same makefile from zephyr_riscv in magic_wand.\r\n\r\n### Memory load-refill count\r\n#### Original\r\n|       | cycles (M) | Load (M) | Refill (k) |\r\n|-------|:----------:|:--------:|:----------:|\r\n| uint8 |   557.46   |   93.34  |   715.39   |\r\n|  int8 |   503.81   |   87.03  |   561.78   |\r\n\r\n#### Optimized\r\n|       | cycles (M) | Load (M) | Refill (k) | \r\n|:----------:|:--------:|:----------:|:--------:|\r\n|  uint8 |   532.77   |   94.11  |   558.29   |   0.83   |   -21.96   | 157.15 |\r\n|  int8 |   478.38   |   87.62  |   494.90   |   0.67   |   -11.90   | 380.27 |\r\n\r\n#### Comparison (delta)\r\n|       | Load | Refill | Miss Penalty (cycles/refill) |\r\n|:----------:|:--------:|:----------:|:--------:|\r\n|  uint8 |  +0.83%   |   -21.96%   | 157.15 |\r\n|  int8 |   +0.67%   |   -11.90%   | 380.27 |\r\n\r\nAs showed above, with a very marginal amount (< 1%) increment of memory load introduced by the buffer, the optimized kernel reduces the overall memory refill count by at least 10%, which also leads to a speedup (see next section.) Although the measurements are not completely accurate because memory caching is very complicated and it involves a lot of other data in the memory/cache, but the results provide a strong evidence on how improving the memory access pattern impacts the performance on edge devices.\r\n\r\n### Inference\r\n|                          |   Original   | Optimized | Speedup |\r\n|--------------------------|:------------:|:---------------:|:----------:|\r\n|                          | # cycles (M) |   # cycles (M)  | (%) |\r\n| person_detection (uint8) |    557.46    |      532.77     |    4.43%   |\r\n| person_detection (int8)  |    503.81    |      478.38     |    5.05%   |\r\n\r\n## Details about choice of the buffer size `k`:\r\nThe platform I'm mainly testing is Digilent Arty A7 with VexRISC-V soft-CPU. According to the hardware spec specified in [SpinalHDL/VexRiscv](https://github.com/SpinalHDL/VexRiscv), the cache size is 4 KB and the cache block (cache line) size is 32 bytes (search `bytePerLine` in the repo), so there will be 1K blocks available.\r\n\r\nIn this kernel, the input and output are both `int8` (8 bits, or 1 byte) which means that the system brings 32 `int8` data to the cache each time. To fully utilize the data in this block, we will need 32 accumulator (of type `int32`) for these 32 `int8` data. Thus, a reasonable choice of `k` is 32 and this will take an additional `32 * sizeof(int32) = 128 bytes`, or 4 blocks, this will occupy 4 out of those 1K blocks, which is comparably a small and tolerable overhead.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.", "Might be worth linking doc/README.md from the top-level readme."]}, {"number": 42714, "title": "[ROCm] Fix for ROCm CSB Breakage - 200827", "body": "The following commit breaks the ROCm TF build\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/188db19ebf9a645b90ad6ee6e6e127be28e68224\r\n\r\nThe commit adds an extra argument to the `DnnPoolingGradOp<T>::Compute` routine, and updates all (but one) of the call-sites accordingly. The one call-site that was not updated is the one used in the ROCm build, which leads to the folowing compile errror\r\n\r\n```\r\ntensorflow/core/kernels/avgpooling_op.cc: In instantiation of 'void tensorflow::AvgPoolingGradOpCustomGPUKernel<T>::Compute(tensorflow::OpKernelContext*) [with T = Eigen::half]':\r\ntensorflow/core/kernels/avgpooling_op.cc:635:1:   required from here\r\ntensorflow/core/kernels/avgpooling_op.cc:602:35: error: no matching function for call to 'tensorflow::DnnPoolingGradOp<Eigen::half>::Compute(tensorflow::OpKernelContext*&, stream_executor::dnn::PoolingMode, std::vector<int>&, std::vector<int>&, tensorflow::Padding&, tensorflow::TensorFormat&, std::nullptr_t, std::nullptr_t, const tensorflow::Tensor&, tensorflow::TensorShape&, bool)'\r\n       DnnPoolingGradOp<T>::Compute(context, se::dnn::PoolingMode::kAverage,\r\n       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                    ksize_, stride_, padding_, data_format_,\r\n                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                    nullptr, nullptr, out_backprop, output_shape,\r\n                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n                                    /*propagate_nans=*/false);\r\n                                    ~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/core/kernels/avgpooling_op.cc:47:0:\r\n./tensorflow/core/kernels/pooling_ops_common_gpu.h:58:15: note: candidate: static void tensorflow::DnnPoolingGradOp<T>::Compute(tensorflow::OpKernelContext*, stream_executor::dnn::PoolingMode, const std::vector<int>&, const std::vector<int>&, tensorflow::Padding, std::vector<long long int>, tensorflow::TensorFormat, const tensorflow::Tensor*, const tensorflow::Tensor*, const tensorflow::Tensor&, const tensorflow::TensorShape&, bool) [with T = Eigen::half]\r\n   static void Compute(OpKernelContext* context,\r\n               ^~~~~~~\r\n./tensorflow/core/kernels/pooling_ops_common_gpu.h:58:15: note:   candidate expects 12 arguments, 11 provided\r\n```\r\n\r\nThe fix is trivial, which is to add the missing argument.\r\n\r\n\r\n-----------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg @nvining-work \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["hmm....it looks like this commit 93c22a269bec68a3b4e93a2310c9282e22c6522a , is reverting the changes made by the commit (mentioned in the PR description) that breaks the ROCm build.  With that commit gone, this PR is no longer needed...assuming that commit does not come back.\r\n\r\n", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 42713, "title": "issue with iterating over batched mapped tf.data.Dataset object", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): conda installation\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: Nvidia RTX2070 8GB\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen processing pipeline for ```tf.data.Dataset``` which contains the usage of ```tf.numpy_function``` the ```UnknownError``` is thrown sometimes and the code runs the other times  without changing anything. This can be seen in the last cell of the colab notebook\r\n\r\n**Describe the expected behavior**\r\nthe ```tf.numpy_function``` should work all the time in the same way\r\n\r\n**Standalone code to reproduce the issue**\r\nLink to the [colab](https://colab.research.google.com/drive/1iqsYWN6HaM09oA_to7MRcKw0wfzFsrkj?usp=sharing) notebook\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\n/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:789: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\r\n  UserWarning,\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    757             output_types=self._flat_output_types,\r\n--> 758             output_shapes=self._flat_output_shapes)\r\n    759 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2609     except _core._NotOkStatusException as e:\r\n-> 2610       _ops.raise_from_not_ok_status(e, name)\r\n   2611     except _core._FallbackException:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"<ipython-input-24-d518d3df69c5>\", line 3, in aug_fn\r\n    aug_data = transforms(**data)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py\", line 176, in __call__\r\n    data = t(force_apply=force_apply, **data)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\", line 87, in __call__\r\n    return self.apply_with_params(params, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\", line 100, in apply_with_params\r\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py\", line 1630, in apply\r\n    return F.image_compression(image, quality, image_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\", line 54, in wrapped_function\r\n    result = func(img, *args, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\", line 796, in image_compression\r\n    _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))\r\n\r\ncv2.error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'\r\n\r\n\r\n\r\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-46-ef3ac52e7aa5> in <module>()\r\n      3                   num_parallel_calls=AUTOTUNE,deterministic=False).prefetch(AUTOTUNE)\r\n      4 it = iter(ds_alb)\r\n----> 5 batch = next(it)\r\n      6 print(type(batch))\r\n      7 images,labels = batch\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)\r\n    734 \r\n    735   def __next__(self):  # For Python 3 compatibility\r\n--> 736     return self.next()\r\n    737 \r\n    738   def _next_internal(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)\r\n    770   def next(self):\r\n    771     try:\r\n--> 772       return self._next_internal()\r\n    773     except errors.OutOfRangeError:\r\n    774       raise StopIteration\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    763       except AttributeError:\r\n--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    765 \r\n    766   @property\r\n\r\n/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)\r\n   2103     finally:\r\n   2104       ctx.executor = executor_old\r\n-> 2105       executor_new.wait()\r\n   2106 \r\n   2107 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nUnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"<ipython-input-24-d518d3df69c5>\", line 3, in aug_fn\r\n    aug_data = transforms(**data)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py\", line 176, in __call__\r\n    data = t(force_apply=force_apply, **data)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\", line 87, in __call__\r\n    return self.apply_with_params(params, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py\", line 100, in apply_with_params\r\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py\", line 1630, in apply\r\n    return F.image_compression(image, quality, image_type)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\", line 54, in wrapped_function\r\n    result = func(img, *args, **kwargs)\r\n\r\n  File \"/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py\", line 796, in image_compression\r\n    _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))\r\n\r\ncv2.error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'\r\n\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\n```", "comments": ["I have tried in colab with TF version [2.3 gist](https://colab.research.google.com/gist/ravikyram/c2503a0aa4cc13c66768f2b108022dc3/untitled275.ipynb), nightly versions(`2.4.0-dev20200827`) [gist](https://colab.research.google.com/gist/ravikyram/5777f300a7b37177fa0863eda6279729/untitled276.ipynb) and was able to reproduce the issue.Thanks!", "Hi @AkshayRoyal, I'm not sure that it's the use of `tf.numpy_function` that's causing the problem. I think it might be in the `albumentations` library you're using. I commented out the line that calls the `transforms` in your `aug_fn`\r\n\r\n```  \r\naug_data = data\r\n# aug_data = transforms(**data)\r\n```\r\n and I no longer see the error in the last cell. \r\n\r\nYou can see from the stack trace that the problem seems to be with opencv and trying to encode an image that is too large.\r\n```\r\nUnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'\r\n```\r\n\r\nI'm not familiar with the `albumentations` library but I would suggesting taking a look at `\"/albumentations/augmentations/functional.py\"` which seems to be causing the problem based on your stack trace, [specifically this line](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py) ` _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))`", "Hi @nikitamaia , Thanks for pointing it out. I verified that the ```image``` is not too large as I have applied the transformations on ```resized_ds``` which is a ```tf.data.Dataset``` object which contains tuples of ```(image,label)``` where each image has been resized to ```120x120```.  I don't understand why it shows that error sometimes and works fine the other times? Had any image been too large to decode the final chunk of code wouldn't work at all.\r\n", "I'm seeing a different error now when I run your code. \r\n\r\n`UnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/imgwarp.cpp:2594: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'`\r\n\r\nThe error again is from the albumentations library. Was there a change to that library or your code?", "I commented out the various operations you have in `transforms`. All of them seem to work fine, except for Rotate and RandomBrightness\r\n\r\nWhen you add Rotate, the error message is:\r\n`cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/imgwarp.cpp:2594: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'`\r\n\r\nAnd when you include RandomBrightness, we see the orginal error\r\n`cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'`\r\n\r\nAgain the stack trace shows that the issue is with the albumentations library, and not in tf.data. I would suggest trying to run these transformations without tf.data as a first step to debugging.", "@nikitamaia \r\nI tried it and it has the same issue but I also noticed another thing that when I pick one element of the resized_ds and apply transformations to it, it produces a black image and with max pixel value in the order of 1e-2. Can this be the issue?\r\nI have updated the [colab ](https://colab.research.google.com/drive/1iqsYWN6HaM09oA_to7MRcKw0wfzFsrkj?usp=sharing#scrollTo=oqV4tSFs5HNK) for you to reproduce it.\r\nThanks \r\n\r\n\r\n**EDIT\r\nThere was an issue with ```tf_resize``` (custom) method definition. Earlier I was using ```tf.image.convert_dtype``` which wasn't working as I expected. On replacing it with ```tf.cast``` worked and now the above is not an issue but the primary issue still persists\r\n```\r\ndef tf_resize(image,label,size):\r\n  \r\n  resized_img = tf.image.resize(image,size=[size,size])\r\n  resized_img = tf.cast(resized_img,'uint8')\r\n  return resized_img,label\r\n```", "Have you tried running the albumentations transformations on the data without using a tf.data.dataset? Since the error message is occurring in the albumentations library, I think this will help us to localize what the issue is.\r\n\r\nAdditionally, to rule out whether or not the problem is with tf.numpy_function can you remove that from your code? I think you should be able to just map the aug_fn on your dataset, if you hard code the img_size. If the error persists, then we know that the use of tf.numpy_function is not what is causing the problem.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, The issue has been resolved. albumentations library doesn't support mapping to a batched dataset for now.\r\n\r\nThanks everyone for your prompt responses. I really appreciate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42713\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42713\">No</a>\n"]}, {"number": 42712, "title": "mis-indent in conv_ops_test python scripts", "body": "The changed code block was inside the for loop iterating over values from GetTestConfigs().\r\nIt make more sense to me to move these line out from the loop, so instead evaluating the unfinished expected_results & computed_values lists for multiple times, only a one time evaluation will be performed on them.\r\n\r\nI am not quite sure whats the original purpose how it was writing this way, from my view, I would like to move them out, please correct me if I am wrong. \r\n\r\nAnd a question, could there be a problem to evaluate these tensors over and over again?", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42712) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42712) for more info**.\n\n<!-- ok -->", "Good catch! I'm also not sure why it the entire list was originally computed inside the loop. I don't think problems would arise evaluating the tensors over and over again, but its slow and it's better not to.\r\n\r\nIt would probably be best to not even have a list and assert the two values are all close inside the for-loop but that can always be done later."]}, {"number": 42711, "title": "TPU Pods: Batches not scaling according to the TPU pod size", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am using a custom training loop to train a model on TPUs. To iterate over the dataset I'm enumerating over it. I'm not using any steps here.\r\n\r\nfor One TPU device: TPU v3-8 the total batches found to be 800 per epoch\r\n\r\nso for a TPU pod: TPU v3-32 the total batches should be **200** per epoch but still, the epoch completes after 800 batches I don't quite understand why.\r\n\r\nattached a snippet of the code \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n#Connecting to TPU\r\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-2')\r\nprint('Running on TPU ', tpu.master())\r\n\r\ntf.config.experimental_connect_to_cluster(tpu)\r\ntf.tpu.experimental.initialize_tpu_system(tpu)\r\nstrategy = tf.distribute.TPUStrategy(tpu)\r\n\r\n\r\n\r\n---------------------------------------------------\r\nBATCH_SIZE = 128 * strategy.num_replicas_in_sync\r\nper_replica_batch_size = BATCH_SIZE // strategy.num_replicas_in_sync\r\n\r\ntrain_dataset = strategy.experimental_distribute_datasets_from_function(\r\n\tlambda _: get_training_dataset(per_replica_batch_size))\r\n\r\n#the loss_plot array will be reset many times\r\nloss_plot = []\r\n\r\n@tf.function\r\ndef train_step(iterator):\r\n\tdef step_fn(inputs):\r\n\r\n\t\timages, labels = inputs\r\n\t\twith tf.GradientTape() as tape:\r\n\t\t\tlogits = model(images, training=True)\r\n\t\t\tloss = tf.keras.losses.sparse_categorical_crossentropy(\r\n\t\t\tlabels, logits, from_logits=True)\r\n\t\t\tloss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\r\n\t\tgrads = tape.gradient(loss, model.trainable_variables)\r\n\t\toptimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\r\n\t\ttraining_loss.update_state(loss * strategy.num_replicas_in_sync)\r\n\t\ttraining_accuracy.update_state(labels, logits)\r\n\t\treturn training_loss, training_accuracy\r\n\r\n\tper_replica_losses, l_loss = strategy.run(step_fn, args=(iterator,))\r\n\treturn strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, l_loss,axis=None)\r\n\r\nfor epoch in range(start_epoch, EPOCHS):\r\n\tstart = time.time()\r\n\ttotal_loss = 0\r\n\r\n\tfor (batch, (img_tensor, target)) in enumerate(train_dataset):\r\n\t\tprint(target)\r\n\t\ttraining_loss, training_accuracy = train_step((img_tensor, target))\r\n\t\ttotal_loss += t_loss\r\n\r\n\t\tif batch % 50 == 0:\r\n\t\t\tprint ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / BATCH_SIZE), flush=True)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@caffeine-lab,\r\nOn running the code I am facing an error stating `NameError: name 'get_training_dataset' is not defined`. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "@amahendrakar It's difficult to share the dataset, But the question is quite straightforward, it doesn\u2019t have anything to do with the dataset, The problematic part of the code also given as a snippet. I hope that's enough?", "> @amahendrakar It's difficult to share the dataset, But the question is quite straightforward, it doesn\u2019t have anything to do with the dataset, The problematic part of the code also given as a snippet. I hope that's enough?\r\n\r\n@caffeine-lab,\r\nIn this case, could you please provide dummy data with similar shape. \r\n\r\nAlso, without a minimal reproducible code it would be difficult for us to determine the source of the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42711\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42711\">No</a>\n"]}, {"number": 42710, "title": "[Intel MKL] Changes to graph rewrite to support native format", "body": "This PR makes changes to MKL graph rewrite to enable native format support for MKL ops based on an environment variable setting.", "comments": ["@mahmoud-abuzaina  Can you please check @penpornk's comments and keep us posted ? Thanks!"]}, {"number": 42708, "title": "tf.debugging.set_log_device_placement(True) does not seem to be working correctly.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): ('v2.3.0-0-gb36436b087', '2.3.0')\r\n- Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) \r\n- GPU model and memory: Tesla K80, 11441 MiB\r\n\r\n**Describe the current behavior**\r\n1. I open the documentation at https://www.tensorflow.org/guide/gpu in a colab\r\n2. I change the runtime to GPU\r\n3. I run all the cells\r\n4. Looking at the Manual device placement section - I see the following output:\r\n<img width=\"1094\" alt=\"Screen Shot 2020-08-27 at 11 16 29 AM\" src=\"https://user-images.githubusercontent.com/6924410/91461401-f1d01980-e856-11ea-986a-2a0539def458.png\">\r\n\r\n\r\n**Describe the expected behavior**\r\nI would have expected the computation to be done on the CPU instead - am I missing something here that is colab specific?\r\n\r\n**Standalone code to reproduce the issue**\r\nThe colab link is the one generated from visiting `https://www.tensorflow.org/guide/gpu`\r\n\r\n", "comments": ["@marwan116 \r\n\r\nSince `c = tf.matmul(a, b)` is outside the scope of ` with tf.device('/CPU:0'):` it is being placed on GPU.\r\nBut if we include `c = tf.matmul(a, b)` inside ` with tf.device('/CPU:0'):` then it will be placed on CPU as mentioned in the document [here](https://www.tensorflow.org/guide/gpu).\r\nPlease, find the [gist](https://colab.research.google.com/gist/ravikyram/ddbeac0143d637e2c71d009faf755d93/untitled277.ipynb) for your reference. Thanks!", "@ravikyram - thanks for confirming this - I had just discovered the issue with c being outside of the context manager scope :) I guess the docs need to be updated - and then this issue can be closed.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42708\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42708\">No</a>\n"]}, {"number": 42707, "title": "Post Quantization with TensorFlow in Python", "body": "I am totally beginner in Python and I have a Convolutional Neural Network which consists of some conv1D maxpooling1D and dense layers\r\n\r\nI need to Quantize this model to integer for running on FPGA\r\n\r\nI follow post-Quantization trough this link: https://www.tensorflow.org/lite/performance/post_training_integer_quant\r\n\r\nBut I got a strange error which is:\r\n\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:316 input->dims->data[3] != filter->dims->data[3] (4 != 3)Node number 1 (CONV_2D) failed to prepare\r\n\r\nBut I don\u2019t have any conv2D at all.\r\n\r\nI would be very grateful if anyone can help me.", "comments": ["@giahi,\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue and the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 42706, "title": "Memory leak when using tensorflow c++ api", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version:\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 5.5.0/c++11\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nOur project using bazel and tensorflow c++ api for inference. \r\n\r\nIn our workspace, we using \"https://github.com/tensorflow/tensorflow/archive/v1.15.0.tar.gz\".\r\n\r\nWhen inference on features repeatedly, memory keeps rising.\r\n\r\nOur model is an old style pb file, not saved model. It is generated from onnx to tensorflow project.  Our graph has two inputs and one output. Here is some key code.\r\n\r\n```\r\nclass TfModel {\r\n public:\r\n  /**\r\n   * @brief Construct.\r\n   */\r\n  TfModel() : pb_path_(xxx), input_names_{\"input1:0\", \"input2:0\"}, output_names_{\"output:0\"}) {\r\n    sess_options_.config.set_use_per_session_threads(true);\r\n    sess_options_.config.set_intra_op_parallelism_threads(0);\r\n    sess_options_.config.set_inter_op_parallelism_threads(0);\r\n\r\n    sess_.reset(tensorflow::NewSession(sess_options_));\r\n    tensorflow::GraphDef graph_def;\r\n    auto default_env = tensorflow::Env::Default();\r\n    auto status = tensorflow::ReadBinaryProto(default_env, pb_path, &graph_def);\r\n    if (!status.ok()) {\r\n      throw std::invalid_argument(\"Error!\");\r\n    }\r\n    sess_->Create(graph_def);\r\n  }\r\n\r\n  /**\r\n   * @brief Compute.\r\n   */\r\n  void Compute(const tensorflow::Tensor &tensor1,\r\n               const tensorflow::Tensor &tensor2) const {\r\n    std::vector<std::pair<std::string, tensorflow::Tensor>> tf_input;\r\n    tf_input.emplace_back(input_names_[0], tensor1);\r\n    tf_input.emplace_back(input_names_[1], tensor2);\r\n    std::vector<tensorflow::Tensor> output;\r\n    auto status = sess_->Run(tf_input, output_names_, {}, &output);\r\n  }\r\n\r\n private:\r\n  const std::string pb_path_;\r\n  const std::vector<std::string> input_names_;\r\n  const std::vector<std::string> output_names_;\r\n  std::unique_ptr<tensorflow::Session> sess_;\r\n  tensorflow::SessionOptions sess_options_;\r\n};\r\n\r\nint main() {\r\n  model = TfModel();\r\n  while (true) {\r\n    model.Compute(tensor1, tensor2);\r\n    PrintRssMemory();\r\n  }\r\n}\r\n```\r\n\r\nI have searched some solutions and try tcmalloc. The memory leak seems negligible, but is ~25% slower than before, which is not acceptable.\r\n\r\nThe memory leak problem seems very severe if I set inter_thead and intra_thead to 0. If set to 1, the problem seems disappear. But we must use larger inter_thead and intra_thead for multi-thead.\r\n\r\nI don't attach model since it's a little large. Hope anyone can help, Thanks.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo memory leak.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I'm being troubled by the same problem, and the memory still keeps growing even when I set the intra and inter multi thread num to 1. After trying lots of solutions supplied by others, nothing gets better.\r\nAny useful suggestion will be appreciated.\r\nTHX!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42706\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42706\">No</a>\n"]}, {"number": 42705, "title": "Clarify that negative values are valid for adjust_brightness.", "body": "Fixes #42704.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42705) for more info**.\n\n<!-- need_sender_cla -->", "@hwaxxer Can you please sign CLA. Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42705) for more info**.\n\n<!-- ok -->", "@hwaxxer  Can you please address Ubuntu Sanity errors? Thanks!", "@hwaxxer  Any update on this PR? Please. Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42705) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F42705) for more info**.\n\n<!-- ok -->", "@gbaned Should be fixed now!"]}, {"number": 42704, "title": "Clarify that tf.image.adjust_brightness accepts negative values", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/image/adjust_brightness\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation states that delta should be in the range [0, 1). However, the delta is added to the image so negative values in the range (-1, 0] are also valid, and useful if the image should be made darker.\r\n\r\n### Clear description\r\n\r\nIt is useful to be able to darken images, and the documentation should make it clear how to do so.", "comments": ["I wondered this myself. Since `random_adjust_brightness` provides a value in the interval `[-max_delta, max_delta)` it seems that negative values should be accepted.\r\n\r\nI wondered whether this could be an issue with underflow for unsigned integer representations? The calculation is done as a float, but scaling and casting negative results (e.g. -0.1) to uint could give some weird behaviour. You can't just set a hard limit of 0 either, because some projects scale pixel values between -1 and 1. (https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception/preprocess_input)\r\n\r\nEdit: never mind, I've just seen that the return command for `adjust_brightness` takes care of overflow/underflow with the saturate argument:\r\n`return convert_image_dtype(adjusted, orig_dtype, saturate=True)`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42704\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42704\">No</a>\n"]}, {"number": 42703, "title": "Error building tensorflow_cc.dll (Windows 10)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version: 2.3.0\r\n- Bazel version (if compiling from source): 3.1.0\r\n\r\n**Describe the problem**\r\nError building tensorflow_cc.dll\r\n```\r\nERROR: D:/tensorflow-2.3.0/tensorflow/BUILD:754:1: Linking of rule '//tensorflow:tensorflow_cc.dll' failed (Exit 1127): link.exe failed: error executing command\r\n  cd C:/users/lotte/_bazel_lotte/2enhdaow/execroot/org_tensorflow\r\n  SET LIB=c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\ATLMFC\\lib\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.18362.0\\um\\x64;\r\n    SET PATH=c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.25.28610\\bin\\HostX64\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\VCPackages;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\bin\\Roslyn;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\HTML Help Workshop;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\Tools\\;;C:\\Windows\\system32;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja;c:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\VC\\Linux\\bin\\ConnectionManagerExe\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\lotte\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n    SET TMP=C:\\Users\\lotte\\AppData\\Local\\Temp\r\n  c:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params\r\nExecution platform: @local_execution_config_platform//:platform\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nexternal\\mkl_windows\\lib\\libiomp5md.lib : fatal error LNK1127: library is corrupt\r\nTarget //tensorflow:tensorflow_cc failed to build\r\nINFO: Elapsed time: 571521.839s, Critical Path: 351212.43s\r\nINFO: 8818 processes: 8818 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\npython configure.py\r\n    empty\r\n    empty\r\n    n\r\n    n\r\n    empty\r\n    n\r\n    n\r\n\r\nbazel build -c opt --config=mkl //tensorflow:tensorflow_cc\r\n```\r\n\r\n**Edit: Possibly related to #42730 (Error building tensorflow.dll)**", "comments": ["To clarify: This is not a duplicate issue. This issue concerns tensorflow_cc.dll (C++ API) and #42730 concerns tensorflow.dll (C API). Please keep both issues open until resolved. Thanks!", "It is a duplicate of the other referenced issue. The error is exactly the same:\r\n```\r\nexternal\\mkl_windows\\lib\\libiomp5md.lib : fatal error LNK1127: library is corrupt\r\n```\r\n\r\n@claynerobison could you reassign?", "@Lotte1990 We will check it soon.", "@Lotte1990  We reproduce the same issue locally.\r\nWe are checking it now.", "@Lotte1990 \r\nCould you try tf 2.4 or latest code?\r\n\r\nThere are some fix for windows.", "@NeoZhangJianyu Yes, I am able to compile Tensorflow 2.4.0-rc1 with MKL now. Thank you! I will close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42703\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42703\">No</a>\n"]}, {"number": 42702, "title": "TFLu: Update Ethos-U kernel", "body": "Update with new TfLiteEvalTensor API.\r\nAlso resolve error, variable length array by using scratch buffer\r\nrequest/get.\r\n\r\nFIXED=166478549", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\r\n\r\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\r\nRead the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\r\nCreated a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\r\nLinked to the issue from the PR description\r\n\r\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.", "Thanks @mansnils for breaking up the work into small PRs.\r\n\r\nI have edited the PR description (by adding FIXED=166478549) as a test to see if the PR description gets pulled into our internal systems with the appropriate formatting to close internal bugs (similar to how we can close issues on github).\r\n\r\nThis is just an FYI for you, no action needed from your side."]}, {"number": 42701, "title": "Impossible to use non mirrored variables in distributed training", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\nI would like to use a non trainable variable placed on cpu in a multi-gpu training. It is useful for using a memory bank of negative examples for instance in self-supervised learning.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nlayers = tf.keras.layers\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nclass Test(layers.Layer):\r\n\r\n    def __init__(self, memory, **kargs):\r\n        super(Test, self).__init__(**kargs)\r\n        \r\n        self.memory = memory\r\n        self.dense = layers.Dense(128)\r\n\r\n    def call(self, inputs):\r\n        \r\n        res = tf.matmul(inputs, self.memory, transpose_b=True)\r\n        res = self.dense(res)\r\n\r\n        return res\r\n    \r\ndef create_model(memory):\r\n    \r\n    input_tensor = layers.Input(\r\n        shape=[128], name=\"input_tensor\"\r\n    )\r\n \r\n    x = layers.Dense(128)(input_tensor)\r\n    \r\n    output = Test(memory)(x)\r\n\r\n    return keras.Model(inputs=input_tensor, outputs=output)\r\n    \r\nwith tf.device('/cpu'):\r\n    \r\n    memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)\r\n    \r\n    \r\nwith strategy.scope():\r\n    \r\n    model = create_model(memory)\r\n    \r\nmodel.compile()\r\n\r\nmodel.summary()\r\n\r\nprint(memory)\r\n```\r\n\r\nHowever creating a variable outside strategy.scope() results in an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"code_minimal_bug_memory.py\", line 44, in <module>\r\n    model.compile()\r\n  File \"/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 538, in compile\r\n    self._validate_compile(optimizer, metrics, **kwargs)\r\n  File \"/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2512, in _validate_compile\r\n    '  model.compile(...)' % (v, strategy))\r\nValueError: Variable (<tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=\r\narray([[3.0940974e-01, 9.5359027e-01, 9.3334043e-01, ..., 2.3660135e-01,\r\n        4.1169703e-01, 7.6202512e-02],\r\n       [1.0460985e-01, 2.4617815e-01, 6.7584288e-01, ..., 8.3745670e-01,\r\n        3.0766165e-01, 6.8396461e-01],\r\n       [7.1668625e-04, 6.1034310e-01, 8.1730366e-02, ..., 3.1000912e-01,\r\n        8.4088564e-01, 3.5998344e-02],\r\n       ...,\r\n       [8.0560517e-01, 6.6493630e-02, 5.6640983e-02, ..., 2.0186901e-03,\r\n        4.2683721e-01, 6.6080117e-01],\r\n       [6.1158764e-01, 8.3118451e-01, 2.3782039e-01, ..., 9.0197289e-01,\r\n        3.4783411e-01, 5.6294477e-01],\r\n       [2.9402018e-02, 2.2738779e-01, 5.7802427e-01, ..., 7.3360741e-01,\r\n        8.2319343e-01, 1.2766552e-01]], dtype=float32)>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fef7d028710>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.\r\nwith strategy.scope():\r\n  model=_create_model()\r\n  model.compile(...)\r\n```\r\n\r\nCreating the variable memory inside the strategy.scope() works but creates a MirroredVariable replicated on every gpu which is not the desired behavior:\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nlayers = tf.keras.layers\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nclass Test(layers.Layer):\r\n\r\n    def __init__(self, memory, **kargs):\r\n        super(Test, self).__init__(**kargs)\r\n        \r\n        self.memory = memory\r\n        self.dense = layers.Dense(128)\r\n\r\n    def call(self, inputs):\r\n        \r\n        res = tf.matmul(inputs, self.memory, transpose_b=True)\r\n        res = self.dense(res)\r\n\r\n        return res\r\n    \r\ndef create_model(memory):\r\n    \r\n    input_tensor = layers.Input(\r\n        shape=[128], name=\"input_tensor\"\r\n    )\r\n \r\n    x = layers.Dense(128)(input_tensor)\r\n    \r\n    output = Test(memory)(x)\r\n\r\n    return keras.Model(inputs=input_tensor, outputs=output)\r\n    \r\n\r\nwith strategy.scope():\r\n    \r\n    with tf.device('/cpu'):\r\n    \r\n        memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)\r\n    \r\n    model = create_model(memory)\r\n    \r\nmodel.compile()\r\n\r\nmodel.summary()\r\n\r\nprint(memory)\r\n\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\nMirroredVariable:{\r\n  0: <tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=\r\narray([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,\r\n        0.48011708],\r\n       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,\r\n        0.52998424],\r\n       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,\r\n        0.7379434 ],\r\n       ...,\r\n       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,\r\n        0.5342829 ],\r\n       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,\r\n        0.38599086],\r\n       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,\r\n        0.71457684]], dtype=float32)>,\r\n  1: <tf.Variable 'Variable/replica_1:0' shape=(100, 128) dtype=float32, numpy=\r\narray([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,\r\n        0.48011708],\r\n       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,\r\n        0.52998424],\r\n       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,\r\n        0.7379434 ],\r\n       ...,\r\n       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,\r\n        0.5342829 ],\r\n       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,\r\n        0.38599086],\r\n       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,\r\n        0.71457684]], dtype=float32)>,\r\n  2: <tf.Variable 'Variable/replica_2:0' shape=(100, 128) dtype=float32, numpy=\r\narray([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,\r\n        0.48011708],\r\n       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,\r\n        0.52998424],\r\n       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,\r\n        0.7379434 ],\r\n       ...,\r\n       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,\r\n        0.5342829 ],\r\n       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,\r\n        0.38599086],\r\n       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,\r\n        0.71457684]], dtype=float32)>,\r\n  3: <tf.Variable 'Variable/replica_3:0' shape=(100, 128) dtype=float32, numpy=\r\narray([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,\r\n        0.48011708],\r\n       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,\r\n        0.52998424],\r\n       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,\r\n        0.7379434 ],\r\n       ...,\r\n       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,\r\n        0.5342829 ],\r\n       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,\r\n        0.38599086],\r\n       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,\r\n        0.71457684]], dtype=float32)>\r\n}\r\n\r\n```\r\nThis is a MirroredVariable replicated on each gpu and not a Variable placed on CPU as I would like.", "comments": ["I am able to replicate the issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/59b46c713294c647e379e380b350d91e/untitled397.ipynb).", "Hi @guillaumelorre28, this functionality is not supported, as you point out all of the variables get mirrored with `MirroredStrategy`. Can you provide more information on why you would want the non trainable variables on your CPU instead of having them mirrored across your GPUs? In MirroredStrategy, each GPU gets a copy of the model, and then computes the forwards and backwards pass to compute gradients. The gradients are then aggregated across each GPU and reduced. All of this computation happens efficiently between the GPUs, and the CPU is just responsible for passing new batches of your training data. So if your `memory` variables are stored on CPU, each time your GPUs do the forward pass the `memory` tensors would need to be copied over from the CPU, or that specific computation would have to happen on the CPU, neither of which seem very efficient to do in the middle of each training step.\r\n\r\nAlternatively, if you don't want any of the variables mirrored, and want them all stored on your CPU, you could try `CentralStorageStrategy` which provides synchrnous training with mirroring variables across devices since it's the operations that are replicated across all local GPUs.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42701\">No</a>\n"]}, {"number": 42700, "title": "error: undefined reference to '__umoddi3'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Loongnix OS(based on Fedora) with kernel:Linux localhost.localdomain 3.10.84-22.fc21.loongson.7.mips64el\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.4.0\r\n- Python version: Python\uff1a2.7.9      Python3\uff1a3.4.1\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):3.4.1\r\n- GCC/Compiler version (if compiling from source):gcc version 7.3.1 20180303 (Red Hat 7.3.1-6) (GCC)\r\n- CUDA/cuDNN version:no cuda\r\n- GPU model and memory: no gpu mode \r\n\r\n\r\n\r\n**Describe the problem**\r\nwhen I build the tensorflow 2.4.0 source code on loongson platform\uff08arch\uff1amips64\uff09use bazel 3.4.1.\r\n[loongson@localhost tensorflow-206]$ ./configure \r\nYou have bazel 3.4.1- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: \r\nFound possible Python library paths:\r\n  /usr/lib64/python3.4/site-packages\r\n  /usr/lib/python3.4/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib64/python3.4/site-packages]\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n\r\nand then build:    bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbuild  failed! log as below:\r\nRepository rule git_repository defined at:\r\n  /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (161 packages loaded, 27000 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 452.104s, Critical Path: 44.81s\r\nINFO: 307 processes: 307 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["could you please help me to resolve this build error? \r\nThank you!", "@zhangqiang-hf \r\n\r\nLooking the error log it seems you are using python 3.4 .As per the tested build configurations Tensorflow supports python 3.5-3.8.\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#tested_build_configurations) and make sure you have installed all compatible dependencies.Thanks!", "> @zhangqiang-hf\r\n> \r\n> Looking the error log it seems you are using python 3.4 .As per the tested build configurations Tensorflow supports python 3.5-3.8.\r\n> Please, see tested build configurations from [here](https://www.tensorflow.org/install/source#tested_build_configurations) and make sure you have installed all compatible dependencies.Thanks!\r\n\r\nDear sir,\r\n  Thank you for your reply! The build failed \"undefined reference to '__moddi3'\" and so on, is caused by python version?\r\n I searched the same issue\"undefined reference to '__moddi3'\" from the internet, it said add \"-fno-tree-scev-cprop\" \r\n   http://www.voidcn.com/article/p-vphyzuwg-py.html", "but it is still built fail\r\n\r\n> > @zhangqiang-hf\r\n> > Looking the error log it seems you are using python 3.4 .As per the tested build configurations Tensorflow supports python 3.5-3.8.\r\n> > Please, see tested build configurations from [here](https://www.tensorflow.org/install/source#tested_build_configurations) and make sure you have installed all compatible dependencies.Thanks!\r\n> \r\n> Dear sir,\r\n> Thank you for your reply! The build failed \"undefined reference to '__moddi3'\" and so on, is caused by python version?\r\n> I searched the same issue\"undefined reference to '__moddi3'\" from the internet, it said add \"-fno-tree-scev-cprop\"\r\n> http://www.voidcn.com/article/p-vphyzuwg-py.html  \r\nbut it is still built fail\r\n\r\n", "Dear Sir\uff0c\r\nI have used the python3.7.9, but still failed! log as bellow:\r\n[loongson@localhost tensorflow]$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=171\r\nINFO: Reading rc options for 'build' from /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc:\r\n  'build' options: --action_env=BAZEL_LINKLIBS=-l%:///opt/rh/devtoolset-7/root/usr/lib/gcc/mips64el-redhat-linux/7/libstdc++.a --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/loongson/tensorflow/tensorflow-206/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/opt/python/python-3.7.9/lib/python3.7/site-packages --python_path=/usr/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:opt in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:linux in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/loongson/tensorflow/tensorflow-206/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (387 packages loaded, 30985 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 401.100s, Critical Path: 43.80s\r\nINFO: 242 processes: 242 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nPlease give me some suggestion, thanks a lot\uff01", "Dear Sir\uff0c\r\nfrom the website \"https://www.tensorflow.org/install/source\", it tells \"\u60a8\u53ef\u4ee5\u4ece\u6e90\u4ee3\u7801\u6784\u5efa TensorFlow pip \u8f6f\u4ef6\u5305\u5e76\u5c06\u5176\u5b89\u88c5\u5728 Ubuntu Linux \u548c macOS \u4e0a\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u8bf4\u660e\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u7cfb\u7edf\uff0c\u4f46\u4ec5\u9488\u5bf9 Ubuntu \u548c macOS \u8fdb\u884c\u4e86\u6d4b\u8bd5\u5e76\u5728\u8fd9\u4e24\u79cd\u5e73\u53f0\u4e0a\u53d7\u652f\u6301\u3002\"\r\n \r\nit means the tensorflow only can be build on Ubuntu or macOS? our OS is loongnix OS,it is based on Fedora OS", "looks like a protobuf issue? @jvishnuvardhan do we have a contact in protobuf team to look into this?", "Actually, this looks like it is being built on mips64el architecture. We do not have support for this system yet, and have no experience. We welcome any patches, but I am afraid I do not have experience with this architecture. I will mark this as \"community support\" You may want to reach out to build@tensorflow.org", "Dear Sirs\uff0c\r\n  Please help me to resolve this build fail issue, it cost us a few days...\r\nThe build failed logs  is as above!", "I think that you need to link `-lgcc` in the receipts.", "@zhangqiang-hf,\r\n\r\nCan you take a look at this above [comment](https://github.com/tensorflow/tensorflow/issues/42700#issuecomment-685935063) and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42700\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42700\">No</a>\n"]}, {"number": 42699, "title": "[TFLite 16x8, documentation] Added section on 16x8 quantization scheme to model-optimization.", "body": "In this PR the section on the 16x8 quantization scheme is added to model-optimization overview document.\r\n", "comments": ["Hi @jdduke Corrected, could you please re-approve ? thanks."]}, {"number": 42698, "title": "Tensorflow & Keras update causes significantly worse model performance. ", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhile doing research, I make sure to architect it to be reproducible and have quality assurance checks. When using Google Colab which has the most recent version of Tensorflow and Keras, I noticed that the loss was dramatically worse for the exact same code, as referenced here, https://github.com/hammad93/hurricane-net/issues/7\r\n\r\nThe solution was to downgrade Keras to 2.2 although I think 2.3 would work too. I'm not sure if it's the learning rate because even after increasing the epochs, I wasn't able to achieve the same accuracy. I won't be able to switch over to Tensorflow or the newest version of Keras. Just for reference, the model loss as reported by mean squared error is supposed to be 0.31 but the lowest I have been able to get with the exact same code is 0.40 and doing a gaussian hyper parameter search does not fix it. \r\n\r\nHere's the notebook for reference, https://github.com/hammad93/hurricane-net/blob/master/hurricane_net.ipynb", "comments": ["@hammad93,\r\nOn running the code with TF v2.3 and Keras 2.4.3, I did get the loss value upto 0.3291. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/18ab5a3df44ef2d62bf391eaa887f15a/hurricane_net.ipynb#scrollTo=0ETjdPZJTA6H).\r\n  \r\nHowever, running the code with Keras 2.2 take much longer. Each epoch takes ~10s when compared to less than 0s on TF v2.3, take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/df0b2de345afd8e96e4471335f31111a/tf-1-14-and-keras-2-2.ipynb) for reference. \r\n\r\nAlso, the example you have provided is fairly complex. Can you remove the dependencies and get the example down to the simplest possible repro? So that it will be easier to debug the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]