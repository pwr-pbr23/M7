[{"number": 19574, "title": "Fix build error", "body": "Fix build error: 'function' in namespace 'std' does not name a template type.\r\n\r\nWhen build tensorflow from source on ubuntu 18.04 with g++-7.3, the error messages below appears.\r\nAdd `<functional>` header solves this error.\r\n\r\n```\r\nERROR: /home/guo/Github/tensorflow/tensorflow/python/BUILD:310:1: C++ compilation of rule '//tensorflow/python:cpp_python_util' failed (Exit 1)\r\ntensorflow/python/util/util.cc:276:16: error: 'function' in namespace 'std' does not name a template type\r\n     const std::function<int(PyObject*)>& is_sequence_helper,\r\n                ^~~~~~~~\r\ntensorflow/python/util/util.cc:276:24: error: expected ',' or '...' before '<' token\r\n     const std::function<int(PyObject*)>& is_sequence_helper,\r\n                        ^\r\ntensorflow/python/util/util.cc: In function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)':\r\ntensorflow/python/util/util.cc:280:16: error: 'is_sequence_helper' was not declared in this scope\r\n   int is_seq = is_sequence_helper(nested);\r\n                ^~~~~~~~~~~~~~~~~~\r\ntensorflow/python/util/util.cc:280:16: note: suggested alternative: 'IsSequenceHelper'\r\n   int is_seq = is_sequence_helper(nested);\r\n                ^~~~~~~~~~~~~~~~~~\r\n                IsSequenceHelper\r\ntensorflow/python/util/util.cc:288:8: error: 'next_values_getter' was not declared in this scope\r\n   if (!next_values_getter(nested, &next_values)) return false;\r\n        ^~~~~~~~~~~~~~~~~~\r\ntensorflow/python/util/util.cc:288:8: note: suggested alternative: 'next_values'\r\n   if (!next_values_getter(nested, &next_values)) return false;\r\n        ^~~~~~~~~~~~~~~~~~\r\n        next_values\r\ntensorflow/python/util/util.cc:295:61: error: 'next_values_getter' was not declared in this scope\r\n         FlattenHelper(item.get(), list, is_sequence_helper, next_values_getter);\r\n                                                             ^~~~~~~~~~~~~~~~~~\r\ntensorflow/python/util/util.cc:295:61: note: suggested alternative: 'next_values'\r\n         FlattenHelper(item.get(), list, is_sequence_helper, next_values_getter);\r\n                                                             ^~~~~~~~~~~~~~~~~~\r\n                                                             next_values\r\ntensorflow/python/util/util.cc:297:10: error: in argument to unary !\r\n     if (!success) {\r\n          ^~~~~~~\r\ntensorflow/python/util/util.cc: In function 'PyObject* tensorflow::swig::Flatten(PyObject*)':\r\ntensorflow/python/util/util.cc:472:66: error: invalid conversion from 'int (*)(PyObject*) {aka int (*)(_object*)}' to 'int' [-fpermissive]\r\n   if (FlattenHelper(nested, list, IsSequenceHelper, GetNextValues)) {\r\n                                                                  ^\r\ntensorflow/python/util/util.cc:472:66: error: too many arguments to function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)'\r\ntensorflow/python/util/util.cc:274:6: note: declared here\r\n bool FlattenHelper(\r\n      ^~~~~~~~~~~~~\r\ntensorflow/python/util/util.cc: In function 'PyObject* tensorflow::swig::FlattenForData(PyObject*)':\r\ntensorflow/python/util/util.cc:485:41: error: invalid conversion from 'int (*)(PyObject*) {aka int (*)(_object*)}' to 'int' [-fpermissive]\r\n                     GetNextValuesForData)) {\r\n                                         ^\r\ntensorflow/python/util/util.cc:485:41: error: too many arguments to function 'bool tensorflow::swig::{anonymous}::FlattenHelper(PyObject*, PyObject*, int)'\r\ntensorflow/python/util/util.cc:274:6: note: declared here\r\n bool FlattenHelper(\r\n      ^~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n```", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 19573, "title": "UnicodeDecodeError from tf.train.import_meta_graph", "body": "I serialized a Tensorflow model with the following code ...\r\n\r\n    save_path = self.saver.save(self.session, os.path.join(self.logdir, \"model.ckpt\"), global_step)\r\n    logging.info(\"Model saved in file: %s\" % save_path)\r\n\r\n... and I'm now trying to restore it from scratch in a separate file using the following code:\r\n\r\n    saver = tf.train.import_meta_graph(PROJ_DIR + '/logs/default/model.ckpt-54.meta')\r\n    session = tf.Session()\r\n    saver.restore(session, PROJ_DIR + '/logs/default/model.ckpt-54')\r\n    print('Model restored')\r\n\r\nWhen `tf.train.import_meta_graph` is called, the following exception is thrown:\r\n\r\n    [libprotobuf ERROR google/protobuf/io/coded_stream.cc:207] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\r\n    Traceback (most recent call last):\r\n      File \"/home/reid/projects/research/ccg/taggerflow_modified/test/tf_restore.py\", line 4, in <module>\r\n    saver = tf.train.import_meta_graph(PROJ_DIR + '/logs/default/model.ckpt-54.meta')\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1711, in import_meta_graph\r\n    read_meta_graph_file(meta_graph_or_file), clear_devices)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1563, in read_meta_graph_file\r\n    text_format.Merge(file_content.decode(\"utf-8\"), meta_graph_def)\r\n      File \"/usr/lib/python2.7/encodings/utf_8.py\", line 16, in decode\r\n    return codecs.utf_8_decode(input, errors, True)\r\n    UnicodeDecodeError: 'utf8' codec can't decode byte 0xa7 in position 1: invalid start byte\r\n\r\nFor reference, here's the first few lines of `<PROJ_DIR>/logs/default/model.ckpt-54.meta`:\r\n\r\n    <A7>:^R<A4>:\r\n    9\r\n    ^CAdd^R^F\r\n    ^Ax\"^AT^R^F\r\n    ^Ay\"^AT^Z^F\r\n    ^Az\"^AT\"^Z\r\n    ^AT^R^Dtype:^O\r\n    ^M2^K^S^A^B^D^F^E^C    ^R^G\r\n\r\nI think that Tensorflow is using a different encoding when serializing vs when deserializing. How do we specify the encoding that Tensorflow uses when serializing/deserializing? Or is the solution something different?\r\n\r\nThis was originally a Stack Overflow question. Someone asked me to make it a GitHub issue. I'm using version r0.11.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@reid-fu \r\nI was facing the same issue. Have you ensured that apart from the\r\n\r\n> .meta, .data-00000-of-00001 and the .index files\r\n\r\nthe file named \r\n\r\n> 'checkpoint'\r\n\r\n too is there in the directory from which you're loading the model?\r\n\r\nMy issue got resolved after I made sure of this. Hope this helps!\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "For the moment, it's no longer an issue. I was able to deserialize Tensorflow models copied from a different machine. This issue may resurface later, but it's OK to close this thread.", "This is still an issue. I've encountered the same problem.\r\n\r\nCurrent setup:\r\ntensorflow-gpu 1.12\r\nLinux Mint 19\r\n\r\nReproduce the issue by loading the xview2018 dataset vanilla checkpoint files, results in:\r\n\r\n`UnicodeDecodeError: 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte`", "> \r\n> \r\n> This is still an issue. I've encountered the same problem.\r\n> \r\n> Current setup:\r\n> tensorflow-gpu 1.12\r\n> Linux Mint 19\r\n> \r\n> Reproduce the issue by loading the xview2018 dataset vanilla checkpoint files, results in:\r\n> \r\n> `UnicodeDecodeError: 'utf-8' codec can't decode byte 0x91 in position 1: invalid start byte`\r\n\r\nI have the exact same issue with the same model. Have you been able to solve it?", "@jvliai yes, you have to use the training and eval scripts included in the Tensorflow Object Detection API. I am not sure how they read the files but using the SSD training and eval script with the config files set to load from the checkpoints seems to work."]}, {"number": 19572, "title": "[Feature] tf.reduce_dims() to reduce dimension as opposite to tf.expand_dims()", "body": "### Describe the problem\r\nIt seems that there is no `tf.reduce_dims()` (the opposite of `tf.expand_dims()`). This functionality will be useful in the following scenario:\r\n\r\n```python\r\n# I want to reduce the last dimension \r\n[[1,2,3], [4,5,6], [7,8,9]]\r\n# such that it becomes\r\n[1,2,3,4,5,6,7,8,9]\r\n```\r\n\r\nI know this can be done with `tf.reshape()` but\r\n 1. that requires extra `tf.shape()` for dynamic dimensions\r\n 2. `tf.expand_dims()` can also be implemented with `tf.reshape()` but we have `tf.expand_dims()`\r\n\r\nWould it be a bad idea to implement `tf.reduce_dims()`?\r\n", "comments": ["1. Your example makes shape [3, 3] become shape [9]. This is NOT the opposite of `expand_dims`. As said in the API document, `expand_dims` \"__Inserts__ a dimension of 1 into a tensor's shape.\".\r\n\r\n2. `tf.squeeze` is an opposite of `expand_dims`. As mentioned in the API document.\r\nhttps://www.tensorflow.org/api_docs/python/tf/expand_dims", "@ppwwyyxx My example make `(3,3)` become `(9, )`, not `(9, 1)`.", "Sorry that was a mistake and I've edited my comment 5 hours ago.", "@ppwwyyxx  Though not the opposite, would `tf.reduce_dims()` be useful?", "If not the opposite, you need to define what it is then, before others could possibly comment whether it's useful.\nAn example alone does not define an operation.", "I took a look at the `tf.expand_dims()` implementation, and is seems that it is more than just a thin wrapper around `tf.reshape`. I think barring a very compelling reason it's simply not worth the additional API glut and maintenance cost of adding `reduce_dims`. As @walkerlala points out, for most cases `tf.squeeze()` is sufficient, which also makes the addition of another similar op less attractive.\r\n\r\nThanks for putting forward the idea though."]}, {"number": 19571, "title": "Allow full deallocation of GPU memory - like in Catboost", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno (slightly modified)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.8.0 <pip>\r\n- **Python version**: \r\nPython version: 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) \r\n[GCC 7.2.0]\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0.176 / 7.1.2\r\n- **GPU model and memory**:\r\nGPU[0] GeForce GTX 1080 Ti\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\nimport numpy\r\nimport matplotlib.pyplot as plt\r\nimport sklearn.datasets.samples_generator as sample_gen\r\nrng = numpy.random\r\n\r\n# Parameters\r\nlearning_rate = 0.01\r\ntraining_epochs = 500\r\ndisplay_step = 50\r\n\r\n# Training set\r\nn_features=1\r\nn_samples=100\r\n\r\n# Training Data\r\n# train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\r\n#                          7.042,10.791,5.313,7.997,5.654,9.27,3.1])\r\n# train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\r\n#                          2.827,3.465,1.65,2.904,2.42,2.94,1.3])\r\ntrain_X, train_Y = sample_gen.make_regression(n_samples=n_samples, n_features=n_features, random_state=0)\r\n# n_samples = train_X.shape[0]\r\n\r\n# tf Graph Input\r\nX = tf.placeholder(\"float\")\r\nY = tf.placeholder(\"float\")\r\n\r\n# Set model weights\r\nW = tf.Variable(rng.randn(), name=\"weight\")\r\nb = tf.Variable(rng.randn(), name=\"bias\")\r\n\r\n# Construct a linear model\r\npred = tf.add(tf.multiply(X, W), b)\r\n\r\n# Mean squared error\r\ncost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\r\n# Gradient descent\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\r\n\r\n# Initialize the variables (i.e. assign their default value)\r\ninit = tf.global_variables_initializer()\r\n\r\n# Start training\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # Fit all training data\r\n    for epoch in range(training_epochs):\r\n        for (x, y) in zip(train_X, train_Y):\r\n            sess.run(optimizer, feed_dict={X: x, Y: y})\r\n\r\n        #Display logs per epoch step\r\n        if (epoch+1) % display_step == 0:\r\n            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\r\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \r\n                \"W=\", sess.run(W), \"b=\", sess.run(b))\r\n\r\n    print(\"Optimization Finished!\")\r\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\r\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\r\n    \r\n    #Graphic display\r\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\r\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\r\n    plt.legend()\r\n    plt.show()\r\n    plt.show()\r\n```\r\n### Describe the problem\r\nSame issue as #15880 here, with a fully reproducible example using latest TF 1.8 with CUDA 9.0 and cuDNN 7.1 on Ubuntu 16.04. So same old story, but this time I'm giving you a model solution to GPU memory management -  the [Catboost library](https://github.com/catboost/catboost) by Yandex.\r\n\r\nI confirm that Tensorflow does not release GPU memory after preallocating most of the available VRAM \r\n(leaving only a few percent free). This memory should be freed by TF immediately after use for other modeling frameworks to use, so it is a bug that needs to be repaired. To see how it is done, refer to how Catboost manages GPU resources.\r\n\r\nIf you are using Jupyter Notebook, restarting python kernel is relatively easy and it \"solves\" the problem, but of course at the cost of losing all your data loaded to CPU memory.\r\n\r\n", "comments": ["For fellow TF users here I illustrate - using the sample code provided above - the two currently available methods to minimize and limit GPU memory usage (which can be combined together):\r\n- allow_growth and \r\n- per_process_gpu_memory_fraction:\r\n\r\n```\r\n# Initialize the variables (i.e. assign their default value)\r\ninit = tf.global_variables_initializer()\r\n\r\n# Limit GPU memory use\r\ncfg = tf.ConfigProto()\r\n# allow dynamic GPU memory allocation (increased as needed,\r\n# but not released once allocated - requiring python kernel restart)\r\ncfg.gpu_options.allow_growth=True\r\n# define the hard limit on GPU memory allocation\r\n# (caution: can cause OOM errors)\r\ncfg.gpu_options.per_process_gpu_memory_fraction = 0.20\r\n\r\n# Start training (notice we used custom config)\r\nwith tf.Session(config=cfg) as sess:\r\n    sess.run(init)\r\n    [..]\r\n\r\n```\r\n\r\nNote: to induce python's GPU memory usage to amounts visible in nvidia-smi (larger than the default minimum of 200 MiB used by most frameworks) we need to increase the number of samples in the example above to at least 20k:\r\n```\r\nn_samples=20000\r\n```\r\n", "I'm not sure whether to categorize this as a duplicate of #15880 or an independent feature request.  My best understanding is that you simply want a lightweight solution to shrinking the GPU memory allocation without completely killing the TF process.  Towards that end, I think @zheng-xq 's suggestion about extending Session.reset() sounds most promising.    I don't understand how catboost provides a model solution.  I'm not familiar with that program.", "It is a duplicate with a twist. I'm not calling for lower utilization of GPU - it is perfectly fine to claim all of it, if there is an option to limit the usage to a certain percentage (per_process_gpu_memory_fraction). What's useful in catboost's approach is the idea to deallocate GPU memory after returning results from the GPU back to CPU. BTW you have already something in common with catboost: you are the only two GPU-enabled frameworks (among the 12 or so I've tested for our containers at work) that give such control over GPU memory usage to the final user - something that even docker is not allowing us to do.\r\n\r\nI noticed that it is customary among deep learning frameworks to create such GPU memory leaks (which it is) - none of them release GPU memory, requiring users to restart python kernel to release memory. Incidentally, none apart from Tensorflow claims nearly all of it, but that's what's so special about you:). In contrast to DNNs, GBDT frameworks tend to release what they no longer need - that applies not only to catboost but also to lightgbm. \r\n\r\nIMO it would be good to contact catboost's GPU guy, Vasily ([Noxomo](https://github.com/Noxoomo)) from Yandex team, because catboost manages to not only allocate 95% of GPU memory but also to use 100% of GPU compute power as well (despite sequential nature of boosting itself), which I've seen only in one DNN - the sadly neglected Theano... ", "Nagging Assignee @poxvoculi: It has been 18 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As pointed out in the #15880 thread, it's not plausible to immediately and automatically release GPU memory after GPU ops complete because TF has persistent state objects (Variables) that live in GPU memory and are not restricted to any special sub-region.\r\n\r\n", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @poxvoculi: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "But there is certainly excessive greediness in Tensorflow\u2019s memory allocation: it claims all RAM on all available GPU\u2019s, but only uses by default one GPU \u2013 the one with the lowest ID.", "https://www.tensorflow.org/guide/using_gpu", "Allocating video memory on all GPUs but using only 1 GPU's cores looks like a bug that could be addressed (by setting up less greedy defaults) - it is particularly important because multi-tenant environments such as OpenShift do not allow us to limit video RAM usage or GPU cores usage (unlike standard CPU case), so the current approach leads to blocking all resources by just one user operating on default settings.\r\n![image](https://user-images.githubusercontent.com/36706320/46210685-b3abe180-c331-11e8-8257-241cb010eb42.png)\r\n"]}, {"number": 19570, "title": "[Bug] Very slow operation of Cumsum in tensorflow 1.6", "body": "It seems in tensorflow the tf.cumsum operation is extremely slow on GPU, and takes a huge amount of time (~99% of the total time).\r\n\r\nIn Pytorch, as expected, the sort operation is the one that takes the most time, cumsum is virtually instant on GPU.\r\n\r\nI give an issues on other\u2018s\u2019 gitHub, we get conclusion that is cumsum is very slow aginst pytorch.\r\n \r\nwe issue is https://github.com/bermanmaxim/LovaszSoftmax/issues/6\r\n\r\ncode is https://github.com/bermanmaxim/LovaszSoftmax/tree/master/tensorflow\r\n\r\n```\r\ndef lovasz_grad(gt_sorted):\r\n    \"\"\"\r\n    Computes gradient of the Lovasz extension w.r.t sorted errors\r\n    See Alg. 1 in paper\r\n    \"\"\"\r\n    gts = tf.reduce_sum(gt_sorted)\r\n    **intersection = gts - tf.cumsum(gt_sorted)\r\n    union = gts + tf.cumsum(1. - gt_sorted)**\r\n    jaccard = 1. - intersection / union\r\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\r\n    return jaccard\r\n```", "comments": ["![20180526173149](https://user-images.githubusercontent.com/18137551/40574769-e04b3f30-610a-11e8-8308-e162e7365a16.png)\r\n", "@yjl9122 Can you provide the extra details about your setup as well as full code to replicate in docker from the [New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)?", "@angersson \r\nI use tensorflow1.6 and 1.8, gpu is tesla P40.\r\n", "```\r\nIn [1]:\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\nIn [2]:\r\nN = 5 * 500 * 500\r\nIn [3]:\r\nx = np.array(np.random.randn(N), dtype=np.float32)\r\npytorch ops\r\nIn [4]:\r\nimport torch\r\nIn [9]:\r\n%%timeit -o\r\nXt = torch.tensor(x).cuda()\r\ntorch.cuda.synchronize()\r\n1.36 ms \u00b1 48.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\nOut[9]:\r\n<TimeitResult : 1.36 ms \u00b1 48.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)>\r\nIn [10]:\r\npytorch = {}\r\nIn [11]:\r\ntorch_gputransfert = np.array(_.timings)\r\nIn [12]:\r\n%%timeit -o\r\nXt = torch.tensor(x).cuda()\r\ntorch.cumsum(Xt, 0)\r\ntorch.cuda.synchronize()\r\n1.44 ms \u00b1 11.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\nOut[12]:\r\n<TimeitResult : 1.44 ms \u00b1 11.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)>\r\nIn [13]:\r\npytorch['cumsum'] = np.array(_.timings) - torch_gputransfert\r\nIn [14]:\r\n%%timeit -o\r\nXt = torch.tensor(x).cuda()\r\ntorch.sort(Xt, 0)\r\ntorch.cuda.synchronize()\r\n5.21 ms \u00b1 75.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\nOut[14]:\r\n<TimeitResult : 5.21 ms \u00b1 75.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)>\r\nIn [15]:\r\npytorch['sort'] = np.array(_.timings) - torch_gputransfert\r\nTensorflow ops\r\nIn [16]:\r\nX = tf.placeholder(tf.float32, shape=(None,), name=None)\r\nY = tf.cumsum(X)\r\nZ = tf.nn.top_k(X, tf.shape(X)[0])\r\nA = tf.identity(X)\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\n\r\nsess = tf.Session(config=config)\r\nIn [17]:\r\n%%timeit -o\r\nsess.run(A, feed_dict={X: x})\r\n1.59 ms \u00b1 44.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\nOut[17]:\r\n<TimeitResult : 1.59 ms \u00b1 44.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)>\r\nIn [18]:\r\ntensf = {}\r\nIn [19]:\r\ntf_gputransfert = np.array(_.timings)\r\nIn [20]:\r\n%%timeit -o\r\nsess.run(Y, feed_dict={X: x})\r\n330 ms \u00b1 1.36 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\r\nOut[20]:\r\n<TimeitResult : 330 ms \u00b1 1.36 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)>\r\nIn [21]:\r\ntensf['cumsum'] = np.array(_.timings) - tf_gputransfert\r\nIn [22]:\r\n%%timeit -o\r\nsess.run(Z, feed_dict={X: x})\r\n10.9 ms \u00b1 36.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\nOut[22]:\r\n<TimeitResult : 10.9 ms \u00b1 36.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)>\r\nIn [23]:\r\ntensf['sort'] = np.array(_.timings) - tf_gputransfert\r\nSummary\r\nIn [40]:\r\nr = 1/2\r\nx = np.arange(len(pytorch))\r\nfuns = pytorch.keys()\r\n\r\nplt.bar(x - r/4, [v.mean() for v in pytorch.values()], width=r/2, yerr=[v.std() for v in pytorch.values()], label='pytorch')\r\nplt.bar(x + r/4, [v.mean() for v in tensf.values()], width=r/2, yerr=[v.std() for v in tensf.values()], label='tensorflow')\r\nplt.legend()\r\nplt.gca().set_yscale('log')\r\nplt.xticks(x, funs);\r\nplt.ylabel('time (s)')\r\nplt.savefig('timings.svg')\r\nplt.title('timings ({} pixels)'.format(N))\r\nOut[40]:\r\nText(0.5,1,'timings (1250000 pixels)')\r\n```", "the code is provided at [https://github.com/bermanmaxim/LovaszSoftmax/blob/master/tensorflow/profile_ops.ipynb](url)\r\n\r\nso you can use it", "@angersson Any news?", "Thanks for the update and the code, @yjl9122. @zheng-xq, can you take a look at this?", "Having the same issue here with cumsum", "@angersson @zheng-xq Any news\r\n", "No news @tensorflowbutler @angersson @zheng-xq ", "Nagging Assignee @zheng-xq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Was this ever resolved? I'm also running into performance issues using tf.cumsum on GPU", "@yjl9122 Have you solved this? I am also facing this issue when using LovaszSoftmax. Thx.", "Up", "Should be reasonably fixed now.", "As of Tensorflow 2.2 this op is still extremely slow on GPU with tf.complex128 inputs\r\n\r\nSplitting into real and imag parts and doing the cumsums separately, then merging to complex again did the trick, as this sped up my forward propagation by a factor of 10."]}, {"number": 19569, "title": "I am not able to install tensorflow through anaconda it is showing the following error.....", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n![tff](https://user-images.githubusercontent.com/32293756/40574201-1316f586-60ec-11e8-8349-113b722f788a.png)\r\n![Uploading tff1.png\u2026]()\r\n", "comments": ["Now, You can use `conda install -c hesi_m keras` to install the latest version of **Keras (2.2.0)** and **Tensorflow(1.8.0)** for you."]}, {"number": 19568, "title": "sess.run([train_step]) freezes when using batch_normalization and Collection Update", "body": "Hi,\r\n\r\nI have faced a strange situation with Tensorflow. I explored a lot about this problem but just found one other thread (unsolved) on StackOverflow. (here: https://stackoverflow.com/questions/47047124/tf-layers-batch-normalization-freezes-during-sess-run-1-5-0-dev20171031). So, I decided to ask it here.\r\n\r\nBasically, when I call sess.run(), it freezes. By freeze, I mean, the GPU utilization is zero, I get no errors and the process is on GPU (GPU memory is allocated).  I have the following code segment:\r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n  with tf.control_dependencies(update_ops):\r\n        train_step1 = optimizer1.minimize(loss = loss_fill +lossL2+ loss_detection,var_list=vars)\r\n```\r\n\r\nWhen I change this part to:\r\n```\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n     with tf.control_dependencies(update_ops):\r\n             pass\r\ntrain_step1 = optimizer1.minimize(loss = loss_fill +lossL2+ loss_detection,var_list=vars)\r\n```\r\n\r\nwhich basically ignores some necessary moving average updates, it doesn't freeze anymore. I have a lot of tf.layers.batch_normalization() instances in my code and this is the first time I am facing this issue.\r\n\r\nThanks\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It is my custom code. One simple example is provided on the StackOverflow link that I shared, but my own script is much more complicated.\r\nUbuntu 16.04\r\nTensorflow 1.8.0 - Pip install\r\nCuda 9.0, Cudnn 7\r\nTitan X, 12 G\r\n", "I'm seeing this issue as well. Here's my configuration:\r\n\r\nUbuntu 16.04.5\r\nTF 1.9.0 (v1.9.0-0-g25c197e) from source\r\nBazel 0.15.2\r\nCUDA Version 9.1.85\r\ncuDNN 7.1.3\r\nGeForce GTX 1080 Ti 11GB\r\n\r\nThe following sample code demonstrates the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nBATCH_SIZE = 1\r\nTIME_STEPS = 32  # no lock up with TIME_STEPS < 32\r\n\r\nmemory = tf.placeholder(tf.float32, [BATCH_SIZE, 1, 3])\r\n\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n  tf.contrib.rnn.LSTMCell(3),\r\n  tf.contrib.seq2seq.BahdanauAttention(3, memory),\r\n)\r\n\r\ninputs = np.zeros((BATCH_SIZE, TIME_STEPS, 1), dtype=np.float32)\r\nhelper = tf.contrib.seq2seq.TrainingHelper(inputs, [TIME_STEPS])\r\ninitial_state = cell.zero_state(BATCH_SIZE, tf.float32)\r\ndecoder = tf.contrib.seq2seq.BasicDecoder(cell, helper, initial_state)\r\n\r\nfinal_output, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\r\n\r\nres = final_output.rnn_output\r\nres = tf.layers.batch_normalization(res, training=True)\r\n\r\nglobal_step = tf.train.get_or_create_global_step()\r\noptimizer = tf.train.AdamOptimizer()\r\n\r\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n  grads_and_vars = optimizer.compute_gradients(res)\r\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(train_op, feed_dict={memory: [[[1, 2, 3]]]})\r\n```\r\n\r\nThere seems to be a connection between the number of decoder steps and batch normalization. The issue is reproducible with `TIME_STEPS >= 32` but not with `TIME_STEPS < 32`.\r\n\r\n@mrry, could you please help find someone to triage this issue? Thanks!", "Friendly ping. Checking in to see if we can get this bug triaged.", "same for me,  work around with something like:  \r\n```python\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) \r\ntrain_op = optimizer.minimize(loss)\r\n....\r\nsession.run([update_ops , train_op, ...])\r\n```\r\n\r\n", "@raofengyun thanks for the suggestion! You're right that eliminating the `control_dependency` will prevent the lockup from occurring. However, `update_ops` isn't guaranteed to complete before `train_op` even if it's placed first in the `session.run` call unless that control dependency is introduced.\r\n\r\nI've also noticed that the above code snippet runs to completion when executing on CPU.", "@tfboyd", "@sharvil you're right, it's not guaranteed, but I can't find any other way. ", "@asimshankar, @tfboyd any update on this issue?", "@sharvil : Apologies for the late response. Nope, no update yet, haven't gotten around to debugging this yet. \r\n\r\n@zheng-xq @chsigg - would either of you have some cycles to look into this?\r\n@drpngx @qlzh727 have been looking at the LSTMCell implementations recently and may also have some insight.\r\n\r\nFor the record, if I understand correctly, the following reproduces the problem (picked up from the [StackOverflow question](https://stackoverflow.com/questions/47047124/tf-layers-batch-normalization-freezes-during-sess-run-1-5-0-dev20171031), haven't tried it myself yet):\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nstarter_learning_rate = 0.001\r\ndecay_steps = 100\r\ndecay_rate = 0.96\r\nnum_RNN_layers = 3\r\nLSTM_CELL_SIZE = 100\r\nkeep_prob = 0.95\r\n\r\nwith tf.name_scope('Inputs'):\r\n    x = tf.placeholder(dtype=tf.float32, shape=[None, 200])\r\n    y = tf.placeholder(dtype=tf.float32, shape=[None, 200])\r\n    length = tf.placeholder(dtype=tf.int32, shape=[None])\r\n    Flg_training = tf.placeholder(dtype=tf.bool, shape=[])\r\n\r\n    x_1 = tf.expand_dims(x, -1)\r\n\r\nwith tf.name_scope('BiLSTM'):\r\n    dropcells = []\r\n    for iiLyr in list(range(num_RNN_layers)):\r\n        cell_iiLyr = tf.nn.rnn_cell.LSTMCell(num_units=LSTM_CELL_SIZE, state_is_tuple=True)\r\n        dropcells.append(tf.nn.rnn_cell.DropoutWrapper(cell=cell_iiLyr, output_keep_prob=keep_prob))  #,, input_keep_prob=self.keep_prob input_keep_prob=1.0, seed=None\r\n\r\n    MultiLyr_cell = tf.nn.rnn_cell.MultiRNNCell(cells=dropcells, state_is_tuple=True)\r\n\r\n    outputs, states  = tf.nn.bidirectional_dynamic_rnn(\r\n        cell_fw=MultiLyr_cell, \r\n        cell_bw=MultiLyr_cell, \r\n        dtype=tf.float32,\r\n        sequence_length=length, #tf_b_lens \r\n        inputs=x_1, #stacked_RefPts_desc, #tf_b_VCCs_AMs_BN1\r\n        scope = \"BiLSTM\"\r\n        )\r\n\r\n    #output_fw, output_bw = outputs\r\n    states_fw, states_bw = states\r\n\r\n    c_fw_lstLyr, h_fw_lstLyr = states_fw[-1]\r\n    c_bw_lstLyr, h_bw_lstLyr = states_bw[-1]\r\n\r\n    states_concat1 = tf.concat([h_fw_lstLyr, h_bw_lstLyr], axis = 1, name = 'states_concat')\r\n\r\nwith tf.name_scope(\"cs_BN1\"):\r\n    x_BN = tf.layers.batch_normalization(\r\n        states_concat1,\r\n        axis=-1, # axis that should be normalized (typically the features axis, in this case the concated states or hidden vectors)\r\n        momentum=0.99,\r\n        epsilon=1e-10, #0.001,\r\n        center=False, #True,\r\n        scale=False, #True,\r\n        beta_initializer=tf.zeros_initializer(),\r\n        gamma_initializer=tf.ones_initializer(),\r\n        moving_mean_initializer=tf.zeros_initializer(),\r\n        moving_variance_initializer=tf.ones_initializer(),\r\n        beta_regularizer=None,\r\n        gamma_regularizer=None,\r\n        beta_constraint=None,\r\n        gamma_constraint=None,\r\n        training=Flg_training, #False,\r\n        trainable=True,\r\n        name=\"test_BN\", #None,\r\n        reuse=None,\r\n        renorm=False,\r\n        renorm_clipping=None,\r\n        renorm_momentum=0.99,\r\n        fused=False,\r\n        virtual_batch_size=None,\r\n        adjustment=None\r\n        )\r\n\r\nwith tf.name_scope(\"Regression\"):\r\n    a = tf.get_variable(\"a\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(1.0))\r\n    b = tf.get_variable(\"b\", shape=[1], dtype=tf.float32, initializer=tf.constant_initializer(0.0))\r\n\r\nwith tf.name_scope(\"Prediction\"):\r\n    y_pred = tf.multiply(x_BN, a) + b\r\n\r\nwith tf.name_scope('Loss'):\r\n    losses = tf.losses.mean_squared_error(y, y_pred, reduction=tf.losses.Reduction.NONE)\r\n    mean_loss = tf.reduce_mean(losses)\r\n\r\nwith tf.name_scope('Training'):\r\n    global_step = tf.Variable(0, trainable=False)\r\n    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\r\n                                               decay_steps, decay_rate, staircase=True) \r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\n    with tf.control_dependencies(update_ops):\r\n        train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(losses, global_step=global_step)\r\n\r\n\r\n#x_mean = tf.reduce_mean(x_BN, axis=0)\r\n\r\nsess = tf.InteractiveSession()\r\ntrain_writer = tf.summary.FileWriter(\"G:\\\\Surface_Ozone\\\\Temp\\\\\", sess.graph)   \r\nsess.run(tf.global_variables_initializer())\r\n\r\nfor ii in list(range(2000)):\r\n    x_in = (np.random.rand(20, 200))\r\n    y_in = x_in * 1.5 + 3.0\r\n    length_in = np.full([20], 200, dtype=np.int32)\r\n\r\n    _, mean_loss_val, a_val, b_val = sess.run([train_step, mean_loss, a, b], feed_dict={\r\n        x: x_in, \r\n        Flg_training: True, \r\n        y: y_in,\r\n        length: length_in\r\n        })\r\n\r\n    if (ii < 50):\r\n        print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\r\n    else:\r\n        if (ii % 100 == 0):\r\n            print(\"step {}: {} | a: {} | b: {}\".format(ii, mean_loss_val, a_val, b_val))\r\n\r\nprint(\"Normal End.\")\r\n```", "Thanks for the update, @asimshankar. I posted a shorter code snippet that reproduces the problem in one of the comments above. You can copy/paste that in an interactive python session and verify in a few seconds.", "Thanks @sharvil. I quickly try the snippet and was able to reproduce the issue with tf-nightly-gpu binary. Interestingly, the issue is only produced on GPU version, but not tf CPU binary, which I suspect has something to do with the GPU ops. \r\n\r\nWill need some time to dig more for details. ", "@qlzh727 thanks for taking a look. I observed the GPU/CPU behavior difference in a comment above. Allow me to summarize what's already in this thread:\r\n\r\n1) this issue has been present since at least TF 1.5 and has been repro'd now on 1.8, 1.9, and tf-nightly-gpu.\r\n2) this issue only occurs when executing on GPU; using a CPU build or running with no GPU devices succeeds.\r\n3) this issue only occurs when `TIME_STEPS` is >= 32.\r\n\r\nMy suspicion is that there's a synchronization issue in the GPU code path. Consider different values of `parallel_iterations` on `dynamic_decode`.", "Checking in to see if there's any update.", "This issue seems to be the same as https://github.com/tensorflow/tensorflow/issues/15874. (especially the `TIME_STEPS>=32` part.", "@ppwwyyxx you're right, that issue looks suspiciously similar and probably has the same root cause. On another note, I've come up with a workaround for the sample code I posted above.\r\n\r\nThe following code:\r\n\r\n```python\r\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n  grads_and_vars = optimizer.compute_gradients(res)\r\n  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n```\r\n\r\ncan be transformed into:\r\n\r\n```python\r\nres = tf.tuple([res], control_inputs=tf.get_collection(tf.GraphKeys.UPDATE_OPS))[0]\r\ngrads_and_vars = optimizer.compute_gradients(res)\r\ntrain_op = optimizer.apply_gradients(grads_and_vars, global_step)\r\n```\r\n\r\nwhich forces the batch norm ops to run before gradients on `res` are computed, as desired.", "I had similar issue with this magical number 32. Length of the sequence which i passed into rnn was always 70 but bidirectional rnn worked only when i set manually max length to 32. In other case tf hangs. I changed `parallel_iterations` argument to 71 (max size + 1) and it is working.\r\n\r\nUnfortunately i have no idea if number of parallel iterations affects somehow the results and how does the performance changes looks like but if I see such problems, expect the worst.\r\n\r\nPrevious code:\r\n`outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, rnn_input, dtype=tf.float32)`\r\n\r\nNew code:\r\n`outputs, _ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, rnn_input, dtype=tf.float32, parallel_iterations=71)`", "@amirmazaheri1990,\r\nThe [code ](https://github.com/tensorflow/tensorflow/issues/19568#issuecomment-409361140) mentioned by @sharvil  could be executed successfully in the **`Tensorflow Version 1.15.2`**, in **`Google Colab`** with **`GPU Runtime`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/73e78757976c4fe7cdd2ed4dc233aae4/gh_19568.ipynb) of the working code. Thanks!", "Cool cool. I'm relieved this got sorted out.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 19567, "title": "[BUG] max_pooling1d can not run in GPU ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 2.71.0\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.0/7102\r\n- **GPU model and memory**: 3x GeForce GTX 1080 at 8G\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\n\r\nI have a rank-2 tensor, say a 1024*512 tensor, and I want to apply 1D max pooling. If I run in CPU, codes work great. However, If I run in GPU, codes will throw exceptions like the following.\r\n\r\n```\r\n2018-05-25 18:28:34.272767: E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7102 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n2018-05-25 18:28:34.273613: W ./tensorflow/stream_executor/stream.h:2018] attempting to perform DNN operation using StreamExecutor without DNN support\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 903, in _train_model\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 546, in run\r\n    run_metadata=run_metadata)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1022, in run\r\n    run_metadata=run_metadata)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1113, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1098, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1170, in run\r\n    run_metadata=run_metadata)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 950, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1140, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    run_metadata)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: cudnn PoolForward launch failed\r\n\t [[Node: dnn/input_from_feature_columns_2/max_pooling1d/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 1, 3], padding=\"SAME\", strides=[1, 1, 1, 3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/input_from_feature_columns_2/max_pooling1d/MaxPool-0-TransposeNHWCToNCHW-LayoutOptimizer)]]\r\n\t [[Node: dnn/gradients/dnn/input_from_feature_columns_2/input_layer/b_media_appbundle_format_embedding/b_media_appbundle_format_embedding_weights_grad/Select_1/_313 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1344_...d/Select_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op u'dnn/input_from_feature_columns_2/max_pooling1d/MaxPool', defined at:\r\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/deploy/models/tf_dnn/training/main/task.py\", line 66, in <module>\r\n    tf.app.run()\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/deploy/models/tf_dnn/training/main/task.py\", line 61, in main\r\n    t.train_eval()\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/training/trainer.py\", line 127, in train_eval\r\n    self.model.train_eval(config, model_dir, train_data, eval_data)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/models/tf_deep.py\", line 170, in train_eval\r\n    training.train_and_evaluate(m, train_spec, eval_spec)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/training.py\", line 401, in train_and_evaluate\r\n    executor.run_local()\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/training.py\", line 572, in run_local\r\n    self._estimator.train(input_fn=self._train_spec.input_fn, max_steps=max_steps, hooks=train_hooks)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/dnn.py\", line 466, in _model_fn\r\n    config=config)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/dnn.py\", line 224, in _dnn_model_fn\r\n    last_layer_feats=ll_feats.values())\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/logit_ops.py\", line 301, in build_dnn_logits\r\n    last_layer_feats=last_layer_feats)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/logit_ops.py\", line 206, in build_hidden_layers\r\n    net = apply_pooling(net, pooling, pooling_size, pooling_stride)\r\n  File \"/data2/jenkins/src/marvel2/python/moloco/learn/estimators/logit_ops.py\", line 343, in apply_pooling\r\n    o = tf.layers.max_pooling1d(o, pooling_size, strides=pooling_stride, padding=\"same\")\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/layers/pooling.py\", line 231, in max_pooling1d\r\n    return layer.apply(inputs)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 825, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 714, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/layers/pooling.py\", line 86, in call\r\n    data_format=data_format)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 2144, in max_pool\r\n    name=name)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4587, in max_pool\r\n    data_format=data_format, name=name)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3290, in create_op\r\n    op_def=op_def)\r\n  File \"/var/lib/jenkins/venvs/gpumoloco/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1654, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): cudnn PoolForward launch failed\r\n\t [[Node: dnn/input_from_feature_columns_2/max_pooling1d/MaxPool = MaxPool[T=DT_FLOAT, data_format=\"NCHW\", ksize=[1, 1, 1, 3], padding=\"SAME\", strides=[1, 1, 1, 3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dnn/input_from_feature_columns_2/max_pooling1d/MaxPool-0-TransposeNHWCToNCHW-LayoutOptimizer)]]\r\n\t [[Node: dnn/gradients/dnn/input_from_feature_columns_2/input_layer/b_media_appbundle_format_embedding/b_media_appbundle_format_embedding_weights_grad/Select_1/_313 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1344_...d/Select_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nINFO:root:RETURN CODES: 1\r\n```\r\n\r\n\r\n### Source code / logs\r\n\r\nHere are my codes:\r\n\r\n```python\r\ndef apply_pooling(inputs, pooling, pooling_size, pooling_stride):\r\n  \"\"\"Apply pooling operations on inputs tensor.\r\n\r\n  Args:\r\n    inputs: A tensor with at least 3 rank.\r\n    pooling: When not `None`, `max` means max pooling, and `average` means average pooling.\r\n    pooling_size: The pooling size in all spatial dimensions.\r\n    pooling_stride: The stride operation size in all spatial dimensions.\r\n\r\n  Returns:\r\n    A tensors after applying for pooling strategy.\r\n  \"\"\"\r\n  if pooling is None or pooling == Pooling.NONE:\r\n    return inputs\r\n\r\n  # Expand dimension if inputs has not enough rank\r\n  ndims = len(inputs.get_shape().as_list())\r\n  if ndims == 1:\r\n    raise ValueError(\"inputs must be at least rank 2. Given it's {}\".format(ndims))\r\n  o = inputs\r\n  if ndims == 2:\r\n    o = tf.expand_dims(o, -1)\r\n  # TODO: fix the issue of not being able to run pooling in GPU\r\n  if pooling == Pooling.MAX:\r\n    o = tf.layers.max_pooling1d(o, pooling_size, strides=pooling_stride, padding=\"same\")\r\n  elif pooling == Pooling.AVERAGE:\r\n    o = tf.layers.average_pooling1d(o, pooling_size, strides=pooling_stride, padding=\"same\")\r\n  else:\r\n    raise ValueError(\"Unsupported pooling strategy: {}\".format(pooling))\r\n  # Squeeze dimenion if necessary\r\n  if ndims == 2:\r\n    o = tf.squeeze(o, -1)\r\n  return o\r\n```", "comments": ["@simon-moloco I've solved this problem by `pip install tf-nightly-gpu`. This may help.", "I have the same problem with MaxPooling directly after input (`tf.reshape`). This happens only with CUDA 10, but multiple versions of tensorflow (1.13 nightly, 1.10 stable).\r\n\r\nI solved my problem by adding useless intermediate layers (e.g. summation and division).\r\n\r\nAny updates @simon-moloco ?", "> \r\n> \r\n> @simon-moloco I've solved this problem by `pip install tf-nightly-gpu`. This may help.\r\n\r\nand where you used it ?", "Is this problem fixed in any of the current Tensorflow versions? For instance, 1.15.0?", "Does this solved?", "> Does this solved?\r\n\r\nNo", "@simon-moloco,\r\nSorry for the delayed response. Your code could be executed with **`GPU`**, successfully, with **`Tensorflow Version, 2.4.1`** . Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/88da49b94d0776944ed504c05067dece/gh_19567.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19567\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/19567\">No</a>\n"]}, {"number": 19566, "title": "'infiniband/verbs.h' file not found", "body": "**ERROR**: /Users/abc/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1)\r\nIn file included from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:\r\nIn file included from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21:\r\nIn file included from ./tensorflow/contrib/verbs/rdma_mgr.h:24:\r\n**./tensorflow/contrib/verbs/rdma.h:21:10: fatal error: 'infiniband/verbs.h' file not found**\r\n#include <infiniband/verbs.h>", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Mac OS 10.13.2 \r\nTensorFlow installed from: Github\r\nTensorFlow version: r1.8\r\nBazel version: 0.13.1-homebrew\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: are following\r\n\r\n`$ cd tensorflow  # cd to the top-level directory created`\r\n`$ ./configure`\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]Y\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler? [y/N]Y\r\n**Do you wish to build TensorFlow with VERBS support? [y/N]Y**\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]\r\nDo you wish to build TensorFlow with CUDA support? [y/N] N\r\n\r\n`$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`", "When configuring with all default option that is all No it worked. Its only occurs when in configuration I select for **VERBS support** I guess\r\nI have checked the code of the file /tensorflow/contrib/verbs/rdma.h and its including infiniband/verbs.h\r\n`#include <infiniband/verbs.h>`\r\nBut there is no such file anywhere.", "If you want verbs support, you need to have installed the necessary OFED components, which would provide this header file among other things.  If you don't have InfiniBand (or RoCE), then you don't need IB verbs.", "Thanks, @cliffwoolley . @quadrixm - does installing the necessary components resolve your issue?"]}, {"number": 19565, "title": "Getting wrong values from slice of a variable array", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3.6.5\r\n- **CUDA/cuDNN version**: CUDA Driver Version / Runtime Version:  9.1 / 9.0;  cuDNN: 7.1.3.16-1+cuda9.0\r\n- **GPU model and memory**: GeForce 940MX \r\n- **Have I written custom code**: N/A\r\n- **Bazel version**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n\r\n### Describe the problem\r\nBug: I am getting wrong values when I try to run/evaluate slice of a variable array with rank 3. \r\nWrong values are all zeros, no matter how I initialize the array. Problem occurs for some dimension values (see screenshots below).\r\n\r\n### Source code / logs\r\nTrying to run this code with different k and d values. For instance there is no problem for k=3 and d=4, but k=3 and d=4 replicates the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nk = 4\r\nd = 3\r\nsigma_init = np.repeat([np.eye(d, dtype=np.float32)], k, axis=0)\r\n\r\nsigma = tf.get_variable(\"sigma\",\r\n                      shape=(k,d,d),\r\n                      dtype=tf.float32,\r\n                      initializer=tf.constant_initializer(sigma_init),\r\n                      trainable=False)\r\n\r\n\r\nsess = tf.Session()\r\n\r\nsess.run([sigma.initializer])\r\n#sess.run(tf.global_variables_initializer())\r\nprint(\"whole:\")\r\nprint(sigma.eval(session=sess))\r\n\r\n\r\nprint(\"slices:\")\r\nprint(sigma[0].eval(session=sess))\r\n\r\nwith sess.as_default():\r\n    print(sigma[0].eval())\r\n    \r\nprint(sess.run(sigma[0]))\r\n```\r\n\r\n![kd34](https://user-images.githubusercontent.com/9108649/40570982-228658b4-6091-11e8-83d6-aa59d15900e0.png)\r\n\r\n![kd43](https://user-images.githubusercontent.com/9108649/40570984-264f6d00-6091-11e8-8b19-224ef3d5045d.png)\r\n\r\nSome additional info about system:\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2041188/tf_env.txt)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nExact command to reproduce", "I have updated the post\r\n\r\nRe-installing/updating tensorflow did not help. Also: I cannot reproduce the error when I force tf to use the cpu\r\n\r\n", "Turned out to be driver specific problem, reinstalling driver, cuda and cudnn resolved it"]}, {"number": 19564, "title": "Building TF from the source: compilation error with enabling AVX512 (Eigen/src/Core/arch/AVX512/PacketMath.h)", "body": "### System information\r\n\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n\r\n-**OS Platform and Distribution \r\nLinux Ubuntu 16.04:\r\n\r\n-**CPU info\r\nIntel(R) Xeon(R) Gold 5118 CPU\r\n\r\n-**TensorFlow master branch commit-id:\r\ncommit 38926b8a0fa89bef74085be0e321c13e739795d4\r\nMerge: 00aa1d3 2457cae\r\n\r\n- **Python version**: \r\npython2.7\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.12.0\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 6.4.0 20180424 (Ubuntu 6.4.0-17ubuntu1~16.04)\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 9. 2 / cuDNN 7.1\r\n\r\n- **GPU model and memory**:\r\nNVIDIA Quadro P2000, 5G DDR Mem.\r\n\r\n- **Exact command to reproduce**:\r\n\r\nbazel build --jobs 48 --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt=\"-DEIGEN_USE_VML\" --copt \"-DEIGEN_ENABLE_AVX512\" --copt \"-DEIGEN_ENABLE_AVX2\" //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n\r\nERROR: tensorflow/tensorflow/core/kernels/BUILD:3446:1: C++ compilation of rule '//tensorflow/core/kernels:bincount_op' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:400:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/bincount_op.h:19,\r\n                 from tensorflow/core/kernels/bincount_op.cc:20:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux(const Packet&) [with Packet = __vector(16) float]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:866:1: error: unrecognizable insn:\r\n }\r\n ^\r\n(insn 12 11 13 2 (set (mem/c:V4SF (plus:DI (reg/f:DI 82 virtual-stack-vars)\r\n                (const_int -320 [0xfffffffffffffec0])) [5 lane0+0 S16 A128])\r\n        (vec_merge:V4SF (vec_select:V4SF (reg:V16SF 87 [ _4 ])\r\n                (parallel [\r\n                        (const_int 0 [0])\r\n                        (const_int 1 [0x1])\r\n                        (const_int 2 [0x2])\r\n                        (const_int 3 [0x3])\r\n                    ]))\r\n            (reg:V4SF 102 [ D.798038 ])\r\n            (reg:QI 105))) external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:857 -1\r\n     (nil))\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX512/PacketMath.h:866:1: internal compiler error: in extract_insn, at recog.c:2287\r\nPlease submit a full bug report,\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nTensorFlow version\nGPU model and memory", "Updated all needed fields. Thx.\r\n\r\n", "Eigen library support for AVX512 is limited. Instead, MKL-DNN (--config=mkl) utilizes AVX512 better. Can you remove all AVX512 flags ? This removal only affects Eigen code path.", "Hi Peng,\r\nI have the same configuration. Did you manage to solve this problem ?", "AVX512 is not officially supported, configuring using MKL can be good option in this case. Is this still an issue for you? Also can you please share your solution if you were able to solve it? Thanks!\r\n", "> AVX512 is not officially supported, configuring using MKL can be good option in this case. Is this still an issue for you? Also can you please share your solution if you were able to solve it? Thanks!\r\n\r\nHi ymodak,\r\nI managed to install tensorflow and tensorflow-gpu. What is your final goal ?", "@amapic That's great to hear. Can you please share your steps on this post?\r\nPerformance optimizations for MKL, linux and windows builds is on our [roadmap for TensorFlow 2.0](https://www.tensorflow.org/community/roadmap)\r\nI will close this issue due to lack of activity. Thanks!"]}, {"number": 19563, "title": "Branch 198122165", "body": "", "comments": ["@ankurtaly @case540 \r\n\r\nQuestion: this pushing PR seems to start from CL/197586515 and ends on CL/198122165.\r\n\r\nThere seem to be at least one CL in this range that is missing: CL/197949416.\r\n\r\nCan you look at what happened to them?", "On never mind. The GitHub page folded it. It's there. Thanks.", "Closing stale push."]}, {"number": 19562, "title": "Branch 198098380", "body": "", "comments": []}, {"number": 19561, "title": "AddSymbolicGradients cannot compute the partial derivative for merged node", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n- **Python version**: \r\n3.5.0\r\n- **Bazel version (if compiling from source)**:\r\n0.13.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n- **CUDA/cuDNN version**:\r\nNo CUDA\r\n- **GPU model and memory**:\r\nNo GPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nCreate a graph in C++ as following\r\n\r\n      input\r\n       /    \\\r\n      |      |\r\n      x     y\r\n       \\    / \r\n         + (or concat)\r\n         |\r\n     output \r\n\r\ngetting following error:\r\n\r\nNon-OK-status: AddSymbolicGradients(root, {loss}, vars, &grads) status: Invalid argument: Cannot compute the partial derivative for node 'Variable_1' as it's unreachable from the output node(s).\r\n\r\nThe 'Variable_1' is the weight of x op. It should be reachable from output, but it looks the C++ library is not able to handle branch merge or adding.\r\n", "comments": ["Can you provide a C++ script to the reproduce this problem?", "Hi, I found an bug in my own code, and it works now. I'll close this issue. Thanks for your attention."]}, {"number": 19560, "title": "Branch 198092991", "body": "", "comments": []}, {"number": 19559, "title": "TF1.8 is 10% slower than TF1.4 on most cases", "body": "I have tested on quantities of tensorflow-based applications, including tf_cnn_benchmark and keras typically applications and even for multi-GPU applications. All performance benchmarks are 10% dropped when TF1.4 is upgraded to TF1.8. However, once I revert the upgrade to use TF1.4 again, the 10% performance loss will be back.", "comments": ["Originally, training inceptionV3 per epoch costs 62sec, after bare upgrade from TF1.4 to TF1.8 without user code changes, finishing 1 epoch costs 85sec.", "CC @tfboyd ", "@ghostplant   Can you provide an example command line for tf_cnn_benchmarks?  I am assuming a default tensorflow install, if you are custom compiling please let me know.  ", "@tfboyd It is from https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\r\nThe multi-GPU Resnet50 performance stably drops from 840images/sec to 760images/sec.\r\n\r\nBesides, I run a typical multi-GPU keras_inceptionV3 model (using tensorflow backend), it costs 62sec to finish an epoch with 20000 samples using TF1.4, and it costs 85sec to finish an epoch with 20000 samples using TF1.8.", "I tested TF1.6 as well, which has a same performance with TF1.4. So there might be a performance regression between TF1.6 and TF1.8.", "@ghostplant \r\n\r\nI know the script well.  I published our original benchmarks and run nightly tests.  Knowing the command-line and your hardware helps me understand the problem.  I do not test Keras and I think Keras multi-gpu places all of the variables on GPU:0, which is often not great but can be \"ok\" with small numbers of GPUs and maybe something is making that slower.  tf_cnn_benchmarks is nice as we can kind of isolate that and put the parameters almost anywhere.  Hardware is good to know as you could have anything from nvlinked V100s to K80s PCIe with no GPU peer-to-peer or two GTX1080tis in a workstation also most likely not peer-to-peer.  You could be using nccl, hierarchical copy, fp16, parameter server = cpu and one of a number of args for tf_cnn_benchmarks.  \r\n\r\nMy tests show that we are speeding up but **I do not doubt your regression**.  I know many argument combinations are not tracked, which is my fault and a result of pragmatic time use and maybe bad choices. :-)\r\n\r\nThere is light at the end of the tunnel.  While it will not help you with Keras, DistributedStrategies (the standard way of doing TensorFlow multi-GPU) will be enabled for Keras models and that will over a simple, flexible, and monitored way to do multi-GPU.\r\n\r\nSend me a little more information and I will run some tests.  I could run some tests now but I fear they would not match your situation and just use up time.  ", "My environment is Tesla P100x4 without NV-link. The initialization time cost for Keras applications are all increased, and I think both CUDA environment and Keras engine itself are not the root-causes, and the example script is also not the root-cause as well, because all of these 3 components are not changed, while the only changes is the version of Tensorflow for cuda.", "Again, can you provide your command line for tf_cnn_benchmarks? \r\n\r\nThere are many command line options in tf_cnn_benchmarks, and for resnet50 which I'm regularly tracking, the best performing option is getting faster since 1.4. You're probably using a different combination of options and that's why a command line is needed.", "`tf_cnn_benchmarks/tf_cnn_benchmarks.py --model resnet50 --batch_size 64 --num_batches 50 --data_format=NCHW --local_parameter_device cpu --num_inter_threads 0`\r\n\r\nBesides, I refer from the benchmark links: https://www.tensorflow.org/performance/benchmarks#details_for_nvidia_dgx-1tm_nvidia_tesla_p100\r\n\r\nwhich says inceptionV3 can have a 569 images/sec using P100 x 4 (parameter on cpu), and I have the same hardware configuration but I can never get this speed (it only reaches 490 images/sec). So can you also show how to make it 569 images/sec using P100 x 4 (parameter on cpu), any special configurations?", "Running your command on 4 P100s (with the extra argument --num_gpus=4) gave me 870 im/s. This command may not be the best but at least it's slightly better than the old numbers (852) in https://www.tensorflow.org/performance/benchmarks. So it looks alright. My environment does have nvlink (the above link does mention nvlink), btw. Note that the best combination of options would also depend on your environment. Perhaps there is a regression on environment without nvlink but more information will be needed to find out if that's true.", "`--local_parameter_device cpu` doesn't require NVlink. How about your inceptionV3 performance?", "I got 535 im/s with the same command above except for `--model inception3`. This is a bit slower than the official number but I don't know if that's a regression -- unlike resnet50 I don't track the performance of inceptionv3 myself so I don't know what its performance was on my environment. Just to provide a data point for your reference. @tfboyd may have more information on it.", "@ppwwyyxx Hi, I have an extra question about this tf_cnn_benchmark script. As for Resnet50 model containing CNN layers, I just wonder whether Tensorflow engine supports CNN `bias weight` when training Resnet50 model? Why I see no `cudnnConvolutionBackwardBias` is invoked? It seems to be only way to get the `bias weight` of a CNN layer, if it is not even invoked, I think Tensorflow doesn't manage any CNN `bias weight`, does anyone know whether I am right?", "I don't quite understand from above what exactly is your observation or how you obtain them. But it seems related to the fact that none of the convolutional layers in ResNet50 have bias.", "@ppwwyyxx It can be profiled by any tools, typically, nvtools, syscall counters. Are you sure Resnet50 CNN layers don't include bias weight?", "I'm sure none of the convolutional layers in ResNet50 have bias. I'm not sure you mean the same thing by \"bias weight\". The convolution filters are often referred to as weight as well.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks all, I'll close this issue. I think larger ops and function libraries in new versions might cost more memory which is reasonable to slightly reduce the performance."]}, {"number": 19558, "title": "Updated embedding column example", "body": "The embedding column example had a comment that seemed to incorrectly relate to the indicator column. Updated the comment to reflect the embedding column info. Also updated the name of the dimension variable to be consistent with the example from a few lines earlier.", "comments": []}, {"number": 19557, "title": "OpenCL GPU support issue", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04 LTS (Bionic Beaver)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-2217-g7ad3964dcb 1.8.0\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: Build label: 0.13.1\r\n- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: AMD Radeon R9290\r\n- **Exact command to reproduce**: bazel test -c opt --config=sycl --test_output=all //tensorflow/python/kernel_tests:basic_gpu_test\r\n\r\n### Describe the problem\r\nI built TF 1.8 from source with 'y' for OpenCL SYCL and ComputeCPP support in build configuration ('n' for other options). Build was complete successfully, but GPU device seems unavailable in TF, only CPU:0 is listed by\r\n`tensorflow.python.client.device_lib.device_lib.list_local_devices()`\r\nand\r\n`tensorflow.contrib.eager.list_devices()`\r\nAlso, `tensorflow.matmul(a, b)` is placed on CPU:0, log_device_placement says: \"Device mapping: no known devices.\"\r\n\r\n`bazel test -c opt --config=sycl --test_output=all //tensorflow/python/kernel_tests:basic_gpu_test`\r\nfails with errors from SYCL.\r\n\r\nBazel test, clinfo and computecpp_info logs are listed below.\r\n\r\nRequirements for OpenCL GPU support seem to be met, but it doesn't work. Am I missing something?\r\n\r\n### Source code / logs\r\nclinfo\r\n\r\n> Number of platforms                               1\r\n  Platform Name                                   AMD Accelerated Parallel Processing\r\n  Platform Vendor                                 Advanced Micro Devices, Inc.\r\n  Platform Version                                OpenCL 2.1 AMD-APP (2633.3)\r\n  Platform Profile                                FULL_PROFILE\r\n  Platform Extensions                             cl_khr_icd cl_amd_event_callback cl_amd_offline_devices \r\n  Platform Host timer resolution                  1ns\r\n  Platform Extensions function suffix             AMD\r\n  Platform Name                                   AMD Accelerated Parallel Processing\r\nNumber of devices                                 1\r\n  Device Name                                     Hawaii\r\n  Device Vendor                                   Advanced Micro Devices, Inc.\r\n  Device Vendor ID                                0x1002\r\n  Device Version                                  OpenCL 1.2 AMD-APP (2633.3)\r\n  Driver Version                                  2633.3\r\n  Device OpenCL C Version                         OpenCL C 1.2 \r\n  Device Type                                     GPU\r\n  Device Board Name (AMD)                         AMD Radeon R9 200 Series\r\n  Device Topology (AMD)                           PCI-E, 01:00.0\r\n  Device Profile                                  FULL_PROFILE\r\n  Device Available                                Yes\r\n  Compiler Available                              Yes\r\n  Linker Available                                Yes\r\n  Max compute units                               40\r\n  SIMD per compute unit (AMD)                     4\r\n  SIMD width (AMD)                                16\r\n  SIMD instruction width (AMD)                    1\r\n  Max clock frequency                             947MHz\r\n  Graphics IP (AMD)                               7.2\r\n  Device Partition                                (core)\r\n    Max number of sub-devices                     40\r\n    Supported partition types                     none specified\r\n  Max work item dimensions                        3\r\n  Max work item sizes                             1024x1024x1024\r\n  Max work group size                             256\r\n  Preferred work group size (AMD)                 256\r\n  Max work group size (AMD)                       1024\r\n  Preferred work group size multiple              64\r\n  Wavefront width (AMD)                           64\r\n  Preferred / native vector sizes                 \r\n    char                                                 4 / 4       \r\n    short                                                2 / 2       \r\n    int                                                  1 / 1       \r\n    long                                                 1 / 1       \r\n    half                                                 1 / 1        (n/a)\r\n    float                                                1 / 1       \r\n    double                                               1 / 1        (cl_khr_fp64)\r\n  Half-precision Floating-point support           (n/a)\r\n  Single-precision Floating-point support         (core)\r\n    Denormals                                     No\r\n    Infinity and NANs                             Yes\r\n    Round to nearest                              Yes\r\n    Round to zero                                 Yes\r\n    Round to infinity                             Yes\r\n    IEEE754-2008 fused multiply-add               Yes\r\n    Support is emulated in software               No\r\n    Correctly-rounded divide and sqrt operations  Yes\r\n  Double-precision Floating-point support         (cl_khr_fp64)\r\n    Denormals                                     Yes\r\n    Infinity and NANs                             Yes\r\n    Round to nearest                              Yes\r\n    Round to zero                                 Yes\r\n    Round to infinity                             Yes\r\n    IEEE754-2008 fused multiply-add               Yes\r\n    Support is emulated in software               No\r\n  Address bits                                    64, Little-Endian\r\n  Global memory size                              2471116800 (2.301GiB)\r\n  Global free memory (AMD)                        2398160 (2.287GiB)\r\n  Global memory channels (AMD)                    16\r\n  Global memory banks per channel (AMD)           16\r\n  Global memory bank width (AMD)                  256 bytes\r\n  Error Correction support                        No\r\n  Max memory allocation                           2082456371 (1.939GiB)\r\n  Unified memory for Host and Device              No\r\n  Minimum alignment for any data type             128 bytes\r\n  Alignment of base address                       2048 bits (256 bytes)\r\n  Global Memory cache type                        Read/Write\r\n  Global Memory cache size                        16384 (16KiB)\r\n  Global Memory cache line size                   64 bytes\r\n  Image support                                   Yes\r\n    Max number of samplers per kernel             16\r\n    Max size for 1D images from buffer            134217728 pixels\r\n    Max 1D or 2D image array size                 2048 images\r\n    Base address alignment for 2D image buffers   256 bytes\r\n    Pitch alignment for 2D image buffers          256 pixels\r\n    Max 2D image size                             16384x16384 pixels\r\n    Max 3D image size                             2048x2048x2048 pixels\r\n    Max number of read image args                 128\r\n    Max number of write image args                8\r\n  Local memory type                               Local\r\n  Local memory size                               32768 (32KiB)\r\n  Local memory syze per CU (AMD)                  65536 (64KiB)\r\n  Local memory banks (AMD)                        32\r\n  Max number of constant args                     8\r\n  Max constant buffer size                        2082456371 (1.939GiB)\r\n  Preferred constant buffer size (AMD)            16384 (16KiB)\r\n  Max size of kernel argument                     1024\r\n  Queue properties                                \r\n    Out-of-order execution                        No\r\n    Profiling                                     Yes\r\n  Prefer user sync for interop                    Yes\r\n  Profiling timer resolution                      1ns\r\n  Profiling timer offset since Epoch (AMD)        1527256350447623220ns (Fri May 25 10:52:30 2018)\r\n  Execution capabilities                          \r\n    Run OpenCL kernels                            Yes\r\n    Run native kernels                            No\r\n    Thread trace supported (AMD)                  Yes\r\n    Number of async queues (AMD)                  2\r\n    Max real-time compute queues (AMD)            0\r\n    Max real-time compute units (AMD)             0\r\n    SPIR versions                                 1.2\r\n  printf() buffer size                            4194304 (4MiB)\r\n  Built-in kernels                                \r\n  Device Extensions                               cl_khr_fp64 cl_amd_fp64 cl_khr_global_int32_base_atomics cl_khr_global_int32_extended_atomics cl_khr_local_int32_base_atomics cl_khr_local_int32_extended_atomics cl_khr_int64_base_atomics cl_khr_int64_extended_atomics cl_khr_3d_image_writes cl_khr_byte_addressable_store cl_khr_gl_sharing cl_amd_device_attribute_query cl_amd_vec3 cl_amd_printf cl_amd_media_ops cl_amd_media_ops2 cl_amd_popcnt cl_khr_image2d_from_buffer cl_khr_spir cl_khr_gl_event \r\nNULL platform behavior\r\n  clGetPlatformInfo(NULL, CL_PLATFORM_NAME, ...)  No platform\r\n  clGetDeviceIDs(NULL, CL_DEVICE_TYPE_ALL, ...)   No platform\r\n  clCreateContext(NULL, ...) [default]            No platform\r\n  clCreateContext(NULL, ...) [other]              Success [AMD]\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_DEFAULT)  Success (1)\r\n    Platform Name                                 AMD Accelerated Parallel Processing\r\n    Device Name                                   Hawaii\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CPU)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_GPU)  Success (1)\r\n    Platform Name                                 AMD Accelerated Parallel Processing\r\n    Device Name                                   Hawaii\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ACCELERATOR)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_CUSTOM)  No devices found in platform\r\n  clCreateContextFromType(NULL, CL_DEVICE_TYPE_ALL)  Success (1)\r\n    Platform Name                                 AMD Accelerated Parallel Processing\r\n    Device Name                                   Hawaii\r\n\r\n/usr/local/computecpp/bin/computecpp_info\r\n\r\n>ComputeCpp Info (CE 0.8.0)\r\nToolchain information:\r\nGLIBC version: 2.27\r\nGLIBCXX: 20160609\r\nThis version of libstdc++ is supported.\r\nDevice Info:\r\nDiscovered 1 devices matching:\r\n  platform    : <any>\r\n  device type : <any>\r\nDevice 0:\r\n  Device is supported                     : UNTESTED - Untested OS\r\n  CL_DEVICE_NAME                          : Hawaii\r\n  CL_DEVICE_VENDOR                        : Advanced Micro Devices, Inc.\r\n  CL_DRIVER_VERSION                       : 2633.3\r\n  CL_DEVICE_TYPE                          : CL_DEVICE_TYPE_GPU \r\n\r\nbazel test -c opt --config=sycl --test_output=all //tensorflow/python/kernel_tests:basic_gpu_test\r\n\r\n> INFO: Analysed target //tensorflow/python/kernel_tests:basic_gpu_test (0 packages loaded).\r\nINFO: Found 1 test target...\r\nERROR: /home/dmitry/Soft/dev/source/TensorFlow/tensorflow/tensorflow/core/BUILD:715:1: C++ compilation of rule '//tensorflow/core:ctc_ops_op_lib' failed (Exit 1)\r\nIn file included from tensorflow/core/ops/ctc_ops.cc:17:\r\nIn file included from ./tensorflow/core/framework/shape_inference.h:20:\r\nIn file included from ./tensorflow/core/framework/node_def_util.h:23:\r\nIn file included from ./tensorflow/core/framework/attr_value_util.h:23:\r\nIn file included from ./tensorflow/core/framework/partial_tensor_shape.h:20:\r\nIn file included from ./tensorflow/core/framework/tensor_shape.h:21:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:99:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:163:19: error: no matching member function for call to 'get_access'\r\n    auto ptr =buf.get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::host_buffer>().get_pointer();\r\n              ~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:593:3: note: candidate template ignored: invalid explicitly-specified argument for template parameter 'accessMode'\r\n  get_access() {\r\n  ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:611:53: note: candidate function template not viable: requires single argument 'cgh', but no arguments were provided\r\n  accessor<T, dimensions, accessMode, accessTarget> get_access(\r\n                                                    ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:633:53: note: candidate function template not viable: requires 3 arguments, but 0 were provided\r\n  accessor<T, dimensions, accessMode, accessTarget> get_access(\r\n                                                    ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:653:53: note: candidate function template not viable: requires at least 2 arguments, but 0 were provided\r\n  accessor<T, dimensions, accessMode, accessTarget> get_access(\r\n                                                    ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:670:68: note: candidate function template not viable: requires 2 arguments, but 0 were provided\r\n  accessor<T, dimensions, accessMode, access::target::host_buffer> get_access(\r\n                                                                   ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/buffer.h:684:68: note: candidate function template not viable: requires at least argument 'range', but no arguments were provided\r\n  accessor<T, dimensions, accessMode, access::target::host_buffer> get_access(\r\n                                                                   ^\r\nIn file included from tensorflow/core/ops/ctc_ops.cc:17:\r\nIn file included from ./tensorflow/core/framework/shape_inference.h:20:\r\nIn file included from ./tensorflow/core/framework/node_def_util.h:23:\r\nIn file included from ./tensorflow/core/framework/attr_value_util.h:23:\r\nIn file included from ./tensorflow/core/framework/partial_tensor_shape.h:20:\r\nIn file included from ./tensorflow/core/framework/tensor_shape.h:21:\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:99:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:195:61: error: no member named 'map_allocator' in namespace 'cl::sycl'\r\n      auto src_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(static_cast<void*>(const_cast<Index*>(src))), cl::sycl::range<1>(n));\r\n                                                  ~~~~~~~~~~^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:195:75: error: unexpected type name 'uint8_t': expected expression\r\n      auto src_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(static_cast<void*>(const_cast<Index*>(src))), cl::sycl::range<1>(n));\r\n                                                                          ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:195:86: warning: expression result unused [-Wunused-value]\r\n      auto src_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(static_cast<void*>(const_cast<Index*>(src))), cl::sycl::range<1>(n));\r\n                                                                                     ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:198:31: error: expected unqualified-id\r\n        auto src_acc =src_buf.template get_access<cl::sycl::access::mode::read, cl::sycl::access::target::global_buffer>(cgh);\r\n                              ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:198:107: error: nested name specifier 'cl::sycl::access::target::' for declaration does not refer into a class, class template or class template partial specialization\r\n        auto src_acc =src_buf.template get_access<cl::sycl::access::mode::read, cl::sycl::access::target::global_buffer>(cgh);\r\n                                                                                ~~~~~~~~~~~~~~~~~~~~~~~~~~^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:198:120: error: expected ';' at end of declaration\r\n        auto src_acc =src_buf.template get_access<cl::sycl::access::mode::read, cl::sycl::access::target::global_buffer>(cgh);\r\n                                                                                                                       ^\r\n                                                                                                                       ;\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:217:62: error: no member named 'map_allocator' in namespace 'cl::sycl'\r\n      auto dest_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(dst), cl::sycl::range<1>(n));\r\n                                                   ~~~~~~~~~~^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:217:76: error: unexpected type name 'uint8_t': expected expression\r\n      auto dest_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(dst), cl::sycl::range<1>(n));\r\n                                                                           ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:217:87: warning: expression result unused [-Wunused-value]\r\n      auto dest_buf = cl::sycl::buffer<uint8_t, 1, cl::sycl::map_allocator<uint8_t> >(static_cast<uint8_t*>(dst), cl::sycl::range<1>(n));\r\n                                                                                      ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:220:32: error: expected unqualified-id\r\n        auto dst_acc =dest_buf.template get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::global_buffer>(cgh);\r\n                               ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:220:117: error: nested name specifier 'cl::sycl::access::target::' for declaration does not refer into a class, class template or class template partial specialization\r\n        auto dst_acc =dest_buf.template get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::global_buffer>(cgh);\r\n                                                                                          ~~~~~~~~~~~~~~~~~~~~~~~~~~^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDeviceSycl.h:220:130: error: expected ';' at end of declaration\r\n        auto dst_acc =dest_buf.template get_access<cl::sycl::access::mode::discard_write, cl::sycl::access::target::global_buffer>(cgh);\r\n                                                                                                                                 ^\r\n                                                                                                                                 ;\r\n2 warnings and 11 errors generated.\r\nTarget //tensorflow/python/kernel_tests:basic_gpu_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10.427s, Critical Path: 9.81s\r\nINFO: 4 processes, local.\r\nFAILED: Build did NOT complete successfully\r\nExecuted 0 out of 1 test: 1 fails to build", "comments": ["I would recommend reaching out to codeplay. OpenCL support for TF is still in early stages, and there may be some additional constraints to use it.", "OpenCL support is integrating into 1.8:\r\nhttps://github.com/lukeiwanski/tensorflow/tree/integration/1.8\r\n", "@Kenop  - Is this still an issue ? Feel free to close if this doesn't exist with the latest tensorflow version.", "Just for reference, this branch with TensorFlow 1.8 was only experimental.\r\nThe official branch is now using TensorFlow 1.9 here: https://github.com/codeplaysoftware/tensorflow/tree/eigen_sycl\r\nIt requires bazel 0.16 and the latest ComputeCpp version.", "Closing as duplicate of https://github.com/tensorflow/tensorflow/issues/22"]}, {"number": 19555, "title": "[WIP] Add Apache Parquet support for TensorFlow Dataset", "body": "Apache Parquet is a widely used columnar storage format available in the Hadoop ecosystem.\r\n\r\nThis PR is a preliminary attempt to add Apache Parquet support for TensorFlow's Dataset API. It should help many to working on existing parquet-formatted big data with TesnorFlow.\r\n\r\nThe PR may not cover all the use cases though it could be served as a starting point for further improvement in the future.\r\n\r\nThe ParquetDataset depends on parquet-cpp (Apache) project as well as other dependencies (e.g, Thrift, etc.). The ParquetDataset only builds on Linux at the moment. This PR also adds the option in ./configure so that those dependencies could be skipped.", "comments": []}, {"number": 19554, "title": "TF for mobile", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Hi TF Team,\r\nDo you have support TF for mobile using GPU on ios/android?", "It should be supported by GPU driver vendors. So Arm or Qualcomm should be done it. You can use SNPE which is providing SDK from Qualcomm. I heard that ARM was released similar SDK as well. \r\n\r\n", "Thank namyikim, I still using coreML for model which trained by tensorflow. I do not make sense with your comment so I will research later :)\r\n", "Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @shivaniag: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @llhe \u2014\u00a0please don't use TensorFlow GitHub issues for promotional purposes, it's off-topic. We are glad that there are many competing open source frameworks, but let's stay on-topic for TensorFlow assistance within the TensorFlow Github organization. That will help us all use our time most efficiently to help users.\r\n\r\n"]}, {"number": 19553, "title": "Fix broken link", "body": "Also, as a side note, I found this link confusing. I was expecting it to take me to a list of pre-made estimators. Not a definition of what a pre-made estimator is. (maybe the glossary definition should link to a list of pre-made estimators?)", "comments": ["@MarkDaoust about the link going to glossary instead of API docs.\r\n\r\nThanks for the link fix.", "> Maybe the glossary definition should link to a list of pre-made estimators? \r\n\r\nSGTM. Fixed internally. Will get pushed out to github soon. "]}, {"number": 19552, "title": "Fix broken link", "body": "Also, as a side note, I found this link confusing. I was expecting it to take me to a list of pre-made estimators. Not a definition of what a pre-made estimator is. (maybe the glossary definition should link to the list of pre-made estimators?)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19551, "title": "Cannot use AdagradOptimizer with MirroredStrategy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: 3.6.3 64-bit\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0 / 6.0\r\n- **GPU model and memory**: Nvidia GTX 1080 8 GB \r\n- **Exact command to reproduce**: `python train_model.py`\r\n\r\n### Describe the problem\r\nIt took me a while to work out what was going on, but it seems that  tf.train.AdagradOptimizer has some specific implementation detail that causes an error when used with MirroredStrategy. I did a spot check with GradientDescentOptimizer and RMSPropOptimizer and they both appear to work in my environment. I'm happy to use a different optimizer as a workaround but I thought at the very least this might save others some time hunting down the cause of the error!\r\n\r\n### Source code / logs\r\nThis is almost exactly copied from the example at https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute (except for the choice of optimizer)\r\n\r\n    import tensorflow as tf\r\n\r\n    def model_fn(features, labels, mode):\r\n        layer = tf.layers.Dense(1)\r\n        logits = layer(features)\r\n\r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n            predictions = {\"logits\": logits}\r\n            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n        loss = tf.losses.mean_squared_error(\r\n                labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            train_op = tf.train.AdagradOptimizer(0.2).minimize(loss)\r\n            return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n    def input_fn():\r\n        features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n        labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n        return tf.data.Dataset.zip((features, labels))\r\n\r\n    distribution = tf.contrib.distribute.MirroredStrategy()\r\n    config = tf.estimator.RunConfig(train_distribute=distribution)\r\n    classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n    classifier.train(input_fn=input_fn)\r\n\r\nLog output:\r\n\r\n    2018-05-25 15:30:12.300908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n    2018-05-25 15:30:14.231174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\n    name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898\r\n    pciBusID: 0000:03:00.0\r\n    totalMemory: 7.93GiB freeMemory: 7.81GiB\r\n    2018-05-25 15:30:14.557081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: \r\n    name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898\r\n    pciBusID: 0000:81:00.0\r\n    totalMemory: 7.93GiB freeMemory: 7.81GiB\r\n    2018-05-25 15:30:14.557174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\r\n    2018-05-25 15:30:15.198082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n    2018-05-25 15:30:15.198134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \r\n    2018-05-25 15:30:15.198142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N \r\n    2018-05-25 15:30:15.198145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N \r\n    2018-05-25 15:30:15.198488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n    2018-05-25 15:30:15.324510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)\r\n    WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqglycjzk\r\n    2018-05-25 15:30:15.455314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1\r\n    2018-05-25 15:30:15.455414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n    2018-05-25 15:30:15.455423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 \r\n    2018-05-25 15:30:15.455427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N \r\n    2018-05-25 15:30:15.455431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N \r\n    2018-05-25 15:30:15.455615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n    2018-05-25 15:30:15.455720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)\r\n    Traceback (most recent call last):\r\n      File \"train_model.py\", line 62, in <module>\r\n        classifier.train(input_fn=input_fn)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n        loss = self._train_model(input_fn, hooks, saving_listeners)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n        return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n        self.config)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\r\n        return self._call_for_each_tower(fn, *args, **kwargs)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n        coord.join(threads)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n        six.reraise(*self._exc_info_to_raise)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n        raise value\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n        yield\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n        self, *merge_args, **merge_kwargs)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 671, in _distributed_apply\r\n        self._create_slots(var_list)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\r\n        with ops.colocate_with(v):\r\n      File \"/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n        return next(self.gen)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4186, in _colocate_with_for_gradient\r\n        with self.colocate_with(op, ignore_existing):\r\n      File \"/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n        return next(self.gen)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4239, in colocate_with\r\n        op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1262, in internal_convert_to_tensor_or_indexed_slices\r\n        value, dtype=dtype, name=name, as_ref=as_ref)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n      File \"/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 243, in _tensor_conversion\r\n        assert not as_ref\r\n    AssertionError\r\n\r\n", "comments": ["I got the same problem when using the `MovingAverageOptimizer`\r\n```python\r\ndef model_fn(features, labels, mode):\r\n    layer = tf.layers.Dense(1)\r\n    logits = layer(features)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\"logits\": logits}\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.mean_squared_error(\r\n            labels=labels, predictions=tf.reshape(logits, []))\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.GradientDescentOptimizer(0.02)\r\n        optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\r\n        train_op = optimizer.minimize(loss)\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\ndef input_fn():\r\n    features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)\r\n    labels = tf.data.Dataset.from_tensors(1.).repeat(100)\r\n    return tf.data.Dataset.zip((features, labels))\r\n\r\ndistribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)\r\nconfig = tf.estimator.RunConfig(train_distribute=distribution)\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\nclassifier.train(input_fn=input_fn)\r\n```\r\nThe error with some context\r\n```\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-114-62e57e2880d4> in <module>()\r\n     27 config = tf.estimator.RunConfig(train_distribute=distribution)\r\n     28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)\r\n---> 29 classifier.train(input_fn=input_fn)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    361 \r\n    362     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    364     logging.info('Loss for final step: %s.', loss)\r\n    365     return self\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    839   def _train_model(self, input_fn, hooks, saving_listeners):\r\n    840     if self._distribution:\r\n--> 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n    842     else:\r\n    843       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)\r\n    882             labels,  # although this will be None it seems\r\n    883             model_fn_lib.ModeKeys.TRAIN,\r\n--> 884             self.config)\r\n    885 \r\n    886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)\r\n    754     \"\"\"\r\n    755     _require_cross_tower_context(self)\r\n--> 756     return self._call_for_each_tower(fn, *args, **kwargs)\r\n    757 \r\n    758   def _call_for_each_tower(self, fn, *args, **kwargs):\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)\r\n    252       for t in threads:\r\n    253         t.should_run.set()\r\n--> 254       coord.join(threads)\r\n    255 \r\n    256     return values.regroup({t.device: t.main_result for t in threads})\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n/usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)\r\n    295     \"\"\"\r\n    296     try:\r\n--> 297       yield\r\n    298     except:  # pylint: disable=bare-except\r\n    299       self.request_stop(ex=sys.exc_info())\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)\r\n    463                 self._captured_var_scope, reuse=self.tower_id > 0), \\\r\n    464             variable_scope.variable_creator_scope(self.variable_creator_fn):\r\n--> 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    466           self.done = True\r\n    467       finally:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    829 \r\n    830     logging.info('Calling model_fn.')\r\n--> 831     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    832     logging.info('Done calling model_fn.')\r\n    833 \r\n\r\n<ipython-input-114-62e57e2880d4> in model_fn(features, labels, mode)\r\n     16         optimizer = tf.train.GradientDescentOptimizer(0.02)\r\n     17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)\r\n---> 18         train_op = optimizer.minimize(loss)\r\n     19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n     20 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    422 \r\n    423     return self.apply_gradients(grads_and_vars, global_step=global_step,\r\n--> 424                                 name=name)\r\n    425 \r\n    426   def compute_gradients(self, loss, var_list=None,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\r\n     97     if self._sequential_update:\r\n     98       with ops.control_dependencies([train_op]):\r\n---> 99         ma_op = self._ema.apply(var_list)\r\n    100     else:\r\n    101       ma_op = self._ema.apply(var_list)\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)\r\n    423         zero_debias = self._averages[var] in zero_debias_true\r\n    424         updates.append(assign_moving_average(\r\n--> 425             self._averages[var], var, decay, zero_debias=zero_debias))\r\n    426       return control_flow_ops.group(*updates, name=scope)\r\n    427 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)\r\n     82   with ops.name_scope(name, \"AssignMovingAvg\",\r\n     83                       [variable, value, decay]) as scope:\r\n---> 84     with ops.colocate_with(variable):\r\n     85       decay = ops.convert_to_tensor(1.0 - decay, name=\"decay\")\r\n     86       if decay.dtype != variable.dtype.base_dtype:\r\n\r\n/usr/lib/python3.5/contextlib.py in __enter__(self)\r\n     57     def __enter__(self):\r\n     58         try:\r\n---> 59             return next(self.gen)\r\n     60         except StopIteration:\r\n     61             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)\r\n   4184   def _colocate_with_for_gradient(self, op, gradient_uid,\r\n   4185                                   ignore_existing=False):\r\n-> 4186     with self.colocate_with(op, ignore_existing):\r\n   4187       if gradient_uid is not None and self._control_flow_context is not None:\r\n   4188         try:\r\n\r\n/usr/lib/python3.5/contextlib.py in __enter__(self)\r\n     57     def __enter__(self):\r\n     58         try:\r\n---> 59             return next(self.gen)\r\n     60         except StopIteration:\r\n     61             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)\r\n   4237     if op is not None and not isinstance(op, Operation):\r\n   4238       # We always want to colocate with the reference op.\r\n-> 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n   4240 \r\n   4241     # By default, colocate_with resets the device function stack,\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)\r\n   1260   else:\r\n   1261     return internal_convert_to_tensor(\r\n-> 1262         value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1263 \r\n   1264 \r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1102 \r\n   1103     if ret is None:\r\n-> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1105 \r\n   1106     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)\r\n    241   # Try to avoid assignments to and other mutations of MirroredVariable\r\n    242   # state except through a DistributionStrategy.update() call.\r\n--> 243   assert not as_ref\r\n    244   return ops.internal_convert_to_tensor(\r\n    245       var.get(), dtype=dtype, name=name, as_ref=as_ref)\r\n\r\nAssertionError: \r\n\r\n```\r\n", "Thanks for bringing this to our attention. We will look into why AdagadOptimizer and MovingAverageOptimizer don't work. ", "Hi All,\r\n\r\nI'm getting this same Error with Adagrad (the default for pre-canned `tf.estimarot.DNNClassifier`)\r\n\r\nHere's my definitions:\r\n\r\n```\r\n# Setup MirroredStrategy\r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\r\n\r\n# Define Specs                                                                                                                                                                               \r\ntrain_spec_dnn = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\r\neval_spec_dnn = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\r\n\r\n\r\nDNNClassifier = tf.estimator.DNNClassifier(\r\n    feature_columns = [tf.feature_column.numeric_column(key='feats', dtype=tf.float64, shape=(nDims,))],                                                                    \r\n    hidden_units = [256, 256, 256, 256],                                                                                                                                 \r\n    n_classes = 200,\r\n    model_dir = '/tmp/tf',\r\n    config = run_config)\r\n\r\n```\r\n\r\nBelow is the ERROR message:\r\n\r\n```\r\n2018-07-30 11:57:19.736726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-07-30 11:57:19.736801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 11:57:19.736818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \r\n2018-07-30 11:57:19.736826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \r\n2018-07-30 11:57:19.736833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \r\n2018-07-30 11:57:19.736840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \r\n2018-07-30 11:57:19.736846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \r\n2018-07-30 11:57:19.737320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\r\n2018-07-30 11:57:19.737953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\r\n2018-07-30 11:57:19.738083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\r\n2018-07-30 11:57:19.738209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\nWARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.\r\nTraceback (most recent call last):\r\n  File \"train_and_eval.py\", line 137, in <module>\r\n    tf.estimator.train_and_evaluate(DNNClassifier, train_spec_dnn, eval_spec_dnn)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 439, in train_and_evaluate\r\n    executor.run()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 518, in run\r\n    self.run_local()\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 650, in run_local\r\n    hooks=train_hooks)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n    self.config)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py\", line 756, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 671, in _distributed_apply\r\n    self._create_slots(var_list)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\r\n    with ops.colocate_with(v):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4186, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4239, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1262, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1104, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py\", line 243, in _tensor_conversion\r\n    assert not as_ref\r\nAssertionError\r\n```", "Nagging Assignee @anj-s: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "    optimizer1 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.tnet_lr)\r\n    optimizer1 = tf.contrib.estimator.TowerOptimizer(optimizer1)\r\n    optimizer2 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.mnet_lr)\r\n    optimizer2 = tf.contrib.estimator.TowerOptimizer(optimizer2)\r\n    tnet_variables = get_model_variables('tnet')\r\n    mnet_variables = get_model_variables('mnet')\r\n\r\n    train_op1 = slim.learning.create_train_op(loss,\r\n                                              optimizer1,\r\n                                              variables_to_train=tnet_variables,\r\n                                              summarize_gradients=True)\r\n    train_op2 = slim.learning.create_train_op(loss,\r\n                                              optimizer2,\r\n                                              variables_to_train=mnet_variables,\r\n                                              summarize_gradients=True)\r\n    train_op = tf.group(train_op1, train_op2)\r\n\r\nAbove two GradientDescentOptimizers do not work either. "]}, {"number": 19550, "title": "Output of Two .pb files why the outputs are different , How to make them same.", "body": ">>> gf.ParseFromString(open('OptimizedGraph.pb','rb').read())5713824\r\n>>> [n.name + '=>' +  n.op for n in gf.node if n.op in ( 'Softmax','Placeholder')]\r\n[u'inputTensor=>Placeholder', u'dropout_keep_prob=>Placeholder']\r\n>>> gf.ParseFromString(open('test.pb','rb').read())4543637\r\n>>> [n.name + '=>' +  n.op for n in gf.node if n.op in ( 'Softmax','Placeholder')]\r\n[u'inputTensor=>Placeholder', u'dropout_keep_prob=>Placeholder', u'output/softmax=>Softmax']\r\n", "comments": []}, {"number": 19549, "title": "Export GRPC symbols along with the C API", "body": "Hi,\r\n\r\nGiven the recent changes to the eager API, that requires GRPC, the eager API cannot be used by external libraries, because it cannot link to the GRPC symbols in TensorFlow. A possible solution is to add `*grpc*` to the C API version script. @asimshankar @alextp is it ok if I make a PR to add that?\r\n\r\nThanks,\r\nAnthony", "comments": ["@eaplatanios : Could you elaborate a bit? What cannot be used by external library (the eager C API?).\r\n\r\nIf possible, I'd rather work out a scheme were we don't have to export more symbols instead of exporting `*grpc*` from the C API.\r\n\r\n@akshaym would know more about how to hide those symbols.", "@asimshankar Sorry I was actually being too vague. I looked into it more and the main issue appears to be [this line](https://github.com/eaplatanios/tensorflow_scala/blob/67f125c40b3ef7f2ef23486f72793071b1d8e9f6/jni/src/main/native/checkpoint_reader.cc#L62). Creating a TFE_TensorHandle from a Tensor is not part of the public API. This forces me to include `eager/c_api_internal.h`, which includes dependencies on GRPC. In this case, maybe those symbols should not be exported in `libtensorflow.so`, but rather in `libtensorflow_framework.so`. This can be done by adding `\"//tensorflow/c/eager:c_api\"` as a dependency [here](https://github.com/tensorflow/tensorflow/blob/fe3f9dddb39171dd7cd9fbb9e044a40e08072c50/tensorflow/BUILD#L452). Do you think this is an acceptable solution? If not, how do you suggest I create the TFE_TensorHandle? Thanks! :)", "By the way, my current temporary solution is to add this instead of including `eager/c_api_internal.h`:\r\n```\r\n#include \"tensorflow/core/common_runtime/eager/tensor_handle.h\"\r\n\r\nstruct TFE_TensorHandle {\r\n  TFE_TensorHandle(const tensorflow::Tensor& t)\r\n      : handle(new tensorflow::TensorHandle(t, nullptr, nullptr, nullptr)) {}\r\n\r\n  tensorflow::TensorHandle* handle;\r\n};\r\n```\r\nHowever, I believe that I shouldn't need to replicate code here. If you think this is ok for now, feel free to close this issue. :)", "Ah, thanks for the additional information. You're using APIs that we've explicitly marked \"internal\" so I'm not surprised you're seeing some friction :). I'd rather not add dependencies/symbols to export things we want to hide from the public API as that will only make it harder down the road.\r\n\r\nI think the workaround you have is okay for now. The situation will change as the eager C API matures, but for now I feel it's okay to have some hacks in code that uses APIs we don't want to commit to :)\r\n\r\nThanks for your understanding.", "No problem. Yes, I am aware I'm using internal APIs that might change and totally understand that. I guess pointing out such issues related to how I am using those APIs might help with evolving the public API.\r\n\r\nOn another note, what happened to the `nightly-libtensorflow` CI builds? They seem to have been removed. Is there some alternative I could use? I was using them to obtain cross-compiled binaries to package with my Scala library.", "Yup, definitely do point these out. Thanks.\r\nRegarding nightly-libtensorflow: @av8ramit @case540 might know more.", "Sorry about the disruption. We recently shuffled around our CI builds. Just took a look and Windows libtensorflow seems to be having some build issues. Will look to fix next week sometime. Also, if you were downloading the builds from TensorFlow Jenkins, that will no longer be where the build artifacts will live. Will post their new location once everything gets settled.", "Yes, we are doing a huge reshuffle of our nightly build process. Please bear with us until we have it all resolved. Thanks!", "@av8ramit @case540  Is there any update on when a new way to access the precompiled binaries might become available?"]}, {"number": 19548, "title": "Inference accuracy depends on batch size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.9\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: 16GB\r\n- **Exact command to reproduce**: pip3 install tensorflow_gpu-1.8.0-cp35-cp35m-manylinux1_x86_64.whl\r\n\r\n### Describe the problem\r\nThe inference accuracy of a trained model for image recognition, e.g. ResNet-18, Incpetion_v3, depends on the batch size. For example, if the batch size of the input is set to one, the accuracy is around 0.2. However, if feed 32 images at ones, the accuracy becomes over 0.82... If feed even more images at ones, the accuracy can even increase... This is really strange.\r\n\r\n### Source code / logs\r\nThe model will classify 16 categories.  Images for training and validating are subsets of imagenet. (Over 223,918 training images and 22, 016 validating images)\r\n\r\n1. The input size for training is set as:\r\n```\r\ntf.placeholder(tf.float32, [None im_height, im_width, n_channel], name = \"input_x\")\r\n```\r\n\r\n2. data is feeded by dataset  from tfrecord. \r\n```\r\ndataset = tf.data.TFRecordDataset(filename)\r\ndataset = dataset.map(tfrecord_parser).prefetch(buffer_size = 4 * batch_size).batch(batch_size)\r\n```\r\n\r\n3. model loading from pb file\r\n```\r\ndef load_pb_graph(frozen_graph_file):\r\n    with tf.gfile.GFile(frozen_graph_file, 'rb') as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n        \r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(graph_def, name=\"\")\r\n    \r\n    return graph, graph_def\r\n```\r\n\r\n4. Computation of accuracy\r\n```\r\nwith g2.as_default():        \r\n    in_logits = tf.placeholder(tf.float32, [None, FLAGS.n_class])\r\n    in_labels = tf.placeholder(tf.float32, [None, FLAGS.n_class])\r\n    accuracy = tf.metrics.accuracy(tf.argmax(in_logits, 1), tf.argmax(in_labels, 1))[1]\r\n        \r\n    sess2 = tf.Session(graph = g2, config = config)\r\n    sess2.run(tf.global_variables_initializer())\r\n    sess2.run( tf.local_variables_initializer())\r\n....\r\n_logits = sess.run(logits, feed_dict = val_feed_dict) \r\n_accuacy = sess2.run(accuracy, {in_logits: _logits, in_labels: val_batch_y})\r\n```\r\n\r\nFeeding one image for inference:\r\n> 2018-05-24 21:14:36 Step 22016 : Validation Accuracy:0.21825 \r\n\r\nFeeding 16 images for inference\uff1a\r\n> 2018-05-24 21:16:22 Step 22016 : Validation Accuracy:0.80069\r\n\r\nFeeding 32 images for inference :\r\n> 2018-05-24 21:18:14 Step 22016 : Validation Accuracy:0.81486\r\n\r\nFeeding 128 images for inference:\r\n> 2018-05-24 21:22:42 Step 22016 : Validation Accuracy:0.82744\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since we don't help debug custom code here. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "I have the same problem regarding the accuracy. When training with a greater batch size (e.g. 128), the reported accuracy is around 95% (I am using the resnet model supplied by tensorflow on a subset of the imagenet dataset). Here is the filled issue template.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.4 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**: binary \r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda-9.0 / libcudart.so.9.0.176\r\n- **GPU model and memory**: Tesla K80, 11441MiB\r\n- **Exact command to reproduce**: train the resnet18 model supplied by TensorFlow here: https://github.com/tensorflow/models/tree/master/official/resnet with batch size 128 for example. After some epochs, the accuracy reported is about 95%. Then, train the same model taking the last checkpoint for 1 epoch with batch size 1. The accuracy drops down to 15%.\r\n\r\n### Describe the problem\r\nThe reported accuracy drops down after only 1 epoch of training at batch size 1 when the model has previously been trained enough at batch size 128 to get an accuracy of 95%. This seems like an issue, either regarding the reported accuracy for validation, or regarding the training with different batch sizes. The latter seems less likely since only 1 epoch of training at batch size 1 has been made.\r\n\r\n### Source code / logs\r\nAfter many epochs of training at batch size 128:\r\n```\r\n06:59:31.217821 139740337723136 tf_logging.py:116] Saving dict for global step 1671932: accuracy = 0.9535984\r\n```\r\n\r\nAdding 1 epoch at batch size 1 [edit, copy paste error]:\r\n```\r\n07:30:56.291565 139897669494528 tf_logging.py:116] Saving dict for global step 1678933: accuracy = 0.15321627\r\n```\r\n", "Did your inference include all the validation images?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@HwMohanLiu Among other things batch size does impact accuracy to some extent, but I wouldn't use batch size 1."]}, {"number": 19547, "title": "TFServing returns inconsistent predict result under stress load", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:CPU only\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWe trained a model and use TFServing to Predict API to return the matched result. When we run our test scripts in single threaded, we can get the same result every time. But when we run the scripts in 2 threads, or run them in multiple machines, sometimes we the returned result is slightly (10%) different from the original one. The more threads we use, the more frequent the result become inconsistent. After we added a lock/unlock when calling \"TF_RETURN_IF_ERROR(bundle->session->Run(run_options, input_tensors, output_tensor_names, {}, &outputs, &run_metadata));\" in SaveModelPredict() in tensorflow_serving/servables/tensorflow/predict_impl.cc, the issue got suppressed.\r\n\r\n", "comments": ["Here's the code change I made in tensorflow_serving/servables/tensorflow/predict_impl.cc (three lines added):\r\n\r\n`\r\n+std::mutex g_lock;\r\n// Implementation of Predict using the SavedModel SignatureDef format.\r\nStatus SavedModelPredict(const RunOptions& run_options, ServerCore* core,\r\n                         const PredictRequest& request,\r\n                         PredictResponse* response) {\r\n  // Validate signatures.\r\n  ServableHandle<SavedModelBundle> bundle;\r\n  TF_RETURN_IF_ERROR(core->GetServableHandle(request.model_spec(), &bundle));\r\n\r\n  ......\r\n\r\n  if (privateLogLevel >= 1) {\r\n     clock_gettime(CLOCK_MONOTONIC, &start);\r\n  }\r\n+g_lock.lock();\r\n  TF_RETURN_IF_ERROR(bundle->session->Run(run_options, input_tensors,\r\n                                          output_tensor_names, {}, &outputs,\r\n                                          &run_metadata));\r\n+g_lock.unlock();\r\n\r\n  if (privateLogLevel >= 1) {\r\n      if (sessionRunCounter >= OFFSET) {\r\n              clock_gettime(CLOCK_MONOTONIC, &end);\r\n              elapsed_millisecs = (TimeSpecToSeconds(&end) - TimeSpecToSeconds(&start))*1000;\r\n              int currentIndex= (sessionRunCounter - OFFSET)%MAX_BUFFER_SIZE;\r\n              elapsedTimeBuffer[currentIndex]= elapsed_millisecs;\r\n              if (currentIndex == (MAX_BUFFER_SIZE-1))\r\n              {\r\n                  if (privateLogLevel >= 2) {\r\n                        std::cout << \"Elements: \";\r\n                        for (auto i = elapsedTimeBuffer.begin(); i != elapsedTimeBuffer.end(); ++i)\r\n`"]}, {"number": 19546, "title": "Estimator API for DQN?", "body": "Hello,\r\nThis is my first time using Estimator API. I'm trying to use Estimator to train a DQN on multiple GPUs. Here is my code:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom constants import *\r\n\r\nclass Net:\r\n    def __init__(self, n_state, n_action, dueling=False, grad_clip=10, double_q=True):\r\n        self.n_state = n_state\r\n        self.n_action = n_action\r\n        self.dueling = dueling\r\n        self.grad_clip = grad_clip\r\n        self.double_q = double_q\r\n\r\n        params ={'lr':LR_NN,\r\n                 'eps':1e-3,\r\n                 'gamma':DISCOUNT_FACTOR,\r\n                 'double_q':double_q,\r\n                 'grad_clip':grad_clip,\r\n                 'n_action':n_action}\r\n\r\n        self.model = tf.estimator.Estimator(model_fn=tf.contrib.estimator.replicate_model_fn(self._model_fn), params=params)\r\n\r\n    def _inference(self, state_in, scope_name, reuse=None):\r\n        with tf.variable_scope(scope_name, reuse=reuse):\r\n            h1 = tf.layers.conv2d(state_in, CONV1_SIZE, CONV1_KERNEL, CONV1_STRIDE, padding='same', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0))\r\n            h2 = tf.layers.conv2d(h1, CONV2_SIZE, CONV2_KERNEL, CONV2_STRIDE, padding='same', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0))\r\n            h3 = tf.layers.conv2d(h2, CONV3_SIZE, CONV3_KERNEL, CONV3_STRIDE, padding='same', activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0))\r\n            h4 = tf.layers.dense(tf.layers.flatten(h3), 256, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(2.0))\r\n            if self.dueling:\r\n                V = tf.layers.dense(h4, 1)\r\n                A = tf.layers.dense(h4, self.n_action)\r\n                Q = A + V - tf.reduce_mean(A, 1, keepdims=True)\r\n            else:\r\n                Q = tf.layers.dense(h4, self.n_action)\r\n        return Q\r\n\r\n    def _model_fn(inp, mode, params):\r\n        s = inp['s']\r\n        a = inp['a']\r\n        r = inp['r']\r\n        ns = inp['ns']\r\n        d = inp['d']\r\n\r\n        online_Q = self._inference(s, 'online')\r\n        target_Q = self._inference(ns, 'target')\r\n        next_online_Q = self._inference(ns, 'online', reuse=True)\r\n\r\n        if params['double_q']:\r\n            best = tf.reduce_sum(target_Q*tf.one_hot(tf.argmax(next_online_Q, axis=1), params['n_action']), axis=1, keepdims=True)\r\n        else:\r\n            best = tf.reduce_max(target_Q, axis=1, keepdims=True)\r\n        target = r + (1.0 - d)*params['gamma']*tf.stop_gradient(best)\r\n        pred = tf.reduce_sum(online_Q*tf.one_hot(a, params['n_action']), axis=1, keepdims=True)\r\n\r\n        loss = tf.losses.huber_loss(target, pred, reduction=tf.losses.Reduction.MEAN)\r\n\r\n        optimizer = tf.train.AdamOptimizer(params['lr'], epsilon=params['eps'])\r\n        optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\r\n\r\n        if mode == tf.estimator.ModeKeys.TRAIN:\r\n            if params['grad_clip'] is not None:\r\n                grads = optimizer.compute_gradients(loss)\r\n                for i, (grad, var) in enumerate(grads):\r\n                    if grad is not None:\r\n                        grads[i] = (tf.clip_by_norm(grad, params['grad_clip']), var)\r\n                optimizer_op = optimizer.apply_gradients(grads)\r\n            else:\r\n                optimizer_op = optimizer.minimize(loss)\r\n\r\n            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=optimizer_op)\r\n        elif mode == tf.estimator.ModeKeys.PREDICT:\r\n            pred_action = tf.argmax(online_Q)\r\n            return tf.estimator.EstimatorSpec(mode=mode, predictions=pred_action)\r\n\r\nif __name__ == '__main__':\r\n    sess = tf.InteractiveSession()\r\n    nn = Net(n_state=(84,84,4), n_action=4)\r\n    sess.run(tf.global_variables_initializer())\r\n    s = np.random.rand(32, 84, 84, 4)\r\n    ns = np.random.rand(32, 84, 84, 4)\r\n    a = np.random.randint(4, size=32)\r\n    r = np.random.randint(30, size=32).reshape(-1,1)\r\n    d = np.random.randint(2, size=32).reshape(-1,1)\r\n    inp = {}\r\n    inp['s'] = np.array(s)\r\n    inp['a'] = np.array(a)\r\n    inp['r'] = np.array(r)\r\n    inp['ns'] = np.array(ns)\r\n    inp['d'] = np.array(d)\r\n\r\n    input_f = tf.estimator.inputs.numpy_input_fn(x=inp, batch_size=32, shuffle=False)\r\n    nn.model.train(input_fn=input_f, steps=1)\r\n```\r\n\r\nThe output/error when running the last (final) line:\r\n```python\r\nINFO:tensorflow:Calling model_fn.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-1a67111ae305> in <module>()\r\n----> 1 nn.model.train(input_fn=input_f, steps=1)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    361 \r\n    362       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 363       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    364       logging.info('Loss for final step: %s.', loss)\r\n    365       return self\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1053       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1054     else:\r\n-> 1055       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1056 \r\n   1057   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1066       worker_hooks.extend(input_hooks)\r\n   1067       estimator_spec = self._call_model_fn(\r\n-> 1068           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n   1069       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1070                                              hooks, global_step_tensor,\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1041 \r\n   1042     logging.info('Calling model_fn.')\r\n-> 1043     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1044     logging.info('Done calling model_fn.')\r\n   1045 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py in replicated_model_fn(features, labels, mode, params, config)\r\n    238         config=config,\r\n    239         devices=devices,\r\n--> 240         local_ps_devices=ps_devices)\r\n    241 \r\n    242     if mode == model_fn_lib.ModeKeys.TRAIN:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py in _get_loss_towers(model_fn, mode, features, labels, params, config, devices, local_ps_devices, loss_reduction, name_scope_pattern)\r\n    522   tower_specs = []\r\n    523 \r\n--> 524   model_fn_args = util.fn_args(model_fn)\r\n    525   optional_params = {}\r\n    526   if 'params' in model_fn_args:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/util.py in fn_args(fn)\r\n     60     args = tf_inspect.getfullargspec(fn).args\r\n     61     if _is_bounded_method(fn):\r\n---> 62       args.remove('self')\r\n     63   return tuple(args)\r\n     64 \r\n\r\nValueError: list.remove(x): x not in list\r\n```\r\n\r\nAny idea where the error happens?", "comments": ["I'd guess it's because of this line:\r\n\r\n```python\r\n    def _model_fn(inp, mode, params):\r\n```\r\n\r\n...and changing it to the following should work around the error:\r\n\r\n```python\r\n    def _model_fn(self, mode, params):\r\n```\r\n\r\nAs to the bug, I'm not sure why the `args.remove('self')` is not `args.pop(0)` (to remove the first positional argument), but perhaps the original author @alexgorban knows?", "Thank you very much @mrry \r\nCouple of change, that I did, make the code work:\r\n```python\r\n    def _model_fn(self, features, labels, mode, params):\r\n        s = features['s']\r\n        a = features['a']\r\n        r = features['r']\r\n        ns = features['ns']\r\n        d = features['d']\r\n```\r\nand also need to change the type of input:\r\n```python\r\n    s = np.random.rand(32, 84, 84, 4).astype(np.float32)\r\n    ns = np.random.rand(32, 84, 84, 4).astype(np.float32)\r\n    a = np.random.randint(4, size=32).astype(np.int32)\r\n    r = np.random.randint(30, size=32).reshape(-1,1).astype(np.float32)\r\n    d = np.random.randint(2, size=32).reshape(-1,1).astype(np.float32)\r\n```\r\n\r\n\r\n**Couple things that does not work yet**:\r\n```python\r\nfor _ in range(10):\r\n    s = np.random.rand(32, 84, 84, 4).astype(np.float32)\r\n    ns = np.random.rand(32, 84, 84, 4).astype(np.float32)\r\n    a = np.random.randint(4, size=32).astype(np.int32)\r\n    r = np.random.randint(30, size=32).reshape(-1,1).astype(np.float32)\r\n    d = np.random.randint(2, size=32).reshape(-1,1).astype(np.float32)\r\n    inp = {}\r\n    inp['s'] = np.array(s)\r\n    inp['a'] = np.array(a)\r\n    inp['r'] = np.array(r)\r\n    inp['ns'] = np.array(ns)\r\n    inp['d'] = np.array(d)\r\n\r\n    input_f = tf.estimator.inputs.numpy_input_fn(x=inp, batch_size=32, shuffle=False)\r\n    nn.model.train(input_fn=input_f)\r\n```\r\nTensorflow seems to run initialization every iteration:\r\n```bash\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\n```\r\n\r\nAlso prediction doesn't seem to work:\r\n```python\r\ninp = {'s':np.array(s)}\r\npred_f = tf.estimator.inputs.numpy_input_fn(x=inp, batch_size=1, shuffle=False)\r\npre = nn.model.predict(input_fn=pred_f) # This return a \"generator object Estimator.predict\" instead\r\n```\r\nAny helps are appreciated!!!", "Nagging Assignee @cy89: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @cy89: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19545, "title": "[tfgan] Add possibility to export GANEstimator saved model", "body": "This allows GANEstimator's to be easily exported via [`GANEstimator::export_saved_model()`](https://www.tensorflow.org/api_docs/python/tf/contrib/gan/estimator/GANEstimator#export_savedmodel).\r\n\r\nCurrently when trying to when trying to export the model, `.export_saved_model()` raises:\r\n```python-traceback\r\nValueError: export_outputs must be a dict and not<class 'NoneType'>\r\n```\r\n\r\nFixes https://stackoverflow.com/questions/48448431/tensorflow-tfgan-ganestimator-export-model", "comments": ["I have stumbled upon this limitation too.", "@joel-shor Friendly ping \ud83d\ude09 \r\n\r\nAre we able to get this reviewed before tf 1.9 is out, since this PR is somewhere in between a feature and a bug fix?", "Seem to be test failures...\r\n\r\nFAIL: test_modes_predict (__main__.GANHeadTest)\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/bin/bazel_pip/tensorflow/contrib/gan/head_test.runfiles/org_tensorflow/bazel_pip/tensorflow/contrib/gan/python/estimator/python/head_test.py\", line 81, in test_modes_predict\r\n    self.assertItemsEqual(('predict',), spec.export_outputs.keys())\r\nAssertionError: Element counts were not equal:\r\nFirst has 0, Second has 1:  'serving_default'\r\n\r\nAlso, in response to your previous comment, I think this will be too late to make it into TF 1.9 release", "The test fails because spec.export_output.keys() also includes the default key \"serving_default\".\r\n\r\nFrom estimator.export_savedmodel:\r\n\r\n>     The exported `MetaGraphDef` will provide one `SignatureDef` for each\r\n>     element of the export_outputs dict returned from the model_fn, named using\r\n>     the same keys.  One of these keys is always\r\n>     signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which\r\n>     signature will be served when a serving request does not specify one.\r\n>     For each signature, the outputs are provided by the corresponding\r\n>     `ExportOutput`s, and the inputs are always the input receivers provided by\r\n>     the serving_input_receiver_fn.\r\n\r\nNote that signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is \"serving_default\".\r\n[https://github.com/tensorflow/tensorflow/blob/57031c7565fe996270bafdd1940d50c37be35d04/tensorflow/python/saved_model/signature_constants.py](url)\r\n\r\nMaybe this should be imported in the test and the constant included in the assertion?\r\n", "@case540 @AndrewLeach Thanks for the help. I added the default serving key to the test.\r\n\r\nSorry for pushing a failing unit test. I still didn't found time to build tensorflow from source on my machine so I'm unable to run the test suite locally for now. Since this is only a small python fix I'm using   this as a patch on top of tf@1.8 installed via pip for my work.\r\n\r\n> Also, in response to your previous comment, I think this will be too late to make it into TF 1.9 release\r\n\r\nSure no problem \ud83d\udc4d \r\n", "@lgeiger Thanks for adding the feature!\r\n@joel-shor Looks like it should be good to go, would be awesome to get this merged soon :D"]}, {"number": 19544, "title": "Remove duplicate `from six import text_type` in upload_test_benchmarks.py", "body": "The `from six import text_type` was imported twice in upload_test_benchmarks.py, this fix removes the duplication.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]