[{"number": 50105, "title": " error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04.1 [64-bit]\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.7.5 [64-bit]\r\n- Installed using: virtualenv\r\n- Bazel version: 3.4.0\r\n- GCC/Compiler version (if compiling from source):  7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) \r\n- Hardware: Raspberry Pi 4 Model B Rev 1.4\r\n- Architecture: aarch64\r\n\r\n**Describe the problem**\r\nI could build TF 2.4 on a RPi 4 with Ubuntu 18.04 (64-bit) using bazel. However, when I tried doing this on a fresh one, it failed to build. What should I look into to make a comparison and see where the problem is?\r\nBoth have the same GCC version (7.5.0) and same bazel version (3.4.0).  Also in both caes, I'm using virtual enviornment (Python 3.7). I get this error:\r\n`tensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'\r\n                      dense_shape.AddDimWithStatus(input_shape_flat(i)));`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n<pre>ubuntu@ubuntu:~$ sudo apt-get install build-essential pkg-confi gi2c-tools avahi-utils joystick libopenjp2\u20137-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp\r\n\r\nubuntu@ubuntu:~$ sudo apt-get install python3.7 python3.7-dev python3.7-venv python3.7-distutils python3.7-lib2to3 python3.7-gdbm python3.7-tk python3-pip\r\n\r\nubuntu@ubuntu:~$ python3.7 -m venv py37env\r\n\r\nubuntu@ubuntu:~$ source py37env/bin/activate\r\n\r\n(py37env) ubuntu@ubuntu:~$ python3.7 -m pip install \u2014 upgrade pip\r\n\r\n(py37env) ubuntu@ubuntu:~$ pip install cython  wheel numpy h5py pybind11\r\n\r\n(py37env) ubuntu@ubuntu:~$ pip install -U keras_preprocessing \u2014 no-deps\r\n\r\n(py37env) ubuntu@ubuntu:~$ git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n(py37env) ubuntu@ubuntu:~$ cd tensorflow/\r\n\r\n(py37env) ubuntu@ubuntu:~/tensorflow$ git checkout r2.4\r\n\r\nubuntu@ubuntu:~/tensorflow$ python3.7 configure.py \r\nYou have bazel 3.4.0 installed.\r\nPlease specify the location of python. [Default is /home/ubuntu/py37env/bin/python3.7]:\r\nFound possible Python library paths:\r\n /home/ubuntu/py37env/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use. Default is [/home/ubuntu/py37env/lib/python3.7/site-packages]\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\nPlease specify optimization flags to use during compilation when bazel option \u201c \u2014 config=opt\u201d is specified [Default is -Wno-sign-compare]:\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\nPreconfigured Bazel build configs. You can use any of the below by adding \u201c \u2014 config=<>\u201d to your build command. See .bazelrc for more details.\r\n \u2014 config=mkl # Build with MKL support.\r\n \u2014 config=mkl_aarch64 # Build with oneDNN support for Aarch64.\r\n \u2014 config=monolithic # Config for mostly static monolithic build.\r\n \u2014 config=ngraph # Build with Intel nGraph support.\r\n \u2014 config=numa # Build with NUMA support.\r\n \u2014 config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n \u2014 config=v2 # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n \u2014 config=noaws # Disable AWS S3 filesystem support.\r\n \u2014 config=nogcp # Disable GCP support.\r\n \u2014 config=nohdfs # Disable HDFS support.\r\n \u2014 config=nonccl # Disable NVIDIA NCCL support.\r\n\r\n(py37env) ubuntu@ubuntu:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n(py37env) ubuntu@ubuntu:~/tensorflow$ python\r\nPython 3.7.5 \r\n[GCC 8.4.0] on linux\r\nType \u201chelp\u201d, \u201ccopyright\u201d, \u201ccredits\u201d or \u201clicense\u201d for more information.\r\n>>> </pre>\r\n\r\n\r\n**Any other info / logs**\r\n<pre>(py37env) ubuntu@ubuntu:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=142\r\nINFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/py37env/bin/python3.7 --action_env PYTHON_LIB_PATH=/home/ubuntu/py37env/lib/python3.7/site-packages --python_path=/home/ubuntu/py37env/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/ubuntu/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/ubuntu/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/ubuntu/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/ubuntu/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (171 packages loaded, 26394 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:4689:18: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)\r\nIn file included from ./tensorflow/core/framework/op_kernel.h:35:0,\r\n                 from tensorflow/core/kernels/sparse_split_op.cc:19:\r\ntensorflow/core/kernels/sparse_split_op.cc: In member function 'void tensorflow::SparseSplitOp<T>::Compute(tensorflow::OpKernelContext*)':\r\ntensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'\r\n                      dense_shape.AddDimWithStatus(input_shape_flat(i)));\r\n                                  ^\r\n./tensorflow/core/framework/op_requires.h:52:29: note: in definition of macro 'OP_REQUIRES_OK'\r\n     ::tensorflow::Status _s(__VA_ARGS__);                    \\\r\n                             ^~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 46060.242s, Critical Path: 750.27s\r\nINFO: 9369 processes: 9369 local.\r\nFAILED: Build did NOT complete successfully</pre>\r\n", "comments": ["@hassan-shehawy \r\nThis issue is already reported and tracked at #49850.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50105\">No</a>\n"]}, {"number": 50104, "title": "Incorrect figure on word2vec tutorial", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/word2vec#summary\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe summary figure on https://www.tensorflow.org/tutorials/text/word2vec#summary seems incorrect.\r\n\r\nThe figure shows the index of shimmered is 7 while the code above says the index is 5. \r\n\r\nThe red part of the figure has word 'temperature' and 'code', which didn't appear in the input sentence at all. \r\n\r\nThe figure also has indexes like 784 and 589. I don't know what's going on here.\r\n\r\nAs a result, the figure becomes confusing for learners.\r\n\r\nPlease fix the figure.\r\n\r\n### Submit a pull request?\r\n\r\nNo\r\n", "comments": ["@MarkDaoust  I would like to work on this issue.\r\nIs there any software or website that can help me for correcting this image ?", "1. Are the indices in the code deterministic? \r\n2. It may be easier to update the code to match the images.\r\n3. Usually for diagrams like this I recommend using google-drawings. \r\n\r\nThe original author for that tutorial was @enigmaeth (Afroz).\r\n\r\nAfroz, could you share a public link to those drawing files so @aavishkarmishra might copy and fix them?\r\n", "@gqqnbig \r\nIs this still an issue", "My description still holds true and I didn't see any documentation changes or code changes. I suspect it is still an issue.", "Actually here the words 'temperature' and 'code' didn't appear in the input sentence but they are some other words used from the vocabulary (A dictionary of all the words in a corpus). Similarly, the indexes in figures like 784 and 598, are indices of some other words from the vocabulary. Hence, this part of the figure is correct. However, this seems confusing for the first time, hence I have added a note to explain the same below the picture. Though I agree that, index of the word shimmered should be 4 instead of 7, and the index of wide should be 2 instead of 7 in red boxes. Hence, I have corrected the same.", "https://github.com/tensorflow/docs/pull/1999"]}, {"number": 50103, "title": "Ambiguous examples on word2vec tutorial?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/text/word2vec\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe code of the tutorial outputs the following:\r\n\r\n```\r\ntarget_index    : 3\r\ntarget_word     : road\r\ncontext_indices : [1 2 1 4 3]\r\ncontext_words   : ['the', 'wide', 'the', 'shimmered', 'road']\r\nlabel           : [1 0 0 0 0]\r\n```\r\n\r\nThe context word 'the' has label both 1 and 0. I feel this example would be ambiguous.\r\n\r\nWhen generating negative samples, should the one in postive samples be excluded?\r\n\r\n### Submit a pull request?\r\n\r\nNo. I'm learning word2vec, not capable of writing a correct algorithm.", "comments": ["duplicate"]}, {"number": 50102, "title": "[MLIR][DISC] Add initial version of LhloLegalizeRootsToParallelLoops and InputInlineFusion", "body": "This PR is currently WIP for a discussion in advance. More UTs will be added.\r\n\r\nThe basic codegen flow will be:\r\nstep 1, LhloLegalizeRootsToParallelLoops\r\nstep 2, InputInlineFusion\r\nstep 3, canonicalize & CSE to optimize the redundant index calculations, we will need a MemRefLoadCSE here since the general cse will not process memref.load (this is only an optimization and will sent PRs as second priority)\r\nstep 4, Inline the lmhlo.Fusion\r\nstep 5, ParallelToGPULaunch/ParallelToOpenMP\r\n\r\nCodegen schedules other than the basic loop schedule are seperated into incoming PRs. And lmhlo Ops support other than RealDynamicSlice, DynamicBroadcastInDim & BroadcastInDim in lhlo_elemental_util are also seperated into subsequent PRs.\r\nOther known TODOs includes:\r\n(1) There is potential redundant Linearize/Delinearize calculations in the initial version. memref.LinearizeIndex/Delinearize op will be brought in future PRs to optimize the index calculation. As discussed in https://llvm.discourse.group/t/add-an-expanded-load-store-op-in-memref-dialect/3503/17.\r\n\r\nfusion_utils depends on https://github.com/tensorflow/tensorflow/pull/50020 and will be removed after it's done.", "comments": ["Can you add the missing CMake files to keep the standalone repo operational https://github.com/tensorflow/mlir-hlo ?", "Overall question: I have two modes of reviewing (1) review something usable by general mlir-hlo users, which I'll take a serious look at the design of, and probably ask for fixing/a plan for all \"hacky\" bits; or (2) reviewing something as part of the e2e pipeline, not \"drop in replacements\" for general mlir-hlo users. But I'd expect a distinct `disc` directory for this kind of files. However, now I see names like `disc_utils.h` *not* under a `disc` directory.\r\n\r\nI'm fine with reviewing in either mode, just want to set the expectations correctly. @joker-eph how do you want to review this?", "I suspect there are things in both categories: in general we should aim to have as much as possible of transformations that are generally applicable and runtime independent. Then there will likely be a bunch of transformations specific to the runtime or to some of the things done in DISC. Indeed it'd be nice to clearly separate both categories.\r\n\r\nIn general transforming an HLO op into a loop nest should be independent and reusable for example.", "> Overall question: I have two modes of reviewing (1) review something usable by general mlir-hlo users, which I'll take a serious look at the design of, and probably ask for fixing/a plan for all \"hacky\" bits; or (2) reviewing something as part of the e2e pipeline, not \"drop in replacements\" for general mlir-hlo users. But I'd expect a distinct `disc` directory for this kind of files. However, now I see names like `disc_utils.h` _not_ under a `disc` directory.\r\n> \r\n> I'm fine with reviewing in either mode, just want to set the expectations correctly. @joker-eph how do you want to review this?\r\n\r\nI hope it's (1), at least for this PR. However, there are still some logics are highly related with the context of the compiler passpipeline. For example, here we lower the GPU nodes into parallel loops and CPU nodes into loops. This is related with the design of the whole pass pipeline. \r\n\r\nThe file name 'disc_util.h' is tricky and i'll seperate the contents in our disc_util into multiple files according to features, such as 'codegen_util', 'fusion_util', 'placement_util' etc. ", "> Just had a quick look at `input-inline-fusion`, but maybe I should have started with the legalization one?\r\n> (if they are independent, it'd be easier to review in two PRs)\r\n\r\nYes, please start from LhloLegalizeRootsToParallelLoops pass. The two passes are highly coupled so I put them in one PR. The first pass expand the root nodes of a fusion pattern into loops, and the second pass fuse the rest of nodes into the loops. They work together as an XLA-style fusion codegen. Refer to http://pai-blade.cn-hangzhou.oss.aliyun-inc.com/disc/fusion_codegen.png", "Hi, @joker-eph @timshen91 \r\nI fixed all the existing comments (except the CMake one is ongoing). Please proceed with the code review and let me know if any concerns. Thanks.", "I did some high-level reviews and I didn't dive into too much details. Will keep looking.", "> Can you add the missing CMake files to keep the standalone repo operational https://github.com/tensorflow/mlir-hlo ?\r\n\r\ndone", "> I did some high-level reviews and I didn't dive into too much details. Will keep looking.\r\n\r\nthe existed comments are fixed for now, pls proceed:)", "(flushing some pending comments I had accumulated)", "> (flushing some pending comments I had accumulated)\r\n\r\nThe code conficts has been fixed. Please have a look and proceed.", "> I did some high-level reviews and I didn't dive into too much details. Will keep looking.\r\n\r\n@timshen91 any update here?", "> Other than the `elemwise{Fuse,Lower}HelperOr` and that we should adopt the existing elementwise trait, I don't have other comments worth mentioning. I do believe that if I take a microscope I'll have more comments, but I decided to leave out those nits.\r\n\r\nHere it is not only because of the existing mapping of lhlo -> std is with templates, but also the elementalLower<> in lhlo_elemental_utils.cc is also implemented with templates. Here \"elementwise_op_list.h\" should be renamed as \"disc_supported_elemwise_op_list.h\", will this be better and acceptable? Anyhow we need to maintain a \"closed collection of all supported possible ops\" for a certain compiler implementation, such kind of information should not be put in the dialects itself.\r\n\r\nBTW I also agree it's not strictly necessary to mapping lhlo -> std with templates. However the fact is map_lmhlo_to_scalar_op.h has already been used in several existing passes  (lhlo_legalize_to_affine, legalize_to_linalg & lhlo_legalize_to_gpu). The code amount of map_lmhlo_to_scalar_op.h is more than 1000 lines, so i believe the effort to get rid of templates will not be trivial. Maybe we should hear the comments of Adrian Kuegel <akuegel@google.com> here (how to @Adrian Kuegel here?).", "> > Other than the `elemwise{Fuse,Lower}HelperOr` and that we should adopt the existing elementwise trait, I don't have other comments worth mentioning. I do believe that if I take a microscope I'll have more comments, but I decided to leave out those nits.\r\n> \r\n> Here it is not only because of the existing mapping of lhlo -> std is with templates, but also the elementalLower<> in lhlo_elemental_utils.cc is also implemented with templates. Here \"elementwise_op_list.h\" should be renamed as \"disc_supported_elemwise_op_list.h\", will this be better and acceptable? Anyhow we need to maintain a \"closed collection of all supported possible ops\" for a certain compiler implementation, such kind of information should not be put in the dialects itself.\r\n> \r\n> BTW I also agree it's not strictly necessary to map elementwise lhlo -> std with templates. However the fact is map_lmhlo_to_scalar_op.h has already been used in several existing passes (lhlo_legalize_to_affine, legalize_to_linalg & lhlo_legalize_to_gpu). The code amount of map_lmhlo_to_scalar_op.h is more than 1000 lines, so i believe the effort to get rid of templates will not be trivial. Maybe we should hear the comments of Adrian Kuegel [akuegel@google.com](mailto:akuegel@google.com) here (how to @adrian Kuegel here?).\r\n\r\n", "> > > Other than the `elemwise{Fuse,Lower}HelperOr` and that we should adopt the existing elementwise trait, I don't have other comments worth mentioning. I do believe that if I take a microscope I'll have more comments, but I decided to leave out those nits.\r\n> > \r\n\r\nHere it is not only because of the existing mapping of lhlo -> std is with templates, but also the elementalLower<> in lhlo_elemental_utils.cc is also implemented with templates. Anyhow we need to maintain a \"closed collection of all supported possible ops\" for a certain compiler implementation, such kind of information should not be put in the dialects itself. Here \"elementwise_op_list.h\" should be renamed as \"disc_supported_elemwise_op_list.h\", will this be better and acceptable?\r\n\r\nBTW I agree it's not strictly necessary to map elementwise lhlo -> std with templates. However the fact is map_lmhlo_to_scalar_op.h has already been used in several existing passes (lhlo_legalize_to_affine, legalize_to_linalg & lhlo_legalize_to_gpu). The code amount of map_lmhlo_to_scalar_op.h is more than 1000 lines, so i believe the effort to get rid of templates will not be trivial. Maybe we should hear the comments of Adrian Kuegel [akuegel@google.com](mailto:akuegel@google.com) here (how to @adrian Kuegel here?).\r\n\r\n", "> > > > Other than the `elemwise{Fuse,Lower}HelperOr` and that we should adopt the existing elementwise trait, I don't have other comments worth mentioning. I do believe that if I take a microscope I'll have more comments, but I decided to leave out those nits.\r\n> \r\n> Here it is not only because of the existing mapping of lhlo -> std is with templates, but also the elementalLower<> in lhlo_elemental_utils.cc is also implemented with templates. Anyhow we need to maintain a \"closed collection of all supported possible ops\" for a certain compiler implementation, such kind of information should not be put in the dialects itself. Here \"elementwise_op_list.h\" should be renamed as \"disc_supported_elemwise_op_list.h\", will this be better and acceptable?\r\n> \r\n> BTW I agree it's not strictly necessary to map elementwise lhlo -> std with templates. However the fact is map_lmhlo_to_scalar_op.h has already been used in several existing passes (lhlo_legalize_to_affine, legalize_to_linalg & lhlo_legalize_to_gpu). The code amount of map_lmhlo_to_scalar_op.h is more than 1000 lines, so i believe the effort to get rid of templates will not be trivial. Maybe we should hear the comments of Adrian Kuegel [akuegel@google.com](mailto:akuegel@google.com) here (how to @adrian Kuegel here?).\r\n\r\nReplied under the `elementwise_op_list.h` comment from Jacques.", "> I don't have other comments. I'm not sure if this PR is meant to be submitted or just for reviewing, so I'll leave the approval to others with more knowledge of that.\r\n\r\nThanks Tim. \r\n@joker-eph , all the comments has been fixed for now, would you further have a look?", "> I'm in the process of merging this, our internal checks required me to fix a few clang-tidy and other linter related stuff, as well as shown some test coverage issue I pointed inline. Can you follow up with another PR after this one is merged?\r\n\r\nSure. There will be a number of PRs following this one to fullfill the coverage of ops & codegen schedules, (I manually seperate them previously for code review), and the coverage for now will be solved after these PRs are done.", "> I'm in the process of merging this, our internal checks required me to fix a few clang-tidy and other linter related stuff, as well as shown some test coverage issue I pointed inline. Can you follow up with another PR after this one is merged?\r\n\r\nI updated with a fix on comments (missing doc in lhlo_elemental_utils.h). And remove some currently used functions in the utils. For the rest of the coverage issue, i prefer to keep them as they are for now, and the code will be covered in the following PRs. ", "> > I'm in the process of merging this, our internal checks required me to fix a few clang-tidy and other linter related stuff, as well as shown some test coverage issue I pointed inline. Can you follow up with another PR after this one is merged?\r\n> \r\n> I updated with a fix on comments (missing doc in lhlo_elemental_utils.h). And remove some currently used functions in the utils. For the rest of the coverage issue, i prefer to keep them as they are for now, and the code will be covered in the following PRs.\r\n\r\nUnfortunately I was in the process of merging this, which required some changes in the process: this is why I mentioned to follow up with another PR. \r\nIn general we're cautious about landing code that has low-coverage, and in particular when entire functions are dead."]}, {"number": 50101, "title": "[MLIR][DISC] pattern conversion from tf2mhlo: add ConvertPrintOp", "body": "We are porting our MLIR-based dynamic shape compiler to tf community (From OP def, Patttern, to Optimization pass, etc).\r\nThis is the 6th PR about tf2mhlo pattern conversion, which including ConvertPrintOp. We use printOp to debug func issue.\r\nThe rest pattern conversions we will add:\r\n\r\n- ConvertSqueezeOpxxx\r\n- ConvertStridedSliceOpxxx\r\n", "comments": ["@azazhu  Can you please resolve conflicts? Thanks!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50101) for more info**.\n\n<!-- need_author_cla -->", "@azazhu  Can you please resolve conflicts? Thanks!", "> @azazhu Can you please resolve conflicts? Thanks!\r\n\r\nDone"]}, {"number": 50100, "title": "[MLIR][DISC] Bufferize DynamicIotaOp and DynamicPadOp", "body": "support hlo-to-lhlo conversion for DynamicIotaOp and DynamicPadOp", "comments": []}, {"number": 50099, "title": "Tensoflow tf.data.Dataset.from_tensor_slices couldn't generate a simple dataset to fit a linear model", "body": "Just want to generate a dataset by myself, but it get errors. It puzzled me a lot. Any help will be greatly appreciated!\r\n\r\nMy system is Windows 10, Tensorflow version is tensorflow-cpu 2.5.\r\n\r\nHere is code:\r\n\r\n      import tensorflow as tf\r\n      import numpy as np\r\n    \r\n      def simulation_linear_function(x, a, b):  # y = ax+b\r\n          y = tf.matmul(a, x)+b\r\n          return y\r\n      \r\n      # Generate data set\r\n      num_input = 3\r\n      num_dense_unit = 2\r\n      a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))\r\n      b = np.array([0.5,0.6]).reshape((2,1))\r\n      \r\n      num_data = 300\r\n      inputs = np.random.random((num_data,2))\r\n      labels = np.zeros((num_data,2))\r\n      for n in range(num_data):\r\n          x = inputs[n]\r\n          y = simulation_linear_function(np.reshape(x, (2,1)), a, b)\r\n          labels[n] = np.reshape(y, (2,))\r\n      dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n      \r\n      linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))\r\n      linear_model = tf.keras.Sequential([linear_layer])\r\n      \r\n      linear_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(1))\r\n      history = linear_model.fit(dataset, epochs=100, verbose=True)\r\n      # history = linear_model.fit(inputs, labels, epochs=100, verbose=True)\r\n  \r\nThe error is: Traceback (most recent call last): ..... ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape (2, 1)\r\n\r\nBut if you switch to the last commented line, the model can be trained correctly.\r\n\r\nThanks, Tom", "comments": [">  `b = np.array([0.5,0.6]).reshape((2,1))`\r\n\r\nThis line should be:\r\n\r\n `b = np.array([0.5,0.6]).reshape((1,2))`\r\n\r\nand `y = simulation_linear_function(np.reshape(x, (2,1)), a, b)` should be\r\n\r\n`y = simulation_linear_function(np.reshape(x, (1,2)), a, b)`\r\n", "I tested your modification.  But other errors came out:\r\n\r\n              Traceback (most recent call last):\r\n                File \"D:/Learning/Tensorflow/dataset_issue.py\", line 21, in <module>\r\n                  y = simulation_linear_function(np.reshape(x, (1, 2)), a, b)\r\n                File \"D:/Learning/Tensorflow/dataset_issue.py\", line 5, in simulation_linear_function\r\n                  y = tf.matmul(a, x)+b\r\n                File \"D:\\Learning\\Tensorflow\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 206, in wrapper\r\n                  return target(*args, **kwargs)\r\n                File \"D:\\Learning\\Tensorflow\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 3490, in matmul\r\n                  a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\r\n                File \"D:\\Learning\\Tensorflow\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5699, in mat_mul\r\n                  _ops.raise_from_not_ok_status(e, name)\r\n                File \"D:\\Learning\\Tensorflow\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 6897, in raise_from_not_ok_status\r\n                  six.raise_from(core._status_to_exception(e.code, message), None)\r\n                File \"<string>\", line 3, in raise_from\r\n              tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] mismatch In[1] shape: 2 vs. 1: [2,2] [1,2] 0 0 [Op:MatMul]", "Note that these errors are due to shape mismatch only. If you trace the shapes in your tensors correctly, then these erorrs will definitely go away.", "Those errors are introduced when making the change as you suggested. Maybe I didn't explain the issue well. Let me re-explain it in other way. This is a simple piece of code to test tf.data.Dataset. I generate \"inputs\" and \"labels\" by myself. If I use \"inputs\" and \"labels\" directly in the training:\r\n      history = linear_model.fit(inputs, labels, epochs=100, verbose=True)\r\nThe code is running successfully without error, But if I use tf.data.Dataset:\r\n      dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n      history = linear_model.fit(dataset, epochs=100, verbose=True)\r\nthen there are error out:\r\n      .....\r\n      ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape (2, 1)\r\n\r\nSo it seems that the issue is only caused when using \"dataset\".\r\nTo make issue more clear, I re-orgnized the code by introducing a flag: \"use_dataset_flag\". If it is set to true, \"dataset\" will be used and you will see the error; if it is set to False, \"inputs\" and \"labels\" are used directly, the running will be success without error.  If you could have try, you will see the issue clearly. Here is the code:\r\n\r\n              import tensorflow as tf\r\n              import numpy as np\r\n              \r\n              def simulation_linear_function(x, a, b):  # y = ax+b\r\n                  y = tf.matmul(a, x)+b\r\n                  return y\r\n              \r\n              use_dataset_flag = True\r\n              \r\n              # Generate data set\r\n              num_input = 3\r\n              num_dense_unit = 2\r\n              a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))\r\n              b = np.array([0.5,0.6]).reshape((2,1))\r\n              \r\n              num_data = 300\r\n              inputs = np.random.random((num_data,2))\r\n              labels = np.zeros((num_data,2))\r\n              for n in range(num_data):\r\n                  x = inputs[n]\r\n                  y = simulation_linear_function(np.reshape(x, (2,1)), a, b)\r\n                  labels[n] = np.reshape(y, (2,))\r\n              \r\n              \r\n              linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))\r\n              linear_model = tf.keras.Sequential([linear_layer])\r\n              linear_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(1))\r\n              \r\n              if use_dataset_flag is False:\r\n                  history = linear_model.fit(inputs, labels, epochs=100, verbose=True)\r\n              else:\r\n                  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                  history = linear_model.fit(dataset, epochs=100, verbose=True)\r\n\r\n\r\n", "Use\r\n\r\n```python\r\ninputs = inputs.reshape((num_data,1,2))\r\nlabels = labels.reshape((num_data,1,2))\r\n```\r\n\r\nbefore the if statement. It worked in Colab. ", "Thanks for quick response! I added the two lines, it is running without error. But the result with \"dataset\" seems not correct. You can turn the flag on/off and had a quick comparison. \r\nMy model input is defined as input_shape=(2,), so the input shape should be (num_data, 2), but what is the reason we have to change to (num_data, 1, 2), when applying \"dataset\", while we can use (num_data, 2) when applying inputs and labels directly.", "Note that when you run with `use_dataset = False` and without those lines you get a warning: \r\n```\r\nWARNING:tensorflow:Model was constructed with shape (None, None, 2) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 2), dtype=tf.float32, name='dense_1_input'), name='dense_1_input', description=\"created by layer 'dense_1_input'\"), but it was called on an input with incompatible shape (None, 2).\r\n```\r\n\r\nSo we now know that the model expects (batch_size,  channels) type of input but your input is only (channels, )\r\nFor more info see [here](https://keras.io/api/layers/core_layers/dense/). \r\n\r\nThus when we reshape to (num_data,1,2) wee add a batch_size dimension, as num_data is the whole dataset, and when we do `dataset_from_slices` we are basically saying that the elements of the dataset look like this (one row at a time):\r\n`(inputs = [1.32, 1.45]), (labels = [3.44, 6.87])` (I took random numbers, nothing special about them). Thus, if the data has the shape `(num_data,2)` then it will not have a batch_size dimension when it gets passed to the dataset.\r\n\r\nHope this helps.", "What I observed is opposite. The warning message you mentioned is introduced by the two line you added:\r\n              inputs = inputs.reshape((num_data,1,2))\r\n              labels = labels.reshape((num_data,1,2))\r\nIf you comment out these two lines and set use_dataset = False, there is no warning message. I also double checked it on Colab:\r\n              import tensorflow as tf\r\n              import numpy as np\r\n              \r\n              def simulation_linear_function(x, a, b):  # y = ax+b\r\n                  y = tf.matmul(a, x)+b\r\n                  return y\r\n              \r\n              use_dataset_flag = False\r\n              \r\n              # Generate data set\r\n              num_input = 3\r\n              num_dense_unit = 2\r\n              a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))\r\n              b = np.array([0.5,0.6]).reshape((2,1))\r\n              \r\n              num_data = 300\r\n              inputs = np.random.random((num_data,2))\r\n              labels = np.zeros((num_data,2))\r\n              for n in range(num_data):\r\n                  x = inputs[n]\r\n                  y = simulation_linear_function(np.reshape(x, (2,1)), a, b)\r\n                  labels[n] = np.reshape(y, (2,))\r\n              \r\n              \r\n              linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))\r\n              linear_model = tf.keras.Sequential([linear_layer])\r\n              linear_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(1))\r\n              \r\n              # inputs = inputs.reshape((num_data,1,2))\r\n              # labels = labels.reshape((num_data,1,2))\r\n              if use_dataset_flag is False:\r\n                  history = linear_model.fit(inputs, labels, epochs=10, verbose=True)\r\n              else:\r\n                  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                  history = linear_model.fit(dataset, epochs=10, verbose=True)\r\n ", "Right, it's because when Dataset is not used, the whole array of (10,2) is processed in one go.  But the Dataset contains tuples of (input[i] , labels[i]). ", "Then, this come back to my original question. \r\nGiven \" linear_model.fit(inputs, labels, epochs=10, verbose=True) \" works, \r\nhow to construct a dataset to make \" linear_model.fit(dataset, epochs=10, verbose=True) \"  work.\r\nIf  \"dataset = tf.data.Dataset.from_tensor_slices((inputs, labels)) doesn't work\", what is your suggestion?\r\nThanks,", "Adding the two lines will definitely work. Other than that it depends upon your use case. See Dataset.batch() and Dataset.map() for more info. \r\nRemember, the Model expects (batch_size, channel) type of tensor. That's all you need to keep in mind. ", "As you know, adding those two lines will cause warning when setting use_dataset_flag = False. So I move two lines under the if-else condition with dataset. I tried to make sure I will get similar result with dataset application comparing to applying \"input\", \"label\" pair directly. But the results are very different. This a very simple linear model, and converge should be very quick. When applying \"input\" and \"label\" directly, that is, set use_dataset_flag = False,  the model trained very quick as expected:\r\n\r\n              Epoch 1/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.8602\r\n              Epoch 2/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2004\r\n              Epoch 3/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0719\r\n              Epoch 4/10\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0307\r\n              Epoch 5/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0118\r\n              Epoch 6/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0047\r\n              Epoch 7/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0019\r\n              Epoch 8/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 7.3513e-04\r\n              Epoch 9/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 2.5075e-04\r\n              Epoch 10/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 1.0522e-04\r\n\r\nBut if you use_dataset_flag = True, the result is very strange and the training is not converged:\r\n\r\n            Epoch 1/10\r\n            300/300 [==============================] - 0s 366us/step - loss: 0.0676\r\n            Epoch 2/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.2486\r\n            Epoch 3/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 9.5453e-04\r\n            Epoch 4/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.0278\r\n            Epoch 5/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.2648\r\n            Epoch 6/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.0272\r\n            Epoch 7/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.1645\r\n            Epoch 8/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.1767\r\n            Epoch 9/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.1414\r\n            Epoch 10/10\r\n            300/300 [==============================] - 0s 313us/step - loss: 0.0012\r\n\r\nI feel somehow we may not use dataset correctly and the data are messed up in it.  Here is the code:\r\n\r\n              import tensorflow as tf\r\n              import numpy as np\r\n              \r\n              def simulation_linear_function(x, a, b):  # y = ax+b\r\n                  y = tf.matmul(a, x)+b\r\n                  return y\r\n              \r\n              use_dataset_flag = True\r\n              \r\n              # Generate data set\r\n              num_input = 3\r\n              num_dense_unit = 2\r\n              a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))\r\n              b = np.array([0.5,0.6]).reshape((2,1))\r\n              \r\n              num_data = 300\r\n              inputs = np.random.random((num_data,2))\r\n              labels = np.zeros((num_data,2))\r\n              for n in range(num_data):\r\n                  x = inputs[n]\r\n                  y = simulation_linear_function(np.reshape(x, (2,1)), a, b)\r\n                  labels[n] = np.reshape(y, (2,))\r\n              \r\n              \r\n              linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))\r\n              # linear_layer = tf.keras.layers.Dense(units=2)\r\n              linear_model = tf.keras.Sequential([linear_layer])\r\n              linear_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(1))\r\n              \r\n              if use_dataset_flag is False:\r\n                  history = linear_model.fit(inputs, labels, epochs=10, verbose=True)\r\n              else:\r\n                  inputs = inputs.reshape((num_data, 1, 2))\r\n                  labels = labels.reshape((num_data, 1, 2))\r\n                  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                  history = linear_model.fit(dataset, epochs=10, verbose=True)\r\n\r\n", "@tom970 ,\r\n\r\nPlease take a look @AdityaKane2001 above comment and let us know if it helps to resolve the issue.Also please take a look at [link](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) for more information on dataset.\r\n\r\nThanks!", "@tom970 \r\nAs I said earlier,\r\n>Right, it's because when Dataset is not used, the whole array of (10,2) is processed in one go. But the Dataset contains tuples of (input[i] , labels[i]).\r\n\r\n\r\nWhen the model is trained on plain numpy arrays, it takes whole data as one batch. On the other hand, when Dataset is used, the model takes *_each_* entry as a separate data point and thus the weights update is much more noisier. This is causing the discrepancy. \r\n", "If it is that case, I set the batch size of dataset as 300 (the whole data), then the performance should the similar. But I didn't see that:\r\nHere is code change:\r\n              ...\r\n              if use_dataset_flag is False:\r\n                  history = linear_model.fit(inputs, labels, epochs=10, verbose=True)\r\n              else:\r\n                  inputs = inputs.reshape((num_data, 1, 2))\r\n                  labels = labels.reshape((num_data, 1, 2))\r\n                  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                  dataset.repeat().batch(300)\r\n                  history = linear_model.fit(dataset, epochs=10, verbose=True)\r\n\r\nHere is the performace:\r\n\r\n              Epoch 1/10\r\n              300/300 [==============================] - 0s 366us/step - loss: 0.1313\r\n              Epoch 2/10\r\n              300/300 [==============================] - 0s 314us/step - loss: 0.1116\r\n              Epoch 3/10\r\n              300/300 [==============================] - 0s 366us/step - loss: 0.5312\r\n              Epoch 4/10\r\n              300/300 [==============================] - 0s 313us/step - loss: 0.0355\r\n              Epoch 5/10\r\n              300/300 [==============================] - 0s 313us/step - loss: 0.0467\r\n              Epoch 6/10\r\n              300/300 [==============================] - 0s 366us/step - loss: 0.1714\r\n              Epoch 7/10\r\n              300/300 [==============================] - 0s 313us/step - loss: 0.0023\r\n              Epoch 8/10\r\n              300/300 [==============================] - 0s 366us/step - loss: 1.3178e-04\r\n              Epoch 9/10\r\n              300/300 [==============================] - 0s 313us/step - loss: 0.0023\r\n              Epoch 10/10\r\n              300/300 [==============================] - 0s 314us/step - loss: 0.2683", "Did you try this on the numpy array as well?\r\n", "Yes, this is the numpy array result:\r\n\r\n              Epoch 1/10\r\n              10/10 [==============================] - 0s 2ms/step - loss: 1.0422\r\n              Epoch 2/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2581\r\n              Epoch 3/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0924\r\n              Epoch 4/10\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0429\r\n              Epoch 5/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0141\r\n              Epoch 6/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0053\r\n              Epoch 7/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0020\r\n              Epoch 8/10\r\n              10/10 [==============================] - 0s 2ms/step - loss: 7.0293e-04\r\n              Epoch 9/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 2.5369e-04\r\n              Epoch 10/10\r\n              10/10 [==============================] - 0s 0s/step - loss: 8.6971e-05", "Can you try optimizer = 'adam' ?", "The optimizer was 'adam' please check code posted previously", "All the previous codes have\r\n`optimizer=tf.keras.optimizers.Adam(1)`\r\n\r\nTry changing it to\r\n`optimizer = 'adam'`\r\n", "Great! I got good results of dataset after change to 'adam'. I removed \"data.set.repeat().batch(300)\" and did comparison between numpy array and dataset and found an interesting result: numpy array needs almost 20 times more epochs to reach the similar results of dataset.  So it seems the model train the numpy array element one by one, but train dataset in patch.\r\n\r\nnumpy array's result:\r\n              history = linear_model.fit(inputs, labels, epochs=200, verbose=True)\r\n\r\n              Epoch 1/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.3582\r\n              Epoch 2/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.3423\r\n              Epoch 3/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.3273\r\n              Epoch 4/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.3134\r\n              Epoch 5/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2997\r\n              Epoch 6/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2872\r\n              Epoch 7/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2751\r\n              Epoch 8/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.2637\r\n              Epoch 9/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2524\r\n              Epoch 10/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2419\r\n              Epoch 11/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2317\r\n              Epoch 12/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.2221\r\n              Epoch 13/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2129\r\n              Epoch 14/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.2041\r\n              Epoch 15/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1957\r\n              Epoch 16/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.1878\r\n              Epoch 17/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1802\r\n              Epoch 18/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1731\r\n              Epoch 19/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1663\r\n              Epoch 20/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.1599\r\n              Epoch 21/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1538\r\n              Epoch 22/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1481\r\n              Epoch 23/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1427\r\n              Epoch 24/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.1375\r\n              Epoch 25/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1326\r\n              Epoch 26/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1280\r\n              Epoch 27/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1235\r\n              Epoch 28/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.1193\r\n              Epoch 29/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1154\r\n              Epoch 30/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1117\r\n              Epoch 31/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1081\r\n              Epoch 32/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.1050\r\n              Epoch 33/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.1018\r\n              Epoch 34/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0990\r\n              Epoch 35/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0961\r\n              Epoch 36/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0935\r\n              Epoch 37/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0910\r\n              Epoch 38/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0886\r\n              Epoch 39/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0864\r\n              Epoch 40/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0843\r\n              Epoch 41/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0823\r\n              Epoch 42/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0804\r\n              Epoch 43/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0786\r\n              Epoch 44/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0769\r\n              Epoch 45/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0753\r\n              Epoch 46/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0737\r\n              Epoch 47/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0722\r\n              Epoch 48/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0708\r\n              Epoch 49/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0694\r\n              Epoch 50/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0681\r\n              Epoch 51/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0668\r\n              Epoch 52/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0656\r\n              Epoch 53/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0645\r\n              Epoch 54/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0633\r\n              Epoch 55/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0622\r\n              Epoch 56/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0612\r\n              Epoch 57/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0601\r\n              Epoch 58/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0592\r\n              Epoch 59/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0582\r\n              Epoch 60/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0573\r\n              Epoch 61/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0563\r\n              Epoch 62/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0554\r\n              Epoch 63/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0546\r\n              Epoch 64/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0537\r\n              Epoch 65/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0529\r\n              Epoch 66/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0521\r\n              Epoch 67/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0513\r\n              Epoch 68/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0505\r\n              Epoch 69/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0497\r\n              Epoch 70/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0489\r\n              Epoch 71/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0482\r\n              Epoch 72/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0474\r\n              Epoch 73/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0467\r\n              Epoch 74/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0460\r\n              Epoch 75/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0453\r\n              Epoch 76/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0446\r\n              Epoch 77/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0439\r\n              Epoch 78/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0432\r\n              Epoch 79/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0425\r\n              Epoch 80/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0418\r\n              Epoch 81/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0412\r\n              Epoch 82/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0405\r\n              Epoch 83/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0399\r\n              Epoch 84/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0393\r\n              Epoch 85/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0387\r\n              Epoch 86/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0380\r\n              Epoch 87/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0374\r\n              Epoch 88/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0368\r\n              Epoch 89/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0362\r\n              Epoch 90/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0356\r\n              Epoch 91/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0350\r\n              Epoch 92/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0345\r\n              Epoch 93/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0339\r\n              Epoch 94/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0333\r\n              Epoch 95/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0327\r\n              Epoch 96/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0322\r\n              Epoch 97/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0317\r\n              Epoch 98/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0311\r\n              Epoch 99/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0306\r\n              Epoch 100/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0301\r\n              Epoch 101/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0296\r\n              Epoch 102/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0290\r\n              Epoch 103/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0285\r\n              Epoch 104/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0280\r\n              Epoch 105/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0276\r\n              Epoch 106/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0271\r\n              Epoch 107/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0266\r\n              Epoch 108/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0261\r\n              Epoch 109/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0256\r\n              Epoch 110/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0252\r\n              Epoch 111/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0247\r\n              Epoch 112/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0243\r\n              Epoch 113/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0238\r\n              Epoch 114/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0234\r\n              Epoch 115/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0230\r\n              Epoch 116/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0225\r\n              Epoch 117/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0221\r\n              Epoch 118/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0217\r\n              Epoch 119/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0213\r\n              Epoch 120/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0209\r\n              Epoch 121/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0205\r\n              Epoch 122/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0201\r\n              Epoch 123/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0197\r\n              Epoch 124/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0193\r\n              Epoch 125/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0189\r\n              Epoch 126/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0186\r\n              Epoch 127/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0182\r\n              Epoch 128/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0178\r\n              Epoch 129/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0175\r\n              Epoch 130/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0171\r\n              Epoch 131/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0168\r\n              Epoch 132/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0164\r\n              Epoch 133/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0161\r\n              Epoch 134/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0158\r\n              Epoch 135/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0154\r\n              Epoch 136/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0151\r\n              Epoch 137/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0148\r\n              Epoch 138/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0145\r\n              Epoch 139/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0142\r\n              Epoch 140/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0139\r\n              Epoch 141/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0136\r\n              Epoch 142/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0133\r\n              Epoch 143/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0130\r\n              Epoch 144/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0127\r\n              Epoch 145/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0125\r\n              Epoch 146/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0122\r\n              Epoch 147/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0119\r\n              Epoch 148/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0117\r\n              Epoch 149/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0114\r\n              Epoch 150/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0112\r\n              Epoch 151/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0109\r\n              Epoch 152/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0107\r\n              Epoch 153/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0104\r\n              Epoch 154/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0102\r\n              Epoch 155/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0100\r\n              Epoch 156/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0098\r\n              Epoch 157/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0095\r\n              Epoch 158/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0093\r\n              Epoch 159/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0091\r\n              Epoch 160/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0089\r\n              Epoch 161/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0087\r\n              Epoch 162/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0085\r\n              Epoch 163/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0083\r\n              Epoch 164/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0081\r\n              Epoch 165/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0079\r\n              Epoch 166/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0077\r\n              Epoch 167/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0075\r\n              Epoch 168/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0074\r\n              Epoch 169/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0072\r\n              Epoch 170/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0070\r\n              Epoch 171/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0069\r\n              Epoch 172/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0067\r\n              Epoch 173/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0065\r\n              Epoch 174/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0064\r\n              Epoch 175/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0062\r\n              Epoch 176/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0061\r\n              Epoch 177/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0059\r\n              Epoch 178/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0058\r\n              Epoch 179/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0056\r\n              Epoch 180/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0055\r\n              Epoch 181/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0053\r\n              Epoch 182/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0052\r\n              Epoch 183/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0051\r\n              Epoch 184/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0049\r\n              Epoch 185/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0048\r\n              Epoch 186/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0047\r\n              Epoch 187/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0046\r\n              Epoch 188/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0044\r\n              Epoch 189/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0043\r\n              Epoch 190/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0042\r\n              Epoch 191/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0041\r\n              Epoch 192/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0040\r\n              Epoch 193/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0039\r\n              Epoch 194/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0038\r\n              Epoch 195/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0037\r\n              Epoch 196/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0036\r\n              Epoch 197/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0035\r\n              Epoch 198/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0034\r\n              Epoch 199/200\r\n              10/10 [==============================] - 0s 0s/step - loss: 0.0033\r\n              Epoch 200/200\r\n              10/10 [==============================] - 0s 2ms/step - loss: 0.0032\r\n\r\ndataset result:\r\n               inputs = inputs.reshape((num_data, 1, 2))\r\n                labels = labels.reshape((num_data, 1, 2))\r\n                dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                # dataset.repeat().batch(300)\r\n                 history = linear_model.fit(dataset, epochs=12, verbose=True)\r\n\r\n                Epoch 1/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.7572\r\n                Epoch 2/20\r\n                300/300 [==============================] - 0s 261us/step - loss: 0.3218\r\n                Epoch 3/20\r\n                300/300 [==============================] - 0s 261us/step - loss: 0.1201\r\n                Epoch 4/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0434\r\n                Epoch 5/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0195\r\n                Epoch 6/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0125\r\n                Epoch 7/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0094\r\n                Epoch 8/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0074\r\n                Epoch 9/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0058\r\n                Epoch 10/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0044\r\n                Epoch 11/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0034\r\n                Epoch 12/20\r\n                300/300 [==============================] - 0s 313us/step - loss: 0.0025\r\n           \r\n\r\n", "Are you sure you removed it and then ran the code? \r\n>So it seems the model train the numpy array element one by one, but train dataset in patch.\r\n\r\nThis is incorrect. If the code which you shared in latest comment is correct, then the model trained on individual data points only. But the numpy array trained on the whole batch. But I suspect that you had not removed the Dataset.batch() statement..\r\n\r\nAnyway, note that the dataset takes lot less time for one epoch. The main accuracy difference came due to change in optimizer. Previously, you were using Adam with learning rate = 1, which is extremely high, and thus it lead to unstable results. But the default config is way better, and it leads to stable training.\r\n\r\nPlease move this issue to closed status if your problem is resolved.\r\nThank you.", "Yes, I removed the Dataset.batch() and ran the code.  Here is the code:\r\n\r\n              import tensorflow as tf\r\n              import numpy as np\r\n              \r\n              def simulation_linear_function(x, a, b):  # y = ax+b\r\n                  y = tf.matmul(a, x)+b\r\n                  return y\r\n              \r\n              # Generate data set\r\n              a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))\r\n              b = np.array([0.5,0.6]).reshape((2,1))\r\n              num_data = 300\r\n              inputs = np.random.random((num_data,2))\r\n              labels = np.zeros((num_data,2))\r\n              for n in range(num_data):\r\n                  x = inputs[n]\r\n                  y = simulation_linear_function(np.reshape(x, (2,1)), a, b)\r\n                  labels[n] = np.reshape(y, (2,))\r\n              \r\n              # construct model\r\n              linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))\r\n              linear_model = tf.keras.Sequential([linear_layer])\r\n              linear_model.compile(loss='mean_squared_error', optimizer='adam')\r\n              \r\n              #Train\r\n              use_dataset_flag = True\r\n              if use_dataset_flag is False:\r\n                  history = linear_model.fit(inputs, labels, epochs=200, verbose=True)\r\n              else:\r\n                  inputs = inputs.reshape((num_data, 1, 2))\r\n                  labels = labels.reshape((num_data, 1, 2))\r\n                  dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\r\n                  history = linear_model.fit(dataset, epochs=12, verbose=True)\r\n\r\nYou can just copy and paste to Colab and double check it.  I just did that.\r\n", "@tom970 ,\r\n\r\nPlease feel free to move this issue to closed status if your issue has resolved.\r\nThanks!", "Thanks @dityaKane2001 for responsively discussing and solving this issue.\r\nThe conclusions are that \r\n1) for model input shape (2, ),  \r\n     a) if we use input data directly, the input data shape could be (300, 2)\r\n     b) if we construct a dataset, the input data shape has to be reshape to (300, 1, 2)\r\n2) Large learning rate will be more easily cause training not converging when applying  dataset\r\n3) @dityaKane2001 may not agree, but from running previously attached code on Colab, I observed that the model trained the numpy array element one by one, but trained dataset in batch.\r\n\r\nThanks a lot !\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50099\">No</a>\n"]}, {"number": 50098, "title": "[tflite] make tflite related stuff build again on non-NNAPI platform", "body": "The NNAPI SL patch 4b949de introduced using of `CreateNnApiFromSupportLibrary(NnApiSLDriverImplFL5 const*)` which caused a problem.\r\n\r\nWhen running `bazel build --config opt tensorflow/lite/tools/benchmark:benchmark_model` on macOS,  I got:\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"CreateNnApiFromSupportLibrary(NnApiSLDriverImplFL5 const*)\", referenced from:\r\n      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)\r\n      tflite::StatefulNnApiDelegate::StatefulNnApiDelegate(NnApiSLDriverImplFL5 const*, tflite::StatefulNnApiDelegate::Options) in libnnapi_delegate.a(nnapi_delegate.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nTo fix this, I created a simple dummy function\r\n\r\n", "comments": ["Hi Terry, would you like help to take a look? Thanks!", "close this since similar code is committed in 53d3617374a171acf847f5d23bdcd7061e5a2809"]}, {"number": 50097, "title": "Another attempt at fixing branch", "body": "", "comments": []}, {"number": 50096, "title": "How to address ptxas warning?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : CentOS Linux release 7.9.2009 (Core)\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.9\r\n- Installed using: virtualenv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA Version: 11.0/ Cuda compilation tools, release 9.1, V9.1.85\r\n- GPU model and memory: Nvidia Tesla P100-16GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nW tensorflow/stream_executor/gpu/asm_compiler.cc:99] *** WARNING *** You are using ptxas 9.1.121, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\r\n\r\nYou do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["TensorFlow 2.5 requires CUDA 11.2.  Can you make sure you have CUDA 11.2 available and the relevant `ptxas` is in your `PATH`?", "Can you please guide me how to get CUDA version and check the ptxas path?", "I will recommend using the [TF GPU docker images](https://www.tensorflow.org/install/docker) which have all the dependencies pre-installed in the correct locations.", "@zeydabadi,\r\n\r\nCan you take a look at the above [comment](https://github.com/tensorflow/tensorflow/issues/50096#issuecomment-864743661) from @sanjoy and let us know if it helps? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50096\">No</a>\n"]}, {"number": 50095, "title": "Fix branch after cherrypick", "body": "", "comments": []}, {"number": 50094, "title": "Update ragged_tensor_to_variant_op.cc", "body": "", "comments": []}, {"number": 50093, "title": "Update ragged_tensor_to_variant_op.cc", "body": "", "comments": []}, {"number": 50092, "title": "Update ragged_tensor_to_variant_op.cc", "body": "", "comments": []}, {"number": 50091, "title": "Fix usage of TF_LITE_ENSURE_MSG in constructor", "body": "", "comments": []}, {"number": 50090, "title": "Fix usage of TF_LITE_ENSURE_MSG in constructor", "body": "", "comments": []}, {"number": 50089, "title": "Update conv.cc", "body": "", "comments": []}, {"number": 50088, "title": "Update conv.cc", "body": "", "comments": []}, {"number": 50087, "title": "Update conv.cc", "body": "", "comments": []}, {"number": 50085, "title": "Update conv.cc", "body": "", "comments": []}, {"number": 50084, "title": "[tf.data] graduate snapshot API from experimental to tf.data.Dataset", "body": "This PR graduates the `tf.data.experimental.snapshot` API into `tf.data.Dataset.snapshot` by making the following changes:\r\n\r\n- [x] Adds the deprecation decorator for the experimental API.\r\n- [x] Add the `snapshot()` method to `DatasetV2` class.\r\n- [x] Update example in docstring with new API.\r\n- [x] Regenerate golden API's.\r\n- [x] Moved and updated the `snapshot_test` target from `experimental/kernel_tests` to `kernel_tests`\r\n- [x] Updated the RELEASE.md file\r\n\r\nTEST LOG\r\n```\r\nINFO: Elapsed time: 37.966s, Critical Path: 31.78s\r\nINFO: 369 processes: 133 internal, 236 local.\r\nINFO: Build completed successfully, 369 total actions\r\n//tensorflow/python/data/kernel_tests:snapshot_test                      PASSED in 10.4s\r\n  Stats over 16 runs: max = 10.4s, min = 3.6s, avg = 6.2s, dev = 1.8s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\n\r\ncc: @jsimsa  The `legacy_snapshot` API is still experimental and has not been graduated.", "comments": []}, {"number": 50083, "title": "Training stuck with custom loss", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: RTX 2070 Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe training stops running after printing Epoch 1/20 when I use model.fit to run my model. It was fine until I added this loss function:\r\n\r\n```\r\nclass snc_loss(tf.keras.losses.Loss):\r\n    def call(self, y_true, y_pred):\r\n        l_sigma_I = tf.constant(100.0, dtype=tf.float32)\r\n        l_sigma_X = tf.constant(16.0, dtype=tf.float32)\r\n        l_R = tf.constant(5.0, dtype=tf.float32)\r\n        shape = tf.shape(y_pred)\r\n        loss = tf.ones((shape[0],)) * tf.cast(shape[3], dtype=tf.float32)\r\n        for channel in tf.range(shape[-1]):\r\n            sub = tf.zeros((shape[0],))\r\n            denom = tf.ones((shape[0],)) * 1e-6\r\n            for i in tf.range(shape[1] * shape[2]):\r\n                y_i = i%shape[2]\r\n                x_i = i//shape[2]\r\n                for j in tf.range(i+1, shape[1]*shape[2]):\r\n                    y_j = j%shape[2]\r\n                    x_j = j//shape[2]\r\n                    eu_dis = tf.cast(tf.math.square(x_i - x_j)+tf.math.square(y_i - y_j), dtype=tf.float32)\r\n                    pixel_dis = tf.cast(tf.math.square(y_true[:, x_i, y_i] - y_true[:, x_j, y_j]), dtype=tf.float32)\r\n                    if tf.math.less(tf.math.sqrt(eu_dis), l_R):\r\n                        weight = tf.math.exp(-1 * pixel_dis / l_sigma_I - eu_dis / l_sigma_X)\r\n                        sub += weight * y_pred[:, x_j, y_j, channel] * y_pred[:, x_i, y_i, channel]\r\n                        denom += weight * y_pred[:, x_i, y_i, channel]\r\n\r\n            loss -= sub/denom\r\n\r\n        return loss\r\n```\r\n\r\n**Describe the expected behavior**\r\nIt should be training\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1CqZTxlyUNlUDkNipoVRPhA3HHzjlvuUj?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThe error was still there when I was using Tensorflow 2.4.1, but it used to tell me SubProcess ended with return code : 0\r\nAlso, when I run a similar code as a job on a server, it throws a bus error, so might be related to memory.", "comments": ["I was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2bf52ce28c35b618568956ac8f384921/50083.ipynb).", "@pjsjongsung \r\nYou should define your constants in init, You are creating constants at every step is , there is a lot of complexity in your function.\r\nplease try to simplify your function with tf ops and broadcasting concepts instead of the python loops,The training does proceed but the problem is with the loss which is taking too long.\r\n\r\nKindly open a stackoverflow/tf discussion forum issue as this is not a bug or feat request and move this to closed status.", "I am using loops because of a memory issue and cannot initialize weights because then the matrix is too big. But I get the constant issue. Closing this. Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50083\">No</a>\n"]}, {"number": 50082, "title": "tf not imported in core.py, yet used.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **CentOS 7.n**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below):  **v2.4.0-49-g85c8b2a817f 2.4.1**\r\n- Python version: **3.7.6**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\n`...full.path.../lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py` code uses `tf`, yet it isn't imported.  I'm not sure why it failed in this particular case.\r\n\r\n**Describe the expected behavior**\r\nI am reporting this because I think its incorrect that a file references a name (import) which isn't imported \"above\".  But I don't understand the code base, so it may be perfectly acceptable.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - ** no **\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs**\r\n\r\n_Log slightly reduced and cleaned _\r\n\r\n```\r\nIn [2]: input_model = keras.models.load_model(config['model']['path'])\r\n2021-06-04 10:26:23.467261: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-06-04 10:26:23.470714: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2021-06-04 10:26:23.470834: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-06-04 10:26:23.470916: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (us01odcvde12327): /proc/driver/nvidia/version does not exist\r\n2021-06-04 10:26:23.471718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-04 10:26:23.471955: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-06-04 10:26:24.001545: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.\r\n2021-06-04 10:26:24.060087: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.\r\n2021-06-04 10:26:24.091049: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.\r\n2021-06-04 10:26:24.161196: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.\r\n2021-06-04 10:26:24.211141: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:1059: UserWarning: backend is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n${PROJECT_PATH}/capsule/export_model.py in <module>\r\n----> 1 input_model = keras.models.load_model(config['model']['path'])\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    210       if isinstance(filepath, six.string_types):\r\n    211         loader_impl.parse_saved_model(filepath)\r\n--> 212         return saved_model_load.load(filepath, compile, options)\r\n    213 \r\n    214   raise IOError(\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)\r\n    145 \r\n    146   # Finalize the loaded layers and remove the extra tracked dependencies.\r\n--> 147   keras_loader.finalize_objects()\r\n    148   keras_loader.del_tracking()\r\n    149 \r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in finalize_objects(self)\r\n    610 \r\n    611     # Initialize graph networks, now that layer dependencies have been resolved.\r\n--> 612     self._reconstruct_all_models()\r\n    613 \r\n    614   def _unblock_model_reconstruction(self, layer_id, layer):\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)\r\n    629       all_initialized_models.add(model_id)\r\n    630       model, layers = self.model_layer_dependencies[model_id]\r\n--> 631       self._reconstruct_model(model_id, model, layers)\r\n    632       _finalize_config_layers([model])\r\n    633 \r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_model(self, model_id, model, layers)\r\n    676       (inputs, outputs,\r\n    677        created_layers) = functional_lib.reconstruct_from_config(\r\n--> 678            config, created_layers={layer.name: layer for layer in layers})\r\n    679       model.__init__(inputs, outputs, name=config['name'])\r\n    680       functional_lib.connect_ancillary_layers(model, created_layers)\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1283       if layer in unprocessed_nodes:\r\n   1284         for node_data in unprocessed_nodes.pop(layer):\r\n-> 1285           process_node(layer, node_data)\r\n   1286 \r\n   1287   input_tensors = []\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)\r\n   1231         input_tensors = (\r\n   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))\r\n-> 1233       output_tensors = layer(input_tensors, **kwargs)\r\n   1234 \r\n   1235       # Update node index map.\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    951       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 952                                                 input_list)\r\n    953 \r\n    954     # Maintains info about the `Layer.call` stack.\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1089         # Check input assumptions set after layer building, e.g. input shape.\r\n   1090         outputs = self._keras_tensor_symbolic_call(\r\n-> 1091             inputs, input_masks, args, kwargs)\r\n   1092 \r\n   1093         if outputs is None:\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    821     else:\r\n--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    823 \r\n    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    861           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n    862           self._maybe_build(inputs)\r\n--> 863           outputs = call_fn(inputs, *args, **kwargs)\r\n    864 \r\n    865         self._handle_activity_regularization(inputs, outputs)\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)\r\n    915     with backprop.GradientTape(watch_accessed_variables=True) as tape,\\\r\n    916         variable_scope.variable_creator_scope(_variable_creator):\r\n--> 917       result = self.function(inputs, **kwargs)\r\n    918     self._check_variables(created_variables, tape.watched_variables())\r\n    919     return result\r\n\r\n${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in space_to_depth_x2(x)\r\n     48         # the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)\r\n     49         def space_to_depth_x2(x):\r\n---> 50             return tf.nn.space_to_depth(x, block_size=2)\r\n     51 \r\n     52         # Layer 1\r\n\r\nNameError: name 'tf' is not defined\r\n\r\n```\r\n", "comments": ["Can you try importing tf module on top of your script and check?\r\n`import tensorflow as tf`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50082\">No</a>\n"]}, {"number": 50081, "title": "How to get feature value in custom loss function", "body": "How can i get not only y_true and y_pred, but the last feature value from row that predicts y_pred in custom loss function to calculate loss with it. I use rnn in tf.keras.", "comments": ["@Gynshu Can you please create a simple standalone code to reproduce the issue? A simple code example to demonstrate what you are trying to achieve in custom loss function. Thanks!\r\n\r\nSome useful resources for custom loss functions are https://medium.com/swlh/custom-loss-and-custom-metrics-using-keras-sequential-model-api-d5bcd3a4ff28,  and https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 50078, "title": "Failed to convert SparseTensor to Tensor (TensorFlow 2.5)", "body": "Dear experts,\r\nI am using TensorFlow 2.5 with Python 3.8.10. I have pip-installed TF2.5 within an anaconda environment.\r\nWhen trying out this code [1] (where I create a dummy dataset consisting of 2 classes (label) of `SparseTensor`s and use it to train a simple CNN), I get the error [2]. I have seen that this issue has been raised for TF2.4 (https://github.com/tensorflow/tensorflow/issues/47931), but didn't see an issue raised for TF2.5. I know I can convert the sparse tensor to dense 'on-the-fly' in a generator Dataset and pass that is input. However, I wanted to run the training on sparse tensors themselves to speed it up. Is there any workaround known for TF2.5?\r\n\r\nThanks in advance.\r\n\r\n[1]\r\n```\r\nimport numpy\r\nimport tensorflow\r\n\r\nfrom tensorflow.keras import datasets, layers, models\r\nfrom tensorflow.keras import mixed_precision\r\n\r\n\r\n# No. of images\r\nnImg = 1000\r\n\r\nnBinX = 50\r\nnBinY = 50\r\n\r\n# No. of layers/channels\r\nnLayer = 1\r\n\r\nnPixelTot = nBinX*nBinY\r\n\r\nl_idx = []\r\nl_val = []\r\nl_label = []\r\n\r\n# Create a sparse dataset with random entries\r\nfor iImg in range(0, nImg) :\r\n    \r\n    # Fill at most 60 pixels\r\n    nFill = numpy.random.randint(low = 1, high = 61)\r\n    \r\n    for iFill in range(0, nFill) :\r\n        \r\n        # Index of the filled pixel\r\n        # [image idx, row idx, col idx, layer]\r\n        idx = [\r\n            iImg,\r\n            numpy.random.randint(low = 0, high = nBinY),\r\n            numpy.random.randint(low = 0, high = nBinX),\r\n            nLayer-1,\r\n        ]\r\n        \r\n        if (idx in l_idx) :\r\n            continue\r\n        \r\n        l_idx.append(idx)\r\n        l_val.append(numpy.random.rand())\r\n    \r\n    l_label.append(numpy.random.randint(low = 0, high = 2))\r\n\r\n\r\nimg_shape = (nBinY, nBinX, nLayer)\r\ndense_shape = (nImg, nBinY, nBinX, nLayer)\r\n\r\n# Create the sparse rensor\r\ninput_img_sparseTensor = tensorflow.sparse.reorder(tensorflow.sparse.SparseTensor(\r\n    indices = l_idx,\r\n    values = l_val,\r\n    dense_shape = dense_shape,\r\n))\r\n\r\n\r\nprint(\"=====> Creating dataset...\")\r\n\r\ndataset_img = tensorflow.data.Dataset.from_tensor_slices(input_img_sparseTensor)\r\ndataset_label = tensorflow.data.Dataset.from_tensor_slices(l_label)\r\n\r\nbatch_size = 100\r\n\r\ndataset = tensorflow.data.Dataset.zip((dataset_img, dataset_label)).batch(batch_size)\r\n\r\nprint(\"dataset.element_spec:\", dataset.element_spec)\r\nprint(\"=====> Created dataset...\")\r\n\r\n\r\n# Dummy CNN model\r\nmodel = models.Sequential()\r\n\r\n##model.add(layers.InputLayer(input_shape = img_shape, sparse = True, batch_size = batch_size))\r\nmodel.add(layers.Conv2D(10, kernel_size = (10, 10), activation = \"relu\", input_shape = img_shape))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Conv2D(5, kernel_size = (5, 5), activation = \"relu\"))\r\nmodel.add(layers.MaxPooling2D((2, 2)))\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(50, activation = \"relu\"))\r\nmodel.add(layers.Dense(2, activation = \"relu\"))\r\n\r\nmodel.summary()\r\n\r\nprint(\"=====> Compiling model...\")\r\n\r\nmodel.compile(\r\n    optimizer = \"adam\",\r\n    loss = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\r\n    metrics = [\"accuracy\"],\r\n)\r\n\r\nprint(\"=====> Compiled model...\")\r\n\r\nprint(\"=====> Starting fit...\")\r\n\r\n# Use the same data for train and test, just to check if it runs\r\nhistory = model.fit(\r\n    x = dataset,\r\n    epochs = 5,\r\n    #batch_size = batch_size,\r\n    validation_data = dataset,\r\n    shuffle = False,\r\n)\r\n```\r\n\r\n[2]\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_tensorflowSparseTensor.py\", line 100, in <module>\r\n    history = model.fit(\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1183, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 889, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 933, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 763, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3050, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3444, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3279, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 999, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 672, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in user code:\r\n\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\r\n        return step_function(self, iterator)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\r\n        outputs = model.train_step(data)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 train_step\r\n        y_pred = self(x, training=True)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:380 call\r\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:420 call\r\n        return self._run_internal_graph(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:556 _run_internal_graph\r\n        outputs = node.layer(*args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py:249 call\r\n        outputs = self._convolution_op(inputs, self.kernel)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:1012 convolution_v2\r\n        return convolution_internal(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:1142 convolution_internal\r\n        return op(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:2596 _conv2d_expanded_batch\r\n        return gen_nn_ops.conv2d(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py:969 conv2d\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:525 _apply_op_helper\r\n        raise err\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:517 _apply_op_helper\r\n        values = ops.convert_to_tensor(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\r\n        return func(*args, **kwargs)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\r\n        return constant(v, dtype=dtype, name=name)\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant\r\n        return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl\r\n        tensor_util.make_tensor_proto(\r\n    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:551 make_tensor_proto\r\n        raise TypeError(\"Failed to convert object of type %s to Tensor. \"\r\n\r\n    TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"DeserializeSparse:0\", shape=(None, 4), dtype=int64), values=Tensor(\"DeserializeSparse:1\", shape=(None,), dtype=float32), dense_shape=Tensor(\"stack:0\", shape=(4,), dtype=int64)). Consider casting elements to a supported type.\r\n```", "comments": ["@Saduf2019 \r\n\r\nI was able to replicate the issue tf2.5 and tf nightly.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/64694ec1286c6d6d40620666088d2709/untitled79.ipynb).Thanks", "I think [this](https://stackoverflow.com/questions/67933642/tensoflow-sparsetensor-input-gives-value-error-for-multi-dimensional-input-shape) is a simple version of the same problem:\r\n\r\nI have a keras model which takes multi-dimensional input data. When I build a model, setting the input shape to anything with multiple dimensions  and sparse=True, I get the following error:\r\n> ValueError: Attempt to convert a value (<tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000002D24207B400>) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.\r\n\r\nFollowing [the sparse tensor guide][1], this works for me:\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.keras.Input(shape=(4,), sparse=True)\r\ny = tf.keras.layers.Dense(4)(x)\r\nmodel = tf.keras.Model(x, y)\r\n```\r\nBut if I add in another dimension, I get the error (which occurs at the second line):\r\n```\r\nx = tf.keras.Input(shape=(4,2, ), sparse=True)\r\ny = tf.keras.layers.Dense(4)(x)\r\nmodel = tf.keras.Model(x, y)\r\n```\r\nIf I set `sparse=False` then the error goes away:\r\n```\r\nx = tf.keras.Input(shape=(4,2, ), sparse=False)\r\ny = tf.keras.layers.Dense(4)(x)\r\nmodel = tf.keras.Model(x, y)\r\n```\r\n\r\n\r\n  [1]: https://www.tensorflow.org/guide/sparse_tensor#tfkeras", "You may follow [this colab](https://colab.research.google.com/drive/1tKQlRLhD7hvsuewM2NXSUqbEx1EwO3s8?usp=sharing), where the issue does not occur.\r\n\r\nLambda layer created and appended on top of your model, which converts your sparse tensor to dense.", "> You may follow [this colab](https://colab.research.google.com/drive/1tKQlRLhD7hvsuewM2NXSUqbEx1EwO3s8?usp=sharing), where the issue does not occur.\r\n> \r\n> Lambda layer created and appended on top of your model, which converts your sparse tensor to dense.\r\n\r\n@Saduf2019 Thank you for the pointer.\r\nYes, I'm aware that I can create a Lamda layer and convert the sparse tensor to dense 'on the fly', but that adds another preprocessing step. I was wondering if I can apply Conv2D on a sparse tensor directly. From the documentation, this does not seem to be forbidden -- so is this bug fixed in some nightly version?", "@SohamBhattacharya I think sparse input is not supported by `Conv2D`. Some other layers like `Dense` supports sparse tensors. As mentioned above, You could convert sparse tensors into dense tensors and pass it to the Conv layers. Thanks!\r\n\r\nhttps://www.tensorflow.org/guide/sparse_tensor\r\n\r\nIf your use case demands this kind of feature, then open a feature request in [keras-team/keras](https://github.com/keras-team/keras/issues) repo as Keras team moved to that repo to focus entirely on keras development. Also, mention more details about your feature request when you open the issue. Thanks!", "@jvishnuvardhan Thanks for the clarification and suggestion.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50078\">No</a>\n"]}, {"number": 50077, "title": "Updated LSTM Cell inputs description to 3D tensor", "body": "Actually the LSTM Cell takes a 3D tensor as input as we can see on line number 801 but the description at line number 860 mentions that the input should be a 2D tensor which is incorrect. The LSTM cell gives error with 2D tensor input. Hence modified the description as 3D tensor.  ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50077) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot  I signed it!", "No, this is not correct. LSTMCell only take 2D input and it will only process one timestep (so there isn't a dim for timestep in the input). On the otherhand, LSTM layer will take 3D tensor, and the current docstring is correctly reflecting that.", "@qlzh727 But the example used on the tensorflow official docs itself uses 3D tensor as input for LSTMCell.\r\n[https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell](url) ", "> @qlzh727 But the example used on the tensorflow official docs itself uses 3D tensor as input for LSTMCell.\r\n> [https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell](url)\r\n\r\nPlease pay attention to the sample code, the LSTMCell is wrapped in a RNN layer, which takes 3D tensor as input. When the input reaches the cell, the input shape is 2D. You can debug the code step by step to see the shape of input."]}, {"number": 50076, "title": "how to get the Tensor's numpy in process ?", "body": "hi,dear\r\nif I just want to get the tensor's values in process?\r\nchould you please help me ?\r\ncodes down\r\n```\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = len(x_train)\r\nBATCH_SIZE_PER_REPLICA = 64\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\n# Add a channels dimension\r\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\r\nx_test = x_test[..., tf.newaxis].astype(\"float32\")\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(GLOBAL_BATCH_SIZE)\r\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_ds)\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')\r\n        self.flatten = tf.keras.layers.Flatten()\r\n        self.d1 = tf.keras.layers.Dense(128, activation='relu')\r\n        self.d2 = tf.keras.layers.Dense(10)\r\n    #\r\n    def call(self, x):\r\n        x = self.conv1(x)\r\n        x0 = self.flatten(x)\r\n        x = self.d1(x0)\r\n        return self.d2(x)\r\n```\r\nhow to get the x0's values ?\r\nthx\r\n", "comments": ["@ucasiggcas ,\r\n\r\nPlease go through the links with similar references for more information.It helps.[Link1](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten),[Link2](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=nightly#add).\r\nThanks!", "I got success,thx"]}, {"number": 50075, "title": "Using GradientTape with gather, with axis parameter > 1, can cause InvalidArgumentError and wrong gradients.", "body": "**System information**\r\n- Have I written custom code: yes (I might be misunderstanding this question)\r\n- OS Platform and Distribution: Linux Ubuntu 20.04.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.3 ('default', 'Jul  2 2020 16:21:59')\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: Not using GPU at the moment.\r\n\r\n**Describe the current behavior**\r\nUsing `gather` in the code block below causes an `InvalidArgumentError`:\r\n`InvalidArgumentError: segment_ids[1] = 20 is out of range [0, 12) [Op:UnsortedSegmentSum]`\r\nCode example ([colab notebook](https://colab.research.google.com/drive/1mU1l_k9yx8cu06NMh3hTkw0Ehg4QwI0r?usp=sharing)):\r\n```\r\nimport tensorflow as tf\r\nA = tf.Variable(tf.random.normal([4, 20, 3]))\r\na = tf.random.uniform([4],0,3,dtype=tf.int32)\r\nwith tf.GradientTape() as tape:\r\n    x = tf.gather(A,a,batch_dims=1,axis=2)\r\n    gradients = tape.gradient(tf.reduce_mean(x), [A])\r\nprint(gradients,A,a,x)\r\n```\r\nThis doesn't occur if `A` is smaller, so for example, if `A = tf.Variable(tf.random.normal([4, 2, 3]))` then the gather operation seems correct, although the gradients are wrong still.\r\n\r\nAlso I've just discovered that if I transpose A so `gather`'s axis parameter is `axis=1`, it all works. I'll use this as a work-around for now:\r\n```\r\nimport tensorflow as tf\r\nA = tf.Variable(tf.random.normal([4, 3, 200]))\r\na = tf.random.uniform([4],0,3,dtype=tf.int32)\r\nwith tf.GradientTape() as tape:\r\n    x = tf.gather(A,a,batch_dims=1,axis=1)\r\n    gradients = tape.gradient(tf.reduce_mean(x), [A])\r\nprint(gradients,A,a,x)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nFor this example `a = tf.Tensor([0 0 1 0], shape=(4,), dtype=int32)`.\r\n\r\nSo for the simple example with `A = tf.Variable(tf.random.normal([4, 2, 3]))` the `gather` output is correct, although the gradients I would *expect* to be:\r\n```\r\n[<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=\r\narray([[[0.125, 0.   , 0],\r\n        [0.125, 0.   , 0]],\r\n\r\n       [[0.125   , 0.   , 0],\r\n        [0. 125  , 0.   , 0]],\r\n\r\n       [[0.  , 0.125,  0.   ],\r\n        [0.  , 0.125, 0.   ]],\r\n\r\n       [[0.125   , 0.   , 0.   ],\r\n        [0.125   , 0.   , 0.   ]]], dtype=float32)>] \r\n```\r\nbut they are:\r\n```\r\n[<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=\r\narray([[[0.125, 0.   , 0.125],\r\n        [0.125, 0.   , 0.125]],\r\n\r\n       [[0.   , 0.   , 0.125],\r\n        [0.   , 0.   , 0.125]],\r\n\r\n       [[0.125, 0.   , 0.   ],\r\n        [0.125, 0.   , 0.   ]],\r\n\r\n       [[0.   , 0.   , 0.   ],\r\n        [0.   , 0.   , 0.   ]]], dtype=float32)>] \r\n```\r\nIn this example `A` is:\r\n```\r\n<tf.Variable 'Variable:0' shape=(4, 2, 3) dtype=float32, numpy=\r\narray([[[ 1.34239629e-01, -9.54777598e-01, -9.89130437e-01],\r\n        [-2.85939670e+00, -6.14062808e-02,  1.24629235e+00]],\r\n\r\n       [[ 2.51984522e-02, -2.28253782e-01, -1.09111404e+00],\r\n        [-5.17144203e-01, -8.65574062e-01,  8.04844439e-01]],\r\n\r\n       [[-1.41558272e-03,  2.71541625e-01, -1.02697104e-01],\r\n        [ 4.04205054e-01, -5.29084206e-01,  2.75117427e-01]],\r\n\r\n       [[-5.37024558e-01,  7.28068471e-01,  1.82375908e+00],\r\n        [-5.38604558e-01,  1.40457046e+00, -1.63010335e+00]]],\r\n      dtype=float32)> \r\n```\r\nSo `x` (the output of gather) is correct:\r\n```\r\ntf.Tensor(\r\n[[ 0.13423963 -2.8593967 ]\r\n [ 0.02519845 -0.5171442 ]\r\n [ 0.27154163 -0.5290842 ]\r\n [-0.53702456 -0.53860456]], shape=(4, 2), dtype=float32)\r\n```\r\n\r\nSo in this case it has picked the 0 column from the top 2x3 sub-tensor, the 0 column from the next one, the 1 column from the next and the 0 column from the last.\r\n\r\nI want this to work for bigger tensors, i.e. if `A = tf.Variable(tf.random.normal([4, 20, 3]))`, for example. And for the gradients to be correct.\r\n\r\nIt's quite possible I've just massively misunderstood what the 'axis' parameter means in `gather`. I'm very sorry if that's the case!\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\nNo\r\n\r\n**Standalone code to reproduce the issue**\r\n[colab notebook](https://colab.research.google.com/drive/1mU1l_k9yx8cu06NMh3hTkw0Ehg4QwI0r?usp=sharing).\r\n\r\n**Other info / logs**\r\n\r\nHere's the trace:\r\n\r\n```---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-49-2079e0db2264> in <module>\r\n      4 with tf.GradientTape() as tape:\r\n      5     x = tf.gather(A,a,batch_dims=1,axis=2)\r\n----> 6     gradients = tape.gradient(tf.reduce_mean(x), [A])\r\n      7 print(gradients,A,a,x)\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1072                           for x in nest.flatten(output_gradients)]\r\n   1073 \r\n-> 1074     flat_grad = imperative_grad.imperative_grad(\r\n   1075         self._tape,\r\n   1076         flat_targets,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     69         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\r\n     70 \r\n---> 71   return pywrap_tfe.TFE_Py_TapeGradient(\r\n     72       tape._tape,  # pylint: disable=protected-access\r\n     73       target,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\r\n    157       gradient_name_scope += forward_pass_name_scope + \"/\"\r\n    158     with ops.name_scope(gradient_name_scope):\r\n--> 159       return grad_fn(mock_op, *out_grads)\r\n    160   else:\r\n    161     return grad_fn(mock_op, *out_grads)\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py in _GatherV2Grad(op, grad)\r\n    694     values_transpose = array_ops.transpose(values, transpose_dims)\r\n    695 \r\n--> 696     params_grad = _BatchGatherGrad(params_shape, values_transpose, indices,\r\n    697                                    batch_dims, params_shape[axis])\r\n    698 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py in _BatchGatherGrad(params_shape, values, indices, batch_dims, gather_dim_size)\r\n    625 \r\n    626   indices = array_ops.reshape(indices, indices_size)\r\n--> 627   params_grad = math_ops.unsorted_segment_sum(values, indices, gather_dim_size)\r\n    628 \r\n    629   if batch_dims:\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py in unsorted_segment_sum(data, segment_ids, num_segments, name)\r\n  11431       return _result\r\n  11432     except _core._NotOkStatusException as e:\r\n> 11433       _ops.raise_from_not_ok_status(e, name)\r\n  11434     except _core._FallbackException:\r\n  11435       pass\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: segment_ids[1] = 20 is out of range [0, 12) [Op:UnsortedSegmentSum]```\r\n", "comments": ["Was able to reproduce the issue in TF 2.5. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/a4842d97f2b1b74ad2e194e8da9ae865/gatherbug.ipynb).Thanks!", "Thanks for filing this issue!  I think this is probably a bug in https://github.com/tensorflow/tensorflow/blob/4ae0971abe3ad5c3a43c5c79283c98cfb08f4bdc/tensorflow/python/ops/array_grad.py#L607", "Just tried the original code snippet from @lionfish0 on TF 2.6.0 and it runs without errors. Seems like for 2.6-2.7 this issue has been resolved by https://github.com/tensorflow/tensorflow/commit/0ed3191b62bc383e1c35f7a926bcb57fb910d4c2 ", "Great!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50075\">No</a>\n", "Thanks for following up & solving this!"]}, {"number": 50074, "title": "[MLIR] Handle tf.variant in tf-tensor-list-ops-decomposition pass", "body": "-- Motivation : Currently `tf-tensor-list-ops-decomposition` pass requires full\r\n                shape of the tensor list to be inferred.\r\n                It bails out on tf.variant and tf.variant<tf.variant> where the\r\n                max size and shape of the element is unknown and not immediately\r\n                apparent.\r\n\r\n-- This commit aims to handle tf.variant and tf.variant<tf.variant>.\r\n   It does so by finding out the maximum number of elements being pushed in the\r\n   empty tensor list and the size of each element.\r\n\r\n-- It currently handles two cases:\r\n   1. Variant declared by an empty tensor list op to which tensors are pushed.\r\n   2. Variant declared by an empty tensor list op to which other variant declared\r\n      by a reserve op is pushed.\r\n\r\nSigned-off-by: Abhishek Varma <abhishek.varma@polymagelabs.com>", "comments": ["Hi @joker-eph @sanjoy @sherhut \r\nCan you please review this PR?", "I'm not the correct reviewer for this.  I've pinged folks internally to help find an appropriate reviewer.", "@smit-hinsu  said they'd look into it.", "Thanks for responding @sanjoy and @joker-eph .\r\nAwaiting @smit-hinsu 's review comments.", "> Could you also expand the commit description to include motivation?\r\n\r\nHi @smit-hinsu I've made the appropriate changes and included the motivation in commit summary.", "Thanks for adding motivation. That is useful. Look at compiler/mlir/tensorflow/transforms/shape_inference.cc pass that does some shape inference which you might be able to utilize. ShapeInference::InferShapeForTensorListInitOps and around code.", "@avarmapml Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "@avarmapml  Any update on this PR? Please. Thanks!", "Apologies for the delay.\r\nI tried exploring ShapeInference::InferShapeForTensorListInitOps but what I understand from it is :- \r\n1. It tries inferring the basic elemental shape of TensorList ops. eg: `tf.variant<?x1xf32>` -> `tf.variant<16x1xf32>`. We can perhaps sequence the passes like : `-tf-shape-inference -tf-tensor-list-ops-decomposition` in order to further deal with these ops.\r\n2. Thus the state of the IR where `tf-shape-inference` lands us is where `tf-tensor-list-ops-decomposition` begins.\r\n\r\nCan you tell me what else can be done here? @smit-hinsu ", "@smit-hinsu  Can you please assist on above comments from @avarmapml. Thanks!", "@smit-hinsu Can you please check this when you get a chance ? ", "@smit-hinsu  Can you please review this PR ? Thanks!", "> Can you tell me what else can be done here? @smit-hinsu\r\n\r\nI wonder if we can do shape refinement in the shape inference pass itself? I haven't thought through it but do you see any downside with that?", "@avarmapml Can you please check @smit-hinsu's comments and keep us posted ? Thanks!", "@avarmapml Any update on this PR? Please. Thanks!"]}, {"number": 50073, "title": "[MLIR] Add GatherOp lowering from lmhlo to Affine.", "body": "-- Lowering of `GatherOp` is added from lmhlo to Affine. The lowering\r\n   has been added as a part of `lhlo-legalize-to-affine` pass.\r\n\r\nSigned-off-by: Abhishek Varma <abhishek.varma@polymagelabs.com>", "comments": ["Hi @joker-eph  @sanjoy @sherhut \r\nCan you please review this PR?", "@bondhugula can you review this?", "> @bondhugula can you review this?\r\n\r\nSure.", "Thanks @bondhugula . Resolved all comments.", "(I'll fix these issues in the integration)", "> (I'll fix these issues in the integration)\r\n\r\nThank you @joker-eph ."]}]