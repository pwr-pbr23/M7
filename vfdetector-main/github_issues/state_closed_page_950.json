[{"number": 24933, "title": "Export model with \"REGRESS\" URI not working in tensorflow-serving", "body": "Hi,\r\n\r\nI've created the linear regression model which I want to serve using the tf-serving for which I have exported the model using the export model and **REGRESS_METHOD_NAME** in the signature_def after saving which the log is:\r\n\r\n```\r\ninputs {\r\n  key: \"inputs\"\r\n  value {\r\n    name: \"data_pipeline/IteratorGetNext:0\"\r\n    dtype: DT_FLOAT\r\n    tensor_shape {\r\n      dim {\r\n        size: 1\r\n      }\r\n    }\r\n  }\r\n}\r\noutputs {\r\n  key: \"outputs\"\r\n  value {\r\n    name: \"prediction/add:0\"\r\n    dtype: DT_FLOAT\r\n    tensor_shape {\r\n      dim {\r\n        size: 1\r\n      }\r\n    }\r\n  }\r\n}\r\nmethod_name: \"tensorflow/serving/regress\"\r\n```\r\n\r\nAfter deploying the saved_model with tf-serving for prediction I'm using \r\n\r\n1. Working API for PREDICTION - with **predict** in uri\r\n```\r\nimport json\r\ndata = json.dumps({\"signature_name\": \"tensorflow/serving/regress\", \"instances\": [150]})\r\ndata\r\n\r\nimport requests\r\nheaders = {\"content-type\": \"application/json\"}\r\njson_response = requests.post('http://localhost:8502/v1/models/linear:predict', data=data, headers=headers)\r\npredictions = json.loads(json_response.text)\r\npredictions\r\n```\r\n**Output**:\r\n```\r\n{\r\n    \"predictions\": [465.759\r\n    ]\r\n}\r\n```\r\n\r\n2. Not working API for PREDICTION - with **regress** in uri\r\n```\r\nimport json\r\ndata = json.dumps({\"signature_name\": \"tensorflow/serving/regress\", \"instances\": [150]})\r\ndata\r\n\r\nimport requests\r\nheaders = {\"content-type\": \"application/json\"}\r\njson_response = requests.post('http://localhost:8502/v1/models/linear:regress', data=data, headers=headers)\r\npredictions = json.loads(json_response.text)\r\npredictions\r\n```\r\n**Output**:\r\n```\r\n{\r\n\"error\": \"JSON Value: {\\n \\\"signature_name\\\": \\\"tensorflow/serving/regress\\\",\\n \\\"instances\\\": [\\n 150\\n ]\\n} When method is classify, key \\\\'examples\\\\' is expected and was not found\"\r\n}\r\n\r\n```\r\n\r\ntensorflow version: 1.10.0\r\ntf-serving version: Latest docker image\r\n", "comments": ["This issue is more appropriate on TesnorFlow Serving repo. Please post it on TF Serving from [here](https://github.com/tensorflow/serving/issues/new). Thanks!"]}, {"number": 24932, "title": "[XLA][Rematerialization] Add error message", "body": "Add error message when facing an error.", "comments": ["@jlebar thanks for the answer, makes sense to me, I've updated the patch. \r\n\r\nOriginally I was trying to understand how \"rematerizalization\" works in term of xla and after some code transformation I've faced an CHECK, so, it, possible, could be useful for someone who is newbie in xla and wants to understand how it works.\r\nThanks.", "@jlebar thanks for review, I've updated the patch regarding your comments,\r\nby the way,\r\n\r\n> seems not to add much value; it's still just repeating what is already in the CHECK (and in doing so, adding a lot of visual noise to the code).\r\n\r\nI was just watching the CHECK's like that https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo_rematerialization.cc#L602 and so on.\r\nThanks."]}, {"number": 24931, "title": "unexpected keyword argument 'serialized_options'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Are you getting the error during installation or during running some code. It would be great if you can provide a small code to reproduce the error. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Please reopen this bug. I am getting this error on Python 3.7.3, TensorFlow 1.13.1, MacOS Mojave 10.14.4. On import TensorFlow immediately explodes.\r\n\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/Users/raymond/Depot/ai-worker/duplicate_ai/test.py\", line 1, in <module>\r\n    from model import SiameseDream\r\n  File \"/Users/raymond/Depot/ai-worker/duplicate_ai/model.py\", line 1, in <module>\r\n    import keras.backend as K\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/backend/__init__.py\", line 89, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import node_def_pb2 as tensorflow_dot_core_dot_framework_dot_node__def__pb2\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/node_def_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/attr_value_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/tensor_pb2.py\", line 15, in <module>\r\n    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\r\n  File \"/Users/raymond/env/ai-worker/lib/python3.7/site-packages/tensorflow/core/framework/resource_handle_pb2.py\", line 22, in <module>\r\n    serialized_pb=_b('\\n/tensorflow/core/framework/resource_handle.proto\\x12\\ntensorflow\\\"r\\n\\x13ResourceHandleProto\\x12\\x0e\\n\\x06\\x64\\x65vice\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\tcontainer\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x03 \\x01(\\t\\x12\\x11\\n\\thash_code\\x18\\x04 \\x01(\\x04\\x12\\x17\\n\\x0fmaybe_type_name\\x18\\x05 \\x01(\\tBn\\n\\x18org.tensorflow.frameworkB\\x0eResourceHandleP\\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\\xf8\\x01\\x01\\x62\\x06proto3')\r\nTypeError: __init__() got an unexpected keyword argument 'serialized_options'\r\n"]}, {"number": 24930, "title": "Continuously differentiable eigendecomposition", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes, but I'd probably need some guidance.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, when any two eigenvalues are equal, the `tf.gradients( tf.self_adjoint_eig(matrix), matrix) ` function returns NaN. \r\nIt was proposed to return a subgradient (https://github.com/tensorflow/tensorflow/issues/12853) in this case, which equals `v_k^T*v_k` for kth eigenvalue. However, there [exists](https://www.win.tue.nl/analysis/reports/rana06-33.pdf) a procedure which allows to generate continuously differentiable eigendecomposition. This allows to have calculate true gradient (of a particluar decomposition) as soon as eigenbasis exists. Intuitively, if we choose a particular consistent way to choose eigenvectors from subspace, the eigendecomposition would be well defined.\r\n\r\nPossibly related issue (same about SVD): https://github.com/tensorflow/tensorflow/issues/13641\r\n**Will this change the current api? How?** The eigendecomposition will probably change in cases it is not unique.\r\n\r\n**Who will benefit with this feature?** People dealing with optimizations based on eigenvalues/eigenvectors.", "comments": ["ping", "@rmlarsen as far as I understand, you are the prime developer of linalg related stuff. Could you mentor me on this one or propose someone who could mentor me?", "@Randl,\r\nSorry for the delayed response. Can you please let us know if [tf.linalg.eigh](https://www.tensorflow.org/api_docs/python/tf/linalg/eigh) is the API that you are looking for? Thanks!\r\n ", "Yes, I think it was fixed by introducing Lorenz broadening here: https://github.com/tensorflow/tensorflow/pull/33808/files#diff-589f3bb3ae164fc660ae23ebe4eceabf5700612ecef44785c47ce788bd8e4584R712 "]}, {"number": 24929, "title": "TF Keras generic_utils_test missing test cases add", "body": "test case added for\r\n1-func_dump\r\n2-func_load", "comments": ["@fchollet \r\n\r\nPlease help to review the test case", "@fchollet \r\n\r\nThanks for your quick review, accepted all your comments and update my PR. Please review and thanks....", "@fchollet  Please review and approve this PR or suggest changes(if required)", "@Dayananda-V can you please resolve conflicts ?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 24928, "title": "Compiler error : To build Android TensorFlow library on mac os", "body": "\r\nOS platform : mac os 10.13.6\r\nTensorFlow : source code\r\nTensorFlow : 1.12\r\n\r\nBuild Cmd:\r\nNDK_ROOT=/Users/cheer67/android_ndk/android-ndk-r15c tensorflow/contrib/makefile/compile_android_protobuf.sh -c\r\n\r\nError Log:\r\n\r\nchecking whether make sets $(MAKE)... yes\r\nchecking whether make supports nested variables... yes\r\nchecking whether UID '501' is supported by ustar format... yes\r\nchecking whether GID '20' is supported by ustar format... yes\r\nchecking how to create a ustar tar archive... gnutar\r\nchecking for arm-linux-androideabi-gcc...  arm-linux-androideabi-gcc --sysroot /Users/cheer67/android_ndk/android-ndk-r15c/platforms/android-21/arch-arm\r\nchecking whether the C compiler works... no\r\nconfigure: error: in `/Users/cheer67/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: C compiler cannot create executables\r\nSee `config.log' for more details\r\ncheer67de-MacBook-Pro:tensorflow-master cheer67$ \r\n\r\n\r\n[config.log](https://github.com/tensorflow/tensorflow/files/2758840/config.log)\r\n", "comments": ["@resec Can you please take a look? Thanks!", "We're trying to deprecate mainline TensorFlow Android builds in favor of TensorFlow Lite. Thus, closing this one."]}, {"number": 24927, "title": "Update options.md", "body": "typo", "comments": []}, {"number": 24926, "title": "I have installed python 3.6.5 on windows and installed tensorflow. tensorflow not getting imported and throwing error", "body": "\r\n\r\n\r\n\r\n\r\n\r\n\r\nPFB the error message:\r\nC:\\Users\\hp>python\r\nPython 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the fields in the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Thanks.", "@sumitdas46 Please provide a template as requested earlier. If this was already resolved then please close the issue. Could you also check whether python is working on your system? Also, try the instructions provided [here](https://github.com/rnreich/ubuntu-tensorflow-gpu-all-versions). Thanks!", "I am closing the issue due to lack of recent activity. Please open new ticket if you see similar issue again. Thanks!"]}, {"number": 24925, "title": "Update README.md", "body": "typo", "comments": []}, {"number": 24924, "title": "Connection loops occur while Reading TFRecord from S3 ", "body": "Connection loops occur while Reading TFRecord from S3 \r\nnote that i've successfully installed aws credential already and have no trouble reading data from S3 \r\n( using method from tensorflow.python.lib.io import file_io )\r\n\r\nthe problem occurs  when i pass my tfrecords S3 Address directly through input_fn to tensorflow.**estimator**, it takes about 2 hours while do mnist classification training which takes 5 minutes only when same tfrecord from local disc\r\n\r\n\r\n\r\npart of my src are as below: \r\n\r\n    data_path = s3://mybucket/mydirectory/my.tfrecord\r\n    tensorflow.run_train_with_validation(model.train_input_fn,\r\n                                         data_path,\r\n                                         model.vali_input_fn,\r\n                                         steps=steps,\r\n                                         ..........)\r\n\r\n\r\nread data from S3 -  logs as below : (over 2H)\r\n\r\n\r\n2019-01-09 11:26:09.896151: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\n2019-01-09 11:26:10.150447: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/user/.aws/config and using profilePrefix = 1\r\n2019-01-09 11:26:10.150481: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/user/.aws/credentials and using profilePrefix = 0\r\n2019-01-09 11:26:10.150493: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/user/.aws/credentials for credentials file and /home/rubenchu//.aws/config for the config file , for use with profile default\r\n2019-01-09 11:26:10.150501: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http\r\n2019-01-09 11:26:10.150527: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2\r\n2019-01-09 11:26:10.150537: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2019-01-09 11:26:10.150550: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.152144: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25\r\n2019-01-09 11:26:10.154101: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.154241: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2\r\n2019-01-09 11:26:10.154253: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.288550: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.288649: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.348006: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.348144: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.362769: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.362973: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.396204: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.396380: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.409498: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.409625: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.423777: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.423993: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.442704: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.442862: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.462751: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.462928: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.479342: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n2019-01-09 11:26:10.479519: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.\r\n2019-01-09 11:26:10.500023: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key\r\n............\r\n....\r\n\r\n..\r\n\r\n\r\n..\r\n\r\n\r\n\r\nread data from local - logs as below (ONLY 5 MIN)\r\n\r\n2019-01-09 11:17:17.072920: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\n2019-01-09 11:17:17.907945: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.\r\n2019-01-09 11:17:17.907945: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.\r\n2019-01-09 11:17:18.158344: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.\r\n2019-01-09 11:17:18.158344: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.\r\n2019-01-09 11:17:18.360677: W tensorflow/core/framework/allocator.cc:122] Allocation of 22579200 exceeds 10% of system memory.\r\n2019-01-09 11:17:35,059|INFO|TrainingAndValiCheckpointSaverListener : end\r\n2019-01-09 11:17:35,059|INFO|TrainingAndValiCheckpointSaverListener : end\r\n\r\n\r\n......\r\n\r\n\r\ndoes anyone have the same issue?\r\n", "comments": ["I have the same issue. It takes really long and file read part is not optimal", "@jazztronomers , does this happen if you use the tf.data API as well? https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset", "@jazztronomers Could you try @karmel suggestion? Please let us know whether your issue was resolved or not? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!"]}, {"number": 24923, "title": "tf_upgrade_v2 AnnotationError ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version: 1.13\r\n- Python version: 3.4.3\r\n- Installed using virtualenv? pip? conda?: virtualenv \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n\r\n\r\n**Describe the problem**\r\nError found when run update the code for 2.0 preview\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`tf_upgrade_v2 --intree tensorlayer --outtree tensorlayer2`\r\n\r\n**Any other info / logs**\r\n```bash\r\n(env3)myhost:tensorlayer2 haodong$ tf_upgrade_v2 --intree tensorlayer --outtree tensorlayer2\r\n/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  return f(*args, **kwds)\r\nLine 98:11: Added keywords to args of function 'tf.reduce_sum'\r\nLine 117:9: Renamed 'tf.variable_scope' to 'tf.compat.v1.variable_scope'\r\nLine 118:16: Added keywords to args of function 'tf.reduce_mean'\r\nLine 118:31: Renamed 'tf.log' to 'tf.math.log'\r\nTraceback (most recent call last):\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/bin/tf_upgrade_v2\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/tf_upgrade_v2_main.py\", line 97, in main\r\n    args.input_tree, args.output_tree, args.copy_other_files, args.in_place)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 570, in process_tree\r\n    _, l_report, l_errors = self.process_file(input_path, output_path)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 441, in process_file\r\n    temp_file)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 489, in process_opened_file\r\n    self.update_string_pasta(\"\".join(lines), in_filename))\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/tensorflow/tools/compatibility/ast_edits.py\", line 453, in update_string_pasta\r\n    t = pasta.parse(text)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/__init__.py\", line 25, in parse\r\n    annotator.visit(t)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1153, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 245, in visit\r\n    return visitor(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 47, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 211, in visit_Module\r\n    self.generic_visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 253, in generic_visit\r\n    self.visit(item)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1153, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 245, in visit\r\n    return visitor(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 90, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 400, in visit_FunctionDef\r\n    self.visit(stmt)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1153, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 245, in visit\r\n    return visitor(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 90, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 294, in visit_With\r\n    return self.visit_With_3(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 344, in visit_With_3\r\n    self.visit(stmt)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1153, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 245, in visit\r\n    return visitor(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 90, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 282, in visit_For\r\n    self.visit(stmt)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1153, in visit\r\n    super(AstAnnotator, self).visit(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 127, in visit\r\n    super(BaseVisitor, self).visit(node)\r\n  File \"/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/python3.4/ast.py\", line 245, in visit\r\n    return visitor(node)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 90, in wrapped\r\n    f(self, node, *args, **kwargs)\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 219, in visit_If\r\n    default=':\\n')\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1334, in attr\r\n    attr_parts.append(self.token(attr_val))\r\n  File \"/Users/haodong/Documents/Projects/python-workspace/env3/lib/python3.4/site-packages/pasta/base/annotate.py\", line 1260, in token\r\n    token_val, token.src, token.start[0], token.line))\r\npasta.base.annotate.AnnotationError: Expected ':' but found 'b\":\"'\r\nline 564:             if line.startswith(b\":\"):  # Skip comments.\r\n```", "comments": ["@zsdonghao it will be more helpful if you could provide a `self-contained` code that could trigger the  error. \r\n\r\nWithout the content of the `tensorlayer` the log you posted may not be very easy to see where the issue is.", "I fixed this in google/pasta at head. I have to cut a new release of pasta and update the deps in TF.", "@zsdonghao Upgrading google_pasta (pip install -U google_pasta) should fix this. Give it a try? I'll update the deps in the night tf package. ", "@yongtang @martinwicke Thanks, I successfully convert the codes one-by-one instead of converting the entire folder.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!\r\n"]}, {"number": 24922, "title": "Update tf_ops_compatibility.md", "body": "typo", "comments": []}, {"number": 24921, "title": " Update ragged_array_ops", "body": "typo fixed ", "comments": []}, {"number": 24920, "title": "how to use resnet_v1", "body": "\r\n", "comments": ["@huangyihe2017 Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance.\r\n\r\nFor transfer learning, TF hub may be a good reference. It provides some guides and tutorials for different ml tasks, such as [image classification](https://www.tensorflow.org/hub/tutorials/image_retraining). I highly recommend you to check it first.\r\nThere are several resources [like](https://www.kaggle.com/gaborfodor/resnet50-example) and [this](https://stackoverflow.com/questions/40054067/looking-for-resnet-implementation-in-tensorflow). Thanks!", "I am sorry for that and  I will ask this kind of question at Stackoverflow next time. Thank you very much.", "@huangyihe2017 Thanks! I am closing this issue."]}, {"number": 24919, "title": "About the MFCC feature in input_data.py", "body": "Dear,\r\nIn the script,\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/input_data.py](url)\r\nis there any related pdf or other file about the mfcc feature extraction,\r\nCould U help me about this, please?\r\nAny advice or suggestion will be okey.\r\nThx.", "comments": ["Second,\r\nWhat's the meaning of the func get_features_range in input_data.py ?\r\n`  # TODO(petewarden): These values have been derived from the observed ranges\r\n of spectrogram and MFCC inputs. If the preprocessing pipeline changes,\r\n   they may need to be updated.`\r\nand also the up words ?\r\nIf the sample_rate change, need change the range?\r\nAny help will be good.\r\nThx", "@ucasiggcas Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. I found this [paper](http://www.cs.cmu.edu/~chanwook/MyPapers/b_li_interspeech_2017.pdf) and there are several resources covering advanced MFCC. Thanks!", "> @ucasiggcas Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support. Github is mainly for addressing bugs in installation and performance. I found this [paper](http://www.cs.cmu.edu/~chanwook/MyPapers/b_li_interspeech_2017.pdf) and there are several resources covering advanced MFCC. Thanks!\r\n\r\nThanks,\r\nI have another question,\r\nin the rows of 420,437,443\r\n` tf.summary.image(\r\n          'spectrogram', tf.expand_dims(spectrogram, -1), max_outputs=1)`\r\nWhat does it mean?No Variable?Is it useful?\r\nThx", "@ucasiggcas -1 refers to expanding dimensions in the end of the tensor. Please take a look at [here](https://www.tensorflow.org/api_docs/python/tf/expand_dims) for more details. If max_outputs is 1, the summary value tag is 'name/image'. please check [here](https://www.tensorflow.org/api_docs/python/tf/summary/image). Thanks!\r\n\r\nI am closing the issue but please post this kind of support questions at Stackoverflow. Thanks!\r\n"]}, {"number": 24918, "title": "Tflite round", "body": "ROUND operator for TFLITE", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@siju-samuel  Please rebase to resolve the branch conflicts.", "> @jdduke @hgadig sorry for the trouble, could you please approve again. I needed to rebase due to the cos op merge. Thanks for the support for review.\r\n\r\nSure. @jdduke  is the one who needs to approve.", "@jdduke could you please approve this? The approval got removed after rebasing. ", "@jdduke thanks for your review and approval, ELU & REVERSE_SEQUENCE got merged after your approval, so i again needed to rebase.\r\ncould you please approve again. TIA", "@jdduke kindly approve again. Added scalar. Thanks for pointing it out.", "@rthadur gentle ping to merge this PR, I'm afraid of new conflicts happening because of other merges. Thanks in advance.", "> @rthadur gentle ping to merge this PR, I'm afraid of new conflicts happening because of other merges. Thanks in advance.\r\n\r\n@siju-samuel we can not merge these changes until conflicts are resolved , can you please rebase your branch once again.", "@jdduke Thanks for helping me previously with the review and approval. Gentle ping to approve again. Again got conflicts and rebased.", "Hmm, it might be that the optimized version is correct, but I'm seeing slightly different results on mobile. It's possible TensorFlow(Mobile) has the same issue.", "@jdduke Thank you for the reviews and helping me to improve this PR. I have added testcase and updated with rint instead of lrint. Could you please review again. TIA\r\n\r\n", "@jdduke Thank you so much for helping me find the missing things. The code is updated based on 2 review comments, Could you please check again and approve if everything is fine.", "@hgadig @rthadur Could you please help to merge this PR before any conflicts happen. TIA.", "@jdduke i needed to rebase due to MATRIX_DIAG op addition, could you please help to approve again. Thanks in advance.", "Thanks for your patience!", "@jdduke Sorry to bother you again, QUANTIZE operator got merged and i needed to rebase. Kindly go through again and approve.", "@hgadig @rthadur Gentle ping to merge this PR. TIA", "@jdduke  `make_round_test` function moved from   `generate_examples.py` to `generate_examples_lib.py`\r\ndue to commit `17ca6549ba Refactoring: Split logic from generate_examples py_binary target.`\r\nPlease check and approve. Thanks a lot for your support.", "Out of curiosity, is there a specific model you're using which requires this op? Is it blocking you?", "Im just studying and exploring tflite with my own model. Im using my private build, not blocking me. ", "@jdduke Because of `MATRIX_SET_DIAG` i rebased it recently, please check and approve once again. Thank you. ", "@rthadur Gentle ping, please help to merge this PR. TIA.", "Just a note, this is stuck on review internally as older Android NDK versions don't work properly with cfenv. In general, it requires min API 21, which is too restrictive (we still support API 18/19). Can you think of a workaround?", "@jdduke Could you please help to review again and approve if no issues. Thanks.", "@rthadur @hgadig could you please help to merge this PR? TIA. Its quite sometime since it got approved and im afraid of conflicts.", "Sorry for the delay, this hit another snag in our import pipeline. Working to land now.", "This has been merged in PR #24918.", "Thanks @jdduke "]}, {"number": 24917, "title": "Typo error fixed execution_callbacks.py", "body": "", "comments": []}, {"number": 24916, "title": "The Object Detection Tutorial is broken", "body": "**System information**\r\n- TensorFlow version: 1.13.0-dev20190114\r\n- Doc Link: https://www.tensorflow.org/lite/\r\n**Describe the documentation issue**\r\nTFLite's home page links to this Object Detection tutorial: https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193 \r\nThe tutorial is not compatible with the latest TensorFlow (1.13). It has multiple references to the TOCO convertor which used to be in tensorflow/contrib/lite/toco:toco but has been recently removed. I believe, it has been replaced by TFLiteConverter which has a different interface, so the tutorial needs to be updated. Plus, it would make a lot of sense to provide an iOS specific object detection tutorial. You already have both Android and iOS image classifier tutorials, so it would be sweet to give object detection the same coverage.\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nThis tutorial must be fixed by somebody who knows how to get TFLiteConverter to work. I gave it a shot but was unable to convert the post-processing operation. I already filed issue 24910 with all the details on this.", "comments": ["The latest tutorial has been updated @ https://www.tensorflow.org/lite/models/object_detection/overview."]}, {"number": 24915, "title": "Make MutableHashTableOfTensors trainable", "body": "This is a part implementation of issues https://github.com/tensorflow/tensorflow/issues/19324 and https://github.com/tensorflow/tensorflow/issues/24539.\r\n\r\nThe main idea is making `MutableHashTableOfTensors` trainable and segments the unordered_map in  `MutableHashTableOfTensors` to speed up. In our testing, it can achieve about 80% training speed in 6-maching distribution compared with `MutableDenseHashTable`. But you can add new features to the table flexibility.\r\n\r\nCurrently, the changes only support `MutableHashTableOfTensors` and imperfect. But can you give some suggestion for the implementation route? \r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "This is a useful change, but comes at an unfortunate time.  We're in the process of updating all lookup tables for TF2, and this involves anumber of changes to the python code.  Could I ask you to wait until @rohan100jain is finished with the refactor?  at that point you may need to rebase.", "waiting for this feature to be accepted~", "@ebrevdo Thanks for your reply. I will update the modification after TF2.0 released.", "Sorry. I'm busy with other works currently. And I will start the rebase work next week.", "Nagging Reviewer @ysuematsu, @rohan100jain, @allenlavoie: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Sorry\uff0c I need more time. And avoiding the robot reminding again, I will close this pr for the moment, and reopen when the work accomplished.", "@yejw5 \r\nI tried to use your code to do training( I compiled a new TF package, all test case passed), but got following error.\r\n\r\ncode looks like:\r\n```\r\nW = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64,\r\n                                          value_dtype=tf.float32,\r\n                                          default_value=0,\r\n                                          name=\"w\")\r\nfeat_wgts = tf.nn.embedding_lookup(W, feat_ids)\r\ny_w = tf.multiply(feat_wgts, feat_vals)\r\ny = y_bias + y_w\r\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y, labels=labels))\r\ntrain_op = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\r\n```\r\nhere is the error:\r\n\r\n```\r\n    train_op = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 433, in minimize\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 615, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/adam.py\", line 135, in _create_slots\r\n    self._zeros_slot(v, \"m\", self._name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 1176, in _zeros_slot\r\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/slot_creator.py\", line 178, in create_zeros_slot\r\n    slot_shape = primary.get_shape()\r\nAttributeError: 'MutableHashTable' object has no attribute 'get_shape'\r\n```\r\n\r\nAnything wrong about about using 'MutableHashTable'? Thanks a lot!", "@zhjunqin Sorry. It's an experiment to show our design. So there are some limits existing, such as only support `tf.train.GradientDescentOptimizer`, scope not working and so on....", "@yejw5  Thanks for explanation.", "@yejw5  We checkout the branch from your github(https://github.com/yejw5/tensorflow.git). And we get the following errors:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 527, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 527, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got <tensorflow.contrib.lookup.lookup_ops.MutableHashTable object at 0x7f4ac40eb4a8>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf/tf_sparse_embedding_test_dist_big-2-hq-hashtable.py\", line 294, in <module>\r\n    train()\r\n  File \"tf/tf_sparse_embedding_test_dist_big-2-hq-hashtable.py\", line 177, in train\r\n    sparse0_input = get_input(\"sparse0_weight\", tf.zeros_initializer(), sparse0_feature_dim, emb0_dim, sparse0_feature)\r\n  File \"tf/tf_sparse_embedding_test_dist_big-2-hq-hashtable.py\", line 90, in get_input\r\n    partition_strategy='div', combiner='sum') + b\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 463, in embedding_lookup_sparse\r\n    params, ids, partition_strategy=partition_strategy, max_norm=max_norm)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 344, in embedding_lookup\r\n    transform_fn=None)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 138, in _embedding_lookup_and_transform\r\n    with ops.colocate_with(params[0]):\r\n  File \"/opt/conda/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4094, in _colocate_with_for_gradient\r\n    with self.colocate_with(op, ignore_existing):\r\n  File \"/opt/conda/lib/python3.6/contextlib.py\", line 81, in __enter__\r\n    return next(self.gen)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4146, in colocate_with\r\n    op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1307, in internal_convert_to_tensor_or_indexed_slices\r\n    value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1146, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 229, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 208, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 531, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'tensorflow.contrib.lookup.lookup_ops.MutableHashTable'> to Tensor. Contents: <tensorflow.contrib.lookup.lookup_ops.MutableHashTable object at 0x7f4ac40eb4a8>. Consider casting elements to a supported type.\r\n```\r\nCan you help us to resolve it ? Thanks.\r\n@zhjunqin Did you meet this error?", "@lxl910915 It seems hard to location the problem. This version was a draft of our thinking. It's not surely to work well...", "> @lxl910915 It seems hard to location the problem. This version was a draft of our thinking. It's not surely to work well...\r\n\r\n@yejw5\r\nThank you for your replay, we have resolved it.\r\nCan you provide a demo to use this. For instance, we use get_variable like this:\r\n```\r\nw = tf.get_variable(name=\"var1\", shape=(1000, 16))\r\ninput1 = tf.nn.embedding_lookup_sparse(params=w, sp_ids=feature)\r\ninput = tf.concat([input1] + [input2], 1)\r\n```\r\n\r\nWe can assign variable shape and concat several 'input'. But for MutableHashTable, we don't known how to use it.\r\n```\r\nw = tf.contrib.lookup.MutableHashTable(key_dtype=tf.int64, value_dtype=tf.float32, default_value=0)\r\ninput1 = tf.nn.embedding_lookup_sparse(params=w, sp_ids=feature)\r\ninput = tf.concat([input1] + [input2], 1)\r\n```\r\n\r\n\r\nWhat's more, do you have a plan to commit a new PR ?", "Sorry, this pr was an early edition. The examples haven't been preserved... And we are focused on inner requirements currenly. There's no new plans...", "@ebrevdo @rohan100jain @yejw5 Is there any forward here?"]}, {"number": 24914, "title": "Fix spelling error: adverserial -> adversarial", "body": "", "comments": ["Thanks!", "@alextp  could you please look at the failing checks", "@rthadur it just says \"page not found\"", "@alextp  thank you , @DanCard please create your PR in \"tensorflow:master \"", "> @alextp thank you , @DanCard please create your PR in \"tensorflow:master \"\r\n\r\n@DanCard  gentle ping", "Master has the fix.  Sorry for the noise."]}, {"number": 24913, "title": "When I compile andorid so using bazel build and reference this so in a new project, the generated so will be linked to the absolute path.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 16.04\r\n- NA\r\n- TensorFlow installed from source\r\n- Python version: Python 2.7.12\r\n- Bazel version: release 0.21.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 4.9.3-13ubuntu2) 4.9.3\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nRecurring step\r\n1.  Use the following command to compile libtensorflow_demo.so in tensorflow project\r\n\r\nbazel build -c opt //tensorflow/examples/android:tensorflow_demo    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=arm64-v8a\r\n\r\n2. use libtensorflow_inference.so with ndk-build and get libCameraTest.so\r\nthe Fragment of Android.mk like this\r\ninclude $(CLEAR_VARS)\r\nLOCAL_MODULE := libtensorflow_demo-prebuild\r\nLOCAL_SRC_FILES := $(LOCAL_PATH)/lib/libtensorflow_demo.so\r\nLOCAL_EXPORT_C_INCLUDES := $(LOCAL_PATH)/include\r\ninclude $(PREBUILT_SHARED_LIBRARY)\r\n\r\ninclude $(CLEAR_VARS)\r\nLOCAL_PATH := $(MY_PATH)\r\nLOCAL_MODULE := CameraTest\r\nLOCAL_SHARED_LIBRARIES  += libtensorflow_demo-prebuild\r\nLOCAL_CPPFLAGS  += -std=c++11\r\nLOCAL_CFLAGS := -g\r\nLOCAL_LDLIBS    += -llog -landroid -lz \r\n#LOCAL_SHARED_LIBRARIES += libc libdl\r\n\r\nLOCAL_ARM_MODE := arm  \r\ninclude $(BUILD_SHARED_LIBRARY)\r\n\r\n3. use objdump -T libCameraTest.so | grep Needed\r\ni got the follow result\r\n NEEDED               D:/workspace/new_invisioncontroller/InVisionBTControl_ball_tensorflow/InVisionBTControl_ball_tensorflow/InVisionBTControl_ball_tensorflow/app/build/intermediates/ndkBuild/debug/obj/local/arm64-v8a/libtensorflow_demo.so\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["@SuperSaiyan30 Please check [tested build configurations](https://www.tensorflow.org/install/source). I think this error may be due to bazel version 0.21.0. For the system configuration you mentioned, Bazel 0.15.0 is supported. Could you try to install 0.15.0 version and check whether the issue persists? Thanks! ", "@jvishnuvardhan \uff0c thank you for reply\uff0c\r\nthe tensorflow need bazel 0.18.0 at least, i try with bazel 0.18.1 and the issue persists", "This does not look related to TensorFlow Mobile, this looks more like an Android toolchain issue in how you're generating your app .so. StackOverflow is probably a better place for this question."]}, {"number": 24912, "title": "When passing tf.data.Dataset instance to model.predict method which is instantiated by tf.keras.Sequential, tf.keras.Model, subclassing tf.keras.Model, return of model.predict is different from return of model.call ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.6\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nWhen passing `tf.data.Dataset` instance to `model.predict` method which is instantiated by `tf.keras.Sequential`, `tf.keras.Model`, subclassing `tf.keras.Model`, return of model.predict is different from return of `model.call`\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n```\r\n```python\r\n1.12.0\r\n```\r\n```python\r\nx = np.random.randn(60,30).astype(np.float32)\r\ny = np.random.randint(low=0, high=10, size = 60).astype(np.int32)\r\n\r\nx_tr = x[0:20]\r\ny_tr = y[0:20]\r\nx_val = x[20:40]\r\ny_val = y[20:40]\r\nx_tst = x[40:60]\r\ny_tst = y[40:60]\r\n\r\nprint(x_tr.shape, y_tr.shape)\r\nprint(x_val.shape, y_val.shape)\r\nprint(x_tst.shape, y_tst.shape)\r\n\r\n\r\ntr_dataset = tf.data.Dataset.from_tensor_slices((x_tr,y_tr))\r\ntr_dataset = tr_dataset.batch(batch_size=4).repeat()\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val))\r\nval_dataset = tr_dataset.batch(batch_size=4).repeat()\r\ntst_dataset = tf.data.Dataset.from_tensor_slices((x_tst,y_tst))\r\ntst_dataset = tst_dataset.batch(batch_size=4)\r\n\r\nprint(tr_dataset)\r\nprint(val_dataset)\r\nprint(tst_dataset)\r\n```\r\n```python\r\n(20, 30) (20,)\r\n(20, 30) (20,)\r\n(20, 30) (20,)\r\n<RepeatDataset shapes: ((?, 30), (?,)), types: (tf.float32, tf.int32)>\r\n<RepeatDataset shapes: ((?, ?, 30), (?, ?)), types: (tf.float32, tf.int32)>\r\n<BatchDataset shapes: ((?, 30), (?,)), types: (tf.float32, tf.int32)>\r\n```\r\n```python\r\nclass Model(keras.Model):\r\n    def __init__(self, num_classes):\r\n        super(Model, self).__init__()\r\n        self.dense = keras.layers.Dense(units=10, activation='softmax')\r\n    def call(self, inputs):\r\n        score = self.dense(inputs)\r\n        return score\r\n  \r\nmodel = Model(10)\r\nmodel.compile(optimizer=tf.train.GradientDescentOptimizer(.1),\r\n              loss=keras.losses.sparse_categorical_crossentropy)\r\nmodel.fit(tr_dataset, epochs=5, steps_per_epoch=20//4,\r\n          validation_data=val_dataset, validation_steps=20//4)\r\n```\r\n```python\r\nEpoch 1/5\r\n5/5 [==============================] - 0s 36ms/step - loss: 2.5626 - val_loss: 1.9200\r\nEpoch 2/5\r\n5/5 [==============================] - 0s 2ms/step - loss: 1.9345 - val_loss: 1.4256\r\nEpoch 3/5\r\n5/5 [==============================] - 0s 2ms/step - loss: 1.4506 - val_loss: 1.0624\r\nEpoch 4/5\r\n5/5 [==============================] - 0s 1ms/step - loss: 1.0906 - val_loss: 0.8049\r\nEpoch 5/5\r\n5/5 [==============================] - 0s 2ms/step - loss: 0.8314 - val_loss: 0.6274\r\n```\r\n```python\r\n# yhat_from_call_method\r\nsess = keras.backend.get_session()\r\nx_tst_tensor = tf.convert_to_tensor(x_tst)\r\nyhat_from_call_method = sess.run(model(x_tst_tensor))\r\nyhat_from_call_method = np.argmax(yhat_from_call_method, axis = -1)\r\nprint(yhat_from_call_method)\r\n```\r\n```python\r\n[7 4 7 1 8 8 3 4 8 9 8 4 0 7 1 7 4 3 0 0]\r\n```\r\n```python\r\n# yhat_from_predict_method \r\nyhat_from_predict_method = model.predict(tst_dataset, steps=20//4)\r\nyhat_from_predict_method = np.argmax(yhat_from_predict_method, axis =-1)\r\nprint(yhat_from_predict_method)\r\n```\r\n```python\r\n[8 4 2 4 1 8 4 8 8 7 6 1 9 1 1 3 6 5 4 1]\r\n```\r\n```python\r\nprint(yhat_from_call_method == yhat_from_predict_method)\r\n```\r\n```python\r\n[False  True False False False  True False False  True False False False\r\n False False  True False False False False False]\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Facing a similar issue with `LSTM` in both `v1.13.1` and `2.0.0-alpha0`. I have tried both eager_execution and Session api. A simple code to reproduce the issue, where first predictions are same for both, but consecutive predictions are not:\r\n\r\nhttps://colab.research.google.com/drive/1D-kgD7NiRXTNTNwVr18H8C-edaL2zpqw\r\n\r\nI have verified that predictions from the `call` function are valid, but not those from `predict`.\r\n", "@aisolab Sorry for the late response. This is not a bug in tensorflow. \r\n\r\nIf you replace the following line in your code \r\n\r\n```\r\ntr_dataset = tf.data.Dataset.from_tensor_slices((x_tr,y_tr))\r\ntr_dataset = tr_dataset.batch(batch_size=4).repeat()\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val))\r\nval_dataset = tr_dataset.batch(batch_size=4).repeat()\r\ntst_dataset = tf.data.Dataset.from_tensor_slices((x_tst,y_tst))\r\ntst_dataset = tst_dataset.batch(batch_size=4)\r\n```\r\n\r\nThe following will work without any issue.\r\n\r\n```\r\ntr_dataset = tf.data.Dataset.from_tensor_slices((x_tr,y_tr)).batch(batch_size=4).repeat()\r\n#tr_dataset = tr_dataset.batch(batch_size=4).repeat()\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(batch_size=4).repeat()\r\n#val_dataset = tr_dataset.batch(batch_size=4).repeat()\r\ntst_dataset = tf.data.Dataset.from_tensor_slices((x_tst,y_tst)).batch(batch_size=4)\r\n#tst_dataset = tst_dataset.batch(batch_size=4)\r\n```\r\nPlease check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/f0c6d3f471c3706d52c328a8903dfb50/tf_24912_dataset.ipynb). \r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24912\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24912\">No</a>\n", "Ao repetir a rotina para \" yhat_from_call_method\" tive como resposta: RuntimeError: `get_session` is not available when using TensorFlow 2.0. Existe uma rotina que fa\u00e7a a mesma coisa no TensorFlow 2.0?"]}, {"number": 24911, "title": "Disable InterpreterFlexTest", "body": "This test currently only works with --config=monolithic. Disable for\r\nnow until the select .so supports execution against the shared TF lib.\r\n\r\nPiperOrigin-RevId: 229269278", "comments": []}, {"number": 24910, "title": "TFLiteConverter unable to convert object detection model from export_tflite_ssd_graph.py", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.13.0-dev20190114\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, MUL, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco_from_protos\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 59, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 33, in execute\r\n    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\nssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03\r\n\r\n**Any other info / logs**\r\nI trained the above mentioned model on my images, and then exported the graph with the --add_postprocessing_opt option (which I assume adds the TFLite_Detection_PostProcess op):\r\npython /tf/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n    --pipeline_config_path /tf/notebooks/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config \\\r\n    --trained_checkpoint_prefix /tf/notebooks/model.ckpt \\\r\n    --output_directory /tf/notebooks/tflite \\\r\n    --add_postprocessing_op=true\r\n\r\nThen I used the following Python code in order to convert the model to TFLite format:\r\n\r\nimport tensorflow as tf\r\ngraph_def_file = \"/tf/notebooks/scripts/both_training/tflite/tflite_graph.pb\"\r\ninput_arrays = [\"normalized_input_image_tensor\"]\r\noutput_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\ninput_shapes = {\"normalized_input_image_tensor\" : [1, 640, 640, 3]}\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"/tf/notebooks/tflite/model.tflite\", \"wb\").write(tflite_model)\r\n\r\nI tried this with and without target_ops=SELECT_TF_OPS and the convert() call always crashes due to TFLite_Detection_PostProcess operation. The TFLite file is not created. Please help!", "comments": ["The FPN model is not supported yet. ", "Ouch... I was using the official Tensorflow Detection Model Zoo and assumed that all the \"official\" models can be converted to TFLite. If this is not the case, I would highly recommend two things:\r\n\r\n1) Clearly indicate which models are compatible with TFLite on the list of models https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md (P.S. I really hope that all models that don't use FPN are supported. If there are some other exceptions please help me identify them!)\r\n\r\n2) TFLiteConverter must provide a specific message when it sees an unsupported model. The current message points to TFLite_Detection_PostProcess which is potentially confusing. If it said something like \"FPN feature extractors are not supported by TFLite yet\" that would be much more clear. ", "Can you please confirm if TFLiteConverter is compatible with any Object Detection models besides the basic ssd_mobilenet? I am not satisfied with SSD models because their prediction accuracy is not nearly as good as FPN or RCNN models... Also, I noticed that the list of \"hosted models\" for TFLite does not include ANY object detection models: https://www.tensorflow.org/lite/models even though there is an official tutorial that successfully converts an old SSD Mobilenet model to TFLite: https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n", "There is no support for complex custom models yet as I am also stuck in this some operators are not supported by the Tensorflow Lite yet like AsString AAnd and few more.\r\n\r\nTake a look at this if you can make sense out of it somehow.\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\n\r\n", "We're actively working on support for FPN/RCNN models, and will have more to report in the near future. Apologies for any confusion, but for now only the SSD models are likely to convert cleanly. https://www.tensorflow.org/lite/models/object_detection/overview is the current sample for this.", "Thanks Jared. I have a trained SSD keras model (h5).   I am trying to convert it to tflite using tflite converter. However, it complains:\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_model_file)\r\n\r\n_ValueError: Unknown layer: Normalize_\r\n\r\nIs there a way to add/register custom layers in python to  TFLite converter ? \r\n\r\n\r\n", "Starting 1.14, [`TFLiteConverter.from_keras_model_file`](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter#from_keras_model_file) takes in the argument `custom_objects` which is passed directly into the Keras loading function. The logic should look something like the following:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(\r\n   keras_model_file,\r\n   custom_objects={'Normalize': Normalize})\r\n```\r\n\r\nI'm not sure where in the Keras library the `Normalize` layer is described. There are some normalization libraries under `keras.layers.normalization`. However, there is no `Normalize` there in 1.14.\r\n\r\nIf you need to use 1.13 then you can load the Keras model yourself and use `TFLiteConverter.from_session` with logic similar to the following:\r\n\r\n```\r\ntf.keras.backend.set_learning_phase(False)\r\nkeras_model = tf.keras.models.load_model(model_file, custom_objects)\r\nsess = tf.keras.backend.get_session()\r\nconverter = tf.lite.TFLiteConverter.from_session(sess, keras_model.inputs, keras_model.outputs)\r\n```", "Thanks Nupur. Now I am able to convert my custom keras SSD (with a custom Normalize layer) to  tflite format. How can i incorporate the post processing layer (TFLite_Detection_PostProcess ) in the tflite model ? Can i add that custom post processing layer in python/keras without using bazel tool? \r\n\r\n", "In order to use the `TFLite_Detection_PostProcess` op, you need to:\r\n1. Freeze the graph.\r\n2. Use [`export_tflite_ssd_graph.py`](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py) to add `TFLite_Detection_PostProcess` to the frozen graph. I don't know the specifics of this script. However, there are a handful of other bugs you might be able to reference if you run into issues with this step.\r\n\r\nIn order to freeze the graph there are two approaches you can try:\r\n- Save the model as a SavedModel. In 1.14 there is [`tf.keras.models.save_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model). After getting the SavedModel, freeze the graph using [`freeze_graph.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) which takes in a SavedModel. I am unfortunately not familiar enough with the internals of the SavedModel to know if this will have issues due to the `Normalize` layer.\r\n- Alternatively try using [`convert_variables_to_constants`](https://www.tensorflow.org/api_docs/python/tf/graph_util/convert_variables_to_constants) which takes in a Session. Try passing in the Keras session directly.\r\n", "Did someone successfully convert a ssd model from model zoo and has now a tflite model with 4 outputs (detection_boxes, detection_classes, detection_scores, num_boxes)? I somehow only get a model with two outputs (raw_outputs/box_encodings, raw_outputs/class_predictions) when using export_tflite_ssd_graph.py (I also set `add_postprocessing_op True`). I opened a new issue on that: https://github.com/tensorflow/tensorflow/issues/31015\r\nThanks for helping!", "Thanks again Nupur. \r\n\r\nIs there a python module for TFLite_Detection_PostProcess ? I can perhaps just add that as a custom keras output layer to my model. Would that work?\r\n\r\n\r\n ", "Unfortunately there is no Python module for the op. To elaborate a bit, `TFLite_Detection_PostProcess` is a custom op that is only available in TensorFlow Lite - so once you run the `export_tflite_ssd_graph.py` script to add the op the model can't be loaded into the TensorFlow runtime.\r\n\r\nWe are working on adding support for control flow which should hopefully make models like SSD easier to convert. However, that is a longer term project.", "> Can you please confirm if TFLiteConverter is compatible with any Object Detection models besides the basic ssd_mobilenet? I am not satisfied with SSD models because their prediction accuracy is not nearly as good as FPN or RCNN models... Also, I noticed that the list of \"hosted models\" for TFLite does not include ANY object detection models: https://www.tensorflow.org/lite/models even though there is an official tutorial that successfully converts an old SSD Mobilenet model to TFLite: https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\r\n\r\nYes,You are right, I have converted 0 model to tflite ", "There have been quite a few changes/improvements in this space over the last several years, it's probably worth trying again.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24909, "title": "Cherrypick cl/20190114", "body": "Avoid exceeding the GPU memory limit by reducing workspace limit\r\n\r\nPiperOrigin-RevId: 228555498", "comments": []}, {"number": 24908, "title": "More disk space debugging messages.", "body": "", "comments": []}, {"number": 24907, "title": "Add the data file to the data section in BUILD.", "body": "PiperOrigin-RevId: 228782621", "comments": []}, {"number": 24906, "title": "tf.contrib.eager.defun does not handle variables as documented", "body": "TensorFlow version: 1.12.0\r\n\r\nThe docs for `defun` contain this example:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef fn():\r\n  x = tf.Variable(0.0)\r\n  x.assign_add(1.0)\r\n  return x.read_value()\r\n\r\n# `fn` is a Python function, so x is created, initialized, and destroyed upon\r\n# every invocation\r\nassert fn().numpy() == fn().numpy() == 1.0\r\n\r\ncompiled = tf.contrib.eager.defun(fn)\r\n\r\n# Compiling `fn` with `defun` hoists all variables outside of the generated\r\n# graph, so initialization happens exactly once.\r\nassert compiled().numpy() == 1.0\r\nassert compiled().numpy() == 2.0\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe example should run as is.\r\n\r\n**Describe the current behavior**\r\nBut instead it throws this error:\r\n> AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.", "comments": ["Looks like in TensorFlow 1.11.0 this still worked.", "This is working as intended. tfe.defun now does not keep references to the variables you create inside it; you're supposed to do that via objects or something similar. Consider tf.wrap_function if you just want to wrap arbitrary variable-creating code in functions.", "Thanks @alextp, but then the docs are wrong; the code is taken directly from there. ", "@alextp Is there any way to get the `wrap_function` in TF 1.12. `defun` doesn\u2019t do it anymore and `wrap_function` does not yet exist?!", "I think wrap_function is only on nightly / will be on 1.13. There are other\nusable workarounds (make the variables global; append them to a global\nlist; etc).\n\nOn Mon, Jan 14, 2019 at 8:42 PM Jonas Rauber <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> Is there any way to get the\n> wrap_function in TF 1.12. defun doesn\u2019t do it anymore and wrap_function\n> does not yet exist?!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24906#issuecomment-454264945>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxStBsH_jZTEiYGDCTfeW9g0YwI7jks5vDVwagaJpZM4Z_w2b>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 24905, "title": "Build AVX512 Error (v1.13.0): target specific option mismatch  _mm512_xor_ps (__m512 __A, __m512 __B)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 4.4.103-6.38-default x86_64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.0\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): 7.3.0\r\n\r\n**Describe the problem**\r\nThe build stoped, complaining about \"error: inlining failed in call to always_inline '__m512 _mm512_xor_ps(__m512, __m512)': target specific option mismatch\r\n _mm512_xor_ps (__m512 __A, __m512 __B)\"\r\n\r\nSee the following error message for details: \r\nERROR: /lus/theta-fs0/projects/datascience/hzheng/build/tensorflow/1.13/tensorflow/compiler/xla/service/cpu/BUILD:538:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1): gcc failed: error executing command\r\n  (cd /lus/theta-fs0/projects/datascience/hzheng/bazel_cache/_bazel_hzheng/49b930504bfa76e4e005c923edac9eb5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHONPATH=/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/soft/datascience/tensorflow/tf1.12/lib/python3.5/site-packages:/soft/datascience/tensorflow/tf1.12:/home/hzheng/mylibs/python:/home/hzheng/mylibs/python: \\\r\n    PYTHON_BIN_PATH=/opt/intel/python/2017.0.035/intelpython35/bin/python \\\r\n    PYTHON_LIB_PATH=/soft/datascience/tensorflow/tf1.12 \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_NEED_ROCM=0 \\\r\n  /opt/gcc/7.3.0/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_syc\\\r\nl -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -g -mavx -mavx2 -mfma -mavx512f -mavx512pf -mavx512cd -mavx512er '-mtune=knl' -DEIGEN_USE_VML -DEIGEN_AVOID_STL_ARRAY -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/compiler/xla/service/cpu/runtime_fft.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from /opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/immintrin.h:57:0,\r\n                 from external/eigen_archive/Eigen/src/Core/util/ConfigureVectorization.h:313,\r\n                 from external/eigen_archive/Eigen/Core:22,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from ./tensorflow/compiler/xla/types.h:21,\r\n                 from ./tensorflow/compiler/xla/executable_run_options.h:20,\r\n                 from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:20:\r\n/opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/avx512dqintrin.h: In function 'Packet Eigen::internal::pconj(const Packet&) [with Packet = Eigen::internal::Packet8cf]':\r\n/opt/gcc/7.3.0/snos/lib/gcc/x86_64-suse-linux/7.3.0/include/avx512dqintrin.h:435:1: error: inlining failed in call to always_inline '__m512 _mm512_xor_ps(__m512, __m512)': target specific option mismatch\r\n _mm512_xor_ps (__m512 __A, __m512 __B)\r\n ^~~~~~~~~~~~~\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --verbose_failures --config=mkl -c opt --copt=-g --strip=never --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er --copt='-mtune=knl' --copt=\"-DEIGEN_USE_VML\" //tensorflow/tools/pip_package:build_pip_package\r\n", "comments": ["@zhenghh04 The issue could be due to Bazel 0.21.0 as another [user](https://github.com/tensorflow/tensorflow/issues/24549) mentioned. His got it solved by following [this](https://docs.bazel.build/versions/0.21.0/install-compile-source.html#bootstrap-windows-bootstrap). Check [this](https://stackoverflow.com/questions/53975108/bazel-installation-error-when-attempting-to-install-tensorflow-from-source) also. Please let me know how it progress? Thanks!", "@zhenghh04 Could you try the a [solution](https://github.com/tensorflow/tensorflow/issues/24905) that worked for another user? Thanks! ", "> @zhenghh04 Could you try the a [solution](https://github.com/tensorflow/tensorflow/issues/24905) that worked for another user? Thanks!\r\n\r\n@jvishnuvardhan The Bazel 0.21.0 works fine. I did follows the similar instruction for Linux. I also tried 0.19.1, 0.20.0. I got the same errors.  The error is related to AVX512 (Intel KNL). Looks like not from Bazel. ", "This looks like an issue that was fixed in Eigen a few weeks ago.\r\n\r\nhttps://bitbucket.org/eigen/eigen/commits/49b5bfcfba835474762c424e53328cd1123243e2\r\n\r\nNote the build error doesn't happen if you compile with avx512dq.  So you could try adding --copt=-mavx512dq assuming your build machine supports this, or manually patching Eigen in your build.\r\n\r\nNote to get a working Eigen based AVX512 build of TF 1.13 you will also need this patch\r\n\r\nhttps://bitbucket.org/eigen/eigen/commits/9a28171b87b280b9ef77502a33b77fc7f4532dcf\r\n\r\n", "> This looks like an issue that was fixed in Eigen a few weeks ago.\r\n> \r\n> https://bitbucket.org/eigen/eigen/commits/49b5bfcfba835474762c424e53328cd1123243e2\r\n> \r\n> Note the build error doesn't happen if you compile with avx512dq. So you could try adding --copt=-mavx512dq assuming your build machine supports this, or manually patching Eigen in your build.\r\n> \r\n> Note to get a working Eigen based AVX512 build of TF 1.13 you will also need this patch\r\n> \r\n> https://bitbucket.org/eigen/eigen/commits/9a28171b87b280b9ef77502a33b77fc7f4532dcf\r\n\r\nThanks Mark, this sounds like a possible solution. I'll try. An naive question, how to I apply the patch to the TensorFlow package? I could not find the Eigen source code in the TensorFlow source directory. It looks like it is managed by Bazel. \r\n\r\n", "@markdryan By the way, I am building it on Knights Landing (Xeon Phi), it does not support avx512dq. ", "@rmlarsen should we bump the Eigen version?", "Actually, now I think about it, it's a bit more complicated that I made it sound as the patches need to be back ported to the version of Eigen used by TF 1.13.  Luckily, I've already created a fork of the version of Eigen that TF 1.13 uses and have back ported the patches to this fork.\r\n\r\nhttps://github.com/markdryan/eigen-git-mirror/tree/tf1.13\r\n\r\nTo get things to build you could clone my fork, switch to the tf1.13 branch and then copy the files modified by the top two commits to your TF source tree, under bazel-tensorflow/external/eigen-archive and then rebuild.  So assuming the TensorFlow source is in ~/src/tensorflow and my eigen fork is in ~/src/eigen you would do\r\n\r\ncp ~/src/eigen/Eigen/src/Core/arch/AVX512/Complex.h ~/src/tensorflow/bazel-tensorflow/external/eigen-archive/Eigen/src/Core/arch/AVX512/Complex.h\r\ncp ~/src/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h  ~/src/tensorflow/bazel-tensorflow/external/eigen-archive/Eigen/src/Core/arch/AVX512/PacketMath.h\r\n\r\nNote I've only tested on SkylakeX and not KNL.", "@martinwicke, @rmlarsen Updating Eigen would help with these issues but would also introduce at least one more AVX512 regression that is not currently present in the 1.13 branch.  The commit\r\n\r\nhttps://bitbucket.org/eigen/eigen/commits/b500fef42ced6cfb1fce9bdf43f8d56054619daa\r\n\r\ncauses the following unit test to fail on Eigen/AVX512 builds.\r\n\r\n//tensorflow/python/kernel_tests:init_ops_test\r\n\r\nI haven't quite figured out why yet but it looks like its something to do with the parallel tensor contraction code in Eigen.\r\n", "A quick summary of my understanding of the status of the non MKL AVX512 builds based on testing I did on the 14th of Jan.\r\n\r\nOn TF 1.13\r\n\r\n- There's a build failure when building without avx512dq ( patch here https://bitbucket.org/eigen/eigen/commits/49b5bfcfba835474762c424e53328cd1123243e2 )\r\n- There's a memory corruption error that leads to about 9 unit test cases failures ( patch here https://bitbucket.org/eigen/eigen/commits/9a28171b87b280b9ef77502a33b77fc7f4532dcf )\r\n- The embedding ops unit tests fail but this is a problem with the tests themselves (patch here https://github.com/tensorflow/tensorflow/pull/21677)\r\n\r\nOn master as of Monday\r\n\r\n- There's a memory corruption error that leads to about 9 unit test cases failures ( patch here https://bitbucket.org/eigen/eigen/commits/9a28171b87b280b9ef77502a33b77fc7f4532dcf )\r\n- The embedding ops unit tests fail but this is a problem with the tests themselves (patch here https://github.com/tensorflow/tensorflow/pull/21677)\r\n- The unit test //tensorflow/python/kernel_tests:init_ops_test fails (no patch available yet, but the commit that causes the regression seems to be https://bitbucket.org/eigen/eigen/commits/b500fef42ced6cfb1fce9bdf43f8d56054619daa )\r\n\r\nI haven't tested the latest version of Eigen against master.  The latest version I have tested is https://bitbucket.org/eigen/eigen/commits/ad3bcd81cc4973a193a14c20fbbd2f2b24aaa125, which is the version master is currently using.\r\n\r\nThe safest way of fixing the AVX512 TF 1.13 builds might be to stick with the version of Eigen that is currently used, but to patch it with the two AVX512 fixes.  Would that be possible?\r\n", "> Actually, now I think about it, it's a bit more complicated that I made it sound as the patches need to be back ported to the version of Eigen used by TF 1.13. Luckily, I've already created a fork of the version of Eigen that TF 1.13 uses and have back ported the patches to this fork.\r\n> \r\n> https://github.com/markdryan/eigen-git-mirror/tree/tf1.13\r\n> \r\n> To get things to build you could clone my fork, switch to the tf1.13 branch and then copy the files modified by the top two commits to your TF source tree, under bazel-tensorflow/external/eigen-archive and then rebuild. So assuming the TensorFlow source is in ~/src/tensorflow and my eigen fork is in ~/src/eigen you would do\r\n> \r\n> cp ~/src/eigen/Eigen/src/Core/arch/AVX512/Complex.h ~/src/tensorflow/bazel-tensorflow/external/eigen-archive/Eigen/src/Core/arch/AVX512/Complex.h\r\n> cp ~/src/eigen/Eigen/src/Core/arch/AVX512/PacketMath.h ~/src/tensorflow/bazel-tensorflow/external/eigen-archive/Eigen/src/Core/arch/AVX512/PacketMath.h\r\n> \r\n> Note I've only tested on SkylakeX and not KNL.\r\n@markdryan I quickly tested it. Now the errors are: \r\n\r\n* external/eigen_archive/Eigen/src/Core/arch/AVX512/Complex.h:133:39: error: 'extract256' was not declared in this scope\r\n      return predux_mul(pmul(Packet4cf(extract256<0>(a.v)),\r\n\r\n* external/eigen_archive/Eigen/src/Core/arch/AVX512/MathFunctions.h:380:23:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:266:60: error: no type named 'integer_packet' in 'struct Eigen::internal::unpacket_traits<__vector(16) float>'\r\n   typedef typename unpacket_traits<Packet>::integer_packet PacketI;\r\n                                                            ^~~~~~~\r\n* external/eigen_archive/Eigen/src/Core/arch/AVX512/MathFunctions.h:386:23:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:266:60: error: no type named 'integer_packet' in 'struct Eigen::internal::unpacket_traits<__vector(16) float>'", "@zhenghh04 Did you copy the PacketMath.h file as well?  That's where the extract256 function is defined.", "> @zhenghh04 Did you copy the PacketMath.h file as well? That's where the extract256 function is defined.\r\n\r\nThat's right, I forgot to copy the PacketMath.h under tf1.13 branch. Now it passed the Eigen build. This patch seems to work with KNL. I am dealing with other issues. I'll let you know if it passes all the process. Thanks! ", "Hello, this is to confirmed that the solution provided by @markdryan works. I was able to build TF 1.13.0rc0 on KNL. Thanks to all the help! \r\n"]}, {"number": 24904, "title": "Add dense option to `categorical_column_with_vocabulary_list` Feautre Column", "body": "Right now categorical_column_with_vocabulary_list only provides SparseTensor. It's hard for people to use it with tfrecords and Canned estimator. labels have to be Tensor and no one is converting the sparse string tensor into string tensor anywhere", "comments": ["@rohan100jain  Could you PTAL and approve.", "@yupbank - Hi, could you please look into these build failures? Thanks!", "@yupbank Did you get a chance to look on build failures? Please let us know on the update. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]