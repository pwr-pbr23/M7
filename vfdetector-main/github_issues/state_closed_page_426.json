[{"number": 41101, "title": "MultiWorkerMirroredStrategy: Multi-workers don't train once synced", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n**No. I used Keras tutorial**: [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n**Chief:** Ubuntu 20.04\r\n**Worker**: Ubuntu 20.04\r\n- TensorFlow installed from (source or binary):**From a Docker image:  tensorflow/tensorflow:latest-gpu**\r\n- TensorFlow version (use command below): **v2.2.0-rc4-8-g2b96f3662b 2.2.0**\r\n- Python version:  **3.6.9**\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: **V10.1.243**\r\n- GPU model and memory:\r\n**Chief: Titan RTX 24 Go\r\nWorker: 4x GTX 1060 6Go**\r\n- You can collect some of this information using our environment capture: [tf_env.txt](https://github.com/tensorflow/tensorflow/files/4874929/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\nI'm running the multi_worker_with_keras tutorial on 2 computers: 1 chief and 1 worker, both equipped with different GPUs. \r\nI run the tutorial into a tensorflow-gpu docker image on each computer. Each computer has been configured to communicate with an ssh key, without a password needed.\r\n\r\n1. The single_worker_model part of the tutorial works fine on both computers: each computer go through the epochs individually\r\n2. Regarding the MultiWorkerMirroredStrategy part, If I omit to set TF_CONFIG as an environment variable, both nodes train correctly on their side. Note: On the worker, the strategy distributes the work across the 4 local GPUs as expected.\r\n\r\n3. When I run the final script - TF_CONFIG + MultiWorkerMirroredStrategy - by starting the chief first, then the worker, the chief wait for the worker, then they sync. They finally both start epoch 1 and hold.\r\n\r\n**Describe the expected behavior**\r\n1. Have both workers to train after logging Epoch 1/60\r\n2. Have both workers to sync\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n**On the chief:**\r\n```\r\ndocker run --gpus all -it -p 12345:12345 --rm tensorflow/tensorflow:latest-gpu\r\nexport TF_CONFIG='{\"cluster\": {\"worker\": [\"192.168.1.31:12345\", \"192.168.1.46:12345\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}'\r\n[copy the script to the docker container]\r\npython multi_worker_with_keras.py\r\n```\r\n\r\n**On the worker:**\r\n```\r\ndocker run --gpus all -it -p 12345:12345 --rm tensorflow/tensorflow:latest-gpu\r\nexport TF_CONFIG='{\"cluster\": {\"worker\": [\"192.168.1.31:12345\", \"192.168.1.46:12345\"]}, \"task\": {\"index\": 1, \"type\": \"worker\"}}'\r\n[copy the script to the docker container]\r\npython multi_worker_with_keras.py\r\n```\r\nNote: Only the index changes\r\n\r\n**The script:**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mnist_dataset(batch_size):\r\n  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n  x_train = x_train / np.float32(255)\r\n  y_train = y_train.astype(np.int64)\r\n  train_dataset = tf.data.Dataset.from_tensor_slices(\r\n      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\r\n  return train_dataset\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.Input(shape=(28, 28)),\r\n      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\r\n      tf.keras.layers.Flatten(),\r\n      tf.keras.layers.Dense(128, activation='relu'),\r\n      tf.keras.layers.Dense(10)\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nper_worker_batch_size = 512\r\nnum_workers = 2\r\n\r\nglobal_batch_size = per_worker_batch_size * num_workers\r\nmulti_worker_dataset = mnist_dataset(global_batch_size)\r\n\r\nwith strategy.scope():\r\n  multi_worker_model = build_and_compile_cnn_model()\r\n\r\nmulti_worker_model.fit(multi_worker_dataset, epochs=60, steps_per_epoch=60)\r\n\r\n```\r\n**Note**: I have raised the number of epochs, steps_per_epoch, and per_worker_batch_size. I have changed the number of num_workers from 4 to 2. \r\n**Note**: I haven't done the \"ModelCheckpoint callback\" part as I wanted to validate the training sync first.\r\n**Note**: I have tested with auto sharding policy OFF (code from the tutorial), and I don't get the \"Found an unshardable source dataset\" error. But the workers don't train either.\r\n```\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\r\ndataset_no_auto_shard = multi_worker_dataset.with_options(options)\r\n```\r\n\r\n\r\n**Other info / logs** \r\n**On the chief:**\r\n[chief.log](https://github.com/tensorflow/tensorflow/files/4874928/chief.log)\r\n```\r\n2020-07-05 12:52:14.867922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-05 12:52:14.867929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-05 12:52:14.867935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-05 12:52:14.867941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-05 12:52:14.867948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-05 12:52:14.867954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-05 12:52:14.867960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-05 12:52:14.868007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-05 12:52:14.868671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-05 12:52:14.869292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-07-05 12:52:14.869303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-05 12:52:14.869307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-07-05 12:52:14.869311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-07-05 12:52:14.869374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-05 12:52:14.870033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-05 12:52:14.870662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 21960 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)\r\n2020-07-05 12:52:14.873278: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> 192.168.1.46:12345}\r\n2020-07-05 12:52:14.873849: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-07-05 12:52:22.818158: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:434] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 28\r\n        }\r\n      }\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nEpoch 1/60\r\n2020-07-05 12:52:25.479341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-05 12:52:25.731883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n\r\n```\r\n\r\n\r\n**On the worker:**\r\n[worker.log](https://github.com/tensorflow/tensorflow/files/4874930/worker.log)\r\n```\r\n2020-07-05 12:52:20.891307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-05 12:52:20.891332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-05 12:52:20.891358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-05 12:52:20.891383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-05 12:52:20.891407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-05 12:52:20.891431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-05 12:52:20.891455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-05 12:52:20.898699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1, 2, 3\r\n2020-07-05 12:52:20.898900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-05 12:52:20.898916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 2 3 \r\n2020-07-05 12:52:20.898928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y Y Y \r\n2020-07-05 12:52:20.898938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N Y Y \r\n2020-07-05 12:52:20.898948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 2:   Y Y N Y \r\n2020-07-05 12:52:20.898958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 3:   Y Y Y N \r\n2020-07-05 12:52:20.904332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 5644 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:19:00.0, compute capability: 6.1)\r\n2020-07-05 12:52:20.905569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:1 with 5644 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:1a:00.0, compute capability: 6.1)\r\n2020-07-05 12:52:20.906769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:2 with 5644 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1060 6GB, pci bus id: 0000:67:00.0, compute capability: 6.1)\r\n2020-07-05 12:52:20.907967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:3 with 5631 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1060 6GB, pci bus id: 0000:68:00.0, compute capability: 6.1)\r\n2020-07-05 12:52:20.915563: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 192.168.1.31:12345, 1 -> localhost:12345}\r\n2020-07-05 12:52:20.917163: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:12345\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2020-07-05 12:52:22.833446: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:434] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\r\nop: \"TensorSliceDataset\"\r\ninput: \"Placeholder/_0\"\r\ninput: \"Placeholder/_1\"\r\nattr {\r\n  key: \"Toutput_types\"\r\n  value {\r\n    list {\r\n      type: DT_FLOAT\r\n      type: DT_INT64\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"output_shapes\"\r\n  value {\r\n    list {\r\n      shape {\r\n        dim {\r\n          size: 28\r\n        }\r\n        dim {\r\n          size: 28\r\n        }\r\n      }\r\n      shape {\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nEpoch 1/60\r\n2020-07-05 12:52:25.979992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-05 12:52:26.242036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n```", "comments": ["Hi @OdysseeT,  to clarify you are trying to run multiworker training with two machines, following the tutorial in the docs. When you run your script with `TF CONFIG` set on each machine, the program hangs. Is that correct?\r\n\r\nI don't know what the exact issue is here, but as a first step can you take a look at these similar issues and let me know if that helps?  [#37108](https://github.com/tensorflow/tensorflow/issues/37108) [#35878](https://github.com/tensorflow/tensorflow/issues/35878) [#36846](https://github.com/tensorflow/tensorflow/issues/36846)\r\n\r\nAdditionally, can you run on tf nightly and let me know if the issue persists?", "Hi @nikitamaia, thanks for your answer. Indeed, it looks similar to #35878. I would require a couple of days to set up the environment again.\r\nIs there a way to add more verbosity to the logs?", "Adding this for reference:\r\n`export GRPC_VERBOSITY=DEBUG`\r\nIt provides more log.\r\n\r\nI tried to reproduce the issue #35878 with working code on two aws ec2 instances without any success.\r\n[tf-distributed-ec2.log](https://github.com/tensorflow/tensorflow/files/5002142/tf-distributed-ec2.log)\r\nThe two instances are syncing, they can connect without any password through SSH using a public key.\r\n\r\n\r\nWithout any way to print more logs, I can't provide more input", "I think I'm a little confused now. Can you clarify what you mean when you say you tried to reproduce the issue #35878? You ran the code from that issue and worked or didn't work? Also what code did you run?", "I ran two codes separately:\r\n- My code from the current issue. Taken from the online Keras tutorial mentioned above.\r\n- The working code of the issue #35878 (from [this comment](https://github.com/tensorflow/tensorflow/issues/35878#issuecomment-587979355))\r\n\r\nI couldn't get any of them working and they hang at the same step.", "Oh gosh... got it working \ud83d\ude04 it was painful \ud83e\udd15 \r\nHere are a couple of environment variables to set which could help to debug:\r\n```\r\nGRPC_TRACE=all\r\nGRPC_VERBOSITY=DEBUG\r\nGRPC_GO_LOG_SEVERITY_LEVEL=info\r\nGRPC_GO_LOG_VERBOSITY_LEVEL=2\r\nCGO_ENABLED=1\r\nNCCL_DEBUG=INFO\r\n```\r\nAt least there are some logs displayed on both machines\r\n\r\n```\r\n2020-07-31 12:10:26.242083: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://172.31.15.58:12345\r\nip-172-31-15-58:6249:6375 [0] NCCL INFO Bootstrap : Using [0]ens5:172.31.15.58<0>\r\nip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nip-172-31-15-58:6249:6375 [0] NCCL INFO NET/IB : No device found.\r\nip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.15.58<0>\r\nip-172-31-15-58:6249:6375 [0] NCCL INFO Using network Socket\r\nD0731 12:10:33.011138327    6360 dns_resolver.cc:261]        Start resolving.\r\nI0731 12:10:33.011426331    6293 subchannel.cc:1055]         New connected subchannel at 0x7fe070010690 for subchannel 0x7fe084018f00\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nEpoch 1/235\r\nWARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Iterator.get_next_as_optional()` instead.\r\n2020-07-31 12:10:36.476241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2020-07-31 12:10:36.688205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n235/235 [==============================] - 8s 33ms/step - loss: 2.2482 - accuracy: 0.3359\r\nEpoch 2/235\r\n235/235 [==============================] - 8s 33ms/step - loss: 2.0450 - accuracy: 0.6375\r\n[...]\r\n```\r\nBut I have no idea if there is any sync happening between the epochs as nothing shows up...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41101\">No</a>\n", "> Oh gosh... got it working \ud83d\ude04 it was painful \ud83e\udd15 \n> Here are a couple of environment variables to set which could help to debug:\n> ```\n> GRPC_TRACE=all\n> GRPC_VERBOSITY=DEBUG\n> GRPC_GO_LOG_SEVERITY_LEVEL=info\n> GRPC_GO_LOG_VERBOSITY_LEVEL=2\n> CGO_ENABLED=1\n> NCCL_DEBUG=INFO\n> ```\n> At least there are some logs displayed on both machines\n> \n> ```\n> 2020-07-31 12:10:26.242083: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://172.31.15.58:12345\n> ip-172-31-15-58:6249:6375 [0] NCCL INFO Bootstrap : Using [0]ens5:172.31.15.58<0>\n> ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n> ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/IB : No device found.\n> ip-172-31-15-58:6249:6375 [0] NCCL INFO NET/Socket : Using [0]ens5:172.31.15.58<0>\n> ip-172-31-15-58:6249:6375 [0] NCCL INFO Using network Socket\n> D0731 12:10:33.011138327    6360 dns_resolver.cc:261]        Start resolving.\n> I0731 12:10:33.011426331    6293 subchannel.cc:1055]         New connected subchannel at 0x7fe070010690 for subchannel 0x7fe084018f00\n> WARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\n> WARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\n> Epoch 1/235\n> WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n> Instructions for updating:\n> Use `tf.data.Iterator.get_next_as_optional()` instead.\n> 2020-07-31 12:10:36.476241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n> 2020-07-31 12:10:36.688205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n> 235/235 [==============================] - 8s 33ms/step - loss: 2.2482 - accuracy: 0.3359\n> Epoch 2/235\n> 235/235 [==============================] - 8s 33ms/step - loss: 2.0450 - accuracy: 0.6375\n> [...]\n> ```\n> But I have no idea if there is any sync happening between the epochs as nothing shows up...\n\n@houseofai How did you manage to make it work?\nWhat have you changed/added in your code? ", "@diman82 As far as I remember, I haven't change my code. The environement variables mentionned above helped to understand the different steps of synchronization. I mainly tried to switch the first to start: worker or chief. I gueess there is also a \"moment\" when to start the right one. But this is still obscure to me."]}, {"number": 41100, "title": "Broken link in www.tensorflow.org/resources/learn-ml", "body": "https://www.tensorflow.org/resources/learn-ml\r\n-> The four areas of machine learning education\r\n-> Build your own projects\r\n-> [colab]\r\n-> https://colab.research.google.com/github/tensorflow/docs/blob/r2.0rc/site/en/r2/tutorials/keras/basic_classification.ipynb\r\n\r\nGives me \"Notebook not found\"\r\nFetch for https://api.github.com/repos/tensorflow/docs/contents/site/en/r2/tutorials/keras?per_page=100&ref=r2.0rc failed: {\r\n  \"message\": \"No commit found for the ref r2.0rc\",\r\n  \"documentation_url\": \"https://developer.github.com/v3/repos/contents/\"\r\n}\r\n", "comments": ["Thanks for reporting, we've moved this to the internal bug tracker. I'm closing this since there's no loop back."]}, {"number": 41099, "title": "Installation error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6 / 3.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): - \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: NVIDIA GeForce GTX 960 6,0 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile having installed the Tensorflow package via Pycharm Keras does not work. It does not want to import the Sequential Model from Keras. We want to run a unit test on a Keras-based neural network an it is not working. Here is my error message:\r\n\r\nError\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\unittest\\case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\unittest\\case.py\", line 601, in run\r\n    testMethod()\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\unittest\\loader.py\", line 34, in testFailure\r\n    raise self._exception\r\nImportError: Failed to import test module: test_neural_network\r\nTraceback (most recent call last):\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\r\n    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Philip\\AppData\\Local\\Programs\\Python\\Python36\\lib\\unittest\\loader.py\", line 153, in loadTestsFromName\r\n    module = __import__(module_name)\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\hw08_neural_networks\\test_neural_network.py\", line 2, in <module>\r\n    from hw08_neural_networks import get_data, lstm, cnn\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\hw08_neural_networks\\lstm.py\", line 2, in <module>\r\n    from keras.models import Sequential\r\n  File \"D:\\Uni\\SS 20\\Computerlinguistische Anwendungen\\jophya\\src\\venv\\lib\\site-packages\\keras\\__init__.py\", line 6, in <module>\r\n    'Keras requires TensorFlow 2.2 or higher. '\r\nImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\r\n\r\n\r\n\r\nAssertion failed\r\n\r\nAssertion failed\r\n\r\n\r\nRan 1 test in 0.050s\r\n\r\nFAILED (errors=1)\r\n\r\nProcess finished with exit code 1\r\n\r\nAssertion failed\r\n\r\nAssertion failed\r\n\r\n\r\n\r\nHas anybody any advise?\r\nThank you in advance\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "Please close all sessions of Jupyter Notebook and relaunch. Try to import again as a first step and it should work fine.", "@adamczykp,\r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156) from a similar issue and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41099\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41099\">No</a>\n"]}, {"number": 41098, "title": "TFlite runtime wheel not supported", "body": "**System information**\r\n- Ubuntu 20.04 with linux kernel 5.7.1\r\n- Bare Python 3.8.2 and Conda Env with Python 3.7.6\r\n\r\n**The problem**\r\n \r\nWhen trying to install *tflite_runtime* I get the following error:\r\n```\r\n$ pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl\r\nERROR: tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.\r\n```\r\nI've tried a bare installation in Python 3.8.2 and in a conda environment with python 3.7.6. Same error for each try.", "comments": ["@albydnc \r\nCould you please check if you are using 64-bit version of Python?\r\nTensorFlow is tested and supported on 64-bit systems, for more information please check the [system requirements here](https://www.tensorflow.org/install/pip#system-requirements). Thanks!\r\n\r\n\r\nTf  2.1 does not support Python 3.8 please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/39768#issuecomment-632799481).\r\nPlease refer to these links:\r\n[link](https://github.com/tensorflow/tensorflow/issues/40890#issuecomment-651037848) [link](https://stackoverflow.com/questions/33622613/tensorflow-installation-error-not-a-supported-wheel-on-this-platform) [link1](https://github.com/Qengineering/TensorFlow-Raspberry-Pi/issues/1) [link2](https://github.com/tensorflow/tensorflow/issues/9722) [link3](https://www.tensorflow.org/lite/guide/python) ", "@Saduf2019 \r\nThanks for your answer. Both python are 64-bit. I've assumed TF 2.1 wasn't available for 3.8 as you reported.\r\nIn fact, I was mainly trying to install it on a conda env with Python 3.7.6 64-bit. From conda navigator I've installed and used TF without problems.\r\nThe issue comes with a new env where I just wanted the TFlite_runtime package and I get the error reported above.\r\n\r\n```\r\n(tensorflow-dev) albertoperro@alberto-XPS:~$ python3\r\nPython 3.7.7 (default, May  7 2020, 21:25:33) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>> print(tensorflow.__version__)\r\n2.1.0\r\n>>> quit()\r\n(tensorflow-dev) albertoperro@alberto-XPS:~$ pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl\r\nERROR: tflite_runtime-2.1.0.post1-cp37-cp37m-linux_x86_64.whl is not a supported wheel on this platform.\r\n```", "Can you try upgrading your pip to latest version and installing again?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41098\">No</a>\n"]}, {"number": 41097, "title": "Unexpected crash when loading a modified saved_model.pb file", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.0.2-0-g2c2fdd3205 2.0.2\r\n\r\n- Python version:\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): clang++ 10.0\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI used the LibFuzzer to mutate an intact saved_model.pb file and uses `LoadSavedModel` C API to load it.\r\nInstead of returning a `Status` whose `.ok()` is `false`, the program directly crashes with following errors:\r\n\r\n```\r\n2020-06-20 15:29:05.816403: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/xxx/Playground/tensorflow/saved_model/crashes\r\n2020-06-20 15:29:05.817167: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-06-20 15:29:05.829727: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\r\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/map.h:1059] CHECK failed: it != end(): key not found: value\r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: it != end(): key not found: value\r\n[1]    26722 abort (core dumped)  ./loader_test\r\n```\r\n**Describe the expected behavior**\r\nSince `LoadSavedModel` returns an `Status` object, the `Status.ok()` should return `false`, rather than a direct crash.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```c++\r\n#include \"tensorflow/cc/saved_model/loader.h\"\r\n#include \"tensorflow/cc/saved_model/constants.h\"\r\n#include \"tensorflow/cc/saved_model/tag_constants.h\"\r\n\r\n#include <fstream>\r\nusing namespace tensorflow;\r\nint main(){\r\n        const string export_dir = \"/home/xxx/Playground/saved_model/intact\"; // original pb file --> Status.ok() == true\r\n        const string export_dir2 = \"/home/xxx/Playground/saved_model/test\"; // modified pb file --> Status.ok() == false\r\n        const string export_dir3 = \"/home/xxx/Playground/saved_model/crashes\"; // modified pb file --> crash\r\n        SavedModelBundle bundle;\r\n        SessionOptions session_options;\r\n        RunOptions run_options;\r\n\t    std::cout<<\"hello\"<<std::endl;\r\n        if(LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagServe}, &bundle).ok()){\r\n\t\tstd::cout<<\"Load Successful!\"<<std::endl;\r\n\t}\r\n\tstd::cout<<\"-------------------------\"<<std::endl;\r\n\tif(LoadSavedModel(session_options, run_options, export_dir2, {kSavedModelTagServe}, &bundle).ok()){\r\n\t\tstd::cout<<\"Load Successful!\"<<std::endl;\r\n\t}\r\n\tstd::cout<<\"-------------------------\"<<std::endl;\r\n\tif(LoadSavedModel(session_options, run_options, export_dir3, {kSavedModelTagServe}, &bundle).ok()){\r\n\t\tstd::cout<<\"Load Successful!\"<<std::endl;\r\n\t}\r\n\tstd::cout<<\"-------------------------\"<<std::endl;\r\n\r\n    }\r\n```\r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/4874697/saved_model.zip)\r\n[libfuzzer output.txt](https://github.com/tensorflow/tensorflow/files/4874698/libfuzzer.output.txt)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI attached three saved_model.pb files and the log of libfuzzer as well.\r\n\r\n**Personal understanding**\r\nI think the following codes in `tensorflow/core/grappler/costs/graph_properties.cc` lead to the error\r\n```c++\r\n  Status MaybeUpdateNodeContextOutput(const NodeDef& node, const bool is_fed,\r\n                                      NodeContext* c) {\r\n    // Propagate tensors and shape tensors unless the node is fed.\r\n    // TODO(bsteiner) We should still propagate the shapes to the ports that\r\n    // aren't fed in the case of a ShapeN node.\r\n\r\n    InferenceContext* ic = c->inference_context.get();\r\n    if (!is_fed) {\r\n      if (IsConstant(node)) {\r\n        c->output_tensor_protos.resize(1);\r\n        const TensorProto& tensor_proto = node.attr().at(\"value\").tensor(); // at(\"xx\") if \"xx\" does not exist will bring a crash\r\n```\r\nOnce the `value` attribute cannot be obtained, an exception will be thrown.", "comments": ["@DongShuaike \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\n", "> @DongShuaike\r\n> \r\n> Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\nHi, this is an issue around tensorflow C API `LoadSavedModel`, I have attached the test code below.\r\n\r\n[loader_test.zip](https://github.com/tensorflow/tensorflow/files/4877295/loader_test.zip) The assets files are uploaded in the previous post. Thank you.\r\n", "Hi.\r\n\r\nThank you for the report. We are working on fixing this. Could not reply on the email you sent to security list as your email address was not included in the headers.", "> Hi.\r\n> \r\n> Thank you for the report. We are working on fixing this. Could not reply on the email you sent to security list as your email address was not included in the headers.\r\n\r\nThank you for your response, I will change the email settings for the future communication.", "Hello,\r\n\r\nCan you reply back to the mailing list with the preferred format to use in attribution?", "> Hello,\r\n> \r\n> Can you reply back to the mailing list with the preferred format to use in attribution?\r\n\r\nHi, I have sent another email to the mailing list, thank you very much!", "Hello, have any patches released for this bug?", "Hi.\r\n\r\nUnfortunately, not yet. We encountered several delays due to other vulnerabilities needing to change a large part of the codebase. Since the patch release process takes a long time it's not worth issuing two separate patches in just a few months.\r\n\r\nApologies for the delays. I am working on this and looking for a release before end of September.", "> Hi.\r\n> \r\n> Unfortunately, not yet. We encountered several delays due to other vulnerabilities needing to change a large part of the codebase. Since the patch release process takes a long time it's not worth issuing two separate patches in just a few months.\r\n> \r\n> Apologies for the delays. I am working on this and looking for a release before end of September.\r\n\r\nThank you for the quick response. Will be patches released as single files or as a newer version of TF? Can I find them in `release` tags?", "Sorry for the delay.\r\n\r\nThe patches will be released on master as well as being cherrypicked on old release branches. Then, we will release patch versions of old releases.\r\n\r\nFor both the changes in master and the patch releases, there will be updates to the release notes.\r\n\r\nThere also be security advisories published, as well as CVE numbers assigned.", "I'm requesting CVE numbers today and will publish advisories and patch releases in a few days", "> I'm requesting CVE numbers today and will publish advisories and patch releases in a few days\r\n\r\nThank you for letting me know that!", "Fix is now in master (adf095206f25471e864a8e63a0f1caef53a0e3a6) and cherrypicked on all branches. Keeping this open until all patch releases are released.", "We have released patches for all version. There is [a security advisory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-010.md) and CVE-2020-15206 for this", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41097\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41097\">No</a>\n", "> We have released patches for all version. There is [a security advisory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/advisory/tfsa-2020-010.md) and CVE-2020-15206 for this\r\n\r\nHi, really thank you for the efforts in fixing this, I'm reading the patches to understand better."]}, {"number": 41096, "title": "Update \"hello world\" example notebook to TF v2.2.0", "body": "I started these changes because the notebook for training and creating the model fails to run in Colab out-of-the-box, as gast dependency is not compatible with tensorflow-probability. This is the error observed when running the cell with `! pip install -q tensorflow==2`\r\n\r\n> ERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\r\n\r\nSimply by upgrading to TF v2.2.0 everything works again without any other workarounds", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/41096\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "Colab, would have by default have as of now version 2.2.0 \r\n\r\nSo might be helpful to skip pip install and just validate current version of TF", "Somehow I can not see how to get quantisation back working, even with proper \r\n```python\r\nconverter.target_spec = tf.lite.TargetSpec(supported_types=[tf.int8])\r\n```\r\n\r\n@sayakpaul do you have any idea? :)", "@lc0 could you leave a link to the notebook? ", "> @lc0 could you leave a link to the notebook?\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb", "@lc0 I tried with the following:\r\n\r\n```python\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n```\r\n\r\nBut weirdly, it resulted in:\r\n\r\n```\r\nModel is 2540 bytes\r\nQuantized model is 2560 bytes\r\nDifference is -20 bytes\r\n```\r\n\r\n@khanhlvg could you shed some light? ", "I haven't fully investigated the model but here is my assumption:\r\n* The model itself is very small with just a handful number of weights. So using quantization won't save you much space. \r\n* However, quantization adds some overhead to the model size (e.g. a map between quantized values and float value) which result in the small difference that you see.", "@khanhlvg @sayakpaul That's also what I tried and got the same result.\r\n\r\nBtw, it looks like the previous version was able to reduce file size. Did we change how much overhead it requires? \r\nAlso, I tested with saved model vs keras. And it seems like saved models generates bigger TFLite file, than the same Keras model.\r\n\r\nThanks for your explanations! ", "@carloshvp Can you please resolve conflicts? Thanks!", "@carloshvp Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 41095, "title": "categorical_crossentropy with label smoothing support on tf.bfloat", "body": "It seems that when enabling label smoothing, categorical_crossentropy only supports inputs in float32, so not useful when using tf.bfloat on TPU. I changed it to the following to make it work:\r\n```\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.framework import smart_cond\r\nfrom tensorflow.python.keras import backend as K\r\nfrom tensorflow.python.ops import array_ops\r\n\r\ndef categorical_crossentropy(y_true,\r\n                             y_pred,\r\n                             from_logits=False,\r\n                             label_smoothing=0):\r\n\r\n  y_pred = ops.convert_to_tensor_v2(y_pred)\r\n  y_true = math_ops.cast(y_true, y_pred.dtype)\r\n  label_smoothing = ops.convert_to_tensor_v2(label_smoothing, dtype=tf.bfloat16)\r\n\r\n  def _smooth_labels():\r\n    num_classes = math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)\r\n    return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\r\n\r\n  y_true = smart_cond.smart_cond(label_smoothing,\r\n                                 _smooth_labels, lambda: y_true)\r\n  return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41095\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41095\">No</a>\n"]}, {"number": 41094, "title": "TFLite inference is slower than benchmark reported", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 4\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhile I benchmark the tflite model using the benchmark tool, latency is only 6ms. However, when I actually run this inside an apk, the latency increases to 11ms. I'm wondering if it's due to the usage of buffer copy? Do they actually take that much time (~4ms)? Could you please suggest an alternative option that would be faster?\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nvoid mask_speech(const float* input, float* mask) {\r\n    TfLiteTensor* tflite_input = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n    TfLiteTensorCopyFromBuffer(tflite_input, input, NUM_INPUT * sizeof(float));\r\n    TfLiteInterpreterInvoke(interpreter);\r\n    const TfLiteTensor* tflite_output = TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\n    TfLiteTensorCopyToBuffer(tflite_output, mask, NUM_OUTPUT * sizeof(float));\r\n}\r\n```\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi,\r\nThe benchmark tool actually does not copy buffer while measuring the inference time repeatedly. So it might be possible.\r\nCould you measure the time for copying buffer in your app to make it clear?\r\nAlso is there any heavy job on other thread while running your app?", "@andreydung \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I'm facing a similar issue. The timings from the Benchmarking tool don't match that of the one's seen on Android Studio. The discrepence is large, almost double infact. \r\n ", "@andreydung were you able to resolve this?"]}, {"number": 41093, "title": "tf.keras.layers.DepthwiseConv2D and tf.keras.layers.SeparableConv2D does not handle kernel regularizer correctly.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.3\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: NA\r\n-   **TensorFlow installed from (source or binary)**: binary, from anaconda\r\n-   **TensorFlow version (use command below)**: 2.2.0\r\n-   **Python version**: 3.7.7\r\n-   **Bazel version (if compiling from source)**: NA\r\n-   **GCC/Compiler version (if compiling from source)**: NA\r\n-   **CUDA/cuDNN version**: 10.1 / 7.6.5\r\n-   **GPU model and memory**: NVIDIA Tesla V100 / 32GB\r\n-   **Exact command to reproduce**: Source code below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nkernel_regularizer parameter on tf.keras.layers.DepthwiseConv2D and  tf.keras.layers.SeparableConv2D does not register the loss to model losses, despite of bias_regularizer works. I'll describe full test code below.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nExpected case 1: Dense layer\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Dense(3, input_dim=5, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.Dense(3, input_dim=5, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.19434488>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.22874394>]\r\n```\r\n\r\nExpected case 2: Conv2D layer\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(32, 32, 3))\r\nm.add(tf.keras.layers.Conv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.Conv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.29766324>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.7301255>]\r\n```\r\n\r\nExpected case 3: ConvLSTM2D layer\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(3, 32, 32, 3))\r\nm.add(tf.keras.layers.ConvLSTM2D(3, 3, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.ConvLSTM2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.47271797>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.3>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.49562755>]\r\n```\r\n\r\n\r\n\r\nDoes not expected case 1: DepthwiseConv2D layer\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(32, 32, 3))\r\nm.add(tf.keras.layers.DepthwiseConv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.DepthwiseConv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\n```\r\n\r\nDoes not expected case 2: SeparableConv2D layer\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(32, 32, 3))\r\nm.add(tf.keras.layers.SeparableConv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.SeparableConv2D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\n```\r\n\r\nAm I have a mistake using regularizer with these layers?", "comments": ["SeparableConv1D also has same issue.\r\n\r\nCase Conv1D:\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(32, 3))\r\nm.add(tf.keras.layers.Conv1D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.Conv1D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\n\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.30436406>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\r\n <tf.Tensor: shape=(), dtype=float32, numpy=0.31131223>]\r\n```\r\n\r\nCase SeparableConv1D:\r\n```python\r\nm = tf.keras.models.Sequential()\r\nm.add(tf.keras.layers.Input(32, 3))\r\nm.add(tf.keras.layers.SeparableConv1D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01), bias_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nm.add(tf.keras.layers.SeparableConv1D(3, 3, kernel_regularizer=tf.keras.regularizers.l2(1.0E-01)))\r\nprint(m.losses)\r\n```\r\n\r\nOutput:\r\n```\r\n[<tf.Tensor: shape=(), dtype=float32, numpy=0.0>]\r\n```", "@jyhpsycho \r\n\r\nRequest you to share colab link or complete code snippet with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\n\r\nOh, I found that some typo on my examples(shape of input layer should be tuple, but not). I'll upload IPython notebook file and converted .py script for reproduce it.\r\n\r\n[41093.zip](https://github.com/tensorflow/tensorflow/files/4882572/41093.zip)\r\n\r\n\r\n\r\n", "I have tried in colab with TF versions 2.2, nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/13d016c1c285c1c3a92bfadc9efaa5fb/untitled92.ipynb).Thanks!", "Please pay attention that according to the docs there is no `kernel_regularizer` parameter in `DepthwiseConv2D`, but there is `depthwise_regularizer`: https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D\r\nThe problem would be then that while keras accepts the both, it does not apply `kernel_regularizer` without actually warning the user.", "@mkarpushin-upstride The document seems to be changed after v2.3 release. On v2.2 there's no `depthwise_regularizer`. The end-user cannot know what the option name should be used for specifying regularizer at that time, even if `depthwise_regularizer` works with v2.2.\r\nhttps://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/ConvLSTM2D\r\n\r\nBTW, if `depthwise_regularizer` works on 2.2 and further version, this issue can be closed.", "https://colab.research.google.com/gist/jyhpsycho/3fd5a2d8ff6071c013137a69af6acb18/untitled92.ipynb\r\n\r\nI checked the `depthwise_regularizer` option (based on @ravikyram's colab code) and it seems to work with TF 2.2. @mkarpushin-upstride Thank you for clarification.\r\n\r\nIf there's any problem with test code, feel free to reopen this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41093\">No</a>\n"]}, {"number": 41092, "title": "Unable to use tf-hub models locally", "body": "I've been working on a task using bert model [.](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2) As per the instructions I've added the line:\r\n\r\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\r\n\r\nBut problem comes when it downloads the model again on every run. That's annoying. I've already spent a lot of time on searching How to download and work with tf-hub models . But i got no proper answer. That's why I'm here for guidance from official tf team.\r\n\r\nI've downloaded the .tar.gz file from https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2 and extracted the file. It contains <assets, variable, saved_model.pb>.\r\n\r\nI've tried this\r\n`\r\nos.environ[\"TFHUB_CACHE_DIR\"] = r'C:\\Users\\devendra\\PycharmProjects\\NeuralMachineTranslation\\tf_hub'\r\n\r\nhandle = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\r\nhashlib.sha1(handle.encode(\"utf8\")).hexdigest()\r\nprint(hashlib.sha1(handle.encode(\"utf8\")).hexdigest())  #to rename directory\r\n\r\n\r\nembed = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\")\r\nprint('Loaded')\r\nprint(embed)\r\n`\r\nbut it is giving me error:\r\n`FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for C:\\Users\\devendra\\PycharmProjects\\NeuralMachineTranslation\\tf_hub/ce53fe6769d2ac3a260e92555120c54e1aecbea6/variables/variables\r\n If trying to load on a different device from the computational device, consider using setting the `experimental_io_device` option on tf.saved_model.LoadOptions to the io_device such as '/job:localhost'.\r\n`\r\n\r\nI've using tensorflow and tf-hub latest upto the version on Ubuntu 18.04\r\n\r\n Now please guide me how to use the model properly and efficiently. Please provide concerned information and script. Thanking You.", "comments": ["@devspartan,\r\nIssues related to TF-Hub are handled in the hub repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/tensorflow/hub/issues/new) and fill in the template, so that we can track the issue there. Thanks!", "I've already submitted a issue over there. But i wasn't getting reply for 3 days. But i did get and it solved my problem. Sorry for inconvenience."]}, {"number": 41091, "title": "Loaded runtime CuDNN library: 7.3.1 but source was compiled with: 7.5.0.", "body": "**I have checked everywhere else for a solution and this is my last resort**\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:no\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10 v2004\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:conda/pip (unsure bc installed with both b4)\r\n-   **TensorFlow version**: 1.13.2 gpu\r\n-   **Python version**: 3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: 10.0 + 7.5.0 , 9.2 + 7.5.0 and 9.0 7.5.0\r\n-   **GPU model and memory**: GTX 1060 6g\r\n-   **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nIt says i have cudnn 7.3.1 but I haven't / don't think I have installed it at any point and have only ever installed 7.5.0.\r\n\r\n### Source code / logs\r\n020-07-05 01:42:25.906817: E tensorflow/stream_executor/cuda/cuda_dnn.cc:324] Loaded runtime CuDNN library: 7.3.1 but source was compiled with: 7.5.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2020-07-05 01:42:25.916794: E tensorflow/stream_executor/cuda/cuda_dnn.cc:324] Loaded runtime CuDNN library: 7.3.1 but source was compiled with: 7.5.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n", "comments": ["@wesleyr36 \r\nCan you please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/23715#issuecomment-454206467) and let us know if it helps.\r\n[link](https://stackoverflow.com/questions/49960132/cudnn-library-compatibility-error-after-loading-model-weights)  [link1](https://askubuntu.com/questions/1086949/loaded-runtime-cudnn-library-7-0-5-but-source-was-compiled-with-7-2-1) ", "That didn't really help.", "@wesleyr36 Can you please check your CUDnn version as mentioned [here](https://stackoverflow.com/questions/45641087/on-windows-how-do-you-verify-the-version-number-of-cudnn-installed). Can you please verify it once again. Thanks!", "cuda 9.0 and 9.2\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 5\r\n#define CUDNN_PATCHLEVEL 0\r\ncuda 10.0\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 6\r\n#define CUDNN_PATCHLEVEL 5", "I found out the cause, apparently there was a version of cudnn in my main windows directory and sure enough it was 7.3.1, I deleted it and everything is working fine now, sorry for wasting you guys's time.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41091\">No</a>\n", "I too have the same issue:\r\nos: ubuntu 18.\r\nCUDA is 10.0\r\nCUDNN is 7.5\r\nTensorflow 1.15.2\r\nGPU : 1060Ti 6gb\r\n\r\n\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 5\r\n#define CUDNN_PATCHLEVEL 0\r\n\r\n\r\n\r\n Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\ncan anyone help me plz.\r\nThank you!", "upgrade your cudnn to 7.6.0 download [here](https://developer.nvidia.com/rdp/cudnn-archive) u may need to make an nvidia dev account if you don't have one ", "os: ubuntu 18.\r\nCUDA is 10.0\r\nCUDNN is 7.5\r\nTensorflow 1.15.2\r\nGPU : 1060Ti 6gb\r\npython : 3.5\r\n\r\n#define CUDNN_MAJOR 7\r\n#define CUDNN_MINOR 5\r\n#define CUDNN_PATCHLEVEL 0\r\n\r\nLoaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0. CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library. If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n\r\n\r\n\r\n                   \r\nSolution 1: ( I don't have root-access) so I have downgraded the TensorFlow version from 1.15.2 to 1.14. It worked for me.\r\nSolution 2: As per @wesleyr36 said we can upgrade the cudnn version in my case it is 7.6\r\n                   refer this link [https://developer.nvidia.com/rdp/cudnn-archive](url) for compactable versions\r\n", "I'm on CUDNN 7.5/Python 3.6 and downgrading to 1.14 like @ashiqak worked for me too !\r\n```sh\r\npip3 install tensorflow_gpu==1.14\r\n```"]}, {"number": 41090, "title": "Issue using segment_prod in custom keras layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): nightly\r\n- TensorFlow version (use command below): nightly\r\n- Python version: colab\r\n\r\n\r\nI'm trying to write a custom keras layer that uses `segment_prod` on model features, i.e., not the batch dimension. To do that, I've been using tf.transpose to put the feature dimensions first, and then transposing the resulting calculation. Forward calculation seems to work using this approach, but there appears to be an issue with gradients with `segment_prod`: the following code fails with `LookupError: gradient registry has no entry for: SegmentProd`:\r\n\r\n```python\r\nclass MyLayer1(layers.Layer):\r\n\r\n  def call(self, inputs):\r\n\r\n    segments = tf.constant([0, 0, 0, 1, 1])\r\n    return tf.transpose(\r\n        tf.math.segment_prod(tf.transpose(inputs), segments))\r\n\r\ninputs = layers.Input(10)\r\nembed = layers.Embedding(20, 5)(inputs)\r\noutput = MyLayer1()(embed)\r\noutput = layers.GlobalAveragePooling1D()(output)\r\nmodel = tf.keras.Model(inputs, output)\r\n\r\nX = np.random.randint(20, size=(100, 10))\r\ny = np.random.randn(100,2)\r\n\r\nmodel.compile(loss='mae')\r\nmodel.fit(X, y)\r\n```\r\n\r\n\r\nReplacing `tf.math.segment_prod` with other operations, like `tf.math.unsorted_segment_prod(..., num_segments=2)` or `tf.math.segment_sum` seems to work, however.\r\n\r\nColab gist:\r\nhttps://colab.research.google.com/gist/pstjohn/8fa2faf274679742ba628290a94c2b2b/untitled0.ipynb", "comments": ["I have tried in colab with TF versions 2.2,2.3-rc0,nightly versions(`2.4.0-dev20200705`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/95302a74087ba08432ece90213095b22/untitled83.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41090\">No</a>\n"]}, {"number": 41089, "title": "Invalid link in documentation to code: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb", "body": "TensorFlow documentation issue. \r\n\r\n## URL(s) with the issue:\r\nThe link/page not found(404); \"images -> object detection API\" under https://www.tensorflow.org/tutorials/\r\nnamely: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n## Description of issue (what needs changing):\r\nMaterial was moved to https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb", "comments": ["@jajas,\r\nLooks like this issue is a duplicate of [#8736](https://github.com/tensorflow/models/issues/8736)\r\n\r\nCan we close this issue since it is already being tracked in the models repo? Thanks!", "Thanks @amahendrakar \r\nBut https://www.tensorflow.org/tutorials/ should be fixed too, which is not mentioned at [#8736](https://github.com/tensorflow/models/issues/8736) ", "@jajas,\r\nSimilar issue has been raised [here](https://github.com/tensorflow/models/issues/8698). \r\n\r\nAlso, seems like the tutorials page has been updated. There is no `Object detection API` under `Images`. Instead it has been replaced with `Object detection with TF Hub`. \r\n\r\n\r\n![Screenshot 2020-07-12 at 10 06 55 PM](https://user-images.githubusercontent.com/57165142/87251846-4dec0380-c48c-11ea-989d-9f0cd044d217.png)\r\n\r\nPlease check the above image and feel free to close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41088, "title": "Update output_handler.cc", "body": "fix incorrectly documented led-color order", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41088) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41088) for more info**.\n\n<!-- ok -->", "No problem. Thought I would just report it and provide this small fix. I\nsee that another one was fixed already. If you start with tensortlow, it's\njust nice that your first example is completely correct. Kind regards\n\nOp zo 5 jul. 2020 09:32 schreef TommyWang <notifications@github.com>:\n\n> *@Dynmi* commented on this pull request.\n>\n> Thank you for the PR but I don't think this is worth the hours of CI it\n> will take to integrate it. There are ongoing PRs which fix typos across an\n> entire directory, and I think they already cover this too.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/41088#pullrequestreview-442653876>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJ7X3RLMBLPZG4A2SJNATLR2AUBHANCNFSM4OQSHHPQ>\n> .\n>\n"]}, {"number": 41087, "title": "Tensorflow Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\r\n- TensorFlow installed from (source or binary): Conda installation\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.10\r\n- CUDA version: 10.1.243\r\n- cuDNN version:  7.6.5 (windows 7)\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\n**Describe the current behavior**\r\nWhen I run my tensorflow keras model it will sometimes stop after an epoch and throw the following error:\r\n\r\n```\r\n    tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_EXECUTION_FAILED\r\n    in tensorflow/stream_executor/cuda/cuda_dnn.cc(1867): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n    2020-06-27 17:19:45.741256: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should run the model without any crashes.\r\n\r\n**Standalone code to reproduce the issue**\r\nIm using the GPU version of tensorflow and it has worked without problems up until about 3 days ago when it began giving me this error. I also have the following callbacks in my model so that it saves checkpoints and epochs locally. Please note I am using Autokeras thus why the nodes have the ak. library prefix. However it works just like keras, it just automatically tries to find the best set of hyperparameters when running the .fit function.\r\n\r\n```\r\ninput_node = ak.Input()\r\n    #output_node1 = ak.Normalization()(input_node)\r\n    output_node1 = ak.RNNBlock(return_sequences=True,\r\n                               layer_type='lstm')(input_node)\r\n    output_node2 = ak.RNNBlock(return_sequences=True,\r\n                               layer_type='lstm')(output_node1)\r\n    output_node3 = ak.RNNBlock(return_sequences=True,\r\n                               layer_type='lstm')(output_node2)\r\n    output_node4 = ak.RNNBlock(layer_type='lstm')(output_node3)\r\n    output_node5 = ak.DenseBlock()(output_node4)\r\n    output_node = ak.Merge()(\r\n        [output_node1, output_node2, output_node3, output_node4, output_node5])\r\n    output_nodefinal = ak.ClassificationHead()(output_node)\r\n    \r\n    my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_accuracy\", mode=\"auto\"),\r\n                    tf.keras.callbacks.ModelCheckpoint(filepath=\"D:\\AutoKerasProject\\TimeseriesCallbackModel\\TimeseriesCallbackModel\", save_weights_only=True, monitor='val_accuracy', mode='auto', save_best_only=True)]\r\n    \r\n    classifier = ak.AutoModel(\r\n        inputs=input_node, outputs=output_nodefinal, max_trials=500, directory=\"D:\\AutoKerasProject\\TimeseriesModel\", overwrite=True)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@sword134 \r\nPlease refer to the links below and let us know if it helps you resolve the issue.\r\n#40877 #35950 #31645 #25709 #35791 #35950 [link](https://stackoverflow.com/questions/43147983/could-not-create-cudnn-handle-cudnn-status-internal-error) .", "I encounter the same problem, the problem only appears when Autograph was used when runing,\r\n-----------------------------------\r\nConfigure: \r\nOS: Win10\r\nGPU: RTX 2080TI\r\ntensorflow: 2.1.0\r\nCUDA: v11.0.167\r\nCUDNN:  v8.0.1.13\r\n----------------------------------\r\nTwo ways may help:\r\n1.  Clear the template file at path: C:\\Users\\admin\\AppData\\Local\\Temp\r\n2.  Try simplify the network, \r\nI think the question is related to memory capacity,", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41087\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41087\">No</a>\n"]}, {"number": 41086, "title": "FAILED: Build did NOT complete successfully", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: just building for CPU\r\n- GPU model and memory: GTX 980\r\n\r\nI am following the instructions laid out in this tutorial (starting at Step 2) to convert my model for use by tflite:\r\nhttps://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi#part-1---how-to-train-convert-and-run-custom-tensorflow-lite-object-detection-models-on-windows-10\r\n\r\neverything seems to work find until I try and build using babel with this command:\r\n> bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nIt start off fine and runs for a while but then fails with this output:\r\n\r\nINFO: From Linking tensorflow/python/gen_lookup_ops_py_wrappers_cc.exe:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/gen_lookup_ops_py_wrappers_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/gen_lookup_ops_py_wrappers_cc.exp\r\nINFO: From Linking tensorflow/python/gen_script_ops_py_wrappers_cc.exe:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/gen_script_ops_py_wrappers_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/gen_script_ops_py_wrappers_cc.exp\r\nINFO: From Linking tensorflow/python/gen_bitwise_ops_py_wrappers_cc.exe:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/gen_bitwise_ops_py_wrappers_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/gen_bitwise_ops_py_wrappers_cc.exp\r\nINFO: From Linking tensorflow/python/gen_ragged_conversion_ops_py_wrappers_cc.exe:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/gen_ragged_conversion_ops_py_wrappers_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/gen_ragged_conversion_ops_py_wrappers_cc.exp\r\nINFO: From Linking tensorflow/python/gen_ragged_math_ops_py_wrappers_cc.exe:\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/python/gen_ragged_math_ops_py_wrappers_cc.lib and object bazel-out/x64_windows-opt/bin/tensorflow/python/gen_ragged_math_ops_py_wrappers_cc.exp\r\n**ERROR: C:/tensorflow-build/tensorflow/tensorflow/python/BUILD:293:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 2): cl.exe failed: error executing command**\r\n  cd C:/users/james/_bazel_james/j7bi4x5j/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6 Tools\\x64\\;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/envs/tensorflow-build/python.exe\r\n    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/envs/tensorflow-build/lib/site-packages\r\n    SET TEMP=C:\\Users\\james\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\james\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Ibazel-out/x64_windows-opt/bin/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Ibazel-out/x64_windows-opt/bin/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/genfiles/external/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/lmdb /Ibazel-out/x64_windows-opt/genfiles/external/lmdb /Ibazel-out/x64_windows-opt/bin/external/lmdb /Iexternal/org_sqlite /Ibazel-out/x64_windows-opt/genfiles/external/org_sqlite /Ibazel-out/x64_windows-opt/bin/external/org_sqlite /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Ibazel-out/x64_windows-opt/bin/external/png_archive /Iexternal/icu /Ibazel-out/x64_windows-opt/genfiles/external/icu /Ibazel-out/x64_windows-opt/bin/external/icu /Iexternal/grpc /Ibazel-out/x64_windows-opt/genfiles/external/grpc /Ibazel-out/x64_windows-opt/bin/external/grpc /Iexternal/com_github_nanopb_nanopb /Ibazel-out/x64_windows-opt/genfiles/external/com_github_nanopb_nanopb /Ibazel-out/x64_windows-opt/bin/external/com_github_nanopb_nanopb /Iexternal/boringssl /Ibazel-out/x64_windows-opt/genfiles/external/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Ibazel-out/x64_windows-opt/bin/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Ibazel-out/x64_windows-opt/bin/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Ibazel-out/x64_windows-opt/bin/external/zlib_archive /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/genfiles/external/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_python/numpy_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/numpy_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/numpy_include /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include/crt /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Ibazel-out/x64_windows-opt/bin/external/png_archive /Iexternal/icu/icu4c/source/common /Ibazel-out/x64_windows-opt/genfiles/external/icu/icu4c/source/common /Ibazel-out/x64_windows-opt/bin/external/icu/icu4c/source/common /Iexternal/grpc/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/include /Ibazel-out/x64_windows-opt/bin/external/grpc/include /Iexternal/grpc/third_party/address_sorting/include /Ibazel-out/x64_windows-opt/genfiles/external/grpc/third_party/address_sorting/include /Ibazel-out/x64_windows-opt/bin/external/grpc/third_party/address_sorting/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/genfiles/external/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /DTF_USE_SNAPPY /DSQLITE_OMIT_DEPRECATED /DGRPC_ARES=0 /DPB_FIELD_32BIT=1 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w /arch:AVX /Fobazel-out/x64_windows-opt/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.obj /c tensorflow/python/lib/core/bfloat16.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n**tensorflow/python/lib/core/bfloat16.cc(634): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(634): note: None of the functions with this name in scope match the target type**\r\ntensorflow/python/lib/core/bfloat16.cc(638): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(638): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(641): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(641): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(645): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(645): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(649): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(649): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(653): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_d0df84676ae54709cf34c880a157d294>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(653): note: None of the functions with this name in scope match the target type\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 107.347s, Critical Path: 59.11s\r\nINFO: 206 processes: 206 local.\r\n**FAILED: Build did NOT complete successfully**\r\n", "comments": ["There is a change in numpy 1.19 that breaks the build. Open tensorflow\\python\\lib\\core\\bfloat16.cc and make the following changes:\r\n\r\nAround line # 508, is the function CompareUFunc, add \"const\" to the parameters, like this:\r\n\r\ntemplate <typename Functor>\r\nvoid CompareUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n                  void* data) {\r\n  BinaryUFunc<bfloat16, npy_bool, Functor>(args, dimensions, steps, data);\r\n}\r\n\r\nLine # 493, is the function BinaryUFunc add \"const\" here too:\r\n\r\ntemplate <typename InType, typename OutType, typename Functor>\r\nvoid BinaryUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n                 void* data) {\r\n\r\nYou should be able to get past this error.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41086\">No</a>\n", "> Are you satisfied with the resolution of your issue?\r\n> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41086)\r\n> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41086)\r\n\r\nI had a similar problem with version 2.2", "> Hi There,\r\n> \r\n> We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help.\r\n> \r\n> This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.\r\n\r\nHello, I have encountered this problem in other versions. Do you need to correct your own mistakes?", "> There is a change in numpy 1.19 that breaks the build. Open tensorflow\\python\\lib\\core\\bfloat16.cc and make the following changes:\r\n> \r\n> Around line # 508, is the function CompareUFunc, add \"const\" to the parameters, like this:\r\n> \r\n> template\r\n> void CompareUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n> void* data) {\r\n> BinaryUFunc<bfloat16, npy_bool, Functor>(args, dimensions, steps, data);\r\n> }\r\n> \r\n> Line # 493, is the function BinaryUFunc add \"const\" here too:\r\n> \r\n> template <typename InType, typename OutType, typename Functor>\r\n> void BinaryUFunc(char** args, npy_intp const* dimensions, npy_intp const* steps,\r\n> void* data) {\r\n> \r\n> You should be able to get past this error.\r\n\r\nIf I reduce to version 1.18, can I solve this problem", "@sanatmpa1 I switched to doing this is Colab and made a walk through if anyone wants to see it:\r\nhttps://colab.research.google.com/drive/1uEkP5j7KM9eSkCUtUyauy7-Dyd5XAY6e?usp=sharing"]}, {"number": 41085, "title": "Converting RESIZE_NEAREST_NEIGHBOR", "body": "**System information**\r\n- OS Ubuntu 20.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version (2.1,2.2,2.3)-Explained below\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\n\r\nif len(sys.argv) < 2:\r\n    print(f\"Usage: %s model.h5\" % (sys.argv[0]))\r\n    sys.exit(1)\r\n\r\nmodel = tf.keras.models.load_model(sys.argv[1])\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ndef representative_dataset_gen():\r\n  for _ in range(250):\r\n    data = np.random.uniform(0.0, 1.0, size=(1, 416, 416, 3)).astype(np.float32)\r\n    yield [data]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_model = converter.convert()\r\n\r\nwith open('out.tflite', \"wb\") as f:\r\n    f.write(tflite_model)`\r\n\r\n**The output from the converter invocation**\r\nerror: failed while converting: 'main'\r\nOps that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor\r\n\r\n**h5 model**\r\nhttps://drive.google.com/file/d/1iYGntXy-uAiIwxhlfOQEopqNacCPM5_P/view?usp=sharing\r\n**Explanation of the whole problem**\r\nhttps://github.com/google-coral/edgetpu/issues/152\r\n**In short**\r\nI tried to run the model on Coral TPU and I ran into a problem when I converted the model to .tflite using tf 2.2 and 2.3 because when I went to convert this .tflite model using edgetpu_compiler I got an error:\r\nEdge TPU Compiler version 2.1.302470888\r\nERROR: Didn't find op for builtin opcode 'RESIZE_NEAREST_NEIGHBOR' version '3'\r\n\r\nERROR: Registration failed.\r\n\r\nInvalid model: yolo_rcj.tflite\r\nModel could not be parsed.\r\n\r\nBut when I used older versions of tf (1.15,2.0) I got this error:\r\nerror: failed while converting: 'main'\r\nOps that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor\r\nBut when I used TensorFlow 2.1 to convert, it worked. \r\n\r\nI am writing this problem because I think it will be a problem in the future", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/26690e600826a6183b4b9e27729630f1/41085-2-2.ipynb). Facing an error stating `Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag): ResizeNearestNeighbor.`, on running the code.\r\n\r\nHowever, with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/7c2126c63ee2c7b9d6ab4e263c8fad7b/41085-2-1.ipynb), [TF v2.3.0-rc0](https://colab.research.google.com/gist/amahendrakar/c96b4b320610765544e36d02ce55e997/41085-2-3.ipynb#scrollTo=Hzhl1Xj4qf-5) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/55b0bfecd2df72a1d6aa34f2c7ee3e5e/41085-tf-nightly.ipynb) I was able to convert the model successfully. Please find the attached gist. Thanks!", "@amahendrakar I guess this won't be fixed for `tf2.2` then, since future version will be working?\r\n", "@JakubGal I ran your code and the converted model has `int32` dtype for bias (FusedBatchNormV3) in the first `Conv2D`.  Please check the attached image. Can you please share the code used for model building? Coral Edge TPU requires fully quantized integer model. Thanks!\r\n<img width=\"654\" alt=\"Screen Shot 2020-07-08 at 5 29 58 PM\" src=\"https://user-images.githubusercontent.com/46058173/86983714-74830500-c141-11ea-9332-344462706da2.png\">\r\n\r\n", "@jvishnuvardhan \r\nFirst I trained the model using [darknet](https://github.com/AlexeyAB/darknet):\r\nhttps://drive.google.com/drive/folders/1dm_q5hhxIC7NtbN31IepR0thMxFMytyW?usp=sharing\r\nThen I converted the model to .h5:\r\nhttps://drive.google.com/drive/folders/1s1N8PoJojF6dCY6AJnDGHWtLdZgnsYUu?usp=sharing\r\nNext to .tflite:\r\nhttps://drive.google.com/drive/folders/1nJfaqerZ97CR19TxZe2tioD0LCYzr6HN?usp=sharing\r\nAnd finally I used [edgetpu_compiler](https://coral.ai/docs/edgetpu/compiler/) to compile model for Coral TPU:\r\nhttps://drive.google.com/drive/folders/1MDLHuVRw5HHUpOt2_0CtXwaEwlZu7mtx?usp=sharing\r\n\r\nAnd I found out another issue:\r\nhttps://drive.google.com/file/d/1WPB8-rag9pDKELnMiGl2yngtM-JvKlnE/view?usp=sharing\r\nI don't know why the QUANTIZE can't by converted. It is not a big deal but I Am wonder why.\r\n\r\nI found a huge reduction in accuracy when using post-training quantization so I started to reimplement the model directly using Tensorflow so that I could use quantization-aware training. This might also help with this problem.But I still don't know where the mistake might be. Any ideas?", "\r\n@JakubGal Could you please refer to [this](https://github.com/google-coral/edgetpu/issues/206) if there is any specific residual issue with RESIZE_NEAREST on edge_tpu and let us know if it helps ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41085\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41085\">No</a>\n"]}, {"number": 41084, "title": "Wrong Error message", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nFor 1d dimension error, it shows 2d error message.\r\nBut on using 1d's property it works.\r\n\r\n**Describe the expected behavior**\r\nIt should raise an error saying that input dimension should be 3\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@DextroLaev \r\nCould you please share a simple stand alone code to replicate the issue reported or if possible share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41084\">No</a>\n"]}, {"number": 41083, "title": "Requirement already up to date when running pip install --upgrade tensorflow. Stuck at version 1.14.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Rasbian GNU/Linux 10\r\n- TensorFlow installed from: python3 -m pip\r\n- TensorFlow version: 1.14.0 trying to get to 2.+\r\n- Python version: 3.7.3\r\n- Installed using virtualenv: pip\r\n- CUDA/cuDNN version: Not sure, this is an off-the-shelf Raspberry Pi 4\r\n- GPU model and memory: ^\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI tend to write scripts on my MacBook Pro and transfer them to my Raspberry Pi to run indefinitely. I set up my first LTSM network (so excited) and got it predicting and training. Brought it to the Pi, it threw an error while training complaining about the shape. After much googling I found a github issue where someone said this was fixed in tensorflow 2. Realized the pi was running tensorflow 1.14.0.\r\n\r\nLong story short I've gone through all of the system requirements (upgrade pip, python is correct version, raspbian is correct version) and when I run `pip install --upgrade tensorflow` the console prints `Requirement already up to date: tensorflow....` and tensorflow remains at 1.14.0.\r\n\r\n\r\n**Any other info / logs**\r\nI figure this is a system requirement issue but it seems like the Raspberry Pi satisfies everything.", "comments": ["I've been troubleshooting this for a few hours now. I tried to manually download the pip wheels and for the linux wheel and both rpi wheels that I tried pip install throws \"xxxx.whl is not a supported wheel on this platform\"\r\nIs it because I am using a Raspberry Pi 4 and not a Raspberry Pi 3?", "The issue is still relevant since there are no working Tensorflow 2.x builds for Python 3.7 (or newer) on the Raspberry Pi.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41083\">No</a>\n", "Again: Please reopen the issue, it's still not resolved.\r\n\r\nI mean I would upgrade to TF2.x, but it's either not available or not working.", "Please go through this link for [reference](https://nitin9809.medium.com/getting-started-with-raspberry-pi-installation-guide-for-tensorflow-2-3-1-and-opencv-4-5-1-3a5bd0b6de2d) . Can you also try with these follow steps\r\n\r\n```\r\n1.upgrade your pip to latest version \r\n2.sudo pip install --upgrade tensorflow\r\n```\r\n\r\nif that does not work , uninstall then install latest version from above link. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41083\">No</a>\n"]}, {"number": 41082, "title": "Add create dir, delete dir/file", "body": "This PR adds create dir, delete dir/file", "comments": []}, {"number": 41080, "title": "Relink `/usr/local/lib/libtensorflow_framework.so.2' with `/lib/x86_64-linux-gnu/libz.so.1' for IFUNC symbol `crc32_z'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: libtensorflow nightly build\r\n- Python version: \r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: NVIDIA Quadro RTX5000\r\n\r\n\r\nI have a trained model which I want to execute from my c++ program. My c++ program is a big piece of software that also uses google protocol buffers/ Intel MKL / Intel IPP / Boost. Linking goes through when I link to libtensorflow c API. However, when I run my exe I get a segmentation fault with the message given in subject line. If I write a separate program with libtensorflow without using any of the other dependencies mentioned above, things work fine.\r\n\r\n1) Downloaded nightly build of libtensorflow (other older versions also lead to the same crash)\r\n2) compiled and linked my main program. Execution results in crash\r\n3) Execution works fine if I run with a sample code without linking to other libraries mentioned above\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@suth1807 \r\nPlease refer to [this link](https://stackoverflow.com/questions/48306849/lib-x86-64-linux-gnu-libz-so-1-version-zlib-1-2-9-not-found) and let us know if it helps.", "Thank you @Sadug2019, I could find the root cause of the problem. It is\nactually the use of  Protocolbuffer library both in my application and in\nlibtensorflow.so. I am guessing there are static initializations in\nlibprotobuf  that happen twice. This leads to this mysterious message and a\ncrash (zlib is already on my system). I also tried to build protocol\nbuffers without zlib but that didn't solve the crash (The message went away\nthough). The only workaround I have is to load libtensorflow in my\napplication with dlopen with RTLD_DEEPBIND flag. This solves the crash and\nworks fine but I still think it is a workaround. My problem is the same as\nmentioned in\nhttps://github.com/tensorflow/tensorflow/issues/24976 (I found the\nRTLD_DEEPBIND trick from there). Is there a possibility of resolving this\ncorrectly?\n\nOn Sun, Jul 5, 2020 at 7:49 PM Saduf2019 <notifications@github.com> wrote:\n\n> @suth1807 <https://github.com/suth1807>\n> Please refer to this link\n> <https://stackoverflow.com/questions/48306849/lib-x86-64-linux-gnu-libz-so-1-version-zlib-1-2-9-not-found>\n> and let us know if it helps.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41080#issuecomment-653894696>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AONTBSMKPQWDUJF3AOHDK3TR2CDYXANCNFSM4OQL6PJA>\n> .\n>\n", "Can you try again with the latest Libtensorflow C nightly or stable packages from here https://www.tensorflow.org/install/lang_c#nightly_libtensorflow_c_packages and let us know the outcome. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41080\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41080\">No</a>\n"]}, {"number": 41079, "title": "Implementation of \"k-decay Method For Learning Rate Schedule\"", "body": "See [Tao Zhang & Wei Li, 2020], k-decay: A New Method For Learning Rate Schedule. \r\nhttps://arxiv.org/abs/2004.05909\r\n\r\n> This paper,  we propose the k-decay method, in which the LR is changed by its k-th order derivative, to obtain the new LR schedule. In the new LR schedule, a hyper-parameter $k$ is used to control the change degree of LR, whereas the\r\n  original method of $k$ at 1. We then derive the k-decay factors $\\frac{t^k}{T^k}$ for LR schedule and apply it to polynomial, cosine and exponential function. By repeatedly using the k-decay method, one can identify the best LR schedule. We then evaluate the k-decay method on CIFAR And ImageNet datasets with different neural networks (ResNet, Wide ResNet and DenseNet). Our experiments show that the k-decay method can achieve improvements over the state-of-the-art results on most of them. The accuracy can be improved by 1.08 % on the CIFAR-10 dataset, and by 2.07 % on the CIFAR-100 dataset. On the ImageNet, the accuracy can be improved by 1.25%.  Our method is not only efficient in computation but easy to use, and thus greatly improve the training performance.\r\n\r\n![image](https://user-images.githubusercontent.com/16068578/88020347-28cb5500-cb5e-11ea-960c-17e19de94146.png)\r\n\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41079) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41079) for more info**.\n\n<!-- ok -->", "@omalleyt12  Could you please review this PR? Thank you!\r\n\r\n", "@gbaned It seems like @omalleyt12 is currently busy with other things or doesn't receive GitHub notifications. Could you request a review from another member of the TensorFlow team? Thanks!", "@atztao Can you please resolve conflicts? Thanks!", "> @atztao Can you please resolve conflicts? Thanks!\r\n\r\nThanks for your remainding me.", "@tanzhenyu  Could you please review this PR? This is just a simple and small change. It has been submitted for more than ten days. Thanks!\r\n\r\n", "@gbaned Sorry to bother you again. But this PR has been submitted for more than ten days. It seems like @tanzhenyu  is busy with other things and doesn't receive GitHub notifications. Could you request a review from another member of the TensorFlow team? Thanks!\r\n\r\n", "@atztao Thanks for the review! I think the best place for a new object like this is in the [TensorFlow Addons Repo](https://github.com/tensorflow/addons)\r\n\r\nCould you please open a PR there? Thank you!", "@omalleyt12 Thanks for your advice. But you can see this method in the PR is not a new fuction, it just perfect the learning rate decay. I only changed $\\frac{t}{T}$ to $\\frac{t^k}{T^k}$. When the k =1 , it back to the orignal method. And i found the SGDR method in the core  #11749, that's good schedule, but this method is more better and simple. You can see the experiments part. Please think about it again. \r\n\r\n![image](https://user-images.githubusercontent.com/16068578/87996311-598e9880-cb24-11ea-989b-3fa4a6398328.png)\r\n\r\n![image](https://user-images.githubusercontent.com/16068578/87996242-264c0980-cb24-11ea-966f-47989f400dbe.png)\r\n\r\n![image](https://user-images.githubusercontent.com/16068578/87996268-36fc7f80-cb24-11ea-962b-2857b453111f.png)\r\n\r\n", "@gbaned Sorry to bother you again. Please request a review from another member of the TensorFlow team. Thanks!", "> @gbaned Sorry to bother you again. Please request a review from another member of the TensorFlow team. Thanks!\r\n\r\n@atztao Sorry for the delay. We are discussing about this PR internally. Thank you!", "We cannot integrate code from all research papers. Instead, new code should go into [Addons Repository](https://github.com/tensorflow/addons) and, in time, after careful experiments and seeing that it properly works on all use cases we can incorporate the change and make it default. Until then, the new schedule has to be provided via an optional layer/operator/code path."]}, {"number": 41078, "title": "Dataset error with uc_merced dataset", "body": "**System information**\r\n- Using Google Colab's default TF installation\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nDataset's samples are not all of the expected 256x256x3 shape, I found 44 out of 2100 samples with different shape. Currently, during preprocessing, I have to rescale the image if necessary.\r\n\r\n**Describe the expected behavior**\r\nAll samples should have 256x256x3 shape.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow.compat.v2 as tf\r\nimport tensorflow_datasets as tfds\r\n\r\ndataset, info = tfds.load('uc_merced',\r\n                            split='train',\r\n                            as_supervised=True,\r\n                            with_info=True,\r\n                            shuffle_files=True)\r\n\r\nexp_shape = (256, 256, 3)\r\nn = 0\r\nfor sample in dataset.take(-1):\r\n    if sample[0].shape != exp_shape:\r\n        n += 1\r\n\r\nprint('Found {} samples with wrong shape'.format(n))\r\n```\r\n\r\n**Other info / logs**\r\n`Found 44 samples with wrong shape`\r\n\r\n**Additional notes**\r\nThis may seem to be a problem with the original dataset (http://weegee.vision.ucmerced.edu/datasets/landuse.html). If a fix is not possible, it may be a good idea to add a note in the documentation page https://www.tensorflow.org/datasets/catalog/uc_merced to let new users know about the issue.\r\nAlternatively, a possible solution that the user would have to implement is something like:\r\n```\r\ndef correct_dims(x, y):\r\n    some_resizing_function(x, (256, 256, 3))\r\n```\r\nand after loading the dataset:\r\n```\r\ndataset = dataset.map(correct_dims)\r\n```\r\n", "comments": ["I have tried in colab with TF versions 2.2,2.3-rc0 and nightly versions(`2.4.0-dev20200705`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cab3b78ef9cebabe2b641cf9fb5e17e5/untitled81.ipynb).Thanks!", "Hi, thanks for reporting this, but this is an issue with [tensorflow-datasets](https://github.com/tensorflow/datasets) can you rep[ort the issue there instead?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41078\">No</a>\n"]}, {"number": 41077, "title": "docs: code style: says to install clang-tidy then examples use clang-format", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/community/contribute/code_style\r\n\r\n## Description of issue (what needs changing):\r\n\r\nDocs say to install `clang-tidy` but the example given says to run `clang-format`. Is this intended? I would have expected to run `clang-tidy`.", "comments": ["Thanks for the issue. \r\nThe example looks correct since we are using `diff` to compare the newly formatted file generated by `clang-format` vs the original file.\r\nPerhaps we can add `clang-tidy` piece to the documentation to output the warnings from the original file if any as in initial check.", "I think the clang-format part of the example is correct. I suppose I\u2019d\r\nexpect to see an extra example for running clang-tidy, and perhaps an\r\napt-get install clang-format too.\r\n\r\nOn Tue, 7 Jul 2020 at 05:11, Yasir Modak <notifications@github.com> wrote:\r\n\r\n> Thanks for the issue.\r\n> The example looks correct since we are using diff to compare the newly\r\n> formatted file generated by clang-format vs the original file.\r\n> Perhaps we can add clang-tidy piece to the documentation to output the\r\n> warnings from the original file if any as in initial check.\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/41077#issuecomment-654415612>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAAZYONC6YEO5F6JKYFX23DR2IOVTANCNFSM4OQKE4MQ>\r\n> .\r\n>\r\n", "Would you be interested in raising a PR to add it?\r\nThis the file we can try editing https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/code_style.md\r\n", "I could, if I knew what flags you run clang-tidy with?\n\nOn Wed, 8 Jul 2020 at 07:03, Yasir Modak <notifications@github.com> wrote:\n\n> Would you be interested in raising a PR to add it?\n> This the file we can try editing\n> https://github.com/tensorflow/docs/blob/master/site/en/community/contribute/code_style.md\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41077#issuecomment-655132853>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAAZYOOQUINWTJY6P3MKN5LR2OEQRANCNFSM4OQKE4MQ>\n> .\n>\n", "Perhaps I should just remove the reference to `clang-tidy`, it's not easy to invoke (you have to pass all the includes, for example), so it's probably not actionable to just include in the docs without detailed instructions.\r\n\r\nhttps://clang.llvm.org/extra/clang-tidy/", "CONTRIBUTING.md also needs to be updated in the main tensorflow repo", "Thank you for your contribution. Closing this issue since the associated PR is merged."]}, {"number": 41076, "title": "tf.data, construct a batch with different data? Are there easy ways to do it? TensorFlow1.15", "body": "I want to construct a batch of data with batchsize 16, using `tf.data`, where `[:8]` is a kind of data A, `[8:16]` is a kind of data B.\r\n\r\nIt is easy to do without `tf.data`. If use `tf.data`, the code could be:\r\n\r\n```\r\ndef _decode_record(record, name_to_features):\r\n    example = tf.parse_single_example(record, name_to_features)\r\n    return example\r\n\r\ndataA = tf.data.TFRecordDataset(input_files)\r\ndataA = dataA.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                lambda record: _decode_record(record, name_to_features),\r\n                batch_size=batch_size)\r\n           )\r\n```\r\nHow to do it next? I try:\r\n```\r\ndataB = tf.data.TFRecordDataset(input_files2)\r\ndataB = dataB.apply(\r\n            tf.contrib.data.map_and_batch(\r\n                lambda record: _decode_record(record, name_to_features),\r\n                batch_size=batch_size)\r\n           )\r\ndataC = dataA.concatenate(dataB) \r\n```\r\nBut `concatenate` is: Append the whole dataset `dataB` to the end of `dataA`.\r\n\r\nFor `concatenate`, note that `name_to_features` should be same for `dataA` and `dataB`, which means I should pad a lot dummy data.\r\n\r\nI don't want to use `tf.cond` or `tf.where` to judge different data inside the `model_fn` of `tf.estimator`, where it is also very hard to debug.\r\n\r\nAre there easy ways to do it?\r\n\r\nThank you.", "comments": ["One solution is to judge different data:\r\n```\r\nimport tensorflow as tf\r\n\r\ndata_type = tf.constant([1, 2, 1, 2])\r\nwhere_index1 = tf.where(tf.equal(data_type, 1))\r\nwhere_index2 = tf.where(tf.equal(data_type, 2))\r\n\r\ndata = tf.constant([[10,10],[20,20],[30,30],[40,40]])\r\n\r\ndata1 = tf.gather_nd(data,where_index1)\r\ndata2 = tf.gather_nd(data,where_index2)\r\n\r\nsess = tf.Session()\r\n\r\nprint(sess.run(data1))\r\nprint(sess.run(data2))\r\n\r\n```\r\nBut this answer just bypass the question somehow.", "CC @jsimsa ", "```\r\nimport tensorflow as tf\r\n\r\ndataset_1 = tf.data.Dataset.from_tensors(1).repeat(100)\r\ndataset_2 = tf.data.Dataset.from_tensors(2).repeat(100)\r\n\r\ndataset = tf.data.Dataset.zip((dataset_1, dataset_2))\r\ndataset = dataset.batch(8)\r\ndataset = dataset.map(lambda a, b: tf.concat([a, b], 0))\r\n```\r\nI havn't try it in my real code. ", "You can achieve this with `Batching datasets elements`  by simple batching using `tf.data.Dataset.zip` or batching tensors with padding using `dataset.padded_batch`. More details can be found [here](https://www.tensorflow.org/guide/data#batching_dataset_elements). ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 41075, "title": "TPU errro InvalidArgumentError: NodeDef expected inputs 'string' do not match 0 inputs specified;", "body": "TPU is not working with any version greater than TF 2.2.0 . Even in TF 2.2.0, it gets stuck indefinitely. \r\n\r\nhttps://colab.research.google.com/gist/legacyai/0895547e3d7fa573aab03a2073afff7e/tpu.ipynb", "comments": ["I am not seeing any issue with TF version 2.2.0. However i am able to reproduce the issue in colab with TF versions 2.3-rc0 ,nightly versions(`2.4.0-dev20200703`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/19198e0210466deb8218777103608c5e/untitled80.ipynb).Thanks!", "This looks like it is a duplicate of #40622. There is probably a mismatch of TPU versions (Colab doesn't have 2.3 builds yet) causing the error (the OpKernel expects one thing on 2.3, but the TPU version itself is only 2.2).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41075\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41075\">No</a>\n"]}, {"number": 41074, "title": "All operations are tensorflow operations, or wrapped with @tf.function, but still getting \"NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function...\"", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab, python 3 default. \r\n\r\n- TensorFlow installed from (source or binary): Google Colab, python 3 default. \r\n\r\n- TensorFlow version (use command below):  tensorflow:2.2.0\r\n- Python version: 3.6.9\r\n- GPU model and memory: Using colab TPU\r\n\r\n**Describe the current behavior** \r\n\r\nEven though I wrapped all my functions, and am only using Tensorflow functions, I get this error \r\n\r\n>NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into strategy.run is a tf.function or strategy.run is called inside a tf.function if eager behavior is enabled.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nModel should train on TPU without \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is a colab notebook that reproduces the error. \r\n\r\nhttps://colab.research.google.com/drive/11Yo1mdnKA3DqZCr_UpZI4umY8tpBpDzS?usp=sharing\r\n\r\nThe code also needs a datafile on GCP, since it is training on a TPU. Here's a link to the sample data file. \r\n\r\nhttps://drive.google.com/file/d/106gSmcClyshu98SDQ9VsUVOhd-LYamVq/view?usp=sharing\r\n\r\n```\r\n%tensorflow_version 2.x\r\n!pip install transformers --q\r\n\r\n!gcloud auth login\r\n\r\n'''NEED TO RUN THIS CELL TWICE TO AVOID ERROR'''\r\n\r\nfrom google.colab import auth\r\nauth.authenticate_user()\r\n\r\nproject_id = 'machinelearning-264918'\r\n!gcloud config set project {project_id}\r\n\r\n!pip install tfa-nightly\r\nimport tensorflow_addons as tfa\r\n\r\nfrom transformers import TFBertModel, AutoModel, TFRobertaModel\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import (Dense,\r\n                                     Dropout)\r\nimport tensorflow_addons as tfa\r\nimport numpy as np\r\nimport os\r\nfrom copy import deepcopy \r\nfrom time import time\r\n\r\nlogger = tf.get_logger()\r\nlogger.info(tf.__version__)\r\n\r\nautotune = tf.data.experimental.AUTOTUNE\r\n\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\r\n    print('strategy.num_replicas_in_sync', strategy.num_replicas_in_sync)\r\n    logger.info('Running with TPUStrategy on TPU {} with {} cores '\r\n                .format(tpu.cluster_spec().as_dict()['worker'],\r\n                        strategy.num_replicas_in_sync))\r\n    batch_size = 16 * strategy.num_replicas_in_sync\r\nexcept Exception:\r\n    # raise ValueError\r\n    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\r\n    logger.warning('Failed initializing TPU! Running on GPU')\r\n    batch_size = 16\r\n\r\nclass Dora_A(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(Dora_A, self).__init__(**kwargs)\r\n        self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\r\n\r\n    def call(self, inputIds):\r\n        queryInputs, passageInputs = inputIds\r\n\r\n        Q_outputs = self.bioRoberta(queryInputs)[0]\r\n        P_outputs = self.bioRoberta(passageInputs)[0]\r\n\r\n        dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')\r\n\r\n        return dotProductMatrix\r\n\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n    '''\r\n        1. Every sample is its own positive, and  the rest of the\r\n            elements in the batch are its negative.\r\n        2. Each TPU core gets 1/8 * global_batch_size elements, hence\r\n            compute shape dynamically.\r\n        3. Dataset produces dummy labels to make sure the loss_fn matches\r\n            the loss signature of keras, actual labels are computed inside this\r\n            function.\r\n        4. Inputs are logits, for better numerical stability.\r\n    '''\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n\r\nCLS_inputID = tf.constant([0])\r\nSEP_inputID = tf.constant([2])\r\n\r\n@tf.function\r\ndef _parse_example(example_proto):\r\n    features = {\r\n        'bioRoberta_SentenceIndex': tf.io.VarLenFeature( dtype=tf.int64),\r\n        'BioRoberta_IDs': tf.io.VarLenFeature( dtype=tf.int64),\r\n    }\r\n\r\n    parsed_example_dict = tf.io.parse_single_example(example_proto, features)\r\n    bertIds = parsed_example_dict['BioRoberta_IDs']\r\n    bertIds = tf.sparse.to_dense(bertIds)\r\n    bertIds = tf.cast(bertIds, dtype=tf.int32)\r\n\r\n    queryPiece = tf.slice(bertIds, [0], [510])\r\n    restPassagePiece = tf.slice(bertIds, [0], [510])\r\n    # add special tokens for proper input into the model \r\n    queryBertInput = tf.concat( [CLS_inputID, queryPiece, SEP_inputID], axis=0)\r\n    paragraphBertInput = tf.concat( [CLS_inputID, restPassagePiece, SEP_inputID], axis=0)\r\n\r\n    return queryBertInput, paragraphBertInput\r\n\r\nconfig_name = 'model_a'\r\nbase_dir = 'gs://a-dora-semantic-scholar'\r\nmodel_dir = os.path.join(base_dir, config_name)\r\ntensorboard_dir = os.path.join(model_dir, 'logs_' + str(time()))\r\ntfrecords_pattern_train = os.path.join(base_dir, 'VersionA_00022*')\r\ntfrecords_pattern_val = os.path.join(base_dir, 'VersionA_00022*')\r\n\r\n\r\nif 'COLAB_TPU_ADDR' in os.environ:\r\n    print('Setting tf.data objects')\r\n    with strategy.scope():\r\n        filenames = tf.io.gfile.glob(tfrecords_pattern_train)\r\n        train_dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=autotune)\r\n        train_dataset = train_dataset.map(\r\n                                        _parse_example, num_parallel_calls=autotune)\r\n        train_dataset = train_dataset.shuffle(130_000, seed=1000, reshuffle_each_iteration=True)\r\n        train_dataset = train_dataset.padded_batch(batch_size, padding_values=(1, 1))\r\n        train_dataset = train_dataset.prefetch(autotune)\r\n        train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\r\n\r\nwith strategy.scope():\r\n    model = Dora_A(dynamic=True)\r\n    model.layers[0].trainable = False\r\n    model.compile(loss=loss_fn,\r\n                    optimizer=tfa.optimizers.AdamW(weight_decay=1e-5, \r\n                                                   learning_rate=1e-5, \r\n                                                   epsilon=1e-06))\r\n\r\nmodel.fit(train_dataset)\r\n```\r\n\r\n**Other Attempts to fix** \r\n\r\nI decorated the call function, so that my model class looked like this\r\n\r\n```\r\nclass Dora_A(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(Dora_A, self).__init__(**kwargs)\r\n        self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\r\n\r\n    @tf.function\r\n    def call(self, inputIds):\r\n        queryInputs, passageInputs = inputIds\r\n\r\n        Q_outputs = self.bioRoberta(queryInputs)[0]\r\n        P_outputs = self.bioRoberta(passageInputs)[0]\r\n\r\n        dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')\r\n\r\n        return dotProductMatrix\r\n```\r\n\r\nBut I got the same error message. \r\n\r\nI also tried decorating my tf.data parse function\r\n\r\nI tried disabling eager execution, but got an error when trying to compile the model. \r\n\r\nIt seems that when eager mode is disabled, I can not use distributed strategies for a subclassed model. The error message is\u00a0\r\n>\u00a0ValueError: We currently do not support distribution strategy with a `Sequential` model that is created without `input_shape`/`input_dim` set in its first layer or a subclassed model.\r\nThis happens during `model.compile`.\u00a0\r\n\r\nHere is a link to a different colab notebook I tried for this \r\n\r\nhttps://colab.research.google.com/drive/1OhFgxbFoAEsLCDqpwBbe3P8owdJHMcUZ?usp=sharing\r\n\r\nHere is a related question\u00a0https://stackoverflow.com/questions/60444486/use-tf-distribute-strategies-with-tf-keras-model-subclassing\r\n\r\n\r\nIt's been suggested either to use the sequential or functional API, but it seems that I can only use model subclassing. I am unable to use Sequential in the non-minimalized version of my architecture because it has two pipelines. And the functional api has its own issues, which I describe it in this github issue ( https://github.com/tensorflow/tensorflow/issues/40638#event-3468314954 ) but the summary is somehow the functional api seems to be deleting model weights and whole layers for my architecture. \r\n\r\n**Other info / logs** \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n<ipython-input-12-50bee5f74f82> in <module>()\r\n----> 1 model.fit(train_dataset)\r\n\r\n4 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_function(iterator)\r\n    570       data = next(iterator)\r\n    571       outputs = self.distribute_strategy.run(\r\n--> 572           self.train_step, args=(data,))\r\n    573       outputs = reduce_per_replica(\r\n    574           outputs, self.distribute_strategy, reduction='first')\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in run(self, fn, args, kwargs, options)\r\n    166   def run(self, fn, args=(), kwargs=None, options=None):\r\n    167     \"\"\"See base class.\"\"\"\r\n--> 168     validate_run_function(fn)\r\n    169 \r\n    170     # Note: the target function is converted to graph even when in Eager mode,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in validate_run_function(fn)\r\n    104       and not (callable(fn) and isinstance(fn.__call__, def_function.Function)):\r\n    105     raise NotImplementedError(\r\n--> 106         \"TPUStrategy.run(fn, ...) does not support pure eager \"\r\n    107         \"execution. please make sure the function passed into \"\r\n    108         \"`strategy.run` is a `tf.function` or \"\r\n\r\nNotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.\r\n```\r\n\r\n**StackOverflow Question** \r\n\r\nI also put a stack overflow question for this issue, in case I may have overlooked something, but it's been up for a few days, with a bounty, and no further insights. \r\n\r\nhttps://stackoverflow.com/questions/62650379/error-when-running-on-tpu-notimplementederror-tpustrategy-runfn-does-n\r\n", "comments": ["@Santosh-Gupta,\r\nThe Colab gist you have provided does not throw an error, but the `model.fit` cell runs indefinitely. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f0e1c450b2c5fddaa9a87aec570fec8d/41074.ipynb). \r\n\r\nCould you please provide a reproducible code snippet? Thanks!", "> @Santosh-Gupta,\r\n> The Colab gist you have provided does not throw an error, but the `model.fit` cell runs indefinitely. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f0e1c450b2c5fddaa9a87aec570fec8d/41074.ipynb).\r\n> \r\n> Could you please provide a reproducible code snippet? Thanks!\r\n\r\nI ran the gist provided in your reply and got the same result; I got a hanging cell but not an error. I think this is due to your gist copying the data into the colab disk, and feeding that data to the model, rather than getting the model data through GCP. \r\n\r\nHere is a gist that gets the data through GCP, which reproduces \"NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution\"\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/6241611b60c22bad60d4309d2adee5c2/doraa_v3_minimal_tpu_gcpcloud_6-30.ipynb\r\n\r\nGetting training to work by either getting it directly through GCP, or from the colab enviroment would work for me, since I can just download the data from GCP into colab.", "For the issue with the cell hanging when the data was being read from the colab enviroment, I am wondering if this is due to TPUs only being able to read data from GCP. This is the impression I got from this documentation\r\n\r\nhttps://cloud.google.com/tpu/docs/troubleshooting#cannot_use_local_filesystem", "Another thing I looked into is if there was a tensorflow function that was not yet supported on TPUs, there is a non-exhaustive list here \r\n\r\nhttps://cloud.google.com/tpu/docs/tensorflow-ops\r\n\r\nNone of the operations in my examples seem to include operations that are not supported. Also, it seems that if there was an unsupported tensorflow operation, it would give a different error message, something like \r\n\r\n>RuntimeError: Compilation failed: Compilation failure: Detected unsupported operations when trying to compile graph\r\n\r\nSo it looks like the issue is due to something else. ", "I've been training a different architecture that is working on TPU, and this architecture has a lot of similarities to the minimal example posted earlier. \r\n\r\nThis is a notebook of the working architecture (it is not minimal, I figured it doesn't need to be since it is working, but if having a minimal version would help isolate the issue, I would be more than happy to create one). \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/269329b682d0e94f2858d69722602674/bdorab_tpu_7-9-10_working.ipynb\r\n\r\nFor convenience, here is the original minimal (non working) architecture which is the focus of this github issue. \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/6241611b60c22bad60d4309d2adee5c2/doraa_v3_minimal_tpu_gcpcloud_6-30.ipynb\r\n\r\nHere what the non-working example contains which is not in the working example:\r\n\r\n-The non-working architecture returns a result from `tf.linalg.matmul`, the lines of code are below\r\n\r\n```\r\n        dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')\r\n\r\n        return dotProductMatrix\r\n```\r\n\r\n-The non non-working architecture loss function is performed on this matrix (the training is set up so that the diagonals are the true values, everything else is false\r\n\r\n```\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n    '''\r\n        1. Every sample is its own positive, and  the rest of the\r\n            elements in the batch are its negative.\r\n        2. Each TPU core gets 1/8 * global_batch_size elements, hence\r\n            compute shape dynamically.\r\n        3. Dataset produces dummy labels to make sure the loss_fn matches\r\n            the loss signature of keras, actual labels are computed inside this\r\n            function.\r\n        4. Inputs are logits, for better numerical stability.\r\n    '''\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n```\r\n\r\nWhere as the working architecture just uses `loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)` on two outputs from its last classifier output. \r\n\r\n-The working architecture is using the Keras Functional API to create the keras model, where as the non-working architecture is created using model subclassing. Using the functional API is not a workaround for the (non-working) architecture, because it seems to be be erasing whole weights/layers from the model (described in this issue https://github.com/tensorflow/tensorflow/issues/40638#event-3468314954 )\r\n\r\n-There are a few minor differences in how the tf.data objects are handled, for clarity of isolation, I made another notebook where they are handled pretty much the same  \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/170e7a7f49074c1e03d2cfe37d8a7188/doraa_v3_minimal_tpu_gcpcloud_newfile_matmul7-12.ipynb\r\n\r\nIn the notebook above, I also changed `tf.linalg.matmul` to `tf.matmul` because I noticed the latter was included in the official list of supported TPU operations [ https://cloud.google.com/tpu/docs/tensorflow-ops ]\r\n\r\n\r\nSo, my guesses would be that the issue could be combination of `tf.matmul` and model subclassing. I don't think either alone could be an issue, the `tf.matmul` is an officially supported TPU operation, and the Transformers library has been using keras model subclassing to develop the tensorflow version of their models, and they have options to do TPU training in their trainers. \r\n\r\nAnother guess is the loss function; maybe the way its wrapped with tf.function, but I have used this type of loss function in the past for TPU training in a different architecture ( https://github.com/Santosh-Gupta/NaturalLanguageRecommendations/blob/master/src/model.py#L83 ) and it was working just fine. \r\n\r\nAnother guess is that it is the way I set up the architecture; maybe using a whole Bert architecture as a single layer\r\n\r\n```\r\nclass Dora_A(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(Dora_A, self).__init__(**kwargs)\r\n        self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\r\n\r\n    @tf.function\r\n    def call(self, inputIds):\r\n        queryInputs, passageInputs = inputIds\r\n\r\n        Q_outputs = self.bioRoberta(queryInputs)[0]\r\n        P_outputs = self.bioRoberta(passageInputs)[0]\r\n\r\n        dotProductMatrix = tf.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')\r\n\r\n        return dotProductMatrix\r\n```\r\n", "Hi @Santosh-Gupta, thank you for providing detailed information to help debug this issue. Based on the error message, it seems to be failing the [validate_run_function(fn)](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/tpu_strategy.py#L88). I am not sure why but the train fn passed to strategy.run is not one of the following, hence the error:\r\n\r\n```\r\n# We allow three types of functions/objects passed into TPUStrategy\r\n# run in eager mode:\r\n#   1. a user annotated tf.function\r\n#   2. a ConcreteFunction, this is mostly what you get from loading a saved\r\n#      model.\r\n#   3. a callable object and the `__call__` method itself is a tf.function.\r\n#\r\n# Otherwise we return an error, because we don't support eagerly running\r\n# run in TPUStrategy.\r\n```\r\n\r\n A few questions:\r\n\r\n-Can you confirm this model trains without using TPUs?\r\n-Keeping everything else the same In the non-working example, if you use a standard loss function (instead of custom) do you still get the error?", "Hi nikitamaia, \r\n\r\nI have to apologize, the last two notebooks didn't work on GPU. I was attempting to get the notebooks to work on TPU by following the error messages, and I didn't realize I completely bugged them in the process. \r\n\r\nThe non-standard loss function wasn't an issue at all. \r\n\r\nHere is a colab gist that works on GPU but not TPU\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/d14f8b76a22582992c7fc4cebc65622d/a_doraa_minimal_notworkingtpu_workinggpu2.ipynb\r\n\r\n\r\nI was able to isolate the issue to the deep copying of layers in the model init\r\n\r\n```\r\n        self.Q_Tlayer0 = deepcopy(self.bioRoberta.layers[0].encoder.layer[8])\r\n        self.P_Tlayer0 = deepcopy(self.bioRoberta.layers[0].encoder.layer[8])\r\n```\r\n\r\nThese layers are the cause of this error \"    AttributeError: Tensor.name is meaningless when eager execution is enabled.\"\r\n\r\nHere is the full error message\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-11-4a1cb08fd12a> in <module>()\r\n     16 \r\n     17 model.fit(train_dataset,\r\n---> 18             epochs=epochs)\r\n     19 \r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    625       # This is the first call of __call__, so we have to initialize.\r\n    626       initializers = []\r\n--> 627       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    628     finally:\r\n    629       # At this point we know that the initialization is complete (or less\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    504     self._concrete_stateful_fn = (\r\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 506             *args, **kwds))\r\n    507 \r\n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2444       args, kwargs = None, None\r\n   2445     with self._lock:\r\n-> 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2447     return graph_function\r\n   2448 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2775 \r\n   2776       self._function_cache.missed.add(call_context_key)\r\n-> 2777       graph_function = self._create_graph_function(args, kwargs)\r\n   2778       self._function_cache.primary[cache_key] = graph_function\r\n   2779       return graph_function, args, kwargs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2665             arg_names=arg_names,\r\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2667             capture_by_value=self._capture_by_value),\r\n   2668         self._function_attributes,\r\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    979         _, original_func = tf_decorator.unwrap(python_func)\r\n    980 \r\n--> 981       func_outputs = python_func(*func_args, **func_kwargs)\r\n    982 \r\n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    440         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    443 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nAttributeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\r\n        outputs = self.distribute_strategy.run(\r\n    <ipython-input-5-3ada42a824f0>:19 train_step  *\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py:149 apply_gradients  *\r\n        return super().apply_gradients(grads_and_vars, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:472 apply_gradients  **\r\n        grads_and_vars = _filter_grads(grads_and_vars)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 _filter_grads\r\n        ([v.name for _, v in grads_and_vars],))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1219 <listcomp>\r\n        ([v.name for _, v in grads_and_vars],))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name\r\n        \"Tensor.name is meaningless when eager execution is enabled.\")\r\n\r\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\r\n```\r\n\r\nI am curious how this error message could be interpreted to better pinpoint the issue. I am also curious why the deep copying works for GPU, but not TPU. \r\n\r\nI found a workaround is to instantiate these layers directly, feed the model a sample datapoint to instantiate the layers, and then set the weights directly with the desired layer. Here is a colab gist for this workaround\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/bcfa4c3e358df3dae47b4711364883ba/a_doraa_minimal_tpu_furtherworkaround_set.ipynb\r\n\r\n\r\n\r\n\r\n\r\n", "I think I'm a little confused now. Is the error message you're facing the `NotImplementedError` mentioned in the title of the issue? Or the `AttributeError` you've just shared?", "> I think I'm a little confused now. Is the error message you're facing the `NotImplementedError` mentioned in the title of the issue? Or the `AttributeError` you've just shared?\r\n\r\nAhh, sorry, I got a little ahead in solving my architecture. This issue is for a minimal example I was stuck at before I solved it. \r\n\r\nHere's a Colab gist that works on GPU, but fails in TPU, and gives this error. \r\n\r\n>NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/ad16030a7f4ed73045f53b71138da456/doraa_github_workinggpu_failingtpu.ipynb\r\n\r\nAnd the following is a colab notebook which is similar to that notebook, but is non-minimal, and is fully working\r\n\r\nhttps://colab.research.google.com/drive/1y1GuE1vr4tgUOFKeVeQEouP-33a3elqp?usp=sharing\r\n\r\n\r\n>-Keeping everything else the same In the non-working example, if you use a standard loss function (instead of custom) do you still get the error?\r\n\r\nFor this I had to change the custom function since the data does not contain labels; in each batch, all the other samples act as negative samples from each other. With a standard loss function, I get the same error. Here is a colab gist for this notebook. \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/0479b6c8a97cef90289ec68924de4fdf/doraa_github_workinggpu_failingtpu_standardloss.ipynb\r\n\r\nLooking at the working and non-working notebooks, I noticed that one of the differences is that in the working notebooking, the model instantiation does not have `dynamic=True`. So I tried to run it without `dynamic=True`. I then got an `UnimplementedError`. Here is the full error message. \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-36-50bee5f74f82> in <module>()\r\n----> 1 model.fit(train_dataset)\r\n\r\n3 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    851               # TODO(b/150292341): Allow multiple async steps here.\r\n    852               if not data_handler.inferred_steps:\r\n--> 853                 context.async_wait()\r\n    854               logs = tmp_logs  # No error, now safe to assign to logs.\r\n    855               callbacks.on_train_batch_end(step, logs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in async_wait()\r\n   2201   an error state.\r\n   2202   \"\"\"\r\n-> 2203   context().sync_executors()\r\n   2204 \r\n   2205 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in sync_executors(self)\r\n    636     \"\"\"\r\n    637     if self._context_handle:\r\n--> 638       pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\n    639     else:\r\n    640       raise ValueError(\"Context is not initialized.\")\r\n\r\nUnimplementedError: {{function_node __inference_train_function_145300}} Compilation failure: Dynamic shape is not supported for non trivial window: size=1x136 pad=0_0x135_0\r\n\t [[{{node tf_roberta_model_3/roberta/embeddings/Cumsum}}]]\r\n\tTPU compilation failed\r\n\t [[tpu_compile_succeeded_assert/_8332457752893656119/_5]]\r\n```\r\n\r\nHere is a copy of the colab gist for this notebook. \r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/720b677c769a060c97aca2a4becbeebd/doraa_github_workinggpu_failingtpu_nodynamic.ipynb\r\n\r\nI remember that I saw on StackOverflow for this error, I should use `dynamic=True`, so that's why I originally put it in. But obviously it's not necessary since I didn't put it in the working notebook. \r\n\r\nAnother difference I noticed between the working and non-working notebooks is that I set a ` padded_shapes=(512, 512)` when doing a batched pad with tf.data, so I tried that. \r\n\r\nAnd the model started training!  \r\n\r\nHere is a colab gist for this\r\n\r\nhttps://colab.research.google.com/gist/Santosh-Gupta/2faaebef587fcc8831e9627a2d944937/doraa_github_workinggpu_failingtpu_paddedshapes_working.ipynb\r\n\r\nSo it looks like there's two areas where TPUs throws an error, that does not occur with GPUs:\r\n\r\n1. Dynamic batch shapes do not work with TPUs (cause of original error of this issue)\r\n\r\n2. Deepcopying a layer to be used downstream in the architecture results in `    AttributeError: Tensor.name is meaningless when eager execution is enabled.` \r\n\r\n\r\n", "Thanks for the summary! \r\nSetting `dynamic=True` runs the layer eagerly, which probably explains the original error message. \r\nTo your first point, dynamic shapes were not supported with TPUs in 1.x, which required specifying static per-replica and global batch sizes (eg previously you needed to set drop_remainder=True in the input dataset), but there is increased support since 2.1 where even if the last partial batch is not even across replicas or some replicas have no data, the training job will run and complete as expected. That being said, not all dynamic shapes use cases are supported with TPUs.\r\n\r\nAs for the deepcopying, I'm not sure why that's happening. If you think this is a bug then please feel free to file a separate issue just for that + minimal example and we can definitely take a look.\r\n\r\nBut overall it sounds like you were able to find the correct workaround to get your model to train?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41074\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41074\">No</a>\n"]}, {"number": 41072, "title": "ValueError raised when weights is a None object", "body": "Previously: raise ValueError when weights list is empty.\r\nNow: raise ValueError when weights list does not exist or is empty.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41072) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41072) for more info**.\n\n<!-- ok -->", "@moselhy can you please check Ubuntu sanity failures ?", "@moselhy can you please check below error ?\r\n```\r\n\"/tensorflow/python/keras/layers/preprocessing/category_encoding_test.py\", line 608, in test_end_to_end_bagged_modeling\r\n    layer.set_weights(weights)\r\n  File \"/tensorflow/python/keras/engine/base_layer.py\", line 1851, in set_weights\r\n    expected_num_weights, str(weights)[:50]))\r\nValueError: You called `set_weights(weights)` on layer \"category_encoding\" with a weight list of length 0, but the layer was expecting 0 weights. Provided weights: []...\r\n```", "@rthadur that's fixed now"]}, {"number": 41071, "title": "version cocossd 2.0.4 does not work use 2.0.3 model", "body": "so after the update 2.0.4 for coco-ssd cdn kept saying issues about loadmodelgraph undefined so I thought I would let you guys know to use 2.0.3 with this https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.0.3 in your script tags otherwise you have have bad time cuz it didnt work when I just had https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd right after da update liek bro u gonna just make a new version dat ruin everyding.  LOL but ye jus bein bro lul", "comments": ["@el1tehAx0r \r\n\r\nI think this issue is more related to TF models repo. Please, raise an issue in models repo from [here](https://github.com/tensorflow/models/issues/new/choose).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41070, "title": "Avoid undefined behavior by union type punning in round_to_bfloat16", "body": "Use `std::memcpy` instead of union based type punning, to avoid undefined behavior.\r\nSee also C++ Core Guidelines: \"Don't use a union for type punning\"\r\nhttps://github.com/isocpp/CppCoreGuidelines/blob/v0.8/CppCoreGuidelines.md#c183-dont-use-a-union-for-type-punning", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41070) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41070) for more info**.\n\n<!-- ok -->", "@N-Dekker: Could you also make this change in Eigen? The relevant file is https://bitbucket.org/eigen/eigen/src/default/Eigen/src/Core/arch/Default/Half.h.", "> @N-Dekker: Could you also make this change in Eigen? The relevant file is https://bitbucket.org/eigen/eigen/src/default/Eigen/src/Core/arch/Default/Half.h.\r\n\r\nThanks for the suggestion, @jaingaurav I never contributed to Eigen before, but I could give it a try. Specifically for their bfloat16 implementation, I think that would be this `union`: https://gitlab.com/libeigen/eigen/-/blob/386d809bde475c65b7940f290efe80e6a05878c4/Eigen/src/Core/arch/Default/BFloat16.h#L315  Right? Bfloat16 is my main interest for now, as I'm maintaining another implementation myself! https://github.com/biovault/biovault_bfloat16\r\n\r\nUnfortunately, it appears that Eigen does not have a convenient `bit_cast` function template like `absl::bit_cast`. So I'd either have to propose `bit_cast` to Eigen as well, or fix their bfloat16 by memcpy-ing the bits from `float` to `unsigned int`, inside their `float_to_bfloat16_rtne`.\r\n\r\nAnyway, it would certainly help if this pull request would make it into the master branch of TensorFlow  \ud83d\ude03  I'll have another look tomorrow!\r\n", "@N-Dekker: Yeah I'd recommend updating both Half.h & BFloat16.h.\r\n\r\nWe're trying to merge your changes but there seem to be some BUILD file issues. I think you need to include the absl dependency else we are getting errors like: `'absl/base/casts.h': No such file or directory`", "> We're trying to merge your changes but there seem to be some BUILD file issues. I think you need to include the absl dependency else we are getting errors like: `'absl/base/casts.h': No such file or directory`\r\n\r\nThank you @jaingaurav, but I'm sorry, I'm not really familiar with the build system! Would it be sufficient to just add \"@com_google_absl//absl/base\" to  tensorflow/core/lib/bfloat16/BUILD ?  I guess somewhere here, right?\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/bfloat16/BUILD#L18 \r\n", "Yep I believe that should work", "@N-Dekker: I believe there is a ubuntu sanity check failure due to the ordering of the build deps. Could you please address that?", ">  I believe there is a ubuntu sanity check failure due to the ordering of the build deps. Could you please address that?\r\n\r\n@jaingaurav: Thanks, I see now in https://source.cloud.google.com/results/invocations/c3882af9-15f4-4c40-b79a-37a233c1d006/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_sanity_out/log\r\n\r\n    FAIL: buildifier found errors and/or warnings in above BUILD files.\r\n    buildifier suggested the following changes:\r\n    tensorflow/core/lib/bfloat16/BUILD:\r\n    18d17\r\n    <         \"@com_google_absl//absl/base\",\r\n    20a20\r\n    >         \"@com_google_absl//absl/base\",\r\n    exit status 1\r\n    Please fix manually or run buildifier <file> to auto-fix.\r\n\r\nI guess that means that it suggests moving the line down, from 18 to 20, right? Do you have a clue why? I'm asking because other BUILD files do have \"@com_google_absl//absl/base\" in front of their \"//tensorflow/core...\" dependencies, for example: https://github.com/tensorflow/tensorflow/blob/v2.3.0-rc0/tensorflow/compiler/aot/BUILD#L43\r\n\r\nAnyway, I'll give it a try.", "@jaingaurav Still build failures, unfortunately! I guess because of the \"absl/base\" dependency as well. Would you suggest me to simply avoid using Abseil for this pull request? (Instead of `absl::bit_cast<uint32_t>(v)`, the member function could directly call `std::memcpy(&u, &v, sizeof(float))`, of course.)\r\n\r\nI'm asking you also because I have another pull request in mind that may or may not use `bit_cast`", "@N-Dekker: Some of the failures seem unrelated. However, given that we're unlikely to use absl in Eigen an alternate fix without absl would be preferred.", "Apologies @N-Dekker but we're actively reworking this code to use the Eigen implementation. As a result, we'll address this casting behavior when performing that work.", ">Apologies @N-Dekker but we're actively reworking this code to use the Eigen implementation. As a result, we'll address this casting behavior when performing that work.\r\n\r\nNo problem @jaingaurav, , thank you for the information! Would you still suggest me to make a merge request on this issue for https://gitlab.com/libeigen/eigen ? \r\n\r\nI'm asking also because I was about to prepare another possible improvement: I believe the performance of conversion from an integer type to bfloat could be improved significantly.  ", "@N-Dekker: I think I'd hold off just a bit until @rmlarsen completes the work. He said he might be able to fix this issue when doing that work.\r\n\r\nIf the performance improvement is isolated, maybe you can start proposing that in eigen right away?", "FYI I just submitted a bfloat32 merge request to Eigen, albeit on a different subject: \"Allow implicit conversion from bfloat16 to float and double\", https://gitlab.com/libeigen/eigen/-/merge_requests/163", "@jaingaurav FYI, The `eigen::bfloat16` merge request that corresponds to this `tensorflow::bfloat16` pull request has just been approved and merged onto the Eigen master branch by your Google colleague Rasmus Munk Larsen (@rmlarsen) \ud83d\ude03\r\n  \"Avoid undefined behavior by union type punning in float_to_bfloat16_rtne\"\r\n  https://gitlab.com/libeigen/eigen/-/merge_requests/164\r\n  https://gitlab.com/libeigen/eigen/-/commit/b11f817bcff04276f3024d6780f56a137968b81a", "@jaingaurav \r\n> If the performance improvement is isolated, maybe you can start proposing that in eigen right away?\r\n\r\nFYI, the `eigen::bfloat16` performance improvement merge request that I submitted is currently being reviewed: \"Faster conversion from integer types to bfloat16\", https://gitlab.com/libeigen/eigen/-/merge_requests/166"]}]