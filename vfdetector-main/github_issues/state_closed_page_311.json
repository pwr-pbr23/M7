[{"number": 44910, "title": "[Question] Adding More Outputs in tflite Interpreter.get_output_details()", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10, Python 3.7.7, \r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (or github SHA if from source):\r\nTensorflow 2.3.0-rc0\r\n\r\nHello, this is more of a question rather than an issue report.\r\nI am trying to convert a yolov4 model into a tflite to no avail.\r\n\r\nAfter reading through:\r\n- https://www.tensorflow.org/lite/convert\r\n- https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\r\n- https://www.tensorflow.org/lite/inference_with_metadata/lite_support\r\n- https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter\r\n\r\nI am unable to find any details regarding appending data in the `interpreter.get_output_details()`.\r\nThere are guides that require more than 1 output details, [here's an example](https://github.com/vardanagarwal/Proctoring-AI/blob/master/coco%20models/tflite%20mobnetv1%20ssd/seg_tflite.py#L64-L69); which then can be translated into bbox, classes, scores, and number of detections respectively.\r\n\r\nAre there any documentations that can give me a lead on this question?", "comments": ["@Rocksus Are you looking for some guides on TF lite converter to convert object detection model? [Here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md) is the guide you can use. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44909, "title": "Simpler integration of TFLM with external IDEs / build systems: add ability to get list of sources, headers etc. from the TFLM Makefile", "body": "@tensorflow/micro\r\n\r\nThe goal here is to make it easier to integrate TFLM as part of external IDEs or build systems.\r\n\r\nThe current approach involves adding project generation logic to the TFLM Makefiles which is both awkward to do and harder to maintain.\r\n\r\nAn alternative approach (being developed in collaboration with the CMSIS team at ARM) is to have the makefile output the necessry information on the terminal and then keep all the integration logic external to the TFLM Makefiles.\r\n\r\nMoving forward, this will be the preferred method of integration.", "comments": ["tagging @MatthiasHertel80 and @freddan80 \r\n", "Closing the current bug in favor of https://github.com/tensorflow/tensorflow/issues/45086", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44909\">No</a>\n"]}, {"number": 44908, "title": "Multiple models sequentially with TFLu", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A\r\n- TensorFlow installed from (source or binary): N/A\r\n- Tensorflow version (commit SHA if source): 2.3.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): N/A\r\n\r\n**Describe the problem**\r\nIs there any example of how to run multiple models sequentially while sharing the same tensor arena and what are the limitations? I could not find anything in the current documentation.\r\n\r\nCurrently I have an interpreter for each model and I have one shared tensor arena but I have to re-initialize the interpreters between each inference otherwise I get the following error:\r\n\r\nType Unknown type (1617751660) not supported.\r\nNode CONV_2D (number 2) failed to invoke with status 1\r\n\r\nIs there a way to do this without having to re-initialize each time?", "comments": ["After more digging it seems it can be done by creating a MicroAllocator and passing this in when creating a MicroInterpreter. You then re-use this MicroAllocator when you create the second MicroInterpreter for the second model. \r\n\r\nYou now don't have to re-initialize the interpreters each time and tensor arena will be re-used.\r\n\r\nHowever there does not seem to be any function to clean up the MicroAllocator after use, surely this means there will be memory leaks?", "@Burton2000  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.6 and let us know if the issue still persists? Please have a look at the [reference](https://www.tensorflow.org/lite/microcontrollers/get_started_low_level) and let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44908\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44908\">No</a>\n"]}, {"number": 44907, "title": "Print internal Makefile variables", "body": "Used by external CMSIS Pack build process\r\n\r\nProgress towards #44909 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44907) for more info**.\n\n<!-- need_sender_cla -->", "@MatthiasHertel80  Can you please sign CLA. Thanks!", "@MatthiasHertel80 Can you please sign CLA. Thanks!", "Sync'd with @MatthiasHertel80  and we're closing this PR in favor of https://github.com/tensorflow/tensorflow/pull/45087"]}, {"number": 44906, "title": "RecursionError with `dynamic=True` when using a `Lambda` layer ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running a `Lambda` layer with `dynamic=True`, the code crashes with a `RecursionError`.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo crash. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ninp = tf.keras.Input(shape=(10,))\r\nout = tf.keras.layers.Lambda(\r\n        lambda x_input: x_input,\r\n        dynamic=True,\r\n)(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=out)\r\n```\r\n\r\n**Other info / logs** \r\n\r\n[traceback_recursion_error.log](https://github.com/tensorflow/tensorflow/files/5546954/traceback_recursion_error.log)\r\n\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3 and nightly version(`2.5.0-dev20201116`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/08c4a080d0ab6b9451e0ec9536747bfb/untitled519.ipynb). Thanks!", "@Lescurel Do you like a better error message?\r\n\r\nExtracted from the doc https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer?version=nightly\r\n>dynamic: Set this to True if your layer should only be run eagerly, and should not be used to generate a static computation graph. This would be the case for a Tree-RNN or a recursive network, for example, or generally for any layer that manipulates tensors using Python control flow. If False, we assume that the layer can safely be used to generate a static computation graph. \r\n\r\nIf you run a similar example you can check:\r\n```\r\nimport tensorflow as tf\r\ninp = tf.keras.Input(shape=(10,))\r\ndef my_lambda_func(x):\r\n    print(tf.executing_eagerly())\r\nx = tf.keras.layers.Lambda(my_lambda_func)(inp)\r\n```\r\n\r\n", "@bhack If passing `dynamic=True` to a Lambda layer is indeed impossible, then yes, I would have expected a better error message. \r\n\r\nBut I don't really see why it would be impossible. It feels to me that it is more likely to end up  writing Python control flow code in a Lambda layer than in any other layer. ", "It Is ok to me to keep this open for a better error message.\nAs you can see with my print in lambda you are no more in eager mode this match with documentation.", "To add a bit of context, I ended finding that \"bug\" in a more complex program that ended up throwing that Traceback : \r\n\r\n[traceback.log](https://github.com/tensorflow/tensorflow/files/5560985/traceback.log)\r\n\r\nIn that Traceback, the first error I saw was : \r\n\r\n> OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\nWhich I ignored, given that the function was already decorated with `@tf.function`. (This is also a strange behaviour, but I can't reproduce it)\r\n\r\nThe second error is: \r\n\r\n> TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.\r\n\r\nThe problem is that apparently, those two suggestions are incompatible with each other, if I believe your example. \r\n", "I cannot see your context with the current code.", "Sorry, I forgot to add that the error was caused by a Lambda layer. Basically something like the example below caused the error. However, I have been unable to reproduce with a minimal example, so the example below is completely functional. \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef python_control_flow_fn(tensor):\r\n    return tf.concat([t for t in tensor],axis=0)\r\n\r\ninp = tf.keras.Input(shape=(10,))\r\nlayer = tf.keras.layers.Lambda(python_control_flow_fn)(inp)\r\n```\r\n\r\nThe traceback seems to suggest that I can either add `@tf.function` to my Python control flow function, or add `dynamic=True` to the layer.  ", "> OperatorNotAllowedInGraphError: iterating over tf.Tensor is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\nThis is not related to lambda:\r\n````python\r\n@tf.function\r\ndef python_control_flow_fn(tensor):\r\n    return tf.concat([t for t in tensor],axis=0)\r\n\r\ninp = tf.keras.Input(shape=(10,))\r\npython_control_flow_fn(inp)", "Adding the `contributions welcome` label to this issue for further investigation by the community. If you are interested in working on this issue, please leave a comment and I will assign it to you. Thanks!", "Hi,\r\nthis error happens even in eager mode (tf 2.4.1):\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\ninput = tf.keras.Input(shape=())\r\n\r\n@tf.function\r\ndef fn(x):\r\n    return x\r\n\r\noutput = tf.keras.layers.Lambda(fn, dynamic=True)(input)\r\ntf.keras.Model(inputs=input, outputs=output)\r\n```\r\nthrows:\r\n\r\n`RecursionError: maximum recursion depth exceeded while calling a Python object`", "@nikitamaia Can you assign this to @fsx950223 he has already a submitted PR.", "Thanks for the heads up @bhack!\r\n\r\n@fsx950223 can you leave a comment on this thread requesting to be assigned? Github settings won't let me assign someone unless they have commented on the thread :)", "Please assign the issue to me.", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210602, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/4b23f0c1e384137b1666c3a22a4c4ddf/44906.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44906\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44906\">No</a>\n"]}, {"number": 44905, "title": "Conv2dTranspose layer creates additional operators on tflite conversion (Shape, Pack and Strided Slice)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):  Binary\r\n- TensorFlow version (or github SHA if from source):  TF 2..3.0, TF Nightly\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nmodel=tf.keras.models.load_model('amnv3seg.h5')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"mnv3_seg_mew_2.3.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n\r\n```\r\n# Copy and paste the output here.\r\n\r\n# In Tf 2.3.0\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /tmp/tmpc0xpp5l1/assets\r\n10776624\r\n\r\n\r\n\r\n# In TF-Nightly\r\nINFO:tensorflow:Assets written to: /tmp/tmpqt986x_h/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpqt986x_h/assets\r\n10775140\r\n```\r\n\r\n**Failure details**\r\n The converted tflite model creates additonal **shape , pack and strided slice** operators and they are not supported by **gpu delegate** in android. The issue persists in TF 2.3 and TF Nightly.  The issue happens with or without bias parameter in Conv2dTranspose layer.\r\n\r\n**Any other info / logs**\r\n\r\nThe original tf model is based on mobilenetv3 pretrained model and is available only in tf-nightly\r\n[conv2dmodels.zip](https://github.com/tensorflow/tensorflow/files/5546200/conv2dmodels.zip)", "comments": ["Hi Ashwin, \r\n\r\nCould you have a look on this converter related issue? Feel free to reassign.\r\n\r\nThanks,\r\n", "Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e40b2d73d45566455a3ae21d1fdda398/44905.ipynb). Thanks!", "The issue is similar to https://github.com/tensorflow/tensorflow/issues/45090#issuecomment-732389188.\r\nIt seems to be a problem associated with dynamic sizes in keras and the new convereter requires you to fix the batch size of input layer before tflite conversion", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44905\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44905\">No</a>\n"]}, {"number": 44904, "title": "Boolean_mask can't be compiled by XLA on CPU", "body": "\r\n\r\n**System information**\r\n- Ubuntu 18.04, Python 3.6, Tensorflow 2.4.0, compiled on my own:\r\n\r\n**Describe the current behavior**\r\n\r\nBoolean_mask can't be compiled by XLA on CPU:\r\n\r\n```\r\nrepl_used = tf.constant([1., 1.])\r\nsigma = tf.constant(tf.ones((4,1,2,1)))\r\n\r\n@tf.function(experimental_compile=True)\r\ndef repl_example(repl_used, sigma):\r\n    res = tf.boolean_mask(sigma, repl_used, axis=2)\r\n    return res\r\n        \r\nrepl_example(repl_used, sigma)\r\n```\r\n\r\nresults in \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-33-24b6292aa5a3> in <module>\r\n      7     return res\r\n      8 \r\n----> 9 repl_example(repl_used, sigma)\r\n\r\n~/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    784     tracing_count = self._get_tracing_count()\r\n    785     with trace.Trace(self._name) as tm:\r\n--> 786       result = self._call(*args, **kwds)\r\n    787       compiler = \"xla\" if self._experimental_compile else \"nonXla\"\r\n    788       new_tracing_count = self._get_tracing_count()\r\n\r\n~/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    851       # If we did not create any variables the trace we have is good enough.\r\n    852       return self._concrete_stateful_fn._call_flat(\r\n--> 853           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n    854 \r\n    855     def fn_with_cond(inner_args, inner_kwds, inner_filtered_flat_args):\r\n\r\n~/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1918       # No tape is watching; skip to running the function.\r\n   1919       return self._build_call_outputs(self._inference_function.call(\r\n-> 1920           ctx, args, cancellation_manager=cancellation_manager))\r\n   1921     forward_backward = self._select_forward_and_backward_functions(\r\n   1922         args,\r\n\r\n~/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    559               inputs=args,\r\n    560               attrs=attrs,\r\n--> 561               ctx=ctx)\r\n    562         else:\r\n    563           outputs = execute.execute_with_cancellation(\r\n\r\n~/.virtualenvs/py36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_repl_example_77940}} = __inference_repl_example_77940[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\\202\\001\\000\", executor_type=\"\"](dummy_input, dummy_input).\r\nUncompilable nodes:\r\nboolean_mask/Where: unsupported op: No registered 'Where' OpKernel for XLA_CPU_JIT devices compatible with node {{node boolean_mask/Where}}\r\n\tStacktrace:\r\n\t\tNode: __inference_repl_example_77940, function: \r\n\t\tNode: boolean_mask/Where, function: __inference_repl_example_77940\r\n [Op:__inference_repl_example_77940]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nBoolean_mask should be compiled by XLA\r\n", "comments": ["Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/ce46b0b2d38a79899036f1140b27821e/44904.ipynb). Thanks!", "In Tensorflow 2.8, it is successfully compiling without any error. Please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/504d1890b47c3c8edb9151d9d788ca3a/44904.ipynb) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44904\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44904\">No</a>\n"]}, {"number": 44902, "title": "Update math_ops.py", "body": "Add a new exit clause to tf.cast as discussed in this issue https://github.com/tensorflow/tensorflow/issues/44844\r\n\r\nIf the input is not a Tensor then simply return tf.convert_to_tensor.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44902) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "I'm not sure how to add a test. Should I add something to here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/cast_op_test.py\r\n\r\nFor the TODO part. Do you have something in mind? Is there a preferred way to catch this 'casting numbers to string' error?\r\n\r\n", "Yes you can add your test case in that file.", "Sorry if I'm a little slow at getting this done. I've updated math_ops.py in this commit https://github.com/tensorflow/tensorflow/pull/44902/commits/df8fbc5d07e14f7c7130190f2a57c264dc89bfdf\r\n\r\nIn a first attempt to test the precision of tf.cast. I'm not sure if this is a good test or not so would appreciate any feedback thanks!\r\n\r\nAfter that I can look into the casting of strings issue.\r\n\r\nCheers!", "Hmm I thought the CI would run for this but I guess I'm wrong. Is it possible to run the CI on this?", "@Cyberface  Can you please fix build failures ? Thanks!", "I'm about to stop for the Winter break now so I'll leave it to you if you want to close this for now and I'll try and get back to it in the new year.", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Sorry I don't think I can work on this anymore.", "/cc @ravikyram "]}, {"number": 44901, "title": "Rename exec_tools to tools", "body": "Follow up to #43156\r\nBased on https://github.com/bazelbuild/bazel/issues/12059#issuecomment-725641997 exec_tools might no longer be needed and hence can be replaced by tools.\r\nThis fixes various build failures caused by missing environment variables in environments where they are required, e.g. using custom compilers.", "comments": ["@Flamefire can you please Ubuntu Sanity build failures ?", "@rthadur Done. Took me a but to realize buildifier wants me to sort the lines", "Could this still be cherry-picked for 2.4?", "I'd say so. Can you make a PR on the 2.4 branch with the cherrypick please?\r\n\r\nRelease owners will have a final say, but I'd say this can be cherrypicked", "Looks like this one was already cherry-picked but the changes from #43156 were missed. Opened #45264 for that."]}, {"number": 44900, "title": "Use correctly shuffle=False in fit_generator() function", "body": "Shuffle parameter in fit_generator function can take two boolean values: True or False. I want to execute fit_generator without altering the order of the batches at each epoch of the training, so I have to assign 'shuffle=False'. Also, I want to specify the number of steps per epoch, but shuffle parameter has effect when 'steps_per_epoch=None'.\r\n\r\n- Is there any way to solve that?\r\n- How can I use correctly shuffle parameter in fit_generator function?\r\n- Is there any way to check batches at each epoch are not being shuffling?\r\n\r\n", "comments": ["@victor-robotic-24, \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@victor-robotic-24,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44900\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44900\">No</a>\n"]}, {"number": 44899, "title": "SaveModelBundle in TensorFlow is generating error", "body": "someone else also face the same bug.\r\nhttps://stackoverflow.com/questions/63848415/savemodelbundle-in-tensorflow-is-generating-error\r\n\r\nsimilar bug\r\nhttps://github.com/tensorflow/tensorflow/issues/40004\r\n\r\nLInux 18.04\r\nTF 2.3\r\n\r\n```\r\n#include <tensorflow/core/platform/env.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include \"tensorflow/cc/saved_model/tag_constants.h\"// kSavedModelTagServe\r\n#include \"tensorflow/cc/saved_model/loader.h\"//savedmodelbundle\r\n#include <tensorflow/core/public/session_options.h>\r\n#include <tensorflow/core/framework/tensor.h>\r\n\r\n#include <iostream>\r\n\r\nusing namespace std;\r\nusing namespace tensorflow;\r\n\r\n// TODO: unfinished.\r\n\r\nint main()\r\n{\r\n    std::string model_path = \"/boosted_trees_model/output/1605514073\";\r\n    std::cout << \"Found model: \" << tensorflow::MaybeSavedModelDirectory(model_path) << std::endl;\r\n\r\n    tensorflow::SavedModelBundle model_bundle_;\r\n    tensorflow::SessionOptions session_options;\r\n    tensorflow::RunOptions run_options;\r\n    std::unordered_set<std::string> saved_model_tags;\r\n    saved_model_tags.insert(tensorflow::kSavedModelTagServe);\r\n    tensorflow::Status status = tensorflow::LoadSavedModel(session_options,\r\n                                                           run_options,\r\n                                                           model_path,\r\n                                                           saved_model_tags,\r\n                                                           &model_bundle_);\r\n    if (!status.ok()) {\r\n        std::cout << \"LoadSavedModel Failed: \" << status.ToString() << std::endl;\r\n    }\r\n    return 0;\r\n}\r\n```\r\n\r\n```\r\nubuntu@ip-172-31-14-28:~/tf_cpp/build$ cmake .. && make\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/ubuntu/tf_cpp/build\r\nScanning dependencies of target example\r\n[ 50%] Building CXX object CMakeFiles/example.dir/example1.cpp.o\r\n[100%] Linking CXX executable example\r\nCMakeFiles/example.dir/example1.cpp.o: In function `google::protobuf::internal::MapField<tensorflow::MetaGraphDef_SignatureDefEntry_DoNotUse, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::SignatureDef, (google::protobuf::internal::WireFormatLite::FieldType)9, (google::protobuf::internal::WireFormatLite::FieldType)11, 0>::GetMap() const':\r\nexample1.cpp:(.text._ZNK6google8protobuf8internal8MapFieldIN10tensorflow39MetaGraphDef_SignatureDefEntry_DoNotUseENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_12SignatureDefELNS1_14WireFormatLite9FieldTypeE9ELSD_11ELi0EE6GetMapEv[_ZNK6google8protobuf8internal8MapFieldIN10tensorflow39MetaGraphDef_SignatureDefEntry_DoNotUseENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_12SignatureDefELNS1_14WireFormatLite9FieldTypeE9ELSD_11ELi0EE6GetMapEv]+0x14): undefined reference to `google::protobuf::internal::MapFieldBase::SyncMapWithRepeatedField() const'\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/example.dir/build.make:95: recipe for target 'example' failed\r\n```", "comments": ["Closed, due to duplicate with https://github.com/tensorflow/tensorflow/issues/40004", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44899\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44899\">No</a>\n"]}, {"number": 44898, "title": "tf-gpu 1.14 - Enabling eager execution not working.", "body": "Hello guys,\r\n\r\nI am using tf-gpu 1.14 version.\r\n\r\nI want to apply the following loss [BinaryCrossentropy](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/losses/BinaryCrossentropy) . But I am getting the error called \"**Tensor object cannot be converted to numpy**\".\r\n\r\nWhen I searched this error on stackoverflow, I found out that I have to enable eager_execution. So I tried doing that but I get the error \" **AttributeError: module 'tensorflow._api.v1.config' has no attribute 'run_functions_eagerly'**\".\r\n\r\nNote that I cannot upgrade tensorflow and I want to convert the tensor to numpy array. Is there a way around this? or is this a bug? \r\n\r\nI appreciate your help.\r\n", "comments": ["@YashRunwal \r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. \r\nAlso, TF 1.x is not actively supported.It will be good idea to switchover to TF 2.x for better performance.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44897, "title": "Cuda verision not documents", "body": "There is no information about which cuda version required for pip packages\r\ni think must be document as an table for old relases and \r\nfor recent release mustbe selection gui something like pytorch \r\n![resim](https://user-images.githubusercontent.com/8981828/99224100-b8280280-27f6-11eb-9511-44237b0a45a0.png)\r\n\r\n", "comments": ["We have listed the build configurations here https://www.tensorflow.org/install/source#gpu", "hard to find  link  of build configurations\r\nand package dont check cuda version \r\nso it causes a problem \r\nand  some times both 1.x and 2.x version required\r\nlatest 1.x version 1.15.4 relased at Sep 2020 but still used Cuda 10 \r\n\r\n\u0130 think clear explantion / documentation is required\r\n", "I understand your confusion however its important to note that dependencies such as cuda are updated very quickly and it is a challenge to keep various TF versions compatible with different cuda versions.\r\nHence we provide newest TF prebuilt binaries for latest/most recent cuda version and we do not backport new cuda versions to older TF versions due to limited resources unfortunately.\r\nAlso now that we are in TF 2.X new cuda versions will be made available for later TF 2.X versions.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44897\">No</a>\n"]}, {"number": 44896, "title": "Convert using integer-only quantization problem", "body": "This program is an example of tensorflow integer quantization\uff1a\r\n\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n    yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n# Ensure that if any ops can't be quantized, the converter throws an error\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# Set the input and output tensors to uint8 (APIs added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\n\r\n\r\nWhat does (1) and (100) in this line mean?\r\n  [for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):]\r\n", "comments": ["Hi LinYouWei,\r\n\r\n1 is the batch size and 100 is the number of samples.\r\n\r\nNormally TFLite infer sample by sample (i.e. batch_size = 1) and representative dataset takes in a few samples in order to have a good calibration on per layer min / max value.\r\n\r\nFeel free to refer to this doc on the usage of tf.data.Dataset https://www.tensorflow.org/guide/data", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44896\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44896\">No</a>\n"]}, {"number": 44895, "title": "Empty folder", "body": "Hi!\r\nI try to undeertand why the app doesn't work i almost read the FAQ and applyed the olutions but it didn't work, the message said something like: RUN but the  folder is empty\r\n\r\nI hace a Core i7 PC\r\nwith window 10. \r\nI tried to know if the AVX works, i downloaded thee programe to know and it say YES...\r\nI tried the 2.8 version and it was OK but then when i dowload the last one.. the programe never run again T.T \r\n\r\nPlease help!!", "comments": ["No issue template being filled in, closed"]}, {"number": 44894, "title": "Added bf16 support for Sparse_Xent_op.", "body": "Added bf16 support for Sparse_Xent_op.", "comments": ["@penpornk Hello Penporn, This is the updated PR of https://github.com/tensorflow/tensorflow/pull/43920.\r\nI will close the other PR, please review this one instead. Thanks!!!\r\n"]}, {"number": 44893, "title": "model.fit trains on (# of training samples)/batch_size", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["I'm using tensorflow 2.1.0 on windows 10, python 3.7.9, cudatoolkit 10.1.243, cudnn 7.6.5. \r\nThe problem started after using autokeras.\r\nWhen I'm trying to run model.fit, it only trains on part of the training samples: (# of training samples)/batch_size.\r\nFor example, my batch size is 16 and the training samples size is 135,000, so it trains on 8438\r\n[rinacnnGPU_t.txt](https://github.com/tensorflow/tensorflow/files/5544353/rinacnnGPU_t.txt)\r\n\r\n.\r\nAttached code.\r\nWhen executing it prints:\r\n2020-11-15 20:35:16.615885: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-15 20:35:20.084617: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2629556b420 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-11-15 20:35:20.084743: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-11-15 20:35:20.087375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-11-15 20:35:20.135019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-11-15 20:35:20.135068: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-15 20:35:20.143744: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-15 20:35:20.151341: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-15 20:35:20.154467: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-11-15 20:35:20.163847: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-15 20:35:20.168680: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-15 20:35:20.186400: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-15 20:35:20.186549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-15 20:35:21.157331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-15 20:35:21.157396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-15 20:35:21.157416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-11-15 20:35:21.157689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 22016 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-11-15 20:35:21.163602: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x262c257d2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-11-15 20:35:21.163636: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1\r\n2020-11-15 20:35:21.165841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-11-15 20:35:21.165881: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-15 20:35:21.165907: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-15 20:35:21.165929: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-15 20:35:21.165950: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-11-15 20:35:21.165972: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-15 20:35:21.165993: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-15 20:35:21.166014: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-15 20:35:21.166183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-15 20:35:21.167224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-11-15 20:35:21.167264: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-15 20:35:21.167289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-15 20:35:21.167312: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-15 20:35:21.167334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-11-15 20:35:21.167355: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-15 20:35:21.167376: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-15 20:35:21.167397: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-15 20:35:21.167467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-15 20:35:21.167535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-15 20:35:21.167557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-15 20:35:21.167574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-11-15 20:35:21.167736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22016 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-11-15 20:35:26.419022: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-15 20:35:26.736009: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-15 20:35:27.753582: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-11-16 07:35:10.172382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-11-16 07:35:10.172518: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-16 07:35:10.172551: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-16 07:35:10.172576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-16 07:35:10.172600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-11-16 07:35:10.172624: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-16 07:35:10.172647: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-16 07:35:10.172670: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-16 07:35:10.172861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-16 07:35:10.172976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-16 07:35:10.173003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-16 07:35:10.173023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-11-16 07:35:10.173171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 22016 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2020-11-16 07:35:52.132852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro P6000 computeCapability: 6.1\r\ncoreClock: 1.645GHz coreCount: 30 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 403.49GiB/s\r\n2020-11-16 07:35:52.132914: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-11-16 07:35:52.132942: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-11-16 07:35:52.132966: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-16 07:35:52.132991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-11-16 07:35:52.133015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-16 07:35:52.133038: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-11-16 07:35:52.133061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-11-16 07:35:52.133140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-11-16 07:35:52.133231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-16 07:35:52.133255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-11-16 07:35:52.133273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-11-16 07:35:52.133400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 22016 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n1 Physical GPUs, 1 Logical GPU\r\nx_train shape (135000, 57, 57, 3)\r\ny_train shape (135000, 6)\r\n 629/8438 [=>............................] - ETA: 53s - loss: 1.7925 - accuracy: 0.1705", "@rinashalibo,\r\nSince you have specified the batch size as 16 here, the training dataset of size 135,000 is split into 8438 batches (i.e. each batch consisting of 16 samples). Please check the [documentation for batch_size](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) for more information.\r\n\r\nAlso, please take a look at [this gist](https://colab.research.google.com/gist/amahendrakar/8653525cad3f28443a798d1b8fc62fc7/44893.ipynb) and let us know if it helps. Thanks!", "Yes, but the model.fit should still output to screen the number of training\nsamples.\nI have 135,000 samples, and the output to screen is:\n4219/4219 [==============================] - 27s 6ms/step - loss: 1.7902 -\naccuracy: 0.1698 - val_loss: 1.7917 - val_accuracy: 0.1675\nFrom this I understand that only a part of the training samples are being\nused for model.fit.\nBefore the autokeras installation, the output to screen was:\n135000/135000 [======================== ...\n\nOn Tue, Nov 17, 2020 at 3:56 PM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> @rinashalibo <https://github.com/rinashalibo>,\n> Since you have specified the batch size as 16 here, the training dataset\n> of size 135,000 is split into 8438 batches (i.e. each batch consisting of\n> 16 samples). Please check the documentation for batch_size\n> <https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit> for more\n> information.\n>\n> Also, please take a look at this gist\n> <https://colab.research.google.com/gist/amahendrakar/8653525cad3f28443a798d1b8fc62fc7/44893.ipynb>\n> and let us know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44893#issuecomment-728943878>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANLYL7MYQ4LKONUP2FCCBPTSQJ6IJANCNFSM4TWXEEMQ>\n> .\n>\n\n\n-- \n\n*The University* <http://facebook.com/theuniversity> - My favourite *Jazz\nBand* in Tel Aviv *- **\u05d4\u05d0\u05d5\u05e0\u05d9\u05d1\u05e8\u05e1\u05d9\u05d8\u05d4*  <http://facebook.com/theuniversity>\n      \u266a\u266b\n", "> Before the autokeras installation, the output to screen was: 135000/135000 [======================== ...\r\n\r\n@rinashalibo,\r\nCould you please provide a minimal code snippet to reproduce this issue on our end? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44893\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44893\">No</a>\n"]}, {"number": 44892, "title": "Create crunch42-analysis.yml", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44892) for more info**.\n\n<!-- need_sender_cla -->", "@bajecogem83  Can you please sign CLA. Thanks!", "@bajecogem83 Can you please sign CLA. Thanks!", "@bajecogem83 Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 44891, "title": "While implementing SEGNET, getting error of \"python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 293, in _get_input_channel     raise ValueError('The channel dimension of the inputs ' ValueError: The channel dimension of the inputs should be defined. Found `None`.\"", "body": "I am implementing SEGNET segmentation Network in python but getting the following error, \r\n\r\n**_Traceback (most recent call last):\r\n  File \"/scratch/pkasar.dbatu/training/NEW_SEGNET_updated_on_16_11_20.py\", line 370, in <module>\r\n    model=segnet(input_shape=(256,256,3),n_labels=1)\r\n  File \"/scratch/pkasar.dbatu/training/NEW_SEGNET_updated_on_16_11_20.py\", line 161, in segnet\r\n    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 897, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 2416, in _maybe_build\r\n    self.build(input_shapes)  # pylint:disable=not-callable\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 153, in build\r\n    input_channel = self._get_input_channel(input_shape)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\", line 293, in _get_input_channel\r\n    raise ValueError('The channel dimension of the inputs '\r\nValueError: The channel dimension of the inputs should be defined. Found `None`._**\r\n\r\nTensorflow image is:-\r\ntensorflow                2.2.0 \r\ntensorflow-gpu            2.2.0\r\nkeras-base                2.4.3                \r\nkeras-gpu                 2.4.3     \r\npython                    3.7.9\r\nPlease help me out\r\nThank you in advance\r\n\r\n@tensorflow-copybara  , @tensorflow-jenkins  @TensorFlow-MKL @tensorflowbutler @tensorflower-gardener \r\n\r\n\r\n\r\n\r\n\r\nThe code snippet is as follows:-\r\n\r\n\r\n```from keras.layers import Input\r\nfrom keras.layers.convolutional import Convolution2D\r\nfrom keras.layers.core import Activation, Reshape\r\nfrom keras.layers.normalization import BatchNormalization\r\nfrom keras.models import Model\r\nfrom keras import backend as K\r\nfrom keras.layers import Layer\r\nclass MaxPoolingWithArgmax2D(Layer):\r\n    def __init__(self, pool_size=(2, 2), strides=(2, 2), padding=\"same\", **kwargs):\r\n        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\r\n        self.padding = padding\r\n        self.pool_size = pool_size\r\n        self.strides = strides\r\n\r\n    def call(self, inputs, **kwargs):\r\n        padding = self.padding\r\n        pool_size = self.pool_size\r\n        strides = self.strides\r\n        if K.backend() == \"tensorflow\":\r\n            ksize = [1, pool_size[0], pool_size[1], 1]\r\n            padding = padding.upper()\r\n            strides = [1, strides[0], strides[1], 1]\r\n            output, argmax = K.tf.nn.max_pool_with_argmax(\r\n                inputs, ksize=ksize, strides=strides, padding=padding\r\n            )\r\n        else:\r\n            errmsg = \"{} backend is not supported for layer {}\".format(\r\n                K.backend(), type(self).__name__\r\n            )\r\n            raise NotImplementedError(errmsg)\r\n        argmax = K.cast(argmax, K.floatx())\r\n        return [output, argmax]\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        ratio = (1, 2, 2, 1)\r\n        output_shape = [\r\n            dim // ratio[idx] if dim is not None else None\r\n            for idx, dim in enumerate(input_shape)\r\n        ]\r\n        output_shape = tuple(output_shape)\r\n        return [output_shape, output_shape]\r\n\r\n    def compute_mask(self, inputs, mask=None):\r\n        return 2 * [None]\r\nclass MaxUnpooling2D(Layer):\r\n    def __init__(self, size=(2, 2), **kwargs):\r\n        super(MaxUnpooling2D, self).__init__(**kwargs)\r\n        self.size = size\r\n    def call(self, inputs, output_shape=None):\r\n        updates, mask = inputs[0], inputs[1]\r\n        with K.tf.variable_scope(self.name):\r\n            mask = K.cast(mask, \"int32\")\r\n            input_shape = K.tf.shape(updates, out_type=\"int32\")\r\n            #  calculation new shape\r\n            if output_shape is None:\r\n                output_shape = (\r\n                    input_shape[0],\r\n                    input_shape[1] * self.size[0],\r\n                    input_shape[2] * self.size[1],\r\n                    input_shape[3],\r\n                )\r\n            self.output_shape1 = output_shape\r\n            # calculation indices for batch, height, width and feature maps\r\n            one_like_mask = K.ones_like(mask, dtype=\"int32\")\r\n            batch_shape = K.concatenate([[input_shape[0]], [1], [1], [1]], axis=0)\r\n            batch_range = K.reshape(\r\n                K.tf.range(output_shape[0], dtype=\"int32\"), shape=batch_shape\r\n            )\r\n            b = one_like_mask * batch_range\r\n            y = mask // (output_shape[2] * output_shape[3])\r\n            x = (mask // output_shape[3]) % output_shape[2]\r\n            feature_range = K.tf.range(output_shape[3], dtype=\"int32\")\r\n            f = one_like_mask * feature_range\r\n            # transpose indices & reshape update values to one dimension\r\n            updates_size = K.tf.size(updates)\r\n            indices = K.transpose(K.reshape(K.stack([b, y, x, f]), [4, updates_size]))\r\n            values = K.reshape(updates, [updates_size])\r\n            ret = K.tf.scatter_nd(indices, values, output_shape)\r\n            return ret\r\n    def compute_output_shape(self, input_shape):\r\n        mask_shape = input_shape[1]\r\n        return (\r\n            mask_shape[0],\r\n            mask_shape[1] * self.size[0],\r\n            mask_shape[2] * self.size[1],\r\n            mask_shape[3],\r\n        )\r\ndef segnet(input_shape, n_labels, kernel=3, pool_size=(2, 2), output_mode=\"softmax\"):\r\n    # encoder\r\n    inputs = Input(shape=input_shape)\r\n\r\n    conv_1 = Convolution2D(64, (kernel, kernel), padding=\"same\")(inputs)\r\n    conv_1 = BatchNormalization()(conv_1)\r\n    conv_1 = Activation(\"relu\")(conv_1)\r\n    conv_2 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_1)\r\n    conv_2 = BatchNormalization()(conv_2)\r\n    conv_2 = Activation(\"relu\")(conv_2)\r\n    pool_1, mask_1 = MaxPoolingWithArgmax2D(pool_size)(conv_2)\r\n    conv_3 = Convolution2D(128, (kernel, kernel), padding=\"same\")(pool_1)\r\n    conv_3 = BatchNormalization()(conv_3)\r\n    conv_3 = Activation(\"relu\")(conv_3)\r\n    conv_4 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_3)\r\n    conv_4 = BatchNormalization()(conv_4)\r\n    conv_4 = Activation(\"relu\")(conv_4)\r\n\r\n    pool_2, mask_2 = MaxPoolingWithArgmax2D(pool_size)(conv_4)\r\n\r\n    conv_5 = Convolution2D(256, (kernel, kernel), padding=\"same\")(pool_2)\r\n    conv_5 = BatchNormalization()(conv_5)\r\n    conv_5 = Activation(\"relu\")(conv_5)\r\n    conv_6 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_5)\r\n    conv_6 = BatchNormalization()(conv_6)\r\n    conv_6 = Activation(\"relu\")(conv_6)\r\n    conv_7 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_6)\r\n    conv_7 = BatchNormalization()(conv_7)\r\n    conv_7 = Activation(\"relu\")(conv_7)\r\n\r\n    pool_3, mask_3 = MaxPoolingWithArgmax2D(pool_size)(conv_7)\r\n\r\n    conv_8 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_3)\r\n    conv_8 = BatchNormalization()(conv_8)\r\n    conv_8 = Activation(\"relu\")(conv_8)\r\n    conv_9 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_8)\r\n    conv_9 = BatchNormalization()(conv_9)\r\n    conv_9 = Activation(\"relu\")(conv_9)\r\n    conv_10 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_9)\r\n    conv_10 = BatchNormalization()(conv_10)\r\n    conv_10 = Activation(\"relu\")(conv_10)\r\n\r\n    pool_4, mask_4 = MaxPoolingWithArgmax2D(pool_size)(conv_10)\r\n\r\n    conv_11 = Convolution2D(512, (kernel, kernel), padding=\"same\")(pool_4)\r\n    conv_11 = BatchNormalization()(conv_11)\r\n    conv_11 = Activation(\"relu\")(conv_11)\r\n    conv_12 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_11)\r\n    conv_12 = BatchNormalization()(conv_12)\r\n    conv_12 = Activation(\"relu\")(conv_12)\r\n    conv_13 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_12)\r\n    conv_13 = BatchNormalization()(conv_13)\r\n    conv_13 = Activation(\"relu\")(conv_13)\r\n\r\n    pool_5, mask_5 = MaxPoolingWithArgmax2D(pool_size)(conv_13)\r\n    print(\"Build enceder done..\")\r\n\r\n    # decoder\r\n\r\n    unpool_1 = MaxUnpooling2D(pool_size)([pool_5, mask_5])\r\n\r\n    conv_14 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_1)\r\n    conv_14 = BatchNormalization()(conv_14)\r\n    conv_14 = Activation(\"relu\")(conv_14)\r\n    conv_15 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_14)\r\n    conv_15 = BatchNormalization()(conv_15)\r\n    conv_15 = Activation(\"relu\")(conv_15)\r\n    conv_16 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_15)\r\n    conv_16 = BatchNormalization()(conv_16)\r\n    conv_16 = Activation(\"relu\")(conv_16)\r\n\r\n    unpool_2 = MaxUnpooling2D(pool_size)([conv_16, mask_4])\r\n\r\n    conv_17 = Convolution2D(512, (kernel, kernel), padding=\"same\")(unpool_2)\r\n    conv_17 = BatchNormalization()(conv_17)\r\n    conv_17 = Activation(\"relu\")(conv_17)\r\n    conv_18 = Convolution2D(512, (kernel, kernel), padding=\"same\")(conv_17)\r\n    conv_18 = BatchNormalization()(conv_18)\r\n    conv_18 = Activation(\"relu\")(conv_18)\r\n    conv_19 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_18)\r\n    conv_19 = BatchNormalization()(conv_19)\r\n    conv_19 = Activation(\"relu\")(conv_19)\r\n\r\n    unpool_3 = MaxUnpooling2D(pool_size)([conv_19, mask_3])\r\n\r\n    conv_20 = Convolution2D(256, (kernel, kernel), padding=\"same\")(unpool_3)\r\n    conv_20 = BatchNormalization()(conv_20)\r\n    conv_20 = Activation(\"relu\")(conv_20)\r\n    conv_21 = Convolution2D(256, (kernel, kernel), padding=\"same\")(conv_20)\r\n    conv_21 = BatchNormalization()(conv_21)\r\n    conv_21 = Activation(\"relu\")(conv_21)\r\n    conv_22 = Convolution2D(128, (kernel, kernel), padding=\"same\")(conv_21)\r\n    conv_22 = BatchNormalization()(conv_22)\r\n    conv_22 = Activation(\"relu\")(conv_22)\r\n\r\n    unpool_4 = MaxUnpooling2D(pool_size)([conv_22, mask_2])\r\n\r\n    conv_23 = Convolution2D(128, (kernel, kernel), padding=\"same\")(unpool_4)\r\n    conv_23 = BatchNormalization()(conv_23)\r\n    conv_23 = Activation(\"relu\")(conv_23)\r\n    conv_24 = Convolution2D(64, (kernel, kernel), padding=\"same\")(conv_23)\r\n    conv_24 = BatchNormalization()(conv_24)\r\n    conv_24 = Activation(\"relu\")(conv_24)\r\n\r\n    unpool_5 = MaxUnpooling2D(pool_size)([conv_24, mask_1])\r\n\r\n    conv_25 = Convolution2D(64, (kernel, kernel), padding=\"same\")(unpool_5)\r\n    conv_25 = BatchNormalization()(conv_25)\r\n    conv_25 = Activation(\"relu\")(conv_25)\r\n\r\n    conv_26 = Convolution2D(n_labels, (1, 1), padding=\"valid\")(conv_25)\r\n    conv_26 = BatchNormalization()(conv_26)\r\n    conv_26 = Reshape(\r\n        (input_shape[0] * input_shape[1], n_labels),\r\n        input_shape=(input_shape[0], input_shape[1], n_labels),\r\n    )(conv_26)\r\n\r\n    outputs = Activation(output_mode)(conv_26)\r\n    print(\"Build decoder done..\")\r\n\r\n    model = Model(inputs=inputs, outputs=outputs, name=\"SegNet\")\r\n\r\n    return model\r\n\r\nmodel=segnet(input_shape=(256,256,3),n_labels=1)```\r\n\r\n", "comments": ["@pankajkasar \r\nPlease provide with indented code such that we can replicate the issue or if possible share a colab gist with the error, can you try with tf nightly or 2.4 and let us know if the issue exists.", "The following only line which I forget to add:\r\n`model=segnet(input_shape=(256,256,3),n_labels=1)`\r\n\r\nI also added in above code. \r\nThanks for reply.", "@pankajkasar \r\nPlease move the issue to closed status is resolved.", "@Saduf2019 TF 2.4.0 is not yet available to us for download and use. What else option?? I will immediately close issue if i get all answers.", "gist:93f2dafae3cb5a0da41746960bc6718a\r\n\r\nCreated public gist", "Finally I found solution on above error. We need to change **class MaxUnpooling2D** definition as follows:-\r\n\r\n```\r\nclass MaxUnpooling2D(Layer):\r\ndef __init__(self, size=(2, 2), **kwargs):\r\n    super(MaxUnpooling2D, self).__init__(**kwargs)\r\n    self.size = size\r\n\r\ndef call(self, inputs, output_shape=None):\r\n    updates, mask = inputs[0], inputs[1]\r\n    with tf.compat.v1.variable_scope(self.name):\r\n        mask = K.cast(mask, 'int32')\r\n        input_shape = tf.shape(updates, out_type='int32')\r\n        #print(updates.shape)\r\n        #print(mask.shape)\r\n        if output_shape is None:\r\n            output_shape = (\r\n                input_shape[0],\r\n                input_shape[1] * self.size[0],\r\n                input_shape[2] * self.size[1],\r\n                input_shape[3])\r\n\r\n        ret = tf.scatter_nd(K.expand_dims(K.flatten(mask)),\r\n                              K.flatten(updates),\r\n                              [K.prod(output_shape)])\r\n\r\n        input_shape = updates.shape\r\n        out_shape = [-1,\r\n                     input_shape[1] * self.size[0],\r\n                     input_shape[2] * self.size[1],\r\n                     input_shape[3]]\r\n    return K.reshape(ret, out_shape)\r\n\r\ndef get_config(self):\r\n    config = super().get_config().copy()\r\n    config.update({\r\n        'size': self.size\r\n    })\r\n    return config\r\n\r\ndef compute_output_shape(self, input_shape):\r\n    mask_shape = input_shape[1]\r\n    return (\r\n            mask_shape[0],\r\n            mask_shape[1]*self.size[0],\r\n            mask_shape[2]*self.size[1],\r\n            mask_shape[3]\r\n            )\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44891\">No</a>\n"]}, {"number": 44890, "title": "yolov3-tiny tflite model of shape [1, 2535, 85] representation", "body": "I found a tflite model of shape [1, 2535, 85] with 80 classes.\r\n\r\nI am aware that each item in the array represents each bounding box information.\r\n\r\nSo yolo-v3 tiny model uses two scales `13 x 13` and` 26 x 26`\r\n\r\n3 * (13 * 13) = 507\r\n3 * (26 * 26)  = 2028 +\r\n                       = 2535\r\n\r\nEach cell has 3 bounding box information.\r\n\r\nI'm not sure how are this information arranged in the array of 2535. \r\n\r\n\r\nIf the 0th index represents the first cell (0,0) of 13 scale for the first bounding box\r\n\r\nwhat does the 1st index represent then?\r\n\r\n**Does it represent the second bounding box information for the first cell (0,0) of 13 scale**\r\n\r\nor\r\n\r\n**Does it represent the first bounding box information for the second cell (0,1) of 13 scale?**", "comments": ["TBH, its difficult to gauge what the model tensors mean without looking at the TF code that generated the model :-).\r\nWhat is the source of the model? Is it from some TF repo?", "Could you please update as per above comment", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44890\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44890\">No</a>\n"]}, {"number": 44886, "title": "Bazel Build Issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Kubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 8.4\r\n- CUDA/cuDNN version: 11.1/8\r\n- GPU model and memory: 2070 / 8GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nTrying to install tensorflow from source, I get the following error\r\n\r\nERROR: /home/pablo/.cache/bazel/_bazel_pablo/73a3c430b087017173c0bbcfb3b9758b/external/llvm-project/llvm/BUILD:1852:1: C++ compilation of rule '@llvm-project//llvm:Core' failed (Exit 1)\r\nexternal/llvm-project/llvm/lib/IR/Function.cpp:46:10: fatal error: llvm/IR/IntrinsicsVE.h: No such file or directory\r\n #include \"llvm/IR/IntrinsicsVE.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 64.162s, Critical Path: 26.84s\r\nINFO: 593 processes: 593 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nTensorflow configuration went ok, no problems. I compile with Bazel, and I get the error: bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Can you clone again please? It is likely you have cloned at a time when LLVM dependency (coming via MLIR) broke us.", "Yes, it works now, thank you!", "@parayamelo,\r\nThank you for the update. Marking the issue as closed, as it it resolved. Please feel free to re-open the issue if necessary. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44886\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44886\">No</a>\n"]}, {"number": 44885, "title": "Could not create cudnn handle TF2 GPU", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cudart64_101.dll / 7.6.0.64\r\n- GPU model and memory: RTX 2080\r\n\r\n\r\n**Describe the current behavior**\r\n2020-11-15 16:46:03.303728: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-11-15 16:46:03.307912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-11-15 16:46:03.308022: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\nI checked some \"Could not create cudnn handle\" errors but I couldn't find any solutions yet. And I installed also exactly same edition as tf mentioned in their sites.\r\n-NVIDIA\u00ae GPU drivers \u2014CUDA\u00ae 10.1 requires 418.x or higher.\r\n-CUDA\u00ae Toolkit \u2014TensorFlow supports CUDA\u00ae 10.1 (TensorFlow >= 2.1.0)\r\n-CUPTI ships with the CUDA\u00ae Toolkit.\r\n-cuDNN SDK 7.6 (see cuDNN versions).\r\n-(Optional) TensorRT 6.0 to improve latency and throughput for inference on some models.\r\n\r\nI'm trying to use my simple CNN code;\r\n\r\n```\r\n# Convolutional Neural Network\r\n\r\n# Importing Libraries\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\n# Part 1 - Data Preprocessing\r\n\r\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\r\n                                   shear_range = 0.2,\r\n                                   zoom_range = 0.2,\r\n                                   horizontal_flip = True)\r\n\r\ntraining_set = train_datagen.flow_from_directory('C:/Users/batuh/OneDrive/Masa\u00fcst\u00fc/Udemy Courses/Deep Learning A-Z/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/dataset/training_set',\r\n                                                 target_size = (64, 64),\r\n                                                 batch_size = 32,\r\n                                                 class_mode = 'binary')\r\n\r\n## Preprocessing the Test set\r\n\r\ntest_datagen = ImageDataGenerator(rescale = 1./255)\r\ntest_set = test_datagen.flow_from_directory('C:/Users/batuh/OneDrive/Masa\u00fcst\u00fc/Udemy Courses/Deep Learning A-Z/Volume 1 - Supervised Deep Learning/Part 2 - Convolutional Neural Networks (CNN)/dataset/test_set',\r\n                                                 target_size = (64, 64),\r\n                                                 batch_size = 32,\r\n                                                 class_mode = 'binary')\r\n\r\n# Part 2 - Building the CNN\r\n## Initializing the CNN\r\n\r\ncnn = tf.keras.models.Sequential()\r\n\r\n## Step 1 - Convolution\r\n\r\ncnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', \r\n                               input_shape = [64, 64, 3]))\r\n\r\n## Step 2 - Pooling\r\n\r\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\r\n\r\n### Adding Second Conv. Layer\r\n\r\ncnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\r\ncnn.add(tf.keras.layers.MaxPooling2D(pool_size = 2, strides = 2))\r\n\r\n## Step 3 - Flattening\r\n\r\ncnn.add(tf.keras.layers.Flatten())\r\n\r\n## Step 4 - Full Connection\r\n\r\ncnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\r\n\r\n## Step 5 - Output Layer\r\n\r\ncnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\r\n\r\n# Part 3 - Training the CNN\r\n## Compiling the CNN\r\n#with tf.device('/GPU:0'):\r\ncnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\n## Training the CNN\r\ncnn.fit(x = training_set, validation_data = test_set, epochs = 50)\r\n```", "comments": ["I am having the same problem. Setting the kernel size to 2 instead of of 3 makes it work. \r\n", "> I am having the same problem. Setting the kernel size to 2 instead of of 3 makes it work.\r\nThis time I've got that one;\r\n```\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-11-15 18:53:39.858320: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node sequential/conv2d/Conv2D}}]]\r\n```", "I think you should upgrade your tensorflow version to 2.3.0 or 2.4.0rc1", "That reminds me of a common error with RTX cards.\r\nDid you try to set the env variable TF_FORCE_GPU_ALLOW_GROWTH to \"true\" ?\r\n\r\nCheck this:\r\nhttps://github.com/tensorflow/tensorflow/issues/37559", "Also try adding this to the start of this projects just after upgrading your tensorflow  version and importing it \r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)`", "Thanks for your help. I would like to summarize the solution briefly in case others have problems and investigate that problem.\r\n`physical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\nThis section must be added right after the import section. However, for usage of this code, people who face that issue, should use tensorflow 2.3 at least. I've double checked from [here](https://www.tensorflow.org/api_docs/python/tf/config), config added on Tensorflow 2.3.0. Again I really appreciated you both @king398 and @jnd77 . Have a nice day you all, <3 Cheers!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44885\">No</a>\n", "@BestSithInEU Glad that you found it useful ", "Hey I'm using TF 2.2.0 with CUDA 10.1 I think it is compatible \r\nI'm getting the same error even after performing the function to limit memory growth\r\n\r\n> Also try adding this to the start of this projects just after upgrading your tensorflow version and importing it\r\n> `physical_devices = tf.config.list_physical_devices('GPU') tf.config.experimental.set_memory_growth(physical_devices[0], True)`\r\n\r\n E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n", "@RohanRatwani which gpu you are using and please show your code in the issue and the full log file. Also try upgrading your tensorflow version to 2.3.0\r\n`pip install --upgrade tensorflow-gpu==2.3.0`. If you read the above answer you should tensorflow version 2.3.0 is minimum required for this"]}, {"number": 44884, "title": "tf.io.gfile can read but can't write file ", "body": "**System information**\r\n\r\nRunning on Ai Platform using Tensorflow Cloud.\r\n- Docker container: tensorflow/tensorflow:2.3.1\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\nUsing tf.io to read/write files to Google Cloud Storage. I can listdir, read files but not write.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nAssuming \"gs://bucket/folder\" existes, and \"gs://bucket/folder/folder1\" doesn't exist.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.io.gfile.exists(\"gs://bucket/folder/folder1\"))  # prints false\r\ntf.io.gfile.makedirs(\"gs://bucket/folder/folder1\")\r\nprint(tf.io.gfile.exists(\"gs://bucket/folder/folder1\"))  # prints true\r\n```\r\n\r\nSo it seems like the code worked, but when I have a look at the bucket, I can't find the file. and when I run the script again, it has the same output, meaning it didn't find the file.\r\n\r\nI did not touch the permissions as Ai Platform has required permissions. Also, the Ai Platform job and the bucket are in the same region.\r\n\r\nI'll be happy to provide more information or any logs if relevant. I also did a small test on Compute Engine and had the same issue.\r\n", "comments": ["Because `GCS` [stores](https://cloud.google.com/storage/docs/gsutil/addlhelp/HowSubdirectoriesWork) data differently from usual filesystem like `posix`, we are forcing the behavior of `posix` upon `GCS` and sometime, the filesystem does not work as intented, especially with directory ( since `GCS` does not have the concept of directory ). \n\nThe directory you just created is an empty file and it might not show on `GCS` server. I think you could try `tf.io.gfile.Gfile.write` to see if you could actual write to the server or not.", "Thank you @vnvo2409 for the explanation, I guess tf.io is not stable for Google Cloud Storage and I shouldn't really on it.\r\n\r\nAlthough this is strange, on [Tensorflow's API docs](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) it says that this is exactly what Gfile is here for - to support GCS:\r\n\r\n\" The C++ FileSystem API supports multiple file system implementations, including local files, Google Cloud Storage (using a gs:// prefix, and HDFS (using an hdfs:// prefix). \"\r\n\r\nThere is documentation and examples using tf.io for GCS, [here is one example](https://www.tensorflow.org/official_models/fine_tuning_bert#resources). Gfile is also one of the [suggested ways to use GCS in Google AI Platform](https://cloud.google.com/ai-platform/training/docs/working-with-cloud-storage#read-during-training).\r\n\r\n\r\nI have done some tests with the following and no success:\r\n\r\n```\r\ntf.io.write_file(file_name, data)\r\n```\r\n\r\n```\r\nwith tf.io.gfile.GFile(file_name, mode=\"w+\") as f:\r\n    f.write(data)\r\n```\r\n\r\nNo errors from Tensorflow, but the files are not saved in GCS.", "There are some bugs in GCS. Internally Google uses different filesystem implementations, so GCS, Hadoop, etc. don't get as tested as Posix/Windows/internal ones. That's why we're modularizing them now so that community can take control over the cloud filesystems.\r\n\r\nNow, regarding your code in the last comment, is `file_name` starting with `gs://` in the GCS example? There might also be a need for a wait before the file gets written. If that still does not work, this is a serious bug (please also paste full version of `file_name`, modulo hiding bucket name and path prefix, if you want to hide them)", "@zvikarp \r\n\r\n> I guess tf.io is not stable for Google Cloud Storage and I shouldn't really on it.\r\n\r\nThis isn't what I mean. `tf.io` is stable enough for Google Cloud Storage. I mean It will have some weird [problems](https://github.com/tensorflow/tensorflow/pull/41282#issuecomment-657236354) ( however, they are very rare cases and require an unusual process to reproduce ) with directory operation ( especially `isdir` ) but with reading/writing file, there should be no problem.\r\n\r\nIndeed, I just tested `tf.io.write_file` and I did not have any problem.\r\n```python\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.4.0-rc0'\r\n>>> tf.io.write_file(\"gs://tfvnvo/tf.txt\", \"tensorflow\")\r\n2020-11-19 12:51:47.994065: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>>\r\n```\r\n<img width=\"903\" alt=\"image\" src=\"https://user-images.githubusercontent.com/17511625/99627099-3b02c480-2a66-11eb-82ee-3184cb935255.png\">\r\n\r\n---\r\n\r\nRegarding your problem, I think it is related to your bucket configurations (  [`IAM`, `ACL`](https://cloud.google.com/storage/docs/access-control) or your credentials ). In order to confirm that your bucket doesn't have any problems, I think you could try and see if you could write to your bucket with GCS Python Client:\r\n``` python\r\n# pip install google-cloud-storage\r\n>>> from google.cloud import storage\r\n>>> client = storage.Client()\r\n>>> bucket = client.get_bucket('tfvnvo')\r\n>>> blob = bucket.blob('python-gcs')\r\n>>> blob.upload_from_string('python-gcs')\r\n```\r\n\r\n---\r\n\r\n> No errors from Tensorflow, but the files are not saved in GCS.\r\n\r\nHandling error in TensorFlow GCS is a little bit outdated ( It was writtern 4 years ago ) so it might be the reason why TensorFlow does not catch any error. However, I am working on a modular filesystem plugin which will replace the current filesystem and it will have a much better handling error mechanic for sure.", "Sorry for the delay.\r\n\r\nIn the last few days, I re-tried and all the commands worked for me with GAE and GCS. I suspect it might have been an issue with Google AI Platform (that I tried using earlier), the permissions it had, or the way I implemented the modal.\r\n\r\nAnyway, sorry for the destruction, I indeed learned a lot form this conversion.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44884\">No</a>\n", "as @vnghia mentioned above, you have to verify if you have (read and write) access to the GCS. \r\nOnce you can access the basket, writing file back to gcs buck can be like:\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.python.lib.io import file_io\r\na = np.random.rand(100, 100, 4)\r\nb = np.random.rand(100, 100, 1)\r\nBUCKET_NAME = f\"{your-bucket-name}\"\r\nfout = f\"gs://{BUCKET_NAME}/data/train/test.npz\"\r\nnp.savez(file_io.FileIO(fout, 'w+'), x=a, y=b)\r\n```\r\nRead more on [StackOverflow](https://stackoverflow.com/questions/41633748/load-numpy-array-in-google-cloud-ml-job)"]}, {"number": 44883, "title": "Can you please provide Nvidia Driver version for all tensorflow version? Is tensorflow 1.15.0 compatible with 410 Driver version?", "body": "Reusing code across different machines is difficult. The problem is that even if one could install a compatible tensorflow version, it's challenging to run code due to, `RuntimeError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version` error.\r\nThe driver update is not accessible if one is using a remote server and I cant run the code in the server. Wastage of resources.\r\nIn my case, `nvidia-smi` says the driver version is 410 and has tensorflow version 1.12.0 which is an older version compared to my code developed. Unfortunately, I can't run my code developed in my local drive belonging to tensorflow version 1.14.0 or above. ", "comments": ["@WrathofBhuvan11,\r\nTensorFlow v1.15 is compatible with CUDA 10.0 and cuDNN 7.4. Please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) for more information. \r\n\r\nAnd please check 'Table 2' from [this link](https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#title-new-features) for the respective CUDA toolkit and compatible driver versions. Thanks!", "TF pre built binaries follows [Nvidia cudnn documentation](https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html).", "Try to install focal fossa it give's great updates.\r\nMy pc runs on intel from mainboard :-)\r\nFor Nvidia for Ubuntu the way to go ;-)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44883\">No</a>\n"]}, {"number": 44882, "title": "AttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'", "body": "I have a problem with the conversion model using template code.\r\nI am running on Jupyter Notebook\r\nThis is the template code\uff1a\r\n\r\n# Convert the model to the TensorFlow Lite format without quantization\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\r\nmodel_no_quant_tflite = converter.convert()\r\n\r\n# Save the model to disk\r\nopen(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)\r\n# Convert the model to the TensorFlow Lite format with quantization\r\ndef representative_dataset():\r\n  for i in range(500):\r\n    yield([x_train[i].reshape(1, 1)])\r\n# Set the optimization flag.\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# Enforce integer only quantization\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n# Provide a representative dataset to ensure we quantize correctly.\r\nconverter.representative_dataset = representative_dataset\r\nmodel_tflite = converter.convert()\r\n\r\n# Save the model to disk\r\nopen(MODEL_TFLITE, \"wb\").write(model_tflite)\r\n\r\nOutput result\uff1a\r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-50-0b17c299ecad> in <module>()\r\n     18 # Provide a representative dataset to ensure we quantize correctly.\r\n     19 converter.representative_dataset = representative_dataset\r\n---> 20 model_tflite = converter.convert()\r\n     21 \r\n     22 # Save the model to disk\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py in convert(self)\r\n    745         self.inference_input_type, self.inference_output_type)\r\n    746     if flags_modify_model_io_type:\r\n--> 747       result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n    748 \r\n    749     if self._experimental_sparsify_model:\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py in modify_model_io_type(model, inference_input_type, inference_output_type)\r\n    833     return model\r\n    834 \r\n--> 835   model_object = _convert_model_from_bytearray_to_object(model)\r\n    836 \r\n    837   if len(model_object.subgraphs) > 1:\r\n\r\nF:\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\util.py in _convert_model_from_bytearray_to_object(model_bytearray)\r\n    570 def _convert_model_from_bytearray_to_object(model_bytearray):\r\n    571   \"\"\"Converts a tflite model from a bytearray into a parsable object.\"\"\"\r\n--> 572   model_object = schema_fb.Model.GetRootAsModel(model_bytearray, 0)\r\n    573   model_object = schema_fb.ModelT.InitFromObj(model_object)\r\n    574   model_object = copy.deepcopy(model_object)\r\n\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\nI want to know the reason", "comments": ["Hi Yan,\r\n\r\nWhich version are you running? Could you provide a list steps to reproduce your issue?\r\nAlso if possible, please also check if tensorflow/lite/python/schema_py_generated file under dist packages exists or have Model class defined.\r\n\r\nThanks,\r\n\r\nHi Maghna,\r\n\r\nI re-assign this bug to you in case you have seen similar issues. Feel free to reassign.\r\n\r\nThanks,", "This issue should be fixed with the latest TF code (tf-nightly). Could you try with that let us know if it works?\r\n\r\neg:\r\n```\r\npip install tf-nightly\r\n```\r\n\r\n(you need to restart your runtime if you are using https://colab.research.google.com)", "> Hi Yan,\r\n> \r\n> Which version are you running? Could you provide a list steps to reproduce your issue?\r\n> Also if possible, please also check if tensorflow/lite/python/schema_py_generated file under dist packages exists or have Model class defined.\r\n> \r\n> Thanks,\r\n> \r\n> Hi Maghna,\r\n> \r\n> I re-assign this bug to you in case you have seen similar issues. Feel free to reassign.\r\n> \r\n> Thanks,\r\n\r\nFirstly,thank you very much for your message.\r\nWhat I run in jupyter notebook is   :   ! pip install tensorflow==2.4.0rc0 , so tensorflow should be version 2.4.0and python is 3.7.0\r\nThe source code of the model I run:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb\r\nChapter title is \" Generate a TensorFlow Lite Model\"\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-23-448f10b1b0c8> in <module>()\r\n     18 # Provide a representative dataset to ensure we quantize correctly.\r\n     19 converter.representative_dataset = representative_dataset\r\n---> 20 model_tflite = converter.convert()\r\n     21 \r\n     22 # Save the model to disk\r\n\r\nAttributeError: module 'tensorflow.lite.python.schema_py_generated' has no attribute 'Model'\r\nBut my schema_py_generated file has no code.\r\nI am looking forward to your reply", "Hi Yan,\r\n\r\nAs Meghna mentioned, please try tf-nightly instead of the RC version for now.\r\n\r\nhttps://pypi.org/project/tf-nightly/", "> Hi Yan,\r\n> \r\n> As Meghna mentioned, please try tf-nightly instead of the RC version for now.\r\n> \r\n> https://pypi.org/project/tf-nightly/\r\n\r\nMy tf-nightly is already 2.5.0 dev version, but still the same problem", "I need the code under the schema_py_generated file", "@YANxu-666 I was able to run this on tf-nightly with version `2.5.0-dev20201117` (as of today). It also worked for `2.4.0rc0` Here is the link: https://colab.research.google.com/drive/1tE3bJNXOv1NW1OPncgpLfPb1bk8RZnAE?usp=sharing\r\n\r\n(Change EPOCH=1 to EPOCH=500 to exactly get the same results as the hello_world example.)\r\n\r\nIf it helps, you can use the [schema_py_generated.py.zip](https://github.com/tensorflow/tensorflow/files/5556749/schema_py_generated.py.zip) attached here.\r\n\r\nMarking issue as resolved.\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44882\">No</a>\n", "> @YANxu-666 I was able to run this on tf-nightly with version `2.5.0-dev20201117` (as of today). It also worked for `2.4.0rc0` Here is the link: https://colab.research.google.com/drive/1tE3bJNXOv1NW1OPncgpLfPb1bk8RZnAE?usp=sharing\r\n> \r\n> (Change EPOCH=1 to EPOCH=500 to exactly get the same results as the hello_world example.)\r\n> \r\n> If it helps, you can use the [schema_py_generated.py.zip](https://github.com/tensorflow/tensorflow/files/5556749/schema_py_generated.py.zip) attached here.\r\n> \r\n> Marking issue as resolved.\r\n\r\nI ran it in jupyter notebook according to the link you provided, and the same error still appeared.", "I had the same problem and found that schema_py_generated.py did exist in my tensorflow installation, but was empty.\r\nAnd it was independent of installing freshly or with --upgrade via pip, and for both nightly (several dates) and 2.4 (both rc).\r\nIs there a problem with the pip config?\r\n\r\nAnyway, thanks @MeghnaNatraj  for the file, this solved the issue for me\r\n\r\nBtw., I think #44725 is a duplicate of this one", "The problem was with TF release itself (due to a change that was rolled out). However it was fixed soon after - refer to #41846 \r\n\r\nThe latest builds do not have this issue. (or at-least I am unable to reproduce this issue). If you face this issue even now, please create a new issue or post a comment in #41846 so we can look into it.", "I got this error too.\r\ntensorflow: 2.4.1\r\nwindow 10 \r\npython 3.7", "This issue is fixed on 2.4.1 ?", "@wqcsim Trye downloading tf-nightly. `pip install tf-nightly`", "I had the exact same problem using TensorFlow 2.4.1 with python 3.7 on Windows 10, and unzipping the above-mentioned schema_py_generated.py.zip to replace an existing but 0-byte file schema_py_generated.py in my python 3.7 installation subfolder Python37\\Lib\\site-packages\\tensorflow\\lite\\python fixed the problem. I'd rather not use nightly versions unless I really have to.\r\n\r\nThanks!\r\n", "> I had the exact same problem using TensorFlow 2.4.1 with python 3.7 on Windows 10, and unzipping the above-mentioned schema_py_generated.py.zip to replace an existing but 0-byte file schema_py_generated.py in my python 3.7 installation subfolder Python37\\Lib\\site-packages\\tensorflow\\lite\\python fixed the problem. I'd rather not use nightly versions unless I really have to.\r\n> \r\n> Thanks!\r\n\r\nit works!"]}, {"number": 44881, "title": "Exception: Some of the operators in the model are not supported by TensorFlow Flex runtime: Enter, Exit.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux 2\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\nException: Some of the operators in the model are not supported by TensorFlow Flex runtime: Enter, Exit.\r\n```\r\n\r\n**Any other info / logs**\r\nTrying to convert a LSTM model trained in TensorFlow 1.15 to TFLite 1.15; however, cannot resolve the error message for these two control ops - Enter and Exit. I have tried the following settings, and can only resolve the problems with Switch and Merge operators. Any idea whether Enter and Exit will be supported in TFLite 1.15?  Thanks.\r\n\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.post_training_quantize = True\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\n", "comments": ["Hi chelseaju,\r\n\r\nAny chance you could try converting your model in TF2? TF2 has good support on converting stateless Keras LSTM models. \r\n\r\nhttps://www.tensorflow.org/lite/convert/rnn\r\n\r\nAlso add Renjie who's the expert in this area.", "Thanks for the quick response. Unfortunately, TF2 is currently not supported with our online system, and will require sometime for the upgrade.  Is there a way to still make it work with TF1?  Thanks.", "One idea is to still train the model in TF1.x and export the saved model. Then try importing the model in TF2 for the conversion?", "yes, we don't support v1 control flow.\r\n\r\ndepending on the version you're using, maybe you can try with `tf.enable_control_flow_v2()` with tf1.x and see if works for you", "I try setting `tf.enable_control_flow_v2()`, but it gives me errors on freeze_graph - \r\n\r\n```\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 361, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py\", line 190, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 834, in __init__\r\n    self.build()\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 846, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 884, in _build\r\n    build_restore=build_restore)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 488, in _build_internal\r\n    names_to_saveables)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 343, in validate_and_slice_inputs\r\n    for converted_saveable_object in saveable_objects_for_op(op, name):\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 206, in saveable_objects_for_op\r\n    variable, \"\", name)\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\", line 83, in __init__\r\n    self.handle_op = var.op.inputs[0]\r\n  File \"/ec2-user-home/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 2154, in __getitem__\r\n    return self._inputs[i]\r\nIndexError: list index out of range\r\n```\r\n\r\nI used ` tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()` to convert the graph, and my tensorflow version is 1.15.0. ", "hi I have also tried the above step to export into TF2.x. I found the same errors as @chelseaju when trying to convert to TFLite. The model was also trained in TF1.x, converted to TF2.x, and then ran using `tf.compat.v1.lite.TFLiteConverter.from_frozen_graph()`. \r\n\r\nAre there any next steps or ways to help? It seems that converting LSTMs are fairly limited at the moment. ", "frozen_graph is more like 1.x concept, can you guys try export as saved_model and convert from the saved_model?\r\n\r\nthanks!", "from the saved model in TF1.15, `tf.lite.TFLiteConverter.from_saved_model` with the above converter attributes set throws the same error as before. ", "emm, it seems we need to migrate to 2.x then", "Consider using newer TF version and enable control flow v2 https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2\r\n\r\nIf you still have problems, consider upgrading to TF 2.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44881\">No</a>\n"]}, {"number": 44880, "title": "Change tf_upgrade_v2 to be compatible with tf1 and tf2 for tf.nn.dropout", "body": "According to [the recommended upgrade process](https://www.tensorflow.org/guide/upgrade#recommended_upgrade_process), even after converting,\r\nWe can run and pass the test with tensorflow 1.14 as well.\r\n\r\nI found that If we run tf_upgrade_v2 on tf.nn.dropout, tf1 and tf2 are incompatible.\r\nIn the case of `tf.nn.dropout(x, keep_prob)`, if you don't specify it\r\nwith a keyword, such as, tf_upgrade_v2 converts to `tf.nn.dropout(x, 1-keep_prob)`.\r\n\r\nIn tf2, it is compatible with tf1 since the second argument is rate and rate=1-keep_prob.\r\nIn 1.14, on the other hand, the result is not the same because the second argument is keep_probe.\r\n\r\nFor compatibility, it should be `tf.nn.dropout(x, rate=1-keep_prob)`.\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44880) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 44879, "title": "TensorFlowLiteSelectTfOps: Interpreter creation fails on iOS when scheme is set to Release", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\niOS 14.1. Development environment is Xcode 12.2 (12B45b), macOS Big Sur 11.0.1.\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\niPhone SE 2016\r\n\r\n- TensorFlow installed from (source or binary) / TensorFlow version (use command below):\r\npod 'TensorFlowLiteSwift', '~> 0.0.1-nightly.20201107', :subspecs => ['CoreML', 'Metal']\r\npod 'TensorFlowLiteSelectTfOps', '0.0.1-nightly.20201031'\r\n\r\n**Describe the current behavior**\r\n\r\nInterpreter is created successfully when target scheme's build configuration is set to Debug. When build configuration is set to Release, it fails with:\r\n\r\n> TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n> TensorFlow Lite Error: Node number 6 (FlexRFFT) failed to prepare.\r\n\r\n**Describe the expected behavior**\r\n\r\nInterpreter should be created successfully in Release builds.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport TensorFlowLite\r\n...\r\nlet modelPath = Bundle.main.bundleURL.appendingPathComponent(\"model.tflite\").path\r\ndo {\r\n    let interpreter = try Interpreter(modelPath: modelPath)\r\n    try interpreter.allocateTensors()\r\n} catch {\r\n    NSLog(\"\\(error)\")\r\n}\r\n```\r\n\r\n**Other info / logs**\r\n\r\n**Can work around by going to target > Build Settings and changing Dead Code Stripping to No.**", "comments": ["@thaink Can you please take a look?", "I was able to reproduce this issue. let me investigate more.", "@clo-dan 's work-around works when running from XCode, but this issue seems to persist (even with work-around) when building/deploying to TestFlight.", "> @clo-dan 's work-around works when running from XCode, but this issue seems to persist (even with work-around) when building/deploying to TestFlight.\r\n\r\nWhat work-around is that?", "From original post: \"Can work around by going to target > Build Settings and changing Dead Code Stripping to No.\"\r\n\r\nThis allows app to run on device with Release config. Otherwise will get the error described. But even with this, TestFlight builds still don't work.", "Can confirm, our TestFlight builds also produce the error even with the workaround. We have bitcode disabled but symbolicated reports are enabled.", "Also reproducible by doing an ad hoc build and sideloading the IPA to phone via Xcode Devices and Simulators window, which is easier to test than uploading to TestFlight. Even tried setting Archive build config to Debug instead of Release, same result.\r\n\r\nSomething about Xcode's packaging process is ignoring the Dead Code Stripping setting. Xcode forces stripSwiftSymbols in all archive builds now with no option to disable so possibly it's that.", "I found that the problem can be fixed when adding \"-u _TF_AcquireFlexDelegate\" to Build Settings -> Other Linker Flags.\r\n@clo-dan @rapuckett  Could you give it a try?", "This lets me run from Xcode without dead stripping but ad hoc builds still fail. Have not tried TestFlight yet but assume it will fail given ad hoc does.", "Same here. Also tried on TestFlight and it unfortunately did not work there\neither.\n\nOn Tue, Nov 24, 2020 at 8:18 AM clo-dan <notifications@github.com> wrote:\n\n> This lets me run from Xcode without dead stripping but ad hoc builds still\n> fail. Have not tried TestFlight yet but assume it will fail given ad hoc\n> does.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44879#issuecomment-733085333>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAG7SDBPKOBW6ENCTR745A3SRPMGTANCNFSM4TVY4P4A>\n> .\n>\n", "> This lets me run from Xcode without dead stripping but ad hoc builds still fail. Have not tried TestFlight yet but assume it will fail given ad hoc does.\r\n\r\nDo you mean it is OK with dead_stripping=yes or not?\r\nCan you guide me how to build for ad hoc and TestFight? I haven't done that before.", "Sorry, meant I can run it without _disabling_ dead stripping now.\r\n\r\nFor ad hoc: Product > Archive, once complete select the archive in Organizer > Distribute App > Ad Hoc > Next > Auto manage signing > Export. Then, with phone connected, Window > Devices and Simulators > select your phone > click + under Installed Apps > select IPA file that was exported.\r\n\r\nFor TestFlight, same initial steps but instead of Ad Hoc distribution, choose App Store Connect > Upload.", "Same issuse.\r\nAdding \"-u _TF_AcquireFlexDelegate\" to Build Settings -> Other Linker Flags does work when using Xcode, but still not work on TestFlight", "In Debug, \"TF_AcquireFlexDelegate\" will be called by  \"TfLiteInterpreterCreate\"   to create TensorFlow Lite delegate for select TF ops, but \"TF_AcquireFlexDelegate\" was not called in Release.\r\n\r\nThis issuse can be easily reproduced and should be able to locate the problem", "I was able to get ad hoc and TestFlight builds working by going to target > Build Settings > **changing Strip Linked Product to No.**", "@clo-dan TestFlight builds still not working for me, are there other configurations?", "1.  Build Settings > Strip Linked Product: NO\r\n2. Build Settings > Other Linker Flags : -u _TF_AcquireFlexDelegate\r\n\r\nwith these two configurations, it works on TestFlight.", "As @clo-dan suggested, when set Build Settings > changing Strip Linked Product to No, it will work.\r\nBesides, I submitted another fix, you can try it in tomorrow's nightly build.", "This is fixed in master.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44879\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44879\">No</a>\n"]}, {"number": 44878, "title": "tf.data.Dataset.list_files result in segfault", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not really\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):from pip install tf-nightly\r\n- TensorFlow version (use command below):tf-nightly==2.5.0.dev20201114\r\n- Python version:3.7.0\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Irrelevant\r\n- GPU model and memory: GTX680, compute capability too low to be used (3.0)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nv1.12.1-45796-gbefea92a3d 2.5.0-dev20201114\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nIn [1]: import tensorflow as tf\r\n2020-11-14 21:20:53.955783: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:53.955823: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nIn [2]: list_ds = tf.data.Dataset.list_files(\"/\")\r\n2020-11-14 21:20:58.154084: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-14 21:20:58.155020: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-14 21:20:58.193006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-14 21:20:58.193331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 680 computeCapability: 3.0\r\ncoreClock: 1.176GHz coreCount: 8 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 179.05GiB/s\r\n2020-11-14 21:20:58.193434: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:58.193503: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:58.193567: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:58.195053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-14 21:20:58.195400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-14 21:20:58.197271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-14 21:20:58.197427: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:58.197525: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \r\n2020-11-14 21:20:58.197545: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1761] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-11-14 21:20:58.198519: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-14 21:20:58.198566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1265] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-14 21:20:58.198579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1271]      \r\nErreur de segmentation (core dumped)\r\n\r\n```\r\n**Describe the expected behavior**\r\n\r\nWould just work and produce a dataset object\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf \r\nlist_ds = tf.data.Dataset.list_files(\"/\")\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nI initially thought I could use my GTX680 GPU, so I installed cuda libraries and cudnn with tensorflow-gpu. When I realized compute capability was too low, I just intalled tensorflow, in the hope that I would not experience any gpu related issue. Eventually I still got errors.\r\n\r\nI also tried a bit of gdb:\r\n\r\nThread 30 \"python\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fff2b7fe700 (LWP 561116)]\r\n__strnlen_sse2 () at ../sysdeps/x86_64/multiarch/../strlen.S:117\r\n\r\nbacktrace gives:\r\n#0  __strnlen_sse2 () at ../sysdeps/x86_64/multiarch/../strlen.S:117\r\n#1  0x00007ffff7e7f1a6 in __fnmatch (pattern=<optimized out>, string=<optimized out>, flags=1) at fnmatch.c:342\r\n#2  0x00007fffd5869ff7 in tensorflow::FileSystem::Match(std::string const&, std::string const&) ()\r\n   from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007fffd586f2e4 in std::_Function_handler<void (int), tensorflow::internal::GetMatchingPaths(tensorflow::FileSystem*, tensorflow::Env*, std::string const&, std::vector<std::string, std::allocator<std::string> >*)::{lambda(int)#1}::operator()(int) const::{lambda(int)#1}>::_M_invoke(std::_Any_data const&, int&&) ()\r\n   from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007fffd586ed62 in std::_Function_handler<void (), tensorflow::internal::(anonymous namespace)::ForEach(int, int, std::function<void (int)> const&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007fffd5876021 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007fffd5872d33 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007fffd1008335 in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) ()\r\n   from /home/gnthibault/Documents/tests/python/tf_install/venv/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#8  0x00007ffff7f8b609 in start_thread (arg=<optimized out>) at pthread_create.c:477\r\n#9  0x00007ffff7eb2293 in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44878\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44878\">No</a>\n", "installed tf-nightly-cpu that fixed the issue"]}, {"number": 44877, "title": "Error while import tensorflow module", "body": "I've installed TensorFlow by command (in power shell). It works\r\n`python -m pip install --upgrade tensorflow`\r\n\r\nthen\r\n`import tensorflow as tf`\r\n\r\nI get this errors\r\n`Traceback (most recent call last): File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(file)]) File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 296, in find_module raise ImportError(_ERR_MSG.format(name), name=name) ImportError: No module named '_pywrap_tensorflow'`\r\n", "comments": ["@raffibaihaqy02 \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/).\r\nPlease, check Your CPU/Python is on 32 bits?\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44877\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44877\">No</a>\n"]}]