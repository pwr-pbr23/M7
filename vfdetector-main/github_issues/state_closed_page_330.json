[{"number": 44273, "title": "More CI fixes on r2.4", "body": "", "comments": []}, {"number": 44272, "title": "DLL error importing python tensorflow library in pywrap_tensorflow.py", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home, x64, version 18362.1139 in Spanish (ESPA\u00d1OL)\r\n- Mobile device: I will test in a mobile device after development.\r\n- TensorFlow installed from (source or binary): binary, pip install.\r\n- TensorFlow version:\r\nC:\\>pip list | findstr tensorflow\r\ntensorflow             1.15.0\r\ntensorflow-estimator   1.15.1\r\n* i tried version 2\r\n- Python version: Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): I dont know\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: no cuda\r\n- GPU model and memory:  Intel UHD Graphics, 600. 4G memory\r\n- AVX:\r\nAVX             -       Supports AVX instruction extensions\r\n\r\n\r\n\r\n**Describe the problem**\r\nIm trying some code on python that includes tensorflow, when i do import tensorflow as tf it results into an DLL error \r\n\r\ni just installed Visual C++ downloads and i updraded to tensorflow 2,  it didnt work\r\n\r\n*****Provide the exact sequence of commands / steps that you executed before running into the problem*****\r\npython command line:\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\n.... ERROR\r\n\r\n****** Any other info / logs******\r\n\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Error en una rutina de inicializaci\u00f3n de biblioteca de v\u00ednculos din\u00e1micos (DLL).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Error en una rutina de inicializaci\u00f3n de biblioteca de v\u00ednculos din\u00e1micos (DLL).\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@ajtorresd,\r\nCould you please share the make and model of the CPU on your machine. Make sure you have all the compatible softwares as per the [system requirements](https://www.tensorflow.org/install/pip#system-requirements).\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/42058#issuecomment-672240986) from a similar issue and let us know if it helps. Thanks!", "Hi Abhilash,\n\nI installed Nvidia CUDA 10 as per your suggestion and my code is still\nfailing just importing... may you have any other suggestions?\n\nmy cpu details are:\n\nCoreinfo v3.5 - Dump information on system CPU and memory topology\nCopyright (C) 2008-2020 Mark Russinovich\nSysinternals - www.sysinternals.com\n\n\nIntel(R) Celeron(R) N4000 CPU @ 1.10GHz\nIntel64 Family 6 Model 122 Stepping 1, GenuineIntel\nMicrocode signature: 0000002C\nHTT             *       Hyperthreading enabled\nHYPERVISOR      -       Hypervisor is present\nVMX             *       Supports Intel hardware-assisted virtualization\nSVM             -       Supports AMD hardware-assisted virtualization\nX64             *       Supports 64-bit mode\n\nSMX             -       Supports Intel trusted execution\nSKINIT          -       Supports AMD SKINIT\n\nNX              *       Supports no-execute page protection\nSMEP            *       Supports Supervisor Mode Execution Prevention\nSMAP            *       Supports Supervisor Mode Access Prevention\nPAGE1GB         *       Supports 1 GB large pages\nPAE             *       Supports > 32-bit physical addresses\nPAT             *       Supports Page Attribute Table\nPSE             *       Supports 4 MB pages\nPSE36           *       Supports > 32-bit address 4 MB pages\nPGE             *       Supports global bit in page tables\nSS              *       Supports bus snooping for cache operations\nVME             *       Supports Virtual-8086 mode\nRDWRFSGSBASE    *       Supports direct GS/FS base access\n\nFPU             *       Implements i387 floating point instructions\nMMX             *       Supports MMX instruction set\nMMXEXT          -       Implements AMD MMX extensions\n3DNOW           -       Supports 3DNow! instructions\n3DNOWEXT        -       Supports 3DNow! extension instructions\nSSE             *       Supports Streaming SIMD Extensions\nSSE2            *       Supports Streaming SIMD Extensions 2\nSSE3            *       Supports Streaming SIMD Extensions 3\nSSSE3           *       Supports Supplemental SIMD Extensions 3\nSSE4a           -       Supports Streaming SIMDR Extensions 4a\nSSE4.1          *       Supports Streaming SIMD Extensions 4.1\nSSE4.2          *       Supports Streaming SIMD Extensions 4.2\n\nAES             *       Supports AES extensions\nAVX             -       Supports AVX instruction extensions\nFMA             -       Supports FMA extensions using YMM state\nMSR             *       Implements RDMSR/WRMSR instructions\nMTRR            *       Supports Memory Type Range Registers\nXSAVE           *       Supports XSAVE/XRSTOR instructions\nOSXSAVE         *       Supports XSETBV/XGETBV instructions\nRDRAND          *       Supports RDRAND instruction\nRDSEED          *       Supports RDSEED instruction\n\nCMOV            *       Supports CMOVcc instruction\nCLFSH           *       Supports CLFLUSH instruction\nCX8             *       Supports compare and exchange 8-byte instructions\nCX16            *       Supports CMPXCHG16B instruction\nBMI1            -       Supports bit manipulation extensions 1\nBMI2            -       Supports bit manipulation extensions 2\nADX             -       Supports ADCX/ADOX instructions\nDCA             -       Supports prefetch from memory-mapped device\nF16C            -       Supports half-precision instruction\nFXSR            *       Supports FXSAVE/FXSTOR instructions\nFFXSR           -       Supports optimized FXSAVE/FSRSTOR instruction\nMONITOR         *       Supports MONITOR and MWAIT instructions\nMOVBE           *       Supports MOVBE instruction\nERMSB           *       Supports Enhanced REP MOVSB/STOSB\nPCLMULDQ        *       Supports PCLMULDQ instruction\nPOPCNT          *       Supports POPCNT instruction\nLZCNT           -       Supports LZCNT instruction\nSEP             *       Supports fast system call instructions\nLAHF-SAHF       *       Supports LAHF/SAHF instructions in 64-bit mode\nHLE             -       Supports Hardware Lock Elision instructions\nRTM             -       Supports Restricted Transactional Memory\ninstructions\n\nDE              *       Supports I/O breakpoints including CR4.DE\nDTES64          *       Can write history of 64-bit branch addresses\nDS              *       Implements memory-resident debug buffer\nDS-CPL          *       Supports Debug Store feature with CPL\nPCID            -       Supports PCIDs and settable CR4.PCIDE\nINVPCID         -       Supports INVPCID instruction\nPDCM            *       Supports Performance Capabilities MSR\nRDTSCP          *       Supports RDTSCP instruction\nTSC             *       Supports RDTSC instruction\nTSC-DEADLINE    *       Local APIC supports one-shot deadline timer\nTSC-INVARIANT   *       TSC runs at constant rate\nxTPR            *       Supports disabling task priority messages\n\nEIST            *       Supports Enhanced Intel Speedstep\nACPI            *       Implements MSR for power management\nTM              *       Implements thermal monitor circuitry\nTM2             *       Implements Thermal Monitor 2 control\nAPIC            *       Implements software-accessible local APIC\nx2APIC          *       Supports x2APIC\n\nCNXT-ID         -       L1 data cache mode adaptive or BIOS\n\nMCE             *       Supports Machine Check, INT18 and CR4.MCE\nMCA             *       Implements Machine Check Architecture\nPBE             *       Supports use of FERR#/PBE# pin\n\nPSN             -       Implements 96-bit processor serial number\n\nPREFETCHW       *       Supports PREFETCHW instruction\n\nMaximum implemented CPUID leaves: 00000018 (Basic), 80000008 (Extended).\nMaximum implemented address width: 48 bits (virtual), 39 bits (physical).\n\nProcessor signature: 000706A1\n\nLogical to Physical Processor Map:\n*-  Physical Processor 0\n-*  Physical Processor 1\n\nLogical Processor to Socket Map:\n**  Socket 0\n\nLogical Processor to NUMA Node Map:\n**  NUMA Node 0\n\nNo NUMA nodes.\n\nLogical Processor to Cache Map:\n*-  Data Cache          0, Level 1,   24 KB, Assoc   6, LineSize  64\n*-  Instruction Cache   0, Level 1,   32 KB, Assoc   8, LineSize  64\n**  Unified Cache       0, Level 2,    4 MB, Assoc  16, LineSize  64\n-*  Data Cache          1, Level 1,   24 KB, Assoc   6, LineSize  64\n-*  Instruction Cache   1, Level 1,   32 KB, Assoc   8, LineSize  64\n\nLogical Processor to Group Map:\n**  Group 0\n\n\n\nEl mar., 27 oct. 2020 a las 5:05, Abhilash Mahendrakar (<\nnotifications@github.com>) escribi\u00f3:\n\n> @ajtorresd <https://github.com/ajtorresd>,\n> Could you please share the make and model of the CPU on your machine. Make\n> sure you have all the compatible softwares as per the system requirements\n> <https://www.tensorflow.org/install/pip#system-requirements>.\n>\n> Also, please take a look at this comment\n> <https://github.com/tensorflow/tensorflow/issues/42058#issuecomment-672240986>\n> from a similar issue and let us know if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44272#issuecomment-717094691>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACBETB5AARZC34HPHKETUVLSM2EPJANCNFSM4S5EJ4ZQ>\n> .\n>\n", "@ajtorresd,\r\nAs per Intel's [product specification](https://ark.intel.com/content/www/us/en/ark/products/128988/intel-celeron-processor-n4000-4m-cache-up-to-2-60-ghz.html), your CPU doesn't support AVX. For more information, please take a look at the [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements) to install TensorFlow with pip. \r\n\r\nAs an alternative in this case, you can [build TensorFlow from source](https://www.tensorflow.org/install/source_windows) or use [Google Colab](https://colab.research.google.com/). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi Abhilash\n\n*Sorry for the late answer i was trying to find out another laptop with\ncpu's AVX enable... i failed...*\n\n*Im going to try your suggestion this weekend... i really want to finish\nthis product.*\n\n*Regards*\n\n*Antonio.*\n\n\nEl lun., 9 nov. 2020 a las 14:20, tensorflow-butler[bot] (<\nnotifications@github.com>) escribi\u00f3:\n\n> This issue has been automatically marked as stale because it has not had\n> recent activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44272#issuecomment-724187716>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACBETB6VP7C6NIPBBJKUTILSPAXFJANCNFSM4S5EJ4ZQ>\n> .\n>\n", "An aditional question, can i compile tensorflow in windows????\n\nEl mi\u00e9., 11 nov. 2020 a las 8:58, Antonio Torres (<ajtorresd@gmail.com>)\nescribi\u00f3:\n\n> Hi Abhilash\n>\n> *Sorry for the late answer i was trying to find out another laptop with\n> cpu's AVX enable... i failed...*\n>\n> *Im going to try your suggestion this weekend... i really want to finish\n> this product.*\n>\n> *Regards*\n>\n> *Antonio.*\n>\n>\n> El lun., 9 nov. 2020 a las 14:20, tensorflow-butler[bot] (<\n> notifications@github.com>) escribi\u00f3:\n>\n>> This issue has been automatically marked as stale because it has not had\n>> recent activity. It will be closed if no further activity occurs. Thank you.\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/44272#issuecomment-724187716>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ACBETB6VP7C6NIPBBJKUTILSPAXFJANCNFSM4S5EJ4ZQ>\n>> .\n>>\n>\n", "> An aditional question, can i compile tensorflow in windows????\r\n\r\n@ajtorresd,\r\nYes, you can compile TensorFlow on Windows. Please follow [this guide](https://www.tensorflow.org/install/source_windows) for the instructions. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Im working on it friend!\n\nEl lun, 23 nov 2020 a las 3:42, tensorflow-butler[bot] (<\nnotifications@github.com>) escribi\u00f3:\n\n> This issue has been automatically marked as stale because it has not had\n> recent activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44272#issuecomment-731985220>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACBETBZGQEYSDHVHKFWYE63SRIG5JANCNFSM4S5EJ4ZQ>\n> .\n>\n", "Hi abilash...\n\nI have bee following your recomendations\n\nthe ones listed here: https://www.tensorflow.org/install/source_windows\n\nbut in the step:\n\nbazel build //tensorflow/tools/pip_package:build_pip_package\n\nit finished with an error (late in this email)  Visual C++ build tools not\nfound on your machine ... but i downloaded it and installed it previously...\n\n\n  C:/tensorflow/tensorflow/workspace.bzl:109:30: in tf_repositories\nRepository rule def_file_filter_configure defined at:\n\nC:/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl:55:44:\nin <toplevel>\nERROR: An error occurred during the fetch of repository\n'local_config_def_file_filter':\n   Traceback (most recent call last):\n        File\n\"C:/tensorflow/tensorflow/tools/def_file_filter/def_file_filter_configure.bzl\",\nline 33, column 28, in _def_file_filter_configure_impl\n                auto_configure_fail(\"Visual C++ build tools not found on\nyour machine\")\n        File\n\"C:/users/lenovo/_bazel_lenovo/xv6zejqw/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\",\nline 112, column 9, in auto_configure_fail\n                fail(\"\\n%sAuto-Configuration Error:%s %s\\n\" % (red,\nno_color, msg))\nError in fail:\nAuto-Configuration Error: Visual C++ build tools not found on your machine\nERROR: C:/tensorflow/tensorflow/python/BUILD:6295:8:\n//tensorflow/python:pywrap_tensorflow_filtered_def_file depends on\n@local_config_def_file_filter//:def_file_filter in repository\n@local_config_def_file_filter which failed to fetch. no such package\n'@local_config_def_file_filter//':\nAuto-Configuration Error: Visual C++ build tools not found on your machine\nERROR: Analysis of target\n'//tensorflow/tools/pip_package:build_pip_package' failed; build aborted:\nAnalysis failed\nINFO: Elapsed time: 3172.774s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (364 packages loaded, 13711\ntargets configured)\n    Fetching @llvm-project; fetching 1902s\n    Fetching ...roject; Extracting\nC:/users/lenovo/_bazel_lenovo/xv6zejqw/external/llvm-project/temp411294270894680330\\\n4/3c696a212ba4328e4f8f92136bc4d728a6490ef7.tar.gz 623s\n\nEl lun, 16 nov 2020 a las 3:01, Abhilash Mahendrakar (<\nnotifications@github.com>) escribi\u00f3:\n\n> An aditional question, can i compile tensorflow in windows????\n>\n> @ajtorresd <https://github.com/ajtorresd>,\n> Yes, you can compile TensorFlow on Windows. Please follow this guide\n> <https://www.tensorflow.org/install/source_windows> for the instructions.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44272#issuecomment-727779425>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ACBETB3AM2EFOYJULQQIACDSQDE4NANCNFSM4S5EJ4ZQ>\n> .\n>\n", "@ajtorresd,\r\nCould you please provide the following details \r\n\r\n- TensorFlow version you are trying to install:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nAnd also, make sure you have all the compatible softwares installed as per the [tested build configurations](https://www.tensorflow.org/install/source_windows#cpu). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44272\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44272\">No</a>\n"]}, {"number": 44271, "title": "Attempt to fix the pip wheel rename on Windows.", "body": "There are different builds with 3 different solutions. Don't know yet\r\nwhich one works, by reading code all 3 should work but Windows paths are\r\ndubious since we call to bash during the release script.\r\n\r\nThat's why I'm also adding a debug line to display the contents of\r\n`PATH`.", "comments": []}, {"number": 44270, "title": "Improve grappler layout optimizer to support 5D tensors", "body": "This PR further improves the grappler layout optimizer to support 5D tensors. Specifically, this PR does:\r\n\r\n1. Fix the issue of the Conv3D/Conv3DBackpropInput/Conv3DBackpropFilter Transposers that fail to apply ScopedDataFormatUpgrader.\r\n2. Improve the unit tests for (1).\r\n3. Add support for 5D tensors for BiasAdd, BiasAddGrad, Shape, ShapeN, Slice, AddN, Concat.\r\n4. Add new tests for (3).\r\n\r\nfyi. @nluehr ", "comments": ["Currently this is failing for some XLA GPU tests (`//tensorflow/python/keras/layers:convolutional_test_xla_gpu`, `//tensorflow/python/keras/layers:convolutional_transpose_test_xla_gpu`) due to `DataFormatVecPermute` tf2xla kernel not being updated for 5D data formats https://github.com/tensorflow/tensorflow/blob/ab55c62645c7968cb2ebda456cc3ca85c4a411e9/tensorflow/compiler/tf2xla/kernels/data_format_ops.cc#L88-L162. Can you also update that tf2xla kernel?", "@andyly PTAL.", "It seems here is a rollback. Do we know which tests are broken? @andyly ", "> It seems here is a rollback. Do we know which tests are broken? @andyly\r\n\r\nCurrently I am investigating the test that is failing. It is an internal test and I don't have easy access to running it under a certain configuration.", "Any update on this? @andyly ", "> Any update on this? @andyly\r\n\r\nLet me check with the owner of the test again. I'll also try bisecting/splitting this PR (we can also try to split and land parts of this PR).", "Sounds good. Please keep me posted and let me know if I can help with the bisecting/splitting.", "I got access to a repro and the issue seems to be coming from BiasAdd(Transposer). Has that op been updated for 5D data format support? From some time back I remembered the op kernel (at least on CPU) hard coding the channel dimension to 1 for NCHW and the last dimension for NHWC, but it supported 3D, 4D, and 5D tensors.", "The BiasAdd transposer is newly added since we cannot directly use the DefaultLayoutSensitiveOpTransposer which does dataformatUpgrade -> updateNode -> updateFaninEdges -> updateFanoutEdges. However, the execution path we need in the BiasAdd is like updateNode -> dataformatUpgrade -> updateFaninEdges -> updateFanoutEdge (I have added some comments to explain why in the code.)\r\n\r\nFor both DefaultLayoutSensitiveOpTransposer and BiasAdd, we restrict the optimization only to rank = 4 or 5. As for the BiasAdd op, I don't think we have changed anything about it.", "Not sure if you can paste the related log here so that we can think about the potential problem.", "Actually the error is in the ReduceTransposer. It hasn't been completely updated for 5D tensor support and was triggering an error transitively from the BiasAdd 5D transposes. Let me correct the ReduceTransposer.", "For some context, https://github.com/tensorflow/tensorflow/blob/89e81228719825b36d6278276efcd6fc146574a3/tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc#L1439-L1443 needs to be updated also.", "Awesome. This makes more sense.\r\n\r\nThanks for fixing the part. Do I need to submit a new PR or you could help re-open this PR from your side.", "I'll submit a quick fix for the ReduceTransposer and then we can try to roll forward your PR.", "The fix is landed in https://github.com/tensorflow/tensorflow/commit/28c5e97db99302890451b3d607a5a3572ee6ca2b", "> The fix is landed in [28c5e97](https://github.com/tensorflow/tensorflow/commit/28c5e97db99302890451b3d607a5a3572ee6ca2b)\r\n\r\nThanks for the fix. Hope this fix + this PR can make those tests pass.", "Do the internal tests pass now? Can we roll forward this PR? @andyly Thanks.", "> Do the internal tests pass now? Can we roll forward this PR? @andyly Thanks.\r\n\r\nI think we are good to go with rolling forward this PR. Can you create it? Also, when I was scanning the PR (for debugging the failed test internally), there may still need to be a few edits (specifically `IsBiasAdd` allows for BiasAdd and BiasAddV1 but BiasAdd is the op that supports different data formats, so it needs to be restricted further).", "Sure. Created #44270 which includes the changes about restricting optimizer to BiasAdd node."]}, {"number": 44269, "title": "Add parallel matching for `tf.gfile.Glob`", "body": "This is a PR from JIZHI Team & TaiJi AI platform in Tencent.\r\n\r\nAdd a argument `max_parallel_num` for  `tf.gfile.Glob`, which will setup a threadpool to parallelize the matching for mulitple pattern strings. It can bring nice performance improvement if `max_parallel_num` is set appropriately.\r\n\r\nThanks for your review.", "comments": ["Since the matching happensinside C++ code, I'm afraid this PR does not do what it says it does.", "@mihaimaruseac Thanks for your reply! \r\n\r\nI also considered the issue you mentioned in fact. In the C++ implement of different `FileSystem`, there may be some mutexes which cause the multi-threaded parallel from python client does not work. However, `glob` is a I/O-bound operation as we known, which always consumes a lot of time in blocking wait and multi-threads can ease this overhead a lot.\r\n\r\nBTW, I have obtained nice effect in my case that using `tf.gfile.Glob` for hdfs:\r\n```python\r\npattern = ['hdfs://xxxx_1/*', 'hdfs://xxxx_2/', 'hdfs://xxxx_3/, 'hdfs://xxxx_4/', 'hdfs://xxxx_5/']\r\nbegin_time = int(time.time())\r\nfiles = tf.gfile.Glob(pattern, max_parallel_num=5)\r\nprint('size: {}, used time: {} s'.format(len(files), int(time.time()) - begin_time))\r\n```\r\nAs the final size of `files` is 20000, the above code costed **203 seconds** as it printed, and **809 seconds** while `max_parallel_num` is set to `1` (It is too slow for me so I have to make some optimization...).\r\n\r\nThe above is my reason for proposing this pr and there may be some incompleteness. I am looking forward to your valuable suggestions for improvement.", "This looks good to me now. Since it changes API, let's wait for API owners approval too", "@fchollet Thanks for your review!\r\n\r\n> Maybe the choice of the number of threads to use should be automated, rather than specified by the user. Most users will not be aware of this argument. What do you think?\r\n\r\nIn fact I have implemented automatically using `len(pattern)` as the number of parallel processes inside the api, however, I overturned my own ideas in my practice. As for the cases which run multiple tf instance processes on the same machine (e.g. tf with horovod), users tend to control the parallel number of different processes based on the CPU resources on their machine. If the api automatically tries the maximum parallelism, it may cause unexpected resource preemption. \r\n\r\nThis is my reason for setting it as a configurable parameter and I would like to discuss further with you.", "@fchollet Could you please check this again?", "> Please don't forget about posting some benchmark results\r\n\r\n@mihaimaruseac Sorry for my delayed benchmark..\r\n\r\n***System environment: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz 80 threads***\r\n\r\nI used the following 5 different directories of posix file sytem for benchmarking, the first of which is an empty directory to observe the overhead of the operation itself:\r\n\r\n- d1: an empty directory\r\n- d2: 4912 directories, 44711 files\r\n- d3: 7601 directories, 76354 files\r\n- d4: 64520 directories, 905496 files\r\n- d5: 62456 directories, 881247 files\r\n\r\nEach test is repeated 10 times and the average is taken as the result. The all benchmark results are as follows:\r\n\r\n| num_threads   | patterns    | Duration (ms) |\r\n| ------------- | ----------- | ------------- |\r\n| 1             | [ ]         | 130           |\r\n| 1             | [d1]          | 372           |\r\n| 1             | [d1+d2]       | 6,635          |\r\n| 1             | [d1+d2+d3]    | 10,355         |\r\n| 1             | [d1+d2+d3+d4] | 50,824         |\r\n| 1             | [d1+d2+d3+d4+d5] | 88,656         |\r\n| len(patterns) | [ ]         | 142           |\r\n| len(patterns) | [d1]          | 395           |\r\n| len(patterns) | [d1+d2]       | 6,719          |\r\n| len(patterns) | [d1+d2+d3]    | 7,883          |\r\n| len(patterns) | [d1+d2+d3+d4] | 38,811         |\r\n| len(patterns) | [d1+d2+d3+d4+d5] | 48,539         |\r\n\r\n", "So this does not seem to bring a huge speed gain and is a slowdown in the first 3 scenarios. Can you run the benchmark again after #44310 landed?", "@mihaimaruseac \r\n\r\nSince the parallelism of this pr is for multiple different patterns in Python, only when multiple patterns are input can the corresponding optimization effect be reflected. Therefore, the benchmarks here mainly use the number of input patterns as experimental variables.\r\n\r\n> So this does not seem to bring a huge speed gain and is a slowdown in the first 3 scenarios.\r\n\r\nFor the test results above,\r\n- The increased duration of the first scenario should be caused by other random factors, because when the input is `[ ]`, it returns directly [here](https://github.com/tensorflow/tensorflow/blob/a8a7a1cd5d5131fb0e409f4c919530d61ec3646e/tensorflow/python/lib/io/file_io.py#L432) without subsequent performing. In fact, the reason I set this is to take into account some random overhead.\r\n- For the second and third results, due to the small number of patterns, I think the [overhead of pool](https://github.com/tensorflow/tensorflow/pull/44269#discussion_r516407046) is the main reason for the increased duration. \r\n\r\n> Can you run the benchmark again after #44310 landed?\r\n\r\nI have made benchmarks again with #44310 landed in the same environment, and the results are as follows:\r\n\r\n| num_threads   | patterns    | Duration (ms) |\r\n| ------------- | ----------- | ------------- |\r\n| 1             | [ ]         | 428           |\r\n| 1             | [d1]          | 489           |\r\n| 1             | [d1+d2]       | 1,796          |\r\n| 1             | [d1+d2+d3]    | 4,194         |\r\n| 1             | [d1+d2+d3+d4] | 16,353         |\r\n| 1             | [d1+d2+d3+d4+d5] | 29,256         |\r\n| len(patterns) | [ ]         | 425           |\r\n| len(patterns) | [d1]          |  496          |\r\n| len(patterns) | [d1+d2]       | 1,654          |\r\n| len(patterns) | [d1+d2+d3]    | 3,216          |\r\n| len(patterns) | [d1+d2+d3+d4] | 11,622         |\r\n| len(patterns) | [d1+d2+d3+d4+d5] | 19,753         |\r\n\r\nConsidering that this increase is actually acceptable for this `Glob` api (right?), and when the number of patterns increases, the effect of parallel optimization can be reflected, we have reason to think this parallel in python is still quite valuable.\r\n", "This is awesome. Let's add a comment saying what the default value of the `num_threads` is (when user passes `None`/doesn't pass anything). And then we can merge.", "@mihaimaruseac @fchollet  Thanks for your review! \r\n\r\nI have updated the comment about the default value of `num_threads`. Could you please check again?", "I would also go with 1 thread by default to maintain compatibility. Alternatively, minimum between number of patterns and number of cores.", "@mihaimaruseac \r\nThanks! I have finished the update of default value setting and I think it has reached a more robust state now.  Could you please check anything need enhancement more again? \ud83d\ude04 ", "Hi, @mihaimaruseac @gbaned \r\n\r\nThere seems to be a internal failed check in the ci process. Could you please help to see if something went wrong?", "It is imported internally, pending some additional review. I'll come back here if there's more to do, but for now this is all good.", "> It is imported internally, pending some additional review. I'll come back here if there's more to do, but for now this is all good.\r\n\r\n@mihaimaruseac Okay, thanks for pushing forward!", "An API owners comment:\r\n\r\n> Can we push this into C++ too? Multiprocessing in Python has been the source of several bugs in the past\r\n\r\nI think this should possible. What do you think?", "> An API owners comment:\r\n> \r\n> > Can we push this into C++ too? Multiprocessing in Python has been the source of several bugs in the past\r\n> \r\n> I think this should possible. What do you think?\r\n\r\n@mihaimaruseac \r\nDoes he mean that C++ OP `_pywrap_file_io.GetMatchingFiles` directly receives a list of patterns, and then processes the multiple patterns in parallel? \r\n\r\nIf so, first of all, I think `push this into C++ too` is worth considering. As we discussed above, [changing the default value to `1`](https://github.com/tensorflow/tensorflow/pull/44269#discussion_r517997186) is to avoid introducing uncertainty from Python multiprocesses in fact. In this pr, python's parallelism is actually chosen by the user according to the specific running environment (just like other `dataset`'s parallelism parameters). The default parameters are always the same value as before and do not pose any uncertainty to the existing user code.\r\n\r\nOn the other hand, this will also result in more changes, including input formats for C++ Op. So I prefer to preserve python's multiprocesses for now. Of course, if the owner sticks to this point, I can also make the changes, but it may take more time and more disscussion. Thanks anyway!", "We can change C++ API (and update the goldens files), it is not an issue, as long as Python API maintains compatibility.\r\n\r\nSeems python multiprocessing has issues so it's preferable to push this code to C++.", "> We can change C++ API (and update the goldens files), it is not an issue, as long as Python API maintains compatibility.\r\n> \r\n> Seems python multiprocessing has issues so it's preferable to push this code to C++.\r\n\r\nSorry for the late reply...\r\nOkay,  I will migrate the relevant logic to C++ later. Thanks!", "@firejq Can you please resolve conflicts? Thanks!", "Due to other tighter issues in the near future, I will temporarily close this pr, and reopen it if there are new developments in the future. Thanks to all reviewers!"]}, {"number": 44268, "title": "Avoid unnecessary `for` cycle overhead for `tf.gfile.Glob`", "body": "Avoid unnecessary `for` cycle overhead to speed up `tf.gfile.Glob` in v1.15.x.\r\n\r\nThanks for your review.", "comments": ["Do we want this only against 1.15?", "@mihaimaruseac Thanks for your reply! It is indeed a general improvement but for [v2.x](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/file_system_helper.cc#L135), I have found that there is no this unnecessary `for` cycle anymore. As v1.15 is the latest (& last?) version for v1.x, I put this pr only against 1.15.\r\n\r\nOf course, this may just be my immature idea. May you have any better suggestions? ", "We no longer update TF 1.x code, unless for security patches. It is better to reopen against master and then cherrypick if needed.", "@mihaimaruseac Thanks for your relpy!  I have made optimizion against master #44269 and #44310 and I will close this pr."]}, {"number": 44267, "title": "Disable mwms_peer_failure test on Python 3.8", "body": "PiperOrigin-RevId: 338525039\r\nChange-Id: Id7b8de6848e32373fa6618ca2677fad070a430c1\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/tensorflow/tensorflow/44267)\n<!-- Reviewable:end -->\n", "comments": []}, {"number": 44266, "title": "Training is slow when using tf.keras.utils.Sequence with large Numpy arrays", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux RHEL Server 7.6**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.3.1**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: **10.1/7.6.4**\r\n- GPU model and memory: **NVIDIA V100 32GB**\r\n\r\n**Describe the current behavior**\r\nI'm using `tf.keras.Model.fit()` on a GPU with input as `tf.keras.utils.Sequence` generating batches of `Numpy` arrays. When the generated input or target `Numpy` arrays are large I notice that the training slows down dramatically. Part of this is of course expected due to the host (CPU) to device (GPU) transfer, but the times I'm seeing suggest an additional overhead somewhere else.\r\n\r\nBelow is a minimal example reproducing the issue. To keep things simple, the `Numpy` arrays are created before `data_sequence` starts generating batches, so there is almost no work done by `data_sequence[idx]`. Also, the Model consists of a single linear activation, which should just translate to a NoOp.\r\n\r\nOn an NVIDIA Tesla V100 GPU I get ca. 900ms/step.\r\n```\r\nEpoch 1/10\r\n100/100 [==============================] - 90s 899ms/step - loss: -3.4149e-04\r\nEpoch 2/10\r\n100/100 [==============================] - 91s 907ms/step - loss: -3.4149e-04\r\n...\r\n```\r\nIn TensorFlow Profiler trace_viewer one can see most of the time, namely 700ms, is spent in IteratorGetNextOp::DoCompute (or equally Iterator::Model and Iterator::Prefetch):\r\n![tf_profiler](https://user-images.githubusercontent.com/35227976/97080852-c66f6e00-15fe-11eb-92f4-ea0fef54e98e.png)\r\n\r\n**Describe the expected behavior**\r\nThe input and target `Numpy` arrays per batch combined have a size of `2*N*H*W*C*sizeof(np.float32) = 320 MB`. The CPU to GPU transfer bandwidth should be around `12.4 GB/s`, e.g. according to the [sample bandwidth test](https://github.com/NVIDIA/cuda-samples/tree/v10.1/Samples/bandwidthTest) from NVIDIA:\r\n```\r\nHost to Device Bandwidth, 1 Device(s)\r\n PINNED Memory Transfers\r\n   Transfer Size (Bytes)        Bandwidth(GB/s)\r\n   32000000                     12.4\r\n```\r\nso the CPU to GPU transfer cannot explain the almost 900ms per batch alone.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tqdm\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\nclass NumpySequence(Sequence):\r\n    def __init__(self, x, y):\r\n        self.x = x\r\n        self.y = y\r\n\r\n    def __len__(self):\r\n        return len(self.x)\r\n\r\n    def __getitem__(self, idx):\r\n        return self.x[idx], self.y[idx]\r\n\r\nworkers = 1\r\nsteps_per_epoch, epochs = 100, 10\r\nN, H, W, C = 4, 1000, 1000, 10\r\n\r\nx_train = []\r\ny_train = []\r\nrng = np.random.default_rng()\r\nfor step in tqdm.tqdm(range(steps_per_epoch)):\r\n    x_train.append(rng.standard_normal(size=(N, H, W, C), dtype=np.float32))\r\n    y_train.append(rng.standard_normal(size=(N, H, W, C), dtype=np.float32))\r\n\r\ndata_sequence = NumpySequence(x_train, y_train)\r\n\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.Activation(\"linear\")])\r\nmodel.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy())\r\nmodel.fit(data_sequence, epochs=epochs, workers=workers)\r\n```\r\n\r\n**Is `TensorFlow` doing some additional serialization or memory copies of the `Numpy` arrays? If so, is there any way to prevent them?**\r\n\r\nI'm aware of `tf.data.Dataset` and the optimizations it applies. Unfortunately, my actual pre-processing pipeline, of which the above example is only a simple caricature, is too complex to port to `tf.data.Dataset`. Also, if I understand it correctly, `tf.keras.Model.fit()` internally transforms the `tf.keras.utils.Sequence` to `tf.data.Dataset` using the `from_generator` generator method.\r\n", "comments": ["@daniel-pp I can reproduce the issue. We will look into the root-cause of the issue. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/2cc2d1b50157feac339e4ba3d412a2e6/untitled.ipynb) is a gist for our reference. Thanks!", "Hi, I have a very similar issue in a similar setting:\r\n\r\nI train with model.fit and tf.keras.utils.Sequence on a RTX 2080TI. My batches are quite large (approx 250MB).\r\nnvidia-smi reported by GPU wasn't reaching 100% usage, and I used tensorflow profiling tools to investigate.\r\n\r\nThe profiling tools showed that the batch data Host->GPU (_Recv, IteratorGetNext) upload is not interleaved with the computation of the previous batch. Instead when the processing of the batch starts, it performs the upload (and waits for it).\r\nA fourth of the GPU time is wasted on this upload which could have been done during the processing of the previous batch.\r\n\r\nI don't know if that is exactly the problem reported here, but it seems quite related.", "@jvishnuvardhan Is there any activity on this issue? We are facing the exact same problem on our side.\r\n@daniel-pp Were you able to workaround the problem?", "@dakshvar22 I could not find a workaround, and I am also wondering if there has been any work on the issue.", "@daniel-pp @dakshvar22 Sorry for late response. \r\n\r\nCan you please open this issue with the standalone code in Keras-team/keras repository? I am not able to move this to keras repo as it is outside of TF repository. Is it possible for one of you to open it in keras-team/keras repo? \r\n\r\nKeras team is focussed on resolving issues in Keras repo. Sorry for the inconvenience. Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Please note that I moved this issue to keras-team/keras repo where keras team focussed to resolved important bugs. Thanks"]}, {"number": 44265, "title": "TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. TensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.  Fatal error: Failed to allocate memory for input tensors.", "body": "Hi,\r\n\r\nI am trying to use Activity i3D model on mobile devices. Have successfully Converted the i3D model check points to tflite. Now while loading the tflite model into the mobile application using TensorFlowLite Library. Getting the following error.\r\n\r\nGitHub link to checkpoints : [https://github.com/deepmind/kinetics-i3d](https://github.com/deepmind/kinetics-i3d)\r\n\r\n**TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nTensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.\r\n\r\nFatal error: Failed to allocate memory for input tensors.**\r\n\r\nPlease help me find anything I am missing.\r\nThanks :-)", "comments": ["@HemantMehtaM17 Can you please share a simple standalone to reproduce the error? Thanks!", "Hi @jvishnuvardhan ,\r\n\r\nThanks for your reply. Here is the things you have asked for\r\n\r\nI have downloaded the frozen graph from I3D git for kinetics\r\n\r\n[https://download.01.org/opencv/public_models/032020/i3d-rgb/rgb.frozen.pb](https://download.01.org/opencv/public_models/032020/i3d-rgb/rgb.frozen.pb) \r\n\r\nwith the help of following script have successfully converted frozen graph to tflite file to use in mobile application.\r\n\r\n\r\nimport tensorflow as tf\r\nwith tf.compat.v1.Session() as sess:\r\n    sess.run(tf.compat.v1.global_variables_initializer())\r\n    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\r\n    'downloaded_frozen_activity.pb', ['Placeholder'], ['Softmax'])\r\n   \r\n    converter.allow_custom_ops=True\r\n    converter.target_ops = [tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\r\n    \r\n     \r\n    tflite_model = converter.convert()\r\n    open(\"coverted.tflite\", \"wb\").write(tflite_model)\r\n\r\nwith the help of this tflite file I have created a demo project in iOS. You can see the project file from below link\r\n\r\n[https://github.com/HemantMehtaM17/i3DDemo](https://github.com/HemantMehtaM17/i3DDemo)\r\n\r\ngetting this error\r\n\r\n**TensorFlow Lite Error: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nTensorFlow Lite Error: Node number 0 (FlexConv3D) failed to prepare.\r\n\r\nFatal error: Failed to allocate memory for input tensors.**\r\n\r\n\r\n\r\nThanks\r\n\r\n\r\n\r\n", "@HemantMehtaM17 You are using SELECT_TF_OPS during conversion. In this case, you will need to use the Interpreter which supports SELECT mode.\r\n\r\nPlease check [this comment](https://github.com/tensorflow/tensorflow/issues/43934#issuecomment-714832778) for more info on the root-cause and resource to resolve. Thanks!", "@jvishnuvardhan I have used the same. But still getting the same error. You can check the pod file. I am already using the same.", "Did you add the linker flag to force linking of TF ?\r\n\r\ngo to Build Settings -> Other Linker Flags, and add:\r\n-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps\r\n\r\nSee\r\nhttps://www.tensorflow.org/lite/guide/ops_select#ios", "Yeah I did but nothing changes.", "@yyoon Can you please check\r\n\r\nThanks", "We've recently fixed a similar issue here: https://github.com/tensorflow/tensorflow/issues/44879.\r\nCan you try running `pod update` to update to the newest version and see if that resolves your issue?", "I have updated the pod and try run the project again, getting same issue. I have already uploaded the code to GitHub please go through that.\r\n\r\nhttps://github.com/HemantMehtaM17/i3DDemo", "What is the `TensorFlowLiteSelectTfOps` version actually installed for your project? Can you copy & paste the content of the `Podfile.lock` file here? It should point to the most recent nightly version.\r\n\r\nI cloned your repository and confirmed that it's working fine with the latest release.\r\nI ran the following commands from the terminal:\r\n\r\n```sh\r\ngit clone git@github.com:HemantMehtaM17/i3DDemo.git\r\npod update --repo-update\r\n```\r\n\r\nNext, I opened the workspace with Xcode by running `open i3DDemo.xcworkspace` from the terminal, and added the linker flag mentioned above.\r\n\r\n![image](https://user-images.githubusercontent.com/1309817/100971291-eab55780-3579-11eb-8972-aaf4b66a759b.png)\r\n\r\nAfter these steps, I was able to see the following output message from the debug console, instead of the error you mentioned originally.\r\n\r\n```\r\n2020-12-03 15:06:59.384628+0900 i3DDemo[50920:12375299] Initialized TensorFlow Lite runtime.\r\n2020-12-03 15:06:59.385169+0900 i3DDemo[50920:12375299] Created TensorFlow Lite delegate for select TF ops.\r\n2020-12-03 15:06:59.385991+0900 i3DDemo[50920:12375299] TfLiteFlexDelegate delegate: 199 nodes delegated out of 199 nodes with 1 partitions.\r\n```\r\n\r\nPlease try following these steps exactly, and let me know if this works for you.", "Hi @yyoon , Im Facing same issue on iOS 15 device , \r\ntried mentioned workarounds, using following pods -\r\n \r\n ` pod 'TensorFlowLiteObjC'\r\n  pod 'libextobjc' \r\n  pod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'`\r\n  \r\n  added additional linker flag to force load the select TF ops framework into project , Settings -> Other Linker Flags, \r\n\r\n`-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps`\r\n`-u _TF_AcquireFlexDelegate`\r\nGetting following error - \r\nUndefined symbol: _TfLiteTensorCo\r\n<img width=\"1424\" alt=\"Screenshot 2021-12-08 at 4 48 22 PM\" src=\"https://user-images.githubusercontent.com/60549753/145199662-5136e74c-7629-4057-8633-b2e689c2923b.png\">\r\npy", "@yyoon  then I Updated TensorFlowLiteSelectTfOps to latest version 2.7 getting following errors \r\n\r\n<img width=\"1375\" alt=\"Screenshot 2021-12-08 at 5 03 07 PM\" src=\"https://user-images.githubusercontent.com/60549753/145201640-01a36229-6b88-4790-be7b-29017a7bc36f.png\">\r\n", "Could you try to use 2.7 for `TensorFlowLiteObjC` pod ?\r\nI think it's better to use the same version with `TensorFlowLiteSelectTfOps`.", "@terryheo thank you for quick response, it started working after I updated all pods to 2.6 version,\r\n\r\npod 'TensorFlowLiteObjC' , '2.6.0'\r\npod 'libextobjc'\r\npod 'TensorFlowLiteSelectTfOps' ,'2.6.0'", "It's good to hear. Thanks for the update.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44265\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44265\">No</a>\n", "> @terryheo thank you for quick response, it started working after I updated all pods to 2.6 version,\r\n> \r\n> pod 'TensorFlowLiteObjC' , '2.6.0' pod 'libextobjc' pod 'TensorFlowLiteSelectTfOps' ,'2.6.0'\r\n\r\nHad the same issue:\r\n```\r\n  // Didn't work\r\n  pod 'TensorFlowLiteSwift', '2.7.0'\r\n  pod 'TensorFlowLiteSelectTfOps', '2.7.0'\r\n```\r\n```\r\n  // Worked\r\n  pod 'TensorFlowLiteSwift', '2.6.0'\r\n  pod 'TensorFlowLiteSelectTfOps', '2.6.0'\r\n```\r\nMaybe this issue is related with arm64 mac's and 2.7.0 version?"]}, {"number": 44264, "title": "[FeatureRequest]Support tilde as alias for home directory in file paths", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI was trying to use `~/dir/file` style file-path to create `Dataset` using `tf.data.Dataset.list_files`, but it failed with `NotFoundError: No such file or directory`. With an expanded home directory path it works fine. Is it possible to add this feature to automatically resolve `tilde` as home directory.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNot sure.\r\n\r\n**Who will benefit with this feature?**\r\nMaybe everyone (Or at least some lazy people like me)\r\n\r\n**Any Other info.**\r\n", "comments": ["Typically Python APIs don't expand `~`, e.g. `open('~/file')` will not expand the `~`. `~` expansion is usually only done automatically by shells. To get the behavior of starting a path with `~`, users should call `os.path.expanduser(\"~\")` directly. It is possible for a filename to start with the character `'~'`, so this change would technically break backwards compatibility. Closing for now, feel free to reopen if you think there are more good arguments for automatically expanding `~/`", "Just spent 3 hours head banging not knowing why the `save` function was not working. I would recommend that if you do not support `~` as a prefix then at least display/log a warning of some kind so the user is aware of this..."]}, {"number": 44263, "title": "I had tried this code in Xcode with objective c but I am getting the error \"Explicit specialization of undeclared template struct 'NumTraits'\" could anyone help me out please?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please fill in issue template", "Closing since issue template was not filled."]}, {"number": 44262, "title": "TFLu: Update builds with CMSIS-NN", "body": "Do not download if an external path is already provided.\r\nForce download (immediately call download script) when using recursive_find.\r\n\r\nThis is fixing issue: https://github.com/tensorflow/tensorflow/issues/44261\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@mansnils  Can you please check @advaitjain's comments and resolve conflicts?. Thanks!", "> Just highlighting here as well that I misspoke about other targets relying on CMSIS. If Arduino and mbed are ok without patching then we can remove it completely with this PR to keep things simple.\r\n> \r\n> Other than that your proposed changes look good to me.\r\n\r\nDid some more digging and chatted with @petewarden and we will need to patch for the Arduino even though the `test_arduino.sh` script passes.\r\n\r\nLet's stick to the plan of continuing to patch for the Arduino and nothing else. Separately, I'm going to change the script so that it will catch this issue.", "@mansnils: I created https://github.com/tensorflow/tensorflow/pull/44406 to first remove the patching of the downloaded CMSIS (and partially document why the Arduino still works).\r\n\r\nNext I think we might be able to remove the leading cmsis in the include paths and then we should be good in terms of supporting a non-downloaded CMSIS. I'll create one more PR and send it your way tomorrow.", "@advaitjain  Cool, CMSIS patching is now removed. I wait for your next PR then to remove the leading cmsis in the include path. After that I propose to go back to using wildcards as it makes maintenance easier (less merge conflicts). I can make a PR for that or update this.", "> @advaitjain Cool, CMSIS patching is now removed. I wait for your next PR then to remove the leading cmsis in the include path. After that I propose to go back to using wildcards as it makes maintenance easier (less merge conflicts). I can make a PR for that or update this.\r\n\r\nWildcards for the CMSIS/NN directory sounds ok to me. Let's keep the full list for files in Core and DSP."]}, {"number": 44261, "title": "CMSIS repository always has to be downloaded an patched", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): 392321a9a02f98baf7d9c0e34005d19568b38848\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nSome targets like Arduino need to patch the CMSIS repository when downloading it. Most other targets that don't need this, still always do it as default. It would be better if only those targets need patching, do that.\r\nAlso when supplying a path to an external CMSIS on the command line, the CMSIS repository is still downloaded. Actually a combination of the two different CMSIS repositories are then used. It would be better to not download when supplying an external CMSIS and only use that.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nFor example this will still download a CMSIS repository and patch it:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile CMSIS_PATH=/path/to/CMSIS_5/ TAGS=cmsis-nn TARGET=stm32f4  test\r\n", "comments": ["We are no longer patching CMSIS with #44406 "]}, {"number": 44260, "title": "How to force to run a function on GPU", "body": "Hello:\r\n\r\nI have the following code:\r\n\r\n`config=tf.compat.v1.ConfigProto(log_device_placement=True)\r\nconfig.gpu_options.visible_device_list='0'\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\r\ntf.compat.v1.enable_eager_execution()\r\ntf.compat.v1.reset_default_graph()`\r\n\r\n`with tf.compat.v1.Session(config=config) as sess:\r\n    x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\r\n                                                 TRAIN_SPLIT, past_history,\r\n                                                 future_target, STEP)\r\n\r\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n    c = tf.matmul(a, b)\r\n    #print(sess.run(x_train_multi, y_train_multi))\r\n    print(sess.run(x_train_multi, y_train_multi))\r\n    sess.close()`\r\n\r\nThe function, that makes \r\n\r\n`def multivariate_data(dataset, target, start_index, end_index, history_size,\r\n                      target_size, step, single_step=False):\r\n  #data = []\r\n  #labels = []\r\n  data = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\n  labels = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\r\n  #data = tf.placeholder(name=\"data\", shape=[None, 720, 3], dtype=tf.float32)\r\n  #labels = tf.placeholder(name= \"labels\", shape=[None, 36], dtype=tf.float32)\r\n  start_index = start_index + history_size\r\n  if end_index is None:\r\n    end_index = len(dataset) - target_size\r\n  #print(history_size)\r\n  for i in range(start_index, end_index):\r\n    indices = range(i-history_size, i, step)\r\n    for j in indices:\r\n     data = data.write(j, dataset[j])\r\n    #data.append(dataset[indices])\r\n    if single_step:\r\n      labels = labels.write(i, target[i+target_size])\r\n      #labels.append(target[i+target_size])\r\n    else:\r\n       for j in range(i, i+target_size):\r\n          labels = labels.write(j, target[j])\r\n      #labels.append(target[i:i+target_size])\r\n  return data.stack(), labels.stack()\r\n  #return np.array(data), np.array(labels)`\r\n\r\nIt runs on CPU, not on GPU:\r\nCPU - 40%, Physical Memory - 97%, GPU - 2%\r\n\r\n", "comments": ["You can try using `with tf.device('GPU:0'):`\r\nSee https://www.tensorflow.org/guide/gpu#manual_device_placement", "I tried to use it, but mu function runs on CPU anyway:\r\nCPU - 23%, GPU - 2%, PhysicalMemory - 86%", "@vasilevskykv \r\n\r\nPlease, fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).Please, let us know which TF version you are using.Thanks!", "I use TensorFlow 2.3.0 and TensorFlow-GPU 2.0.0", "As I understood, I need to make all my operations using tf in the form tf.method() - every variable, indices should be transformed into tf.Variable, tf.Constant. And the only problems remains: how to transform method **append**, appliable to numpy.ndarray to TensorFlow and how to make slices in TensorFlow, that are equivalent to slices of numpy.ndarray such as **target[i:i+target_size]**", "And in my session what should I write in method run\r\nprint(sess.run(data1, data2)), where data1 and data2 are tensors, obtained after splitting?", "What is your cuda/cudnn/nvidia driver version?\r\nTypically you want to compute ops within the `tf.device` namescope to enforce manual device placement.\r\nAlso can you share the complete minimal code to repro the behavior?\r\nThanks!", "NVIDIA GeForce GTX 1050\r\nDriver Version 441.87\r\nCUDA Version 10.1\r\ncuddn version: 7.6.4", "\r\nHere is the piece of my code:\r\n**def multivariate_data(dataset, target, start_index, end_index, history_size,\r\n                      target_size, step, single_step=False):\r\n  data = []\r\n  labels = []\r\n  start_index = start_index + history_size\r\n  if end_index is None:\r\n    end_index = len(dataset) - target_size\r\n  #print(history_size)\r\n  for i in range(start_index, end_index):\r\n    indices = range(i-history_size, i, step)\r\n    data.append(dataset[indices])\r\n    if single_step:\r\n      labels.append(target[i+target_size])\r\n    else:\r\n      labels.append(target[i:i+target_size])\r\n  return np.array(data), np.array(labels)\r\n\r\nfeatures_considered = ['open', 'close', 'ema']\r\nfeatures = df[features_considered]\r\nfeatures.index = df['date']\r\n\r\ndataset = features.values\r\ndata_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\r\ndata_std = dataset[:TRAIN_SPLIT].std(axis=0)\r\ndataset = (dataset-data_mean)/data_std\r\n\r\nconfig=tf.compat.v1.ConfigProto(log_device_placement=True)\r\nconfig.gpu_options.visible_device_list='0'\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\r\n\r\nwith tf.compat.v1.Session(config=config) as sess:\r\n\r\n  with tf.device('/device:GPU:0'):\r\n    x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\r\n                                                 TRAIN_SPLIT, past_history,\r\n                                                 future_target, STEP)\r\n    \r\n  sess,run(x_train_multi, y_train_multi)\r\n  sess.close()\r\n**", "Can you please attach the full logs?", "2020-10-30 12:00:11.707090: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-10-30 12:00:11.769924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-10-30 12:00:11.891164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1050 major: 6 minor: 1 memoryClockRate(GHz): 1.493\r\npciBusID: 0000:01:00.0\r\n2020-10-30 12:00:11.916420: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-10-30 12:00:11.934542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-10-30 12:00:25.256704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-30 12:00:27.077890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-10-30 12:00:27.090521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-10-30 12:00:27.175818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1843 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n2020-10-30 12:00:27.261296: I tensorflow/core/common_runtime/direct_session.cc:359] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\r\n\r\nAnd then I'm waiting for a long time finishing of calculations.", "70% Physical Memory\r\n30% CPU\r\n", "And here is the end:\r\nTraceback (most recent call last):\r\n  File \"D:\\CryptoTrading\\Prediction\\BinanceNeuroPrediction\\Pred4Year.py\", line 81, in <module>\r\n    print(sess.run(x_train_multi))\r\n  File \"I:\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 956, in run\r\n    run_metadata_ptr)\r\n  File \"I:\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1105, in _run\r\n    raise RuntimeError('The Session graph is empty.  Add operations to the '\r\nRuntimeError: The Session graph is empty.  Add operations to the graph before calling run().", "I have already updated my Driver Version. Now it is 457.09", "As I see the problem is absence of method **append** in Tensorflow with the following logic:\r\n**v = tf.Variable([],tf.int32)\r\nfor i in range(3):\r\n tf.append(v, i)**\r\n\r\nAnd after thet we should get something like that:\r\nAn array in **v** must be [0,1,2] instead []", "Hi @vasilevskykv ! you can use tf.device() to force a operation to either use GPU or CPU . You can use `tf.debugging.set_log_device_placement(True)`  to verify which devices you are using . Attaching [Gist](https://colab.sandbox.google.com/gist/mohantym/86eb5726fe00b97ae0f363b904aafbf9/github_44620.ipynb) for reference. Thanks!\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44260\">No</a>\n"]}, {"number": 44259, "title": "How to use tf.keras.texts_to_sequences in tf.dataset", "body": "I have a text classification task whose feeding files are lines of text. Before, I use keras and generator to handle with massive dataset.\r\nNow I want to use tf.dataset which can feed the data in a parallel way that is important to me as the bottleneck now is IO and text transformation.\r\nI tried to implement my code below, like a protype, but is does not work, as it will not operate the get_keras_data function in parallel way.\r\nI suppose that the core problem is that API such as `texts_to_sequences` `pad_sequences` can not be used for the tensor, while they just use for the numpy arrary.\r\n\r\nSo is there any other approaches to meet my expectation?\r\n\r\n```\r\ndef get_keras_data(lines):\r\n    random.shuffle(lines)\r\n    infos_feat, infos_feat_sub, infos_label = [], [], []\r\n    for line in lines:\r\n        line = line.decode('UTF-8')\r\n        label = line.strip().split(' ')[0].replace('__label__','')\r\n        label_ix = app_id[label]\r\n        feat = ' '.join(line.strip().split(' ')[1:])\r\n\r\n        infos_feat.append(feat)\r\n        infos_label.append(label_ix)\r\n        infos_feat_sub.append(feat.replace('.',' '))\r\n\r\n    content_ids_list = tok.texts_to_sequences(infos_feat)\r\n    content_pad_array = tf.keras.preprocessing.sequence.pad_sequences(content_ids_list, maxlen=maxlen, padding='post', truncating='post',value=0)\r\n    content_ids_list = tok_sub.texts_to_sequences(infos_feat_sub)\r\n    content_pad_array_sub = tf.keras.preprocessing.sequence.pad_sequences(content_ids_list, maxlen=max_len_sub, padding='post', truncating='post',value=0)\r\n\r\n    trunk = ([content_pad_array, content_pad_array_sub], \\\r\n            tf.keras.utils.to_categorical(np.array(infos_label), num_classes = 998, dtype='int' ))\r\n    return trunk\r\n\r\n\r\ndef generator(path, batch_size):\r\n    file_list = glob.glob(\"{}/part-*.csv\".format(path))\r\n    dataset = tf.data.TextLineDataset(file_list, num_parallel_reads=32 )\r\n    for lines in dataset.shuffle(buffer_size=buffer_size).repeat().batch(batch_size).prefetch(20).as_numpy_iterator():\r\n        trunk = get_keras_data(lines)\r\n        yield trunk\r\n```", "comments": ["@yananchen1989,\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 44258, "title": "tflite file size is 1KB ?", "body": "**System information**\r\n- OS Platform and Distribution (windows 10):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (2.3):\r\n\r\nim trying to convert my costum model ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8 to tflite so I can deploy it on android phone\r\n\r\nFirst Iam generating tflite graph\r\n\r\n```\r\npython export_tflite_graph_tf2.py --pipeline_config_path=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\models\\my_ssd_mobilenet_v2_fpnlite\\pipeline.config --trained_checkpoint_dir=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\exported-models\\my_mobilenet_model\\checkpoint --output_directory=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\r\n```\r\nthen this command for getting the model.tflite\r\n\r\n# the exact command\r\n```\r\ntflite_convert --saved_model_dir=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\saved_model --output_file=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\TFL.tflite\r\n\r\n```\r\n# Copy and paste the output here.\r\n\r\n```\r\n\r\n(tf) PS D:\\models\\TensorFlowColored89\\models\\research\\object_detection> tflite_convert --saved_model_dir=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\saved_model --output_file=D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\TFL.tflite\r\n2020-10-23 12:01:07.115542: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 12:01:16.832442: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll\r\n2020-10-23 12:01:17.685510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-10-23 12:01:17.685853: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 12:01:17.693815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-10-23 12:01:17.699545: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-23 12:01:17.702309: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-10-23 12:01:17.708703: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-23 12:01:17.713493: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-23 12:01:17.724389: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 12:01:17.724807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-23 12:01:17.725887: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-23 12:01:17.736662: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15884ebf8c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-10-23 12:01:17.736911: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-10-23 12:01:17.737896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-10-23 12:01:17.738470: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 12:01:17.739031: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-10-23 12:01:17.739685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-23 12:01:17.740310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-10-23 12:01:17.740747: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-23 12:01:17.741304: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-23 12:01:17.741781: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 12:01:17.742327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-23 12:01:18.332670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 12:01:18.332908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-10-23 12:01:18.333608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-10-23 12:01:18.334330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-10-23 12:01:18.338688: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x158b91aab40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-10-23 12:01:18.338823: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1\r\nI1023 12:02:20.237044 33924 lite.py:624] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2020-10-23 12:02:23.839000: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\r\n2020-10-23 12:02:23.839271: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\r\n2020-10-23 12:02:23.841449: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\saved_model\r\n2020-10-23 12:02:23.929815: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-10-23 12:02:23.930050: I tensorflow/cc/saved_model/loader.cc:250] Reading SavedModel debug info (if present) from: D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\saved_model\r\n2020-10-23 12:02:23.931573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 12:02:23.931855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]\r\n2020-10-23 12:02:24.264018: I tensorflow/cc/saved_model/loader.cc:215] Restoring SavedModel bundle.\r\n2020-10-23 12:02:24.910611: I tensorflow/cc/saved_model/loader.cc:199] Running initialization op on SavedModel bundle at path: D:\\models\\TensorFlowColored89\\workspace\\training_demo\\tflite_exported\\saved_model\r\n2020-10-23 12:02:25.185379: I tensorflow/cc/saved_model/loader.cc:319] SavedModel load for tags { serve }; Status: success: OK. Took 1343952 microseconds.\r\n2020-10-23 12:02:27.159515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1\r\ncoreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020-10-23 12:02:27.160041: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 12:02:27.161714: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll\r\n2020-10-23 12:02:27.162453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-23 12:02:27.163015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll\r\n2020-10-23 12:02:27.163827: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-23 12:02:27.164425: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-23 12:02:27.164992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 12:02:27.165708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-10-23 12:02:27.166372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 12:02:27.166798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0\r\n2020-10-23 12:02:27.167201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N\r\n2020-10-23 12:02:27.167793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2989 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(tf) PS D:\\models\\TensorFlowColored89\\models\\research\\object_detection>\r\n```\r\nthe tfl.tflite file size is 1Kb\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nopen the file with sublime  and it contains this ;\r\n1c00 0000 5446 4c33 0000 1200 1c00 0400\r\n0800 0c00 1000 1400 0000 1800 1200 0000\r\n0300 0000 1400 0000 1400 0000 8000 0000\r\n1800 0000 2800 0000 0000 0000 0200 0000\r\nc000 0000 7c00 0000 0400 0000 b401 0000\r\nb001 0000 0001 0000 3800 0000 0100 0000\r\n0c00 0000 0800 0c00 0400 0800 0800 0000\r\n0800 0000 0300 0000 1300 0000 6d69 6e5f\r\n7275 6e74 696d 655f 7665 7273 696f 6e00\r\n42ff ffff 0400 0000 1000 0000 0000 0000\r\n0000 0000 0000 0000 0000 0000 0f00 0000\r\n4d4c 4952 2043 6f6e 7665 7274 6564 2e00\r\nceff ffff 1400 0000 1400 0000 1400 0000\r\n1400 0000 1400 0000 0000 0000 0000 0000\r\n0000 0000 0000 0000 0400 0000 4e6f 4f70\r\n0000 0e00 1800 0400 0800 0c00 1000 1400\r\n0e00 0000 1400 0000 1c00 0000 2000 0000\r\n3000 0000 3000 0000 0200 0000 9800 0000\r\n4400 0000 0100 0000 0000 0000 0400 0000\r\n0100 0000 0100 0000 0100 0000 0100 0000\r\n0000 0000 0400 0000 6d61 696e 0000 0600\r\n0800 0400 0600 0000 0400 0000 0400 0000\r\n0000 0000 beff ffff 1000 0000 0200 0000\r\n0c00 0000 2c00 0000 0000 0000 1900 0000\r\n5374 6174 6566 756c 5061 7274 6974 696f\r\n6e65 6443 616c 6c3a 3300 0000 0400 0600\r\n0400 0000 0000 0e00 1400 0400 0000 0800\r\n0c00 1000 0e00 0000 1000 0000 0100 0000\r\n1c00 0000 3400 0000 0400 0000 0100 0000\r\n4001 0000 4001 0000 0300 0000 1700 0000\r\n7365 7276 696e 675f 6465 6661 756c 745f\r\n696e 7075 743a 3000 fcff ffff 0400 0400\r\n0400 0000 \r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nI think its failing to convert to tflite \r\n", "comments": ["@MustafaAlahmid \r\nPlease share complete stand alone code for the issue faced or if possible share a colab gist with the issue reported.", "What is the issue here?", "issue solved, the converting didn't work with TF2.3.1, i had to use Nightly version for this \r\n", "@MustafaAlahmid \r\nPlease move this to closed status as issue is resolved.", "I am having the same issue with TF 2.3.1.  How do I use nightly for this?  Or is it same scripts and everything, just in an env with the latest tf.nightly?", "Actually, I solved it, just created a new conda env as tfnightly and installed package and all dependencies.  Ran the same python script below and it works:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport argparse\r\n\r\n# Convert the model\r\n# normal input is 1x string path of model directory\r\n# normal output is 1x file tf.lite at model directory path\r\n\r\ndef ConvertModel(saved_model_dir, output):\r\n    print(f'TF version is: {tf.__version__}')\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    tflite_model = converter.convert()\r\n    open(output, \"wb\").write(tflite_model)\r\n\r\n\r\n\r\n\r\nif __name__ == \"__main__\":  #upon python script execution in cmd prompt, __name__ variable is issued and becomes __main__ variable\r\n    parser = argparse.ArgumentParser(description='Convert to tf.lite')\r\n    parser.add_argument('-m', '--model_dir', type=str,  metavar='', help='directory location of the tf_light_frozen_graph')\r\n    parser.add_argument('-o', '--output', type=str, metavar='', help='output path and file name of model')\r\n    args = parser.parse_args()\r\n\r\n    ConvertModel(args.model_dir, args.output)\r\n```", "> Actually, I solved it, just created a new conda env as tfnightly and installed package and all dependencies. Ran the same python script below and it works:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import argparse\r\n> \r\n> # Convert the model\r\n> # normal input is 1x string path of model directory\r\n> # normal output is 1x file tf.lite at model directory path\r\n> \r\n> def ConvertModel(saved_model_dir, output):\r\n>     print(f'TF version is: {tf.__version__}')\r\n>     converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n>     tflite_model = converter.convert()\r\n>     open(output, \"wb\").write(tflite_model)\r\n> \r\n> \r\n> \r\n> \r\n> if __name__ == \"__main__\":  #upon python script execution in cmd prompt, __name__ variable is issued and becomes __main__ variable\r\n>     parser = argparse.ArgumentParser(description='Convert to tf.lite')\r\n>     parser.add_argument('-m', '--model_dir', type=str,  metavar='', help='directory location of the tf_light_frozen_graph')\r\n>     parser.add_argument('-o', '--output', type=str, metavar='', help='output path and file name of model')\r\n>     args = parser.parse_args()\r\n> \r\n>     ConvertModel(args.model_dir, args.output)\r\n> ```\r\n\r\nSorry, late replay,\r\n\r\nyes exactly you make 2 environments, one of them with TF nightly just to convert tf to tflite  ", "For anyone having this issue in the future, You **NEED** to use TF2.4 and above for **BOTH** the \"export_tflite_graph_tf2.py\" and the \"tf.lite.TFLiteConverter\"."]}, {"number": 44257, "title": "Tensorflow combining with PySpark pandas_udf yields \"triggered tf.function retracing\".", "body": "Here is a snippet of my code combining Tensorflow with Spark for online prediction:\r\n```\r\n@pandas_udf(StringType())\r\ndef online_predict(values: pd.Series) -> pd.Series:\r\n    pred = Model.from_config(bc_config.value)\r\n    pred.set_weights(bc_weights.value)\r\n    ds = tf.data.Dataset.from_tensor_slices(values)\r\n    ds = ds.map(preprocessing).batch(batch_size)\r\n    res = pred.predict_step(ds)\r\n    res = tf.norm(res, axis=1)\r\n    # res = tf.greater(res, 5.0)\r\n    res = tf.strings.as_string(res).numpy()\r\n    return pd.Series(res)\r\n\r\n\r\nspark = SparkSession.builder.appName(\r\n    'spark_tf').master(\"local[*]\").getOrCreate()\r\nweights = np.load('./ext/weights.npy', allow_pickle=True)\r\nconfig = np.load('./ext/config.npy', allow_pickle=True).item()\r\nbc_weights = spark.sparkContext.broadcast(weights)\r\nbc_config = spark.sparkContext.broadcast(config)\r\n\r\nstream = spark.readStream.format('kafka') \\\r\n    .option('kafka.bootstrap.servers', 'localhost:9092') \\\r\n    .option('subscribe', 'network') \\\r\n    .load()\r\n\r\nstream = stream.select(online_predict(col('value')).alias('value'))\r\n\r\nx = stream.writeStream \\\r\n    .format('kafka') \\\r\n    .option(\"kafka.bootstrap.servers\", 'localhost:9092') \\\r\n    .option('topic', 'dlpred') \\\r\n    .option('checkpointLocation', './kafka_checkpoint') \\\r\n    .start()\r\n\r\nx.awaitTermination()\r\n```\r\n\r\nRunning the code yields this warning when data come repeatedly from Kafka topic, I assume this is due to the repeat of creating dataset:\r\n\r\n> WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ca6abf598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n\r\n\r\n\r\n", "comments": ["@tungnat97 \r\n\r\nRequest you to share complete code snippet with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.\r\n\r\nIf you want to suppress the warnings please use the below code before importing tensorflow.\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\n```\r\nThanks!\r\n", "As for now I can't provide the full code as the code uses other libraries rather than Tensorflow and also interacts with a local server. I am also not able to create an alternative code right now, but the problem seems to come from the `predict` function in the code snippet I posted earlier, as the problem was gone after I changed it to `predict_step` inside a loop of the dataset. I'm not dealing with sequence data (tabular data indeed) so it's not due to the variable length problem. I will try to come up with an alternative code asap.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44256, "title": "Docs for LambdaCallback should mention that it only works in training mode", "body": "## URL(s) with the issue:\r\n\r\nDocumentation page for `tf.keras.callbacks.LambdaCallback`:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback?hl=en\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe docs for the base class `tf.keras.callbacks.Callback` mentions that the methods `on_epoch_begin`, `on_epoch_end`, `on_batch_begin` and `on_batch_end` are supposed to be called in training mode only, but this information is not included in the docs for `tf.keras.callbacks.LambdaCallback`. This may cause confusion when users pass this callback to the `callbacks` parameter of `tf.keras.Model.predict` or `tf.keras.Model.evaluate` and subsequently find out that the supplied functions are not being called.\r\n\r\nI suggest that this information be added to the documentation to make it clear that `tf.keras.callbacks.LambdaCallback` does not do anything in predict and test mode.\r\n\r\nOptionally, the functionality of `tf.keras.callbacks.LambdaCallback` could be expanded to include the rest of the available callback hooks in `tf.keras.callbacks.Callback` (`on_predict_batch_begin`, `on_predict_batch_end` etc.) so that it may be used in predict and test mode as well.", "comments": ["I want to do.", "Thanks for your issue.\r\n`tf.keras.callbacks.LambdaCallback` works with [`model.evaluate`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#evaluate) and [`.predict`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict) methods since they both take `batch_size` arg. The docs are updated to add this information."]}, {"number": 44255, "title": "Different ABI between docker version and public version in tf 2.3.0", "body": "Here is the link, where we first discussed issues with docker tensorflow:2.3.0 and tensorflow-text==2.3.0. https://github.com/tensorflow/text/issues/385\r\nAs pointed there - Docker image with tag 2.3.0 and tensorflow==2.3.0 have different ABIs.\r\nCould it be fixed in future updates?", "comments": ["@mihaimaruseac @broken I'm not very familiar with the ABIs and how this could have happened, or if it could happen again.\r\n\r\nDo either of you have any more insight on this? I'm wondering if it would be fixed if we just re-deployed the 2.3.0 containers.", "Let's try to redeploy the 2.3.0 container. It's possible we generated them using a different branch by mistake.", "We redeployed the containers for `2.3.0` and `2.3.1`. Can you check and see if the issue persists? ", "Closing. Please reopen if there is still an issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44255\">No</a>\n"]}, {"number": 44254, "title": "tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command", "body": "### System information\r\n\r\n-   **NVIDIA Driver**: 455.23.05\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 20.04)**:\r\n-   **TensorFlow installed from binary**: from r2.3 branch\r\n-   **TensorFlow version (use command below)**: 2.3.0\r\n-   **Python version**: 3.8.5\r\n-   **Bazel version (if compiling from source)**: 3.1.0\r\n-   **GCC/Compiler version (if compiling from source)**: 9.3.0\r\n-   **CUDA/cuDNN version**: cuda: 11.1, cuDNN: 8.0.4, TensorRT: 7.2\r\n-   **GPU model and memory**: GTX 1060 6GB\r\n-   **Exact command to reproduce**: `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures` also tried `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\r\n\r\n### Describe the problem\r\nI have the freshest version of CUDA 11.1, cuDNN 8.0.4 and TensorRT 7.2. That's why to use CUDA and GPU I decided to build tf from this link https://www.tensorflow.org/install/source\r\nFirstly, I've build tf with the master branch and successfully built tensorflow 2.4.0 .whl file and installed it with pip. While `./configure` I've specified all actual versions and everything worked fine.\r\nHowever then I needed tf 2.3 and tried to rebuild everything at `r2.3` branch. There I've begun to receive different errors during compilation. Some of them fixed (for instance I've added 11.0 to cuda, cudart in `cuda_configure.bzl`. Also I've installed numpy<1.19.0.\r\nBTW, that what I installed: \r\n```\r\npip install -U --user pip six 'numpy<1.19.0' wheel setuptools mock 'future>=0.17.1' 'gast==0.3.3' typing_extensions\r\npip install -U --user keras_applications --no-deps\r\npip install -U --user keras_preprocessing --no-deps\r\n```\r\nBut that didn't help. If if was the first build, I would've thought - something with cuda, nccl versions etc. However, 2.4.0 build was perfect and still no workarounds from another issues are helpful. Could you help me please?\r\n### Source code / logs\r\n\r\n```INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/kirill/Documents/tensorflow/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kirill/.cache/bazel/_bazel_kirill/aeab598ed30f46d483876f775f0431ef/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib/cuda/include:/usr/lib/cuda/lib64: \\\r\n    PATH=/home/kirill/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -DMLIR_CUDA_CONVERSIONS_ENABLED -DSQLITE_OMIT_DEPRECATED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cusolver/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusolver/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/local_config_cuda/cuda/cusparse/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusparse/include -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ntensorflow/python/lib/core/bfloat16.cc: In function \u2018bool tensorflow::{anonymous}::Initialize()\u2019:\r\ntensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  664 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  668 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  671 |   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n      |                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  675 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  679 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\ntensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to \u2018(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)\u2019\r\n  683 |                       compare_types)) {\r\n      |                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note: candidate: \u2018tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\u2019\r\n  637 |   auto register_ufunc = [&](const char* name, PyUFuncGenericFunction fn,\r\n      |                         ^\r\ntensorflow/python/lib/core/bfloat16.cc:637:25: note:   no known conversion for argument 2 from \u2018<unresolved overloaded function type>\u2019 to \u2018PyUFuncGenericFunction\u2019 {aka \u2018void (*)(char**, const long int*, const long int*, void*)\u2019}\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /home/kirill/Documents/tensorflow/tensorflow/tools/pip_package/BUILD:66:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/kirill/.cache/bazel/_bazel_kirill/aeab598ed30f46d483876f775f0431ef/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib/cuda/include:/usr/lib/cuda/lib64: \\\r\n    PATH=/home/kirill/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o' -DMLIR_CUDA_CONVERSIONS_ENABLED -DSQLITE_OMIT_DEPRECATED -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DGRPC_ARES=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DOPENSSL_IS_BORINGSSL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/host/bin -iquote external/local_config_python -iquote bazel-out/host/bin/external/local_config_python -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif -iquote bazel-out/host/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/host/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/host/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/host/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/local_config_tensorrt -iquote bazel-out/host/bin/external/local_config_tensorrt -iquote external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/host/bin/external/snappy -iquote external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/bin/external/aws -iquote external/aws-c-common -iquote bazel-out/host/bin/external/aws-c-common -iquote external/aws-c-event-stream -iquote bazel-out/host/bin/external/aws-c-event-stream -iquote external/aws-checksums -iquote bazel-out/host/bin/external/aws-checksums -iquote external/mkl_dnn -iquote bazel-out/host/bin/external/mkl_dnn -iquote external/llvm-project -iquote bazel-out/host/bin/external/llvm-project -iquote external/cub_archive -iquote bazel-out/host/bin/external/cub_archive -iquote external/nccl_archive -iquote bazel-out/host/bin/external/nccl_archive -iquote external/png -iquote bazel-out/host/bin/external/png -iquote external/lmdb -iquote bazel-out/host/bin/external/lmdb -iquote external/gemmlowp -iquote bazel-out/host/bin/external/gemmlowp -iquote external/icu -iquote bazel-out/host/bin/external/icu -iquote external/org_sqlite -iquote bazel-out/host/bin/external/org_sqlite -iquote external/com_github_grpc_grpc -iquote bazel-out/host/bin/external/com_github_grpc_grpc -iquote external/upb -iquote bazel-out/host/bin/external/upb -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/host/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusolver_headers_virtual -Ibazel-out/host/bin/external/cub_archive/_virtual_includes/cub -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/nccl -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs -Ibazel-out/host/bin/external/nccl_archive/_virtual_includes/src_hdrs -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cusparse_headers_virtual -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LoopPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/LLVMAVX512ConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AVX512IncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMConversionIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToNVVMGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ParallelLoopMapperAttrGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToROCDLTGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ROCDLOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/GPUToSPIRVIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpUtilsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/SPIRVPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/QuantPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardPatternsIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen -Ibazel-out/host/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsTransformsPassIncGen -Ibazel-out/host/bin/external/local_config_cuda/cuda/_virtual_includes/cupti_headers_virtual -isystem external/local_config_python/numpy_include -isystem bazel-out/host/bin/external/local_config_python/numpy_include -isystem external/local_config_python/python_include -isystem bazel-out/host/bin/external/local_config_python/python_include -isystem external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif -isystem bazel-out/host/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/host/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include -isystem external/aws/aws-cpp-sdk-transfer/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-transfer/include -isystem external/aws-c-common/include -isystem bazel-out/host/bin/external/aws-c-common/include -isystem external/aws-c-event-stream/include -isystem bazel-out/host/bin/external/aws-c-event-stream/include -isystem external/aws-checksums/include -isystem bazel-out/host/bin/external/aws-checksums/include -isystem external/mkl_dnn/include -isystem bazel-out/host/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/host/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/host/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/host/bin/external/mkl_dnn/src/cpu/xbyak -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda/cusolver/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusolver/include -isystem external/png -isystem bazel-out/host/bin/external/png -isystem external/local_config_cuda/cuda/cusparse/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cusparse/include -isystem external/icu/icu4c/source/common -isystem bazel-out/host/bin/external/icu/icu4c/source/common -isystem external/llvm-project/mlir/include -isystem bazel-out/host/bin/external/llvm-project/mlir/include -isystem tensorflow/compiler/mlir/tensorflow/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/tensorflow/include -isystem tensorflow/compiler/mlir/xla/include -isystem bazel-out/host/bin/tensorflow/compiler/mlir/xla/include -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Transforms/InstCombine -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Transforms/InstCombine -isystem external/llvm-project/llvm/lib/Target/NVPTX -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/NVPTX -isystem external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversions/GPUToSPIRV -isystem external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem bazel-out/host/bin/external/llvm-project/mlir/lib/Conversion/StandardToSPIRV -isystem external/llvm-project/llvm/lib/Target/X86 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/X86 -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -isystem external/com_github_grpc_grpc/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/include -isystem external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem bazel-out/host/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated -isystem external/com_github_grpc_grpc/third_party/address_sorting/include -isystem bazel-out/host/bin/external/com_github_grpc_grpc/third_party/address_sorting/include -isystem external/local_config_cuda/cuda/cuda/extras/CUPTI/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/extras/CUPTI/include -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/python/lib/core/bfloat16.cc -o bazel-out/host/bin/tensorflow/python/_objs/bfloat16_lib/bfloat16.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 2.180s, Critical Path: 1.67s\r\nINFO: 4 processes: 4 local.\r\nFAILED: Build did NOT complete successfully```", "comments": ["Switched to branch `v2.3.0` and everything works fine.\r\nHowever I don't know should I close this issue, because on branch `r2.3` it's still an issue.", "Interesting, I am hitting the same issue on tags v2.3.0 and v2.3.1. 2.4.x compiles fine for me though", "FWIW I am hitting the same. on v2.3.0 same config, esp. after complete restart of all services (bazel server caches something?)", "@Ecclesiast Could you please try on latest stable version of TF 2.6.0 and let us know if this is still an issue.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44254\">No</a>\n"]}, {"number": 44253, "title": "tensorflow.map_fn support in Custom Layer.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.8\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: 10.1.243 / 7.6.5\r\n- GPU model and memory: GeForce GTX 950M 2GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to create a custom layer that calculates the forward kinematics for a robotic arm using 'DH parameters'. In my code, I am using the 6 joint angles as the input of the custom layer (Kinematics_Physics) and I am using tensorflow.map_fn to iteratively calculate the forward kinematics of each set of angles in the input. My goal is to set the 'DH parameters' as the trainable weights and train a model to optimize the 'DH parameters'.  when I test the custom layer with tf.Variable as input it works without any error. Also, when I used the custom layer wrapped in a tensorflow.keras.layers.Lambda layers, there are no errors but obviously the weights of my custom layer is not visible to the tensorflow. as can be seen from the model.summary() output. However when I try to use the custom layer as it is in the model, tensorflow throws the error as \"TypeError: 'Tensor' object cannot be interpreted as an integer\".\r\n\r\n**Describe the expected behavior**\r\nThere could be something wrong in my implementation of the custom layer or probably the Tensorflow API doesn't support the map_fn for layers in the Functional API for building models. Is there a workaround for this problem? Any recommendations?\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nfrom tensorflow import math as m\r\nfrom tensorflow.keras import layers\r\n\r\nclass Kinematics_Physics(layers.Layer):\r\n\r\n    def __init__(self,):\r\n        super(Kinematics_Physics, self).__init__()\r\n        # define initial variables\r\n        self.TF_PI = tf.constant(3.1415926535897932, dtype =tf.float32)\r\n        self.TF_180 = tf.constant(180.0, dtype =tf.float32)\r\n        self.TINY_VALUE = tf.constant(1e-6, dtype =tf.float32)\r\n        self.out_Pose=tf.Variable([0,0,0,0,0,0],  dtype =tf.float32, trainable = False)\r\n        self.A_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)\r\n        self.B_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)\r\n        self.C_Deg = tf.Variable(0,dtype = tf.float32, trainable = False)\r\n        self.sA = tf.Variable(0,dtype = tf.float32, trainable = False)\r\n        self.cA = tf.Variable(0,dtype = tf.float32, trainable = False)\r\n        self.b = tf.Variable(tf.zeros((4,4),dtype = tf.float32),dtype = tf.float32, trainable = False)\r\n        \r\n        # initial dh parameters: 6 x 4 parameters for a robotic arm with 6 joints.\r\n        dh = tf.constant([  [0 ,    180.,   -650.,  0.],\r\n                            [270.,   90.,      0.,  0.],\r\n                            [800.,    0.,      0.,  0.],\r\n                            [140.,   90.,   -908.,  0.],\r\n                            [0.,    -96.,      0.,  0.],\r\n                            [0.,    -65.,   260.,   0.]] ,dtype = tf.float32)\r\n        \r\n        # convert to trainable weights?? for further optimization \r\n        self.dh = tf.Variable(initial_value=dh, trainable=True)\r\n\r\n        # initialize a buffer to calculate the the modified angles\r\n        self.dh_processed = tf.Variable(tf.zeros(self.dh.shape,dtype = tf.float32),dtype = tf.float32, trainable = False)\r\n        # buffer for transformation matrix\r\n        self.trans_matrix = tf.Variable(tf.zeros((4,4),dtype = tf.float32),  dtype =tf.float32, trainable = False)\r\n\r\n\r\n    @tf.function\r\n    def radians(self,a):\r\n        return m.multiply(a,(tf.divide(self.TF_PI, self.TF_180 )))\r\n    \r\n    @tf.function\r\n    def degrees(self,a):\r\n        return m.multiply(a,(tf.divide(self.TF_180 , self.TF_PI)))\r\n    \r\n    @tf.function\r\n    def joint_transform(self,input_values):\r\n        '''input: format --> 1D array = [a, \u03b1, d, \u03b8]'''\r\n    \r\n        a = input_values[0] # a\r\n        alpha = self.radians(input_values[1]) #convert degrees to radians\r\n        d = input_values[2] # d\r\n        theta = self.radians(input_values[3]) #convert degrees to radians \u03b8\r\n        \r\n        self.trans_matrix.assign(\r\n                                [[      m.cos(theta),             -m.sin(theta),              0,                a],\r\n                                [m.multiply(m.sin(theta),m.cos(alpha)),  m.multiply(m.cos(theta),m.cos(alpha)), -m.sin(alpha), m.multiply(-m.sin(alpha),d)],\r\n                                [m.multiply(m.sin(theta),m.sin(alpha)),  m.multiply(m.cos(theta),m.sin(alpha)),  m.cos(alpha),  m.multiply(m.cos(alpha),d)],\r\n                                [        0,                             0,                     0,            1        ]])\r\n        return self.trans_matrix\r\n    \r\n    @tf.function\r\n    def to_pose(self,T):\r\n        # converts the transformation matrix to pose [x, y, z, alpha ]\r\n        if(m.abs(T[1,2]) <= self.TINY_VALUE and m.abs(T[2,2]) <= self.TINY_VALUE): #% singular : B = +-90 (Gimbal Lock)\r\n            self.C_Deg.assign(tf.constant(0, dtype =tf.float32))\r\n            self.B_Deg.assign(self.degrees(m.atan2( T[0,2] , m.divide_no_nan(T[2,2], m.cos(self.C_Deg)) ))) #convert radians to degree\r\n            self.A_Deg.assign(self.degrees(m.atan2( T[1,0] ,  m.divide_no_nan(T[1,1], m.cos(self.C_Deg)) ))) #convert radians to degree\r\n        \r\n        else:\r\n            self.A_Deg.assign(self.degrees(m.atan2(-T[0,1] , T[0,0]))) #convert radians to degree\r\n            self.sA.assign(m.sin(self.radians(self.A_Deg)))\r\n            self.cA.assign(m.cos(self.radians(self.A_Deg)))\r\n            self.B_Deg.assign(self.degrees(m.atan2(T[0,2] , m.multiply(self.cA,T[0,0]) - m.multiply(self.sA,T[0,1]))))#convert radians to degree\r\n            self.C_Deg.assign(self.degrees(m.atan2(-T[1,2] , T[2,2]))) #convert radians to degree\r\n        \r\n        self.out_Pose.assign([ T[0,3], T[1,3], T[2,3],self.A_Deg, self.B_Deg , self.C_Deg])\r\n        return self.out_Pose\r\n    \r\n    \r\n    @tf.function\r\n    def forward_kinematics(self, theta):\r\n        \r\n        # add the dh_theta to measured theta to get the final theta\r\n        actual_theta = m.add(self.dh[:,3] ,theta)\r\n        \r\n        # create new dh_table with modified theta for forward kinematics calculation\r\n        self.dh_processed[:,0].assign(self.dh[:,0])\r\n        self.dh_processed[:,1].assign(self.dh[:,1])\r\n        self.dh_processed[:,2].assign(self.dh[:,2])\r\n        self.dh_processed[:,3].assign(actual_theta)\r\n        b = tf.eye(4, dtype = tf.float32)\r\n        for i in tf.range(6):\r\n            b = (tf.linalg.matmul(b,self.joint_transform(self.dh_processed[i])))\r\n        return self.to_pose(b)\r\n      \r\n    def __call__(self, inputs):\r\n        return tf.map_fn(self.forward_kinematics, inputs, parallel_iterations=True, dtype=tf.float32)\r\n    \r\n    def get_config(self):\r\n        config = super(Kinematics_Physics, self).get_config()\r\n        return config\r\n    \r\n\r\nif __name__ == \"__main__\":\r\n    # create a list of six joint angles\r\n    ang = tf.Variable([[20,10,20,10,10,20],[15,10,20,22,20,32]], dtype = tf.float32, trainable=False)\r\n    # Call the Kinematics_physics layer\r\n    fk = Kinematics_Physics()\r\n    print(\"trainable weights: \", len(fk.trainable_weights))\r\n    print(\"non-trainable weights: \", len(fk.non_trainable_weights))\r\n    print(\"trainable_weights: \", fk.trainable_weights)\r\n    print(\"forward kinematic transform: \", fk(ang[:10]))\r\n    # Everything works up to this.\r\n\r\n    # # create a model using functional API and custom layer wrapped in Lambda layer\r\n    in_= layers.Input(shape=(6,))\r\n    for_kin = layers.Lambda(Kinematics_Physics())(in_)\r\n    model = tf.keras.models.Model(inputs=in_, outputs=for_kin) \r\n    model.summary() \r\n    # Works but the weights are not trainable in this case\r\n    \r\n    \r\n    # create a model using functional API\r\n    in_= layers.Input(shape=(6,))\r\n    for_kin = Kinematics_Physics()(in_)\r\n    model = tf.keras.models.Model(inputs=in_, outputs=for_kin) \r\n    model.summary() \r\n    # Error: expected behavior => should create a model with 24 trainable parameters\r\n    #        but the error happens in tf.map_fn of Kinematics_Physics().__call__ function\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n2020-10-23 16:16:20.529396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2.1.0\r\n2020-10-23 16:16:23.763749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-10-23 16:16:23.980052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 950M computeCapability: 5.0\r\ncoreClock: 0.928GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-10-23 16:16:23.980403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 16:16:23.991313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll \r\n2020-10-23 16:16:23.997168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll  \r\n2020-10-23 16:16:23.999492: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll \r\n2020-10-23 16:16:24.006531: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-23 16:16:24.014520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-23 16:16:24.031075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 16:16:24.035576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-10-23 16:16:24.037553: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-10-23 16:16:24.041785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 950M computeCapability: 5.0\r\ncoreClock: 0.928GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-10-23 16:16:24.043759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-10-23 16:16:24.044399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-10-23 16:16:24.045042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-10-23 16:16:24.045625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-10-23 16:16:24.046172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-10-23 16:16:24.046644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-10-23 16:16:24.065410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-10-23 16:16:24.070724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-10-23 16:16:25.336541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-10-23 16:16:25.337273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-10-23 16:16:25.338396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N\r\n2020-10-23 16:16:25.358220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1370 MB memory) -> physical GPU (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\ntrainable weights:  1\r\nnon-trainable weights:  9\r\ntrainable_weights:  [<tf.Variable 'Variable:0' shape=(6, 4) dtype=float32, numpy=\r\narray([[   0.,  180., -650.,    0.],\r\n       [ 270.,   90.,    0.,    0.],\r\n       [ 800.,    0.,    0.,    0.],\r\n       [ 140.,   90., -908.,    0.],\r\n       [   0.,  -96.,    0.,    0.],\r\n       [   0.,  -65.,  260.,    0.]], dtype=float32)>]\r\n2020-10-23 16:16:27.011142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\nforward kinematic transform:  tf.Tensor(\r\n[[ 548.9288   -118.251114 -527.5552     47.53804   -30.621014 -144.55815 ]\r\n [ 511.83533   -85.7232   -492.43372    49.052803  -46.484085 -145.28882 ]], shape=(2, 6), dtype=float32)\r\nWARNING:tensorflow:\r\nThe following Variables were used a Lambda layer's call (lambda), but\r\nare not present in its tracked objects:\r\n  <tf.Variable 'Variable:0' shape=(6, 4) dtype=float32>\r\nIt is possible that this is intended behavior, but it is more likely\r\nan omission. This is a strong indication that this layer should be\r\nformulated as a subclassed Layer rather than a Lambda layer.\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 6)]               0\r\n_________________________________________________________________\r\nlambda (Lambda)              (None, 6)                 0\r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTraceback (most recent call last):\r\n  File \"c:/Users/ABCD/Desktop/physics_layer.py\", line 127, in <module>\r\n    for_kin = Kinematics_Physics()(in_)\r\n  File \"c:/Users/ABCD/Desktop/physics_layer.py\", line 99, in __call__\r\n    return tf.map_fn(self.forward_kinematics, inputs, parallel_iterations=True, dtype=tf.float32)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py\", line 228, in map_fn\r\n    for elem in elems_flat]\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\map_fn.py\", line 228, in <listcomp>\r\n    for elem in elems_flat]\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\", line 1078, in __init__\r\n    name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\tensor_array_ops.py\", line 716, in __init__\r\n    self._tensor_array = [None for _ in range(size)]\r\nTypeError: 'Tensor' object cannot be interpreted as an integer\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.1 and [TF v2.3](https://colab.research.google.com/gist/amahendrakar/9debe2f689e568a98b33ad6c0238cb59/44253.ipynb). \r\n\r\nHowever, on running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/db65d519032cdfa0dfb4952b58961b62/44253-tf-nightly.ipynb#scrollTo=iv9S1HHP_G_S) the error changes to `TypeError: Could not build a TypeSpec for <KerasTensor: shape=(None, 6) dtype=float32 (created by layer 'input_2')> with type KerasTensor`. Please find the attached gist. Thanks!", "It appears that you are overriding the `__call__` method on `Layer`, but you should actually be overriding `call`. Try to see if it fixes your issue (there may be multiple issues with your code).", "@kumar10725 I think this was resolved. when I updated your code based on @fchollet suggestion, I don't see any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2222de56f93623682119d69c27a6b1b3/44253-tf-nightly.ipynb).\r\n\r\nPlease verify once and close the issue. Thanks!", "@kumar10725 Closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks!"]}, {"number": 44252, "title": "How can I obtain the output of an intermediate layer (feature extraction) in Model Subclassing ?", "body": "Please refer to the below link for more details on the issue.\r\n\r\nhttps://stackoverflow.com/questions/64471742/skip-some-layers-in-keras-model-during-evaluation-validation-phase\r\n\r\nMy requirement is to override the Evaluation/Validation step after each epoch, with using the Existing Fit function.\r\n\r\nFollowing below link does not work (when this code is written in test_step method)\r\nhttps://keras.io/getting_started/faq/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction\r\n", "comments": ["@Rajatsharma07 \r\nAs the issue is not a bug or a feature request and is tracked on stackoverflow, and there is also a larger community that reads questions there please move it closed status on here.", "Not resolved, Will be tracking on the StackOverflow link."]}, {"number": 44251, "title": "Failed build with Cuda-11.1/TF2.3/cuDNN 8.0.4", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3\r\n- Python version: 3.8.6\r\n- Installed using virtualenv? pip? conda?: venv with pip\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version: 11.1 / 8.0.4\r\n- GPU model and memory: RTX 3090 w/ 24 GB \r\n\r\n\r\n\r\n**Describe the problem**\r\nBazel (via Baselisk) errors in `local_config_cuda`. I've referenced issues: listed [here](https://github.com/tensorflow/tensorflow/issues/38660#issuecomment-616581116) and [here](https://github.com/tensorflow/tensorflow/issues/37888). I have set the Bazel `BAZEL_VC` path, and MSYS2 is installed and pathed correctly.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nC:\\Users\\Kevin>.virtualenvs\\tf_sourcebuild\\Scripts\\activate\r\n\r\n(tf_sourcebuild) C:\\Users\\Kevin>cd TF_2.3_Source\\tensorflow\r\n\r\n(tf_sourcebuild) C:\\Users\\Kevin\\TF_2.3_Source\\tensorflow>configure.py\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Kevin\\.virtualenvs\\tf_sourcebuild\\Scripts\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Kevin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include\r\nFound cuDNN 8 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.6\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]: --copt=-march=native\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n\r\n(tf_sourcebuild) C:\\Users\\Kevin\\TF_2.3_Source\\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Kevin/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --config=xla --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUBLAS_VERSION=11 --action_env TF_CUDNN_VERSION=8.0.4 --action_env TF_TENSORRT_VERSION=7.2.1 --action_env TF_CUDA_PATHS=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:opt in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.tf_configure.bazelrc: --copt=--copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain\r\nINFO: Found applicable config definition build:windows in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 955, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 657, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 635, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 629, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 0.524s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n```\r\nTF_CUDA_PATHS = \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1, C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\include,C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\lib\\x64,C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin\"\r\n```\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I've now also tried to build the master branch but no luck (same issue)", "@kvndhrty \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source_windows#gpu).\r\nAlso, refer similar issues #43947, #43720 and see if it helps you. Thanks!", "Ok, I tried with TF2.3 and the master dev branch with CUDA 11/cuDNN 8.0.4 and I get the same error. Am I doing something else wrong here? ", "@kvndhrty \r\n\r\nPlease, share the error log with the above mentioned configuration you are seeing. Can you try to reinstall from scratch in a fresh environment and see if the issue still persists. Thanks!", "Ok, I cleaned and re-installed everything (other than the virtualenv, happy to do that as well) and got the same error:\r\n\r\n```\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Kevin/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Kevin/.virtualenvs/tf_sourcebuild/Scripts/python.exe --config=xla --action_env TF_CUDA_VERSION=11.0 --action_env TF_CUBLAS_VERSION=11 --action_env TF_CUDNN_VERSION=8.0.4 --action_env TF_TENSORRT_VERSION=7.2.1 --action_env TF_CUDA_PATHS=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.6 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:opt in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:windows in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\kevin\\tf_2.3_source\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl:1430:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1400\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 217, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/kevin/_bazel_kevin/v3btk44j/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1400\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 217, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1400\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 977, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 666, in _get_cuda_config\r\n                find_cuda_config(repository_ctx, <2 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 643, in find_cuda_config\r\n                _exec_find_cuda_config(<3 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 637, in _exec_find_cuda_config\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/remote_config/common.bzl\", line 217, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nThe system cannot find the path specified.\r\nINFO: Elapsed time: 0.275s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n```\r\n\r\nIs cuDNN 8.0.4 the issue? Is there something else that has a missing path? The error doesn't give me much of a hint\r\n\r\nEDIT: I ran \"git pull --rebase\" since I had forgotten to re-install / update the TF repo so this is the master branch of TF and reattempted the build. The `configure` commands I ran are the same as the top of the file. Note the different error info though!", "The error seems to be a bad path:\r\n\r\n```\r\nThe system cannot find the path specified.\r\n```\r\n\r\nCan you edit `File \"C:/users/kevin/tf_2.3_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1400` to identify what path the code is looking for and then validate that it exists?", "This seems to have stalled. Will close but please reopen if there is more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44251\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44251\">No</a>\n"]}, {"number": 44250, "title": "ValueError: Input 0 of layer sequential is incompatible with the layer: expected ndim=4, found ndim=3.", "body": "tensorflow: 2.2\r\npython: 3.8\r\ni use tf.data.dataset pipline to fit, but it has some error, i check my input shape and simplify my model, error still exists. i don`t know what`s problem happend?\r\n\r\ntrain_x = np.random.uniform(0, 10, (300, 12, 24, 3))\r\ntrain_y = np.random.randint(0, 2, 300)\r\nval_x = np.random.uniform(0, 10, (100, 12, 24, 3))\r\nval_y = np.random.randint(0, 2, 100)\r\n\r\nmodel = Sequential([\r\n    layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(12, 24, 3)),\r\n    layers.MaxPooling2D(),\r\n    layers.GlobalAveragePooling2D(),\r\n    layers.Dense(1, activation='sigmoid'),\r\n])\r\n\r\nmodel.compile(loss='mse', optimizer=optimizers.SGD(1e-3), metrics=['accuracy'])\r\n\r\nAUTOTUNE = tf.data.experimental.AUTOTUNE\r\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y))\r\ntrain_ds = train_ds.shuffle(300)\r\ntrain_ds = train_ds.repeat()\r\ntrain_ds = train_ds.batch(32)\r\ntrain_ds = train_ds.prefetch(AUTOTUNE)\r\n\r\nval_ds = tf.data.Dataset.from_tensor_slices((val_x, val_y)).prefetch(AUTOTUNE)\r\n\r\nmodel.fit(train_ds, validation_data=val_ds, epochs=10, steps_per_epoch=10)\r\n\r\n![image](https://user-images.githubusercontent.com/47907486/96959081-c9505d00-1531-11eb-9392-2ff1d8c201d6.png)\r\n![image](https://user-images.githubusercontent.com/47907486/96959136-e2590e00-1531-11eb-903a-3f400f00dc60.png)\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44250\">No</a>\n", "i find val_ds has not batched"]}, {"number": 44249, "title": "Failed to find the dnn implementation", "body": "UnknownError                              Traceback (most recent call last)\r\n<ipython-input-7-9cfed8880631> in <module>\r\n     14                     validation_data=(data[\"X_test\"], data[\"y_test\"]),\r\n     15                     callbacks=[checkpointer, tensorboard],\r\n---> 16                     verbose=1)\r\n     17 model.save(os.path.join(\"results\", model_name) + \".h5\")\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError:  [_Derived_]  Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_7959]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n\r\n\r\n\u200b\r\n", "comments": ["@rohan1090 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease share a simple indented stand alone code to replicate the issue faced or if possible share a colab gist with the issue reported.\r\n\r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.", "import tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\r\nfrom sklearn import preprocessing\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import accuracy_score\r\nfrom yahoo_fin import stock_info as si\r\nfrom collections import deque\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport time\r\nimport os\r\nimport random\r\n\r\n# set seed, so we can get the same results after rerunning several times\r\nnp.random.seed(314)\r\ntf.random.set_seed(314)\r\nrandom.seed(314)\r\n\r\n## Hyper-parameters\r\nimport os\r\nimport time\r\nfrom tensorflow.keras.layers import LSTM\r\n\r\n# Window size or the sequence length\r\nN_STEPS = 70\r\n# Lookup step, 1 is the next day\r\nLOOKUP_STEP = 1\r\n\r\n# test ratio size, 0.2 is 20%\r\nTEST_SIZE = 0.2\r\n# features to use\r\nFEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\r\n# date now\r\ndate_now = time.strftime(\"%Y-%m-%d\")\r\n\r\n### model parameters\r\n\r\nN_LAYERS = 3\r\n# LSTM cell\r\nCELL = LSTM\r\n# 256 LSTM neurons\r\nUNITS = 256\r\n# 40% dropout\r\nDROPOUT = 0.4\r\n# whether to use bidirectional RNNs\r\nBIDIRECTIONAL = False\r\n\r\n### training parameters\r\n\r\n# mean absolute error loss\r\n# LOSS = \"mae\"\r\n# huber loss\r\nLOSS = \"huber_loss\"\r\nOPTIMIZER = \"adam\"\r\nBATCH_SIZE = 64\r\nEPOCHS = 400\r\n\r\n# Tesla stock market\r\nticker = \"TSLA\"\r\nticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\r\n# model name to save, making it as unique as possible based on parameters\r\nmodel_name = f\"{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\r\nif BIDIRECTIONAL:\r\n    model_name += \"-b\"\r\n\r\ndef load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \r\n                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\r\n    \"\"\"\r\n    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\r\n    Params:\r\n        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\r\n        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\r\n        scale (bool): whether to scale prices from 0 to 1, default is True\r\n        shuffle (bool): whether to shuffle the data, default is True\r\n        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\r\n        test_size (float): ratio for test data, default is 0.2 (20% testing data)\r\n        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\r\n    \"\"\"\r\n    # see if ticker is already a loaded stock from yahoo finance\r\n    if isinstance(ticker, str):\r\n        # load it from yahoo_fin library\r\n        df = si.get_data(ticker)\r\n    elif isinstance(ticker, pd.DataFrame):\r\n        # already loaded, use it directly\r\n        df = ticker\r\n    else:\r\n        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\r\n\r\n    # this will contain all the elements we want to return from this function\r\n    result = {}\r\n    # we will also return the original dataframe itself\r\n    result['df'] = df.copy()\r\n\r\n    # make sure that the passed feature_columns exist in the dataframe\r\n    for col in feature_columns:\r\n        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\r\n\r\n    if scale:\r\n        column_scaler = {}\r\n        # scale the data (prices) from 0 to 1\r\n        for column in feature_columns:\r\n            scaler = preprocessing.MinMaxScaler()\r\n            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\r\n            column_scaler[column] = scaler\r\n\r\n        # add the MinMaxScaler instances to the result returned\r\n        result[\"column_scaler\"] = column_scaler\r\n\r\n    # add the target column (label) by shifting by `lookup_step`\r\n    df['future'] = df['adjclose'].shift(-lookup_step)\r\n\r\n    # last `lookup_step` columns contains NaN in future column\r\n    # get them before droping NaNs\r\n    last_sequence = np.array(df[feature_columns].tail(lookup_step))\r\n    \r\n    # drop NaNs\r\n    df.dropna(inplace=True)\r\n\r\n    sequence_data = []\r\n    sequences = deque(maxlen=n_steps)\r\n\r\n    for entry, target in zip(df[feature_columns].values, df['future'].values):\r\n        sequences.append(entry)\r\n        if len(sequences) == n_steps:\r\n            sequence_data.append([np.array(sequences), target])\r\n\r\n    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\r\n    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\r\n    # this last_sequence will be used to predict future stock prices not available in the dataset\r\n    last_sequence = list(sequences) + list(last_sequence)\r\n    last_sequence = np.array(last_sequence)\r\n    # add to result\r\n    result['last_sequence'] = last_sequence\r\n    \r\n    # construct the X's and y's\r\n    X, y = [], []\r\n    for seq, target in sequence_data:\r\n        X.append(seq)\r\n        y.append(target)\r\n\r\n    # convert to numpy arrays\r\n    X = np.array(X)\r\n    y = np.array(y)\r\n\r\n    # reshape X to fit the neural network\r\n    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\r\n    \r\n    # split the dataset\r\n    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \r\n                                                                                test_size=test_size, shuffle=shuffle)\r\n    # return the result\r\n    return result\r\n\r\ndef create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\r\n                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\r\n    model = Sequential()\r\n    for i in range(n_layers):\r\n        if i == 0:\r\n            # first layer\r\n            if bidirectional:\r\n                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\r\n            else:\r\n                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\r\n        elif i == n_layers - 1:\r\n            # last layer\r\n            if bidirectional:\r\n                model.add(Bidirectional(cell(units, return_sequences=False)))\r\n            else:\r\n                model.add(cell(units, return_sequences=False))\r\n        else:\r\n            # hidden layers\r\n            if bidirectional:\r\n                model.add(Bidirectional(cell(units, return_sequences=True)))\r\n            else:\r\n                model.add(cell(units, return_sequences=True))\r\n        # add dropout after each layer\r\n        model.add(Dropout(dropout))\r\n    model.add(Dense(1, activation=\"linear\"))\r\n    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\r\n    return model\r\n\r\n# create these folders if they does not exist\r\nif not os.path.isdir(\"results\"):\r\n    os.mkdir(\"results\")\r\n\r\nif not os.path.isdir(\"logs\"):\r\n    os.mkdir(\"logs\")\r\n\r\nif not os.path.isdir(\"data\"):\r\n    os.mkdir(\"data\")\r\n\r\n# load the data\r\ndata = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\r\n\r\n# save the dataframe\r\ndata[\"df\"].to_csv(ticker_data_filename)\r\n\r\n# construct the model\r\nmodel = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\r\n                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\r\n\r\n# some tensorflow callbacks\r\ncheckpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\r\ntensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\r\n\r\nhistory = model.fit(data[\"X_train\"], data[\"y_train\"],\r\n                    batch_size=BATCH_SIZE,\r\n                    epochs=EPOCHS,\r\n                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\r\n                    callbacks=[checkpointer, tensorboard],\r\n                    verbose=1)\r\n\r\nmodel.save(os.path.join(\"results\", model_name) + \".h5\")\r\n\r\ndef plot_graph(model, data):\r\n    y_test = data[\"y_test\"]\r\n    X_test = data[\"X_test\"]\r\n    y_pred = model.predict(X_test)\r\n    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\r\n    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\r\n    plt.plot(y_test[-200:], c='b')\r\n    plt.plot(y_pred[-200:], c='r')\r\n    plt.xlabel(\"Days\")\r\n    plt.ylabel(\"Price\")\r\n    plt.legend([\"Actual Price\", \"Predicted Price\"])\r\n    plt.show()\r\n\r\n\r\ndef get_accuracy(model, data):\r\n    y_test = data[\"y_test\"]\r\n    X_test = data[\"X_test\"]\r\n    y_pred = model.predict(X_test)\r\n    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\r\n    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\r\n    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\r\n    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\r\n    return accuracy_score(y_test, y_pred)\r\n\r\n\r\ndef predict(model, data):\r\n    # retrieve the last sequence from data\r\n    last_sequence = data[\"last_sequence\"][-N_STEPS:]\r\n    # retrieve the column scalers\r\n    column_scaler = data[\"column_scaler\"]\r\n    # reshape the last sequence\r\n    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\r\n    # expand dimension\r\n    last_sequence = np.expand_dims(last_sequence, axis=0)\r\n    # get the prediction (scaled from 0 to 1)\r\n    prediction = model.predict(last_sequence)\r\n    # get the price (by inverting the scaling)\r\n    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\r\n    return predicted_price\r\n\r\n# load the optimal model weights\r\nmodel_path = os.path.join(\"results\", model_name) + \".h5\"\r\nmodel.load_weights(model_path)\r\n\r\n# load the data with shuffle = False\r\ndata = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\r\n                feature_columns=FEATURE_COLUMNS, shuffle=False)\r\n\r\n# evaluate the model\r\nmse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\r\n# calculate the mean absolute error (inverse scaling)\r\nmean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\r\nprint(\"Mean Absolute Error:\", mean_absolute_error)\r\n# predict the future price\r\nfuture_price = predict(model, data)\r\nprint(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\r\nprint(\"Accuracy Score:\", get_accuracy(model, data))\r\nplot_graph(model, data)\r\n\r\nTHIS IS THE ERROR\r\nTrain on 2023 samples, validate on 506 samples\r\nEpoch 1/400\r\n  64/2023 [..............................] - ETA: 2:51WARNING:tensorflow:Can save best model only with val_loss available, skipping.\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-6-879dac0b4f9a> in <module>\r\n     28                     validation_data=(data[\"X_test\"], data[\"y_test\"]),\r\n     29                     callbacks=[checkpointer, tensorboard],\r\n---> 30                     verbose=1)\r\n     31 \r\n     32 model.save(os.path.join(\"results\", model_name) + \".h5\")\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    340                 mode=ModeKeys.TRAIN,\r\n    341                 training_context=training_context,\r\n--> 342                 total_epochs=epochs)\r\n    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)\r\n    344 \r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\r\n    126         step=step, mode=mode, size=current_batch_size) as batch_logs:\r\n    127       try:\r\n--> 128         batch_outs = execution_function(iterator)\r\n    129       except (StopIteration, errors.OutOfRangeError):\r\n    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py in execution_function(input_fn)\r\n     96     # `numpy` translates Tensors to values in Eager mode.\r\n     97     return nest.map_structure(_non_none_constant_value,\r\n---> 98                               distributed_function(input_fn))\r\n     99 \r\n    100   return execution_function\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    566         xla_context.Exit()\r\n    567     else:\r\n--> 568       result = self._call(*args, **kwds)\r\n    569 \r\n    570     if tracing_count == self._get_tracing_count():\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    630         # Lifting succeeded, so variables are initialized and we can run the\r\n    631         # stateless function.\r\n--> 632         return self._stateless_fn(*args, **kwds)\r\n    633     else:\r\n    634       canon_args, canon_kwds = \\\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n   2365   @property\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1609          if isinstance(t, (ops.Tensor,\r\n   1610                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1611         self.captured_inputs)\r\n   1612 \r\n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1690       # No tape is watching; skip to running the function.\r\n   1691       return self._build_call_outputs(self._inference_function.call(\r\n-> 1692           ctx, args, cancellation_manager=cancellation_manager))\r\n   1693     forward_backward = self._select_forward_and_backward_functions(\r\n   1694         args,\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    543               inputs=args,\r\n    544               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 545               ctx=ctx)\r\n    546         else:\r\n    547           outputs = execute.execute_with_cancellation(\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~\\anaconda3\\envs\\gputest\\lib\\site-packages\\six.py in raise_from(value, from_value)\r\n\r\nUnknownError:  [_Derived_]  Fail to find the dnn implementation.\r\n\t [[{{node CudnnRNN}}]]\r\n\t [[sequential/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_7959]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n", "@rohan1090 \r\nThe code provided is not indented and we are unable to replicate the issue, can you please share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44248, "title": "tf version 1.10.0 does not have MeanSqauredError()", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version:  1.10.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- GCC/Compiler version (if compiling from source): 5.4\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 1060\r\n\r\nI am working on LSTM architecture using Keras (2.2.0) and tf (1.10.1). I have the below code:\r\n\r\nmodel.compile(loss=tf.losses.MeanSquaredError(), optimizer = tf.optimizers.Adam(), metrics = [tf.metrics.MeanAbsoluteError()])\r\n\r\nwhere AttributeError: module 'tensorflow.losses' has no attribute 'MeanSquaredError'. But tf version 1 is supposed to have this loss function. I checked the documentation part as well. https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError\r\n\r\nStill can't get rid of the error. I dont want to upgrade to tf version 2. Cause i have to upgrade my CUDA drivers as well!!\r\n\r\nHow do I fix this?\r\nThanks and Regards\r\nNiranjan. \r\n\r\n", "comments": ["@NiranjanRavi1993 \r\nAs tf 1.1 does not have support, and we have official support for 2.x, can you try on 1.15 and let us know if you still face the issue.", "TF 1.10 is extremely old and no longer updated.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44248\">No</a>\n"]}, {"number": 44245, "title": "ERROR: No matching distribution found for tensorflow", "body": "<em>Hello guys,\r\nI am trying to install Tensorflow but its giving me this error.\r\n\r\n![Capture](https://user-images.githubusercontent.com/63431339/96945131-3a721f00-14f6-11eb-8a52-42f81bb58eca.PNG)\r\n\r\nI have Python V 3.9, Pip version 20.2\r\nPython x64, Windows 10 x64\r\nWhat I have already Tried:\r\nI have tried creating Virtual environment using the following steps,\r\n![Capture1](https://user-images.githubusercontent.com/63431339/96945257-ac4a6880-14f6-11eb-8c34-4e4fddf61966.PNG)\r\n which was successfully created... \r\nalso tried these...\r\n</em>\r\n\r\n`pip install --upgrade `tensorflow``\r\nand\r\n`pip` install https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-2.3.0-cp38-cp38-win_amd64.whl`\r\n (This link was given @ Tensorflow web for Python v3.8, 3.9 not available)\r\nI have already tried doing all these stuff with python v 3.8\r\nMy Paths are also set in Environment Variables\r\n\r\nI have tried almost all suggestion provided on Stackoverflow and Github and even uninstalled and reinstalled Python, still\r\nAny help will be highly appreciated!\r\n", "comments": ["@Zayn1101 \r\nplease confirm if you have verified the compatibility issues and hardware requirements, please refer to this issue and let us know : #42367, you have to try with 3.8 as of now as per the comment in [this issue](40840).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "same problem", "@TheWiesmin \r\nplease conform if you have referred to the above comment.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44245\">No</a>\n"]}, {"number": 44244, "title": "Hardcode paths to `release_pip_rename.sh` (win/gpu)", "body": "Checked that the hardcoded paths exist in the repo", "comments": []}, {"number": 44243, "title": "Unable to change tensor_content in SavedModel loader.cc on s390x", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.8.0-49081-g13b54c02e4 2.2.0\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): 2.0.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nOn s390x architecture (big-endian),`Tensorflow Serving` is unable to serve model that were saved on little-endian systems containing `tensor_content`.  This happens because `tensor_content` data contains little-endian serialized data and there is no way of determining the endiness of the `tensor_content` data.\r\n\r\n**Describe the expected behavior**\r\n`Tensorflow Serving` should be able to serve models with `tensor_content` on big-endian systems.\r\n\r\n**Standalone code to reproduce the issue**\r\nRunning this testcase on s390x `Tensorflow Serving` code will cause the problem:\r\n```\r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test -c dbg --copt=-O -c opt --copt=-g --strip never --host_javabase=\"@local_jdk//:jdk\" --test_tag_filters=-gpu,-benchmark-test,-v1only -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors --output_filter= -- //tensorflow_serving/model_servers:tensorflow_model_server_test\r\n```\r\n\r\n**Other info / logs** \r\nHere is another related [issue](https://github.com/tensorflow/tensorflow/issues/41652). \r\n\r\nAs part of understanding the flow, I have been trying to prototype a patch which will byte-swap `tensor_content` field of the tensor when [loader.cc](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/cc/saved_model/loader.cc#L316) is done with loading the `MetaGraphDef` from the SavedModel.\r\n\r\nI can byte-swap `tensor_content` field in a function but the changes are not persisted for reasons I am unable to figure out.  I have used `mutable` Protobuf calls to make these changes to no avail.  Here is a rough snippet of what I thought would have worked but it did not:\r\n```\r\n// Swap tensor_content of Const Op Tensors in the named functions\r\nstatic Status SwapTensorContent(MetaGraphDef* meta_graph_def) {\r\n  GraphDef graph_def = *meta_graph_def->mutable_graph_def();\r\n  auto library = graph_def.mutable_library();\r\n  for (auto& function : (*library->mutable_function())) {\r\n    for (auto& node : (*function.mutable_node_def())) {\r\n            if (node.op() == \"Const\") {\r\n                auto node_iterator = node.mutable_attr()->find(\"value\");\r\n                if (node_iterator != node.mutable_attr()->end()) {\r\n                    AttrValue node_value = node_iterator->second;\r\n                    if (node_value.has_tensor()) {\r\n                        TensorProto* tproto = node_value.mutable_tensor();\r\n                        auto tsize = tproto->tensor_content().size();\r\n                        if (tsize != 0)\r\n                        {\r\n                                Tensor parsed(tproto->dtype());\r\n                                bool success = parsed.FromProto(*tproto);\r\n                                DCHECK(success);\r\n\t\t\t\t// Try DT_INT64 first\r\n                                if ( tproto->dtype() == DT_INT64) {\r\n\t\t\t\t\t// swap and set the tensor_content LE/BE\r\n                                        TF_RETURN_IF_ERROR(ByteSwapTensor(&parsed));\r\n                                        (*node.mutable_attr())[\"value\"].mutable_tensor()->set_tensor_content(string(reinterpret_cast<const char*>(parsed.tensor_data().data()), parsed.tensor_data().size())); //<--- this seems to work but not retained once loop exists\r\n                                }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n    }\r\n  }  \r\n  return Status::OK();\r\n}\r\n\r\n//loader.cc -> https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/cc/saved_model/loader.cc#L316\r\nStatus LoadSavedModelInternal(const SessionOptions& session_options,\r\n                              const RunOptions& run_options,\r\n                              const string& export_dir,\r\n                              const std::unordered_set<string>& tags,\r\n                              SavedModelBundle* const bundle) {\r\n  const uint64 read_start_microseconds = Env::Default()->NowMicros();\r\n  TF_RETURN_IF_ERROR(ReadMetaGraphDefFromSavedModel(export_dir, tags,\r\n                                                    &bundle->meta_graph_def));\r\nSwapTensorContent(&meta_graph_def);\r\n// Expected meta_graph_def to contain swapped tensor_content but no effect???\r\n```\r\nI may be missing some fundamental aspects around mutability of Nodes in a `MetaGraphDef`.  Would appreciate any pointers.\r\n\r\nThanks.\r\n", "comments": ["@rposts \r\n\r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here](https://github.com/tensorflow/serving/issues/new). Thanks!", "@ravikyram Thanks for the update.  While the issue surfaced in `Tensorflow Serving` the problem is in SavedModel component of Tensorflow.  As you can see above, SavedModel Loader does not check for endiness of the serialized tensor_content. So the question is more about how the protobuf entries backed up by SavedModel can be modified.  Protobuf docs seem to indicate using `mutable_` methods but that is not working using the code above. It is possible that some protobuf messages are not being invoked correctly but it is not obvious which ones are those.", "@rposts \r\n\r\nYou are seeing the same behavior with recent TF versions like 2.3 and nightly versions? Thanks!", "I think I have found a way of achieving this - closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44243\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44243\">No</a>\n"]}, {"number": 44241, "title": "Metrics names get modified for multi-output models", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from: binary, installed with pip\r\n- TensorFlow version (use command below): 2.3.0, 2.3.1 and nightly (2.4.0-dev20201022)\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA T4, 16 GB\r\n\r\n**Describe the current behavior**\r\nWhen training a multi-output model with a metric in `tf.keras.metric`, the name of the metric is modified during training (the name of the output is added as a prefix to the name of the metric). This causes the model to get saved incorrectly when using `save()` (saving to a `h5` file). When the saved model is reloaded, the name of the output is added another time to the metric. So every time we save/load the metric name gets longer. In TF 2.3.0 and 2.3.1, this problem seems to only affect metrics in `tf.keras.metric` and metrics defined by a string (e.g. `accuracy`) don't seem to be affected. However in the nightly this problem seems to have got worse and affects both types of metrics.\r\n\r\n**Describe the expected behavior**\r\nIf a multi-output model is saved to a `h5` file and reloaded, the metric names should be the same as in the original model.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Softmax\r\nfrom tensorflow.keras.models import Model, load_model\r\n\r\n# Create multi-output model\r\ninp = Input(4)\r\nm = Model(inp, [Softmax(name='head_0')(Dense(3)(inp)), Softmax(name='head_1')(Dense(5)(inp))])\r\nmetric = tf.keras.metrics.BinaryAccuracy()\r\nm.compile(loss='mse', metrics = {'head_0': [metric, 'accuracy']})\r\n\r\nprint(f'Metric name: {metric.name}')\r\n\r\n# Run one iteration\r\nx = np.random.rand(2,4)\r\ny = {'head_0': np.random.randint(2, size=(2,3)), 'head_1': np.random.randint(2, size=(2,5))}\r\nm.fit(x, y, verbose=0)\r\n\r\nprint(f'Metric name: {metric.name}')\r\n\r\nprint(m.metrics_names)\r\n\r\n# Save the model to a h5 file and reload it\r\nmodel_fpath = 'model.h5'\r\nm.save(model_fpath, save_format='h5')\r\nm2 = load_model(model_fpath)\r\n\r\nprint(m2.metrics_names)\r\n```\r\n\r\nIn this case we can see that `metric.name` gets modified after `fit` is called. The `metrics_names` before saving are initially:\r\n```\r\n['loss', 'head_0_loss', 'head_1_loss', 'head_0_binary_accuracy', 'head_0_accuracy']\r\n```\r\nAfter reloading in TF 2.3.x:\r\n```\r\n['loss', 'head_0_loss', 'head_1_loss', 'head_0_head_0_binary_accuracy', 'head_0_accuracy']\r\n```\r\nAnd after reloading in TF 2.4.0:\r\n```\r\n['loss', 'head_0_loss', 'head_1_loss', 'head_0_head_0_binary_accuracy', 'head_0_head_0_accuracy']\r\n```", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/37d03cd116a93751dd730f4d647ced25/44241.ipynb). Thanks!", "Wanted to rebump! Any update here? Thanks so much for all your work on tflow, also :) ", "Not a solution but at least a way to deal with the results:\r\n```\r\ndef declutter_metric_name(metric_name, output_names):\r\n    num_substitutions = 0\r\n    for output_name in output_names:\r\n        metric_name, number_of_subs_made = re.subn('({output_name}_){{2,}}'.format(output_name=output_name), output_name + '_', metric_name, count=1)\r\n        num_substitutions += (number_of_subs_made>0)\r\n    if num_substitutions>1:\r\n        warnings.warn(\"Multiple substitutions detected! The result might not be valid.\")\r\n    return metric_name\r\n\r\ndef declutter_history(history, output_names):\r\n    return {declutter_metric_name(metric_name, output_names): metric_values for metric_name, metric_values in history.items()}\r\n```\r\nUsing these functions, multiple occurrences of output names in the metrics names can be corrected:\r\n```\r\ncluttered_metric_name = 'bacon_spam_spam_spam_spam_spam_spam_egg_and_spam'\r\nprint(declutter_metric_name(cluttered_metric_name, [\"spam\"]))\r\n>>> 'bacon_spam_egg_and_spam'\r\n```\r\n", "Hey! Thanks for writing! Yeah, this makes sense, but in theory you might want to have a name like 'bacon_spam_spam' and it becomes hard to disambiguate in general. ", "I confirm this issue which popped up when I tried to re-compile a multi-output model:\r\n\r\n- compile model\r\n- call fit()\r\n- make layer weights trainable/not trainable\r\n- re-compile the model using the same metrics\r\n- calling fit() again\r\n- Issue occurs \r\n\r\nI use TF 2.2. (EDIT: I just checked my TF version, it is 2.3 not 2.2!)\r\nAny updates on that are very much appreciated. Thank you!", "As anecdotal evidence, we noticed this issue when upgrading from TF 2.1 to 2.3, so given your data point it seems that the issue was introduced by TF 2.2", "Ah sorry, I just checked my tensorflow version, it is TF 2.3!", "Oh ok, never mind then. In any case the issue wasn't there in TF 2.1.", "Thanks for reporting this.  It should be fixed with b8d17793a7aae010754d7776612af5b9b1f5252f and 1bd0748a42e2731e04b0b4d4b6fedcc34222492a.  Both should be available in the upcoming 2.5 release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44241\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44241\">No</a>\n"]}]