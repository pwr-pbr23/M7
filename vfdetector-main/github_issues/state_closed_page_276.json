[{"number": 46098, "title": "quant transpose convolution layer is not supported in nnapi acceleration", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n  Not related\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n  gtihub master branch, but this problem exist on v2.3.0 v2.2.0 etc.\r\n\r\n**Describe the current behavior**\r\nQuant transpose convolution layer not supported in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2228\r\nThe layer version of INT8 transpose convolution layer had a version of 2 which failed the above version checking. See below:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc#L261\r\n\r\ni.e. All deep learning model (esp. segmentation) using quant transpose convolution will not get nnapi acceleration on  all Android phone when they are using tensorflow lite to do the inferencing instead of Qualcomm SNPE DL engine to do the inference.\r\n\r\nRelated to https://github.com/tensorflow/tensorflow/issues/46084", "comments": ["@ek9852,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here.\r\n\r\nAlso, please check if you are facing the same issue with TF v2.4 and TF-nightly as well? Thanks!", "I already mentioned  and point out the code related to the problem. And I already dig out the problem with gdb. Please read my text/log.\r\nAnd the same problem  persist with master branch with github as I already mentioned. ", "PR: https://github.com/tensorflow/tensorflow/pull/46183", "The PR looks good to me. Thanks for the fix.", "Together with this patch https://github.com/tensorflow/tensorflow/commit/915c0b2bbc776cff475fac149f2e7ca9a350a715\r\nfixes the problem. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46098\">No</a>\n"]}, {"number": 46097, "title": "[INTEL MKL] Change order for remapper", "body": "This PR is to test if there will be any regression if the order of remapper in grappler meta_optimizer is moved before arithmetic_optimizer.", "comments": ["@ezhulenev could you please check if this PR would have any regression or not?", "Though I did not test for perf regressions, they still could show up after the change will be submitted.", "@ezhulenev I see this has been reverted. Could you please explain me the reason?", "I caused ~8% perf regression in multi-gpu (and I think single GPU as well) Resnet benchmark on V100. Still don't know the root cause.", "@ezhulenev Thanks for your quick reply. If this PR is not the root cause, may I request to bring this PR back.\r\nI have work for couple of fusions (with DNNL aka INTEL_MKL) based on this PR, i.e., an assumption that remapper runs before arithmetic_optimizer.", "The problem is unfortunately in this PR, I'm just not sure what exactly happens, my guess is that some fusion prevents an important arithmetic rewrite from happening. I have 2 perf profiles, will try to find the clue there. Unfortunately it takes an order of hours to reproduce the benchmark in multi-gpu setup...\r\n\r\nWhat are the current problems with remapper running after arithmetic?", "@ezhulenev Thanks again for your kind reply.\r\nI have been preparing a PR for a fusion (MatMul + BiasAdd + Gelu) through remapper optimizer, using pattern matcher that we have contributed earlier. Subgraph for gelu is generated from the api `tf.nn.gelu(feature, approximate=True)`. This optimization is very useful for BERT_LARGE model inference on Intel CPU. It gives us 20% performance improvement for the model inference. Below are sample subgraphs: (left) generated from python api (middle) input graph to remapper when arithmetic_optimizer runs before remapper and (right) input graph to remapper when arithmetic_optimizer does not run before remapper. So arithmetic_optimzer changes the graph that is not obvious from python APIs, while the left most and right most graph are identical. Although I can provide middle graph as an input to the pattern matcher, for future complex fusion it becomes unpredictable pattern for a potential fusion.\r\n\r\n![Gelu-Fusion](https://user-images.githubusercontent.com/27521767/105393816-0b914500-5bda-11eb-9b37-2cfab9967a5e.jpg)\r\n\r\nAdding @agramesh1  in the discussion."]}, {"number": 46096, "title": "Bert Preprocess Model not working on windows 10", "body": "I have the same issue described here [error-with-using-bert-model-from-tensorflow](https://stackoverflow.com/questions/65298391/error-with-using-bert-model-from-tensorflow)\r\n\r\nI get this exception when i try to use the bert preprocessor on windows 10\r\n\r\n`Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface`\r\n\r\n**Stack trace**\r\n```\r\nFile \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\work\\vpython\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface\r\n\t [[{{node prediction/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_tokenizer/StatefulPartitionedCall/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]] [Op:__inference_train_function_52076]\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["@nassimus26 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced], please share code snippet or colab gist with the issue reported.", "@Saduf2019 \r\nWindows 10: Version 20H2 19042.685\r\nPython version: 3.8.5\r\ntf version: 2.4.0\r\n\r\nSteps to reproduce this bug:\r\nFollowing this exactly: https://www.tensorflow.org/tutorials/text/classify_text_with_bert\r\nuntil\r\n\r\n`text_test = ['this is such an amazing movie!']`\r\n`text_preprocessed = bert_preprocess_model(text_test)`\r\n\r\nAnd I'll get exact the same error mentioned above", "@nassimus26 \r\nCould you please post this in the tfx repo, and move this to closed status.", "Same issue here! \r\n\r\nWindows 10: Version 10.0.18363\r\nPython version: 3.7.9\r\ntf version: 2.4.0\r\n\r\nAny solutions?", "I also have this issue.\r\n\r\nWindows 10: Version: 10.0.18363 \r\nPython version: 3.8.5\r\ntf version: 2.4.0\r\n\r\nIt runs fine in the Colab notebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/classify_text_with_bert.ipynb\r\nbut not in my local jupyter notebook\r\n\r\n", "This is the issue: https://github.com/tensorflow/text/issues/476", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\nOh no you don't\r\n\r\nWhat's the status @nassismus26 and @Saduf2019 ?", "\u662f\u5728windows10\u7528\u4e0d\u4e86\u561b\uff1f", "> \u662f\u5728windows10\u7528\u4e0d\u4e86\u561b\uff1f\r\n\r\nThats right, its not available on Windows10, at least not with me", "Same issue. Merely followed the tutorials step by step on Windows 10.", "@jvishnuvardhan \r\nI ran the code on tf 2.4 and nightly please fidn the [gist here](https://colab.research.google.com/gist/Saduf2019/a23098d57ee8a878822e5453308056c2/untitled517.ipynb)", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">No</a>\n", "Same problem for me. Also Windows 10.", "Sorry, by mistake this issue got closed. I will reopen it. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">No</a>\n", "Upgraded to tensorflow 2.6.0 and the bug is gone. ", "When you use tensorflow-text make sure you match the version(including minor version) with Tensorflow for example for tensorflow==2.8.x use tensorflow_text==2.8.x.\r\nBy applying the above changes you can try again.\r\nIf you still face an error let us know. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46096\">No</a>\n"]}, {"number": 46095, "title": "Bump tensorflow from 2.0.0-beta1 to 2.4.0 in /tensorflow/lite/micro/examples/magic_wand/train", "body": "Bumps [tensorflow](https://github.com/tensorflow/tensorflow) from 2.0.0-beta1 to 2.4.0.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/releases\">tensorflow's releases</a>.</em></p>\n<blockquote>\n<h2>TensorFlow 2.4.0</h2>\n<h1>Release 2.4.0</h1>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>\n<p><code>tf.distribute</code> introduces experimental support for asynchronous training of models via the <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\"><code>tf.distribute.experimental.ParameterServerStrategy</code></a> API. Please see the <a href=\"https://www.tensorflow.org/tutorials/distribute/parameter_server_training\">tutorial</a> to learn more.</p>\n</li>\n<li>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy\"><code>MultiWorkerMirroredStrategy</code></a> is now a stable API and is no longer considered experimental. Some of the major improvements involve handling peer failure and many bug fixes. Please check out the detailed tutorial on <a href=\"https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\">Multi-worker training with Keras</a>.</p>\n</li>\n<li>\n<p>Introduces experimental support for a new module named <a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/numpy\"><code>tf.experimental.numpy</code></a> which is a NumPy-compatible API for writing TF programs. See the <a href=\"https://www.tensorflow.org/guide/tf_numpy\">detailed guide</a> to learn more. Additional details below.</p>\n</li>\n<li>\n<p>Adds Support for\n<a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a> on Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for NVIDIA Ampere based GPUs and is enabled by default.</p>\n</li>\n<li>\n<p>A major refactoring of the internals of the Keras Functional API has been completed, that should improve the reliability, stability, and performance of constructing Functional models.</p>\n</li>\n<li>\n<p>Keras mixed precision API <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly\"><code>tf.keras.mixed_precision</code></a> is no longer experimental and allows the use of 16-bit floating point formats during training, improving performance by up to 3x on GPUs and 60% on TPUs. Please see below for additional details.</p>\n</li>\n<li>\n<p>TensorFlow Profiler now supports profiling <code>MultiWorkerMirroredStrategy</code> and tracing multiple workers using the <a href=\"https://www.tensorflow.org/guide/profiler#profiling_apis\">sampling mode API</a>.</p>\n</li>\n<li>\n<p>TFLite Profiler for Android is available. See the detailed <a href=\"https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android\">guide</a> to learn more.</p>\n</li>\n<li>\n<p>TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.</p>\n</li>\n</ul>\n<h2>Breaking Changes</h2>\n<ul>\n<li>\n<p>TF Core:</p>\n<ul>\n<li>Certain float32 ops run in lower precsion on Ampere based GPUs, including  matmuls and convolutions, due to the use of <a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a>. Specifically, inputs to such ops are rounded from 23 bits of precision to 10\nbits of precision. This is unlikely to cause issues in practice for deep learning models. In some cases, TensorFloat-32 is also used for complex64 ops.\nTensorFloat-32 can be disabled by running <code>tf.config.experimental.enable_tensor_float_32_execution(False)</code>.</li>\n<li>The byte layout for string tensors across the C-API has been updated to match TF Core/C++; i.e., a contiguous array of <code>tensorflow::tstring</code>/<code>TF_TString</code>s.</li>\n<li>C-API functions <code>TF_StringDecode</code>, <code>TF_StringEncode</code>, and <code>TF_StringEncodedSize</code> are no longer relevant and have been removed; see <code>core/platform/ctstring.h</code> for  string access/modification in C.</li>\n<li><code>tensorflow.python</code>, <code>tensorflow.core</code> and <code>tensorflow.compiler</code> modules are now hidden. These modules are not part of TensorFlow public API.</li>\n<li><code>tf.raw_ops.Max</code> and <code>tf.raw_ops.Min</code> no longer accept inputs of type <code>tf.complex64</code> or <code>tf.complex128</code>, because the behavior of these ops is not well defined for complex types.</li>\n<li>XLA:CPU and XLA:GPU devices are no longer registered by default. Use <code>TF_XLA_FLAGS=--tf_xla_enable_xla_devices</code> if you really need them, but this flag will eventually be removed in subsequent releases.</li>\n</ul>\n</li>\n<li>\n<p><code>tf.keras</code>:</p>\n<ul>\n<li>The <code>steps_per_execution</code> argument in <code>model.compile()</code> is no longer experimental; if you were passing <code>experimental_steps_per_execution</code>, rename it to <code>steps_per_execution</code> in your code. This argument controls the number of batches to run during each <code>tf.function</code> call when calling <code>model.fit()</code>. Running multiple batches inside a single <code>tf.function</code> call can greatly improve performance on TPUs or small models with a large Python overhead.</li>\n<li>A <strong>major refactoring</strong> of the internals of the Keras Functional API may affect code that\nis relying on certain internal details:\n<ul>\n<li>Code that uses <code>isinstance(x, tf.Tensor)</code> instead of <code>tf.is_tensor</code> when checking Keras symbolic inputs/outputs should switch to using <code>tf.is_tensor</code>.</li>\n<li>Code that is overly dependent on the exact names attached to symbolic tensors (e.g. assumes there will be &quot;:0&quot; at the end of the inputs, treats names as unique identifiers instead of using <code>tensor.ref()</code>, etc.) may break.</li>\n<li>Code that uses full path for <code>get_concrete_function</code> to trace Keras symbolic inputs directly should switch to building matching <code>tf.TensorSpec</code>s directly and tracing the <code>TensorSpec</code> objects.</li>\n<li>Code that relies on the exact number and names of the op layers that TensorFlow operations  were converted into may have changed.</li>\n<li>Code that uses <code>tf.map_fn</code>/<code>tf.cond</code>/<code>tf.while_loop</code>/control flow as op layers and  happens to work before TF 2.4. These will explicitly be unsupported now. Converting these ops to Functional API op layers was unreliable before TF 2.4, and prone to erroring incomprehensibly  or being silently buggy.</li>\n<li>Code that directly asserts on a Keras symbolic value in cases where ops like <code>tf.rank</code> used to  return a static or symbolic value depending on if the input had a fully static shape or not. Now these ops always return symbolic values.</li>\n<li>Code already susceptible to leaking tensors outside of graphs becomes slightly more likely to do so now.</li>\n<li>Code that tries directly getting gradients with respect to symbolic Keras inputs/outputs. Use <code>GradientTape</code> on the actual Tensors passed to the already-constructed model instead.</li>\n<li>Code that requires very tricky shape manipulation via converted op layers in order to work, where the Keras symbolic shape inference proves insufficient.</li>\n<li>Code that tries manually walking a <code>tf.keras.Model</code> layer by layer and assumes layers only ever have one positional argument. This assumption doesn't hold       true before TF 2.4 either, but is more likely to cause issues now.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\">tensorflow's changelog</a>.</em></p>\n<blockquote>\n<h1>Release 2.4.0</h1>\n<h2>Major Features and Improvements</h2>\n<ul>\n<li>\n<p><code>tf.distribute</code> introduces experimental support for asynchronous training of\nmodels via the [<code>tf.distribute.experimental.ParameterServerStrategy</code>]\n(<a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy\">https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy</a>)\nAPI. Please see the <a href=\"https://www.tensorflow.org/tutorials/distribute/parameter_server_training\">tutorial</a>\nto learn more.</p>\n</li>\n<li>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy\"><code>MultiWorkerMirroredStrategy</code></a>\nis now a stable API and is no longer considered experimental. Some of the\nmajor improvements involve handling peer failure and many bug fixes. Please\ncheck out the detailed tutorial on [Multi-worker training with Keras]\n(<a href=\"https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\">https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras</a>).</p>\n</li>\n<li>\n<p>Introduces experimental support for a new module named [<code>tf.experimental.numpy</code>]\n(<a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/numpy\">https://www.tensorflow.org/api_docs/python/tf/experimental/numpy</a>) which is a\nNumPy-compatible API for writing TF programs. See the [detailed guide]\n(<a href=\"https://www.tensorflow.org/guide/tf_numpy\">https://www.tensorflow.org/guide/tf_numpy</a>) to learn more. Additional details below.</p>\n</li>\n<li>\n<p>Adds Support for\n<a href=\"https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/\">TensorFloat-32</a>\non Ampere based GPUs. TensorFloat-32, or TF32 for short, is a math mode for\nNVIDIA Ampere based GPUs and is enabled by default.</p>\n</li>\n<li>\n<p>A major refactoring of the internals of the Keras Functional API has been\ncompleted, that should improve the reliability, stability, and performance of\nconstructing Functional models.</p>\n</li>\n<li>\n<p>Keras mixed precision API [<code>tf.keras.mixed_precision</code>]\n(<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly\">https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision?version=nightly</a>)\nis no longer experimental and allows the use of 16-bit floating point formats\nduring training, improving performance by up to 3x on GPUs and 60% on TPUs.\nPlease see below for additional details.</p>\n</li>\n<li>\n<p>TensorFlow Profiler now supports profiling <code>MultiWorkerMirroredStrategy</code> and\ntracing multiple workers using the [sampling mode API]\n(<a href=\"https://www.tensorflow.org/guide/profiler#profiling_apis\">https://www.tensorflow.org/guide/profiler#profiling_apis</a>).</p>\n</li>\n<li>\n<p>TFLite Profiler for Android is available. See the detailed [guide]\n(<a href=\"https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android\">https://www.tensorflow.org/lite/performance/measurement#trace_tensorflow_lite_internals_in_android</a>)\nto learn more.</p>\n</li>\n<li>\n<p>TensorFlow pip packages are now built with CUDA11 and cuDNN 8.0.2.</p>\n</li>\n</ul>\n<h2>Breaking Changes</h2>\n<ul>\n<li>TF Core:\n<ul>\n<li>Certain float32 ops run in lower precsion on Ampere based GPUs, including</li>\n</ul>\n</li>\n</ul>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/582c8d236cb079023657287c318ff26adb239002\"><code>582c8d2</code></a> Merge pull request <a href=\"https://github-redirect.dependabot.com/tensorflow/tensorflow/issues/44220\">#44220</a> from tensorflow-jenkins/relnotes-2.4.0rc0-18048</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/c16387f692bf46a95100adc91a68922414b53d4c\"><code>c16387f</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/4cf406c8a617392864efa7d8f50510a2c95e049a\"><code>4cf406c</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3f35ef2452dc0f27797e8a295371065834335944\"><code>3f35ef2</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/3647e8ec38de7891887ca72c9777ab92a2c09ad2\"><code>3647e8e</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/281c7d540508ebf92d5c8f602e52b37836c9f55c\"><code>281c7d5</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/91ec75f872634537d95485b66d8c82c2bb61a497\"><code>91ec75f</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/ed5ad82e763f0842d96762cf3ae214ea5c6eadc8\"><code>ed5ad82</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/1267bba74887f0a4ae47a8758b24b11165daf928\"><code>1267bba</code></a> Update RELEASE.md</li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/commit/13a4067a164648411cb7ef9cb579e2a4d5844260\"><code>13a4067</code></a> Update RELEASE.md</li>\n<li>Additional commits viewable in <a href=\"https://github.com/tensorflow/tensorflow/compare/v2.0.0-beta1...v2.4.0\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=tensorflow&package-manager=pip&previous-version=2.0.0-beta1&new-version=2.4.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n\n</details>", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46094, "title": "cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory", "body": "hello,\r\n\r\nI'm trying to run inference on a tensorrt converted graph (tf-trt) and received the error in the title.\r\nI've followed instructions for \"TF-TRT 2.0 Workflow With A SavedModel\" at https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html.\r\nbut i'm afraid i have not been able to reach the part of the code the performs the inference, I get the OOM error a after loading the saved model , during preparation of the input data. \r\n\r\nI'm running tensorflow 2.0.0 (eager execution). on ubuntu 16.04, cuda 10.0, geforce gtx 1050 ( 4gb ram )\r\n\r\nhere is my conversion code \r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nConvParams = trt.DEFAULT_TRT_CONVERSION_PARAMS\r\nConvParams = ConvParams._replace(max_workspace_size_bytes=1*1024*1024*1024)\r\nConvParams = ConvParams._replace(precision_mode=\"FP32\")\r\nConvParams = ConvParams._replace(max_batch_size=1)\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=args.InputToConvert, conversion_params=ConvParams)\r\n\r\nTensorRT_graph = converter.convert()\r\n\r\nprint('convert completed, building')\r\ndef my_input_fn():\r\n  import numpy as np\r\n  Inp1 = np.random.normal(size=(1,250, 250, 3)).astype(np.uint8)\r\n  yield [Inp1]\r\nconverter.build(input_fn=my_input_fn)\r\n\r\nprint('build completed, saveing results to {}'.format(args.Output))\r\nconverter.save(args.Output)\r\n\r\n\r\nthe conversion process generates warnings but succeeds. \r\n\r\n**in another program/script/\"execution session\" I perform the following:**\r\nload the saved model using the following code\r\n\r\nsaved_model_loaded = tf.saved_model.load('my saved model directory name',tags=[tf.compat.v1.saved_model.tag_constants.SERVING])\r\n    graph_func = saved_model_loaded.signatures[tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n    frozen_func = convert_variables_to_constants_v2(graph_func)\r\n    print('created concreate function')\r\n\r\n\r\ntry to load an image for inference \r\n\r\nReadFile = tf.io.read_file('/home/omerbrandis/ffrobotics/AppleMask/mask_rcnn/test_images/img3095_RECT2_2.jpg')\r\nprint('after read file')\r\nDecodedImg = tf.io.decode_jpeg(ReadFile)\r\nprint('after decode ')\r\nimage_expanded = tf.expand_dims(DecodedImg, axis=0)\r\nprint('read file')\r\n\r\n\r\n**at runtime the code does not reach \"read file\" , only \"after decode\" is written.**\r\n\r\nif i comment out the first section of the program that deals with reading the graph , the part that reads the file works without any problem.\r\n\r\nas stated before, i have 4GB of ram on my gpu , and the conversion process should have limited the graphs usage to 1gb thus the OOM error is surprising. ( the image is only 210X210 pixels).\r\n\r\nplease advise\r\nOmer.\r\n\r\n \r\n", "comments": ["@omerbrandis \r\n\r\nCan you please try in latest stable TF version 2.4 and see if you are facing the same issue?.There are lot of performance improvements in latest releases.\r\n\r\nIf you are facing issue still please share the colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46094\">No</a>\n"]}, {"number": 46093, "title": "When can we expect Tensorflow builds with Cuda 11.1 or Cuda 11.2?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow 2.4 and 2.5 is only built with cuda 11.0.  Can there please be an updated build, which is built with Cuda 11.1 or Cuda 11.2. \r\n\r\n**Will this change the current api? How?**\r\n\r\nTensorflow will point at cuda 11.1/2 files instead of cuda 11.0.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAt least everyone with RTX 3000 series graphics card, since only cuda 11.1+ officially supports compute capability 8.6.\r\n\r\n**Any Other info.**\r\n", "comments": ["CUDA 11.1 support has been merged, so I'd expect it to \"drop\" [ :) ] with the TF 2.5 release, https://github.com/tensorflow/addons/pull/2218\r\n\r\nToday CUDA 11.2 does not have a corresponding cuDNN version, so I'd think this is a blocking prerequisite.\r\n\r\nEDIT: Mistakenly linked the addons 11.1 support, but still looks like the earliest point for getting the Ampere cards fully supported is TF 2.5...", "> CUDA 11.1 support has been merged, so I'd expect it to \"drop\" [ :) ] with the TF 2.5 release, [tensorflow/addons#2218](https://github.com/tensorflow/addons/pull/2218)\r\n> \r\n> Today CUDA 11.2 does not have a corresponding cuDNN version, so I'd think this is a blocking prerequisite.\r\n> \r\n> EDIT: Mistakenly linked the addons 11.1 support, but still TF 2.5 looks like the earliest point for getting the Ampere cards fully supported...\r\n\r\nAh indeed I thought cuda 11.2 already had a compatible cudnn version. Nice to see it will come in the next version.", "I am using the TF nightly build of today 31 December, but it gives a CUDA error on the RTX 3090 card.\r\n\r\nHere are the steps taken:\r\n\r\n```\r\n$ docker run --gpus all -it --rm ${USER} -v $HOME:/home -w /home -p 8888:8888 -p 5000:5000 tensorflow/tensorflow:2.4.0-gpu bash \r\n\r\n# pip install tf-nightly-gpu==2.5.0.dev20201231\r\n\r\n# python\r\n\r\n>>> import tensorflow as tf\r\n2020-12-31 19:40:19.756487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-12-31 19:40:46.403718: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-12-31 19:40:46.440280: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n2020-12-31 19:40:46.440329: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 5651e9c008a2\r\n2020-12-31 19:40:46.440343: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 5651e9c008a2\r\n2020-12-31 19:40:46.440474: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 455.23.4\r\n2020-12-31 19:40:46.440530: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 455.23.4\r\n2020-12-31 19:40:46.440550: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 455.23.4\r\n[]\r\n```\r\n\r\nResults:\r\n\r\nNotice the error ```failed call to cuInit: CUDA_ERROR_NOT_INITIALIZED: initialization error```\r\n\r\nThe RTX 3090 is not listed as a GPU in TensorFlow.\r\n\r\nQuestion: How to get the RTX 3090 listed as a GPU in TensorFlow?", "@teusbenschop If you're ok with the master then it would help to just build from the master by yourself against CUDA 11.1 and corresponding CUDA 11.1 cudnn-8.0.5.39. AFAIK TF nightly is built against CUDA 11.0.", "Thank you @ahtik , yes I am comfortable with the master and comfortable with C / C++, so it should work out well.", "Is it possible that cuDNN 8 for 11.1 also works with 11.2? On the download page of TensorRT (https://developer.nvidia.com/nvidia-tensorrt-7x-download) is the following info:\r\n`TensorRT 7.2.2 for Windows10 and CUDA 11.1 & 11.2 ZIP package`\r\nand\r\n`TensorRT 7.2.2 for Linux and CUDA 11.1 & 11.2`\r\nThe name of the ZIP file for Windows is:\r\n`TensorRT-7.2.2.3.Windows10.x86_64.cuda-11.1.cudnn8.0.zip`\r\n\r\nEdit: If I understood https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-and-upgrades correctly, CUDA versions from 11.0 onwards are backwards compatible, i.e. DLLs for 11.1 should also work with 11.2.", "@OgreTransporter A few days ago we built from source the latest TF master with CUDA 11.2 and the latest 8.x cuDNN meant for CUDA 11.1. Build succeeded.\r\n\r\nAlthough CUDA 11.2 was working fine, as soon as cuDNN was involved, basic LSTM model trainings crashed.\r\n\r\nThe conclusion was that CUDA 11.1 cuDNN can't be used for CUDA 11.2, and we must wait for the CUDA 11.2 cuDNN before trying again.", "> @OgreTransporter A few days ago we built from source the latest TF master with CUDA 11.2 and the latest 8.x cuDNN meant for CUDA 11.1. Build succeeded.\r\n> \r\n> Although CUDA 11.2 was working fine, as soon as cuDNN was involved, basic LSTM model trainings crashed.\r\n> \r\n> The conclusion was that CUDA 11.1 cuDNN can't be used for CUDA 11.2, and we must wait for the CUDA 11.2 cuDNN before trying again.\r\n\r\nDid you also try a build with CUDA 11.1? Or are you guys just aiming for CUDA 11.2 now?", "@ion-elgreco Yes, we built and actively use the latest TF with CUDA 11.1 and corresponding cuDNN 8.x.\r\n\r\nSo far no issues other than the risk of having it built from the master. TF 2.4 and CUDA 11.1 didn't get along.", "> @ion-elgreco Yes, we built and actively use the latest TF with CUDA 11.1 and corresponding cuDNN 8.x.\r\n> \r\n> So far no issues other than the risk of having it built from the master. TF 2.4 and CUDA 11.1 didn't get along.\r\n\r\nHi guys, wondering if you guys have tried the latest?  nVidia claims cuDNN 8.1 is now compatible with CUDA 11.2.\r\n\r\ncuDNN v8.1.0 (January 26th, 2021), for CUDA 11.0,11.1 and 11.2", "Hi all,\r\n\r\nHas anyone installed TF for CUDA11.1 yet?\r\n\r\nI just tried to install tf-nightly-2.5.0.dev20210202 from pip3 but got this error. It seems that tf-nightly still look for CUDA11.0 rather than 11.1. \r\n\r\n```\r\nimport tensorflow as tf\r\n2021-02-03 01:04:35.803100: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/extras/CUPTI/lib64\r\n2021-02-03 01:04:35.803125: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n", "Hi everybody,\r\n\r\nI have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n\r\nTENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried? \r\n\r\nHope for replys! Thanks!! \r\n\r\nEDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA  version: 11.0 (update 1) and CUDNN version: 8.0", "@rTiagoS\r\nSame issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.4.1", "> Hi everybody,\r\n> \r\n> I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> \r\n> TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> \r\n> Hope for replys! Thanks!!\r\n> \r\n> EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n\r\nSame issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\nI ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n", "> > Hi everybody,\r\n> > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > Hope for replys! Thanks!!\r\n> > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> \r\n> Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n\r\nNvidia driver version?", "> > > Hi everybody,\r\n> > > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > > Hope for replys! Thanks!!\r\n> > > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> > \r\n> > \r\n> > Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> > I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n> \r\n> Nvidia driver version?\r\n\r\nstudio driver 461.40", "> > > > Hi everybody,\r\n> > > > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > > > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > > > Hope for replys! Thanks!!\r\n> > > > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> > > \r\n> > > \r\n> > > Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> > > I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n> > \r\n> > \r\n> > Nvidia driver version?\r\n> \r\n> studio driver 461.40\r\n\r\nStrange, could you try 416.09?", "> > > > > Hi everybody,\r\n> > > > > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > > > > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > > > > Hope for replys! Thanks!!\r\n> > > > > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> > > > \r\n> > > > \r\n> > > > Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> > > > I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n> > > \r\n> > > \r\n> > > Nvidia driver version?\r\n> > \r\n> > \r\n> > studio driver 461.40\r\n> \r\n> Strange, could you try 416.09?\r\n\r\nOn the nvidia website the oldest driver compatible with my graphics is 451 (rtx 3070)", "> > > > > > Hi everybody,\r\n> > > > > > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > > > > > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > > > > > Hope for replys! Thanks!!\r\n> > > > > > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> > > > > \r\n> > > > > \r\n> > > > > Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> > > > > I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n> > > > \r\n> > > > \r\n> > > > Nvidia driver version?\r\n> > > \r\n> > > \r\n> > > studio driver 461.40\r\n> > \r\n> > \r\n> > Strange, could you try 416.09?\r\n> \r\n> On the nvidia website the oldest driver compatible with my graphics is 451 (rtx 3070)\r\n\r\nSorry, typo! I meant 461.09", "@keremycdg\r\nSame specs as @rTiagoS and same issue, right now I'm downgrading to Cuda 11.0 as a temporary fix", "> > > > > > > Hi everybody,\r\n> > > > > > > I have installed the latest CUDA 11.2 version from the official documentation and also CUDNN 8.1 successfully, but\r\n> > > > > > > TENSORFLOW doesn't recognize my GPU... In order to do that, I need to built tensorflow from source? Have someone already tried?\r\n> > > > > > > Hope for replys! Thanks!!\r\n> > > > > > > EDIT: I ended up using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0\r\n> > > > > > \r\n> > > > > > \r\n> > > > > > Same issue for me too. CUDA 11.2 , cudNN 8.1 , tensorflow==2.5\r\n> > > > > > I ended up too using the required version of CUDA and CUDNN that tensorflows requires. CUDA version: 11.0 (update 1) and CUDNN version: 8.0 but with this LSTM and CONV layers crash :(\r\n> > > > > \r\n> > > > > \r\n> > > > > Nvidia driver version?\r\n> > > > \r\n> > > > \r\n> > > > studio driver 461.40\r\n> > > \r\n> > > \r\n> > > Strange, could you try 416.09?\r\n> > \r\n> > \r\n> > On the nvidia website the oldest driver compatible with my graphics is 451 (rtx 3070)\r\n> \r\n> Sorry, typo! I meant 461.09\r\n\r\nTested and doesn't work. Normal layers works well but LSTM and Convolutional layers breaks", "On my RTX 3090, I had ptax related errors when using the Tensorflow provided container (tensorflow/tensorflow:latest-gpu-jupyter -> tensorflow 2.4)\r\n\r\nThen I switched to the NVIDIA provided docker image (nvcr.io/nvidia/tensorflow:21.02-tf2-py3 also tensorflow 2.4)\r\navailable at\r\nhttps://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html#running\r\n\r\nIt's already pre-built from source with CUDA 11.2 and works like a charm! no more tensorflow installation hassle\r\n\r\nhighly recommend", "> @keremycdg\r\n> Same specs as @rTiagoS and same issue, right now I'm downgrading to Cuda 11.0 as a temporary fix\r\n\r\nCan I ask how you downgraded? I'm using Tensorflow-gpu on a Linux Tesla server at my university which comes with NVIDIA Driver Version: 460.27.04    CUDA Version: 11.2. I'm guessing I'm going to have to downgrade to CUDA 11.0 locally - any suggestions on how to do this? I'm more familiar with using Tensorflow on Windows so would appreciate any advice", "Nightly should now build with CUDA 11.2\r\n\r\n2.5 branch cut is today so next week there will be an RC0 that you can use and test.", "> Nightly should now build with CUDA 11.2\r\n> \r\n> 2.5 branch cut is today so next week there will be an RC0 that you can use and test.\r\n\r\nHow do I use the Nightly build? Can I use it without Bazel?", "I tried uninstalling tensorflow and tensorflow gpu and reinstalled both tf-nightly and tf-nightly gpu, same errors: \r\n\r\npciBusID: 0000:87:00.0 name: Tesla K80 computeCapability: 3.7\r\ncoreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\r\n2021-03-25 21:29:31.829916: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.831735: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.834609: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.840163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-25 21:29:31.843093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-25 21:29:31.848239: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.850681: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.853139: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n2021-03-25 21:29:31.853174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2021-03-25 21:29:31.853217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-25 21:29:31.853238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1\r\n2021-03-25 21:29:31.853253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N\r\n2021-03-25 21:29:31.853266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N\r\n\r\nAny advice would be greatly appreciated this is driving me crazy!", "I highly recommend using the tensorflow docker image built by NVIDIA ! \r\nsee my [comment upthread](https://github.com/tensorflow/tensorflow/issues/46093#issuecomment-787044411)", "> I highly recommend using the tensorflow docker image built by NVIDIA !\r\n> see my [comment upthread](https://github.com/tensorflow/tensorflow/issues/46093#issuecomment-787044411)\r\n\r\nhi there, \r\n\r\nOk, I'll give it a try!\r\n\r\nMany thanks", "> On my RTX 3090, I had ptax related errors when using the Tensorflow provided container (tensorflow/tensorflow:latest-gpu-jupyter -> tensorflow 2.4)\r\n> \r\n> Then I switched to the NVIDIA provided docker image (nvcr.io/nvidia/tensorflow:21.02-tf2-py3 also tensorflow 2.4)\r\n> available at\r\n> https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html#running\r\n> \r\n> It's already pre-built from source with CUDA 11.2 and works like a charm! no more tensorflow installation hassle\r\n> \r\n> highly recommend\r\n\r\nCan I ask if this was on your own computer? I just tried on a server, pulling the docker image in with Singularity, and the NPC image doesn't support the NVIDIA Tesla K80 GPU, any other suggestions?\r\n ", "Sorry about that. It does [look like](https://forums.developer.nvidia.com/t/can-we-use-ngc-docker-containers-of-deep-learning-libraries-on-tesla-k80/62926) the older architecture of the NVIDIA K80 isn't supported by NGC containers. To answer your question, II was indeed on my own computer, and I don't have experience with the K80 or Singularity, best of luck on your use case and hopefully the tensorflow RC0 mentioned upthread is just around the corner", "> On my RTX 3090, I had ptax related errors when using the Tensorflow provided container (tensorflow/tensorflow:latest-gpu-jupyter -> tensorflow 2.4)\r\n> \r\n> Then I switched to the NVIDIA provided docker image (nvcr.io/nvidia/tensorflow:21.02-tf2-py3 also tensorflow 2.4)\r\n> available at\r\n> https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/running.html#running\r\n> \r\n> It's already pre-built from source with CUDA 11.2 and works like a charm! no more tensorflow installation hassle\r\n> \r\n> highly recommend\r\n\r\nHi, can I ask which command exactly did you use here?\r\n_If you have Docker 19.03 or later, a typical command to launch the container is:\r\ndocker run --gpus all -it --rm -v local_dir:container_dir nvcr.io/nvidia/tensorflow:<xx.xx>-tf<x>-py<x>_\r\n\r\nIt gives me the error:\r\n_bash: syntax error near unexpected token `newline'_\r\n\r\nSo, I have tried\r\n_docker run --gpus all -it --rm -v local_dir:container_dir nvcr.io/nvidia/tensorflow:xx.xx-tfx-pyx_\r\nAnd also (connect: permission denied)\r\n_sudo docker run --gpus all -it --rm -v local_dir:container_dir nvcr.io/nvidia/tensorflow:xx.xx-tfx-pyx_\r\n\r\nBut now it gives me the error:\r\n_Unable to find image 'nvcr.io/nvidia/tensorflow:xx.xx-tfx-pyx' locally\r\ndocker: Error response from daemon: manifest for nvcr.io/nvidia/tensorflow:xx.xx-tfx-pyx not found: manifest unknown: manifest unknown._\r\nI guess I have to rewrite the image part but I am not sure which one I should choose.\r\nThank you for your answer!", "@povolann you can use the image name I put in my original message that you quoted. For example, try this command:\r\n`docker run --gpus all -it --rm nvcr.io/nvidia/tensorflow:21.02-tf2-py3`\r\n\r\nNext step: make sure you check the [docker documentation](https://docs.docker.com/storage/volumes/) to understand the `-v` flag, you'll see you just have to specify `local_dir` and `container_dir` paths that make sense on your system.", "@Sicily-F Latest NGC dockers stopped supporting K80. Try [Tensorflow docker](https://www.tensorflow.org/install/docker)\r\n\r\n", "> @Sicily-F Latest NGC dockers stopped supporting K80. Try [Tensorflow docker](https://www.tensorflow.org/install/docker)\r\n\r\nHi, thanks for this! I've found a workaround with a custom-built nvidia image with the required libraries, but I'll try the Tensorflow docker as it's maybe more stable", "I've been following this issue for a while and was finally was able to use the pre-release `tensorflow-gpu==2.5.0rc0` without any problems on my 3070! I thought I'd share my configuration in the event someone wants to try this method too (unfortunately, I never was able to get the methods above to work). I'll also note that the tf 2.4, cuDNN 8.0, and CUDA 11.0 build configuration didn't work for me either.\r\n\r\nI did the following with miniconda:\r\n- I installed the latest release candidate via `pip install tensorflow-gpu==2.5.0rc0`\r\n- I installed CUDA 11.1 with `conda install cudatoolkit=11.1 -c conda-forge`\r\n- I downloaded the missing [cuDNN 8.1.0](https://developer.nvidia.com/rdp/cudnn-archive#a-collapse810-102) and placed the .dll files into my environment's Library>bin folder as described [here](https://medium.com/analytics-vidhya/install-tensorflow-gpu-2-4-0-with-cuda-11-0-and-cudnn-8-using-anaconda-8c6472c9653f)", "CUDA 11.3 is released!\r\n\r\n- Blog: [Exploring the New Features of CUDA 11.3](https://developer.nvidia.com/blog/exploring-the-new-features-of-cuda-11-3/)\r\n- Blog: [Programming Efficiently with the NVIDIA CUDA 11.3 Compiler Toolchain](https://developer.nvidia.com/blog/programming-efficiently-with-the-cuda-11-3-compiler-toolchain/)\r\n- [Download CUDA 11.3](https://developer.nvidia.com/cuda-downloads)\r\n- [CUDA Samples repo](https://github.com/NVIDIA/cuda-samples)", "> Nightly should now build with CUDA 11.2\r\n> \r\n> 2.5 branch cut is today so next week there will be an RC0 that you can use and test.\r\n\r\nI still have issues with Cuda 11.2 and RC1:\r\n\r\n```\r\n2021-04-23 19:13:13.420425: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-04-23 19:13:13.420452: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nNum GPUs Available: 0\r\n```\r\n\r\nThe same container has 0 issues with PyTorch. It's the official NVIDIA cudnn8 container.\r\n", "+1 . Hope tensorflow2.5.0 can be built with cuda>=11.1", "It should.\r\n\r\nYou can test the RCs, they should work with CUDA 11.2", "Can we close this?", "> Can we close this?\r\n\r\nYes , you can close it since the outdated docs will be updated . [https://github.com/tensorflow/tensorflow/issues/49295](https://github.com/tensorflow/tensorflow/issues/49295) ", "The docs are now updated with commit [69919bd](https://github.com/tensorflow/docs/commit/69919bd2c59a4902534f52f8de09eeead96e2ed6) Thanks!"]}, {"number": 46092, "title": "Wrong output dimension calculation of StrideSlice operation on TensorFlow Lite.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Both pip and source build\r\n- TensorFlow version (use command below): `v2.4.0-rc4-71-g582c8d236cb 2.4.0`\r\n- Python version: `3.6.8`\r\n- Bazel version (if compiling from source): `3.7.1`\r\n- GCC/Compiler version (if compiling from source): `8.3.1`\r\n- CUDA/cuDNN version: CUDA 11.1 / cuDNN 8\r\n- GPU model and memory: RTX3090 24GB\r\n\r\n**Describe the current behavior**\r\nThe `StridedSlice` operation on TensorFlow Lite calculates output dimension incorrectly.\r\n\r\nI'm developing some custom operations for both TensorFlow and TensorFlow Lite.\r\nWhile I debugging my custom operation, inference fails only on TensorFlow Lite.\r\nI found that `StridedSlice` operation of TensorFlow Lite calculates output dimension incorrectly.\r\nI added some `printf` function to [`tflite::ops::builtin::strided_slice::Eval`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L185):\r\n```c++\r\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n  StridedSliceContext op_context(context, node);\r\n\r\n  printf(\"**** [STRIDED_SLICE] Input [\");\r\n  for (int i = 0; i < NumDimensions(op_context.input); i++) {\r\n    printf(\"%d, \", SizeOfDimension(op_context.input, i));\r\n  }\r\n  printf(\"]\\n**** [STRIDED_SLICE] Begin: \");\r\n  for (int i = 0; i < SizeOfDimension(op_context.begin, 0); i++) {\r\n    printf(\"%d, \", GetTensorData<int32_t>(op_context.begin)[i]);\r\n  }\r\n  printf(\"\\n**** [STRIDED_SLICE] End: \");\r\n  for (int i = 0; i < SizeOfDimension(op_context.end, 0); i++) {\r\n    printf(\"%d, \", GetTensorData<int32_t>(op_context.end)[i]);\r\n  }\r\n  printf(\"\\n**** [STRIDED_SLICE] Strides: \");\r\n  for (int i = 0; i < SizeOfDimension(op_context.strides, 0); i++) {\r\n    printf(\"%d, \", GetTensorData<int32_t>(op_context.strides)[i]);\r\n  }\r\n  printf(\"\\n\");\r\n\r\n  if (IsDynamicTensor(op_context.output)) {\r\n    TF_LITE_ENSURE_OK(context, ResizeOutputTensor(context, &op_context));\r\n  }\r\n  StridedSliceParams op_params = BuildStridedSliceParams(&op_context);\r\n\r\n  printf(\"**** [STRIDED_SLICE] Output [\");\r\n  for (int i = 0; i < NumDimensions(op_context.output); i++) {\r\n    printf(\"%d, \", SizeOfDimension(op_context.output, i));\r\n  }\r\n  printf(\"]\\n\");\r\n```\r\n\r\nAnd I got following logs:\r\n```\r\n**** [STRIDED_SLICE] Input [1656, 8, 32, ]\r\n**** [STRIDED_SLICE] Begin: 0, 0, 0, \r\n**** [STRIDED_SLICE] End: 893, \r\n**** [STRIDED_SLICE] Strides: 1, 1, 1, \r\n**** [STRIDED_SLICE] Output [893, 8, 0, ]\r\n```\r\n\r\n**Describe the expected behavior**\r\nAfter strided slicing, output dimension should be `[893, 8, 32, ]` in above example.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nI wrote a smallest [reproducible code](https://github.com/kukdh1/tflite_buf_report/blob/master/test_strided_slice.py).\r\nThe `SimpleLayer` keras layer creates read-only `Tensor` with size of [128, 8, 32].\r\nAfter ten `model.call`s, I saved the model into tflite model file.\r\n\r\nOn TensorFlow (python, no code modification -- installed by pip `tensorflow-2.4.0-cp36-cp36m-manylinux2010_x86_64.whl`) it calculates output dimension correctly.\r\n```\r\nTEST: Input: [128, 8, 32] SiliceTo: 53 Output: (53, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 38 Output: (38, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 64 Output: (64, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 100 Output: (100, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 106 Output: (106, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 90 Output: (90, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 126 Output: (126, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 122 Output: (122, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 62 Output: (62, 8, 32)\r\nTEST: Input: [128, 8, 32] SiliceTo: 99 Output: (99, 8, 32)\r\n```\r\n\r\nI checked the stored model with [netron](https://netron.app/):\r\n![image](https://user-images.githubusercontent.com/6904750/103407030-52be8400-4ba0-11eb-8344-2e2d1b85e0d2.png)\r\nAnd it shows that `StridedSlice` has input `Tensor` of size `[128, 8, 32]` and output `Tensor` has size `[Unknown, 8, 32]`.\r\n\r\nTo test on TensorFlow Lite, I wrote [simple inference code](https://github.com/kukdh1/tflite_buf_report/blob/master/strided_slice.cc) with TensorFlow Lite C++ API.\r\nIt randomly selects input `Tensor` (slice index) and print the output dimension.\r\n\r\nI used the source code (582c8d23 == v2.4.0) with above `StrideSlice` `printf` modification.\r\nI built `libtensorflowlite.so` with following command:\r\n`bazel build --verbose_failures -c opt --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_nccl_support=true --define=build_with_mkl=false --config=monolithic //tensorflow/lite:libtensorflowlite.so`\r\nAnd the content of `.tf_configure.bazelrc` is:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/kukdh1/.virtualenvs/tf_develop/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/kukdh1/.virtualenvs/tf_develop/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/home/kukdh1/.virtualenvs/tf_develop/bin/python3\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda-11.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"7.5,8.6\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda-11.1/lib64:\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_env=LD_LIBRARY_PATH\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nI built test program with `mkdir build && cmake -DTF_SOURCE_DIR=<source directory> ..` ([CMakeLists.txt](https://github.com/kukdh1/tflite_buf_report/blob/master/CMakeLists.txt))\r\nWhen I ran the test program, I got following result:\r\n```\r\n$ CUDA_VISIBLE_DEVICES=-1 ./tflite-strided-slice strided_slice.tflite \r\n2020-12-31 19:47:18.088431: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n**** [STRIDED_SLICE] Input [128, 8, 32, ]\r\n**** [STRIDED_SLICE] Begin: 0, 0, 0, \r\n**** [STRIDED_SLICE] End: 103, \r\n**** [STRIDED_SLICE] Strides: 1, 1, 1, \r\n**** [STRIDED_SLICE] Output [103, 0, 32, ]\r\nInput: 103\r\nOutput: [103 0 32 ]\r\n########## PRINT INTERPRETER STATE BEGIN ##########\r\nInterpreter has 6 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 5\r\n\r\nTensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB) \r\nTensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\nTensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  103 0 32\r\n\r\nNode   0 Operator Builtin Code  83 PACK\r\n  Inputs: 0\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 1 2 4 3\r\n  Outputs: 5\r\n########### PRINT INTERPRETER STATE END ###########\r\n```\r\n\r\nThe output Tensor has size of `[103, 0, 32]` not `[103, 8, 32]`.\r\n\r\nI think this is a bug on [`tflite::ops::builtin::strided_slice::ResizeOutputTensor`](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/lite/kernels/strided_slice.cc#L101).\r\nPlease let me know if I did something wrong.\r\n\r\n**Other info / logs**\r\nYou can find all the code snippets I used to reproduce the problem at [here](https://github.com/kukdh1/tflite_buf_report).\r\n(Download all dependencies by running `./download_dependencies.sh` at `tensorflow/lite/tools/make`)\r\nYou can find tflite model too.\r\n\r\nP.S. The dimension error is quite random. Sometimes it calculates correctly. Sometimes innermost (axis=2) dimension becomes zero. Sometimes middle (axis=1) dimension becoms zero, Sometimes both (axis=1, axis=2) dimensions become zero.\r\n\r\nP.S. 2. I double checked with unmodified tensorflow source downloaded with:\r\n`wget https://github.com/tensorflow/tensorflow/archive/v2.4.0.tar.gz`\r\non Ubuntu 18.04.5 (different machine) with GCC 7.5.0, bazel 3.7.2 and Python 3.6.9.\r\nSame command used to build `libtensorflowlite.so` and `.tf_configure.bazelrc` is:\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/kukdh1/.virtualenvs/tensorflow/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/kukdh1/.virtualenvs/tensorflow/lib/python3.6/site-packages\"\r\nbuild --python_path=\"/home/kukdh1/.virtualenvs/tensorflow/bin/python3\"\r\nbuild --config=xla\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\nThe output on Ubuntu machine was (one example):\r\n```\r\nInput: 41\r\nOutput: [41 0 0 ]\r\n########## PRINT INTERPRETER STATE BEGIN ##########\r\nInterpreter has 6 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 5\r\n\r\nTensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB)\r\nTensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\nTensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic          0 bytes ( 0.0 MB)  41 0 0\r\n\r\nNode   0 Operator Builtin Code  83 PACK\r\n  Inputs: 0\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 1 2 4 3\r\n  Outputs: 5\r\n########### PRINT INTERPRETER STATE END ###########\r\n```\r\n", "comments": ["Hi,\r\n\r\nI found the function `BuildStridedSliceParams` in `tensorflow/lite/kernels/strided_slice.cc` was the problem.\r\nThe following loop assumes the dimension of `begin`, `end` and `strides` are same, but in my case, dimension of `begin` and `strides` was `3` while dimension of `end` was `1`.\r\n```cpp\r\n  int begin_count = GetTensorShape(op_context->begin).Dims(0);\r\n  for (int i = 0; i < begin_count; ++i) {\r\n    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];\r\n    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];\r\n    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];\r\n  }\r\n```\r\n\r\nSo I changed the code to:\r\n```diff\r\ndiff --git a/tensorflow/lite/kernels/strided_slice.cc b/tensorflow/lite/kernels/strided_slice.cc\r\nindex d10e99c1..26f3ab5c 100644\r\n--- a/tensorflow/lite/kernels/strided_slice.cc\r\n+++ b/tensorflow/lite/kernels/strided_slice.cc\r\n@@ -78,19 +78,29 @@ StridedSliceParams BuildStridedSliceParams(StridedSliceContext* op_context) {\r\n   op_params.shrink_axis_mask = op_context->params->shrink_axis_mask;\r\n \r\n   int begin_count = GetTensorShape(op_context->begin).Dims(0);\r\n-  for (int i = 0; i < begin_count; ++i) {\r\n-    op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];\r\n-    op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];\r\n-    op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];\r\n-  }\r\n+  int end_count = GetTensorShape(op_context->end).Dims(0);\r\n+  int strides_count = GetTensorShape(op_context->strides).Dims(0);\r\n+\r\n+  for (int i = 0; i < op_context->dims; ++i) {\r\n+    if (i < begin_count) {\r\n+      op_params.start_indices[i] = GetTensorData<int32_t>(op_context->begin)[i];\r\n+    } else {\r\n+      op_params.start_indices[i] = 0;\r\n+      op_params.begin_mask |= (1 << i);\r\n+    }\r\n \r\n-  // If the length of begin and end smaller than number of input dims, set the\r\n-  // mask bit of begin and end for that index.\r\n-  for (int i = begin_count; i < op_context->dims; ++i) {\r\n-    op_params.start_indices[i] = op_params.stop_indices[i] = 0;\r\n-    op_params.strides[i] = 1;\r\n-    op_params.begin_mask |= (1 << i);\r\n-    op_params.end_mask |= (1 << i);\r\n+    if (i < end_count) {\r\n+      op_params.stop_indices[i] = GetTensorData<int32_t>(op_context->end)[i];\r\n+    } else {\r\n+      op_params.stop_indices[i] = 0;\r\n+      op_params.end_mask |= (1 << i);\r\n+    }\r\n+\r\n+    if (i < strides_count) {\r\n+      op_params.strides[i] = GetTensorData<int32_t>(op_context->strides)[i];\r\n+    } else {\r\n+      op_params.strides[i] = 1;\r\n+    }\r\n   }\r\n   return op_params;\r\n }\r\n```\r\n\r\nAnd it solves the problem.\r\n(In below interpreter state, `Tensor 2 == begin` and `Tensor 3 == strides` has dimension of `3`, but `Tensor 4 == end` has dimension of `1`).\r\n\r\n```\r\nInput: 41\r\nOutput: [41 8 32 ]\r\n########## PRINT INTERPRETER STATE BEGIN ##########\r\nInterpreter has 6 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 5\r\n\r\nTensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB) \r\nTensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\nTensor   2 simple_layer/strided_slice kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   3 simple_layer/strided_slice1 kTfLiteInt32   kTfLiteMmapRo         12 bytes ( 0.0 MB)  3\r\nTensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic      41984 bytes ( 0.0 MB)  41 8 32\r\n\r\nNode   0 Operator Builtin Code  83 PACK\r\n  Inputs: 0\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 1 2 4 3\r\n  Outputs: 5\r\n########### PRINT INTERPRETER STATE END ###########\r\n```\r\n\r\nThis patch solves the problem, but I'm not sure that fixing `BuildStridedSliceParams` function is correct solution.\r\nMaybe, in conversion process, all three (`begin`, `end` and `strides`) tensors should not expanded or should be expanded.\r\n(I used MLIR for the conversion)\r\n\r\nThanks.", "Hi @kukdh1, conversion as well as output seem to be correct in tf-nightly. Can you try it on nightly version instead?\r\n\r\nhttps://colab.research.google.com/drive/1h8MnLZdmRePyHecXzJRRQfAw9-MRwPEA?usp=sharing\r\n![image](https://user-images.githubusercontent.com/11615393/103586386-d5458d00-4e99-11eb-922a-2261aa38005b.png)\r\n", "Hi,\r\n\r\nI just installed `tf_nightly-2.5.0.dev20210104-cp36-cp36m-manylinux2010_x86_64.whl` and the converted result has correct input dimensions.\r\n![image](https://user-images.githubusercontent.com/6904750/103616503-e95bbe00-4f6f-11eb-96f9-1a8601c7caad.png)\r\n\r\nAnd I compiled `libtensorflowlite.so` with master branch bd68b49a and it calculates output dimension as well.\r\n```\r\nInput: 103\r\nOutput: [103 8 32 ]\r\n########## PRINT INTERPRETER STATE BEGIN ##########\r\nInterpreter has 6 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 5\r\n\r\nTensor   0 input_1              kTfLiteInt32   kTfLiteCustom          4 bytes ( 0.0 MB)\r\nTensor   1 simple_layer/ReadVariableOp/resource kTfLiteFloat32   kTfLiteMmapRo     131072 bytes ( 0.1 MB)  128 8 32\r\nTensor   2 simple_layer/strided_slice/stack kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor   3 simple_layer/strided_slice/stack_2 kTfLiteInt32   kTfLiteMmapRo          4 bytes ( 0.0 MB)  1\r\nTensor   4 simple_layer/strided_slice/stack_1 kTfLiteInt32  kTfLiteArenaRw          4 bytes ( 0.0 MB)  1\r\nTensor   5 Identity             kTfLiteFloat32  kTfLiteDynamic     105472 bytes ( 0.1 MB)  103 8 32\r\n\r\nNode   0 Operator Builtin Code  83 PACK\r\n  Inputs: 0\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code  45 STRIDED_SLICE\r\n  Inputs: 1 2 4 3\r\n  Outputs: 5\r\n########### PRINT INTERPRETER STATE END ###########\r\n```\r\n\r\nI hope this will be fixed in v2.4.1.\r\n\r\nThanks.", "@kukdh1 \r\nPlease move the issue to closed status if resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46092\">No</a>\n"]}, {"number": 46091, "title": "Missing functions from TF 2.4 vs TF 2.3 (tf.keras.backend)", "body": "**System information**\r\n- Used code from another repository : https://github.com/LeonLok/Deep-SORT-YOLOv4\r\n- Windows 10 Pro\r\n- pip install via anaconda\r\n- TensorFlow v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python 3.6\r\n- CUDA 11.0 cuDNN 8.04\r\n- GeForce RTX 3060 Ti 8192 MB\r\n\r\nI am trying to run the repository stated above on my 3060Ti and made some modifications to run it on tf 2.4. I noticed that alot of functions from tf.keras.backend for tf 2.4 (https://www.tensorflow.org/api_docs/python/tf/keras/backend) are missing compared to tf 2.3 (https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/backend) in the API docs . I tried to look for alternatives and managed to replace some of them. However, I am not able to find some of the functions mentioned below. Are they obsolete or are they any other alternatives.\r\n\r\nK.arange\r\nK.concatenate \r\nK.dtype\r\nK.min\r\nK,gather \r\nK.sum\r\nK.switch \r\nK.max\r\nK.control_flow_ops.while_loop\r\nK.learning_phase\r\n\r\n**Code**\r\nhttps://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo.py\r\nhttps://github.com/LeonLok/Deep-SORT-YOLOv4/blob/master/tensorflow2.0/deep-sort-yolov4/yolo4/model.py\r\n", "comments": ["@nch1997 Am I missing anything here. I am able to access `K.arange` etc without any problem. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/e4120f8c4f3425fec2b67ab88a07dba1/untitled70.ipynb). Thanks!\r\n\r\nPlease let us know know if you have any issues. Try with a simple code  and share it if you have any issues accessing different functions as you listed above. Thanks!", "Hi, thanks for the help. Not sure why it did not work the last time I tried it but now it works. However, I now encounter a 'failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED'. I attempted  to use the suggestions from here but it does not help https://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed. I attached the logs below if it is useful\r\n\r\n```\r\n2021-01-05 20:03:45.167619: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:46.474887: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-05 20:03:46.477817: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-01-05 20:03:46.499646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:46.499744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:46.504933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:46.505005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:46.507991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:46.509010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:46.515565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:46.517687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:46.518316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:46.518399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:46.928907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-05 20:03:46.929015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-05 20:03:46.929580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-05 20:03:46.929933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-01-05 20:03:46.930555: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:46.937739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:46.937822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:46.938102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:46.938254: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:46.938494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:46.938687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:46.938777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:46.938964: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:46.939002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:46.939233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:46.939419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-05 20:03:46.939589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-05 20:03:46.939767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-05 20:03:46.940003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-01-05 20:03:46.940141: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:46.954529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:46.954614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:46.954684: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:46.955013: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:46.955251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:46.955465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:46.955665: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:46.955865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:46.956071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:46.956329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:46.956499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-05 20:03:46.956674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-05 20:03:46.956830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-05 20:03:46.957081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-01-05 20:03:46.957213: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:46.957565: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:46.957635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:46.957820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:46.957849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:46.958045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:46.958260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:46.958469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:46.958678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:46.958890: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:46.959135: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:46.959325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:47.227676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:47.227750: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:47.228202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:47.228228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:47.228615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:47.228827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:47.228868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:47.228894: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:47.229102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:47.229162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:47.229373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-05 20:03:47.229792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-05 20:03:47.230027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-05 20:03:47.230297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-01-05 20:03:47.230434: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:51.665067: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2021-01-05 20:03:55.109817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6\r\ncoreClock: 1.665GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-01-05 20:03:55.109906: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-01-05 20:03:55.110370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:55.110397: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-01-05 20:03:55.110738: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-01-05 20:03:55.110987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-01-05 20:03:55.111223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-01-05 20:03:55.111519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-01-05 20:03:55.111556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:55.111796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-01-05 20:03:55.112025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-01-05 20:03:55.112233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-01-05 20:03:55.112255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-01-05 20:03:55.112519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6617 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\n2021-01-05 20:03:55.112876: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 20:03:57.044600: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-01-05 20:03:58.065345: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-01-05 20:03:58.099300: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-01-05 20:03:58.142696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-01-05 20:03:58.788922: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.817620: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.864186: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.895198: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.944997: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.955205: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:58.976647: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.008404: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.026395: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.054411: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.100066: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.124254: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.151229: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.166517: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.189911: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.229805: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.254890: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.271920: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.285410: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.306614: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.333288: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.373000: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.391758: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.406642: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.440534: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.469361: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.523541: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.556508: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.568339: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.599717: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.611995: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.696813: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.710269: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.756313: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.777329: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.797895: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:03:59.815704: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.466321: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.482469: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.505733: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.521207: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.526484: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.534348: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.548182: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.554963: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.562900: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.579876: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.581185: E tensorflow/stream_executor/cuda/cuda_blas.cc:226] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2021-01-05 20:04:00.581238: W tensorflow/stream_executor/stream.cc:1455] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n```", "@nch1997 Can you please close this issue and open another issue (with the above content) as the new issue is related to GPU. It will be better for the other users to follow and learn something from your issues. Thanks again.", "No Problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46091\">No</a>\n"]}, {"number": 46090, "title": "call() missing 1 required positional argument", "body": "Tensorflow Version 2.4 pip wheel\r\nPython Version: 3.6\r\nOS: official docker\r\nAs the title, when I try to export a multi-input model, it failed with an error message \"call() missing 1 required positional argument: 'y'\".Thanks for your help.\r\nHere comes the code:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nclass CustomModel(keras.Model):\r\n    def __init__(self, hidden_units):\r\n        super(CustomModel, self).__init__()\r\n        self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]\r\n\r\n    def call(self, x, y):\r\n        x = tf.concat((x, y), axis=1)\r\n        for layer in self.dense_layers:\r\n            x = layer(x)\r\n        return {'x': x}\r\n\r\n\r\nmodel = CustomModel([16, 16, 10])\r\n# Build the model by calling it\r\ninput_arr = tf.random.uniform((1, 5))\r\ninput_arr_2 = tf.random.uniform((1, 10))\r\noutputs = model(input_arr, input_arr_2)\r\nprint(outputs)\r\nmodel.save(\"my_model\")\r\n```", "comments": ["@lizhen2017 \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, let us know which TF version you are using?.\r\nWill it be possible to share complete error log?\r\n\r\nThanks!", "> @lizhen2017\r\n> \r\n> Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> Please, let us know which TF version you are using?.\r\n> Will it be possible to share complete error log?\r\n> \r\n> Thanks!\r\n\r\nHi\uff0cI've update the tf version. Thanks for you help.", "I have tried in colab with TF version 2.3, 2.4, nightly version(`2.5.0-dev20210113`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cf638999d664bbaad2985e9be4c5fb15/untitled616.ipynb). Thanks!", "When I remove `y` from the inputs of the call function, the model saving does not throw an error; see [gist](https://colab.research.google.com/gist/bguetarni/475044bfaea272db92f6fb3ee54cf7cb/untitled616.ipynb).\r\nI suspect Tensorflow to run the model in order for him to recover the graph.\r\nHowever the `tf.keras.Model.call()` function takes only 1 argument.\r\nDuring the call they probably use this function with only 1 argument (as expected by the function prototype) and this is why it throws this error.\r\n\r\nIf you want to concatenate these arrays, do it before running the model so you just pass one array.", "@lizhen2017 Sorry for the late response. I changed couple of lines and it works as expected. [Here](https://colab.research.google.com/gist/jvishnuvardhan/3759195bb062465c63291175a5be1444/untitled616.ipynb) is a gist for reference. Currently model `call` expects single argument `inputs` other than self, traininig=`True` or `False`.\r\n\r\nThe following lines were added or changed\r\n\r\n```\r\n1.         self.hidden_units = hidden_units\r\n2.        self.dense_layers = [keras.layers.Dense(u) for u in self.hidden_units]\r\n\r\n3.   def call(self, inputs): # x, y):\r\n\r\n\r\n4. outputs = model([input_arr, input_arr_2])\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46090\">No</a>\n", "# Fine tune all layers\r\n# Passing layers=\"all\" trains all layers. You can also \r\n# pass a regular expression to select which layers to\r\n# train by name pattern.\r\nmodel.train(dataset_train, dataset_val, \r\n            learning_rate=config.LEARNING_RATE / 10,\r\n            epochs=10, \r\n            layers=\"all\")\r\n\r\nHere shows,\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-36-f6fd060a95c0> in <module>()\r\n      5 model.train(dataset_train, dataset_val, \r\n      6             learning_rate=config.LEARNING_RATE / 10,\r\n----> 7             epochs=10 )\r\n\r\nTypeError: train() missing 1 required positional argument: 'layers'\r\n\r\nWhat should I do?"]}, {"number": 46088, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7.0\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: GeForce GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhenever I build or use tensorflow it gives me an Import error, I tried searching the web for solutions but I can't find any, I even tried reinstalling tensorflow just to be greeted  with the same error.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\nhttps://user-images.githubusercontent.com/49057329/103400118-ec753980-4b7e-11eb-81f1-29a2e2ab7da2.mp4\r\n\r\n\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\LEE_FAMILY\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n```\r\n\r\n", "comments": ["@gravitygo \r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download [the latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46088\">No</a>\n"]}, {"number": 46087, "title": "Newline after printing ConfigError in configuration", "body": "Fix error message printed on configuration exception to have a newline suffix\r\nin CUDA configuration, without this the two lines run into each other.\r\n\r\nFor some reason I didn't investigate further `print(e, file=sys.stderr)` didn't\r\ndisplay output at all in the configuration process (print monkeypatched?),\r\nso I just tacked a newline on. Mostly just didn't want to leave the way it was\r\nafter I had seen it happen on the console. :-)\r\n\r\nBefore:\r\n\r\n```\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]:\r\n\r\nInconsistent CUDA toolkit path: /usr vs /usr/libAsking for detailed CUDA configuration...\r\n```\r\n\r\nAfter:\r\n\r\n```\r\nPlease specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]:\r\n\r\nInconsistent CUDA toolkit path: /usr vs /usr/lib\r\nAsking for detailed CUDA configuration...\r\n```", "comments": ["Note: also observed in some outputs for filed issues like https://github.com/tensorflow/tensorflow/issues/40202#issuecomment-662805003\r\n\r\nLMK if there are corresponding configure tests to run/update/modify, I didn't see any at a glance but didn't look thoroughly, happy to supplement this PR with test updates if I know where to look!", "Closing as PR was merged."]}, {"number": 46086, "title": "dlerror: cusparse64_11.dll not found. Cannot detetct GPU.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64bit\r\n- TensorFlow version: tensorflow-gpu=2.4.0\r\n- Python version: 3.8.5\r\n- Installed using pip \r\n- CUDA/cuDNN version: CUDA 11.1 | cuDNN v8.0.4 for CUDA 11.1\r\n- GPU model and memory: NVIDIA GeForce GTX 1050\r\n\r\n**I try to use NVIDIA GeForce GTX 1050 by installing all the ablove mnetioned specs. Cannot find the cusparse64_11.dll. The GPU is not being detected for running deep learning models. \r\n\r\n\r\nCould not load dynamic library 'cusparse64_11.dll'; **dlerror: cusparse64_11.dll not found**\r\n\r\nCannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices... **\r\n\r\n![image](https://user-images.githubusercontent.com/48597846/103396793-2c2a2a00-4b5b-11eb-9499-8b66759fba37.png)\r\n\r\n", "comments": ["While running the following commands, i am unable to detect my GPU.\r\n`import tensorflow as tf\r\nphysical_devices = tf.config.experimental.list_physical_devices('GPU')\r\nprint(\"Num GPUs Available: \", len(physical_devices))\r\ntf.config.experimental.set_memory_growth(physical_devices[0],True)`\r\n\r\nError\r\n`Num GPUs Available:  0\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-2-1fc9c14b9ae0> in <module>\r\n      1 physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n      2 print(\"Num GPUs Available: \", len(physical_devices))\r\n----> 3 tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\nIndexError: list index out of range`\r\n\r\n> Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n\r\n> Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...", "@ShrinidhiSS \r\n\r\nPlease, see tested build configuration from [here](https://www.tensorflow.org/install/source_windows#gpu).Please, create a fresh environment and check with this configuration and let us know if the issue still persists.\r\nTry adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\nRefer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\nThanks!", "But I am not able to download CUDA 11.0 from https://developer.nvidia.com/cuda-11.0-update1-download-archive\r\nThere is some problem downloading.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46086\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46086\">No</a>\n"]}, {"number": 46085, "title": "Cherry-pick: Lazy load estimator instead of using virtual pip package", "body": "\u2026//github.com/tensorflow/community/pull/182\r\n\r\nPiperOrigin-RevId: 294847967\r\nChange-Id: I327d075a2065e2ccf8ad5317882ebde14e3dc3d6\r\n\r\nHello, I try to use Tensorflow v1.15.0 with vscode.\r\nBut, there is the same issue with #32982.\r\nThat issue was fixed according to https://github.com/tensorflow/tensorflow/issues/32982#issuecomment-589767901, but, only  >= 2.0.0...\r\n\r\nSo, I just cherry-pick the commit 5c00e79.", "comments": []}, {"number": 46084, "title": "u-net always not able to run in a single delegate", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n  No, just use  prebuilt  android_aarch64_benchmark_model\r\n- OS Platform and Distribution: \r\n  ested on Android \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  Qualcomm dragon 855\r\n- TensorFlow installed from (source or binary):\r\n  google tensorflow prebuilt android_aarch64_benchmark_model\r\n\r\n\r\n**Describe the current behavior**\r\nI tried many u-net model for semantic segmentation, none of those can be run on nnapi delegate fully. and most likely the transpose conv will be fallback on CPU. Though the underlying nnapi accel supports transport conv.\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should run the model fully on one single delegate without fallback to CPU which extremely slow. Or have some kinds of indication  why t is not  running fully in nnapi delegate and fallback to CPU. The TRANSPOSE_CONV and CONV2D are clearly supported by the underly nnapi acceleration. But it just fallback to CPU for unknown reason. \r\n\r\n**Standalone code to reproduce the issue**\r\nDownload android_aarch64_benchmark_model from https://www.tensorflow.org/lite/performance/measurement\r\nadb push that into /data/local/tmp/\r\ncreated one using unet with post quant.\r\nRun \r\n/android_aarch64_benchmark_model  --graph=model.tflite  --use_nnapi=true  --nnapi_accelerator_name=qti-gpu --enable_op_profiling=true                                                               \r\n\r\nExpected:\r\nExplicitly applied NNAPI delegate, and the model graph will be completely executed by the delegate.\r\n\r\nCurrent:\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nGraph: [model.tflite]\r\nEnable op profiling: [1]\r\nUse NNAPI: [1]\r\nNNAPI accelerator name: [qti-gpu]\r\nNNAPI accelerators available: [qti-default,qti-dsp,qti-gpu,qti-hta,nnapi-reference]\r\nLoaded model model.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for NNAPI.\r\nExplicitly applied NNAPI delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\r\nThe input model file size (MB): 0.398212\r\nInitialized session in 185.23ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=10 first=72646 curr=55109 min=43832 max=72646 avg=50987.8 std=7966\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=57407 curr=57830 min=39546 max=63652 avg=54400.3 std=5091\r\n\r\nInference timings in us: Init: 185230, First inference: 72646, Warmup (avg): 50987.8, Inference (avg): 54400.3\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=3.77344 overall=14.918\r\nProfiling Info for Benchmark Initialization:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  184.511\t  184.511\t 50.938%\t 50.938%\t  3864.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t           95.737\t  177.694\t   88.858\t 49.062%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t ModifyGraphWithDelegate\t            0.000\t  184.511\t  184.511\t 50.938%\t 50.938%\t  3864.000\t        1\tModifyGraphWithDelegate/0\r\n\t         AllocateTensors\t           95.737\t  177.694\t   88.858\t 49.062%\t100.000%\t     0.000\t        2\tAllocateTensors/0\r\n\r\nNumber of nodes executed: 2\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t ModifyGraphWithDelegate\t        1\t   184.511\t    50.938%\t    50.938%\t  3864.000\t        1\r\n\t         AllocateTensors\t        1\t   177.715\t    49.062%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=1 curr=362226\r\nMemory (bytes): count=0\r\n2 nodes observed\r\n\r\n\r\n\r\nOperator-wise Profiling Info for Regular Benchmark Runs:\r\n============================== Run Order ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t     TfLiteNnapiDelegate\t            0.000\t    7.375\t    6.549\t 12.047%\t 12.047%\t     0.000\t        1\t[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25\r\n\t                 CONV_2D\t            6.554\t   11.632\t    6.679\t 12.286%\t 24.333%\t     0.000\t        1\t[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10\r\n\t                 CONV_2D\t           13.236\t    7.296\t    6.370\t 11.717%\t 36.050%\t     0.000\t        1\t[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11\r\n\t                 CONV_2D\t           19.609\t    4.624\t    5.985\t 11.009%\t 47.059%\t     0.000\t        1\t[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12\r\n\t     TfLiteNnapiDelegate\t           25.596\t    3.368\t    3.671\t  6.753%\t 53.812%\t     0.000\t        1\t[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26\r\n\t          TRANSPOSE_CONV\t           29.271\t    3.924\t    4.570\t  8.407%\t 62.218%\t     0.000\t        1\t[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15\r\n\t                     ADD\t           33.843\t    0.238\t    0.267\t  0.490%\t 62.709%\t     0.000\t        1\t[unet/conv2d_transpose/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose/BiasAdd]:16\r\n\t     TfLiteNnapiDelegate\t           34.111\t    4.639\t    4.690\t  8.627%\t 71.335%\t     0.000\t        1\t[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27\r\n\t          TRANSPOSE_CONV\t           38.804\t    7.393\t    8.092\t 14.884%\t 86.219%\t     0.000\t        1\t[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20\r\n\t                     ADD\t           46.899\t    0.915\t    1.073\t  1.974%\t 88.194%\t     0.000\t        1\t[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21\r\n\t     TfLiteNnapiDelegate\t           47.974\t    5.968\t    6.418\t 11.806%\t100.000%\t     0.000\t        1\t[Identity]:28\r\n\r\n============================== Top by Computation Time ==============================\r\n\t             [node type]\t          [start]\t  [first]\t [avg ms]\t     [%]\t  [cdf%]\t  [mem KB]\t[times called]\t[Name]\r\n\t          TRANSPOSE_CONV\t           38.804\t    7.393\t    8.092\t 14.884%\t 14.884%\t     0.000\t        1\t[unet/conv2d_transpose_1/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose_1/conv2d_transpose1]:20\r\n\t                 CONV_2D\t            6.554\t   11.632\t    6.679\t 12.286%\t 27.170%\t     0.000\t        1\t[unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_8/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_8/BiasAdd/ReadVariableOp;unet/conv2d_8/BiasAdd;StatefulPartitionedCall/unet/conv2d_8/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D/ReadVariableOp;unet/conv2d_8/Conv2D;StatefulPartitionedCall/unet/conv2d_8/Conv2D1]:10\r\n\t     TfLiteNnapiDelegate\t            0.000\t    7.375\t    6.549\t 12.047%\t 39.217%\t     0.000\t        1\t[unet/Relu_1;StatefulPartitionedCall/unet/Relu_1;unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_1/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_1/BiasAdd/ReadVariableOp;unet/conv2d_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_1/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D/ReadVariableOp;unet/conv2d_1/Conv2D;StatefulPartitionedCall/unet/conv2d_1/Conv2D1, unet/Relu_3;StatefulPartitionedCall/unet/Relu_3;unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_3/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_3/BiasAdd/ReadVariableOp;unet/conv2d_3/BiasAdd;StatefulPartitionedCall/unet/conv2d_3/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D/ReadVariableOp;unet/conv2d_3/Conv2D;StatefulPartitionedCall/unet/conv2d_3/Conv2D1, unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_7/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_7/BiasAdd/ReadVariableOp;unet/conv2d_7/BiasAdd;StatefulPartitionedCall/unet/conv2d_7/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D/ReadVariableOp;unet/conv2d_7/Conv2D;StatefulPartitionedCall/unet/conv2d_7/Conv2D1]:25\r\n\t     TfLiteNnapiDelegate\t           47.974\t    5.968\t    6.418\t 11.806%\t 51.023%\t     0.000\t        1\t[Identity]:28\r\n\t                 CONV_2D\t           13.236\t    7.296\t    6.370\t 11.717%\t 62.740%\t     0.000\t        1\t[unet/Relu_7;StatefulPartitionedCall/unet/Relu_7;unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_9/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_9/BiasAdd/ReadVariableOp;unet/conv2d_9/BiasAdd;StatefulPartitionedCall/unet/conv2d_9/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D/ReadVariableOp;unet/conv2d_9/Conv2D;StatefulPartitionedCall/unet/conv2d_9/Conv2D1]:11\r\n\t                 CONV_2D\t           19.609\t    4.624\t    5.985\t 11.009%\t 73.749%\t     0.000\t        1\t[unet/Relu_8;StatefulPartitionedCall/unet/Relu_8;unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_10/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_10/BiasAdd/ReadVariableOp;unet/conv2d_10/BiasAdd;StatefulPartitionedCall/unet/conv2d_10/BiasAdd;unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_11/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_11/BiasAdd/ReadVariableOp;unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/BiasAdd;StatefulPartitionedCall/unet/conv2d_11/Conv2D/ReadVariableOp;unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_11/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D/ReadVariableOp;unet/conv2d_10/Conv2D;StatefulPartitionedCall/unet/conv2d_10/Conv2D1]:12\r\n\t     TfLiteNnapiDelegate\t           34.111\t    4.639\t    4.690\t  8.627%\t 82.376%\t     0.000\t        1\t[unet/Relu_12;StatefulPartitionedCall/unet/Relu_12;unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_14/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_14/BiasAdd/ReadVariableOp;unet/conv2d_14/BiasAdd;StatefulPartitionedCall/unet/conv2d_14/BiasAdd;unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_15/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_15/BiasAdd/ReadVariableOp;unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/BiasAdd;StatefulPartitionedCall/unet/conv2d_15/Conv2D/ReadVariableOp;unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_15/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D/ReadVariableOp;unet/conv2d_14/Conv2D;StatefulPartitionedCall/unet/conv2d_14/Conv2D1]:27\r\n\t          TRANSPOSE_CONV\t           29.271\t    3.924\t    4.570\t  8.407%\t 90.783%\t     0.000\t        1\t[unet/conv2d_transpose/conv2d_transpose;StatefulPartitionedCall/unet/conv2d_transpose/conv2d_transpose1]:15\r\n\t     TfLiteNnapiDelegate\t           25.596\t    3.368\t    3.671\t  6.753%\t 97.535%\t     0.000\t        1\t[unet/Relu_10;StatefulPartitionedCall/unet/Relu_10;unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_12/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_12/BiasAdd/ReadVariableOp;unet/conv2d_12/BiasAdd;StatefulPartitionedCall/unet/conv2d_12/BiasAdd;unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/batch_normalization_13/FusedBatchNormV3;StatefulPartitionedCall/unet/conv2d_13/BiasAdd/ReadVariableOp;unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/BiasAdd;StatefulPartitionedCall/unet/conv2d_13/Conv2D/ReadVariableOp;unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_13/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D/ReadVariableOp;unet/conv2d_12/Conv2D;StatefulPartitionedCall/unet/conv2d_12/Conv2D1]:26\r\n\t                     ADD\t           46.899\t    0.915\t    1.073\t  1.974%\t 99.510%\t     0.000\t        1\t[unet/conv2d_transpose_1/BiasAdd;StatefulPartitionedCall/unet/conv2d_transpose_1/BiasAdd]:21\r\n\r\nNumber of nodes executed: 11\r\n============================== Summary by node type ==============================\r\n\t             [Node type]\t  [count]\t  [avg ms]\t    [avg %]\t    [cdf %]\t  [mem KB]\t[times called]\r\n\t     TfLiteNnapiDelegate\t        4\t    21.327\t    39.233%\t    39.233%\t     0.000\t        4\r\n\t                 CONV_2D\t        3\t    19.033\t    35.013%\t    74.246%\t     0.000\t        3\r\n\t          TRANSPOSE_CONV\t        2\t    12.661\t    23.291%\t    97.537%\t     0.000\t        2\r\n\t                     ADD\t        2\t     1.339\t     2.463%\t   100.000%\t     0.000\t        2\r\n\r\nTimings (microseconds): count=50 first=57372 curr=57796 min=39517 max=63611 avg=54364.5 std=5088\r\nMemory (bytes): count=0\r\n11 nodes observed\r\n", "comments": ["@ek9852 \r\nPlease share minimum stand alone code to replicate the error faced, or if possible share a colab gist with issue reported.", "@Saduf2019 As I already mentioned. Just use the prebuilt benchmark binary on android phone with a unet model There is no application or custom code need.", "This turns out to be quant transpose convolution  layer not supported in  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/nnapi/nnapi_delegate.cc#L2228\r\nThe layer version of INT8 transpose convolution layer had a version  of 2 which failed the above checking.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/versioning/op_version.cc#L261\r\n\r\nI am opening a new case for that.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46084\">No</a>\n", "Opened a new ticket https://github.com/tensorflow/tensorflow/issues/46098 for that"]}, {"number": 46082, "title": "undefined symbol: _ZN6tflite12tensor_utils27NeonSymmetricQuantizeFloatsEPKfiPaPfS4_S4_", "body": "when I run .tflite file in my Raspberry Pi 3B+ ,some errors happened.\r\nPython version: 3.7.3\r\nTensorflow version: 1.13.1\r\nGCC: 8.3.0\r\nDescription:    Raspbian GNU/Linux 10 (buster)\r\nRelease:        10\r\nCodename:       buster\r\nI have changed tf version but it did not worK. \r\nPlease help!!\r\nhere is the error:\r\n`Traceback (most recent call last):\r\n  File \"/home/pi/project/test.py\", line 62, in <module>\r\n    interpreter = tf.lite.Interpreter(model_path=tflite_path)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter.py\", line 154, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n`\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@RunChengNi \r\n\r\nIs it possible for you to try latest TF stable version 2.4 or nightly versivons and let us know whether the issue persists? There were lots of performance improvements in the latest versions. Thanks!", "@ravikyram Yeah,it worked,I try TF version 2.4.But it should be noted that using pip directly was uesless,I downloaded whl file\uff0cand then pip it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46082\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46082\">No</a>\n"]}, {"number": 46081, "title": "Is there have API to automatically calculate #flops (e.g. like flop_counter in pytorch) in tensorflow2?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n***\r\nEnvironment: python3.6+tensorflow2.1, \r\nQuestion:  Is there have API to automatically calculate #flops (e.g.  flop_counter) in tensorflow2?\r\nNote: in Pytorch, this could be easily realized by calling API, e.g. thop.profile(),  flop_counter\uff0c etc.\r\n***\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@v3551G \r\nCould you please explain the issue in brief.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 46079, "title": "benchmark_model build failed", "body": "**System information**\r\n- OS Platform and Distribution ubuntu 18.04\r\n- TensorFlow installed from (source or binary): git hub master branch \r\n- TensorFlow version: master in github, tried also v2.3.0 v2.2.0 \r\n- Python version: 2\r\n- Installed using virtualenv? pip? conda?:NA \r\n- Bazel version (if compiling from source): Bazelisk version: v1.7.4 Build label: 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the problem**\r\nBuild benchmark tool according to README.md  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md failed :\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel  build -c opt   --config=android_arm64   tensorflow/lite/tools/benchmark:benchmark_model  --verbose_failures\r\n\r\n**Any other info / logs**\r\nRepository rule git_repository defined at:\r\n  /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nERROR: /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' **does not contain a toolchain for cpu 'arm64-v8a'**\r\nERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.175s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n", "comments": ["> **System information**\r\n> \r\n> * OS Platform and Distribution ubuntu 18.04\r\n> * TensorFlow installed from (source or binary): git hub master branch\r\n> * TensorFlow version: master in github, tried also v2.3.0 v2.2.0\r\n> * Python version: 2\r\n> * Installed using virtualenv? pip? conda?:NA\r\n> * Bazel version (if compiling from source): Bazelisk version: v1.7.4 Build label: 3.1.0\r\n> * GCC/Compiler version (if compiling from source): 7.5.0\r\n> * CUDA/cuDNN version: NA\r\n> * GPU model and memory: NA\r\n> \r\n> **Describe the problem**\r\n> Build benchmark tool according to README.md https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md failed :\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> bazel build -c opt --config=android_arm64 tensorflow/lite/tools/benchmark:benchmark_model --verbose_failures\r\n> \r\n> **Any other info / logs**\r\n> Repository rule git_repository defined at:\r\n> /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in\r\n> ERROR: /home/keith/.cache/bazel/_bazel_keith/71ff04d0920d083c690fb304f211064a/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' **does not contain a toolchain for cpu 'arm64-v8a'**\r\n\r\nIt looks like your overall building environment isn't set properly. Please follow https://www.tensorflow.org/lite/guide/build_android for details.\r\n\r\nOr you could use pre-built binaries mentioned in https://www.tensorflow.org/lite/performance/measurement\r\n\r\n> ERROR: Analysis of target '//tensorflow/lite/tools/benchmark:benchmark_model' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n> INFO: Elapsed time: 0.175s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n\r\n", "I was using the prebuilt benchmark binary. Thnaks.\r\nBut the problem of building the benchmark tool still exist.\r\nAnd when I tried to build using android_arm instead of android_arm64,  the toolchain can be found and download.  Currently we have a problem in \"does not contain a toolchain for cpu 'arm64-v8a'\"   which might be related to broken url or link in the bazel config.\r\n", "> I was using the prebuilt benchmark binary. Thnaks.\r\n> But the problem of building the benchmark tool still exist.\r\n> And when I tried to build using android_arm instead of android_arm64, the toolchain can be found and download. Currently we have a problem in \"does not contain a toolchain for cpu 'arm64-v8a'\" which might be related to broken url or link in the bazel config.\r\n\r\n@ek9852, this issue looks quite stale now, and I'm closing this issue. If this issue still happens to you, feel free to re-open it. Thx!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46079\">No</a>\n"]}, {"number": 46078, "title": "tensorflow.math.abs crashes Python on Windows", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Trivial example given.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): using pip\r\n- TensorFlow version (use command below): v1.12.1-48227-g77eafffd690 2.5.0-dev20201230 tf-nightly==2.5.0.dev20201230\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: CUDA 11.1, cuDNN 8.0.5\r\n- GPU model and memory: RTX 3080 10 GB\r\n\r\nEDIT:\r\nI've also managed to reproduce the issue using the same software, but on a GTX 970, as well as on a GTX 1060.\r\n\r\n**Describe the current behavior**\r\nAny usage of tensorflow.math.abs crashes Python. Functions which depend on tensorflow.math.abs on the backend also crash, such as keras.losses.Huber and keras.losses.MeanAbsoluteError.\r\n\r\n**Describe the expected behavior**\r\nIt should just compute absolute values without crashing.\r\n\r\n**Standalone code to reproduce the issue**\r\nThis trivial example should yield the error:\r\n```\r\nimport tensorflow\r\nx = tensorflow.convert_to_tensor([0], dtype='float32')\r\ny = tensorflow.math.abs(x)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-12-30 12:47:20.566855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-12-30 12:47:20.597895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-12-30 12:47:20.598347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-30 12:47:20.626950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-30 12:47:20.627203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-30 12:47:20.631687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-30 12:47:20.635184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-30 12:47:20.639243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-30 12:47:20.642267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-30 12:47:20.643017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-30 12:47:20.643278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0\r\n2020-12-30 12:47:20.644054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-30 12:47:20.644953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 10.00GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-12-30 12:47:20.645412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-12-30 12:47:20.645637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-12-30 12:47:20.645846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-12-30 12:47:20.646045: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-12-30 12:47:20.646244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-12-30 12:47:20.646443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-12-30 12:47:20.646648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-12-30 12:47:20.646859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-12-30 12:47:20.647092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1902] Adding visible gpu devices: 0\r\n2020-12-30 12:47:21.201575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-12-30 12:47:21.201806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 \r\n2020-12-30 12:47:21.201945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N \r\n2020-12-30 12:47:21.202263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6703 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6)\r\nProcess finished with exit code -1073741571 (0xC00000FD)\r\n```\r\n", "comments": ["@instigatorofawe \r\n\r\nI have tried in colab with TF nightly version(`2.5.0-dev20201230`) and I am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/be089c459c0f74804254ca51c6032da0/untitled589.ipynb).But colab uses cuda 10.1 . Might be problem related to Cuda 11.1.Thanks!", "> @instigatorofawe\r\n> \r\n> I have tried in colab with TF nightly version(`2.5.0-dev20201230`) and I am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/be089c459c0f74804254ca51c6032da0/untitled589.ipynb).But colab uses cuda 10.1 . Might be problem related to Cuda 11.1.Thanks!\r\n\r\nI don't think it's a problem with CUDA 11.1 so much as it's a problem with CUDA on Windows. I'm able to run without issue on the same hardware in Ubuntu 20.04 using the same versions of CUDA and TensorFlow. I am downgrading and trying again, though.\r\n\r\nEDIT:\r\nTF 2.4 and higher seem to require CUDA 11.0. I downgraded to 11.0 on Windows, yet this error still occurs.", "Thank you for reporting this. I have verified that this is now fixed when building from source. The next nightly build should also contain the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46078\">No</a>\n"]}, {"number": 46076, "title": "Receiving the error: \"ValueError: Input 0 of layer sequential_8 is incompatible with the layer: expected axis -1 of input shape to have value 8 but received input with shape (None, 7, 169)\"", "body": "Hello,\r\n\r\nI would like to apologize if this is posted in the wrong location.  If that is the case, any guidance as to where I can ask this question would be greatly appreciated.  I am new to coding and GitHub, but have found many answers to posts on this forum very helpful to me.  I am trying to setup a code to utilize the Keras model for deep learning with a 3d dataset, and am uncertain how to deal with the error I'm receiving (in the title).  I have found a similar error reported on this forum, but the response was that it would be fixed in TensorFlow v 2.1.  I am using v2.4.0, so I expect I'm probably doing something wrong here.  The code I am using is as follows:\r\n\r\n```\r\n# define the keras model\r\nmodel = Sequential()\r\nmodel.add(Dense(12, input_dim=7, activation='relu'))\r\nmodel.add(Dense(8, activation='relu'))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n# compile the keras model\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n# separate the data\r\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\r\n\r\n# fit the keras model on the dataset\r\nmodel.fit(X_train, y_train, epochs=150, batch_size=10)\r\n\r\n# evaluate the keras model\r\n_, accuracy = model.evaluate(X_test, y_test)\r\nprint('Accuracy: %.2f' % (accuracy*100))\r\n```\r\n\r\nInformation on the variables as follows:\r\n_X is an array of float64, with size (152,7,169)\r\nY is an array of int32, with size (152,)_\r\n\r\nInformation on versions as follows:\r\n_Python version 3.8.5\r\nKeras version 2.4.3\r\nTensorFlow version 2.4.0\r\nWindows 10\r\nIntel(R) Core(TM) i5-8350U CPU @ 1.70GHz   1.90 GHz  (AVX capable)\r\nSpyder version 4.2.0\r\n _\r\n\r\nThe full error I am getting is as follows:\r\n\r\n> Epoch 1/150\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"C:\\Users\\pdeol\\Documents\\CodingPractice\\KerasCombined.py\", line 171, in <module>\r\n>     model.fit(X_train, y_train, epochs=150, batch_size=10)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n>     tmp_logs = self.train_function(iterator)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n>     result = self._call(*args, **kwds)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 871, in _call\r\n>     self._initialize(args, kwds, add_initializers_to=initializers)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 725, in _initialize\r\n>     self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n>     graph_function, _ = self._maybe_define_function(args, kwargs)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3361, in _maybe_define_function\r\n>     graph_function = self._create_graph_function(args, kwargs)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3196, in _create_graph_function\r\n>     func_graph_module.func_graph_from_py_func(\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 990, in func_graph_from_py_func\r\n>     func_outputs = python_func(*func_args, **func_kwargs)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 634, in wrapped_fn\r\n>     out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n> \r\n>   File \"C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 977, in wrapper\r\n>     raise e.ag_error_metadata.to_exception(e)\r\n> \r\n> ValueError: in user code:\r\n> \r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\r\n>         return step_function(self, iterator)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\r\n>         outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\r\n>         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\r\n>         return self._call_for_each_replica(fn, args, kwargs)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\r\n>         return fn(*args, **kwargs)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\r\n>         outputs = model.train_step(data)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\r\n>         y_pred = self(x, training=True)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\r\n>         input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\r\n>     C:\\Users\\pdeol\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:255 assert_input_compatibility\r\n>         raise ValueError(\r\n> \r\n>     ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 7 but received input with shape (None, 7, 169)\r\n\r\nThe above error is what I receive every time I run this code.  I just restarted the software and ran it again, and in addition to the above error, I received the following error immediately below:\r\n\r\n> 2020-12-30 10:17:26.556667: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n> 2020-12-30 10:17:26.557141: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n> 2020-12-30 10:26:08.613023: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2020-12-30 10:26:08.616994: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n> 2020-12-30 10:26:08.617895: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n> 2020-12-30 10:26:08.628902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: LT-B831B58344E3\r\n> 2020-12-30 10:26:08.629094: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: LT-B831B58344E3\r\n> 2020-12-30 10:26:08.634489: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\n> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n> 2020-12-30 10:26:08.636834: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n> 2020-12-30 10:26:08.863911: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n\r\nI'm not sure if this is related, or a completely separate issue.  Any information or guidance you can provide would be greatly appreciated.", "comments": ["@p-deol \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nplease move this to close status once issue is created on SO.", "Thank you.  I received a response from Andrey on StackOverflow that solved my issue.  Response was as follows:\r\n\r\n> Input_dim argument of the first Dense layer is 7. It means that your model expecting the last dimension of your data is 7. But you are feeding (152, 7, 169), where 169 is the last dimension.\r\n> \r\n> Try to transpose it:\r\n> \r\n> X = tf.transpose(X, [0, 2, 1])"]}, {"number": 46075, "title": "fix static path for find_cuda_config.py script, so tensorflow with cuda can be installed outside master directory.", "body": "Master Tensorflow repo is getting used in another tensorflow related projects ( for example [s4tf](https://github.com/tensorflow/swift-apis/blob/main/Documentation/Development.md) )\r\n\r\nAs building in other project are done from upper directory static path for find_cuda_config.py was breaking installation this pull request fix that.", "comments": ["@machineko Can you please address Ubuntu Sanity errors? Thanks!", "> @machineko Can you please address Ubuntu Sanity errors? Thanks!\r\n\r\n\r\nI'm not sure what python version is used but it's looks like its smaller than python 3.4 as pathlib was added to standard lib in 3.4 version =>\r\n\r\n> === Sanity check step 1 of 14: do_configure_test (Run ./configure) ===\r\n> ImportError: No module named pathlib\r\n\r\nWhere i can find some more information about sanity building script?\r\n\r\n\r\n\r\n", "@mihaimaruseac Can you please assist on above comments from @machineko. Thanks!", "It should use py3.6, but I'll take a look", "```\r\ncheck whether pylint is available or not.\r\n\r\n__main__.py 1.6.4,\r\nastroid 1.4.9\r\nPython 3.5.6 (default, Oct 12 2020, 20:06:06)\r\n[GCC 5.4.0 20160609]\r\nThis option 'ignore-iface-methods' will be removed in Pylint 2.0This option 'required-attributes' will be removed in Pylint 2.0\r\npylint available, proceeding with pylint sanity check.\r\n```\r\n\r\nSeems this is wrong. We'll get a fix and then retrigger the CI", "The issue is that we are using a old version of pylint that only works for python3.5. We need to update that, but are currently blocked by #46046 / #41396\r\n\r\nWill have to first do #46046 and then we can continue", "Let's try now", "Build should turn green now and then we can import it. Sorry for the long delay", "> Build should turn green now and then we can import it. Sorry for the long delay\r\n\r\nHey no problem it was quiet fast considering problem was in a different place \ud83d\ude04  \ud83d\udce6 "]}, {"number": 46074, "title": "Are there any excellent github projects on action recognition implemented by TF2?", "body": "I have been searching for official or non-official awesome code projects of action recognition written by TF2 for a long time, but unfortunately fail. I'd like to integrate video classification with other TF2 elements more conviniently. The most majority of video-relevant resources are under the community of Pytorch, though only a few projects were done by TF1, which seem fairly passe and only attract very few stars. Since the TF 2.X version has been released for over 2 years, why does its current ecosystem yet appear to be frustrated? Does anybody else cope with similar questions?", "comments": ["Did you see [action recognition with tf hub using i3d](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub) tutorial?", "> \r\n> \r\n> Did you see [action recognition with tf hub using i3d](https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub) tutorial?\r\n\r\nThanks. Looking forward to more elaborate official tutorials and demos by TF2 "]}, {"number": 46072, "title": "installing Keras package with tensorflow 2.3.1", "body": "I am a beginner in this, I want to make OpenCV and TensorFlow project and I have some issues\r\n\r\nI downloaded python 3.7.6 with pip 19.2.3\r\n\r\nafter that, I used CMD to download Tensorflow 2.3.1 and I made the path in a python project  where I am coding \r\n_C:\\Users\\Desktop\\PycharmProjects\\SudokuSolver\\venv\\Lib\\site-packages\\tensorflow>_**pip install tensorflow==2.3.1**\r\n\r\nnow when I am importing the libraries:\r\n\r\nimport TensorFlow\r\nfrom TensorFlow.Keras.models import load_model\r\n\r\nThat gives me an error\r\n**\"ModuleNotFoundError: No module named 'tensorflow.keras'\"**\r\n\r\nSo what should I do and where is the wrong how to fix it?\r\nMany thanks.", "comments": ["go to stackoverflow, tensorflow github issue is not for teaching\r\n\r\nfor answer, the right import statement is all lower-case, \"tensorflow\" but not \"TensorFlow\"", "@Moaaz421 \r\nPlease use this and let us know if you face any issues:\r\n\r\n```import tensorflow as tf```\r\n```from tensorflow.keras.models import load_model```", "Thanks @Saduf2019 ", "Does this mean this is solved?", "@Moaaz421\r\nPlease move the issue to closed status as it is resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46072\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46072\">No</a>\n"]}, {"number": 46071, "title": "How the parameters of decay_rate & decay_steps are taken into consideration while computing InverseTimeDecay()", "body": "During the last couple of days, I am experimenting with the different schedulers of learning rate decay offered by Keras (link [here][1]). Specifically, I have been using *InverseTimeDecay: A LearningRateSchedule that uses an inverse time decay schedule.* This type of scheduler uses the following two parameters:\r\n1) decay_steps\r\n2) decay_rate\r\n\r\n*Note that those two parameters are also used by ```class ExponentialDecay```*\r\n\r\nI would like to understand how those two parameters are initialized and taken into consideration by the InverseTimeDecay class. So far, what I know is that the following formula is used (based on the documentation):\r\n\r\n```\r\ndef decayed_learning_rate(step):\r\n  return initial_learning_rate / (1 + decay_rate * step / decay_step)\r\n```\r\n\r\nThe parameter initialization I use:\r\n\r\n```\r\ndef optimizer_adam_v2(hparams):\r\n    # Inverse Time Decay\r\n    initial_learning_rate = 0.001\r\n    decay_steps = int(np.ceil(len(x_train)//32))*10 #len(x_train) = 31334, and 10 represents the epoch. So after 10 epochs or similarly 9791 steps the decay rate will be applied.\r\n    decay_rate = 0.5\r\n    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate)\r\n\r\n    return keras.optimizers.Adam(learning_rate=learning_rate_fn)\r\n```\r\n\r\nBased on the values above and the formula, the ```decay_learning_rate``` should be equal to\r\n\r\n```\r\ndecay_learning_rate = 0.001 / (1 + 0.5 * step / 9791) = 0.0009999489353010264\r\n# 9791 = int(np.ceil(31334//32))*10 #decay_steps parameter\r\n```\r\n\r\nWhen the training process begins the learning rate starts to drop by the decay_learning_rate at every step of the training. However, based on the [documentation][2] (\t```decay_steps``` is \"How often to apply decay\"). the learning rate should start dropping after the training pass the value of the decay_steps. So after 9791 steps (approximately 10 epochs). Instead, the learning rate starts to drop from epoch 0. \r\n\r\nWhy is this happening? Why the decay starts from epoch 0 and not from epoch 10 (9791 steps have passed).\r\n\r\nP.S 1: Similar [question][3]\r\n\r\n**[UPDATE] - 02.01.2021**\r\n\r\nThis is also observed with the Piecewise Constant Decay (link [here][3])\r\n\r\nPiecewise constant decay is a similar scheduler to InverseTimeDecay with a much simpler application. The concept is that for every N number of steps the learning rate is constant and changes based on an array of values.\r\n\r\nFor example I have implemented a piecewise decay scheduler like below:\r\n\r\n```\r\ndef optimizer_adam_v2():\r\n    # PiecewiseConstantDecay\r\n    step = tf.Variable(0, trainable=False)\r\n    boundaries = [1000, 5000]\r\n    values = [0.001, 0.0005, 0.00025]\r\n\r\n    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\r\n\r\n    # Later, whenever we perform an optimization step, we pass in the step.\r\n    learning_rate_adam = learning_rate_fn(step)\r\n\r\n    return keras.optimizers.Adam(learning_rate=learning_rate_adam)\r\n```\r\n\r\nIn short, *based always on the relevant documentation written by Tensorflow experts*, what the code above does is to use a learning rate of 0.001 for the first 1000 steps, 0.0005 for the next 5000 steps (until 6000 steps completed), and the for the rest of the training use 0.00025 learning rate.\r\nSince I monitored the learning rate of my scheduler I got the following result [here ](https://drive.google.com/file/d/1jvDzOZMB9YkuUxQ4I0yCapI5Hb9LaQM-/view?usp=sharing)in my Drive folder\r\n\r\nWith the indication ```lr``` I monitor the learning rate of the Adam optimizer. Until epoch 15 the learning rate didn't change whatsoever. Based on step calculation methodology, 15 epochs is approximately equal to ```len(x_train)//batch_size * epochs = (31335//32)*15 = 979*15 = 14,687 steps.```\r\n\r\nAfter all, my question is still on the table. Why the learning rate of Adam does not obey the change rules of each scheduler.\r\n\r\nYou may find the implementation of the ```Piecewise constant decay``` scheduler on my colab notebook [here](https://colab.research.google.com/drive/1BsdrgZDtoDEASTikKa4LEVAz7IBzp3jw?usp=sharing)\r\n\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules\r\n  [2]: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay\r\n  [3]: https://stackoverflow.com/questions/60029027/decay-parameter-of-adam-optimizer-in-keras", "comments": ["@NikosSpanos \r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!", "> @NikosSpanos\r\n> \r\n> Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> Request you to share colab link or simple standalone code to reproduce the issue in our environment. It helps in localizing the issue faster. Thanks!\r\n\r\n@ravikyram \r\nI have added a google colab link to my question. Also, I have initially posted this question as a Documentation Issue (from this [list](https://github.com/tensorflow/tensorflow/issues/new/choose)) but I deleted any irrelevant content/text to my question.", "@NikosSpanos This is a stale issue. Is this still an issue for you?\r\n\r\nAs Keras moved to separate repo (i can't move the issue), can you please close here and open in keras repo https://github.com/keras-team/keras/issues\r\n\r\nKeras team is focussed on the new repo mentioned above. Also, when you open new issue, please share a simple standalone code to reproduce the issue. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @NikosSpanos This is a stale issue. Is this still an issue for you?\r\n> \r\n> As Keras moved to separate repo (i can't move the issue), can you please close here and open in keras repo https://github.com/keras-team/keras/issues\r\n> \r\n> Keras team is focussed on the new repo mentioned above. Also, when you open new issue, please share a simple standalone code to reproduce the issue. Thanks!\r\n\r\nI will close the issue here. I won't re-open it in Keras repo because I didn't  have the time to check any new updates on this matter. Probably it's resolved. But in any case if this issue come to my attention again I will re-open it on the new Keras repo. \r\n\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46071\">No</a>\n"]}, {"number": 46069, "title": "No matching distribution found during installation suing dockerfile", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04 (Docker)\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:1.12\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n\r\n**Describe the problem**\r\nHi, i am building this data analytics project in which i need to use tensorflow using dockerfile running on raspberry pi 4 python 2.7. I did not encounter this issue when i was running the dockerfile in amd64 architecture however in raspberry pi 4 python 2.7 it gives out error no matching distribution found for this platform. P\r\n![tensorflow2](https://user-images.githubusercontent.com/76122954/103345254-d9556180-4acb-11eb-90d7-09a6229fe3be.png)\r\nlease do help me figure this out. Thank you in advance. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nRUN pip install --no-cache-dir pandas sklearn keras tensorflow \\\r\n    && apt-get autoremove -y \\\r\n    && apt-get clean \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@abby-93 \r\nWe see that you are using 1.x and there is no support for it, please upgrade to 2.x and let us know, please refer to these [requirements](https://github.com/tensorflow/tensorflow/issues/42367#issuecomment-674531556) and let us know.", "HI, i am still new to this but I have tried the above mentioned instructions and i am still getting the error. Is it because i am trying to install it in raspberry pi?", "@abby-93 \r\nPlease confirm if you are using 2.x version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46069\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46069\">No</a>\n"]}, {"number": 46068, "title": "Error when build plugin", "body": "When I try to compile TF with a plugin, I run the following commands:\r\nc++ -O3 -rdynamic -std=c++11 -I/usr/local/lib/python2.7/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -Wall -c plugin.cc -o plugin.o\r\nc++ -shared -fPIC -Wl,-soname,libplugin.so.1 -L/usr/local/lib/python2.7/dist-packages/tensorflow -ltensorflow_framework -o libplugin.so plugin.o\r\n\r\nHowever, when I try to tensorflow.load_library('libplugin.so'), error occurs: \r\ntensorflow.python.framework.errors_impl.NotFoundError: ./libplugin.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n\r\nI use the following command: ldd libconfig.so, but no libtensorflow_framework.so is included:\r\n\tlinux-vdso.so.1 (0x00007ffceaa5e000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fd2cc4f2000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fd2cc2da000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fd2cbee9000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fd2cbb4b000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fd2ccaa7000)\r\nI find libtensorflow_framework.so in /usr/local/lib/python2.7/dist-packages/tensorflow.\r\nSo what's wrong with the compilation process?\r\n\r\nAny help would be appreciated! Thanks\r\n\r\nThe system information: ubuntu18.04, cuda10.0, cudnn7, tf1.13.2, gcc 4.8.5", "comments": ["@Rivendile \r\n\r\nCan you refer the similar issues [link1](https://github.com/sadeepj/crfasrnn_keras/issues/53), [link2](https://stackoverflow.com/questions/48189818/undefined-symbol-ztin10tensorflow8opkernele) and see if it helps you.\r\nIf the problem still persists please, provide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "I added --no-as-needed in the compilation command and solved this problem. Thanks!"]}, {"number": 46067, "title": "Data input pipeline does not implement Batch for \"from_tensor_slices(dict(df))\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): latest from source\r\n- TensorFlow version (use command below): latest from source\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.6.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 2070\r\n\r\n**Describe the current behavior**\r\n```https://www.tensorflow.org/guide/data#consuming_csv_data```\r\n\r\n```titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))```\r\n\r\nThat code will have dictionary object slice.\r\nelement_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,\r\n\r\n```def make_window_dataset(ds, window_size=5, shift=1, stride=1):\r\n  windows = ds.window(window_size, shift=shift, stride=stride)\r\n\r\n  def sub_to_batch(sub):\r\n    return sub.batch(window_size, drop_remainder=True)\r\n\r\n  windows = windows.flat_map(sub_to_batch)\r\n  return windows\r\n```\r\n\r\n", "comments": ["closing this as duplicate of #46066", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46067\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46067\">No</a>\n"]}, {"number": 46066, "title": "Input pipe line with consuming CSV data (dictionary object) does not implement Batch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): latest from source\r\n- TensorFlow version (use command below): latest from source\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.6.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 2070\r\n\r\n**Describe the current behavior**\r\n```https://www.tensorflow.org/guide/data#consuming_csv_data```\r\n\r\ntitanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\r\nThat code will have dictionary object slice.\r\nelement_spec returns {dict:XX} This throws exception if we use Batch from the same documentation,\r\n\r\ndef make_window_dataset(ds, window_size=5, shift=1, stride=1):\r\n  windows = ds.window(window_size, shift=shift, stride=stride)\r\n\r\n  def sub_to_batch(sub):\r\n    return sub.batch(window_size, drop_remainder=True)\r\n\r\n  windows = windows.flat_map(sub_to_batch)\r\n  return windows\r\n\r\n\r\n", "comments": ["@summa-code \r\nPlease confirm this issue is faced on windows, as we see you have mentioned linux and the code says windows.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46066\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46066\">No</a>\n"]}, {"number": 46064, "title": "Wrong casting (mixed precision) in attention layers", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Google Colab\r\n- TensorFlow version (use command below): v2.4.0-0-g582c8d236cb 2.4.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\n**Describe the current behavior**\r\nWhen using Attention or AdditiveAttention with mixed precision policy issue occurred due to wrong casting (mask casted to floatx but should be casted to scores.dtype)\r\n\r\n**Describe the expected behavior**\r\nLayers should work without issues with mixed_fp16\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1R2MKXAIrmYBBGkij11m9aG7hLm2AEHjF?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe issue comes from here https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/layers/dense_attention.py#L129\r\nShould be this:\r\n```python\r\nscores -= 1.e9 * math_ops.cast(padding_mask, dtype=scores.dtype)\r\n```", "comments": ["I have tried in colab with TF version 2.4, Nightly version(`2.5.0-dev20201229`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/7b6b6b54fbf4b7df517cb537b726e371/untitled588.ipynb). Thanks!", "Added a PR #46321 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46064\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46064\">No</a>\n"]}, {"number": 46063, "title": "Fix conv layer for partially unknown spatial shape", "body": "Fixes #44092. Though I think it's better to handle this in `tf.nn.conv*`. Is it possible to expose [keras.utils.conv_utils.conv_output_length](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/conv_utils.py#L90) to tf core?", "comments": []}, {"number": 46060, "title": "Propagate optional argv to benchmarks_main", "body": "Allows the following:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass BasicBenchmark(tf.test.Benchmark):\r\n  def benchmark_foo(self):\r\n    print(\"Ran foo\")\r\n \r\nif __name__ == '__main__':\r\n  tf.test.main([__file__, '--benchmarks=.*'])", "comments": ["It defaults to sys.argv if the argument is omitted. What does this change?", "@allenlavoie it allows `benchmarks_main` to use custom `argv` rather than `sys.argv`, like in the example above. If there isn't a `--benchmarks` flag this will just be `main_wrapper`, but the example above demonstrates where you might want to force benchmarks to be run regardless of what CLargs are passed."]}]