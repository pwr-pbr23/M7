[{"number": 1630, "title": "Improve LSTMCell's appearance in TensorBoard", "body": "This is a very small cosmetic change, right now the 'concat' operation\nisn't carried out in the scope created by calling LSTMCell so\nthis makes it appear weirdly inside tensorboard.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Wow, ok, this is a lot of overhead for a one line change, probably easier for someone in the team to make the change than to go though the whole PR process, but I'll leave it open anyway so we don't forget I guess.\n\nKind Regards\n\nAK\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Ok so I've signed the CLA anyway, probably be useful in the future.\n\nCheers!\n", "Jenkins, test this please.\n", "@vrv: Looks like @concretevitamin doesn't have test permissions?\n", "(he is working on that, I think ;)  He's the test case!)\n", "@tensorflow-jenkins : test this please\n"]}, {"number": 1629, "title": "correct worker_service.proto link", "body": "Fixes up a link in the distributed_runtime's README.md\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 1628, "title": "Saver restoring multiple sessions throws errors", "body": "### Environment info\n\nOperating System: Linux\n### Steps to reproduce\n1. Save two sessions independently from each other\n2. Run a Saver.restore on the first session\n3. Run a Saver.restore on the second session\n### Logs or other output that would be helpful\n\nW tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]\n     [[Node: save/Assign_21 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/Adam, save/restore_slice_21)]]\nW tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]\n     [[Node: save/Assign_22 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W/Adam_1, save/restore_slice_22)]]\nW tensorflow/core/common_runtime/executor.cc:1161] 0x3437b20 Compute status: Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [6453,128] rhs shape= [1853,128]\n     [[Node: save/Assign_20 = Assign[T=DT_FLOAT, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding/W, save/restore_slice_20)]]\n", "comments": ["How did you ensure that \"Save two sessions independently from each other\"?\n\nSherry\n", "These are all managed through a class, I have separate training scripts that train models and save them into their own folders. I then had a classify script that I passed a file to, it instantiated 5 of the below classes attempting to load each of the sessions from their respective folders.  It throws the error above on all sessions but the first.  I tried changing the order of the sessions being loaded so it's definitely something being \"shared\" between the first and subsequent sessions.\n\nCode below is truncated\n\n```\nimports...\n\nclass TextModel(object):\n\n   def __init__(self, path, disable_gpu=False):\n        self.path = path.rstrip('/')\n        ...\n\n    ...\n\n    def save_session(self, folder, step=None):\n        with self.graph.as_default():\n            with self.session.as_default():\n                try:\n                    self.saver\n                except:\n                    self.saver = tf.train.Saver()\n\n                if not os.path.exists(folder):\n                    os.makedirs(folder)\n                self.saver.save(self.session, folder+\"/session\", global_step=step)\n\n  def save_model(self):\n        with self.graph.as_default():\n            with self.session.as_default():\n                self.save_session(self.path)\n                with open(self.path+\"/saver_def.pkl\", \"wb\") as f:\n                    pickle.dump(self.saver.as_saver_def(),f)\n                var_names = [v.name for v in tf.all_variables()]\n                with open(self.path+\"/var_names.json\", \"wb\") as f:\n                    json.dump(var_names, f)\n                try:\n                    os.rename(self.path+\"/graph\",self.path+\"/graph_\"+str(time.time()))\n                    print(\"Existing graph folder renamed to \"+self.path+\"/graph\"+time.time()+\"\\n\", file=sys.stderr)\n                except:\n                    pass\n                try:\n                    tf.train.write_graph(self.session.graph_def, self.path+\"/graph\", \"graph\", as_text=False)\n                except:\n                    print(\"Graph folder not created\\n\", file=sys.stderr)\n\ndef load_model(self):\n            with open(self.path+\"/var_names.json\", \"r\") as f:\n                var_names = json.load(f)\n            with open(self.path+\"/saver_def.pkl\", \"r\") as f:\n                saver_def = pickle.load(f)\n                self.session = tf.Session(config=self.tf_config)\n                self.graph = self.session.graph\n                with self.graph.as_default():\n                    with open(self.path+\"/graph/graph\", 'rb') as f:\n                        graph_def = tf.GraphDef.FromString(f.read())\n                        self.graph_variables = tf.import_graph_def(graph_def, return_elements=var_names, name=\"\")\n\n                    self.saver = tf.train.Saver(saver_def=saver_def)\n                    self.saver.restore(self.session, self.path+\"/session\")\n                    with self.session.as_default():\n                        tf.initialize_all_variables()\n                        self.model_loaded = True\n```\n", "To add, \"W\" is a tensorflow variable I use, it's complaining about the shapes so I'd wager that the W variable is getting mixed up between the first graph_def or session and a subsequent one.\n", "When restoring, could you please create a new graph for each of your sessions instead of sharing the same one? Something like this:\n\nself.graph = tf.Graph()  # Creates a new graph.\ntf.Session(graph=self.graph, config=self.tf_config)\n\nSherry\n", "Sorry for the long response time, what you suggested solved my issue, I didn't think using session.graph on different sessions was \"sharing the same one\" still not sure why this is the case but passing in empty graphs let me load multiple sessions via the saver\n", "I ran into this same problem as well.  Although I'm sure having a new graph for each session is ideal, but it should still work the way proposed, right?\n"]}, {"number": 1627, "title": "Make configure hard code path to swig", "body": "I've set it up to just try \"swig\" if configure hasn't been run since\nsyncing to this change, so users who sync and have no swig issues won't\nhave to do anything new.\n\nFixes #1329.\n", "comments": ["Jenkins, test this please.\n", "@vrv: Is the server_lib_test timeout resolved?  http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/459/console\n", "@tensorflow-jenkins: test this please\n\n(not sure, I think it's being \"fixed\")\n", "Our triggering seems broken right now.\n"]}, {"number": 1626, "title": "solve the boringssl problem in grpc", "body": "boringssl in grpc was previously held in googlesouce which block people under the wall,\nupgrade to this commit version will change its source to https://github.com/google/boringssl.git\nand this solve all the problems.\nDetail can be found in https://github.com/tensorflow/tensorflow/issues/1413#issuecomment-200804155\n", "comments": ["Can one of the admins verify this patch?\n", "@mrry @vrv @girving  Could you verify this? I think you have the background on this problem.\n", "@mrry is probably the right person for grpc questions.\n", "@tensorflow-jenkins: test this please\n", "Should be fine as long as (i) it builds, and (ii) the tests pass. I'm not aware of any breaking API changes in gRPC since we first pinned to a commit.\n", "The timeout look suspicious but is unrelated to this change (and will be fixed when one of @vrv's patches gets in). I'm going to merge this now.\n"]}, {"number": 1625, "title": "Update relu link in 2_fullyconnected.ipynb", "body": "", "comments": []}, {"number": 1624, "title": "Delete tf.reduce_mean on scalar", "body": "Remove `tf.reduce_mean()` from `tf.reduce_mean(tf.nn.l2_loss())` in _tensorflow/tools/docker/notebooks/2_getting_started.ipynb_\nAccording to [reference](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#l2_loss), `tf.nn.l2_loss()` always returns a scalar, thus no\nneed to average it.\nAt last code block where loss function is re-implemented by hand,there is\nno _mean()_ operation.\nExcuse me if something is wrong, have little to no exp. in python and tensorflow.\n", "comments": ["Can one of the admins verify this patch?\n", "Looks good.  Please rebase and squash to avoid all the merge commits.  We don't like them in PRs.  Once you do that, I can test and merge.\n", "@tensorflow-jenkins: test this please\n\n(all the extra commits are merges).\n", "@girving: it was [says in another commit](https://github.com/tensorflow/tensorflow/pull/1639) that \n\n> merge commits don't show up in our history\n\nDo i still need to rebase and squash? If i do, am I right that i can rebase in my local git, update this branch on github, and commits will be squashed in this pull requests automatically? Or I need to make a new branch and pull request with squashed commits?\n", "No need to rebase / squash :)\n\n@tensorflow-jenkins: test this please\n", "thx :)\n"]}, {"number": 1623, "title": "Typo in comment, random weights init", "body": "According to the code and tensorflow reference standard deviation would\nbe 0.1\nExcuse me if something is wrong, have little to no experience in python\nand tensorflow\n", "comments": ["Can one of the admins verify this patch?\n", "Already merged as #1617\n"]}, {"number": 1622, "title": "Fix nbviewer link to deepdream.ipynb", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1621, "title": "add matplotlib inline", "body": "to redirect the image of matplotlib to notebook\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 1620, "title": "remove dropout in MNIST tutorial", "body": "remove dropout in MNIST tutorial since it didn't improve the accuracy but make training slower,\nwhat's more, it make the tutorial long and tedious. A simpler demo which can show same results will be cool.\nAfter removing dropout, the accuracy remains around 99.2% and sometimes even higher (test 3 times, 0.9919, 0.9922, 0.9929).\n", "comments": ["Can one of the admins verify this patch?\n", "@vincentvanhoucke: feel free to re-assign to reassess\n", "To throw in my two cents: I think that, while training materials shouldn't be overly tedious in terms of time-to-completion, in this case it's more important to demonstrate the usage of an important API component. Since dropout is such a common and important tool, showcasing how to use it properly with TensorFlow in the MNIST tutorial sets users up for more success (and less frustration) in the long term.\n\nWhile it's nice that the model in this example appears to work well without dropout, the purpose of the tutorial is to demonstrate how to use the TensorFlow API, not create the fastest-to-train accurate model.\n", "@samjabrahams \nI agree with you, we should demo the dropout api but maybe in later chapters where dropout can truly work. However, if it cannot work in this demo, it does not seems to be convincing to let people use it. This is the very begining chapter, so I think we can keep it as simple as we could so that people won't feel it's too troublesome to train a model in tensorflow.\n", "@fayeshine - If this was the \"MNIST for Beginners\" tutorial, I'd agree with you, but seeing as this is tutorial is \"for experts\", we should assume that users will have some experience building neural networks in other frameworks or languages, and will likely have used dropout before. Dropout is such common step to take in these sorts of models that taking it out of what is supposed to be a \"moderately sophisticated\" model would be conspicuous. This would lead to users to go looking for dropout in the API and figure out how to implement it anyway.\n\nUsers get the \"keep it simple\" parts of the model from \"MNIST for beginners\" and the first half of \"MNIST for experts\". Not only does removing dropout from the more advanced model do a disservice to those trying to learn the API, but it also gives cause for data scientists to be skeptical of the library.\n\nIf we want to make TensorFlow seem more friendly in terms of training time, our efforts would be better spent making TensorFlow more efficient instead of trying to cut corners when teaching the library.\n", "@samjabrahams \nI understand what you mean, truly dropout is very important and common for ML learners.\nIt seems to be very necessary to teach users how to use dropout.\n\nHowever, the main point is not to reduce the training time, but dropout seems didn't useful or work at this MNIST tutorial. It may not be a good example here and may mislead users to use dropout all time without thinking the proper situation to use dropout.\n\nSo I just wonder maybe it is not suitable to put a non-useful procedure here, but we should put dropout in later chapter of tutorial where it can truly work and in this case we can also teach the intuitive why it work: dropout can reduce the overfitting problem in complicated models with many parameters. In such manner, people can understand why dropout work and where dropout can work. :smiley: \n", "@fayeshine Absolutely- I think a separate tutorial with a targeted example showcasing when and how dropout is useful would be a great resource for those learning about neural network models. I think our disagreement about this particular section stems from our thoughts on what the primary purpose of this tutorial should be. Correct me if I'm wrong, but from what I gather your opinion is that the most important aspect of this tutorial is to teach users how to design and setup a model that A, works and B. is as efficient as possible. I completely respect this, and it is an important skill for anyone who's learning machine learning to get a grasp of. My opinion, in contrast, is that this tutorial's primary goal should be to teach the fundamental concepts and frequently-used components of the API that one would expect to see in a convolutional neural network. In this way, it can serve as an alternative way of learning the API- showing users how different components are used in practice instead of only having the plain documentation.\n\nAgain- I think your suggestion is valid, but the commit as it stands just removes any mention of dropout anywhere in the tutorial. I don't like the idea of completely removing valuable educational material about the API for the sake of teaching users that dropout isn't always necessary.\n\nPerhaps this change could be bundled with the aforementioned separate dropout tutorial :)\n", "@samjabrahams , Yes, you are right, I think I will submit a commit bundled with the separate dropout tutorial or add dropout usage and demo in later tutorial chapter where it can truly work and improve the accuracy.\n", "@fayeshine I agree that it makes more sense to introduce dropout in a context where it actually helps performance. Good catch! Adding it to a later tutorial sounds like a good idea. Another (possibly simpler) approach would be to modify the model architecture in the MNIST for Experts tutorial in such a way that dropout actually provides a benefit.\n\nI don't think we should devote an entire tutorial to dropout. The goal of the tutorials is to teach people how to use TensorFlow rather than how to do ML research. We definitely want our examples to be scientifically correct and relevant, but we want to focus on the code. In the case of using dropout the code consists of the one function tf.nn.dropout(...), so an entire tutorial would be overkill.\n\nFeel free to bounce ideas or an outline of your planned changes off of me!\n", "Closed since you've opened up other PRS\n"]}, {"number": 1619, "title": "a bug in tensorflow/core/graph/gradient.cc", "body": "in the above file, i think that in line 257 there is a error\nfor (const Edge\\* e : y->in_edges()) {\n    if (e->IsControlEdge()) continue;\n    BackpropAlongEdge(y_grad_node_outputs_[i], { e->src(), e->src_output() });\n}\ni think y->in_edges() should be y->out_edges(). please check this out as soon as possible, thank you\n", "comments": ["sorry,  i think just modifing the above is not right . the line from 253 to  262 should be considered carefully to make it right.\n", "if there is one node y has more than one output , then the node would be counted more than one time.\n", "@cuiguoxin, you might have misunderstood the code. The code is initializing the backprop state starting from y(s). So it needs to look at y's inputs. If you think it's wrong,  do you mind constructing a test case to show it?\n", "@zffchen78 ,does y_grad_node_outputs mean the gradient of y_node_outputs? To be clear, think the function y = 3 \\* x. Now i konw the node y, x and dy, and i want to compute the dx. The source code just put dy in x's backpropedGradient and i think it is wrong.\n", "@cuiguoxin Thank you for your comments. I think that some of this confusion is due to this API being in an intermediate state (it currently just supports functions and not general graphs). As commented in the header file, this API is under construction, and will be changing frequently. Currently, the gradients passed in 'y_grad_node_outputs' are passed through to the input of the 'y_nodes', but these nodes are handled in a specialized way in the current implementation (i.e. function arg nodes). In a future version, the 'y_grad_node_outputs' will contain the gradient node outputs corresponding to the 'y_node_outputs', and will be propagated into the 'y_nodes' during backprop init.\n", "@andydavis1 . Thanks. Does it mean that if I construct a graph and I want to auto differentiate the graph from the y to x, I still can't use the api in gradient.cc? Can you show me how to do it in c++ api?\n", "I would not recommend using gradients.h/cc as it is currently under construction and will change frequently (as mentioned in the header file). If you need to auto differentiate your graph, for now, you need to use the python API (see code at tensorflow/python/ops/gradients.py)\n"]}, {"number": 1618, "title": "Implement reduceByKey or groupByKey?", "body": "Can we implement a Implement reduceByKey or groupByKey operation?\n", "comments": ["Could you please elaborate on your usage model and expectations of these operations? Thanks.\n\nSherry\n", "@sherrym for example, if we have reduceByKey operation then we can reduce the number of instance on every truth-prediction pair so that we can implement the confusion matrix in tensorflow api.\n#1606 \nwhat's more, there's many preprocessing procedure before training a model, if the data is big then we need to use spark or whatever to do the preprocessing distributedly. If tensorflow support map/reduce api, then we can just directly do it in tensorflow and don't need other toolchains.\nActually, the tensorflow graph architecture is quite similar to spark's DAG architecture, and I believe we could do similar things as what spark could do.\n", "@fayeshine: Since we have a confusion matrix op as of #1999, is this obsolete?\n", "@girving thanks for adding confusion matrix, however we may still need groupByKey or reduceByKey for data preprocessing\n", "@fayeshine: Note that `confusion_matrix` was implemented on top of existing TensorFlow ops.  Does that mechanism suffices for your use cases?  If not, can you propose a specific semantics of what you want?\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n"]}, {"number": 1617, "title": "Typo in comment, random weights init", "body": "According to the code and tensorflow reference standard deviation would be 0.1\nExcuse me if something is wrong, have little to no experience in python and tensorflow\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Signed the CLA.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 1616, "title": "Fix arguments in docstrings.", "body": "This PR fixes the mismatched argument names in docstring according to the function signatures and detail descriptions.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, please test this!\n", "@dongjoon-hyun Thanks for your commit, can you rebase master and I'll merge?\n\nThanks!\n", "@ilblackdragon: I can merge if you'd like (no testing needed it seems)\n", "Oh, thank you @ilblackdragon and @vrv .\nAnd, sorry for a little bit late response. :)\n"]}, {"number": 1615, "title": "Fix pipeline example.", "body": "This PR simply adds a missing `)` in `iris_with_pipeline.py`.\n", "comments": ["Can one of the admins verify this patch?\n", "Thank you, @vrv .\n"]}, {"number": 1614, "title": "Typo in 2_fullyconnected.ipynb", "body": "", "comments": []}, {"number": 1613, "title": "Upstream changes from internal for March 23", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins: test this please.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1612, "title": "Add matplotlib inline", "body": "@vincentvanhoucke \nSorry for leak comma.\n", "comments": ["Can one of the admins verify this patch?\n", "I've already fixed it.\n"]}, {"number": 1611, "title": "Update 1_notmnist.ipynb", "body": "Fix typo\n", "comments": []}, {"number": 1610, "title": "1_notminst.ipynb not readable by Jupyter - SHA: b885fe", "body": "The latest version seems unreadable. Earlier version works.\n", "comments": ["I just submitted a pull request. You can add a comma on line 58, after the \"from six.moves import cPickle as pickle\\n\"\n", "Already fixed.\n"]}, {"number": 1609, "title": "Update 1_notmnist.ipynb", "body": "added a comma at the end of line 58 so the ipython notebook is a valid JSON and can run\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Already fixed. Thanks!\n"]}, {"number": 1608, "title": "Branch 117961829", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1607, "title": "Android Ndk 11", "body": "Why Android Ndk 11 and other versions are not being supported by bazel android build?\nIt only supports \"android-ndk-r10e\".\n", "comments": ["There are Bazel compatibility issues with r11 currently, so we suggest you stick with r10e for the time being. See https://github.com/tensorflow/tensorflow/issues/1468 for details and links to relevant r10e downloads.\n", "@andrewharp: is this a bug in bazel or a bug in TensorFlow?  If the former, have we filed a bug with them?\n", "Due to the nature of the errors people are seeing (like missing RELEASE.TXT files) I'm assuming Bazel. I'll see if I can reproduce with the latest Bazel and NDK to make sure.\n", "Ok thanks!  @damienmg in case he's curious :)\n", "Yes this is a bazel bug (or rather an incompatibility between current Bazel and NDK r11). @ahumesky is working on it (internal reference is b/27613662 fyi)\n", "@andrewharp why tensorflow couldn't be build by simple ndk-build and gradle ?\n", "@Muaazbinsaeed \nIt would be possible to make Tensorflow build with ndk-build/Gradle, this just isn't something we can maintain right now.\n\nIf you look at https://github.com/miyosuda/TensorFlowAndroidMNIST/find/master, this is an example of linking Tensorflow into a Gradle/ndk-build environment. So you might have luck by compiling tensorflow/core:android_tensorflow_lib into a static .a library, then dropping it into your Gradle project.\n", "Closing, @ahumesky added NDK 11 support in https://github.com/bazelbuild/bazel/commit/abdaff492440b373bacd016d772ef73611a27901\n"]}, {"number": 1606, "title": "Confusion Matrix", "body": "Is there any operation which generates Confusion Matrix in Tensorflow?\nCan't find it on http://stackoverflow.com/questions/tagged/tensorflow\n", "comments": ["You can do it in this way, maybe a little tedious\n\nres = tf.pack([tf.argmax(y,1), tf.argmax(y_,1)])\nans = res.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\nimport numpy as np\nimport pandas as pd\nconfusion = np.zeros([10,10],int)\nfor p in ans.T:\n    confusion[p[0],p[1]]+=1\nprint(pd.DataFrame(confusion))\n", "@fayeshine thanks\nThere could be many ways to implement it using other libraries \nbut that's not the way tensorflow is.\nTensorflow should make this operation too.\n", "Fixed by #1999.\n", "I am hoping to print the confusion matrix in my code\r\nIn the line where it says \r\nTgtc=11\r\nHow do i add something like \r\nTgtc=[2,5,7,8,11] where i want to do a confusion matrix for the 5 variables\r\n\r\n>>>>>>>>>>>>\r\nimport sys\r\n\r\nif sys.version_info.major != 3:\r\n    raise Exception('Please run with python3')\r\n\r\nimport os\r\nimport time\r\nimport gc\r\nimport numpy as np\r\n\r\nimport afib_common\r\n\r\nfrom afib_common import (\r\n    build_model, get_batch, train_test_split, get_batch2)\r\nfrom afib_data import reload_settings\r\n\r\n# Load constants defined in afib_data\r\nreload_settings()\r\nif not os.path.isdir('modelsave'):\r\n    os.path.mkdir('modelsave')\r\nmodel = build_model()\r\n\r\ndef excld(N, i):\r\n    ''' Generates a list of numbers 0..N-1 without the number i'''\r\n    return tuple(_ for _ in range(N) if _ != i)\r\n\r\n# Target class for performance metric\r\ntgtC = 11\r\n# (normal = \"N\", is the 11th item in afib_common.symbols_keep)\r\n\r\ndef calc_confusion_matrix(test_y, test_yhat):\r\n    ''' Generates a confusion matrix from actual classes and predicted classes.\r\n\r\n    Columns are the predicted class, while rows are the actual.\r\n    '''\r\n    N = test_y.shape[1]\r\n    cm = np.zeros((N, N))\r\n    cls_y = test_y.argmax(1)\r\n    cls_yhat = test_yhat.argmax(1)\r\n    for actual, predicted in zip(cls_y, cls_yhat):\r\n        cm[actual, predicted] += 1\r\n    return cm / cm.sum()\r\n\r\ndef estimate_f1_score():\r\n    ''' Estimate the F1 score using many samples.\r\n\r\n    Numbrer of samples is controlled by the Nbatch constant here.\r\n    The F1 score is evaluated for a specific class. (See tgtC variable defined\r\n    earlier in this file.)\r\n\r\n    F1 definition: https://en.wikipedia.org/wiki/F1_score\r\n\r\n    PS: This function uses the class prevalence as returned by `get_batch`.\r\n    (I.e. all classes are sampled with probability 1/NC where NC = number of\r\n    classes)\r\n    '''\r\n    Nbatch = 1000\r\n    test_batch = get_batch(test_records, Nbatch)\r\n    test_yhat = model.predict(test_batch.X)\r\n    true_positive = (\r\n        (test_batch.y.argmax(1) == tgtC) *\r\n        (test_yhat.argmax(1) == tgtC)).sum()\r\n    false_positive = (\r\n        (test_batch.y.argmax(1) != tgtC) *\r\n        (test_yhat.argmax(1) == tgtC)).sum()\r\n    false_negative = (\r\n        (test_batch.y.argmax(1) == tgtC) *\r\n        (test_yhat.argmax(1) != tgtC)).sum()\r\n    # true_negative = (\r\n    #     (test_batch.y.argmax(1) != tgtC) *\r\n    #     (test_yhat.argmax(1) != tgtC)).sum()\r\n    f1 = 2 * true_positive / (\r\n        2 * true_positive + false_positive + false_negative)\r\n    conf_matrix = calc_confusion_matrix(test_batch.y, test_yhat)\r\n    normal_acc = calc_normal_accuracy(conf_matrix)\r\n    return f1, normal_acc, conf_matrix\r\n\r\n# NC = Number of classes\r\nNC = len(afib_common.symbols_keep)\r\n\r\ndef generate_proper_test_train_split():\r\n    '''Prepare a train-test split such that the split is across patients.\r\n\r\n    This prevents that a data leak occurs where the model learns to identify\r\n    patients instead of heartbeat classes.\r\n\r\n    Although, this makes it more difficult to find a split where the amount\r\n    of beats in the training set vs testing set is not close to 80%:20% that\r\n    we would otherwise easily get with a naive split.\r\n\r\n    The approach here is to randomly repeat the split until the beats are\r\n    split close enough to 80:20.\r\n    (For some classes this is impossible, and we settle for anything between\r\n    40% to 95% in the training set)\r\n    '''\r\n    problems = 1\r\n    retries = -1\r\n    while problems != 0:\r\n        retries += 1\r\n        print(\r\n            'Generating properly balanced test-train split. Try#', retries + 1)\r\n        train_records, test_records = train_test_split()\r\n        problems = 0\r\n        prob_fracs = []\r\n        for k in train_records:\r\n            len_train = len(train_records[k])\r\n            len_test = len(test_records[k])\r\n            frac = len_train / (len_train + len_test)\r\n            if frac < 0.4 or frac > 0.95:  # want 0.8 mostly...\r\n                problems += 1\r\n                prob_fracs.append(frac)\r\n            # print(retries, problems, prob_fracs)\r\n    VIEW_SPLIT_STATS = 0\r\n    if VIEW_SPLIT_STATS:\r\n        for k in train_records:\r\n            len_train = len(train_records[k])\r\n            len_test = len(test_records[k])\r\n            frac = len_train / (len_train + len_test)\r\n            print(k, '   ', '%3f' % frac, '   ', len_train + len_test)\r\n    return train_records, test_records\r\n\r\ndef conf_matrix_for_class(conf_matrix, tgtC):\r\n    ''' Get the 2x2 confidence matrix for a specific class given the confidence\r\n        matrix for all the classes.\r\n    '''\r\n    tp = conf_matrix[tgtC, tgtC]\r\n    tn = conf_matrix[np.ix_(excld(NC,tgtC),excld(NC,tgtC))].sum()\r\n    fp = conf_matrix[np.ix_(excld(NC, tgtC),[tgtC])].sum()\r\n    fn = conf_matrix[np.ix_([tgtC], excld(NC, tgtC))].sum()\r\n    return np.array([[tp, fn], [fp, tn]])\r\n\r\ndef adjust_prevalence(conf_matrix_2x2, prevl_new):\r\n    ''' Takes a 2x2 confidence matrix and adjusts the rows so that the\r\n        prevalnce of the positive class becomes `prevl_new`\r\n\r\n        Prevalence = (TP + FN) / (TP + FN + FP + TN)\r\n    '''\r\n    tmp = conf_matrix_2x2\r\n    tmp = tmp / tmp.sum(1, keepdims=1)\r\n    tmp = tmp * [[prevl_new], [1 - prevl_new]]\r\n    tmp = tmp / tmp.sum()\r\n    return tmp\r\n\r\ndef conf_matrix_metrics(conf_mat_2x2):\r\n    ''' Takes a 2x2 confidence matrix and returns the various binary metrics\r\n        associated with it as a dictionary.\r\n\r\n    definitions: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\r\n    '''\r\n    (tp, fn), (fp, tn) = conf_mat_2x2\r\n    acc = tp + tn\r\n    sens = tp / (tp + fn)\r\n    prec = tp / (tp + fp)\r\n    prev = tp + fn\r\n    f1 = 2 * tp / (2 * tp + fn + fp)\r\n    return {'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\r\n            'accuracy': acc, 'sensitivity': sens, 'precision': prec,\r\n            'prevalence': prev, 'f1': f1}\r\n\r\ndef calc_normal_accuracy(conf_matrix):\r\n    ''' Takes a confidence matrix, converts it to the 2x2 specific confidence\r\n        matrix of class `tgtC`(normal) and rescales the prevalence according\r\n        to the prevalence of the normal class in the databases.\r\n        (At the time of writing 96%)\r\n    '''\r\n    normal_prevalence = 0.96  # Prevalence of \"normal\" beats in the datasets\r\n    tmp0 = conf_matrix_for_class(conf_matrix, tgtC)\r\n    tmp1 = adjust_prevalence(tmp0, normal_prevalence)\r\n    return conf_matrix_metrics(tmp1)['accuracy']\r\n\r\ntrain_records, test_records = generate_proper_test_train_split()\r\n\r\nscore_hist = []\r\nscore_hist2f1 = []  # predictive f1\r\nscore_hist2acc = []  # predictive accuracy\r\nIT1 = 20000  # Number of batches to train on before exiting\r\n\r\nclass_acc_hist = []  # Per-class accuracies\r\ndt_hist = []  # timing performance history (for debugging)\r\n\r\nprint('\\nStarting training...')\r\nk = 0\r\nwhile len(score_hist) < IT1:\r\n    t1 = time.time()\r\n    batch = get_batch2(train_records, 16)\r\n    batch_y_soft = batch.y * 0.99 + 0.01 / batch.y.shape[1]  # soften labels\r\n    score = model.train_on_batch(batch.X, batch_y_soft)\r\n    score_hist.append(score[0])\r\n    t2 = time.time()\r\n    dt_hist.append(t2 - t1)\r\n    if len(score_hist) % 50 == 1:\r\n        gc.collect()  # Helps - although it shouldn't :(\r\n        f1_pred, acc_pred, conf_matrix = estimate_f1_score()\r\n        class_acc_hist.append(\r\n            conf_matrix.diagonal() / conf_matrix.sum(1))  # sensitivity\r\n        print('Batches seen:', len(score_hist))\r\n        print('    f1, acc = %r' % ((f1_pred, acc_pred),))\r\n        score_hist2f1.append(f1_pred)\r\n        score_hist2acc.append(acc_pred)\r\n\r\nprint('Saving trained model...')\r\nmodel.save('modelsave/model.data')\r\nprint('Done!')"]}, {"number": 1605, "title": "Fix /dev/urandom-related hanging on some systems", "body": "cat /dev/urandom causes hanging sometimes on ec2 machines. This PR fixes that.\n", "comments": []}, {"number": 1604, "title": "Feature request: Implementing spatially-sparse conv networks in TensorFlow", "body": "I am inspired by Dr. Ben Graham's recent work regarding spatially-sparse convolutional neural networks:\n\nhttp://arxiv.org/abs/1409.6070 (particularly section 2.3)\nhttp://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/graham/sparse3d.pdf\n\nHe has graciously open-sourced his neural network library but I'd like to utilize these same ideas in my models, which are implemented in Tensorflow.\n\nUnfortunately, I'm finding his implementation and citations a little hard to follow, but his description of a \"feature matrix\" and a \"pointer matrix\" sounds a little like something that can be implemented using sparse variable updates in Tensorflow (https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#sparse-variable-updates), but I'm afraid that it might not be so simple and would require building custom kernels to support a new-ish type of convolution. But I don't know enough to say for certain which direction to take.\n\nAny thoughts on how we can bring spatially-sparse convolutions to Tensorflow? Anyone interested in collaborating on implementing this?\n", "comments": ["Yes, this sounds like it requires custom kernels to have reasonable performance, though I don't see the word \"pointer\" in the paper so I can't confirm.\n\nIn terms of collaboration, do you want to post on the discuss@tensorflow list asking if anyone wants to collaborate?  It's a good use of the list, and most people outside the team are unlikely to notice a Github issue.\n\nNote that we are doing some work on better sparse ops (e.g., @ebrevdo's https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc), but I don't think sparse convolutions is on our radar just yet.  It would be a good thing to have, and I'd be happy to answer questions if you do end up working on it. \n", "Thanks, Geoffrey, that is a great idea, I'll go ahead and post to the email thread. I am definitely interested in implementing this since I am currently working with a sparse dataset (and maybe more in the future), although I don't currently have the skill set to implement it on my own. Thanks for the offer, I'll definitely come back with questions as they come up.\n", "hi, \nWe are interested in the sparse tensor for conv2d as well. Is it supported in the latest version yet?\nThanks!\n", "Just wondering, did anything ever come of this? It would be a real boon to the community!", "Not that I know of.  Still contributions welcome. ", "Thanks! My thesis is a bit far ATM from this sort of core algorithm development but it would be fun to get into this as a side project if I have the chance.", "What's the status of this? Anyone working on it already?\r\n", "+1.  I am also interested in this!   Would be open to volunteering to help write it if there is an initiative going on already?", "There is now implementation in torch/ pytorch: https://github.com/facebookresearch/SparseConvNet. ", "I'm trying to implementing the main idea of SparseCNN described in https://arxiv.org/pdf/1505.02890.pdf (page5) and encountered some problems. Q and H of each layer are different. In order to get the value of Q and H_out, I have to use sess.run() and it may cut the connection of gradient variables. \r\n\r\nI wonder if there are some bottlenecks in implementing SparseCNN using tensorflow API.", "We'd have to see more of your code, or better yet a design proposal for how\nyou want to implement the algorithm in tf.  Your options for avoiding\nmultiple session run calls include generating q and h in tf directly, out\nusing py_func.\n\nOn Thu, Oct 26, 2017, 8:21 PM mushroom1116 <notifications@github.com> wrote:\n\n> I'm trying to implementing the main idea of SparseCNN described in\n> https://arxiv.org/pdf/1505.02890.pdf (page5) and encountered some\n> problems. Q and H of each layer are different. In order to get the value of\n> Q and H_out, I have to use sess.run() and it may cut the connection of\n> gradient variables.\n>\n> I wonder if there are some bottlenecks in implementing SparseCNN using\n> tensorflow API.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1604#issuecomment-339861849>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3LpCos3nSaXMt82t9qhMkRC4MMBks5swUwrgaJpZM4H3caN>\n> .\n>\n", "@ebrevdo  Thank you very much for your reply, and how to generate Q and H in tf directly? \r\n\r\nI want to design a CNN Net with two layers and there are some problems in back propagation. When I got M_out1(M_out1 = (Q1*W)+B) as the input for the second layer, I have to iterate H_out1 of the first layer to determine the H_out2 and Q2 of the second layer.  \r\n\r\nNow I access the active site's coordinate one bye one(for i in range(a_out1), a_out1 must be an int and need sess.run()) stored in H_out1 combined with filter size to infer the active site's coordinate in H_out2. \r\n\r\nIt seems not a linear reversible transformation and compute Q from M_in need coordinate of active sites and a_in(number of active sites) who need to be run out by session, and it can't back propagate. How to solve this problem? Thank you very much for your time.", "Can the custom ops from this approach be brought in under contrib to help address this issue?\r\nhttps://github.com/uber/sbnet", "I implemented the sparse CNN (forward prop only) in python as a learning exercise, computing Q and H with py_func. Comments welcome: https://github.com/IdRatherBeCoding/sparse_cnn", "@IdRatherBeCoding You just implemented the forward propagate, right?", "@mushroom1116 Actually, I recently started work on the gradient, and I'm almost done.\r\n\r\nDo you need an efficient sparse CNN in TF for a project, or are you just curious?\r\n\r\nTo be clear, both my forward and back prop implementations are going to be slower than TF conv2d because I'm computing Q with simple python loops. I intend to write the Op in C++, but it's still just for a learning exercise. Support for maxpooling and strides > 1 will also be needed before it's useful.\r\n", "@IdRatherBeCoding I tried to implement SparseCNN too and I was stopped by some difficulties in BP, so I want to know how  you did it. If may. Could you please let me know the details of code? ", "@mushroom1116 You can take a look at it now:\r\nhttps://github.com/IdRatherBeCoding/sparse_cnn/blob/master/sparse_cnn_back_prop.ipynb\r\n\r\nPlease let me know if something doesn't make sense.\r\n\r\nI have tested the derivatives of Q with respect to M and ground state. I have not yet tested with an end to end CNN training.", "@IdRatherBeCoding Hi, any update on your work ? I can close this issue if your contribution would suffice the request. ", "No such thing as enough or fun or not about it. cpu, Can say any no matter what.", "Closing this issue for now. Feel free to reopen and share the PR number if any contributions are made.", "@harshini-gadige no I didn't pursue this further. As mentioned by @ghexp the sbnets (https://github.com/uber/sbnet) provides similar solution, with better speedups, according to the paper."]}, {"number": 1603, "title": "Benchmark tests", "body": "To run the tests, do (for example):\nexport TF_BUILD_CONTAINER_TYPE=CPU\nexport TF_BUILD_PYTHON_VERSION=PYTHON2\nexport TF_BUILD_IS_OPT=OPT\nexport TF_BUILD_IS_PIP=NO_PIP\nexport TF_BUILD_RUN_BENCHMARKS=1\ntensorflow/tools/ci_build/ci_parameterized_build.sh\n\nUse the environment variable TF_BUILD_BENCHMARK_HOOK to specify the path\nto a binary/script that handles the benchmark test log, to, e.g., enter\nthe benchmark information into a database.\n", "comments": ["@tensorflow-jenkins test this please\n", "@vrv Does this look good to you? I tested the changes internally (with proper sync-repo mappings) and the tests pass. If so, please merge it. \n", "For internal mapping see CL 117950736\n"]}, {"number": 1602, "title": "Merge pull request #1 from tensorflow/master", "body": "The new update of tensorflow on 14.03.2016 from tensorflow/tensorflow\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 1601, "title": "fix: add selective_registration.h header to framework file group", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@awni Thanks for the fix. As it happens, I have the same fix in #1608 that is farther along in the tests. Do you mind if I drop this one? #1608 has other commits and extricating changes to the one file would be painful.\n", "@keveman No problem!\n"]}]