[{"number": 39649, "title": "tf.math.reduce_mean takes too long and produces wrong result when input_tensor is uint32/64 and axis is array", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04, macOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):v2.1.0-rc2-17-ge5bf8de410 2.1.0 & v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:3.7.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n`tf.math.reduce_mean` hangs(?) or takes forever to compute for certain input (with dtype uint64 and uint32, and axis an array). Around when the slow down occurs, the function produces incorrect result.\r\n\r\nThis function affects other functions' performance: `tf.math.reduce_std` which calls `tf.math.reduce_variance` which calls `tf.math.reduce_mean`\r\n**Describe the expected behavior**\r\nIt should not take forever to compute nor produce an incorrect result.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAn example that showed incorrect result:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\ninput_tensor = np.arange(4).astype('uint64')  # also occurs for 'uint32'\r\n\r\nfor i in range(20):\r\n  axis = [0] * i\r\n  print('axis = ', axis)\r\n  start = time.time()\r\n  res = tf.math.reduce_mean(input_tensor, axis=axis)\r\n  end = time.time()\r\n  time_diff = end - start\r\n  print('result = ', res.numpy())\r\n  print('-- took %d sec --' % time_diff)\r\n```\r\nAn example that both became extremely slow and produced an incorrect result:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\n# 15 is a magic number chosen for illustration\r\n# a smaller number may not cause the slow-down but could still show the wrong result\r\ninput_tensor = np.arange(15).astype('uint64')  # also occurs for 'uint32'\r\n\r\nfor i in range(20):\r\n  # produces incorrect result when i == 8.\r\n  # slow down occurs around when i == 10\r\n  axis = [0] * i\r\n  print('axis = ', axis)\r\n  start = time.time()\r\n  res = tf.math.reduce_mean(input_tensor, axis=axis)\r\n  end = time.time()\r\n  time_diff = end - start\r\n  print('result = ', res.numpy())\r\n  print('-- took %d sec --' % time_diff)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.1.0 ,2.2.0 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/5261bff929134c88bd17f6fd57e05c39/untitled904.ipynb).Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39649\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39649\">No</a>\n"]}, {"number": 39648, "title": "no such package '@androidsdk//com.android.support'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (MacOS 10.13.6):\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): Apple Clang 10.0.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello\uff0c dear tensorflow guys.\r\n\r\nI want to build the official android demo from TF team.\r\n\r\nMy command is:\r\n\r\n`bazel build -c opt --cxxopt='--std=c++11'   //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo --experimental_repo_remote_exec\r\n`\r\n\r\nI set both Android SDK and NDK enviroment.\r\n\r\n```\r\nexport ANDROID_SDK_API_LEVEL=26\r\nexport ANDROID_NDK_API_LEVEL=21\r\n```\r\n\r\n\r\nHowever, it has the error:\r\n\r\n> ERROR: /Users/junyan/find_android_project/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:8:1: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'\r\n> ERROR: /Users/junyan/find_android_project/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:8:1: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'\r\n> ERROR: Analysis of target '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo' failed; build aborted: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package\r\n\r\n\r\nThe build script is:\r\n\r\n```\r\nload(\"@build_bazel_rules_android//android:rules.bzl\", \"android_binary\")\r\n\r\npackage(\r\n    default_visibility = [\"//visibility:private\"],\r\n    licenses = [\"notice\"],  # Apache 2.0\r\n)\r\n\r\nandroid_binary(\r\n    name = \"TfLiteCameraDemo\",\r\n    srcs = glob([\"java/**/*.java\"]),\r\n    assets = [\r\n        \"//tensorflow/lite/java/demo/app/src/main/assets:labels_mobilenet_quant_v1_224.txt\",\r\n        \"@tflite_mobilenet_quant//:mobilenet_v1_1.0_224_quant.tflite\",\r\n        \"@tflite_mobilenet_float//:mobilenet_v1_1.0_224.tflite\",\r\n    ],\r\n    assets_dir = \"\",\r\n    custom_package = \"com.example.android.tflitecamerademo\",\r\n    manifest = \"AndroidManifest.xml\",\r\n    nocompress_extensions = [\r\n        \".tflite\",\r\n    ],\r\n    resource_files = glob([\"res/**\"]),\r\n    # In some platforms we don't have an Android SDK/NDK and this target\r\n    # can't be built. We need to prevent the build system from trying to\r\n    # use the target in that case.\r\n    tags = [\"manual\"],\r\n    deps = [\r\n        \"//tensorflow/lite/java:tensorflowlite\",\r\n        \"//tensorflow/lite/java:tensorflowlite_gpu\",\r\n        \"//tensorflow/lite/java/src/testhelper/java/org/tensorflow/lite:testhelper\",\r\n        \"@androidsdk//com.android.support:support-v13-25.2.0\",\r\n        \"@androidsdk//com.android.support:support-v4-25.2.0\",\r\n    ],\r\n```\r\n\r\n\r\nShould I download com.android.support:support-v13-25.2.0 aar and com.android.support:support-v4-25.2.0 under the directory of android-sdk to compile it successfully?\r\n\r\nThanks & Regards!", "comments": ["Is there any solution to fix it?", "How can I do some configuration to correlate the android-support widget?\r\n\r\nThanks & Regards!\r\nMomo", "Did you run \"./configure\" script and enabled Android support?\r\nYour .tf_configure.bazelrc file should have a valid Android SDK / NDK configurations.", "Hello, @terryheo .\r\n\r\nI run the configuration script, however, still have the problem.\r\n\r\nTo be frank, I cannot decide which directory of file path is correct for \"com.android.support\" corresponding directory under the Android-SDK-Home library.\r\n\r\nHere is the presentation of Android-SDK home directory.\r\n\r\n \r\n![image](https://user-images.githubusercontent.com/34225874/82296148-53c0ed80-99e3-11ea-9872-383a673b4c10.png)\r\n\r\nWhich path should I refer to? If I cannot decide which local path to follow, is there a workaround to correlate an on-line android support library?\r\n\r\nThanks & Regards!\r\nMomo\r\n", "Bazel will find the library if you provide a valid SDK home while running the configuration script.\r\n\r\nI'm wonder if you installed Android SDK 26 support.\r\nCould you share the content of your .tf_configure.bazelrc?\r\nCan you verify if you have Android SDK 26 with SDK Manager tool?", "@momo1986  Could you please let us know if you have tried  as per the  [above](https://github.com/tensorflow/tensorflow/issues/39648#issuecomment-635059518)  comment and is it still an issue with the latest stable TF version 2.6.0 ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39648\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39648\">No</a>\n"]}, {"number": 39647, "title": "FailedPreconditionError:  Error while reading resource variable _AnonymousVar878 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar878/N10tensorflow3VarE does not exist. \t [[node mul_584/ReadVariableOp (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_34197]", "body": "I'm getting this error and I'm not able to debug it. This is the complete code of CGAN.\r\n\r\n    from __future__ import print_function, division\r\n    from keras.datasets import mnist\r\n    from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\r\n    from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\r\n    from keras.layers.advanced_activations import LeakyReLU\r\n    from keras.layers.convolutional import UpSampling2D, Conv2D,Conv2DTranspose\r\n    from keras.models import Sequential, Model\r\n    from keras.optimizers import Adam\r\n    from keras.layers import concatenate\r\n    from tensorflow.keras.utils import to_categorical\r\n    import matplotlib.pyplot as plt\r\n    import numpy as np\r\n\r\n    class CGAN():\r\n        def __init__(self):\r\n            # Input shape\r\n            self.img_rows = 28\r\n            self.img_cols = 28\r\n            self.channels = 1\r\n            self.img_shape = (self.img_rows, self.img_cols, self.channels)\r\n            self.num_classes = 10\r\n            self.latent_dim = 100\r\n\r\n            optimizer = Adam(0.0002, 0.5)\r\n\r\n        # Build and compile the discriminator\r\n            self.discriminator = self.build_discriminator()\r\n            self.discriminator.compile(loss=['binary_crossentropy'],\r\n                optimizer=optimizer,\r\n                metrics=['accuracy'])\r\n\r\n        # Build the generator\r\n            self.generator = self.build_generator()\r\n\r\n        # The generator takes noise and the target label as input\r\n        # and generates the corresponding digit of that label\r\n            noise = Input(shape=(self.latent_dim,))\r\n            label = Input(shape=(10,))\r\n            img = self.generator([noise, label])\r\n\r\n        # For the combined model we will only train the generator\r\n            self.discriminator.trainable = False\r\n\r\n        # The discriminator takes generated image as input and determines validity\r\n        # and the label of that image\r\n            valid = self.discriminator([img, label])\r\n\r\n        # The combined model  (stacked generator and discriminator)\r\n        # Trains generator to fool discriminator\r\n            self.combined = Model([noise, label], valid)\r\n            self.combined.compile(loss=['binary_crossentropy'],\r\n            optimizer=optimizer)\r\n\r\n        def build_generator(self):\r\n          image_resize = self.img_rows // 4\r\n      # network parameters\r\n          kernel_size = 5\r\n          layer_filters = [128, 64, 32, 1]\r\n\r\n          inputs = Input(shape=(self.latent_dim,))\r\n          x = inputs\r\n          labels = Input(shape=(10,))\r\n\r\n          x  = concatenate([inputs, labels], axis=1)\r\n          x = Dense(image_resize * image_resize * layer_filters[0])(x)\r\n          x = Reshape((image_resize, image_resize, layer_filters[0]))(x)\r\n\r\n          for filters in layer_filters:\r\n        # first two convolution layers use strides = 2\r\n        # the last two use strides = 1\r\n              if filters > layer_filters[-2]:\r\n                  strides = 2\r\n              else:\r\n                  strides = 1\r\n              x = BatchNormalization()(x)\r\n              x = Activation('relu')(x)\r\n              x = Conv2DTranspose(filters=filters,\r\n                                  kernel_size=kernel_size,\r\n                                 strides=strides,\r\n                                 padding='same')(x)\r\n\r\n          x = Activation('sigmoid')(x)\r\n     \r\n          return Model([inputs, labels], x, name='generator')\r\n\r\n\r\n        def build_discriminator(self):\r\n          kernel_size = 5\r\n         layer_filters = [32, 64, 128, 256]\r\n\r\n          inputs = Input(shape=(self.img_rows,self.img_rows,1))\r\n          x = inputs\r\n          labels = Input(shape=(10,))\r\n          y = Dense(self.img_rows * self.img_rows)(labels)\r\n          y = Reshape((self.img_rows, self.img_rows, 1))(y)\r\n          x = concatenate([x, y])\r\n\r\n          for filters in layer_filters:\r\n        # first 3 convolution layers use strides = 2\r\n        # last one uses strides = 1\r\n            if filters == layer_filters[-1]:\r\n              strides = 1\r\n            else:\r\n              strides = 2\r\n            x = LeakyReLU(alpha=0.2)(x)\r\n            x = Conv2D(filters=filters,\r\n                    kernel_size=kernel_size,\r\n                    strides=strides,\r\n                    padding='same')(x)\r\n\r\n          x = Flatten()(x)\r\n          x = Dense(1)(x)\r\n          x = Activation('sigmoid')(x)\r\n     \r\n          return Model([inputs, labels], x, name='discriminator')\r\n\r\n\r\n        def train(self, epochs, batch_size=128, sample_interval=50):\r\n\r\n        # Load the dataset\r\n            (X_train, y_train), (_, _) = mnist.load_data()\r\n\r\n        # Configure input\r\n            X_train = (X_train.astype(np.float32) - 127.5) / 127.5\r\n            X_train = np.expand_dims(X_train, axis=3)\r\n            #y_train = to_categorical(y_train)\r\n        \r\n\r\n        # Adversarial ground truths\r\n            valid = np.ones((batch_size, 1))\r\n            fake = np.zeros((batch_size, 1))\r\n\r\n            for epoch in range(epochs):\r\n\r\n            # ---------------------\r\n            #  Train Discriminator\r\n            # ---------------------\r\n\r\n            # Select a random half batch of images\r\n                idx = np.random.randint(0, X_train.shape[0], batch_size)\r\n                imgs, labels = X_train[idx], y_train[idx]\r\n                labels = to_categorical(labels)\r\n            # Sample noise as generator input\r\n                noise = np.random.normal(0, 1, (batch_size, 100))\r\n\r\n            # Generate a half batch of new images\r\n                gen_imgs = self.generator.predict([noise, labels])\r\n\r\n            # Train the discriminator\r\n                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\r\n                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\r\n                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n\r\n            # ---------------------\r\n            #  Train Generator\r\n            # ---------------------\r\n\r\n            # Condition on labels\r\n                sampled_labels = to_categorical(np.random.randint(0, 10, batch_size).reshape(-1, 1))\r\n\r\n            # Train the generator\r\n                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\r\n\r\n            # Plot the progress\r\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\r\n\r\n            # If at save interval => save generated image samples\r\n            #if epoch % sample_interval == 0:\r\n             #   self.sample_images(epoch)\r\n\r\n    ''' def sample_images(self, epoch):\r\n        r, c = 2, 5\r\n        noise = np.random.normal(0, 1, (r * c, 100))\r\n        sampled_labels = to_categorical(np.arange(0, 10).reshape(-1, 1))\r\n\r\n        gen_imgs = self.generator.predict([noise, sampled_labels])\r\n\r\n        # Rescale images 0 - 1\r\n        gen_imgs = 0.5 * gen_imgs + 0.5\r\n\r\n        fig, axs = plt.subplots(r, c)\r\n        cnt = 0\r\n        for i in range(r):\r\n            for j in range(c):\r\n                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\r\n                axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\r\n                axs[i,j].axis('off')\r\n                cnt += 1\r\n        fig.savefig(\"images/%d.png\" % epoch)\r\n        plt.close() '''\r\n\r\n\r\n    if __name__ == '__main__':\r\n        cgan = CGAN()\r\n        cgan.train(epochs=20000, batch_size=32, sample_interval=200)\r\n\r\n", "comments": ["@deshiyan1010 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39647\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39647\">No</a>\n", "any solution ??"]}, {"number": 39646, "title": "Update README", "body": "Added some excellent updated resources to learn tensorflow", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39646) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39646) for more info**.\n\n<!-- ok -->"]}, {"number": 39645, "title": "How to implement data-dependent initialization with MultiGPU/TPU [Tensorflow >= 2.x]", "body": "# Environment\r\n- tensorflow == 2.2.0 \r\n\r\n# Reprodusable code\r\nhttps://colab.research.google.com/gist/MokkeMeguru/fc7bc3df0e77a3a5ba2dbf16a806e6c7/data-dep-iinitialization.ipynb\r\n\r\n# Problem\r\nI want to weight initialization by first-batch data. \r\nSo I need  method below condition:\r\n- Aggregate data on Multi GPU / TPU\r\n- Weight which will be initialized is trainable\r\n\r\nI wrote these code\r\n\r\n```python\r\nclass DataDepInit(tf.keras.layers.Layer):\r\n  def __init__(self):\r\n    super().__init__()\r\n\r\n  def build(self, input_shape):\r\n    # weight initialized by first batch\r\n    self.w = self.add_weight(\r\n        name=\"mean\",\r\n        shape=(1, 1, 1, 1),\r\n        dtype=tf.float32,\r\n        trainable=True,\r\n        aggregation=tf.VariableAggregation.MEAN\r\n    )\r\n    # the controller about initialization\r\n    self.initialized = self.add_weight(\r\n        name=\"init\",\r\n        trainable=False,\r\n        dtype=tf.bool,\r\n    )\r\n\r\n    self.initialized.assign(False)\r\n    self.built = True\r\n  \r\n  def initialize(self, x):\r\n    mean = tf.reduce_mean(x, axis=[0, 1, 2], keepdims=True)\r\n    tf.print(\"initialize\")\r\n    self.w.assign(mean)\r\n\r\n  def call(self, x):\r\n    if not self.initialized:\r\n      self.initialize(x)\r\n      self.initialized.assign(True)\r\n    return x - self.w\r\n\r\n# ---------------------------------------------------------\r\nwith strategy.scope():\r\n  x = tf.keras.Input(shape=(32, 32, 1))\r\n  ddi = DataDepInit()\r\n  model = tf.keras.Model(x, ddi(x))\r\n  model.summary()\r\n  \r\n  def _step():\r\n    model(tf.random.normal(shape=[128, 32, 32 , 1]))\r\n\r\n  @tf.function\r\n  def distributed_step():\r\n    strategy.experimental_run_v2(_step, args=())\r\n\r\n  for i in range(10):\r\n    distributed_step()\r\n```\r\nBut it causes error.\r\n\r\n```\r\n    <ipython-input-100-6d7fd2d86109>:15 _step  *\r\n        model(tf.random.normal(shape=[128, 32, 32 , 1]))\r\n    <ipython-input-79-fb0889bce767>:31 call  *\r\n        self.initialize(x)\r\n    <ipython-input-79-fb0889bce767>:27 initialize  *\r\n        self.w.assign(mean)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py:798 assign  **\r\n        return self._mirrored_update(assign_fn, *args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py:786 _mirrored_update\r\n        merge_fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:1071 _merge_call\r\n        \"`merge_call` called while defining a new graph or a tf.function. \"\r\n\r\n    RuntimeError: `merge_call` called while defining a new graph or a tf.function. This can often happen if the function `fn` passed to `strategy.experimental_run()` is decorated with `@tf.function` (or contains a nested `@tf.function`), and `fn` contains a synchronization point, such as aggregating gradients. This behavior is not yet supported. Instead, please wrap the entire call `strategy.experimental_run(fn)` in a `@tf.function`, and avoid nested `tf.function`s that may potentially cross a synchronization boundary.\r\n```\r\n\r\nThis solution may lead us to construct many data-dep-initialization layer is needed from weight normalization, Actnorm, etc. \r\n\r\nThansk! ", "comments": ["I've run into the same issue with a similar operation (assign distributed variable in call) (tensorflow 2.2.0 and 2.3.0). What's really strange is that I can run the model without any problems even though the same assign operator runs, it only fails when training. Unfortunately I cannot provide the code, and the reduced sample works like charm.\r\nDid you find the reason why this code breaks?\r\nAlso I couldn't find documentation on what may trigger synchronization to eliminate problems with code.\r\n\r\nThe sample that worked on my system with 2 Titan RTX gpus:\r\n```python\r\nimport tensorflow as tf\r\n# do not preallocate gpu memory\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n\ttf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nimport numpy as np\r\nimport os\r\n\r\nclass MovingNorm(tf.keras.layers.Layer):\r\n\tdef __init__(self, mean: float = 0.0, var: float = 1.0):\r\n\t\tsuper(MovingNorm, self).__init__()\r\n\t\tself.mean = mean\r\n\t\tself.var = var\r\n\t\r\n\tdef build(self, input_shape):\r\n\t\tshape=(1, 1, 1, input_shape[3])\r\n\t\tself.moving_mean = self.add_weight('mean', shape, initializer=tf.keras.initializers.Constant(0.0), trainable=False, aggregation=tf.VariableAggregation.MEAN)\r\n\t\tself.moving_var = self.add_weight('var', shape, initializer=tf.keras.initializers.Constant(1.0), trainable=False, aggregation=tf.VariableAggregation.MEAN)\r\n\t\tself.target_mean = self.add_weight('target_mean', shape, initializer=tf.keras.initializers.Constant(self.mean))\r\n\t\tself.target_var = self.add_weight('target_var', shape, initializer=tf.keras.initializers.Constant(self.var))\r\n\t\r\n\tdef call(self, x, decay: float):\r\n\t\ttf.print('decay: ', decay)\r\n\t\tif decay<1.0:\r\n\t\t\tmean, var = tf.nn.moments(x, [0, 1, 2], keepdims=True)\r\n\t\t\tvar=tf.maximum(var, 1e-3)\r\n\t\t\tself.moving_mean.assign(decay*self.moving_mean + (1-decay)*mean)\r\n\t\t\tself.moving_var.assign(decay*self.moving_var + (1-decay)*(var**0.5))\r\n\t\t# output\r\n\t\tmult = self.target_var / self.moving_var\r\n\t\tx_norm = mult*x + (self.target_mean - self.moving_mean*mult)\r\n\t\treturn x_norm\r\n\r\ndef train_batch(data):\r\n\twith tf.GradientTape() as tape:\r\n\t\tloss = tf.reduce_sum(net(data, 0.9)**2)\r\n\tx=net.trainable_variables\r\n\tgradients = tape.gradient(loss, x)\r\n\toptimizer.apply_gradients(zip(gradients, x))\r\n\treturn loss\r\n\r\n@tf.function\r\ndef dist_train(data):\r\n\tdistributed_strategy.run(train_batch, (data,))\r\n\r\n#= no NCCL on windows :(\r\nif os.name=='nt':\r\n\tdistributed_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())\r\nelse:\r\n\tdistributed_strategy = tf.distribute.MirroredStrategy()\r\n\r\nwith distributed_strategy.scope():\r\n\tnet=MovingNorm()\r\n\t# need to run the network so that variables are created within distributed scope\r\n\tout = net(tf.random.normal((4,2,2,3)), 0.9)\r\n\r\nwith distributed_strategy.scope():\r\n\toptimizer = tf.keras.optimizers.Adam(1e-3, 0.9, 0.999, 1e-7, amsgrad=False)\r\n\r\nds = tf.data.Dataset.from_tensor_slices([tf.random.normal((4,2,2,3)) for _ in range(64)])\r\ndist_ds = distributed_strategy.experimental_distribute_dataset(ds)\r\n\r\nfor data in dist_ds:\r\n\tdist_train(data)\r\n```", "@MokkeMeguru,\r\nCan you please confirm if above workaround worked for you. Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39645\">No</a>\n", "@akkiss \r\nhello, i readed your code. (however, i'm not tf developer team, ) I think the problem will cause some problems.\r\n\r\n- tf.nn.moments does not deal with multi-gpu (I think your result is the mean of single-gpus' results)\r\n   see . https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization_v2.py#L145-L188 (SyncBatchNorm Implementation, they don't use tf.nn.moments.)", "@rmothukuru \r\nwhere is \"above workaround\"?", "> @akkiss\r\n> hello, i readed your code. (however, i'm not tf developer team, ) I think the problem will cause some problems.\r\n> \r\n> * tf.nn.moments does not deal with multi-gpu (I think your result is the mean of single-gpus' results)\r\n>   see . https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/normalization_v2.py#L145-L188 (SyncBatchNorm Implementation, they don't use tf.nn.moments.)\r\n\r\nI'm not in tf developer team either, just ran into a similar issue. Strangely when I rewrote my model in tf.keras, the problem just vanished - since it's not what your code does it's not meant to be a workaround. As for tf.nn.moments, of course averaging over gpu instances is not mathematically correct, but does the job.", "Is this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39645\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39645\">No</a>\n"]}, {"number": 39644, "title": "how to build tflite python for ios platform?", "body": "how to build tflite python for ios platform. \r\nI want to run tflite on my app python interpreter.", "comments": ["@JunyiXie,\r\nTo convert your TensorFlow model to TF Lite please check [this guide](https://www.tensorflow.org/lite/convert/python_api#python_api). \r\n\r\nAlso, please take a look at [iOS quickstart](https://www.tensorflow.org/lite/guide/ios) and [Build TensorFlow Lite for iOS](https://www.tensorflow.org/lite/guide/build_ios) guide and let us know if it helps. Thanks! ", "https://www.tensorflow.org/lite/guide/python\r\nI hope tensorflow-lite can provide python api tensorlow-lite library for iOS platform.\r\n<img width=\"885\" alt=\"\u622a\u5c4f2020-05-19\u4e0a\u534812 17 14\" src=\"https://user-images.githubusercontent.com/22233256/82236032-19653b00-9966-11ea-91f4-c810ea08585c.png\">\r\n", "@amahendrakar ", "I don't think we support TF Lite python api for iOS platform yet. Perhaps this can be a feature request.", "We don't have any plans to officially release a tflite python library specifically for iOS. Can you explain more details about your use case? Also, what have you already tried so far?", "I hope that I can write code in Python and execute by Python interpreter on iOS.", "how to build https://github.com/tensorflow/tensorflow/tree/v2.2.0/tensorflow/lite/python? I try to build the iOS version. \r\nI need tensorflow lite python version build script on mac os platform as a reference.@yyoon", "You can build the standalone TFLite pip package following this guide:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package", "Closing this issue for now. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39644\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39644\">No</a>\n"]}, {"number": 39643, "title": "predict on batch and predict have different behavior Assert and control depedencies", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI have a custom keras layer with an assertion and a control depedencies: \r\n\r\n```\r\nclass CustomLayer(Layer):\r\n  def call(self, inputs):\r\n    assert_same_batch_size = tf.assert_equal(tf.shape(inputs[0])[0],\r\n                                                 tf.shape(inputs[1])[0], message=\"inputs do not have equal batch_size\")\r\n    with tf.control_dependencies([assert_same_batch_size]):\r\n      return tf.reshape(inputs[0], (tf.shape(inputs[0])[0], 3))   # i want it to break\r\n\r\na = Input(shape=1)\r\nb = Input(shape=1)\r\nout = CustomLayer()([a, b])\r\nm = Model([a, b], out)\r\n```\r\n\r\n> `m.predict_on_batch([tf.constant(np.ones((10, 1))), tf.constant(np.ones((15, 1)))])`\r\n```\r\nValueError: Cannot reshape a tensor with 10 elements to shape [10,3] (30 elements) for '{{node model_2/custom_layer_8/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_2/Cast, model_2/custom_layer_8/Reshape/shape)' with input shapes: [10,1], [2] and with input tensors computed as partial shapes: input[1] = [10,3].\r\n```\r\n\r\n````\r\ndef generator(): # we use a generator to trick tf dataset into letting us use different nbr of rows in each vector\r\n#(otherwise caught by tf.data.dataset.from_tensor_slices)\r\n  for a in range(1):\r\n    yield [tf.constant(np.ones((10, 1))), tf.constant(np.ones((15, 1)))]\r\nm.predict(generator())\r\n````\r\n\r\n````\r\nInvalidArgumentError:  assertion failed: [inputs do not have equal batch_size] [.....\r\n````\r\n**Describe the expected behavior**\r\nThe assertion error should be raised in both scenario with predict_on_batch and with predict. We dont know if its because control depencies is not used with predict_on_batch or if it comes from the assert. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/d36d2a1175cec79143d0a3e9dddfbbad/untitled14.ipynb\r\n\r\n", "comments": ["@tanguycdls \r\nCan you please refer to these links for the value error faced in your gist:[\r\nlink](https://stackoverflow.com/questions/51746179/valueerror-cannot-reshape-a-tensor-with-99872-elements-to-shape-1-125-1-8) [link1](https://www.dotnetperls.com/reshape-tensorflow) and for the invalid argument please refer to: [link](https://stackoverflow.com/questions/58609967/tensorflow-2-0-condition-x-y-did-not-hold-element-wise) [link1](https://github.com/tensorflow/models/issues/2737) [link2](https://github.com/tensorflow/tensorflow/issues/32333), let us know if this helps.", "@Saduf2019 thanks for your message ! \r\n\r\nmy point is not the actual errors (that i produced on purpose): its that for the same input predict yields to an error the assert_same_batch_size error and predict_on_batch breaks later in `ValueError: Cannot reshape a tensor with 10`.\r\nI think its odd that predict and predict_on_batch does not yield to the same error being caught for the same input.", "I am able to replicate the issue reported please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/4abf0d96e9e3d11b30e74dcc98a959cc/untitled189.ipynb)", "@tanguycdls I think there is some typo or extra dimension as input to the `assert_equal`.\r\n\r\nI changed one line as follows and the two errors are similar and shows `InvalidArgumentError` related to mismatch in batch_size. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/62a964d8cba44476778bfa17bde76459/untitled189.ipynb#scrollTo=b5zRT4LoOXzK). Thanks!\r\n\r\n```\r\nassert_same_batch_size = tf.assert_equal(tf.shape(inputs[0]),\r\n                                                 tf.shape(inputs[1]), message=\"inputs do not have equal batch_size\")\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "Hi @jvishnuvardhan Thanks for the response !\r\nI checked and yes it works but actually we only want to check the first shape dim not the second one ! Maybe it's the get_item that breaks it ? maybe i can try a tf.gather instead of [0]? I will try and get back to you. ", "@tanguycdls Is this still an issue for you? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39643\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39643\">No</a>\n"]}, {"number": 39642, "title": "README.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39642) for more info**.\n\n<!-- need_sender_cla -->", "@rahulbordoloi  Can you please sign CLA? Thanks!", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39642) for more info**.\n\n<!-- ok -->", "> @rahulbordoloi Can you please sign CLA? Thanks!\r\n\r\nThank You, That's my 1st Time. Anything I need to do more?\r\n"]}, {"number": 39641, "title": "Inferring using a frozen graph requires more memory", "body": "**System information**\r\n- Have I written custom code: **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.4 LTS**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **v1.14.0-rc1-22-gaf24dc91b5 1.14.0**\r\n- Python version: **3.6.9**\r\n- GPU model and memory: Not used\r\n\r\n**Standalone code to reproduce the issue**\r\nTo freeze graph:\r\n```\r\ngraph = tf.get_default_graph()\r\ninput_graph_def = graph.as_graph_def()\r\noutput_graph_def = tf.graph_util.convert_variables_to_constants(sess, input_graph_def, ['output/predictions'])\r\nf.write(output_graph_def.SerializeToString())\r\n```\r\nTo load graph:\r\n```\r\nwith tf.gfile.GFile(graph_filepath, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\ntf.import_graph_def(graph_def, self.placeholders)\r\nscores = graph.get_tensor_by_name('import/output/scores:0')\r\nself.normalized_scores = tf.nn.softmax(scores)\r\nself.predictions = graph.get_tensor_by_name('import/output/predictions:0')\r\nsess = tf.Session()\r\nsess.graph.finalize()\r\n```\r\nTo load checkpoint, I rebuild the graph from scratch, then use:\r\n```\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsaver = tf.train.Saver()\r\nckpt = tf.train.get_checkpoint_state(ckpt_dir)\r\nsess.graph.finalize()\r\n```\r\n\r\n**Describe the current behavior**\r\nWhen using a checkpoint, a call to `sess.run(tf.global_variables_initializer())` takes up 260 MB (fair, considering raw weights are around that size), and then the process plateaus. \r\n\r\nWhen using a graph however:\r\n* `graph_def.ParseFromString()` requires 255 MB, \r\n* `tf.import_graph_def()` uses an additional 264 MB, \r\n* and the first call to `sess.run` takes up 270 MB more. \r\n\r\nI can retrieve 255 MB by calling `del graph`, but `del graph_def` does nothing and the memory used by the graph_def is never released.\r\n\r\n**Describe the expected behavior**\r\nI should be able to free the 500+ MB used by the graph_def and imported graph, so that the model initialized with a frozen graph takes up as much memory as one initialized with a checkpoint.\r\n\r\nFollow-up questions:\r\n1. Shouldn't the model initialized with a frozen graph use even less memory than a checkpoint, considering that the training-only nodes were taken out? Does checkpoint contain values for training-only nodes? If not, why is it much larger? Is memory allocated for training-only nodes at all if I don't use an optimizer?\r\n2. Why do calls to tf.Session() use different amounts of memory on different machines?\r\n\r\nI use [memory_profiler](https://pypi.org/project/memory-profiler/) to measure the used memory.\r\nThe model is a CNN. ", "comments": ["@drunkinlove \r\n\r\nRequest you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram I prepared two scripts along with a frozen graph and a checkpoint:\r\nhttps://drive.google.com/drive/folders/1dnUp7Nc38C43JpIRfrrL4Q1kh1o6Fz91?usp=sharing", "@drunkinlove \r\n\r\nI tried in colab with TF 1.14 and i am seeing the below error message.`DecodeError: Error parsing message`.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/ea63e70b4c575517784e60db4664c8a9/untitled903.ipynb)Thanks!", "@ravikyram The notebook works for me though:\r\nhttps://colab.research.google.com/drive/1UW1VHp612OtxgQDOv_C71kxfBWZRTZZW?usp=sharing\r\n\r\nI suggest verifying the graph.pb file, the one I uploaded should be fine. Here's the checksum (SHA-256):\r\n3f3abe95abff708d8ae05fb27e6389e9d6700255782af8645ada7df2f0332d0c\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 39640, "title": "Tflite \"Invoke\" function stuck problem", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux debian 9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: rk3399\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.1rc\r\n- Python version: no\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): 6.3.0\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: Mali-T864 GPU\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWe run the inference every frame, after calling the \"Invoke\" function some times, the function stuck. \r\n\r\n\r\n", "comments": ["@chenpengf0223,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "@amahendrakar ,\r\nThanks for you reply, my code is as following, I call \"init\" once at the begining, and call \"run_inference\" every frame.\r\nits parameter \"cfg\" is the default setting.\r\n\r\n********************************************************************************************\r\n\r\nstd::unique_ptr<tflite::Interpreter> interpreter_;\r\nstd::unique_ptr<tflite::FlatBufferModel> model_;\r\nTfLiteDelegate* delegate_;\r\nstruct segmentor_config {\r\n    int input_width = 512;\r\n    int input_height = 512;\r\n    int input_depth = 3;\r\n\r\n    std::string frozen_net_path;\r\n    bool enable_multi_thread = false;\r\n    bool enable_gpu_inference = true;\r\n\r\n    float mean_b = 127.9489;\r\n    float mean_g = 125.3112;\r\n    float mean_r = 125.6642;\r\n    float standard_deviation_reciprocal = 0.0154;\r\n};\r\n\r\nvoid init(const segmentor_config &cfg)\r\n{\r\n    cfg_ = cfg;\r\n    model_ = tflite::FlatBufferModel::BuildFromFile(cfg_.frozen_net_path.c_str());\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    tflite::InterpreterBuilder builder(*model_.get(), resolver);\r\n    builder(&interpreter_);\r\n    if(cfg_.enable_multi_thread)\r\n        interpreter_->SetNumThreads(2);\r\n    interpreter_->AllocateTensors();\r\n\r\n    // Prepare GPU delegate.\r\n    if(cfg_.enable_gpu_inference){\r\n        const TfLiteGpuDelegateOptionsV2 options = {\r\n            .is_precision_loss_allowed = 1,\r\n            .inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,\r\n            //.inference_preference = TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED,\r\n        };\r\n        delegate_ = TfLiteGpuDelegateV2Create(&options);\r\n        //delegate_ = TfLiteGpuDelegateCreate(&options);\r\n        \r\n        if (interpreter_->ModifyGraphWithDelegate(delegate_) != kTfLiteOk) {\r\n            std::cout << \"ModifyGraphWithDelegate return false.....\" << std::endl;\r\n            return;\r\n        }\r\n    }\r\n}\r\nvoid run_inference(const cv::Mat &src_img, cv::Mat& fisheye_mask_result){\r\n    auto width = cfg_.input_width;\r\n    auto height = cfg_.input_height;\r\n    cv::Mat resized_img;\r\n    cv::resize(src_img, resized_img, cv::Size(width, height), 0, 0, cv::INTER_NEAREST);\r\n    resized_img.convertTo(resized_img, CV_8UC3);\r\n    auto input = interpreter_->typed_input_tensor<float>(0);\r\n    for (int i = 0; i < resized_img.rows; ++i){\r\n        for (int j = 0; j < resized_img.cols; ++j)\r\n        {\r\n            cv::Vec3b v = resized_img.at<cv::Vec3b>(i, j);\r\n            int idx = (i * resized_img.cols + j) * 3;\r\n            input[idx] = (v[0] - cfg_.mean_b) * cfg_.standard_deviation_reciprocal;\r\n            input[idx + 1] = (v[1] - cfg_.mean_g) * cfg_.standard_deviation_reciprocal;\r\n            input[idx + 2] = (v[2] - cfg_.mean_r) * cfg_.standard_deviation_reciprocal;\r\n        }\r\n    }\r\n\r\n    if(interpreter_->Invoke() != kTfLiteOk){\r\n        std::cout << \" interpreter_->Invoke()!= kTfLiteOk  \" << std::endl;\r\n        return;\r\n    }\r\n\r\n    auto node_index = interpreter_->outputs()[0];\r\n    TfLiteTensor* p_tflts_output = interpreter_->tensor(node_index);\r\n    cv::Mat seg_res = seg_res_parse(p_tflts_output);\r\n    result = seg_res.clone();\r\n}\r\n\r\ncv::Mat seg_res_parse(TfLiteTensor* p_tflts_output) const\r\n{\r\n    auto dims = p_tflts_output->dims;\r\n    auto data_size = dims->data[2] * dims->data[1];\r\n\r\n    cv::Mat seg_res = cv::Mat::zeros(cv::Size(cfg_.input_width, cfg_.input_height), CV_8U);\r\n    uchar* data_ptr = seg_res.data;\r\n    float* p_output_data = (float*)p_tflts_output->data.uint8;\r\n\r\n    //argmax\r\n    for (int i = 0; i < data_size; ++i)\r\n    {\r\n        int j = i * 2;\r\n        if(p_output_data[j] < p_output_data[j + 1])\r\n            data_ptr[i] = 255;\r\n    }\r\n    return seg_res;\r\n}", "@amahendrakar , \r\nI add log in \"Invoke\", and found that it stuck in function \"Status Run() override\" (which is in tensorflow/lite/delegates/gpu/cl/api.cc), where it stuck in function \"CopyToExternalObject\".\r\n\r\n  Status Run() override {\r\n    if (gl_interop_fabric_) {\r\n      RETURN_IF_ERROR(gl_interop_fabric_->Start());\r\n    }\r\n    for (auto& obj : inputs_) {\r\n      RETURN_IF_ERROR(obj->CopyFromExternalObject());\r\n    }\r\n\r\n    RETURN_IF_ERROR(context_->AddToQueue(queue_));\r\n    clFlush(queue_->queue());\r\n\r\n    std::cout<< \"mylog tf clFlush \" << std::endl;\r\n\r\n    for (auto& obj : outputs_) {\r\n      RETURN_IF_ERROR(obj->CopyToExternalObject());\r\n    }\r\n    std::cout<< \"mylog tf CopyToExternalObject \" << std::endl;\r\n\r\n    if (gl_interop_fabric_) {\r\n      RETURN_IF_ERROR(gl_interop_fabric_->Finish());\r\n    }\r\n    return OkStatus();\r\n  }", "Correct me if I'm wrong, but based on\r\n\r\n```\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux debian 9\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: rk3399\r\n```\r\n\r\nI assume you're not using it on a mobile device.  Unfortunately, we don't have experience with other platforms, and is probably out of scope for our support.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "The same problem happens  on my Rk3399 dev-board"]}, {"number": 39639, "title": "Dataset iterating different behavior in TF 2.1 and 2.2", "body": "System information\r\n- OS Platform and Distribution: Windows 10 Home\r\n- TensorFlow versions: \r\n  - 2.1: v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n  - 2.2: v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n\r\nI was not sure how to report this issue as it might be bug or just expected behavior. There are difference in TF 2.1 and 2.2. This is a code snippet to reproduce my issue:\r\n\r\n```python\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# simple dataset with zeros\r\nbatch_size = 32\r\nfeatures = np.zeros((10000, 60, 2))\r\nlabels = np.zeros((10000, 1))\r\ntrain_data = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size)\r\ntrain_steps = int(math.ceil(features.shape[0] / batch_size))\r\n\r\n# simple model with Dense layers\r\ninputs = tf.keras.Input(shape=(features[0].shape[0], features[0].shape[1]))\r\nx = tf.keras.layers.Dense(32, activation=\"relu\")(inputs)\r\noutputs = tf.keras.layers.Dense(1, activation=\"relu\")(x)\r\nmodel = tf.keras.Model(inputs, outputs, name=\"example_model\")\r\n\r\n# model fitting\r\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mse\"])\r\nmodel.fit(train_data, epochs=100, steps_per_epoch=train_steps)\r\n```\r\n\r\nWhen I run this code in TF2.1 it will produce this error: https://pastebin.com/4M43SE44\r\nAfter the first epoch, there are warnings about end of sequence, that my input ran out of data. And finally, as you can see in the pasted output, it raises `ValueError: Empty training data.`. \r\nWhen I change line with dataset creation to\r\n<pre>\r\ntrain_data = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size).<b>repeat()</b>\r\n</pre>\r\nthan everything works as expected.\r\n\r\nThis is behavior I would expect. (Note `steps_per_epoch` attribute as I wan to control this by myself, of course when I do not use `repeat` and `steps_per_epoch` is set to None, it will work under TF2.1 as it will iterate whole dataset every epoch).\r\n\r\nWhen I run the same code with TF2.2 (no repeat, train_steps are specified) it works without any issue. Is this behavior intentional? Why does it work in TF2.2 and not 2.1? Could anyone elaborate on this issue?", "comments": ["@sondracek \r\nI ran the code shared by you, please find the [gist for tf 2.1](https://colab.sandbox.google.com/gist/Saduf2019/91f6910ea8824e1a85bd5d9f13c107e2/untitled188.ipynb) and [tf 2.2](https://colab.sandbox.google.com/gist/Saduf2019/35c683e26a6d1b5347fab54b21a97395/untitled187.ipynb) and let us know if this confirms your issue.\r\nIf any data is missing please share all dependencies.", "The first gist (tf 2.1) is right, it raises ValueError as I would expect.\r\nBut the second gist is not right (not sure what is the error about). When I run the same code (from your gist) for tf 2.2, it's running without any issues: [gist for tf 2.2](https://colab.research.google.com/gist/sondracek/fbfc8c39c36e146a959e60d26e328d06/untitled187.ipynb)\r\nThank you for your help!", "@tomerk could you please take a look? this looks to be rooted in Keras and not tf.data.", "@omalleyt12 is this one of the known regressions that have fixes for 2.3 that didn't quite make it in to 2.2? Or is this something to add to the list?", "@sondracek Thanks for the issue!\r\n\r\nYep this is something we added support for in 2.2\r\n\r\nWe can sometimes know the exact size of the `Dataset` you pass in. If we can know this size, and you pass the exact size in `steps_per_epoch`, then we will assume that you meant for us to recreate the `Iterator` each epoch.\r\n\r\nThis seems to me to be the most intuitive behavior, since what we do if you don't pass `steps_per_epoch` is we infer it to be the entire size of the `Dataset`\r\n\r\nClosing as intended behavior, but please re-open if you think this behavior is confusing and should be changed, I don't have a strong opinion either way", "Thank you for your response, I didn't find any note on this change so I was just a bit surprised that the behavior is different.\r\n\r\nI don't have a strong opinion here as well, but it makes sense. It would be worth writing it to documentation (not sure whether it's mentioned somewhere?), however, thank you for your explanation."]}, {"number": 39638, "title": "saved_model README.md uses deprecated code", "body": "## URL(s) with the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThat documentation uses deprecated code like 'tf.Session()'\r\n\r\n### Submit a pull request?\r\n\r\nNo, because I don't really know how it should be used now.\r\n", "comments": ["The `SavedModelBuilder` function is deprecated and only available through v1 compatibility.\r\nSo you should be using [`tf.compat.v1.saved_model.Builder`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/Builder#top_of_page) function when using TF 2.X version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "The documentation still needs to be fixed", "The saved_model md file is updated with commit [819eb34](https://github.com/tensorflow/tensorflow/commit/819eb34070c856a7f2881f5d14183dfa5c5c2375). Thanks!"]}, {"number": 39637, "title": "how to build tflite_runtime, I want to modify tflite_runtime.", "body": "how to build tflite_runtime, I want to modify tflite_runtime.\r\nhttps://www.tensorflow.org/lite/guide/python", "comments": ["Here is the way to build it.\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39637\">No</a>\n"]}, {"number": 39636, "title": "Debug keras code on Graph mode.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15.1-2.2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nSometimes I have to work on Graph mode(I can't use eager execution) and I can't debug Keras code with PDB since all code are converted by the autograph.\r\nCould you disable the autograph on Graph mode?\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nDevelopers\r\n**Any Other info.**\r\n", "comments": ["@fsx950223 Is there any actionable item like raising PR? \r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this feature on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!"]}, {"number": 39635, "title": "Bug in tf.keras bidirectional LSTM when time_major is true", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Macos Mojave 10.14.6\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.6.3\r\n\r\n**Bug description**\r\n\r\nWhen using bidirectional layer with forward/backward lstms with time_major=True and merge mode concat(same issue exists in other modes too), it produces incorrect results due to the below line:\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/wrappers.py#L658\r\nIf time_major=true (the input shape of bidi lstm is - [seq_len, batch_size, hidden_size]), axis 1 represents batch dimesnion and we end up reversing y_rev in batch dimension before concatenation while it should have been reversed in the dimension representing seq_len(axis 0).\r\n\r\nThis works fine when LSTM is time_major=False as in that case axis 1 represents seq_len.  Ideally it should see which axis - axis 0 or axis 1 represent the time dimension and reverse on that axis, instead of generically reversing on axis 1.\r\n\r\nCould you please fix this bug as the time_major version of the lstms is more efficient.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nseq_len = 2\r\nbatch_size = 1\r\nfeature_dim = 1\r\n\r\ninput = tf.keras.Input(shape=(seq_len, feature_dim))\r\n# Transpose input to be time major\r\ninput_transposed = tf.transpose(input, perm=[1,0,2])\r\noutput = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1, return_sequences=True, time_major=True), name='bi')(input_transposed)\r\nmodel = tf.keras.Model(inputs=input, outputs=output)\r\n\r\n# Set all the weights to be one for simplicity\r\nrnn_layer = model.get_layer('bi')\r\nweights = rnn_layer.get_weights()\r\nnew_w = [np.ones(x, dtype=np.float32) for x in [(feature_dim, 4), (1, 4), (4)] * 2]\r\nrnn_layer.set_weights(new_w)\r\n\r\nmodel.save(\"test.h5\")\r\nx = np.ones((batch_size, seq_len, feature_dim), dtype=np.float32)\r\nexpected = model.predict(x)\r\nprint(expected)\r\n```\r\n\r\nExpected result is :\r\n```\r\n[[[0.6082834  0.87263733]    \r\n  [0.87263733 0.6082834 ]]]\r\nwhich is:\r\n[[[ forward_layer_seq_1, backward_layer_seq_1 ]]\r\n [[ forward_layer_seq_2, backward_layer_seq_2]]]\r\nBut what we get is:\r\n[[[0.6082834  0.6082834 ]]\r\n [[0.87263733 0.87263733]]]\r\n[[[ forward_layer_seq_1, backward_layer_seq_2 ]]\r\n [[ forward_layer_seq_2, backward_layer_seq_1]]]\r\n```\r\n", "comments": ["@sonu1-p \r\n\r\nI have tried in colab with TF nightly version (2.3.0-dev20200518) and i am not seeing any issue in nightly version.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/ddebb9a159077acb4072d6a8aad25cab/untitled899.ipynb).You could use tf-nightly for now and in the next couple of months new stable version will be released.Please, close this thread if your issue got resolved. Thanks!", "Yes I see that its fixed in master branch:\r\nhttps://github.com/tensorflow/tensorflow/blob/ef45324fc62fc9a911e5771a40f9790900500de9/tensorflow/python/keras/layers/wrappers.py#L656\r\n\r\nLooks like it got fixed by this commit last month - https://github.com/tensorflow/tensorflow/commit/1a09bbf34ea07713a66cc4385800ab2744587884\r\n\r\nI am also not able to reproduce it with tf-nightly. Thanks will close this.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39635\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39635\">No</a>\n"]}, {"number": 39634, "title": "Error with my Customized layers: bound method could not be transformed, Bad argument number for Name: 3, expecting 4", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 16.04:\r\n- TensorFlow version : 1:14\r\n- Python version: 3.7\r\n\r\n**Standalone code to reproduce the issue**\r\nI would like to customize a Dropout layer in which I can cache and reset the dropout mask manually. The code is as below:\r\n\r\n```\r\nclass DropoutControl(Layer):\r\n\r\n    def __init__(self, rate, seed=None, **kwargs):\r\n        super(DropoutControl, self).__init__(**kwargs)\r\n        self.rate = rate\r\n        self.seed = seed\r\n        self.cache_dropout_mask = None\r\n\r\n    def reset_dropout(self):\r\n        rate = ops.convert_to_tensor(\r\n              self.rate, dtype=self.input_dtype, name=\"rate\")\r\n        random_tensor = random_ops.random_uniform(\r\n            shape=self.shape, seed=self.seed, dtype=self.input_dtype)\r\n        keep_prob = 1 - rate\r\n        scale = 1 / keep_prob\r\n        keep_mask = random_tensor >= rate\r\n        self.cache_dropout_mask  = scale * math_ops.cast(keep_mask, self.input_dtype)\r\n\r\n    def get_dropout_mask(self):\r\n        return self.cache_dropout_mask \r\n\r\n    def call(self, inputs, training):\r\n        if self.cache_dropout_mask is None:\r\n            self.shape = array_ops.shape(inputs)\r\n            self.input_dtype = inputs.dtype\r\n            self.reset_dropout()\r\n\r\n        def dropped_inputs():\r\n          return inputs * self.cache_dropout_mask\r\n\r\n        output = tf_utils.smart_cond(training,\r\n                                     dropped_inputs,\r\n                                     lambda: array_ops.identity(inputs))\r\n        return output\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n    def get_config(self):\r\n        config = {\r\n            'rate': self.rate,\r\n            'seed': self.seed\r\n        }\r\n        base_config = super(DropoutControl, self).get_config()\r\n        return dict(list(base_config.items()) + list(config.items()))\r\n```\r\n**Describe the current behavior**\r\nWhen I use this layer, the system gave me the following warning. What is this problem? Is it an issue that I have to fix? \r\n\r\n> WARNING:tensorflow:Entity <bound method DropoutControl.call of <model_utils.DropoutControl object at 0x7ffaa71b2650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DropoutControl.call of <model_utils.DropoutControl object at 0x7ffaa71b2650>>: AssertionError: Bad argument number for Name: 3, expecting 4\r\n\r\nThanks ahead!\r\n\r\n\r\n\r\n", "comments": ["@bzhong2,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Also, please upgrade your TensorFlow to v1.15 or v2.2 and let us know if you are facing the same issue. Thanks!", "Yes, after upgrading the TensorFlow version to 1.15, the problem was solved. Thanks a lot! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39634\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39634\">No</a>\n"]}, {"number": 39633, "title": "centos7 gcc: error: unrecognized command line option '-std=c++14'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.2\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0\r\n- GCC/Compiler version (if compiling from source):  7.3.1\r\n\r\n**Describe the problem**\r\n\r\nI follow the official build flow\uff0cbut I met the the problem with \r\n\r\ngcc: error: unrecognized command line option '-std=c++14'\r\n\r\nI have try install \r\nyum install devtoolset-7\r\nscl enable devtoolset-7 bash\r\n\r\nAnd when I type gcc --version\uff0cit show me 7.3.1\r\n\r\nBut when I run\r\n./configure\r\nbazel build --config=opt --config=monolithic //tensorflow/tools/lib_package:libtensorflow\r\n\r\nIt still show me \r\n\r\ngcc: error: unrecognized command line option '-std=c++14'\r\n\r\nI have no idea how to solve this problem,please help me,thanks\r\n", "comments": ["@cheneyweb \r\nCan you please refer to these links and let us know if it helps:\r\n[Link](https://stackoverflow.com/questions/36245428/c-error-unrecognized-command-line-option-std-c14) [link2](https://unix.stackexchange.com/questions/265668/g-doesnt-recognize-standard-14-std-c14) #32677 #36892 #36896  ", "@Saduf2019 \r\nI have read all the link before,the most close to my problem is \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/32677\r\n\r\nI think the reason to cause this problem is the gcc of centos7 is 4.8.5\uff0cand official doc say tensorflow need to be build with gcc 7.3.1\r\n\r\nand I follow issues/32677 to install gcc 7.3.1 by follow steps\r\n\r\nyum install devtoolset-7\r\nscl enable devtoolset-7 bash\r\n\r\nafter run above commands\uff0cI type gcc --version\uff0c it show me gcc version with 7.3.1\r\n\r\nbut it seem doesn't work with bazel build?\r\n\r\nAnd then I try to copy gcc from /opt/rh/devtoolset-7/root/usr/bin/gcc to  /usr/bin/gcc  (cover)\r\n\r\nand I run bazel build again, the -std=c++14 error change\uff0cand it show me another error\r\n\r\nI guess it because gcc is not the only one I need to move\uff0cbut I don't know how to move all fo the gcc 7.3.1\r\n\r\nis there anyway to set gccpath for tensorflow build? or it hardcode gcc path for bazel?", "@Saduf2019 \r\nwhen I run \r\n\r\nbazel clean --expunge\r\n\r\nit's seem work and begin to compile\r\n\r\nbut when the progress come to [3000/6000]\r\n\r\nit break and show me an error:\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/b984a59284261e4303e87b6b828afcba/external/com_google_protobuf/BUILD:406:1: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o:code_generator.cc:function google::protobuf::compiler::ParseGeneratorParameter(std::string const&, std::vector<std::pair<std::string, std::string>, std::allocator<std::pair<std::string, std::string> > >*): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::io::StringOutputStream::~StringOutputStream(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function non-virtual thunk to google::protobuf::compiler::CommandLineInterface::ErrorPrinter::~ErrorPrinter(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function non-virtual thunk to google::protobuf::compiler::CommandLineInterface::ErrorPrinter::~ErrorPrinter(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::compiler::CommandLineInterface::ErrorPrinter::~ErrorPrinter(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::compiler::(anonymous namespace)::PluginName(std::string const&, std::string const&): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::compiler::CommandLineInterface::MemoryOutputStream::~MemoryOutputStream(): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::compiler::CommandLineInterface::GenerateDependencyManifestFile(std::vector<google::protobuf::FileDescriptor const*, std::allocator<google::protobuf::FileDescriptor const*> > const&, std::unordered_map<std::string, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> > > const&, google::protobuf::compiler::DiskSourceTree*): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/java_helpers.o:java_helpers.cc:function google::protobuf::compiler::java::SortFieldsByNumber(google::protobuf::Descriptor const*): error: undefined reference to '__cxa_throw_bad_array_new_length'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/objectivec_message.o:objectivec_message.cc:function google::protobuf::compiler::objectivec::(anonymous namespace)::SortFieldsByNumber(google::protobuf::Descriptor const*): error: undefined reference to '__cxa_throw_bad_array_new_length'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/objectivec_message.o:objectivec_message.cc:function google::protobuf::compiler::objectivec::MessageGenerator::GenerateSource(google::protobuf::io::Printer*): error: undefined reference to '__cxa_throw_bad_array_new_length'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/dynamic_message.o:dynamic_message.cc:function google::protobuf::DynamicMessageFactory::GetPrototypeNoLock(google::protobuf::Descriptor const*): error: undefined reference to '__cxa_throw_bad_array_new_length'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/extension_set.o:extension_set.cc:function google::protobuf::internal::ExtensionSet::~ExtensionSet(): error: undefined reference to 'operator delete[](void*, unsigned long)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protobuf_lite/extension_set.o:extension_set.cc:function google::protobuf::internal::ExtensionSet::GrowCapacity(unsigned long): error: undefined reference to 'operator delete[](void*, unsigned long)'\r\ncollect2: error: ld returned 1 exit status\r\n", "If you want to change your version of gcc, try cleaning your bazel bazel clean --expunge or bazel sync before you try to build again.", "Ah sorry, didn't see the previous message, looks like you can try\r\n\r\n`BAZEL_LINKLIBS=-l%:libstdc++.a  bazel build --config=opt --config=monolithic //tensorflow/tools/lib_package:libtensorflow` \r\n\r\naccording to #35867", "@daverim thank you very much. I have notice to #35867 too\r\n\r\nAnd After I run the BAZEL_LINKLIBS=-l%:libstdc++.a     as prefix\uff0cit still gone show me another error:\r\n\r\n/usr/local/node/tensorflow-r2.2/tensorflow/core/kernels/BUILD:3683:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_square_root_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\n\r\nthe final progress is [4,713 / 6,306]", "It finish !!!\r\n@daverim  @Saduf2019 ,thank you very much.\r\n\r\nthe reason of above problem is memory out\r\n\r\nI close this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39633\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39633\">No</a>\n"]}, {"number": 39632, "title": "tensorflow-io import error caused by \"com_google_absl\"", "body": "**System information**\r\n- Many Linux\r\n- TensorFlow installed from source (master branch) and binary\r\n- TensorFlow version: latest tf-nightly\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): GCC 8.4.0\r\n\r\n\r\n**Describe the problem**\r\n\r\nA NotImplementedError \r\n\"undefined symbol: _ZNK10tensorflow10FileSystem8BasenameEN4absl11string_viewE\"\r\noccurs when importing tensorflow-io 0.13.0 or a series of tensorflow-io-nightly. The root cause is related with commit 6c7e338ae7f0b0f2e224319de7e2165141c148fb. Do you have any plan to fix this error or to replace the abseil-cpp? Or, do you have any build method to bypass this error?\r\n", "comments": ["@CuiYifeng \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "Platform: CentOS 7 (Linux version 3.10.0-957.el7.x86_64)\r\nTensorFlow version: 2.3.0.dev20200513 (actually including TF built from master branch within the last month at least)\r\nTensorFlow-io version: 0.13.0.dev20200515155704\r\n\r\nHow to reproduce:\r\n`python`\r\n`import tensorflow_io as tfio`\r\n\r\nThen we can see error log as the following:\r\nNotImplementedError: unable to open file: libtensorflow_io.so, from paths: ['path/to/site-packages/tensorflow_io/core/python/ops/libtensorflow_io.so']\r\ncaused by: ['path/to/site-packages/tensorflow_io/core/python/ops/libtensorflow_io.so: undefined symbol: _ZNK10tensorflow10FileSystem8BasenameEN4absl11string_viewE']\r\n\r\nThanks!", "Another issue caused by commit 6c7e338ae7f0b0f2e224319de7e2165141c148fb:\r\n\r\n\r\n```\r\nFAIL: //tensorflow/python/kernel_tests:substr_op_test (see /home/byronyi/.cache/bazel/_bazel_byronyi/af81e00f6f55d49334af721d67cec08e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/substr_op_test/test.log)\r\n\r\nFAILED: //tensorflow/python/kernel_tests:substr_op_test (Summary)\r\n      /home/byronyi/.cache/bazel/_bazel_byronyi/af81e00f6f55d49334af721d67cec08e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/substr_op_test/test.log\r\n      /home/byronyi/.cache/bazel/_bazel_byronyi/af81e00f6f55d49334af721d67cec08e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/substr_op_test/test_attempts/attempt_1.log\r\n      /home/byronyi/.cache/bazel/_bazel_byronyi/af81e00f6f55d49334af721d67cec08e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/substr_op_test/test_attempts/attempt_2.log\r\nINFO: From Testing //tensorflow/python/kernel_tests:substr_op_test:\r\n==================== Test output for //tensorflow/python/kernel_tests:substr_op_test:\r\nRunning tests under Python 3.5.9: /usr/local/bin/python3.5\r\n[ RUN      ] SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'BYTE')\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'BYTE')): 0.02s\r\nI0521 18:41:29.769983 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'BYTE')): 0.02s\r\n[       OK ] SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'BYTE')\r\n[ RUN      ] SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'BYTE')\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'BYTE')): 0.0s\r\nI0521 18:41:29.771886 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'BYTE')): 0.0s\r\n[       OK ] SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'BYTE')\r\n[ RUN      ] SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'UTF8_CHAR')\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'UTF8_CHAR')): 0.0s\r\nI0521 18:41:29.773532 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'UTF8_CHAR')): 0.0s\r\n[       OK ] SubstrOpTest.testBadBroadcast(<class 'numpy.int32'>, 'UTF8_CHAR')\r\n[ RUN      ] SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'UTF8_CHAR')\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'UTF8_CHAR')): 0.0s\r\nI0521 18:41:29.775230 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'UTF8_CHAR')): 0.0s\r\n[       OK ] SubstrOpTest.testBadBroadcast(<class 'numpy.int64'>, 'UTF8_CHAR')\r\n[ RUN      ] SubstrOpTest.testBroadcast(<class 'numpy.int32'>, 'BYTE')\r\n2020-05-21 18:41:29.775809: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-05-21 18:41:29.806852: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\r\n2020-05-21 18:41:29.812239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40c7660 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-21 18:41:29.812276: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBroadcast(<class 'numpy.int32'>, 'BYTE')): 0.05s\r\nI0521 18:41:29.820772 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBroadcast(<class 'numpy.int32'>, 'BYTE')): 0.05s\r\n[       OK ] SubstrOpTest.testBroadcast(<class 'numpy.int32'>, 'BYTE')\r\n[ RUN      ] SubstrOpTest.testBroadcast(<class 'numpy.int64'>, 'BYTE')\r\nINFO:tensorflow:time(__main__.SubstrOpTest.testBroadcast(<class 'numpy.int64'>, 'BYTE')): 0.0s\r\nI0521 18:41:29.822387 140606904841984 test_util.py:1970] time(__main__.SubstrOpTest.testBroadcast(<class 'numpy.int64'>, 'BYTE')): 0.0s\r\n[       OK ] SubstrOpTest.testBroadcast(<class 'numpy.int64'>, 'BYTE')\r\n[ RUN      ] SubstrOpTest.testBroadcast(<class 'numpy.int32'>, 'UTF8_CHAR')\r\npython3.5: external/com_google_absl/absl/strings/string_view.h:292: absl::lts_2020_02_25::string_view::operator[](absl::lts_2020_02_25::string_view::size_type) const::<lambda()>: Assertion `false && \"i < size()\"' failed.\r\nFatal Python error: Aborted\r\n```\r\n\r\ncc @impjdi\r\n\r\nEDIT: #22766 maybe related. Somehow `-DNDEBUG` was not set in our RBE builds. I checked that `-c opt` was set though. @buchgr any suggestions here?", "Also cc @yongtang per this issue", "@byronyi @CuiYifeng tensorflow-io and tensorflow-io-nightly pip packages are still tied to TF 2.2.0. \r\n\r\nTF nightly recently changed quite a few APIs and is not stable enough for us to switch and build against tf-nightly.\r\n\r\nNote this also impact some of our ongoing PRs. As a workaround, tensorflow/io have a `nightly` branch that maintains compatibility with tf-nightly.\r\n\r\nWe will push the content of `nightly` branch tensorflow/io to `master` branch around the time TF 2.3.0 release candidate is out.\r\n\r\nI will take another look to check the absl issue.", "TF doesn't use abseil-cpp lts_2020_02_25, but git commit df3ea785d8c30a9503321a3d35ee7d35808f190d (labeled \"LTS 2020.02.25 Patch 1\") with a patch around `string_view`.  Don't replace the TF-specified abseil-cpp with any other release because it will not work.  It took me a month full time to make it happen.\r\n\r\nAlso,\r\n\r\n```\r\n$ bazel test :substr_op_test  # macOS Catalina v10.15.4\r\n[ ... omitted 9001 compilation warnings ... ]\r\n//tensorflow/python/kernel_tests:substr_op_test                          PASSED in 10.2s\r\n```\r\n\r\nso you're using an official variant of TF, is that correct?", "> TF doesn't use abseil-cpp lts_2020_02_25, but git commit df3ea785d8c30a9503321a3d35ee7d35808f190d (labeled \"LTS 2020.02.25 Patch 1\") with a patch around `string_view`. Don't replace the TF-specified abseil-cpp with any other release because it will not work. It took me a month full time to make it happen.\r\n> \r\n> Also,\r\n> \r\n> ```\r\n> $ bazel test :substr_op_test  # macOS Catalina v10.15.4\r\n> [ ... omitted 9001 compilation warnings ... ]\r\n> //tensorflow/python/kernel_tests:substr_op_test                          PASSED in 10.2s\r\n> ```\r\n> \r\n> so you're using an official variant of TF, is that correct?\r\n\r\nI am using the RBE builds with environment [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/Dockerfile.rbe.cuda10.1-cudnn7-ubuntu16.04-manylinux2010-multipython) and toolchain definitions [here](https://github.com/tensorflow/tensorflow/blob/master/third_party/toolchains/remote_config/configs.bzl#L29-L40). Manually adding `--copt=-DNDEBUG` solves my issue there (which correctly disables `ABSL_ASSERT` in @impjdi's patch), so I guess it's not really related to the absl update. @gunan @buchgr mind to shed some light on this?", "> Don't replace the TF-specified abseil-cpp with any other release because it will not work. It took me a month full time to make it happen.\r\n\r\nThat's truly remarkable. Thanks for the effort!", "It looks like the issue was root caused?", "@gunan\r\n\r\n1. I think @byronyi got his problem solved somehow, but @CuiYifeng hasn't reported back.\r\n2. QQ: I'm not familiar with tensorflow-io.  Does it apply `com_google_absl_fix_mac_and_nvcc_build.patch` before building?", "@impjdi For tensorflow-io, we can only update the abseil once TF 2.3.0 rc0 is out. The master branch of tensorflow-io only depending on TF releases (not tf-nightly releases) due to the C++ API difference in tf-nightly. We will update the tensorflow-io code to make it compatible once TF 2.3.0rc0 is out.", "tensorflow-io depending on TF releases only is fine, but that doesn't explain the situation of commit 6c7e338 breaking the nightly.  If tensorflow-io depends on TF releases and commit 6c7e338 isn't a part of the latest release, it shouldn't break, should it?", "@impjdi From the description by @CuiYifeng I think the build is `tf-nightly` + `tensorflow-io`. As mentioned this will not work due to C++ API change in tf-nightly. A `tensorflow (2.2.0)` + `tensorflow-io (0.13.0)` will work.", "Gotcha.  Thanks for clarifying!\r\n\r\n@CuiYifeng Please try out the recommendation.  I don't think there is anything we can do at this point until TF 2.3.0 rc0 is out.", "> It looks like the issue was root caused?\r\n\r\n@gunan My issue was that my RBE builds with `-c opt` does not correctly set `-DNDEBUG` and hit Abseil's assertion during unit testing, which was introduced in the aforementioned patch. Current workaround is a manual `--copt=-DNDEBUG`, but I'd like to see why `-DNDEBUG` is not set in the RBE platform and/or toolchain definitions.", "@r4nt @hlopko may know.\r\nWhere do we control which flags are set by `-c opt`", "(quick answer from phone)\r\nIn the cc_toolchain_config there is a feature named \"opt\" where these flags usually come from. Or there are other features that are conditionally enabled/flag_sets conditionally expanded based on the \"opt\" feature. I can look into this issue tomorrow.", "> TF doesn't use abseil-cpp lts_2020_02_25, but git commit df3ea785d8c30a9503321a3d35ee7d35808f190d (labeled \"LTS 2020.02.25 Patch 1\") with a patch around `string_view`. Don't replace the TF-specified abseil-cpp with any other release because it will not work. It took me a month full time to make it happen.\r\n> \r\n> Also,\r\n> \r\n> ```\r\n> $ bazel test :substr_op_test  # macOS Catalina v10.15.4\r\n> [ ... omitted 9001 compilation warnings ... ]\r\n> //tensorflow/python/kernel_tests:substr_op_test                          PASSED in 10.2s\r\n> ```\r\n> \r\n> so you're using an official variant of TF, is that correct?\r\n\r\n@impjdi `tensorflow-2.2.0` doesn't meet my needs. Actually I replace the abseil-cpp with the one in r2.2 to build TF with recent master branch. The case you mentioned hasn't affected me yet. May I think you will resolve this issue in TF 2.3.0 rc0?", "According to yongtang, it will be.  Again, I'm not familiar with tensorflow-io, and can't comment.  I only make sure the master branch doesn't break.  If you do custom mix & match by parts, I think you're on your own.", "@CuiYifeng Yes we will make sure tensorflow-io' releases works with \"versioned releases\" (2.2/2.3) of TF. Once TF 2.3.0rc0 is out we will update tensorflow-io.", "@hlopko @gunan @impjdi I just checked that f805153a25b00d12072bd728e91bb1621bfcf1b1 checked-in yesterday fixed my issue above. Thanks!", "@CuiYifeng \r\n\r\nIs this still an issue?\r\nPlease, close this thread if your issue was resolved.Thanks!", "@ravikyram I have got the answer for my issue. Thanks for your information!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39632\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39632\">No</a>\n"]}, {"number": 39631, "title": "TimeDistributed(Dropout()) with the same dropout mask", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nHere is an example block of my code. I am trying to apply a time distributed dropout to the output of a many to many GRU. I would like to keep the dropout to have the same dropout mask for all time steps. However, I did not find a solution to this purpose based on the current API. Did I miss anything or is it a new feature on the roadmap? Thanks a lot! \r\n\r\n```\r\nfrom tensorflow.keras.layers import Dense, Input, GRU, Dropout, TimeDistributed\r\nx= TimeDistributed(Dense(512, activation='relu', kernel_regularizer=l2(1e-5), \\\r\n                bias_regularizer=l2(1e-5), name='cam_fc'))(input_tensor)\r\nout = GRU(\r\n                512,\r\n                dropout=0.1,\r\n                recurrent_dropout=0.1,\r\n                activation='relu', \r\n                kernel_regularizer=l2(1e-5),\r\n                bias_regularizer=l2(1e-5),\r\n                return_sequences=True, \r\n                name='intentNet_gru')(x, training=self.is_train)\r\n\r\nout = TimeDistributed(Dropout(0.1))(out, training=self.is_train)\r\n```\r\n", "comments": ["I think what TimeDistributed does is that it slide the inputs based on timestep, and feed each timestep one by one to the wrapped layer. Note that when feeding multiple input to a dropout layer, each of them will get different dropout mask, which is aligned with current behavior.\r\n\r\nIf you want to apply same mask across time steps, what you can do is just using one dropout layer, but with noise_shape param specified. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout for more details.\r\n"]}, {"number": 39630, "title": "Solved issue with Single Writer Multiple Reader mode", "body": "I add methods for create/delete/check a lock file, while the model is saving to the HDF5 file. This is the simplest cross-platform file locking in Python. You can safely write to a model file and read it from another process at the same time because the concurrent (read) process must wait to write is done.\r\n\r\nIt's working between multiple processes and in shared network storage too.\r\n\r\nSolved issue: https://github.com/tensorflow/tensorflow/issues/39312 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39630) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39630) for more info**.\n\n<!-- ok -->", "@fchollet I think this can be added without modifying the `tf.keras.models.save_model` arguments (by always creating a lockfile). What do you think?", "Okej, I think too.", "@mihaimaruseac Hi Mihai, could you check if this would work across all filesystems? (GCS, windows, etc)?", "It would be better to use `tf.io.gfile` API instead of `open` from Python as that would ensure that the code would work even on systems which only have access to a RAM filesystem", "@markub3327 Can you please check @mihaimaruseac's comments and keep us posted. Thanks!", "Yes, it can be useful. I agree with him.", "@markub3327, @mihaimaruseac Any update on this PR? Please. Thanks!", "I made changes in both files .... please check it.", "I made all changes...", "It's okay. I should have reminded on using methods from another namespace than class GFile (Remove, Exists) ... ", "    FAIL: Found 2 non-whitelisted pylint errors:\r\n    tensorflow/python/keras/saving/hdf5_format.py:70: [C0301(line-too-long), ] Line too long (95/80)\r\n    tensorflow/python/keras/saving/hdf5_format.py:90: [C0301(line-too-long), ] Line too long (113/80)\r\n\r\nDuring check has been found an error and I have an update to fix it."]}, {"number": 39628, "title": "Use of keras Sequence in Model.fit() broken", "body": "", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39628\">No</a>\n"]}, {"number": 39627, "title": "Repost of keras-team/keras #13118.", "body": "#### System information\r\n\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n* TensorFlow version: tf-nightly (2.2)\r\n\r\n#### Describe the current behavior\r\nLoading a model once and then repeatedly calling `model.predict(...)` results in continually increasing memory usage.\r\n\r\n#### Describe the expected behavior\r\nCalling `model.predict(...)` should not result in any permanent increase in memory usage.\r\n\r\n#### Code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmodel = tf.keras.applications.mobilenet_v2.MobileNetV2()\r\nX = np.random.rand(1, 224, 224, 3)\r\n\r\nwhile True:\r\n    # Leaks:\r\n    y = model.predict(X)[0]\r\n    \r\n    # Does not leak:\r\n    # y = [0]\r\n```", "comments": ["I ran the code for 25,000 iterations and don't see any significant increase in memory usage. [Here](https://colab.research.google.com/gist/jvishnuvardhan/d8db5a8099633cfc6ba73b27df85f13c/untitled191.ipynb) is the gist for our reference. Thanks!", "@dynamicwebpaige Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39627\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39627\">No</a>\n"]}, {"number": 39626, "title": "Creating custom RNN layers with different input and state shapes", "body": "Hi,\r\nI'm trying to build a custom RNN cell, which is a wrapper of an LSTM cell (or any other RNN cell), and in particular, I would need to add multiple hidden states to this layer. Also, the dimension of the hidden state would be different from the input shape.\r\nFollowing the RNN layer documentation, I've created the following layer:\r\n```\r\nclass QLSTM(tf.keras.layers.Layer):\r\n    def __init__(self, units, symbols, activation=\"tanh\", **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.lstm_cell = tf.keras.layers.LSTMCell(units, activation=activation)\r\n        self.expectation = tfq.layers.Expectation()\r\n        self.symbol_names = symbols\r\n        # State size: LSTM hidden state, LSTM outputs, QPU expectancy\r\n        self.state_size = [tf.TensorShape([2]),tf.TensorShape([2]),tf.TensorShape([1])]\r\n        self.output_size = tf.TensorShape([1]) # QPU expectation\r\n\r\n    def call(self,inputs,state):\r\n        circuits = inputs[0]\r\n        ops = inputs[1]\r\n\r\n        joined_state = tf.keras.layers.concatenate(state[1],state[2])\r\n\r\n        output_lstm, hidden_state = self.lstm_cell(joined_state,state[0])\r\n        exp_out = self.expectation(circuits,\r\n            symbol_names=self.symbol_names,\r\n            symbol_values=output_lstm,\r\n            operators=ops\r\n        )\r\n        return exp_out, [hidden_state,output_lstm,exp_out]\r\n```\r\nSo, in a nutshell, I would expect to have a hidden state that carries 3 main objects: the previous hidden state of the inner LSTM cell, the output of the previous step of the LSTM cell, and another scalar output (here called `exp_out`). For the inputs, I would expect an array of two tensors, which I'll explain later.\r\n\r\nThen, I create an instance of the layer and wrap it with the RNN layer so as to get all the outputs from the multiple timestamps:\r\n```\r\nrnn = tf.keras.layers.RNN(QLSTM(2,qaoa_symbols),return_sequences=True)\r\n```\r\nFor the input tensors, I have two tensors which have a shape of [None,10,1] each. The second dimension (10) represents the timestamps, while the third dimension (1) represents the dimension of each timestamp. So I create the input layers with the following code:\r\n```\r\nop_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)\r\ncircuit_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)\r\n```\r\nFinally, when I try to pass these inputs to the RNN layer, i do the following:\r\n```\r\nrnn_2 = rnn([circuit_inp,op_inp])\r\n```\r\nBut I get the following error:\r\n```\r\nValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, 10, 1), ndim=3)]); however `cell.state_size` is [2, 2, 1]\r\n```\r\nI've been trying to fins tutorials and other issues on how to create custom RNN but i haven't been able to solve this issue. Any idea on how can I specify the correct input shapes?\r\nThanks in advance!", "comments": ["@ etenedrac\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Saduf2019 \r\nHere you have the details of the platform and the code to reproduce the error.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WINDOWS 10\r\n- TensorFlow installed from (source or binary): BINARY (PIP)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport networkx as nx\r\nimport tensorflow as tf\r\nimport tensorflow_quantum as tfq\r\nimport cirq\r\nimport numpy as np\r\nimport random\r\nimport sympy\r\n\r\nfrom cirq.contrib.svg import SVGCircuit\r\n\r\nrandom.seed(123)\r\n\r\ndef maxcut_qaoa_from_graph(graph, p):\r\n    \"\"\"\r\n    Function to generate a Cirq QAOA circuit for a given\r\n    graph G with depth P.\r\n    This circuit is parametrized using sympy.symbols\r\n    \"\"\"\r\n    qubits = cirq.GridQubit.rect(1, len(graph.nodes))\r\n    qaoa_circuit = cirq.Circuit()\r\n    # Initial equal superposition\r\n    for qubit in qubits:\r\n        qaoa_circuit += cirq.H(qubit)\r\n    qaoa_symbols = []\r\n    # Stack the parameterized costs and mixers\r\n    for l_num in range(p):\r\n        qaoa_symbols.append(sympy.Symbol(\"gamma_{}\".format(l_num)))\r\n        for e in graph.edges():\r\n            qaoa_circuit.append(cirq.CNOT(control=qubits[e[0]], target=qubits[e[1]]),strategy=cirq.InsertStrategy.NEW)\r\n            qaoa_circuit.append(cirq.rz(qaoa_symbols[-1])(qubits[e[1]]),strategy=cirq.InsertStrategy.NEW)\r\n            qaoa_circuit.append(cirq.CNOT(control=qubits[e[0]], target=qubits[e[1]]),strategy=cirq.InsertStrategy.NEW)\r\n        qaoa_symbols.append(sympy.Symbol(\"eta_{}\".format(l_num)))\r\n        qaoa_circuit.append([cirq.rx(2*qaoa_symbols[-1])(qubits[n]) for n in graph.nodes()], strategy=cirq.InsertStrategy.NEW_THEN_INLINE)\r\n    # Define the cost as a Cirq PauliSum\r\n    cost_op = None\r\n    for e in graph.edges():\r\n        if cost_op is None:\r\n            cost_op = cirq.Z(qubits[e[0]])*cirq.Z(qubits[e[1]])\r\n        else:\r\n            cost_op += cirq.Z(qubits[e[0]])*cirq.Z(qubits[e[1]])\r\n    return qaoa_circuit, qaoa_symbols, cost_op\r\n\r\ndef generate_data(n_nodes_min,n_nodes_max,n_points,p):\r\n    \"\"\"\r\n    This function generates `n_points` number of cirq circuits,\r\n    each one corresponding to one random graph, as well as the\r\n    corresponding cost function for the QAOA problem.\r\n    Finally, it also outputs the parameters of the graphs.\r\n    \"\"\"\r\n    datapoints = []\r\n    costs = []\r\n    graphs =[] \r\n    for _ in range(n_points):\r\n        n_nodes = random.randint(n_nodes_min,n_nodes_max)\r\n        random_graph = nx.random_regular_graph(n=n_nodes,d=3)\r\n        circuit, symbols, cost_op = maxcut_qaoa_from_graph(random_graph, p)\r\n        datapoints.append(circuit)\r\n        costs.append([cost_op])\r\n        graphs.append(random_graph)\r\n    return datapoints,symbols,costs,graphs\r\n\r\nqaoa_circuit, qaoa_symbols, cost_op = maxcut_qaoa_from_graph(maxcut_graph, P)\r\n\r\nclass QLSTM(tf.keras.layers.Layer):\r\n    def __init__(self, units, symbols, activation=\"tanh\", **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.lstm_cell = tf.keras.layers.LSTMCell(units, activation=activation)\r\n        self.expectation = tfq.layers.Expectation()\r\n        self.symbol_names = symbols\r\n        # State size: LSTM hidden state, LSTM outputs, QPU expectancy\r\n        self.state_size = [tf.TensorShape([2]),tf.TensorShape([2]),tf.TensorShape([1])]\r\n        self.output_size = tf.TensorShape([1]) # QPU expectation\r\n\r\n    def call(self,inputs,state):\r\n        circuits = inputs[0]\r\n        ops = inputs[1]\r\n\r\n        joined_state = tf.keras.layers.concatenate(state[1],state[2])\r\n\r\n        output_lstm, hidden_state = self.lstm_cell(joined_state,state[0])\r\n        exp_out = self.expectation(circuits,\r\n            symbol_names=self.symbol_names,\r\n            symbol_values=output_lstm,\r\n            operators=ops\r\n        )\r\n        return exp_out, [hidden_state,output_lstm,exp_out]\r\n\r\n# Generate random MaxCut instances as training data.\r\nN_QUBITS_MIN = 8\r\nN_QUBITS_MAX = 8\r\nP = 2\r\nN_POINTS = 2\r\n\r\n# For a more accurate optimizer on testing data, increase N_POINTS\r\ncircuits, symbols, ops, graphs = generate_data(N_QUBITS_MIN, N_QUBITS_MAX, N_POINTS, P)\r\n\r\nTIMESTEPS = 10\r\n\r\ncircuit_tensor = tfq.convert_to_tensor(circuits)\r\nops_tensor = tfq.convert_to_tensor(ops)\r\n\r\ncircuit_tensor_t = tf.expand_dims(circuit_tensor, 1, name=None)\r\nops_tensor_t = tf.expand_dims(ops_tensor, 1, name=None)\r\n\r\ncircuit_tensor_rep = tf.repeat(circuit_tensor_t, TIMESTEPS, axis=-1, name=None)\r\nops_tensor_rep = tf.repeat(ops_tensor_t, TIMESTEPS, axis=-1, name=None)\r\n\r\nops_tensor_rep = tf.reshape(ops_tensor_rep,(-1,TIMESTEPS,1))\r\ncircuit_tensor_rep = tf.reshape(circuit_tensor_rep,(-1,TIMESTEPS,1))\r\n\r\nop_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)\r\ncircuit_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)\r\n\r\nrnn = tf.keras.layers.RNN(QLSTM(2,qaoa_symbols),return_sequences=True)\r\n\r\nrnn_2 = rnn([circuit_inp,op_inp])\r\n```\r\nAfter this last line it generates the mentioned error.\r\nThanks!\r\n", "@etenedrac \r\nI ran the code shared and face a different error, please  find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/8e633bb77bfcaff1f6f8aee28e6cd3c0/untitled188.ipynb)", "Hi @Saduf2019,\r\nI forgot to mention that in order to run that snippet you had to install tensroflow quantum:\r\n```\r\n!pip install tensorflow-quantum\r\n```\r\n[Here](https://colab.research.google.com/gist/etenedrac/2d91149c34cf476e07308864209508f3/untitled188.ipynb) you have the updated gist, which shows the error previously mentioned. Although I installed that package, it is not used at the point in which the error occurs", "@etenedrac \r\nI ran the code shared and face a different error, please share all dependencies or a colab gist if possible for us to analyse the error.\r\nPlease find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/c1010782057684a265c645caea2b62b9/untitled190.ipynb)", "@etenedrac \r\nPlease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@etenedrac \r\nDid you find a solution?"]}, {"number": 39625, "title": "tensorflow.python.distribute.cross_device_utils.build_collective_gather does not support TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nthrow an error\r\n\r\n**Describe the expected behavior**\r\ncollect tensors like running on GPUs\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.distribute import cross_device_utils\r\n\r\nif 'COLAB_TPU_ADDR' in os.environ:\r\n  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n    tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n  tf.config.experimental_connect_to_cluster(resolver)\r\n  tf.tpu.experimental.initialize_tpu_system(resolver)\r\n  strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\nelse:\r\n  strategy = tf.distribute.MirroredStrategy()\r\n\r\n\r\ndef all_gather(context, x):\r\n  num = context.num_replicas_in_sync\r\n  ck = cross_device_utils.CollectiveKeys()\r\n  x = cross_device_utils.build_collective_gather([x], num, ck)\r\n  return x[0]\r\n\r\n\r\n@tf.function\r\ndef step_fn():\r\n  context = tf.distribute.get_replica_context()\r\n  v = tf.zeros([1], tf.int32) + context.replica_id_in_sync_group\r\n  d = all_gather(context, v)\r\n  return d\r\n\r\nret = strategy.run(step_fn)\r\nprint(ret)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n```\r\nTypeError: in user code:\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:896 replicated_fn  *\r\n        result[0] = fn(*replica_args, **replica_kwargs)\r\n    <ipython-input-1-6f3a04d9c55b>:27 step_fn  *\r\n        d = all_gather(context, v)\r\n    <ipython-input-1-6f3a04d9c55b>:19 all_gather  *\r\n        x = cross_device_utils.build_collective_gather([x], num, ck)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:421 build_collective_gather  **\r\n        group_key = collective_keys.get_group_key_of_tensors(input_tensors)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:319 get_group_key_of_tensors\r\n        return self.get_group_key(devices)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:299 get_group_key\r\n        names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:299 <listcomp>\r\n        names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])\r\n\r\n    TypeError: %d format: a number is required, not NoneType\r\n```\r\nOn a two GPUs machine, the output would be\r\n```\r\nPerReplica:{\r\n  0: tf.Tensor([0 1], shape=(2,), dtype=int32),\r\n  1: tf.Tensor([0 1], shape=(2,), dtype=int32)\r\n}\r\n```", "comments": ["This API is only intended for GPUs and CPUs. If you want to do an allgather for TPUs, you can use this function: https://github.com/tensorflow/tensorflow/blob/4e7ce793d996dfed173bd46e90f489df9464a540/tensorflow/python/tpu/ops/tpu_ops.py#L45\r\n\r\nWe are planning to provide an API for allgather in the Strategy class so you don't need to use non exported functionality. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39625\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39625\">No</a>\n"]}, {"number": 39624, "title": "lecun_normal and he_normal crash when type casting: TypeError: he_normal() got an unexpected keyword argument 'dtype'", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): idk\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: none\r\n\r\n**Describe the expected behavior**\r\n\r\nSuccessful type casting in `tf.data.Dataset.map()` function\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nfrom tensorflow.keras import Model\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nimport tensorflow_datasets as tfds\r\nfrom tensorflow.keras.initializers import glorot_normal, he_normal, lecun_normal\r\n\r\ndataset, info = tfds.load('binary_alpha_digits', with_info=True, split='train')\r\ndata = dataset.map(lambda x: (tf.cast(x['image'], tf.float32), x['label'])).batch(8)\r\n\r\n\r\nclass Model(Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.layer1 = Dense(16, kernel_initializer=he_normal)\r\n        self.layer2 = Dense(units=info.features['label'].num_classes)\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        x = self.layer1(inputs)\r\n        x = self.layer2(x)\r\n        return x\r\n\r\n\r\nmodel = Model()\r\n_ = model(next(iter(data))[0])\r\n```\r\n`glorot_normal`: works\r\n`he_normal`: doesn't work\r\n`lecun_normal`: doesn't work\r\n", "comments": ["@nicolas-gervais,\r\nI was able to reproduce the error with TF v2.1 and [TF v2.2](https://colab.research.google.com/gist/amahendrakar/ef75f5e07621f04c2702ef3f789e243c/39624-2-2.ipynb). However, I was able to run the code without any issues with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/7c9fd985c3b0e15ee6b1e475b73a54e4/39624-tf-nightly.ipynb). Please find the attached gist. Thanks!", "> @nicolas-gervais,\r\n> Please find the attached gist. Thanks!\r\n\r\nHi, what response are you awaiting from me exactly?", "@nicolas-gervais,\r\nThe issue you are facing seems to be fixed with the TF-nightly version. I did not face any errors while running the code with TF-nightly and have attached the link for Python notebook below\r\n\r\nhttps://colab.research.google.com/gist/amahendrakar/7c9fd985c3b0e15ee6b1e475b73a54e4/39624-tf-nightly.ipynb\r\n\r\nCould you please verify it on your end and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39624\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39624\">No</a>\n"]}, {"number": 39623, "title": "TFRecordDataset mapped with crop is heavily impacted by image sizes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu 18.04, Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.7.7, 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Colab's GPU, V100 32GB, CPU\r\n\r\n\r\n**Describe the current behavior**\r\nWhen loading a dataset with large images, and mapping it to random crop, iterating over the dataset is significantly slower, even for a small dataset of 10 images in repeat. In my example I compare a dataset of 10 240x240x3 images, to one with ten 2400x2400x3 images, both of them randomly cropped to 120x120x3 - the latter was about 100 times slower to operate on (reduce_sum, in my toy example).\r\n\r\n**Describe the expected behavior**\r\nI would expect there to be no effect from the size of the images in the dataset if they are being cropped right after being loaded once the dataset resides in memory. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1uGeLJJp2gPvgC37WxbP_yANAXB8D83ld#scrollTo=DddLV9gxm5eL\r\n\r\n\r\n", "comments": ["@feature-engineer \r\nI tried accessing the colab shared, i do not have access permission to do so.", "Sorry, try this:\r\nhttps://colab.research.google.com/drive/1uGeLJJp2gPvgC37WxbP_yANAXB8D83ld?usp=sharing\r\n\r\n", "@feature-engineer \r\nCan you please refer to this issue #38947 and let us know if it helps.", "It does not, the warnings you see are the result of running the code under timeit, it complains it can't see the source code in that environment. \r\n\r\nTo alleviate the worries that this is the reason for this performance issue, I have added the same code outside of timeit, just using time.time to test one run of this.\r\n\r\nThe warnings regarding autograph are gone, but the significant run time difference is still there.\r\n\r\nJust for comparison, in the last cell, I've added a dataset from tensor which is equivalent to the size of the dataset from the \"big\" images dataset, i.e. 10x2400x2400x3 in size - it runs about 6 times faster compared to the \"small\" images dataset, and a whopping 600 times faster than its TFRecoreds equivalent.", "`repeat` re-executes the input pipeline, if you would like to cache the results of previous repeat iterations, you should use the `cache` transformation.", "`cache` indeed makes this all run much faster, although there is still a significant run time difference between the TFRecord with 240x240 and the one with 2400x2400. \r\n\r\nI have updated my notebook with cache and re-ran it, you can check it out.", "You need to run one epoch worth of data to populate the cache before you start timing things.\r\n\r\nGet rid of the trailing `repeat()` and do:\r\n\r\n```\r\n# Warm up caches\r\nfor elem in data_from_small:\r\n  pass\r\nfor elem in data_from_big:\r\n  pass\r\n\r\nstart = time.time()\r\ntf.reduce_sum([x for x in data_from_small.repeat().take(1000)])\r\nprint(time.time() - start)\r\n\r\nstart = time.time()\r\ntf.reduce_sum([x for x in data_from_big.repeat().take(1000)])\r\nprint(time.time() - start)\r\n```\r\n\r\nI would expect there to be no difference in runtime at that point.", "That clears it up, thanks!"]}, {"number": 39622, "title": "TensorFlow utilization for traditional HPC scientific application written in FORTRAN for distributed computing", "body": "Hi All,\r\n\r\nI want to discuss and know the feasibility of using tensor flow for HPC scientific application written in FORTRAN and C using MPI for data distribution in distributed computing environment.\r\n\r\nFew  research and feasibility study I did by writing small example of vector dot product in tensor flow. it is fairly very convenient  to write this code using python for tensor flow compared to writing a code using MPI and FORTRAN in which all the data distribution has been done programmer and relatively complex code.\r\n\r\nAdvantages:\r\n1. No data distribution or copying data to GPU is required in Tensor flow which is underlying managed by tensor flow.\r\n2. writing python code for array declaration or random variable initialization is very convenient.\r\n3. Optimized mathematical operations written in tensor flow that can be directly used.\r\n\r\nI am sure if used proper computational and bandwidth load in code we can achieve good performance also.\r\n\r\nMy Question is How feasible it is to port or develop HPC scientific applications in Tensor flow  and what are the challenges which cannot be handled by tensor flow?\r\nCan we suggest clients to port there code in tensor flow for such advantages ?\r\n\r\n", "comments": ["This is not a suitable medium for this question. Please only file here bug reports and feature requests"]}, {"number": 39621, "title": "Unable to install older version of tensorflow in Python3.8.2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: Installed 2.2 but want 1.x\r\n- Python version: 3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am able to install newer version of TensorFlow, i.e, 2.2. However, I need an older version of TensorFlow (1.x) to run existing code (otherwise I have to do a lot of changes as it's big repository)\r\n\r\n\r\nCMD: sudo pip3 install tensorflow-gpu==1.15\r\n\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow-gpu==1.15\r\n", "comments": ["Only TF 2.2 supports python 3.8.\r\n\r\nFor all other versions of TF you have to use Python 3.5, 3.6 or 3.7", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39621\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39621\">No</a>\n"]}, {"number": 39620, "title": "whl compiled, still tries to install hdfs and gcp packages while --config=nogcp --config=nohdfs parameters were passed for build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora release 32\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:  r2.2\r\n- Python version: 3.8.1\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 10.0.1\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nEven though tensorflow build command used had --config=nogcp --config=nohdfs parameters, the whl build while doing a pip install was trying to install dependencies like h5py and other gcp related packages. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nHave used following comand:\r\n\r\n```\r\nbazel build -s --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n```\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /home/riscv/tensorflow_pkg\r\n```\r\n```\r\npip3 install /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n[riscv@fedora-riscv tensorflow]$ pip3 install /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl                                           Defaulting to user installation because normal site-packages is not writeable\r\nProcessing /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl\r\nRequirement already satisfied: astunparse==1.6.3 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.6.3)\r\nRequirement already satisfied: absl-py>=0.7.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.9.0)\r\nCollecting grpcio>=1.8.6\r\n  Using cached grpcio-1.29.0.tar.gz (19.6 MB)\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)\r\nCollecting gast==0.3.3\r\n  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\r\nCollecting h5py<2.11.0,>=2.10.0\r\n  Using cached h5py-2.10.0.tar.gz (301 kB)\r\nRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/lib64/python3.8/site-packages (from tensorflow==2.2.0) (1.4.1)\r\nRequirement already satisfied: wrapt>=1.11.1 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.12.1)\r\nRequirement already satisfied: termcolor>=1.1.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)\r\nCollecting tensorboard<2.3.0,>=2.2.0\r\n  Using cached tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\r\nCollecting google-pasta>=0.1.8\r\n  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\nRequirement already satisfied: protobuf>=3.8.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.11.3)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.2.1)\r\nRequirement already satisfied: six>=1.12.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.14.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.18.3)\r\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.34.2)\r\nRequirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.0)\r\nCollecting werkzeug>=0.11.15\r\n  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\r\nRequirement already satisfied: markdown>=2.6.8 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\r\nCollecting google-auth<2,>=1.6.3\r\n  Using cached google_auth-1.14.3-py2.py3-none-any.whl (89 kB)\r\nCollecting google-auth-oauthlib<0.5,>=0.4.1\r\n  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\r\nCollecting tensorboard-plugin-wit>=1.6.0\r\n  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\r\nRequirement already satisfied: setuptools>=41.0.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (46.1.3)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\r\nCollecting pyasn1-modules>=0.2.1\r\n  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\nCollecting rsa<4.1,>=3.1.4\r\n  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)\r\nCollecting cachetools<5.0,>=2.0.0\r\n  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)\r\nCollecting requests-oauthlib>=0.7.0\r\n  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.9)\r\nRequirement already satisfied: certifi>=2017.4.17 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.4.5.1)\r\nRequirement already satisfied: idna<3,>=2.5 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)\r\nCollecting pyasn1<0.5.0,>=0.4.6\r\n  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\nCollecting oauthlib>=3.0.0\r\n  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\r\nBuilding wheels for collected packages: grpcio, h5py\r\n....\r\n```\r\n\r\nPS: I am trying to get Tensorflow built for RISCV architecture here.", "comments": ["`h5py` isn't related to hdfs nor gcp. It is a Python package to save trained model in the format .h5.  I think that packages with `google` in names aren't related to gcp as well. It just a library created by Google", "Thanks for your reply.\nBut installing this package resulted in error like no libhdfs.so found.\n\nOn Mon, May 18, 2020, 06:20 Vo Van Nghia <notifications@github.com> wrote:\n\n> h5py isn't related to hdfs nor gcp. It is a Python package to save\n> trained model in the format .h5\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39620#issuecomment-629890356>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APTVH7KDSZQF6Z2UTGJBAFDRSCA53ANCNFSM4NDLDFSQ>\n> .\n>\n", "could you be more specific ?", "@arun-sl Are you able to build TF successfully?", "Hello All,\n\nFor the issue I have raised, the logs were as follows:\n\n Loading library to get version: libhdf5.so\n  error: libhdf5.so: cannot open shared object file: No such file or\ndirectory\n  ----------------------------------------\n  ERROR: Failed building wheel for h5py\n\nThis happened while doing following:\n\npip3 install\n/home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl\n\nIf needed I shall attach full logs as well.\n\nYes Yasir I was able to build the whl package.\nBut final installation I am still facing issues wrt dependencies not met.\n\n\nOn Tue, May 19, 2020 at 3:28 AM Yasir Modak <notifications@github.com>\nwrote:\n\n> @arun-sl <https://github.com/arun-sl> Are you able to build TF\n> successfully?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39620#issuecomment-630456512>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/APTVH7IQOJKX7ETRE6VJJHLRSGVRTANCNFSM4NDLDFSQ>\n> .\n>\n", "@arun-sl \nmaybe you could take a look\nhttps://github.com/h5py/h5py/issues/1461", "Closing this issue for now. If this is still an issue, Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39620\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39620\">No</a>\n"]}, {"number": 39619, "title": "Build tensorflow/lite/c/*.cc in TensorFlow Lite Makefile", "body": "Hello,\r\n\r\nI added missing `tensorflow/lite/c/*.cc` to build files of TensorFlow Lite Makefile.\r\nThis is needed to build `tensorflow/lite/c/c_api.cc`, which includes TensorFlow Lite C API implementations.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39619) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39619) for more info**.\n\n<!-- ok -->", "@seanchas116 Can you please fix build failures ? Thanks!\r\n"]}]