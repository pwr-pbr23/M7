[{"number": 32402, "title": "Using inexpensive opcodes to remove a list", "body": "Using fewer/inexpensive [opcodes](https://docs.python.org/3/library/dis.html) to remove a list; this is useful when a list is large. ", "comments": ["I'll temporarily close this one; try to rewrite this part later. Thanks @alextp. "]}, {"number": 32401, "title": "GPU NMS kernel in TF 1.15rc0 was not fixed", "body": "This commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/9480262cbbfc2430b0c53424f0fc133418d7ae3f\r\nwas included in TF 1.15rc0.\r\n\r\nHowever, this commit has a bug as pointed out in https://github.com/tensorflow/tensorflow/pull/28745#issuecomment-512949342.\r\n\r\nThe bug is fixed in https://github.com/tensorflow/tensorflow/commit/5d6158e0d4a736a8ad2fc98b717fed519e4080f0, which is not included in TF 1.15rc0.\r\n\r\nI expect TF 1.15rc0 to include the bugfix for the GPU version of NMS kernel.", "comments": ["Can you make a cherry-pick of https://github.com/tensorflow/tensorflow/commit/5d6158e0d4a736a8ad2fc98b717fed519e4080f0 please? Assign to me", "I merged the cherry-pick, it will land in the final release/next RC (whichever comes first).\r\n\r\nThank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32401\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32401\">No</a>\n"]}, {"number": 32400, "title": "Use experimental_ref() in moving_averages", "body": "In addition, fix zero_debias_true to use experimental_ref.\r\n\r\nPiperOrigin-RevId: 268230742\r\n(cherry picked from commit a0e85fd379c25da3a404e7a8133da4abb3860c4f)", "comments": []}, {"number": 32399, "title": "[r2.0 CherryPick]: Use experimental_ref() in moving_averages", "body": "In addition, fix zero_debias_true to use experimental_ref.\r\n\r\nPiperOrigin-RevId: 268230742\r\n(cherry picked from commit a0e85fd379c25da3a404e7a8133da4abb3860c4f)", "comments": ["Are there missing patches? Seems like the tests are filing"]}, {"number": 32398, "title": "ValueError from invalid weights while loading older .h5 model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1 & v1.12.1-10753-g1c2ae57 2.0.0-dev20190910\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI created a tfkeras model and saved it to .h5 format using TF version 1.13.1. The model can be loaded and used for inference just fine in 1.13.1. After upgrading to TF 2.0 (nightly build), loading the model results in a ValueError (see traceback below).\r\nI am using tf.compat.v1.disable_v2_behavior() in case that might make a difference.\r\n\r\n**Describe the expected behavior**\r\nV1 models should load correctly, or present the user with a way to migrate the model to a more compatible format.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nRelevant traceback info:\r\n```\r\n  File \"*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 146, in load_mode\r\nl\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 171, in lo\r\nad_model_from_hdf5\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"*/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 697, in lo\r\nad_weights_from_hdf5_group\r\n    str(len(weight_values)) + ' elements.')\r\nValueError: Layer #1 (named \"encoder_bn_0\" in the current model) was found to correspond to layer encoder_bn_0 in the save file. However t\r\nhe new layer encoder_bn_0 expects 7 weights, but the saved weights have 8 elements.\r\n```\r\n\r\nI added some print statements to get info on the weights for this batchnorm (w. renorm) layer. The names for the weights in the original model are:\r\n```\r\n['encoder_bn_0/gamma:0', 'encoder_bn_0/beta:0', 'encoder_bn_0/moving_mean:0', 'encoder_bn_0/moving_variance:0', 'encoder_bn_0/renorm_mean:\r\n0', 'encoder_bn_0/renorm_mean_weight:0', 'encoder_bn_0/renorm_stddev:0', 'encoder_bn_0/renorm_stddev_weight:0']\r\n```\r\nThe expected weight placeholders are:\r\n```\r\n[<tf.Variable 'encoder_bn_0/gamma:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/beta:0' shape=(96,) dtype=float32>, <tf.Variab\r\nle 'encoder_bn_0/moving_mean:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/moving_variance:0' shape=(96,) dtype=float32>, <tf.\r\nVariable 'encoder_bn_0/moving_stddev:0' shape=(96,) dtype=float32>, <tf.Variable 'encoder_bn_0/renorm_mean:0' shape=(96,) dtype=float32>,\r\n<tf.Variable 'encoder_bn_0/renorm_stddev:0' shape=(96,) dtype=float32>]\r\n```\r\n\r\nI'm guessing the format for saving batchnorm (w.renorm) parameters changed at some point? Is there a way to make this backwards compatible? Or perhaps a way to migrate the save file?", "comments": ["@ghannum, Make sure that your code fully compatible to TensorFlow 2.0 before  inference. Please refer the [Tensorflow website](https://www.tensorflow.org/beta/guide/migration_guide?hl=en) for further guidance.  Thanks!", "As a first step to check TF2 compatibility, I tried running with the new 1.15-rc0 release. The error remains as-is. I don't believe this is a 2.0-specific issue. Release 1.14 works ok, so the issue is somewhere between 1.14 and 1.15/2.0.", "I also tried to load/save my model using 1.14 to effectively move the problem forward. I get the same issue trying to load it in 1.15.", "@ghannum, Will it be possible to provide the code to reproduce the issue. Thanks!", "\r\ncreate_model.py\r\n```\r\n#!/usr/bin/env python3\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nfrom tensorflow.compat.v1.keras.models import save_model\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\nmodel = keras.Sequential([\r\n    keras.layers.Flatten(input_shape=(28, 28)),\r\n    keras.layers.Dense(128, activation=tf.nn.relu),\r\n    keras.layers.BatchNormalization(renorm=True),\r\n    keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n\r\nmodel.compile(optimizer='adam',\r\n    loss='sparse_categorical_crossentropy',\r\n    metrics=['accuracy'])\r\nmodel.fit(train_images, train_labels, epochs=5)\r\n\r\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\r\nprint('Test accuracy:', test_acc)\r\n\r\nsave_model(model, \"model.h5\")\r\n```\r\nload_model.py\r\n```\r\n#!/usr/bin/env python3\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nfrom tensorflow.compat.v1.keras.models import load_model\r\n\r\nfashion_mnist = keras.datasets.fashion_mnist\r\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\r\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\r\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\nmodel = load_model(\"model.h5\")\r\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\r\nprint('Test accuracy:', test_acc)\r\n```\r\n\r\nHere are two scripts which will reproduce the issue. The first creates a simple keras model and saves it to .h5 format. The second loads the model and prints a validation measure. If you run the first in TF1.13, the second will fail in TF1.15. This is only a problem with the BatchNormalization renorm option set to True.", "@ghannum, Thanks for the code. \r\nI tried on colab with Tf 2.0.0.rc1, I didn't receive any error, Please take a look at colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/7a0df6ba3bf7a2f0fae872eff1c710e1/untitled162.ipynb). Thanks!", "It looks like you ran both scripts in Tf 2.0.0.rc1. The error only occurs if the model was created in Tf 1.13. This is a backwards-compatibility issue with the file format / parser. That gist tool is awesome btw.", "@ghannum,\r\nI tried with Tensorflow 1.13.1 on colab but i didn't get any error. Please see the [gist](https://colab.sandbox.google.com/gist/gadagashwini/f050afc522840a9706dbaeb49d37e6b7/untitled163.ipynb). I executed both the create model and load_model with same TF. Is there any specific reason to use two different versions of TF. Thanks!", "Thanks for trying 1.13. The error is only when the model is created in 1.13 and loaded in 1.1.5/2.0 (two different versions). The backwards compatibility is important because these models take considerable time and resources to train (months in some cases). I currently have several important models in 1.13 and would like to continue using them after upgrading to 1.15. I imagine this sort of continuous compatibility is part of the 1.X design scope. Even if the file format changes, there should be some migration path.\r\n\r\nIn this case, I've identified the difference in the renorm parameters being stored (see errors above and a summary below). Can we identify why these changes were made and how one might make them compatible? Either the code can be made to recognize both formats, or there can be some procedure to upgrade the file?\r\n\r\nThanks again for your help!\r\n\r\nBatchNormalization weight names in 1.13:\r\n```\r\ngamma, beta, moving_mean, moving_variance, renorm_mean, renorm_mean_weight, renorm_stddev, renorm_stddev_weight\r\n```\r\nBatchNormalization weight names in 1.15/2.0:\r\n```\r\ngamma, beta, moving_mean, moving_variance, moving_stddev, renorm_mean, renorm_stddev\r\n```", "I was able to reproduce the reported behavior. Built was model using TF 1.13.1 but failed to load using TF 1.15.0-rc2. It loads successfully in TF 1.14.0\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-3427bf5f75af> in <module>()\r\n     12 test_images = test_images / 255.0\r\n     13 \r\n---> 14 model = load_model(\"model.h51\")\r\n     15 test_loss, test_acc = model.evaluate(test_images, test_labels)\r\n     16 print('Test accuracy:', test_acc)\r\n\r\n2 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)\r\n    689                        str(len(symbolic_weights)) +\r\n    690                        ' weights, but the saved weights have ' +\r\n--> 691                        str(len(weight_values)) + ' elements.')\r\n    692     weight_value_tuples += zip(symbolic_weights, weight_values)\r\n    693   K.batch_set_value(weight_value_tuples)\r\n\r\nValueError: Layer #1 (named \"batch_normalization_v1\" in the current model) was found to correspond to layer batch_normalization_v1 in the save file. However the new layer batch_normalization_v1 expects 7 weights, but the saved weights have 8 elements.\r\n```", "Looks like the `renorm_stddev_weight` variable was removed between 1.14 and 1.15 (https://github.com/tensorflow/tensorflow/commit/af15fb8624a0c0eabdd00ba1653cbbd4734c3b36). Accidentally breaking checkpoints is extremely easy. I would recommend that in the future, use the TensorFlow checkpoints (by specifying `save_format=tf`), which are more robust when dealing with variable changes.\r\n\r\nYou can migrating the Savefile following these steps (requires using different versions of TensorFlow).\r\n\r\n_Using TensorFlow 1.13 (or 1.14):_\r\n- Load the model using `model = tf.keras.models.load_model`\r\n- Save the model into two separate files:\r\n  - `model.save_weights('model_weights.ckpt', save_format='tf')`\r\n    The weights file will still contain the `renorm_stddev_weight` variable, but will be omitted when loading.\r\n  - ```\r\n      with open('model_config.json', 'w') as f:\r\n        f.write(model.to_json()) \r\n    ```\r\n\r\n_Using TensorFlow 1.15:_\r\n- Load the model:\r\n  ```\r\n  model = tf.keras.models.model_from_json(open('model_config.json').read())\r\n  model.load_weights('model_weights.ckpt')\r\n  ```\r\n  The batch normalization layer will not have the `renorm_stddev_weight` variable, but the rest of the variable values will be restored.\r\n- Now save the model by calling `model.save('model.h5')`\r\n\r\n@bcoopers I'm not super familiar with the BatchNormalization layer. I'm a bit concerned about correctness since a variable was entirely removed. If a model were trained using the `renorm_stddev_weight`, and then loaded without it, would it still function correctly?", "I have the same problem, but with the 2.0 beta1 and 2.0 release/tf-nightly-gpu..", "I found a temporary hack which allowed me to load the old model and get it saved into a compatible format.\r\n\r\nI modify the file:\r\ntensorflow_core/python/keras/saving/hdf5_format.py\r\n```\r\n# Temporary workaround to load old weights\r\nif len(weight_values) != len(symbolic_weights) and len(symbolic_weights)==7:\r\n        weight_values = weight_values[0:4] + [np.sqrt(weight_values[3])] + [weight_values[4]] + [weight_values[6]]\r\n\r\n# Rest of the code...\r\nif len(weight_values) != len(symbolic_weights):\r\n      raise ValueError('Layer #' + str(k) + ' (named \"' + layer.name +\r\n...\r\n...\r\n...\r\n```\r\n\r\n", "Hi!\r\nWith the drop weight, you got the same result? Such as, predict or resume train ", "So far it's looking correct...", "@ghannum \r\nIs this still an issue. Please feel free to close the issue if it is resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32398\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32398\">No</a>\n"]}, {"number": 32397, "title": "Support TensorRT 6", "body": "- Update TRT6 headerfiles\r\n- In TRT6, do not manually fall to fp16 in case of missing quant ranges\r\n- Remove WAR of CombinedNMS converter that shrinks last output dim", "comments": []}, {"number": 32396, "title": "The SessionOptions doesn't control the usage of CPU or Thread", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, only use C++ API\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SUSE 12.4\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc 4.8\r\n- CUDA/cuDNN version: only CPU\r\n- GPU model and memory: only CPU\r\n\r\n**Describe the current behavior**\r\nI created a session option with inter=1 and intra=1. Then I used this option to create a session and load a model.\r\nWhen I run the model, the program used almost all the CPU cores on my server, and thread spawned to several hundreds.\r\nAnd No matter what value I set to \"inter\" and \"intra\", there are still many CPU cores were used and many threads were created.\r\n\r\n**Describe the expected behavior**\r\nTF only use 1 thread and 1 cpu core\r\n\r\n**Code to reproduce the issue**\r\n  std::unique_ptr<tensorflow::Session> session;\r\n  tensorflow::SessionOptions options;\r\n  tensorflow::ConfigProto & config = options.config;\r\n  int coresToUse = 1;\r\n  if (coresToUse > 0)\r\n    {\r\n\t\tconfig.set_use_per_session_threads(false);\r\n\t\tconfig.set_inter_op_parallelism_threads(coresToUse);\r\n                config.set_intra_op_parallelism_threads(coresToUse);\t\r\n    }\r\n  status = tensorflow::NewSession(options, session);\r\ntensorflow::MetaGraphDef graph_def;\r\ntensorflow::ReadBinaryProto(tensorflow::Env::Default(), metamodel_path, &graph_def);\r\nsession->Create(graph_def.graph_def());\r\nrun_model();\r\n\r\n**Other info / logs**\r\nWhen I use the python API of TensorFlow 1.14 do the same thing, it seems like the CPU and thread usage can be controlled by the inter and intra parameter.\r\n\r\nHow to control the computing resources used by tensorflow C++ API?\r\n\r\n#32009 @JiayiFu @ezhulenev I saw the similar problem described here, but don't know its solution.", "comments": ["Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32395, "title": "AssertionError: Unreachable when adding or subtracting Dataset range element and tf.constant", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home Version\t10.0.18362 Build 18362\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nI'm trying to calculate the minimum index to take for extracting windows out of the dataset. I've got a constant `window_size` defined of which dtype defaults to `int32`. After running the following code it throws Assertion Error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nds = tf.data.Dataset.range(10)\r\nwindow_size = tf.constant(3)\r\n\r\ndef mapper(idx):\r\n    test = idx - window_size\r\n    return test\r\n\r\nds = ds.map(mapper)\r\n```\r\n\r\nthe same issue happens when I add or multiply (but not divide!).\r\nWhen dividing with this mapper:\r\n```\r\ndef mapper(idx):\r\n    test = idx / window_size\r\n    return test\r\n```\r\nI get `TypeError: x and y must have the same dtype, got tf.int64 != tf.int32`.\r\n\r\nThen I've tried casting `window_size` to `tf.int64`: `window_size = tf.constant(3, dtype=tf.int64)` and that fixed the issue in all cases. \r\n\r\n**Describe the expected behavior**\r\nI'd expect the error to be the same in case of addition/subtraction/multiplication as in case of division as it turns out to be the dtype issue.\r\n\r\n**Other info / logs**\r\n\r\n```\r\n  File \"C:/source/test_python_project/main.py\", line 14, in <module>\r\n    ds = ds.map(mapper)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1772, in map\r\n    MapDataset(self, map_func, preserve_cardinality=False))\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3190, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2555, in __init__\r\n    self._function = wrapper_fn._get_concrete_function_internal()\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1355, in _get_concrete_function_internal\r\n    *args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1349, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1652, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1545, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 715, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2549, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2489, in _wrapper_helper\r\n    ret = func(*nested_args)\r\n  File \"C:/source/test_python_project/main.py\", line 10, in mapper\r\n    test = idx - window_size\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 884, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 11574, in sub\r\n    \"Sub\", x=x, y=y, name=name)\r\n  File \"C:\\ProgramData\\Anaconda3\\envs\\test_python_project\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 621, in _apply_op_helper\r\n    assert False, \"Unreachable\"\r\nAssertionError: Unreachable\r\n```\r\n", "comments": ["I could reproduce the issue on Colab with TF 1.14.0. Please see the [gist here](https://colab.sandbox.google.com/gist/gadagashwini/3e14ff83be3cfa578cc4762b30d30e6a/untitled137.ipynb). Thanks", "This is fixed with TF 1.15. I tested with TF version  '1.15.0-dev20190821' in google colab.\r\nThe error message is more intuitive now.\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nds = tf.data.Dataset.range(10)\r\nwindow_size = tf.constant(3) #, dtype=tf.int64)\r\ndef mapper(idx):\r\n    test = idx - window_size\r\n    return test\r\nds = ds.map(mapper)\r\n```\r\nOutput:\r\n```python\r\n TypeError: Input 'y' of 'Sub' Op has type int32 that does not match type int64 of argument 'x'.\r\n```\r\nFurther casting ```window_size``` to ```tf.int64``` resolves the issue as you mentioned.\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32395\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32395\">No</a>\n"]}, {"number": 32394, "title": "Memory Leak in tf.keras.Model.fit ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nDuring training of a keras model with `fit`, the memory consumption is constantly increasing until the machine eventually runs out of memory.\r\n**Note:** The observed issue is only observed with `tensorflow` installed with `conda` or the one used in the AWS Deep Learning AMI that was optimized for the respective compute instance. If I just install `tensorflow` with `pip` the memory consumption does not increase until failure. Since I am not an expert on building `tensorflow` from source I can only guess that it has something to do with the way tensorflow was build from source by the package provider.\r\n\r\n**Describe the expected behavior**\r\nFor a small training dataset of a couple of MB, a machine with 16 GiB memory should not run out of memory.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nI used this code and a fresh conda environment to reproduce the issue:\r\n```bash\r\n$ conda create -n myenv python=3.6 tensorflow\r\n$ conda activate myenv\r\n```\r\n\r\n```python\r\n\"\"\"Example that reproduces the memory consumption increase during training.\"\"\"\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_model():\r\n    \"\"\"Build a simple logistic regression model.\"\"\"\r\n    x = tf.keras.Input((100,))\r\n    h = tf.keras.layers.Flatten()(x)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(h)\r\n    return tf.keras.Model(inputs=[x], outputs=[output])\r\n\r\n\r\ndef load_data():\r\n    \"\"\"Load some dummy data.\"\"\"\r\n    x = np.random.randn(1000000, 100).astype(np.float32)\r\n    y = np.random.choice([0, 1], size=(1000000, 1)).astype(np.float32)\r\n    return x, y\r\n\r\n\r\ndef train(model, x, y):\r\n    \"\"\"Train the model on some data.\"\"\"\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\r\n                  loss=tf.keras.losses.binary_crossentropy)\r\n    model.fit(x, y, epochs=10000, batch_size=1280)\r\n\r\n\r\nif __name__ == '__main__':\r\n    x, y = load_data()\r\n    model = build_model()\r\n    train(model, x, y)\r\n```\r\n**Other info / logs**\r\nHere are the logs from the tensorflow version installed with `conda` maybe it helps someone to reproduce how tensorflow was compiled and reproduce the issue:\r\n```\r\nOMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #213: KMP_AFFINITY: cpuid leaf 11 not supported - decoding legacy APIC ids.\r\nOMP: Info #149: KMP_AFFINITY: Affinity capable, using global cpuid info\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\r\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 4 threads/core (1 total cores)\r\nOMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 thread 0\r\nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 thread 1\r\nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 thread 2\r\nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 thread 3\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20360 thread 0 bound to OS proc set 0\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0910 21:18:07.612579 139883462330112 deprecation.py:506] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0910 21:18:07.634243 139883462330112 deprecation.py:323] From /home/ubuntu/anaconda3/envs/tfkerasbug/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n2019-09-10 21:18:07.824063: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2019-09-10 21:18:07.829994: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199910000 Hz\r\n2019-09-10 21:18:07.830112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e5e924d9b0 executing computations on platform Host. Devices:\r\n2019-09-10 21:18:07.830142: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-09-10 21:18:07.830228: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2019-09-10 21:18:07.848855: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20372 thread 1 bound to OS proc set 2\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20374 thread 2 bound to OS proc set 1\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20375 thread 3 bound to OS proc set 3\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20376 thread 4 bound to OS proc set 0\r\nKMP_AFFINITY: pid 20360 tid 20373 thread 5 bound to OS proc set 2\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20377 thread 6 bound to OS proc set 1\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20378 thread 7 bound to OS proc set 3\r\nOMP: Info #250: KMP_AFFINITY: pid 20360 tid 20379 thread 8 bound to OS proc set 0\r\n```\r\n", "comments": ["@dekromp ,\r\nWhen i tried installing tensorflow from `conda` and run the given code,I didn't face any error. Can you provide more info on the issue? Also try running the code on latest tensorflow version.Thanks!", "How long did you run it? It takes some time until the increase in memory is clearly visible. On an AWS ec2 m4.xlarge instance (Ubuntu 18.04) it growths roughly 6MB/s. I don't think that my minimal example will cause an error on your machine just a constant increase in memory consumption which you can observe when you look at it for  ~10 minutes.\r\nAnd as I said, I am not an expert in compiling tensorflow from source and do not know how `conda`/AWS compiled their version of tensorflow. By Installing `tensorflow` with `pip`, without any specific optimization everything is fine.", "Hi.\r\n\r\nI have the same problem. I also installed TF2.0 with `pip` in a `conda virtual environment` on my computer. I tried to implement DQN with TF2.0. I found the memory would increase if I used `model.fit`, but it not with `model.train_on_batch`.\r\n\r\nHere's my code:\r\n```\r\nfrom collections import deque\r\nimport random\r\nimport gym\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass DQN(object):\r\n    def __init__(self, state_dim, action_dim, lr, init_buffer_size=100):\r\n        self.step = 0\r\n        self.update_freq = 200\r\n        self.replay_buffer_size = 1000\r\n        self.replay_buffer = deque(maxlen=self.replay_buffer_size)\r\n        self.lr = lr\r\n        self.model = self._create_model(state_dim, action_dim)\r\n        self.target_model = self._create_model(state_dim, action_dim)\r\n        self._state_dim = state_dim\r\n        self._action_dim = action_dim\r\n        self._init_buffer_size = init_buffer_size\r\n\r\n    def _create_model(self, state_dim, action_dim):\r\n        inputs = tf.keras.Input(shape=state_dim)\r\n        fc1 = tf.keras.layers.Dense(512)(inputs)\r\n        q_outs = tf.keras.layers.Dense(action_dim)(fc1)\r\n\r\n        model = tf.keras.Model(inputs=inputs, outputs=q_outs)\r\n        model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(self.lr))\r\n\r\n        return model\r\n\r\n    def act(self, state, epsilon=0.1):\r\n        if np.random.uniform() < epsilon:\r\n            return np.random.choice(range(self._action_dim))\r\n        else:\r\n            return np.argmax(self.model(np.array([state])))\r\n\r\n    def save_model(self, file_path='MountainCar-v0-dqn.h5'):\r\n        print('model saved')\r\n        self.model.save(file_path)\r\n\r\n    def save_transition(self, s, a, r, next_s, terminated):\r\n        self.replay_buffer.append((s, a, r, next_s, terminated))\r\n\r\n    def train(self, batch_size=64, lr=1, gamma=0.99):\r\n        if len(self.replay_buffer) < self._init_buffer_size:\r\n            return\r\n\r\n        self.step += 1\r\n        \r\n        if self.step % self.update_freq == 0:\r\n            print(\"Copying weights.\")\r\n            self.target_model.set_weights(self.model.get_weights())\r\n        \r\n        # print(\"Buffer Length:{}\".format(len(self.replay_buffer)))\r\n\r\n        minibatch = random.sample(self.replay_buffer, batch_size)\r\n        s_batch = np.array([replay[0] for replay in minibatch])\r\n        next_s_batch = np.array([replay[3] for replay in minibatch])\r\n\r\n        \r\n        Q = self.model(s_batch).numpy()\r\n        target_Q = self.target_model(next_s_batch).numpy()\r\n\r\n        for i, exp in enumerate(minibatch):\r\n            _, a, r, _, done = exp\r\n            if done:\r\n                Q[i][a] = r\r\n            else:\r\n                Q[i][a] = r + gamma * np.amax(target_Q[i])\r\n\r\n        self.model.fit(s_batch, Q, verbose=0)\r\n        # self.model.train_on_batch(s_batch, Q)\r\n\r\ndef main():\r\n    env = gym.make('MountainCar-v0')\r\n    episodes = 10000  # train with 1000 episodes\r\n    score_list = []\r\n    agent = DQN(state_dim=env.observation_space.shape, action_dim=env.action_space.n, lr=0.001)\r\n    \r\n    for i in range(episodes):\r\n        s = env.reset()\r\n        score = 0\r\n        while True:\r\n            a = agent.act(s)\r\n            # env.render()\r\n            next_s, reward, done, _ = env.step(a)\r\n            agent.save_transition(s, a, reward, next_s, done)\r\n            agent.train()\r\n            score += reward\r\n            s = next_s\r\n            if done:\r\n                score_list.append(score)\r\n                print('episode:', i, 'score:', score, 'max:', max(score_list))\r\n                break\r\n        if np.mean(score_list[-10:]) > -160:\r\n            agent.save_model()\r\n            break\r\n        \r\n    env.close()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```", "We only control the pip package. If you're seeing an issue with conda but not pip I would suggest that you file an issue with conda.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32394\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32394\">No</a>\n", "Not really helpful, but thanks anyway.", "@robieta \r\n\r\nHi,  not sure why this got closed just because conda was used instead of pip. I'm also running into issues with this (using TF 2.0.0, CPU only on RedHat) and I installed with conda. I'd be happy to show some sample code to replicate and the metrics I've collected from trying to debug this memory lead issue.\r\n\r\nIf you all do not deal with the conda package....who does? And why would there be any difference, as long as the TF version is the same?", "i see this with pip too. memory usage keeps increasing for fit. ", "@ysyyork Can you please create a new issue with a standalone code to reproduce the error? Thanks!"]}, {"number": 32393, "title": "added deprecation warning", "body": "`tf.keras.experimental.export_saved_model` was deprecated as per warning generated by the code. But I don't see any deprecation message in the website. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/19db117b91d00cb815f81f85fd6146ea/keras_model_save_deprecation.ipynb) is a colab gist that shows the warning. Thanks!", "comments": ["We don't update release branches after the final release except in case of security vulnerability and only as needed for the vulnerability patch + patch release.\r\n\r\nAny other PR should go on master.\r\n\r\nClosing PR."]}, {"number": 32392, "title": "Jvishnuvardhan patch 4", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32392) for more info**.\n\n<!-- need_author_cla -->", "Closing as no signal can be extracted from this Pr."]}, {"number": 32391, "title": "Freeze gast==0.2.2 (#32319) in setup.py requirements.", "body": "This fixes breakage caused by minor update breaking tensorflow (#32319)\r\n\r\nPiperOrigin-RevId: 268261146", "comments": []}, {"number": 32390, "title": "Freeze gast==0.2.2 (#32319) in setup.py requirements.", "body": "This fixes breakage caused by minor update breaking tensorflow (#32319)\r\n\r\nPiperOrigin-RevId: 268261146", "comments": []}, {"number": 32389, "title": "[ROCm] enumerate ROCm GPU devices in grappler.", "body": "Co-authored with @jeffdaily .\r\n\r\nEnable enumerating ROCm GPU devices constructing grappler clusters.", "comments": ["@jaingaurav gentle ping for review", "@whchung  Could you please check reviewer comments and keep us posted. Thanks!", "Closing this PR as from empirical testing it doesn't bring merits."]}, {"number": 32388, "title": "tensorflow installation windows 10 python idle 3.6", "body": "C:\\WINDOWS\\system32>python\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\RAHUL\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\RAHUL\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n>>> exit()\r\n\r\nC:\\WINDOWS\\system32>rm -rf\r\n'rm' is not recognized as an internal or external command,\r\noperable program or batch file.", "comments": ["@rahul21rajendran ,\r\nLooks like there is some issue with installation of python and tensorflow, can you uninstall and try reinstalling the both? \r\nAlso you can refer the following #[7529](https://github.com/tensorflow/tensorflow/issues/7529) and [link](https://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example).Thanks!", "Hi guys, \r\nI am having a similar issue on Mac while using a virtualenv. Any help would be greatly appreaciated:\r\n```\r\n\r\n(mlocad) (base) youssouf-traores-macbook-pro:retraining Youssoufj$ python retrain.py --image_dir ~/flower_photos\r\nTraceback (most recent call last):\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.1.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.1.dylib\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"retrain.py\", line 131, in <module>\r\n    import tensorflow as tf\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation\r\n  Referenced from: /Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.1.dylib\r\n  Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security\r\n in /Users/Youssoufj/Projects/tensorf/retrain/september/retraining/mlocad/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.1.dylib\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "https://github.com/tensorflow/tensorflow/issues/32315 could be releated", "Thank you for your support\n\nOn Wed, 11 Sep 2019, 11:48 am oanush, <notifications@github.com> wrote:\n\n> @rahul21rajendran <https://github.com/rahul21rajendran> ,\n> Looks like there is some issue with installation of python and tensorflow,\n> can you uninstall and try reinstalling the both?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32388?email_source=notifications&email_token=AM4GRDSWBQZ7YCOYTOXLJA3QJCESZA5CNFSM4IVLMVY2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6NL6SI#issuecomment-530235209>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AM4GRDWWK7JRCHW2PX6JXU3QJCESZANCNFSM4IVLMVYQ>\n> .\n>\n", "@rahul21rajendran,\r\nCan you please let us know if the issue was resolved so that we can proceed for closure.Thanks!", "my issue is solved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32388\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32388\">No</a>\n", "https://github.com/tensorflow/tensorflow/issues/32627"]}, {"number": 32387, "title": "TypeError: 'Tensor' object does not support item assignment ,  in tf.while_loop", "body": "i am training an unsupervised CNN, for this, I defined a loss function in which CNN input is mapped to the cnn output through a complicated expression.  i am using  tf.while_loop to update the variables of \"gamma_tilde_tensor_local \" and \"D_mat_tensor_local \".  i have uploaded my code  & the error below.\r\nplease suggest me how to update tensor variables in while_loop. \r\n\r\nto avoid confusion, i have avoided the complex loops. originally  \"gamma_tilde_tensor_local \" size (batch_size, M,M,K,K).   \r\n\r\n\r\n```\r\ndef function(self):\r\n        gamma_tilde_tensor_local = tf.Variable(tf.zeros(shape = [self.mini_batchSize, self.num_users]),dtype=tf.float32)\r\n        D_mat_tensor_local = tf.Variable(tf.zeros(shape = [self.mini_batchSize, , self.num_users]), dtype=tf.float32)\r\n        condition_g1  = lambda k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local : k_iter1 < self.num_users\r\n        def body_gm1( k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local):\r\n            tf_x_k = tf.expand_dims(self.PHI_batch[:,:,k_iter1],axis=-1)          \r\n            tf_x_kd = tf.expand_dims(self.PHI_batch[:,:,k_iter2],axis=-1)        \r\n            phi_to_reduce =  tf.matmul( tf.transpose(tf_x_kd, perm=[0, 2, 1]), tf_x_k)\r\n            phi_t = tf.squeeze(phi_to_reduce,[1,2])\r\n            gamma_till = tf.divide(  tf.multiply( self.gammafun[:,1, 2 ], self.channel_Gain[:,1, k_iter1]), self.channel_Gain[:,1, 2])\r\n            \r\n            gamma_tilde_tensor_local[: k_iter1] = tf.multiply(phi_t, gamma_till )    \r\n            D_mat_tensor_local[:,k_iter1] = tf.sqrt(  tf.multiply( self.gammafun[1, 2], self.channel_Gain[1,k_iter1] ))                 \r\n              \r\n            return tf.add(k_iter1, 1), gamma_tilde_tensor_local, D_mat_tensor_local\r\n        k_iter1, gamma_tilde_tensor_local, D_mat_tensor_local = tf.while_loop(condition_g1, body_gm1, [0, gamma_tilde_tensor_local, D_mat_tensor_local])\r\n```\r\n\r\n**> TypeError: 'Tensor' object does not support item assignment\r\n>         gamma_till = tf.divide(  tf.multiply( self.gammafun[:,1, 2 ], self.channel_Gain[:,1, k_iter1]), self.channel_Gain[:,1, 2])**", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the full code snippet to reproduce the issue in our environment. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32387\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32387\">No</a>\n"]}, {"number": 32386, "title": "NVCC bug demonstration", "body": "Would like to submit this, but it crashes GPU.", "comments": ["@chsigg would you help to take a look at this?\r\nThanks.", "@chsigg Can you please review this PR ? Thanks!", "Brian, does this still crash the GPU?", "Just checked, it still crashes. `Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure`", "@sanjoy Can you please take a look on the above comment from @brianwa84. Thanks!", "fwiw, internally this is cl/330040611. I filed this issue to ensure nvidia visibility.", "@brianwa84 Any update on this PR? Please. Thanks!", "Closing pull request to avoid confusion.  As Brian stated above, this is just a placeholder to make some source code public."]}, {"number": 32385, "title": "TF 2.0 save-load SequenceFeatures layer issue", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution: **WIndows 10 Pro build 1903**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below): **2.0.0-alpha0**, **2.0.0-beta0**, **2.0.0-beta1**, **2.0.0-rc0**\r\n- Python version: **Python 3.7.3**\r\n\r\n**Describe the current behavior**\r\n\r\n1. I create model using Keras Functional API. The model includes `tf.keras.experimental.SequenceFeatures` layer and `sequence_categorical_column_with_vocabulary_list` feature_column.\r\n2. Then I save model by `tf.keras.models.model.save('path_to_model.h5')`. \r\n3. Finally, I load model by `tf.keras.models.load_model(path_to_model.h5)`. While loading I get the error message :\r\n\r\n> Unknown layer: SequenceFeatures\r\n\r\n**Describe the expected behavior**\r\nThe model containing SequenceFeatures layer and any corresponding feature_columns is loaded by `tf.keras.models.load_model(MODEL_PATH.h5)` without errors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column\r\n\r\nprint(tf.__version__)\r\n\r\n!python -V\r\n\r\n# Define categorical colunm for our text feature, which is preprocessed into sequence of tokens\r\ntext_column = feature_column.sequence_categorical_column_with_vocabulary_list(key='text', \r\n                                                                     vocabulary_list=list(['asd', 'asdf']))\r\n\r\ntext_embedding = feature_column.embedding_column(text_column, dimension=8)\r\n\r\n# Then define the layers and model it self\r\n# This example uses Keras Functional API instead of Sequential just for more generallity\r\n\r\n# Define SequenceFeatures layer to pass feature_columns into Keras model\r\nsequence_feature_layer = tf.keras.experimental.SequenceFeatures(text_embedding)\r\n\r\nmax_length = 6\r\n# Define inputs for each feature column. See\r\n# \u0441\u043c. https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-502218673\r\nsequence_feature_layer_inputs = {}\r\nsequence_feature_layer_inputs['text'] = tf.keras.Input(shape=(max_length,), name='text', dtype=tf.string)\r\n\r\nsequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)\r\nprint(sequence_feature_layer_outputs)\r\n\r\n# Define outputs of SequenceFeatures layer \r\n# And accually use them as first layer of the model\r\n\r\n# note here that SequenceFeatures layer produce tuple of two tensors as output. We need just first to pass next.\r\nsequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)\r\n\r\nx = tf.keras.layers.Conv1D(8,4)(sequence_feature_layer_outputs)\r\nx = tf.keras.layers.MaxPooling1D(2)(x)\r\nx = tf.keras.layers.Dense(256, activation='relu')(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nx= tf.keras.layers.GlobalAveragePooling1D()(x)\r\n# This example supposes binary classification, as labels are 0 or 1\r\nx = tf.keras.layers.Dense(1, activation='sigmoid')(x)\r\n\r\nmodel = tf.keras.models.Model(inputs=[v for v in sequence_feature_layer_inputs.values()], outputs=x)\r\n\r\nmodel.summary()\r\n\r\n# This example supposes binary classification, as labels are 0 or 1\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy']\r\n              #run_eagerly=True\r\n             )\r\n\r\nmodel.save('model.h5')\r\nmodel_loaded = tf.keras.models.load_model('model.h5')\r\n\r\nmodel_loaded.summary()\r\n# expected result: model_loaded is instantinated\r\n# actual result: exception with text: \"Unknown layer: SequenceFeatures\"\r\n\r\n#other way to save model structure also finishes with error\r\n\r\nmodel_json_string = model.to_json()\r\n# import pprint\r\n# pprint.pprint(json.loads(model_json_string))\r\nfresh_model = tf.keras.models.model_from_json(model_json_string)\r\nfresh_model.summary()\r\n# expected result: model_loaded is instantinated\r\n# actual result: exception with text: \"Unknown layer: SequenceFeatures\"\r\n```\r\n\r\n**Other info / logs**\r\nI have found that this problem is fixed by changes in commit [a29555a8fed1b2a1eb495e220eca7b3e3161ea26](https://github.com/tensorflow/tensorflow/commit/a29555a8fed1b2a1eb495e220eca7b3e3161ea26#diff-66f6b57cc123613d5703a087b48cdb48) and do not reproduced in **tf-nightly-2.0-preview** build. But it still reproduced in **2.0.0-alpha0**, **2.0.0-beta0**, **2.0.0-beta1**, **2.0.0-rc0**\r\n", "comments": ["@EgorBEremeev The recommended format for saving is SavedModel. Can you try using that instead of Hdf5? To save to SavedModel, specify a path without the \".h5\" extension, or set the `save_format` option to `tf`.\r\n\r\ntf-nightly is more up to date than 2.0.0-alpha/beta/rc0, so the issue should be fixed in the next release.", "@k-w-w , I have tried SavedModel in stable release 2.0 and althogh save\\load methods finishes without error the loaded model is corrupted. Specifically\r\n`model_loaded.summary()` aborted with \r\n\r\n> ValueError: You tried to call `count_params` on text, but the layer isn't built. You can build it manually via: `text.build(batch_input_shape)`\r\n\r\nIf I add\r\n`model_loaded.compile()` it is aborted with \r\n\r\n> ValueError: The model cannot be compiled because it has no loss to optimize. \r\n\r\nI reproduced this with the code I gave above, where I just replace\r\n```\r\nmodel.save('model.h5')\r\nmodel_loaded = tf.keras.models.load_model('model.h5')\r\n```\r\n\r\nwith\r\n```\r\nmodel.save('model_as_SavedModel', save_format='tf')\r\nmodel_loaded = tf.keras.models.load_model('model_as_SavedModel')\r\n```\r\nHowever, in the last nightly build it is not reproduced. Is there a way to push working code for SequenceFeatures layer at least to alpha or beta releases, and so it become available on the google cloud platform?", "I tried your code on TF 2.2 without any problems"]}, {"number": 32384, "title": "tensorboard gives ValueError: Duplicate plugins for name projector", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.5\r\n- TensorFlow installed from (source or binary): `pip install tensorflow==1.14.0`\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.4 with virtualenv\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to run `tensorboard --logdir=.` I get this ValueError: Duplicate plugins for name projector\r\n**Describe the expected behavior**\r\nTensorboard should launch\r\n**Code to reproduce the issue**\r\n`tensorboard --logdir=.` \r\n\r\n**Other info / logs**\r\nIdeally, I would ultimately like to run ` tensorboard --logdir=gs://[bucket_name]`, but I have not even been able to use tensorboard for models with logdirs that are local. \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/len/research/.venv/bin/tensorboard\", line 10, in <module>\r\n    sys.exit(run_main())\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/main.py\", line 64, in run_main\r\n    app.run(tensorboard.main, flags_parser=tensorboard.configure)\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/program.py\", line 228, in main\r\n    server = self._make_server()\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/program.py\", line 309, in _make_server\r\n    self.assets_zip_provider)\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 161, in standard_tensorboard_wsgi\r\n    reload_task)\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 194, in TensorBoardWSGIApp\r\n    return TensorBoardWSGI(plugins, path_prefix)\r\n  File \"/Users/len/research/.venv/lib/python3.7/site-packages/tensorboard/backend/application.py\", line 245, in __init__\r\n    raise ValueError('Duplicate plugins for name %s' % plugin.plugin_name)\r\nValueError: Duplicate plugins for name projector\r\n```", "comments": ["@ljstrnadiii Please post tensorboard related questions in [tensorboard repo here](https://github.com/tensorflow/tensorboard/issues). When you post there, please post the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32384\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32384\">No</a>\n", "Refer to https://github.com/pytorch/pytorch/issues/22676"]}, {"number": 32383, "title": "tf Function could not able to transformed into graph.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["WARNING:tensorflow:Entity <function train.<locals>.train_step at 0x000002B43293D0D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train.<locals>.train_step at 0x000002B43293D0D0>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002B94351E598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002B94351E598>: AttributeError: module 'gast' has no attribute 'Num'\r\nWARNING:tensorflow:Entity <function train.<locals>.train_step at 0x000002B43293D0D0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function train.<locals>.train_step at 0x000002B43293D0D0>: AssertionError: Bad argument number for Name: 3, expecting 4\r\nStep 3078/40000 - training_loss 0.61717", "\r\n== check python ===================================================\r\npython version: 3.6.8\r\npython branch: tags/v3.6.8\r\npython build version: ('tags/v3.6.8:3c6b436a57', 'Dec 24 2018 00:16:47')\r\npython compiler version: MSC v.1916 64 bit (AMD64)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Windows\r\nos kernel version: 10.0.17134\r\nos release version: 10\r\nos platform: Windows-10-10.0.17134-SP0\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Windows', node='DONTCARE', release='10', version='10.0.17134', machine='AMD64', processor='Intel64 Family 6 Model 79 Stepping 1, GenuineIntel')\r\narchitecture: ('64bit', 'WindowsPE')\r\nmachine: AMD64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== check pips ===================================================\r\nnumpy                1.17.2              \r\nprotobuf             3.9.1               \r\ntensorflow           2.0.0rc0            \r\ntensorflow-gpu       2.0.0b1             \r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.0.0-beta1\r\ntf.version.GIT_VERSION = unknown\r\ntf.version.COMPILER_VERSION = MSVC 190024215\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Users\\MHN9\\.virtualenvs\\src-wzStTweO\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nTue Sep 10 16:26:43 2019       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 431.70       Driver Version: 431.70       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P6000       WDDM  | 00000000:03:00.0 Off |                  Off |\r\n| 26%   46C    P8    10W / 250W |   1667MiB / 24576MiB |      6%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro P6000       WDDM  | 00000000:04:00.0 Off |                  Off |\r\n| 26%   45C    P8     8W / 250W |    152MiB / 24576MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0       524    C+G   ...x64__8wekyb3d8bbwe\\Microsoft.Photos.exe N/A      |\r\n|    0      1652    C+G   Insufficient Permissions                   N/A      |\r\n|    0      1852    C+G   Insufficient Permissions                   N/A      |\r\n|    0      1864    C+G   Insufficient Permissions                   N/A      |\r\n|    0      4516    C+G   Insufficient Permissions                   N/A      |\r\n|    0      6488    C+G   Insufficient Permissions                   N/A      |\r\n|    0      6584    C+G   Insufficient Permissions                   N/A      |\r\n|    0      7192    C+G   C:\\Windows\\explorer.exe                    N/A      |\r\n|    0      7996    C+G   Insufficient Permissions                   N/A      |\r\n|    0      9940    C+G   Insufficient Permissions                   N/A      |\r\n|    0     10780    C+G   ...51.72.0_x64__kzf8qxf38zg5c\\SkypeApp.exe N/A      |\r\n|    0     11372    C+G   Insufficient Permissions                   N/A      |\r\n|    0     11736    C+G   Insufficient Permissions                   N/A      |\r\n|    0     12188    C+G   ...dows.Cortana_cw5n1h2txyewy\\SearchUI.exe N/A      |\r\n|    0     12668    C+G   ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A      |\r\n|    0     12808    C+G   Insufficient Permissions                   N/A      |\r\n|    0     13240    C+G   Insufficient Permissions                   N/A      |\r\n|    0     13248    C+G   Insufficient Permissions                   N/A      |\r\n|    0     13352    C+G   Insufficient Permissions                   N/A      |\r\n|    0     14160    C+G   Insufficient Permissions                   N/A      |\r\n|    0     15076    C+G   Insufficient Permissions                   N/A      |\r\n|    0     16148    C+G   Insufficient Permissions                   N/A      |\r\n|    0     16644    C+G   ...Files (x86)\\Mozilla Firefox\\firefox.exe N/A      |\r\n|    0     17172    C+G   ...Files (x86)\\Mozilla Firefox\\firefox.exe N/A      |\r\n|    0     17920    C+G   ...Files (x86)\\Mozilla Firefox\\firefox.exe N/A      |\r\n|    0     18840    C+G   Insufficient Permissions                   N/A      |\r\n|    0     20572    C+G   ...sktop App\\AcWebBrowser\\AcWebBrowser.exe N/A      |\r\n|    0     21096    C+G   Insufficient Permissions                   N/A      |\r\n|    0     21276    C+G   ...x86)\\Microsoft Office\\Office16\\lync.exe N/A      |\r\n|    0     21464    C+G   Insufficient Permissions                   N/A      |\r\n|    0     24792    C+G   Insufficient Permissions                   N/A      |\r\n|    0     24976    C+G   Insufficient Permissions                   N/A      |\r\n|    0     28100    C+G   Insufficient Permissions                   N/A      |\r\n|    0     28564    C+G   Insufficient Permissions                   N/A      |\r\n|    0     28624    C+G   Insufficient Permissions                   N/A      |\r\n|    0     28800    C+G   Insufficient Permissions                   N/A      |\r\n|    0     30628    C+G   Insufficient Permissions                   N/A      |\r\n|    0     31052    C+G   Insufficient Permissions                   N/A      |\r\n|    0     31064    C+G   Insufficient Permissions                   N/A      |\r\n|    0     31252    C+G   Insufficient Permissions                   N/A      |\r\n|    0     32148    C+G   Insufficient Permissions                   N/A      |\r\n|    0     32392    C+G   Insufficient Permissions                   N/A      |\r\n|    0     32656    C+G   Insufficient Permissions                   N/A      |\r\n|    0     33036    C+G   F:\\software\\Microsoft VS Code\\Code.exe     N/A      |\r\n|    0     35172    C+G   ...Files (x86)\\Mozilla Firefox\\firefox.exe N/A      |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.0.0rc0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\mhn9\\.virtualenvs\\src-wzsttweo\\lib\\site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 8, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n", "I have the same problem when trying  a simple AutoGraph example form the 2.0 documentation.\r\n\r\nI have tested in both python 3.7.2 and 3.5.6 both with tensorflow==2.0.0rc0.\r\n\r\nA working colab example is found here:\r\nhttps://colab.research.google.com/drive/1uDkPGwGd3l7Gv3-CCmELVsStlE4n-ndL\r\n\r\nI tried to run the exact same python code locally, giving same issue with\r\n\"Cause: Bad argument number for Name: 3, expecting 4\"\r\n\"OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\"\r\n\r\nSee appended files for source code and full stdout + stderr.\r\n\r\n[err.txt](https://github.com/tensorflow/tensorflow/files/3599222/err.txt)\r\n\r\n[test_autograph.py.txt](https://github.com/tensorflow/tensorflow/files/3599230/test_autograph.py.txt)\r\n", "@hussain7 and @daniel-falk, Thanks for reporting the issue.\r\nI could replicate the issue on my local system while it is working as expected on Colab. ", "Same is happening to me (on Colab but not on local):\r\n\r\nWARNING:tensorflow:Entity <..> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <...> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n[...] Repeated on sucesive functions", "I am also experiencing this issue on my local machine.", "My problem looks like it is originating from `tensorflow_core/python/autograph/pyct/templates.py` in the following function\r\n\r\n```python\r\ndef _convert_to_ast(n):\r\n  \"\"\"Converts from a known data type to AST.\"\"\"\r\n  # Note: When generating AST nodes from strings/QNs in isolation, ctx is\r\n  # unknown. ctx must be filled in according to the template being used.\r\n  # See ReplaceTransformer.visit_Name.\r\n  if isinstance(n, str):\r\n    return gast.Name(id=n, ctx=None, annotation=None)\r\n  if isinstance(n, qual_names.QN):\r\n    return n.ast()\r\n  if isinstance(n, list):\r\n    return [_convert_to_ast(e) for e in n]\r\n  if isinstance(n, tuple):\r\n    return tuple(_convert_to_ast(e) for e in n)\r\n  return n\r\n```\r\nProblematic line: \r\n```python\r\n return gast.Name(id=n, ctx=None, annotation=None)\r\n```\r\nThe `gast.Name()` call (I have no prior experience with `gast`) seems to be complaining because it expects `type_comment` argument. It is checking for 4 arguments (`'id', 'ctx', 'annotation', 'type_comment'`) but it is only getting three.\r\n\r\nThis is from my `gast.py` where it states which arguments `gast.Name` should recive\r\n```python\r\n    'Name': (('id', 'ctx', 'annotation', 'type_comment'),\r\n```\r\n", "I was using `gast 0.3.1` but switching to version `0.2.2` seems to have fixed this for me.", "right, switching to gast 0.2 version seems work for me as well.\n\nOn Wed, Sep 11, 2019 at 11:34 PM tsimons89 <notifications@github.com> wrote:\n\n> I was using gast 0.3.1 but switching to version 0.2.2 seems to have fixed\n> this for me.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32383?email_source=notifications&email_token=ABILJBMKRDBTOJ256JVB6KTQJFP7LA5CNFSM4IVILF5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6P6HKY#issuecomment-530572203>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABILJBICRCH6Q6ZQ6FOLYEDQJFP7LANCNFSM4IVILF5A>\n> .\n>\n", "use gast 0.2.2 sovle my issue as well.", "I also experienced this bug and finally sovled by switching to gast 0.2.2.", "I face this error `Bad argument number for Name: 3, expecting 4` too.\r\nIt happens to me when using this on Colab:\r\n\r\n```\r\n%tensorflow_version 2.x\r\n```\r\n\r\nIt works when using:\r\n\r\n```\r\n!pip install -q tensorflow==2.0.0-rc0\r\n!pip freeze\r\n```", "Similar error, gast 0.2.2 also solved for me ", "> I was using `gast 0.3.1` but switching to version `0.2.2` seems to have fixed this for me.\r\nCould you tell me how to switch the version\uff1f", "pip install gast==0.2.2\n\nOn Sun, Oct 6, 2019 at 8:28 AM lrxiao <notifications@github.com> wrote:\n\n> I was using gast 0.3.1 but switching to version 0.2.2 seems to have fixed\n> this for me.\n> Could you tell me how to switch the version\uff1f\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/32383?email_source=notifications&email_token=ABILJBMVOADWICCOM33UV2DQNGAQZA5CNFSM4IVILF5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEAOCO6A#issuecomment-538716024>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABILJBISDWLJFCLN2BZWDQLQNGAQZANCNFSM4IVILF5A>\n> .\n>\n", "Duplicate of #32319 , which was fixed by c72125bd59858ec82a9238b232bbd77c45889c5a#diff-739fc4f018f5288972ae5826b15c36e7\r\n\r\nHowever, [according to](https://github.com/tensorflow/tensorflow/issues/32319#issuecomment-537568070) @mihaimaruseac :\r\n> New update:\r\n> \r\n> `gast` has been pinned to 0.2.2 for 1.15 and 2.0 and later releases as well as nightly.\r\n> TF 1.14 or earlier don't have `gast` pinned and won't get it as we are not doing more releases there (except for security vulnerabilities).\r\n> \r\n> If you encounter this issue on 1.14 or earlier, please install gast==0.2.2 using pip before installing tensorflow.", "Closing as duplicate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32383\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32383\">No</a>\n", "switch to gast==0.2.2 solve the problem for me as well.", "I can confirm a switch to `gast==0.2.2` works. Is there additional information which would help debugging and resolving this?\r\n", "I have solved this problem,thank you!\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"Dan Grahn\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2019\u5e7410\u670831\u65e5(\u661f\u671f\u56db) \u665a\u4e0a6:29\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u67ad\u9f99\"<1447421123@qq.com&gt;;\"Comment\"<comment@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] tf Function could not able to transformed into graph. (#32383)\r\n\r\n\r\n\r\n\r\nI can confirm a switch to gast==0.2.2 works. Is there additional information which would help debugging and resolving this?\r\n \r\n\u2014\r\nYou are receiving this because you commented.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "Even after upgrading `gast` does not solve my issue.\r\n\r\n> pip install gast==0.2.2\r\n> Requirement already satisfied: gast==0.2.2 in /home/chandu/ckm/py3/lib/python3.5/site-packages (0.2.2)\r\n> WARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\r\n> You should consider upgrading via the 'pip install --upgrade pip' command.\r\n> (py3) chandu@chandu-lenovo:~/ckm/code/text-mining/chandresh-code$ python\r\n> Python 3.5.2 (default, Oct  8 2019, 13:06:37) \r\n> [GCC 5.4.0 20160609] on linux\r\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n> >>> import tensorflow as tf\r\n> >>> tf.__version__\r\n> '2.0.0'\r\n> ", "Can you reopen a new issue and fill in template, please? This issue has been closed and won't get updated anymore, since there are so many opened issue"]}, {"number": 32382, "title": "Audio WAV files from iOS are not read by Tensorflow audio_ops.decode_wav: Header mismatch: Expected fmt  but found JUNK", "body": "I have an iOS app that records audio as wav files and sends the files to the server for audio recognition. The iOS app records audio as wav files with the following configurations \r\n```\r\nlet settings = [\r\n                    AVFormatIDKey: Int(kAudioFormatLinearPCM),\r\n                    AVSampleRateKey: 16000,\r\n                    AVNumberOfChannelsKey: 1,\r\n                    AVLinearPCMBitDepthKey: 16,\r\n                    AVEncoderAudioQualityKey:AVAudioQuality.max.rawValue\r\n                    ] as [String : Any]\r\n```\r\nThe files are playing with any media player. But when i try to read the files with tensorflow ops in the server i am getting the following error\r\n\r\n```\r\nInvalidArgumentError: Header mismatch: Expected fmt  but found JUNK\r\n\t [[node DecodeWav_3 (defined at <ipython-input-5-36afd53919f4>:4) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node DecodeWav_3:\r\n ReadFile_3 (defined at <ipython-input-5-36afd53919f4>:3)\r\n\r\nOriginal stack trace for 'DecodeWav_3':\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 505, in start\r\n    self.io_loop.start()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\r\n    self.asyncio_loop.run_forever()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\r\n    self._run_once()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\r\n    handle._run()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/asyncio/events.py\", line 145, in _run\r\n    self._callback(*self._args)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\r\n    self.run()\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\r\n    yielded = self.gen.send(value)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 272, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 542, in execute_request\r\n    user_expressions, allow_stdin,\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 294, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2855, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\r\n    return runner(coro)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3058, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-5-36afd53919f4>\", line 4, in <module>\r\n    data = audio_ops.decode_wav(audio_binary, desired_channels=1)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/ops/gen_audio_ops.py\", line 227, in decode_wav\r\n    desired_samples=desired_samples, name=name)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/minimaci73/anaconda3/envs/Samples/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\nI couldn't find the reason for the crash. My TensorFlow code to read the files are as below\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.framework.python.ops import audio_ops\r\nfile = \"/Users/minimaci73/Downloads/1567690231185.wav\"  #wav file path\r\naudio_binary = tf.read_file(file)\r\ndata = audio_ops.decode_wav(audio_binary, desired_channels=1)\r\npcm16_data = tf.Session().run(data)\r\npcm16_data = pcm16_data[0] \r\nprint(pcm16_data)\r\n```\r\n\r\nOS - OSX Mojave 10.14.6\r\nTensorflow Version: 14\r\nEnvironment - Anaconda VM", "comments": ["I'm also getting what appears to be the same issue using tf2 in the Windows subsystem for Linux. There's a stackoverflow question about this here: https://stackoverflow.com/questions/58295764/import-wav-file-in-tensorflow-2", "> I'm also getting what appears to be the same issue using tf2 in the Windows subsystem for Linux. There's a StackOverflow question about this here: https://stackoverflow.com/questions/58295764/import-wav-file-in-tensorflow-2\r\n\r\nI still couldn't fix this. So instead I adjusted the model to take Float32 inputs and then used [librosa](https://librosa.github.io/librosa/) to read the data from the file in float32 and then used this float array as input to the model. But I am not sure whether the model still trains the same way as while using `decode_wav`. ", "See comment in https://github.com/tensorflow/tensorflow/issues/26247#issuecomment-623155920 , it is available in https://github.com/tensorflow/io with ` tfio.audio.decode_wav` in latest nightly build:\r\n```\r\npip install tensorflow-io-nightly\r\n```\r\n\r\nI will close this issue, but please feel free to open a new issue in https://github.com/tensorflow/io if encountering any problems."]}, {"number": 32381, "title": "cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.", "body": "After installing Cuda 10.1 and CuDNN, I am getting above error when testing if tensorflow 2.0 can recognize my GPU, I am using a GTX 1060, on Windows 10.\r\n\r\nI am trying to run:\r\n`tf.test.is_gpu_available(\r\n    cuda_only=False,\r\n    min_cuda_compute_capability=None\r\n)`", "comments": ["TF 2.0 supports cuda 10.0 Please switch to cuda 10.0 and update cuda paths.\r\nSee [software requirements](https://www.tensorflow.org/install/gpu#software_requirements)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32381\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32381\">No</a>\n", "CUDA 10.1 is still not working?\r\n", "> CUDA 10.1 is still not working?\r\n\r\nNo, bro.", "> > CUDA 10.1 is still not working?\r\n> \r\n> No, bro.\r\n\r\nSame for the TF 1.14? ", "I have the same error while running a deep-learning Keras R-script with tensorflow 2.0 using a GTX 1060, on Windows 10.\r\n\r\nI don't know my CUDA version.\r\n\r\nI did use these steps to install Tensorflow-GPU which was running correct with the Jupyter example:\r\n\r\nhttps://www.thehardwareguy.co.uk/install-tensorflow-gpu\r\n\r\nAfter this:\r\nI installed rstudio in the environment.\r\nI installed Keras\r\nI runned the deep learing Keras R-script.\r\n\r\nThe resulting error is:\r\n\r\n```\r\n\r\n2019-11-12 21:41:52.691280: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n\r\n2019-11-12 21:41:55.281028: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-11-12 21:41:55.305135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:01:00.0\r\n2019-11-12 21:41:55.305460: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-12 21:41:55.306272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-11-12 21:41:55.308379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715\r\npciBusID: 0000:01:00.0\r\n2019-11-12 21:41:55.308674: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-11-12 21:41:55.309506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n Show Traceback\r\n \r\n Rerun with Debug\r\n Error in py_call_impl(callable, dots$args, dots$keywords) : \r\n  InternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found. \r\n```", "If I am correct my CUDA version is 10.020. Pls see below:\r\n\r\n\r\n```\r\n__Python Information__\r\nPython Compiler                               : MSC v.1915 64 bit (AMD64)\r\nPython Implementation                         : CPython\r\nPython Version                                : 3.6.9\r\nPython Locale                                 : en_NL cp1252\r\n\r\n__LLVM information__\r\nLLVM version                                  : 8.0.0\r\n\r\n__CUDA Information__\r\nFound 1 CUDA devices\r\nid 0    b'GeForce GTX 1060 6GB'                              [SUPPORTED]\r\n                      compute capability: 6.1\r\n                           pci device id: 0\r\n                              pci bus id: 1\r\nSummary:\r\n        1/1 devices are supported\r\nCUDA driver version                           : 10020\r\nCUDA libraries:\r\nFinding cublas from <unavailable>\r\n        ERROR: can't locate lib\r\nFinding cusparse from <unavailable>\r\n        ERROR: can't locate lib\r\nFinding cufft from <unavailable>\r\n        ERROR: can't locate lib\r\nFinding curand from <unavailable>\r\n        ERROR: can't locate lib\r\nFinding nvvm from <unavailable>\r\n        ERROR: can't locate lib\r\nFinding libdevice from <unavailable>\r\n        searching for compute_20...     ERROR: can't open libdevice for compute_20\r\n        searching for compute_30...     ERROR: can't open libdevice for compute_30\r\n        searching for compute_35...     ERROR: can't open libdevice for compute_35\r\n        searching for compute_50...     ERROR: can't open libdevice for compute_50\r\n```", "Wtf seriously? When is support for cuda 10.1 planned?", "copying only `cudart64_100.dll` from a 10.0 installation into the 10.1 `bin` folder seems to work (as a workaround until 10.1 support is added)", "I have cuda with version 10.0.130 and I get the same error with a Gtx 1070 on Windows 10.", "> copying only `cudart64_100.dll` from a 10.0 installation into the 10.1 `bin` folder seems to work (as a workaround until 10.1 support is added)\r\n\r\nThis worked for me too. Then for all other functions that are deprecated, I used tf.compat.v1 or tf.compat.v2", "> I have cuda with version 10.0.130 and I get the same error with a Gtx 1070 on Windows 10.\r\n\r\nWere you able to fix it? I am facing the same problems, have the same config as yours.", "Same for me, please fix it", "i have the same error !! did any one fix it?", "I have get the same error too!who can help me?", "I got it working by using CUDA10.0 instead of CUDA10.1. Note that you can have two CUDA versions at the same time, make sure your CUDA_PATH is pointing to CUDA10.0.\r\n![twoCUDAatTheSameTime](https://user-images.githubusercontent.com/30103695/71420615-3183a780-26b1-11ea-8fc1-90a36b5ddced.PNG)\r\n\r\nYou can download archived CUDA10.0 from [here](https://developer.nvidia.com/cuda-10.0-download-archive?target_os=Windows&target_arch=x86_64&target_version=10&target_type=exelocal)\r\n\r\nyou can verify if you GPU is available for training with this code snippet \r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\n```\r\n\r\nreference [here](https://medium.com/@choowilson93/how-to-verify-cuda-and-cudnn-installation-d56a89bab0a8)\r\n", "I got the same problem while running my Keras code in Jupyter Notebook....\r\nthen uninstalling all Keras and tensorflow related packages and reinstalling them in a virtual environment, and creating a new kernel for jupyter note book to run this virtual environment helped me to solve this issue", "I have the same problem, using tensorflow-gpu==2.0 and CUDA10.0, Graphic card is RTX 2070:\r\n\r\nprint(device_lib.list_local_devices())\r\n2020-02-14 00:35:15.639841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:01:00.0\r\n2020-02-14 00:35:15.643490: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-02-14 00:35:15.646311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\fredd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\client\\device_lib.py\", line 41, in list_local_devices\r\n    for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n  File \"C:\\Users\\fredd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 2249, in list_devices\r\n    return ListDevices()\r\ntensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: cudaGetErrorString symbol not found.\r\n\r\nCan anybody help?"]}, {"number": 32380, "title": "BatchNormalization virtual_batch_size does not work with None in input shape", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\nv2.0.0-beta1-5101-gc75bb66 2.0.0-rc0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.6.2.24-1\r\n- GPU model and memory: Nvidia RTX 2070 8 GB\r\n\r\n**Describe the current behavior**\r\nA constructor of a tf.keras Model that uses `tf.keras.layers.BatchNormalization` with `virtual_batch_size` set and unspecified input shape dimensions throws an exception.\r\n\r\n**Describe the expected behavior**\r\nSuch a model should be usable.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ninp = tf.keras.layers.Input(shape=(None, None, 3))\r\nnet = tf.keras.layers.BatchNormalization(virtual_batch_size=8)(inp)\r\n\r\nmodel = tf.keras.Model(inputs=inp, outputs=net)\r\n```\r\n\r\n**Other info / logs**\r\nTraceback of the exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 541, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 541, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/compat.py\", line 71, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got 8\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/test_virtual_batch.py\", line 6, in <module>\r\n    net = tf.keras.layers.BatchNormalization(virtual_batch_size=8)(inp)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 802, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/normalization.py\", line 652, in call\r\n    inputs = array_ops.reshape(inputs, expanded_shape)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py\", line 131, in reshape\r\n    result = gen_array_ops.reshape(tensor, shape, name)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 8117, in reshape\r\n    \"Reshape\", tensor=tensor, shape=shape, name=name)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 530, in _apply_op_helper\r\n    raise err\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 527, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py\", line 265, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/home/ikrets/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 545, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [8, -1, None, None, 3]. Consider casting elements to a supported type.\r\n```", "comments": ["I replicate the issue with TF 2.0.0.rc0. Please take a look at [gist here](https://colab.sandbox.google.com/gist/gadagashwini/514760082d2c9017a6a4cd51977e3a51/untitled138.ipynb). Thanks!", "Added a PR #39131 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32380\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32380\">No</a>\n"]}, {"number": 32378, "title": "*del*", "body": "*del*", "comments": ["You know Github stores history, right? Editing to hide stuff just brings it back to light. See [Streisand effect](https://en.wikipedia.org/wiki/Streisand_effect)"]}, {"number": 32377, "title": "TF2.0  AutoGraph issue", "body": "OS:Ubuntu 18.04\r\nTensorFlow Version:tensorflow-gpu == 2.0.0-rc0\r\nissue:AutoGraph\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd89803cef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fd89803cef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3594563/tf_env.txt)", "comments": ["CUDA Version: CUDA 10.0", "You should add your code as well. I had the same Issue, but the cause was bug in my code, not of tensorflow.", "You can try  `pip install gast==0.2.2` and see if it resolves your issue.\r\n\r\nPlease, go through the below link and see if it helps you.Thanks!\r\n#32319", "@ravikyram @2649 \r\nThe problem has been resolved after installing gast. Thanks!", "Got the same issue, it was resolved after installing gast 0.2.2. Previous gast version was 0.3.2", "OS:Ubuntu 16.04\r\nTensorFlow Version:tensorflow-gpu == 2.0.0-rc1\r\nCUDA:10.0\r\nissue:AutoGraph\r\n\r\nINFO:tensorflow:Converted call: <function simple_nn_layer at 0x7f6de177c598>\r\n    args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n    kwargs: {}\r\n\r\nConverted call: <function simple_nn_layer at 0x7f6de177c598>\r\n    args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"<ipython-input-2-15699e84f48b>\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\nEntity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"<ipython-input-2-15699e84f48b>\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\nINFO:tensorflow:Converting <function simple_nn_layer at 0x7f6de177c598>\r\nConverting <function simple_nn_layer at 0x7f6de177c598>\r\nINFO:tensorflow:Source code of <function simple_nn_layer at 0x7f6de177c598>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n@tf.function\r\ndef simple_nn_layer(x, y):\r\n    return tf.nn.relu(tf.matmul(x, y))\r\n\r\n\r\nSource code of <function simple_nn_layer at 0x7f6de177c598>:\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n@tf.function\r\ndef simple_nn_layer(x, y):\r\n    return tf.nn.relu(tf.matmul(x, y))\r\n\r\n\r\nINFO:tensorflow:Error transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\nTraceback (most recent call last):\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n    free_nonglobal_var_names)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\r\n    entity, program_ctx)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\r\n    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast\r\n    node = node_to_graph(node, context)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph\r\n    node = converter.apply_(node, context, function_scopes)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n    node = converter_module.transform(node, context)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n    return FunctionBodyTransformer(ctx).visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n    node = self.generic_visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 308, in generic_visit\r\n    value = self.visit(value)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n    return super(Base, self).visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n    result = super(Base, self).visit(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n    return visitor(node)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n    value=node.value)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n    replacements[k] = _convert_to_ast(replacements[k])\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n    return gast.Name(id=n, ctx=None, annotation=None)\r\n  File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/gast/gast.py\", line 19, in create_node\r\n    format(Name, nbparam, len(Fields))\r\nAssertionError: Bad argument number for Name: 3, expecting 4\r\nError transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\nWARNING:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\nWARNING: Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4", "same issue fixed with `pip install gast=0.2.2`", "> OS:Ubuntu 16.04\r\n> TensorFlow Version:tensorflow-gpu == 2.0.0-rc1\r\n> CUDA:10.0\r\n> issue:AutoGraph\r\n> \r\n> INFO:tensorflow:Converted call: <function simple_nn_layer at 0x7f6de177c598>\r\n> args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n> kwargs: {}\r\n> \r\n> Converted call: <function simple_nn_layer at 0x7f6de177c598>\r\n> args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n> kwargs: {}\r\n> \r\n> INFO:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\n> Entity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\n> INFO:tensorflow:Converting <function simple_nn_layer at 0x7f6de177c598>\r\n> Converting <function simple_nn_layer at 0x7f6de177c598>\r\n> INFO:tensorflow:Source code of <function simple_nn_layer at 0x7f6de177c598>:\r\n> \r\n> from **future** import absolute_import\r\n> from **future** import division\r\n> from **future** import print_function\r\n> @tf.function\r\n> def simple_nn_layer(x, y):\r\n> return tf.nn.relu(tf.matmul(x, y))\r\n> \r\n> Source code of <function simple_nn_layer at 0x7f6de177c598>:\r\n> \r\n> from **future** import absolute_import\r\n> from **future** import division\r\n> from **future** import print_function\r\n> @tf.function\r\n> def simple_nn_layer(x, y):\r\n> return tf.nn.relu(tf.matmul(x, y))\r\n> \r\n> INFO:tensorflow:Error transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\n> Traceback (most recent call last):\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n> converted_f = conversion.convert(target_entity, program_ctx)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n> free_nonglobal_var_names)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache entity, program_ctx) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast nodes, name, entity_info = convert_func_to_ast(o, program_ctx) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast node = node_to_graph(node, context) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph node = converter.apply_(node, context, function_scopes)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n> node = converter_module.transform(node, context)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n> return FunctionBodyTransformer(ctx).visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n> return super(Base, self).visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n> result = super(Base, self).visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n> return visitor(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n> node = self.generic_visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 308, in generic_visit\r\n> value = self.visit(value)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n> return super(Base, self).visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n> result = super(Base, self).visit(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n> return visitor(node)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n> value=node.value)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n> replacements[k] = _convert_to_ast(replacements[k])\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n> return gast.Name(id=n, ctx=None, annotation=None)\r\n> File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/gast/gast.py\", line 19, in create_node\r\n> format(Name, nbparam, len(Fields))\r\n> AssertionError: Bad argument number for Name: 3, expecting 4\r\n> Error transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\n> WARNING:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> WARNING: Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n-----------------------------\r\nI meet the same problem, did anyone resolve it in tensorflow2-rc1?", "> > OS:Ubuntu 16.04\r\n> > TensorFlow Version:tensorflow-gpu == 2.0.0-rc1\r\n> > CUDA:10.0\r\n> > issue:AutoGraph\r\n> > INFO:tensorflow:Converted call: <function simple_nn_layer at 0x7f6de177c598>\r\n> > args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n> > kwargs: {}\r\n> > Converted call: <function simple_nn_layer at 0x7f6de177c598>\r\n> > args: (<tf.Tensor 'x:0' shape=(3, 3) dtype=float32>, <tf.Tensor 'y:0' shape=(3, 3) dtype=float32>)\r\n> > kwargs: {}\r\n> > INFO:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\n> > Entity <function simple_nn_layer at 0x7f6de177c598> is not cached for key <code object simple_nn_layer at 0x7f6e085cf540, file \"\", line 1> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x7f6d9a2547f0>, frozenset())\r\n> > INFO:tensorflow:Converting <function simple_nn_layer at 0x7f6de177c598>\r\n> > Converting <function simple_nn_layer at 0x7f6de177c598>\r\n> > INFO:tensorflow:Source code of <function simple_nn_layer at 0x7f6de177c598>:\r\n> > from **future** import absolute_import\r\n> > from **future** import division\r\n> > from **future** import print_function\r\n> > @tf.function\r\n> > def simple_nn_layer(x, y):\r\n> > return tf.nn.relu(tf.matmul(x, y))\r\n> > Source code of <function simple_nn_layer at 0x7f6de177c598>:\r\n> > from **future** import absolute_import\r\n> > from **future** import division\r\n> > from **future** import print_function\r\n> > @tf.function\r\n> > def simple_nn_layer(x, y):\r\n> > return tf.nn.relu(tf.matmul(x, y))\r\n> > INFO:tensorflow:Error transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\n> > Traceback (most recent call last):\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\r\n> > converted_f = conversion.convert(target_entity, program_ctx)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\r\n> > free_nonglobal_var_names)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache entity, program_ctx) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast nodes, name, entity_info = convert_func_to_ast(o, program_ctx) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 669, in convert_func_to_ast node = node_to_graph(node, context) File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 699, in node_to_graph node = converter.apply_(node, context, function_scopes)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 409, in apply_\r\n> > node = converter_module.transform(node, context)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 120, in transform\r\n> > return FunctionBodyTransformer(ctx).visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n> > return super(Base, self).visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n> > result = super(Base, self).visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n> > return visitor(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 87, in visit_FunctionDef\r\n> > node = self.generic_visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 308, in generic_visit\r\n> > value = self.visit(value)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/core/converter.py\", line 346, in visit\r\n> > return super(Base, self).visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/transformer.py\", line 480, in visit\r\n> > result = super(Base, self).visit(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/ast.py\", line 253, in visit\r\n> > return visitor(node)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/function_scopes.py\", line 44, in visit_Return\r\n> > value=node.value)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 261, in replace\r\n> > replacements[k] = _convert_to_ast(replacements[k])\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/templates.py\", line 223, in _convert_to_ast\r\n> > return gast.Name(id=n, ctx=None, annotation=None)\r\n> > File \"/home/tommey/anaconda3/envs/tf2.0/lib/python3.6/site-packages/gast/gast.py\", line 19, in create_node\r\n> > format(Name, nbparam, len(Fields))\r\n> > AssertionError: Bad argument number for Name: 3, expecting 4\r\n> > Error transforming entity <function simple_nn_layer at 0x7f6de177c598>\r\n> > WARNING:tensorflow:Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> > WARNING: Entity <function simple_nn_layer at 0x7f6de177c598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\r\n> \r\n> I meet the same problem, did anyone resolve it in tensorflow2-rc1?\r\n\r\nI also met that problem"]}, {"number": 32376, "title": "[TF2.0]Shuffle function on tf.data fills up RAM memory.", "body": "In TF2.0rc, using `shuffle(buffer_size= #_of_elements)` on `tf.data.Dataset` type dataset, it fills up shuffle buffer which also fills up RAM memory so if the data is huge and RAM is not enough then it hangs up the system which leads to system restart. This memory filling happens before every epoch. It of course releases the memory once the epoch is done. To reproduce this problem, I followed [this](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) TF tutorial but with more amount of data.\r\nSince `tf.data` is for handling the dataset  that can not be filled up in the memory then why does shuffle function shows this behaviour and fills up the memory.?", "comments": ["The behavior you are seeing is expected. If your data is such that it cannot be all loaded into memory, then you cannot perform a \"perfect\" shuffle (i.e. using a shuffle buffer size that matches the cardinality of your dataset) and should instead use a smaller buffer size.\r\n\r\nIn other words, tf.data currently does not support \"perfect\" shuffle for dataset that do not fit into memory.", "@jsimsa \r\nHow to avoid this problem? Thank you.", "Use a smaller buffer size.", "Thanks a lot!\r\nYou're answering so quickly, thx!\r\n"]}, {"number": 32375, "title": "tf.concat not accepting arbitrary dtype with functional API model", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Windows 10 Pro, version 1903, OS build 18362.295\r\n- TensorFlow installed via: pip\r\n- TensorFlow version: v2.0.0-beta1-5101-gc75bb66a99 2.0.0-rc0\r\n- Python version: Python 3.6.9 |Anaconda, Inc.| [MSC v.1915 64 bit (AMD64)] on win32\r\n\r\n**Describe the current behavior**\r\n```tf.concat``` only accepts ```float32``` unless explicitly passing ```dtype``` arg to ```tf.keras.Input```.\r\n\r\n**Describe the expected behavior**\r\n```tf.concat``` implicitly accepts any dtype (as long as input dtypes match).\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# Build concat model\r\nx1 = tf.keras.Input([5])\r\nx2 = tf.keras.Input([6])\r\ny = tf.concat([x1, x2], axis=1)\r\nconcat_model = tf.keras.Model(inputs=[x1, x2], outputs=y)\r\n# Generate inputs\r\nx1_ = np.random.random(size=[2, 5])\r\nx2_ = np.random.random(size=[2, 6])\r\nassert x1_.dtype == 'float64'\r\nassert x2_.dtype == 'float64'\r\n# Run model\r\ny_ = concat_model([x1_, x2_])  # EXCEPTION HAPPENS HERE\r\n```\r\n\r\n**Full traceback**\r\n```Traceback (most recent call last):\r\n  File \"C:/Users/wwill/Downloads/tf_bug_report_test.py\", line 10, in <module>\r\n    y_ = concat_model([x1_, x2_])  # Expected output of dtype float64 to match inputs\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 851, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 697, in call\r\n    return self._run_internal_graph(inputs, training=training, mask=mask)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 842, in _run_internal_graph\r\n    output_tensors = layer(computed_tensors, **kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 851, in __call__\r\n    outputs = self.call(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 2525, in call\r\n    return self._defun_call(inputs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1821, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2655, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n    relative to C:\\Users\\wwill\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python:\r\n\r\n    keras\\engine\\base_layer.py:2571 _defun_call  *\r\n        return self._make_op(inputs)\r\n    keras\\engine\\base_layer.py:2549 _make_op\r\n        c_op = ops._create_c_op(graph, node_def, inputs, control_inputs=[])\r\n    framework\\ops.py:1613 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Inconsistent values for attr 'T' DT_DOUBLE vs. DT_FLOAT while building NodeDef 'concat' using Op<name=ConcatV2; signature=values:N*T, axis:Tidx -> output:T; attr=N:int,min=2; attr=T:type; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>```", "comments": ["Issue replicating with TFversion 2.0rc0,please find the [gist](https://colab.research.google.com/gist/oanush/0407693cf5ddc756f6ae143a3a7cf4df/32375.ipynb) of colab.", "The original input data types should match with the data types of your new inputs.\r\nCasting keras input to ```float64``` can help resolve this error.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# Build concat model and set input data type to match the new input data types downstream\r\nx1 = tf.keras.Input([5], dtype='float64')\r\nx2 = tf.keras.Input([6], dtype= 'float64')\r\nassert x1.dtype == 'float64'\r\nassert x2.dtype == 'float64'\r\ny = tf.concat([x1, x2], axis=1)\r\nconcat_model = tf.keras.Model(inputs=[x1, x2], outputs=y)\r\n# Generate inputs\r\nx1_ = np.random.random(size=[2, 5])\r\nx2_ = np.random.random(size=[2, 6])\r\nassert x1_.dtype == 'float64'\r\nassert x2_.dtype == 'float64'\r\n# Run model\r\ny_ = concat_model([x1_, x2_]) \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32375\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32375\">No</a>\n", "Any solution?\r\nHow to switching globally to float64 in TensorFlow 2.x?\r\n\r\nI have similar problem after upgrading to a newer version of the TensorFlow (1.x -> 2.x):\r\n\r\n```\r\nValueError: Inconsistent values for attr 'Tidx' DT_FLOAT vs. DT_INT32 while building NodeDef 'tf_op_layer_Mean/Mean' using Op<name=Mean; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n```"]}, {"number": 32374, "title": "relocate raise place", "body": "The PR relocates to the exact place of assertions. ", "comments": []}, {"number": 32373, "title": "is tf.signal.stft same as librosa.stft ?", "body": "hi,\r\nDear, if I use the tf.signal.stft instead of librosa.stft, will have any difference ?\r\nand tf.signal.inverse_stft could be librosa.istft ?\r\n\r\nthanks a lot\r\n", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32373\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32373\">No</a>\n", "> Are you satisfied with the resolution of your issue?\r\n> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32373)\r\n> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32373)\r\n\r\nNO\r\nCan't open the link\r\n", "They're different and you cannot just replace one with the other. This is pretty easy to test.\r\n\r\n```py\r\nimport librosa as lr\r\nimport tensorflow as tf\r\n\r\nwaveform, samplerate = lr.load(lr.util.example_audio_file(), None, duration=10.0)\r\n\r\nz1 = lr.stft(waveform)\r\nz2 = tf.signal.stft(waveform, 2048, 512).numpy()\r\nprint(z1.shape, z1.min(), z1.max(), z1.mean())\r\nprint(z2.shape, z2.min(), z2.max(), z2.mean())\r\n```\r\n\r\nAside from the transpose, librosa does reflect padding to center analysis frames (`np.pad(waveform, 2048//2, 'reflect')`). That seems to be the major difference, but even after matching that you'd still get some slightly different values (perhaps the underlying FFT isn't identical). They're different enough that you cannot just drop-in replace one for the other with a pretrained model so beware.\r\n\r\n"]}, {"number": 32372, "title": "[INTEL MKL] Added MKL op conversion tests.", "body": "This is the first of a few PRs to add new python tests to verify if a python op is correctly rewritten into MKL versions in graph modes. Only a few operators are covered here, once approved I will follow up with all other ops that MKL supports.", "comments": ["@agramesh1 Could you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@agramesh1 Wondering if you still need this PR, if yes can you please check reviewer comments and keep us posted. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}]