[{"number": 10560, "title": "[Bash] Use cd ... || exit in case cd fails.", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2164", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10559, "title": "MODEL_NAME_FLAG appears unused", "body": "In the bash script [`tensorflow/tools/dist_test/local_test.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local_test.sh) the variable `MODEL_NAME_FLAG` declared in [line 94](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local_test.sh#L94) appears to be unused.\r\n\r\nVariables not used for anything are often associated with bugs.\r\nCould someone take a look at this?", "comments": ["@caisq could you take a look?"]}, {"number": 10558, "title": "LOCAL_K8S_CACHE appears unused", "body": "In the bash script [`tensorflow/tools/dist_test/local_test.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local_test.sh) the variable `LOCAL_K8S_CACHE` declared in [line 66](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/local_test.sh#L66) appears to be unused.\r\n\r\nVariables not used for anything are often associated with bugs.\r\nCould someone take a look at this?", "comments": ["@caisq could you take a look?"]}, {"number": 10557, "title": "[Bash] Instead of 'echo $(cmd)', just use 'cmd'", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2005", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10556, "title": "[Bash] A && B || C is not if-then-else", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2015", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10555, "title": "[Bash] Put 2>&1 behind the redirect", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2069", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10554, "title": "[Bash] Prefer ${variable//search/replace} over sed", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2001", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10553, "title": "[Bash] Use $(...) instead of legacy `...`", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2006", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10552, "title": "[Bash] Use * instead of @ to concatenate", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2124", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10551, "title": "[Bash] Remove unnecessary $/${}", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2004", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10550, "title": "[Bash] A && B || C is not if-then-else", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2015", "comments": ["Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10549, "title": "[Bash] Instead of echo $(cmd), just use cmd", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2005", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10548, "title": "BAZEL_BUILD_ONLY_CMD appears unused", "body": "In the bash script [`tensorflow/tools/ci_build/ci_parameterized_build.sh`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_parameterized_build.sh) the variable `BAZEL_BUILD_ONLY_CMD` declared in [line 129](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_parameterized_build.sh#L129) appears to be unused.\r\n\r\nVariables not used for anything are often associated with bugs.\r\nCould someone take a look at this?", "comments": ["This script is for our Internal CI use only.\r\nUsers should never need this script.\r\n\r\nBut we are slowly moving away from it, too.\r\nIll let @av8ramit evaluate whether it is worth fixing the script, or waiting for it to be deleted completely.", "I'll just drop the complete list here:\r\n\r\nIn ./tensorflow/tools/dist_test/scripts/dist_census_widendeep_test.sh line 37:\r\nTIMEOUT=120  # Timeout for MNIST replica sessions\r\n\r\nIn ./tensorflow/tools/dist_test/scripts/dist_census_widendeep_test.sh line 60:\r\n    SYNC_REPLICAS=\"1\"\r\n\r\nIn ./tensorflow/tools/dist_test/scripts/dist_mnist_test.sh line 151:\r\n  PS=($PS_HOSTS)\r\n\r\nIn ./tensorflow/tools/dist_test/local_test.sh line 66:\r\nLOCAL_K8S_CACHE=${HOME}/kubernetes\r\n\r\nIn ./tensorflow/tools/dist_test/local_test.sh line 94:\r\n    MODEL_NAME_FLAG=\"--model_name ${MODEL_NAME}\"\r\n\r\nIn ./tensorflow/tools/ci_build/windows/bazel/bazel_test_lib.sh line 97:\r\nexclude_gpu_cc_tests=\"${extra_failing_gpu_cc_tests} + ${exclude_cpu_cc_tests}\"\r\n\r\nIn ./tensorflow/tools/ci_build/builds/integration_tests.sh line 42:\r\nTIMEOUT=1800\r\n\r\nIn ./tensorflow/tools/ci_build/builds/run_pip_tests.sh line 100:\r\n  PYTHON_BIN_PATH=\"$(which python)\"\r\n\r\nIn ./tensorflow/tools/ci_build/builds/run_pip_tests.sh line 116:\r\n    PAR_TEST_JOBS=$TF_GPU_COUNT\r\n\r\nIn ./tensorflow/tools/ci_build/builds/builds_common.sh line 134:\r\n  TEST_BLACKLIST_SR=$3\r\n\r\nIn ./tensorflow/tools/ci_build/builds/builds_common.sh line 199:\r\n    for TEST_NAME in ${FAILED_TESTS}; do\r\n\r\nIn ./tensorflow/tools/ci_build/builds/test_tutorials.sh line 46:\r\nTIMEOUT=1800\r\n\r\nIn ./tensorflow/tools/ci_build/builds/libtensorflow.sh line 54:\r\n  BAZEL=\"bazel --bazelrc ./tensorflow/tools/ci_build/install/.bazelrc\"\r\n\r\nIn ./tensorflow/tools/ci_build/ci_parameterized_build.sh line 129:\r\nBAZEL_BUILD_ONLY_CMD=\"bazel build\"\r\n\r\nIn ./tensorflow/contrib/makefile/compile_ios_protobuf.sh line 39:\r\nIPHONEOS_PLATFORM=`xcrun --sdk iphoneos --show-sdk-platform-path`\r\n\r\nIn ./tensorflow/contrib/makefile/compile_ios_protobuf.sh line 41:\r\nIPHONESIMULATOR_PLATFORM=`xcrun --sdk iphonesimulator --show-sdk-platform-path`\r\n\r\nIn ./tensorflow/contrib/makefile/samples/build_and_run_inception_hexagon.sh line 229:\r\n    for i in $(seq 1 \"${TEST_COUNT}\"); do\r\n\r\nIn ./tools/tf_env_collect.sh line 26:\r\nuname=`uname -s`", "Thank you @Androbin for bringing this to our attention. We have plans to migrate away from ci_parameterized_build.sh, so issues with that file should be resolved. As for the complete list, I think those variables are fine for now, but in the event you notice a breakage please feel free to file a PR with a fix."]}, {"number": 10547, "title": "[Bash] Prefer read -a to split exit codes", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2206", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10546, "title": "[Bash] Prefer grep -E over deprecated egrep", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2196", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10545, "title": "incorrect documentation for deep_cnn tutorial", "body": "### System information\r\nnot relevant to issue\r\n\r\n### Describe the problem\r\nThe addresses provided for seeing the code in the \"Code Organization\" chapter of this tutorial https://www.tensorflow.org/tutorials/deep_cnn are no longer valid. \r\n\r\nI think the right place could be https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10\r\n\r\n### Source code / logs\r\nAddress of the tutorial: https://www.tensorflow.org/tutorials/deep_cnn\r\nOne the addresses provided for the code: https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/image/cifar10/\r\n", "comments": ["Hey, we have fixed this, but it won't show up in root until Tensorflow 1.2 becomes the default.\r\n\r\nCheck it out though: https://www.tensorflow.org/versions/r1.2/tutorials/deep_cnn#code_organization"]}, {"number": 10544, "title": "[Bash] Use grep -q instead of [ -n .. ]", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2143", "comments": ["Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10543, "title": "[Bash] read with -r to not mangle backslashes", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2162", "comments": ["Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10542, "title": "[Bash] Removed unnecessary $/${}", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2004", "comments": ["Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10541, "title": "[Bash] Put 2>&1 behind the redirect", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2069", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10540, "title": "[Bash] Fix misspelling: ERRORS_FLIE -> ERRORS_FILE", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 10539, "title": "[Bash] Instead of echo $(cmd), just use cmd", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2005", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10538, "title": "[Bash] Fix bad string / array concat behaviour", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2145", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10537, "title": "tf.layers: Add weights, biases and activations to the respective collections", "body": "For generating weight, bias and activation summaries one needs to have access to these tensors. Similar to how regularization losses are added to the GraphKeys.REGULARIZATION_LOSSES collection, it would make sense to add weights, biases and activations to the collections GraphKeys.WEIGHTS, GraphKeys.BIASES and GraphKeys.ACTIVATIONS. Right now, layers only add the weights and bias tensors to GraphKeys.TRAINABLE_VARIABLES. \r\n\r\nThe summarization of the these collections is already supported [here](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/layers/python/layers/summaries.py).", "comments": ["I believe the pattern of \"one global collection for everything\" has been abandoned some time ago, in favor of fewer collections, and generally a move aways from global objects and towards \"layers as objects\", with kernels/biases/activations being attributes of these objects.\r\n\r\nYou can retrieve kernels and biases from `TRAINABLE_VARIABLES` (they have different names indicating whether they are kernels or biases). To store activations when using functional layers, you should maintain your own dictionary of activations."]}, {"number": 10536, "title": "[Bash] Variable used as array not string", "body": "As proposed by static analysis tool:\r\nhttps://github.com/koalaman/shellcheck/wiki/SC2178", "comments": ["Can one of the admins verify this patch?", "Closing these all out. Mind making all these changes in one pull request? It's easier to run through tests that way."]}, {"number": 10535, "title": "tf.nn.conv3d_transpose really slow on i7 CPU with 100+G free memory", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: virtualenv pip\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: on CPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nIt takes about 10 mins to do the tf.nn.conv3d_transpose computation.\r\nThe input size is [1, 97, 128, 256, 32], kernel size is 3*3*3, strides is [1, 2, 2, 2, 1], and the output size is [1, 193, 256, 512, 1].\r\n\r\nAt first my model runs with normal speed. Before this layer, there is several tf.nn.conv3d and tf.nn.conv3d_transpose computation.\r\nAfter this step's computation, it seems the computation become really slow down.\r\n\r\nBut my model runs smoothly on 12G GPU, if using 4 GPUs, 1.8 examples/sec.\r\n\r\nI notice a similar issue #3128. That issue also reports some problem on CPU, but not on GPU. Is that issue resolved?\r\n\r\n### Source code / logs\r\nSince I am training a large model on a large dataset. I will only include the model's code. If is needed, I will include more codes. \r\n\r\nThe computationally cost layer is the last layer in the variable_scope learning_regularization, right before the scope soft_argmin in `def _build_model`.\r\n\r\nFor more details, I am reimplementing https://arxiv.org/pdf/1703.04309.pdf.\r\n\r\n```python\r\nfrom collections import namedtuple\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport six\r\n\r\nfrom tensorflow.python.training import moving_averages\r\n\r\n# If a model is trained using multiple GPUs, prefix all Op names with tower_name\r\n# to differentiate the operations. Note that this prefix is removed from the\r\n# names of the summaries when visualizing a model.\r\nTOWER_NAME = 'tower'\r\n\r\n# Batch normalization. Constant governing the exponential moving average of\r\n# the 'global' mean and variance for all activations.\r\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\r\n\r\n# The decay to use for the moving average.\r\nMOVING_AVERAGE_DECAY = 0.9999\r\n\r\n\r\nHParams = namedtuple('HParams',\r\n                     ['batch_size', 'lrn_rate',\r\n                     'weight_decay_rate',\r\n                     'relu_leakiness', 'optimizer', 'max_disparity'])\r\n\r\n\r\nclass GCNet(object):\r\n  \"\"\"GCNet model.\"\"\"\r\n\r\n  def __init__(self, hps, left_images, right_images, gt_disparity, mask, mode): \r\n    \"\"\"ResNet constructor.\r\n\r\n    Args:\r\n      hps: Hyperparameters.\r\n      images: Batches of images. [batch_size, image_size, image_size, 3]\r\n      labels: Batches of labels. [batch_size, num_classes]\r\n      mode: One of 'train', 'eval' and 'predict'.\r\n    \"\"\"\r\n    self.hps = hps\r\n    self._left_images = left_images\r\n    self._right_images = right_images\r\n    self.gt_disparity = gt_disparity\r\n    self.mask = mask\r\n    self.mode = mode\r\n    self.debug_op_list = []  \r\n\r\n    self._extra_train_ops = []\r\n    \r\n  def build_graph_to_loss(self):\r\n    self._build_model()\r\n    self._build_loss_op()\r\n\r\n  def _stride_arr(self, stride):\r\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\r\n    return [1, stride, stride, 1]\r\n    \r\n  def _stride_3d_arr(self, stride):\r\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\r\n    return [1, stride, stride, stride, 1]\r\n\r\n  def _build_model(self):\r\n    \"\"\"Build the core model within the graph.\"\"\"\r\n\r\n    layer_idx = 1\r\n    with tf.variable_scope('unary_features', reuse=False):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        left_x = self._left_images\r\n        left_x = self._conv('conv', left_x, 5, 3, 32, self._stride_arr(2))\r\n        left_x = self._relu(left_x, self.hps.relu_leakiness)\r\n        left_x = self._batch_norm('bn', left_x)\r\n      tf.add_to_collection('shapes', tf.shape(left_x))\r\n        \r\n      for i in six.moves.range(8):\r\n        left_x, layer_idx = self._unary_feat_residual(left_x, 3, 32, 32, self._stride_arr(1), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(left_x))\r\n    \r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        left_x = self._conv('conv', left_x, 3, 32, 32, self._stride_arr(1))\r\n      tf.add_to_collection('shapes', tf.shape(left_x))\r\n    \r\n    layer_idx = 1    \r\n    with tf.variable_scope('unary_features', reuse=True):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        right_x = self._left_images\r\n        right_x = self._conv('conv', right_x, 5, 3, 32, self._stride_arr(2))\r\n        right_x = self._relu(right_x, self.hps.relu_leakiness)\r\n        right_x = self._batch_norm('bn', right_x)\r\n        \r\n      for i in six.moves.range(8):\r\n        right_x, layer_idx = self._unary_feat_residual(right_x, 3, 32, 32, self._stride_arr(1), layer_idx)\r\n\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        right_x = self._conv('conv', right_x, 3, 32, 32, self._stride_arr(1))\r\n      \r\n    with tf.variable_scope('cost_volumn'):\r\n      left_cost_volume = tf.stack([tf.identity(left_x)] * (self.hps.max_disparity/2+1), axis=1, name='left_stack')\r\n      right_cost_volume = []\r\n      cur_width = tf.shape(right_x)[2]\r\n\r\n      for depth in six.moves.range(self.hps.max_disparity/2+1):\r\n        right_cost_volume.append(tf.pad(tf.slice(right_x, [0, 0, 0, 0], [-1, -1, cur_width - depth, -1], name='right_slice_'+str(depth)),\r\n                                        [[0, 0], [0, 0], [depth, 0], [0, 0]],\r\n                                        name='right_pad_'+str(depth)\r\n                                        ))\r\n      right_cost_volume = tf.stack(right_cost_volume, axis=1, name='right_stack')\r\n      x = tf.concat([left_cost_volume, right_cost_volume], 4)\r\n      tf.add_to_collection('shapes', tf.shape(x))\r\n      \r\n          \r\n    with tf.variable_scope('learning_regularization'):\r\n      stored_features = []\r\n\r\n      in_filters = [64, 64, 64, 64]\r\n      out_filters = [32, 64, 64, 64]\r\n      in_filters_stride_2 = [64, 64, 64, 64]\r\n      out_filters_stride_2 = [64, 64, 64, 128]\r\n      for i in six.moves.range(4):\r\n        tmp_x, layer_idx = self._regularization_subsample(x, 3, in_filters[i], out_filters[i], self._stride_3d_arr(1), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(tmp_x))\r\n        stored_features.append(tmp_x)\r\n        \r\n        with tf.variable_scope('layer_'+str(layer_idx)):\r\n          layer_idx += 1\r\n          x = self._conv3d('conv3d', x, 3, in_filters_stride_2[i], out_filters_stride_2[i], self._stride_3d_arr(2))\r\n          x = self._relu(x, self.hps.relu_leakiness)\r\n          x = self._batch_norm('bn', x)\r\n          tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n      \r\n      assert stored_features[0] is not stored_features[1]\r\n\r\n      for i in six.moves.range(2):\r\n        with tf.variable_scope('layer_'+str(layer_idx)):\r\n          layer_idx += 1\r\n          x = self._conv3d('conv3d', x, 3, 128, 128, self._stride_3d_arr(1))\r\n          x = self._relu(x, self.hps.relu_leakiness)\r\n          x = self._batch_norm('bn', x)\r\n          tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n      transposed_in_filters = [128, 64, 64, 64]\r\n      transposed_out_filters = [64, 64, 64, 32]\r\n      \r\n      for i in six.moves.range(4):\r\n        x, layer_idx = self._regularization_upsample(x, stored_features[-i-1], 3, transposed_in_filters[i], transposed_out_filters[i], self._stride_3d_arr(2), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n      \r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        input_shape = tf.shape(self.gt_disparity)\r\n        x = self._conv3d_trans('conv_trans', x, 3, 32, 1, self._stride_3d_arr(2), [input_shape[0], self.hps.max_disparity+1, input_shape[1], input_shape[2], 1])\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        self.debug_op_list.append(tf.shape(x))\r\n\r\n    \r\n    with tf.variable_scope('soft_argmin'):\r\n        x = tf.squeeze(x, squeeze_dims=[4], name='squeeze')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        x = tf.transpose(x, perm=[0, 2, 3, 1], name='transpose')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        x = tf.nn.softmax(x, dim=-1, name='softmax')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n        multiplier = tf.range(0, self.hps.max_disparity+1, dtype=tf.float32, name='depth_range')\r\n        x = tf.multiply(x, multiplier, name='softmax_mul_depth')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        self.predicted_disparity = tf.reduce_sum(x, axis=3, name='reduce_sum')       \r\n        tf.add_to_collection('shapes', tf.shape(self.predicted_disparity))\r\n    self.shapes = tf.get_collection('shapes')\r\n    self.debug_op_list.append(self.shapes)\r\n\r\n  def _build_loss_op(self):\r\n    with tf.variable_scope('loss'):\r\n      self.abs_loss = tf.reduce_mean(tf.abs((self.gt_disparity - self.predicted_disparity) * self.mask), name='abs_loss')\r\n      self.total_loss = self.abs_loss + self._decay()\r\n\r\n      \r\n  def _add_loss_summaries(self):\r\n    \"\"\"Add summaries for losses in CIFAR-10 model.\r\n\r\n    Generates moving average for all losses and associated summaries for\r\n    visualizing the performance of the network.\r\n\r\n    Args:\r\n      total_loss: Total loss from loss().\r\n    Returns:\r\n      loss_averages_op: op for generating moving averages of losses.\r\n    \"\"\"\r\n    # Compute the moving average of all individual losses and the total loss.\r\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='loss_avg')\r\n    self.loss_averages_op = loss_averages.apply([self.abs_loss, self.total_loss])\r\n\r\n    # Attach a scalar summary to all individual losses and the total loss; do the\r\n    # same for the averaged version of the losses.\r\n    for l in [self.abs_loss, self.total_loss]:\r\n      # Name each loss as '(raw)' and name the moving average version of the loss\r\n      # as the original loss name.\r\n      tf.summary.scalar(l.op.name + ' (raw)', l)\r\n      tf.summary.scalar(l.op.name, loss_averages.average(l))\r\n    \r\n  def _build_train_op(self, global_step):\r\n    \"\"\"Build training specific ops for the graph.\"\"\"\r\n    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\r\n    tf.summary.scalar('learning_rate', self.lrn_rate)\r\n\r\n    loss_averages_op = self._add_loss_summaries()\r\n    \r\n    with tf.control_dependencies([loss_averages_op]):\r\n      if self.hps.optimizer == 'sgd':\r\n        optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\r\n      elif self.hps.optimizer == 'mom':\r\n        optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\r\n      elif self.hps.optimizer == 'RMSProp':\r\n        optimizer = tf.train.RMSPropOptimizer(self.lrn_rate, decay=0.9, momentum=0.9, epsilon=1)\r\n        \r\n        trainable_variables = tf.trainable_variables()\r\n        grads = optimizer.compute_gradients(self.total_loss, trainable_variables)\r\n\r\n\r\n    apply_op = optimizer.apply_gradients(\r\n        grads,\r\n        global_step=global_step, \r\n        name='train_step')\r\n        \r\n    # Track the moving averages of all trainable variables.\r\n    variable_averages = tf.train.ExponentialMovingAverage(\r\n        MOVING_AVERAGE_DECAY, global_step)\r\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n\r\n    with tf.control_dependencies([apply_op, variables_averages_op]):\r\n      self.train_op = tf.no_op(name='train')\r\n\r\n  def _regularization_upsample(self, x, feature, filter_size, in_filter, out_filter, stride, layer_idx):\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d_trans('conv_trans', x, filter_size, in_filter, out_filter, stride, tf.shape(feature))\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n      \r\n    with tf.variable_scope('residual_after_'+str(layer_idx-1)):\r\n      x += feature\r\n\r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n  def _regularization_subsample(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\r\n\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d('conv3d', x, filter_size, in_filter, out_filter, stride)\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d('conv3d', x, filter_size, out_filter, out_filter, stride)\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n      \r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n  def _unary_feat_residual(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\r\n    orig_x = x\r\n    orig_layer_idx = layer_idx - 1\r\n    \r\n    for i in six.moves.range(2):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        x = self._conv('conv', x, 3, in_filter, out_filter, stride)\r\n        x = self._relu(x, self.hps.relu_leakiness)\r\n        x = self._batch_norm('bn', x)\r\n          \r\n    with tf.variable_scope('residual_btw_'+str(layer_idx-1)+'_'+str(orig_layer_idx)):\r\n      x += orig_x\r\n\r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n\r\n  def _decay(self):\r\n    \"\"\"L2 weight decay loss.\"\"\"\r\n    costs = []\r\n    for var in tf.trainable_variables():\r\n      if var.op.name.find(r'DW') > 0:\r\n        costs.append(tf.nn.l2_loss(var))\r\n        # tf.summary.histogram(var.op.name, var)\r\n\r\n    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\r\n\r\n  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, in_filters, out_filters],\r\n          initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\r\n      \r\n  def _conv3d(self, name, x, filter_size, in_filters, out_filters, strides):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, filter_size, in_filters, out_filters],\r\n           initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      return tf.nn.conv3d(x, kernel, strides, padding='SAME')\r\n      \r\n  def _conv3d_trans(self, name, x, filter_size, in_filters, out_filters, strides, output_shape):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, filter_size, out_filters, in_filters],\r\n            initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      x_shape = tf.shape(x)\r\n      self.debug_op_list.append(tf.shape(kernel))\r\n      return tf.nn.conv3d_transpose(\r\n                x, \r\n                kernel, \r\n                output_shape,\r\n                strides, \r\n                padding='SAME')\r\n\r\n  def _relu(self, x, leakiness=0.0):\r\n    \"\"\"Relu, with optional leaky support.\"\"\"\r\n    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\r\n\r\n  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\r\n  def _batch_norm(self, name, x):\r\n    \"\"\"Batch normalization.\"\"\"\r\n    with tf.variable_scope(name):\r\n      params_shape = [x.get_shape()[-1]]\r\n\r\n      beta = self._variable_on_cpu(\r\n          'beta', params_shape,\r\n          initializer=tf.constant_initializer(0.0, tf.float32))\r\n      gamma = self._variable_on_cpu(\r\n          'gamma', params_shape,\r\n          initializer=tf.constant_initializer(1.0, tf.float32))\r\n\r\n      if self.mode == 'train':\r\n        mean, variance = tf.nn.moments(x, range(len(x.get_shape())-1), name='moments')\r\n\r\n        moving_mean = self._variable_on_cpu(\r\n            'moving_mean', params_shape,\r\n            initializer=tf.constant_initializer(0.0, tf.float32),\r\n            trainable=False)\r\n        moving_variance = self._variable_on_cpu(\r\n            'moving_variance', params_shape,\r\n            initializer=tf.constant_initializer(1.0, tf.float32),\r\n            trainable=False)\r\n\r\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\r\n            moving_mean, mean, BATCHNORM_MOVING_AVERAGE_DECAY))\r\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\r\n            moving_variance, variance, BATCHNORM_MOVING_AVERAGE_DECAY))\r\n      else:\r\n        mean = self._variable_on_cpu(\r\n            'moving_mean', params_shape,\r\n            initializer=tf.constant_initializer(0.0, tf.float32),\r\n            trainable=False)\r\n        variance = self._variable_on_cpu(\r\n            'moving_variance', params_shape,\r\n            initializer=tf.constant_initializer(1.0, tf.float32),\r\n            trainable=False)\r\n      y = tf.nn.batch_normalization(\r\n          x, mean, variance, beta, gamma, 0.001)\r\n      y.set_shape(x.get_shape())\r\n      return y\r\n\r\n  def _variable_on_cpu(self, name, shape, initializer, dtype=tf.float32, trainable=True):\r\n    \"\"\"Helper to create a Variable stored on CPU memory.\r\n\r\n    Args:\r\n      name: name of the variable\r\n      shape: list of ints\r\n      initializer: initializer for Variable\r\n\r\n    Returns:\r\n      Variable Tensor\r\n    \"\"\"\r\n    with tf.device('/cpu:0'):\r\n      var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\r\n    return var\r\n```", "comments": ["Thanks for the report. How are you determining which op is slow? Do you have a [timeline](https://stackoverflow.com/documentation/tensorflow/3850/measure-the-execution-time-of-individual-operations#t=201706081718274193783) for the run?", "My program did not give any output, so I made an debug_op_list, and add some debug ops after each layer of the model, like:\r\n```python\r\n      for i in six.moves.range(4):\r\n        x, layer_idx = self._regularization_upsample(x, stored_features[-i-1], 3, transposed_in_filters[i], transposed_out_filters[i], self._stride_3d_arr(2), layer_idx)\r\n        self.debug_op_list.append(tf.shape(x))\r\n      \r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        input_shape = tf.shape(self.gt_disparity)\r\n        x = self._conv3d_trans('conv_trans', x, 3, 32, 1, self._stride_3d_arr(2), [input_shape[0], self.hps.max_disparity+1, input_shape[1], input_shape[2], 1])\r\n        self.debug_op_list.append(tf.shape(x))\r\n```\r\nand then do\r\n```python\r\nfor debug_op in model.debug_op_list:\r\n  sess.run(debug_op)\r\n```\r\n\r\nThe debug_op before this layer runs quickly, and I need to wait for about ten minutes to see the output of the debug_op right after this layer.", "Could you collect and share a timeline as in the above-referenced instructions? Thanks!", "Hi, my timeline snapshot is as follows:\r\n![timeline](https://github.com/laoreja/Reimplementation-of-GC-Net/raw/master/timeline.png)\r\n\r\nThe timeline.json file is located at https://github.com/laoreja/Reimplementation-of-GC-Net/blob/master/timeline.json", "@andydavis1, could you take a look at this and see if anything strikes you?", "Nothing strikes me. I'd recommend trying to run an isolated 3D conv with the same shapes on the uses i7 machine, and see if its still slow (vs running the entire training step), just to isolate if the 3D Conv implementation is just slow for this shape (which could be the case).", "My model has several 3D conv layers, followed by several 3D conv trans layers. I detected that the last 3D conv trans layer is extremely slow, while the previous layers are much faster. \r\n\r\nAccording to some previous issues, I guess this may be a memory related problem?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@laoreja Is this still a problem for you?  If so, have you tried the latest release of TensorFlow?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of activity. Please reopen if this is still a problem."]}, {"number": 10534, "title": "fixed minor formatting issues for tfprof docs", "body": "fixes some minor issues:\r\n\r\n* headings displayed correctly\r\n* timeline visualization is now shown correctly", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 10533, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "I ain't sure if \"automagically\" was a typo:\r\n[tensorflow/tools/graph_transforms/transform_utils.h#L269](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/transform_utils.h#L269) talks about an \"automagical\" registration\r\nso, why wouldn't the README mean it that way?"]}, {"number": 10532, "title": "Quantized inception-resnet-v2 doesn't predict the images correctly", "body": "I did quantization on inception-resnet-v2 model using https://www.tensorflow.org/performance/quantization#how_can_you_quantize_your_models. Size of freezed graph(input for quantization) is 224,6 MB and quantized graph is 58,6 MB. I ran some tests for quantized graph and the predicted output is totally wrong. I ran accuracy test for certain dataset wherein, for freezed graph the accuracy is 97.4% whereas for quantized graph it is 0%.\r\n\r\nIs there a different way to quantize the model for inception-resnet versions? or, for inception-resnet model, quantization is not support at all?\r\n", "comments": ["I met with a similar problem like this. Try using `graph_transform` instead of `quantize_graph`\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\r\n\r\nBe sure to use `quantize_weights` and `quantize_nodes` successively in the command for graph transform.", "@kwotsin : With this I understand I need to use command like this? Eight-bit Calculations - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#eight-bit-calculations\r\nIsn't it?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10531, "title": "Improve RDMA rendezvous speed", "body": "This PR fixes https://github.com/tensorflow/tensorflow/issues/10530.\r\n\r\nTested with RoCE setting: Mellanox Technologies MT27620 Family\r\n\r\nBefore this PR:\r\n```\r\nDistributed rate: 536.72 MB per second\r\n```\r\nAfter this PR:\r\n```\r\nDistributed rate: 1346.06 MB per second\r\n```\r\n\r\nTODO: memcpy is still slow and need to improved.", "comments": ["Can one of the admins verify this patch?", "Seems to be a reasonable patch. \r\n\r\nBy the way, how do you measure the rates? The MT27620 ConnectX-4 NIC should support 100 GbE fabrics. These rates seem a little bit too small compared to the hardware capabilities.\r\n\r\n", "It's not pure bandwidth test but a tensor copy test like this: https://gist.github.com/yaroslavvb/1124bb02a9fd4abce3d86caf2f950cb2. (change `assign_add` to `assign` in L53).", "Thanks for pointing me to the script. I am actually wondering if the loopback RDMA performance could reflect the actual in-fabric performance. Honestly I am not sure if hardware is involved at all when using loopback; it might use a completely different mechanism than NIC doing DMA.\r\n\r\nNevertheless this patch will still help in the case of local worker-ps communication.", "@junshi15 @anfeng @shamoya  Any comments on this?", "It doesn't converge in GPU device if I patch this PR in tensorflow master branch. It's OK in CPU device, so I guess there're some bugs in `SetProtoFromGPUSync` related code.", "@2sin18 Note that the original implementation uses ``SetProtoFromGPUSync`` even before this patch. If there is anything susceptible it should be ``CopyGPUTensorToCPUSync`` and ``CopyCPUTensorToGPUSync `` which are introduced in this patch.", "@2sin18 Did you tested with RDMA? Can you provide a repro about the convergence issue? Thanks.", "@2sin18 Does @llhe 's new patch fix the convergence problem?", "@junshi15 Thanks to @llhe 's patch, it works now. ", "Jenkins test this please"]}]