[{"number": 25238, "title": "deeplab with tensorflow lite + gpudelegate issue", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nI tried to build deeplab (input_image + float32) and succeeded\r\n\r\n# modified export_model.py # line:77\r\n\r\n```\r\n//input_image = tf.placeholder(tf.uint8, [1, None, None, 3], name=_INPUT_NAME)\r\ninput_image = tf.placeholder(tf.float32, [1, None, None, 3], name=_INPUT_NAME)\r\n```\r\n\r\nS. I converted below command and succeeded\r\n\r\ntflite_convert \\\r\n  --output_file=deeplab_257_float.tflite \\\r\n  --graph_def_file=$PB_FILE \\\r\n  --output_format=TFLITE \\\r\n  --input_arrays=ImageTensor \\\r\n  --output_arrays=SemanticPredictions \\\r\n  --input_shapes=1,257,257,3 \\\r\n  --inference_input_type=FLOAT \\\r\n  --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS \\\r\n  --inference_type=FLOAT \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127 \\\r\n\r\nBut. when i  try to run on android, I saw below issue.\r\n\r\n` java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #38 cannot be handled by this delegate.  Only the first 32 ops will run on the GPU, and the remaining 88 on the CPU.GpuDelegate Prepare: First dimension is supposed to be BATCH and always equal to 1.Node number 120 (GpuDelegate) failed to prepare.`\r\n\r\nwhat's wrong with my model?\r\n\r\ndeeplabv3_257_mv_gpu.tflite is working.\r\n\r\n# Can you tell me about how can I do that?\r\n# Can you tell me about your tflite_convert command?\r\n\r\n**Describe the expected behavior**\r\n\r\nworking.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["if you inspect the `deeplabv3_257_mv_gpu.tflite` you mentioned, you can find that its input node is `sub_7` and output node is `ResizeBilinear_3`. That is, preprocessing ops and ops after `ResizeBilinear_3` are removed.", "nice answer thanks.", "Is this issue still there or is it solved?", "@freedomtan I tried to change the input and output by belows command like **deeplabv3_257_mv_gpu.tflite**\r\n\r\n> tflite_convert \\\r\n>  --output_file=$TF_FILE \\\r\n>  --graph_def_file=$PB_FILE \\\r\n>  --output_format=TFLITE \\\r\n>  --input_arrays=sub_7 \\\r\n>  --output_arrays=ResizeBilinear_2 \\\r\n>  --inference_input_type=FLOAT \\\r\n>  --inference_type=FLOAT \\\r\n\r\nIs it right way?"]}, {"number": 25237, "title": "Error building tensorflow pip package", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.4\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): cl 19.16.27025.1\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\nGet error when building the pip package from build_pip_package:\r\n\r\n> error in tensorflow setup command:\r\n> 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at \"'< 1.14.0'\"\r\n\r\nThis error only appears after a recent pull request (2019/01/26)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n_Get clean copy of tensorflow_ \r\n`git pull`\r\n\r\n _Build CPU version_\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n _Build  pip package:_\r\n`bazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package.exe tmp/tensorflow_pkg`\r\n\r\n> Sun Jan 27 14:02:41 PST 2019 : === Preparing sources in dir: /tmp/tmp.YaQ7qcBkdZ\r\n> Unzipping simple_console_for_windows.zip to create runfiles tree...\r\n> Unzip finished.\r\n> /e/src/gitClones/tensorflow /e/src/gitClones/tensorflow\r\n> /e/src/gitClones/tensorflow\r\n> Sun Jan 27 14:05:55 PST 2019 : === Building wheel\r\n> error in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at \"'< 1.14.0'\"\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nDigging into this I believe the issue is at line 61 in [setup.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L61) \r\nwhich was introduced by c4388465d0d04ed90c814a11216217d19a8267e8\r\n\r\nI made this change locally and the problem goes away\r\n\r\n> @@ -58,7 +58,7 @@ REQUIRED_PACKAGES = [\r\n>      'six >= 1.10.0',\r\n>      'protobuf >= 3.6.1',\r\n>      'tensorboard >= 1.12.0, < 1.13.0',\r\n> **-    'tensorflow_estimator >= 1.13.0 < 1.14.0',**\r\n> **+    'tensorflow_estimator >= 1.13.0, < 1.14.0',**\r\n>      'termcolor >= 1.1.0',\r\n>  ]\r\n\r\ncc @case540 ", "comments": ["This issue is related to PR #25221.", "This issue is resolved with PR #25244 ", "I got the same error on Ubuntu 18.04, CUDA 10, Python3.6. Updating setuptools does not help :(", "I got the same error on Ubuntu 18.04, CUDA 10, Anaconda 3 (Python 3.7).\r\nDoes the 'tensorflow_estimator' means numpy?", "Thanks for @yongtang and @Dayananda-V, through the method in PR #25221, the error is solved.", "Closing this issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 25236, "title": "Merge pull request #1 from tensorflow/master", "body": "update fork", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 25235, "title": "Code example to save a custom model with a serve() method fails", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes and No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-preview'\r\nGIT_VERSION=\"b'v1.12.0-6502-g76aa6cf917'\"\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nThe [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/saved_model/save) of the `tf.saved_model.save()` function shows a code example to save a custom model with a `serve()` method, but it fails:\r\n1. when subclassing the `keras.models.Model` class, it is compulsory to define a `call()` method (or else you get `NotImplementedError: When subclassing the Model class, you should implement a call method.`.\r\n2. when saving the model as shown in the code example, I get `ValueError: Exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: ['_default_save_signature', 'serve']. The signature keys for these functions are ambiguous. Specify signature functions explicitly.`\r\n\r\nHowever it works if I define a `call()` method and I specify the signature explicitly.\r\n\r\n**Describe the expected behavior**\r\nPerhaps the documentation just needs to be fixed to add a `call()` method and an explicit `signature` argument, but it would be nicer if it could work without having to do that.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.probas = keras.layers.Dense(10, activation=\"softmax\")\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec(shape=[None], dtype=tf.string)])\r\n    def serve(self, serialized):\r\n        expected_features = {\r\n            \"image\": tf.io.FixedLenFeature([28 * 28], dtype=tf.float32)\r\n        }\r\n        examples = tf.io.parse_example(serialized, expected_features)\r\n        return self.probas(examples[\"image\"])\r\n    def call(self, inputs):\r\n        return self.probas(inputs)\r\n\r\nm = MyModel()\r\nm.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\r\nm.fit(X_train.reshape(-1, 28*28), y_train)\r\ntf.saved_model.save(m, \"my_model_test\",\r\n    signatures=m.serve.get_concrete_function(\r\n        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"serialized_inputs\")))\r\n```\r\n\r\nYou can try removing the `call()` method, it will fail. You can also try removing the `signatures` argument in the `tf.saved_model.save()` call, it will fail.\r\n\r\n**Other info / logs**\r\n\r\nHere is the output I get when I don't specify the signature:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-424-4d06f50f5010> in <module>\r\n      2 m.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\r\n      3 m.fit(X_train.reshape(-1, 28*28), y_train)\r\n----> 4 tf.saved_model.save(m, \"my_model_test\")#,\r\n      5 #    signatures=m.serve.get_concrete_function(\r\n      6 #        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"serialized_inputs\")))\r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures)\r\n    820 \r\n    821   if signatures is None:\r\n--> 822     signatures = _find_function_to_export(saveable_view)\r\n    823   signatures = _canonicalize_signatures(signatures)\r\n    824 \r\n\r\n~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py in _find_function_to_export(saveable_view)\r\n    144            \"keys for these functions are ambiguous. Specify signature \"\r\n    145            \"functions explicitly.\").format(\r\n--> 146                [previous_attribute_name, name]))\r\n    147     exported_function = value\r\n    148     previous_attribute_name = name\r\n\r\nValueError: Exporting an object with no tf.saved_model.save(..., signatures=...) argument specified, and with more than one @tf.function-decorated method attached to it: ['_default_save_signature', 'serve']. The signature keys for these functions are ambiguous. Specify signature functions explicitly.\r\n```", "comments": ["This works for me now (with a cast to float so fit() works). Probably after https://github.com/tensorflow/tensorflow/commit/24cc2f6d0b86674a34e831357eb60d70d54190e7\r\n\r\nThanks for the report."]}, {"number": 25234, "title": "tf.saved_model.save() exports a model with an empty method_name", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION='2.0.0-preview'\r\nGIT_VERSION=\"b'v1.12.0-6502-g76aa6cf917'\"\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nSaving a Keras model with `tf.saved_model.save()` works, but the `method_name` is empty, so deploying the model to TensorFlow Serving fails.\r\n\r\n**Describe the expected behavior**\r\nI expect the `method_name` to be equal to `\"tensorflow/serving/predict\"`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport os\r\n\r\nX_train = np.random.rand(100, 3)\r\ny_train = np.random.rand(100, 1)\r\nmodel = keras.models.Sequential([keras.layers.Dense(1)])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\nmodel.fit(X_train, y_train)\r\n\r\nmodel_version = 1\r\nmodel_path = os.path.join(\"my_model\", str(model_version))\r\nos.makedirs(model_path)\r\ntf.saved_model.save(model, model_path)\r\n```\r\n\r\nNext inspect the saved model:\r\n\r\n```bash\r\n$ saved_model_cli show --dir my_model/1 --all\r\n```\r\n\r\nNotice that the `method_name` is empty.\r\n\r\n**Other info / logs**\r\n\r\nHere is the output I get (I added comments marked with `# <=`):\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID   # <= what is this?\r\n        shape: unknown_rank # <= what is this?\r\n        name: NoOp\r\n  Method name is:   # <= empty method name\r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 3)\r\n        name: serving_default_input_1:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output_1'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 1)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is:  # <= empty, should be tensorflow/serving/predict\r\n```", "comments": ["Workaround: if I save the model using `keras.experimental.export()`, the saved model has a correct `method_name`.", "Can tf.saved_model.save() save with additional MetaGraphDef in order to have tag 'train'?"]}, {"number": 25233, "title": "import tensorflow.keras,code can not work.import keras,the same code run perfectly.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:4g\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nimport tensorflow.keras,code can not work.\r\nimport keras,the same code run perfectly.\r\n**Describe the expected behavior**\r\nimport tensorflow.keras,code can work.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport numpy as np\r\ntrain_x = np.random.random((1000, 32))\r\ntrain_y = np.random.random((1000, 10))\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\ninputs = tf.keras.Input(shape=(32,)) \r\nx = layers.Dense(60, activation='relu')(inputs)\r\nx = layers.Dense(30, activation='relu')(x)\r\npredictions = layers.Dense(10)(x)\r\nmodel = tf.keras.Model(inputs=inputs, outputs=predictions)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='mse',\r\n              metrics=['accuracy'])\r\n\r\nimport keras.backend as K\r\ndef scheduler(epoch):\r\n    lr = K.get_value(model.optimizer.lr)\r\n    print(\"lr:{}\".format(lr * 1))\r\n    return K.get_value(model.optimizer.lr)\r\n \r\nreduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\r\nhistory = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])\r\nb = history.history['lr']\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```python\r\nFailedPreconditionError (see above for traceback): Error while reading resource variable Adam/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/Adam/lr)\r\n\t [[node Adam/lr/Read/ReadVariableOp (defined at <ipython-input-2-48494f67c56a>:15)  = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Adam/lr)]]\r\n\t [[{{node Adam/lr/Read/ReadVariableOp/_1}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Adam/lr/Read/ReadVariableOp\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\n**Code to reproduce the Success**\r\n```python\r\nimport numpy as np\r\ntrain_x = np.random.random((1000, 32))\r\ntrain_y = np.random.random((1000, 10))\r\n\r\nfrom keras.models import Model\r\nfrom keras.layers import Input, Dense\r\ninputs = Input(shape=(32,)) \r\nx = Dense(60, activation='relu')(inputs)\r\nx = Dense(30, activation='relu')(x)\r\npredictions = Dense(10)(x)\r\n\r\nmodel = Model(inputs=inputs, outputs=predictions)\r\nmodel.compile(optimizer='adam',\r\n              loss='mse',\r\n              metrics=['acc'])\r\n\r\nimport keras.backend as K\r\nfrom keras.callbacks import LearningRateScheduler\r\ndef scheduler(epoch):\r\n    lr = K.get_value(model.optimizer.lr)\r\n    print(\"lr:{}\".format(lr * 1))\r\n    return K.get_value(model.optimizer.lr)\r\n \r\nreduce_lr = LearningRateScheduler(scheduler)\r\nhistory = model.fit(train_x, train_y, batch_size=16, epochs=10,callbacks=[reduce_lr])\r\nb = history.history['lr']\r\n```\r\n\r\n", "comments": ["Hi @mk123qwe , can you please try this with the latest nightly? This should be fixed", "@omalleyt12 The issue persists with latest nightly build. 1.13.0-dev20190221 ", "@tanzhenyu would you be able to take a look at this? looks like the Adam lr is not getting initialized in some case", "@ymodak try having everything in tf.keras, in this particular case replace \"import keras.backend as K\" to \"import tensorflow.keras.backend as K\" and see if it works fine for you?", "Oh great catch, I missed that. Yeah, that's very likely to be it", "@tanzhenyu That worked. Tested against TF '1.13.0-rc2' Thanks!", "Glad it works for you!"]}, {"number": 25232, "title": " failed to process \u201capi_def_Conv2D.pbtxt\u201d: Attribute explicit_paddings not defined in base api for Conv2D", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  -\r\n`Mac OS X 10.13.6`\r\n- TensorFlow version (use command below): - \r\n```\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-rc2-3-ga6d8ffae09\r\ntf.COMPILER_VERSION = v1.12.0-rc2-3-ga6d8ffae09\r\nSanity check: array([1], dtype=int32)\r\n```\r\n- Python version:\r\n`Python 3.6.5`\r\n\r\n\r\n**Describe the current behavior**\r\nHi, I try to run tensorflow using golang, </br>\r\nWhen I execute the following command: </br>`go generate github.com/tensorflow/tensorflow/tensorflow/go/op` </br>\r\nI get this error: </br>\r\n```\r\n2019/01/27 18:21:15 failed to process \u201capi_def_Conv2D.pbtxt\u201d: Attribute explicit_paddings not defined in base api for Conv2D\r\nexit status 1\r\nop/generate.go:18: running \u201cgo\u201d: exit status 1\r\n```\r\n \r\n", "comments": ["@asimshankar Can you PTAL? Thanks", "This is because your local libtensorflow.so has been compiled against older tree, which doesn't include commit ec81825aaf7e848d9f8ddffdf1e0d20aebe9172c which introduced this new attribute", "@davidgtunity \r\nIs this still an issue for you? Did you try this suggestion? \r\n> This is because your local libtensorflow.so has been compiled against older tree, which doesn't include commit [ec81825](https://github.com/tensorflow/tensorflow/commit/ec81825aaf7e848d9f8ddffdf1e0d20aebe9172c) which introduced this new attribute\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n\r\n"]}, {"number": 25231, "title": "tensorflow.contrib.lite.Interpreter.invoke() crashes ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os - Mojave\r\n- TensorFlow installed from (source or binary): from pip\r\n- TensorFlow version (use command below):  from pip -'v1.12.0-rc2-3-ga6d8ffae09', '1.12.0'\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nHey. \r\n-im building a test model from a net I designed for OCR. \r\n-The model is running fine on Tensorflow mobile and I get good results (the model is running fine and I have NO errors).\r\n-All the work is in python.\r\n-Part of the work is to convert the model to 'lite' and running it on mobile, I do it with this code : \r\n\r\n ```\r\n  input_arrays = [\"input\"]\r\n  output_arrays = [\"output\"]\r\n  converter = lite.TFLiteConverter.from_frozen_graph(\r\n  forzen_file_path, input_arrays, output_arrays, input_shapes=shape)\r\n  converter.allow_custom_ops = True\r\n  tflite_model = converter.convert()\r\n  open(output_path, \"wb\").write(tflite_model)\r\n```\r\n\r\nThis part is working well. \r\n\r\n- Using the following code (obtained from the docs ) Im checking the conversion:\r\n\r\n\r\n   ```\r\n    interpreter = lite.Interpreter(model_path=lite_model)\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get input and output tensors.\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_type = input_details[0]['dtype']\r\n    random_sample = np.random.random_sample(input_shape)\r\n    input_data = np.array(random_sample, dtype=input_type)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\n    interpreter.invoke() <---- crash! SIGSEGV 11 \r\n    output_data = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n\r\n\r\nAnd I get a crash with no details on what happened:\r\n\r\n```\r\n2019-01-27 17:45:08.423844: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n\r\n```\r\n\r\nI've tried to compile tensorflow and debug it myself but google uses bazel and I cant get it to import on CLion (with the bazel plugin). \r\n\r\nDoes anybody knows what is going on? \r\n\r\n**Describe the expected behavior**\r\nNot crashing :)\r\n\r\n**Code to reproduce the issue**\r\nI'm not allowed to share the model but i've shared the conversion and the interpreter code above. \r\n\r\n**Other info / logs**\r\nI want to state again these few things:\r\n- The model runs well on Tensorflow (not lite)\r\n- The conversion runs well with no errors. \r\n- The crash occurs when calling interpreter.invoke() \r\n\r\n**Thank you!**\r\n", "comments": ["I wanted to add and it might help, \r\nOut work is based on this project:\r\nhttps://github.com/IBM/tensorflow-hangul-recognition", "@YakirYehuda did you fix a problem?", "@ymodak can we do something with it as workaround?", "@YakirYehuda my problem was related with tensors with shapes > 4. It turned out that TFLite can\u2019t do any operation with them. Maybe it will help you", "Please try what @Oktai15 suggested and feel free to reopen if there's still problem.", "I have the same problem\r\nI was able to convert the [mask-rcnn from matterport](https://github.com/matterport/) model using [this script](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6 ) with tflite 1.3\r\n\r\n\r\nBut now i am facing a segmentation error, when i try to run inference with the tflite interpreter (Tried both 1.3 and 2.0 tflite versions)\r\n```python\r\ndef load_tf_model(model_path=\"mask_rcnn_coco_256_exp.tflite\"):\r\n    interpreter = tf.lite.Interpreter(model_path)\r\n    # Get input tensors.\r\n    input_details = interpreter.get_input_details()\r\n    interpreter.resize_tensor_input(input_details[0]['index'],(1,256,256,3))\r\n    interpreter.allocate_tensors()\r\n\r\n    # Get output tensors.\r\n    output_details = interpreter.get_output_details()\r\n    # Test model on random input data.\r\n    input_shape = input_details[0]['shape']\r\n    input_data = np.array(np.random.random_sample((1, 256, 256,3)), dtype=np.float32)\r\n    interpreter.set_tensor(input_details[0]['index'], input_data)\r\n    interpreter.invoke()\r\n\r\n    # The function `get_tensor()` returns a copy of the tensor data.\r\n    # Use `tensor()` in order to get a pointer to the tensor.\r\n    output_data = interpreter.get_tensor(output_details['index'])\r\n    print(output_data)\r\n```", "Hey @bmabir17 \r\n\r\nI have the same issue, I converted from a modified version of matterport mask rcnn (tf v2) and the session crashes when I invoke the interpreter. Also, isn't there other inputs beside the image ? \r\n\r\n@Oktai15 Can you explain further. Our model gets an input of 3 tensors :\r\n\r\n```\r\ninterpreter.set_tensor(input_details[0]['index'], image)\r\ninterpreter.set_tensor(input_details[1]['index'], A)\r\ninterpreter.set_tensor(input_details[2]['index'], B)\r\n```\r\n\r\nimage.shape = (1,1024,1024,3)", "> Hey @bmabir17\r\n> \r\n> I have the same issue, I converted from a modified version of matterport mask rcnn (tf v2) and the session crashes when I invoke the interpreter. Also, isn't there other inputs beside the image ?\r\n> \r\n> @Oktai15 Can you explain further. Our model gets an input of 3 tensors :\r\n> \r\n> ```\r\n> interpreter.set_tensor(input_details[0]['index'], image)\r\n> interpreter.set_tensor(input_details[1]['index'], A)\r\n> interpreter.set_tensor(input_details[2]['index'], B)\r\n> ```\r\n> \r\n> image.shape = (1,1024,1024,3)\r\n\r\nI suppose your model uses 4D-tensor (or more) in the middle of architecture and it\u2019s the problem for TFLite, i.e this constraint is not only for input, but also for every inner tensors"]}, {"number": 25230, "title": "Export keras merge layer.", "body": "This layer is accessible in raw Keras API, and useful to build custom merge layers.\r\n\r\nAlso fixed a typo preventing wheel from building successfully.", "comments": ["Rebasing since `setup.py` is fixed on master.", "@case540 no longer works on tensorflow, so taking over.\r\n\r\n@martinwicke @karmel @fchollet Does it make to export that symbol?", "No-- @dargor , can you provide an example of what you're trying to do?", "The final layer of a DDQN, for example : \r\n\r\n```python\r\nclass Q(_Merge):\r\n\r\n    def _merge_function(self, inputs):\r\n        advantage, value = inputs\r\n        return value + advantage - K.mean(advantage, axis=-1, keepdims=True)\r\n```\r\n", "Thank you for the PR. This is a private API, we will not export it.\r\n\r\nYour use case should be doable with a custom layer as such:\r\n\r\n```python\r\nclass YourMerge(Layer):\r\n\r\n    def call(self, inputs):\r\n        advantage, value = inputs\r\n        return value + advantage - K.mean(advantage, axis=-1, keepdims=True)\r\n```"]}, {"number": 25229, "title": "Native TF methods not found Exception (Installed as System Apps)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_templateem>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Galaxy S6 Edge\r\n\r\n**Describe the problem**\r\n\r\n<em>I am making an extension of the demo app which requires the app to be installed as a system app in the \"priv-app\" of the device. I have not changed anything in the demo app itself but I still get the following error whenever I open the TF detect app.</em>\r\n\r\n```\r\n2019-01-27 20:29:19.235 8201-8201/? E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n2019-01-27 20:29:19.236 8201-8201/? I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n2019-01-27 20:29:19.238 8201-8201/? D/AndroidRuntime: Shutting down VM\r\n2019-01-27 20:29:19.239 8201-8201/? E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.demo, PID: 8201\r\n    java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present in the APK.\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.prepareNativeRuntime(TensorFlowInferenceInterface.java:544)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:60)\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:97)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:167)\r\n        at org.tensorflow.demo.CameraActivity$5.onPreviewSizeChosen(CameraActivity.java:429)\r\n        at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:454)\r\n        at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:462)\r\n        at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:101)\r\n        at android.view.TextureView.getHardwareLayer(TextureView.java:389)\r\n        at android.view.TextureView.draw(TextureView.java:338)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17296)\r\n        at android.view.View.draw(View.java:18080)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17291)\r\n        at android.view.View.draw(View.java:18080)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)\r\n        at android.view.View.draw(View.java:18321)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17296)\r\n        at android.view.View.draw(View.java:18080)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17291)\r\n        at android.view.View.draw(View.java:18080)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17291)\r\n        at android.view.View.draw(View.java:18080)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:3966)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3752)\r\n        at android.view.View.draw(View.java:18321)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:919)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:17296)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:692)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:698)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:806)\r\n        at android.view.ViewRootImpl.draw(ViewRootImpl.java:3128)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2924)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2516)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1515)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:7091)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:927)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:702)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:638)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:913)\r\n        at android.os.Handler.handleCallback(Handler.java:751)\r\n        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n        at android.os.Looper.loop(Looper.java:154)\r\n        at android.app.ActivityThread.main(ActivityThread.java:6682)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1520)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1410)\r\n2019-01-27 20:29:24.764 8201-8210/org.tensorflow.demo E/System: Uncaught exception thrown by finalizer\r\n2019-01-27 20:29:24.768 8201-8210/org.tensorflow.demo E/System: java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.Session.close()' on a null object reference\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.close(TensorFlowInferenceInterface.java:276)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.finalize(TensorFlowInferenceInterface.java:287)\r\n        at java.lang.Daemons$FinalizerDaemon.doFinalize(Daemons.java:222)\r\n        at java.lang.Daemons$FinalizerDaemon.run(Daemons.java:209)\r\n        at java.lang.Thread.run(Thread.java:762)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSteps :\r\n1. Build Apk(s)\r\n2. Copy the apk to the priv-app of the device\r\n3. Restart the device\r\n4. Error Occurs on opening of TF Detect app\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["You'll need to make sure the native shared libraries are in a location where they can be properly loaded. If you're using a custom shared library name, you'll need to manually call `System.loadLibrary()` first. In any case, TensorFlow Mobile has been officially deprecated in favor of TensorFlow Lite."]}, {"number": 25228, "title": "TF Lite works wrong on quantized TF Hub retrained Inception V3 mnist model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android Emulator\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly 1.13.0.dev20190123\r\n- Python version:3.7.1\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n**Describe the current behavior**\r\n- Using TF Hub to retrain Inception V3 for mnist images\r\n- Converting the retrained model to quantized model\r\n- Deploy the converted quantized model to Android App\r\n- The quantized TF Lite works wrong like below:\r\n![image](https://user-images.githubusercontent.com/2778945/51802538-1385ab00-2286-11e9-8297-cd9c7c3c5350.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n- The mnist images are well classified.\r\n\r\n**Code to reproduce the issue**\r\nhttps://github.com/dpinthinker/TFGrocery/tree/master/MnistClassifier\r\n", "comments": ["@dpinthinker : Can it be due to your preprocessing \r\nhttps://github.com/dpinthinker/TFGrocery/blob/master/MnistClassifier/app/src/main/java/com/dpthinker/mnistclassifier/model/InceptionQuantizedModelConfig.java\r\n\r\nCheck quantized preprocessing and difference between quantized and floating point here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L164\r\n", "Thank you, Shashi.\n\nIt means I need to remove the getImageMean and getImageSTD methods.\n\nCould you tell me when should I call the getImageMean and getImageSTD\nmethods?\n\n\nShashi <notifications@github.com> \u4e8e 2019\u5e741\u670829\u65e5\u5468\u4e8c \u4e0a\u53487:57\u5199\u9053\uff1a\n\n> @dpinthinker <https://github.com/dpinthinker> : Can it be due to your\n> preprocessing\n>\n> https://github.com/dpinthinker/TFGrocery/blob/master/MnistClassifier/app/src/main/java/com/dpthinker/mnistclassifier/model/InceptionQuantizedModelConfig.java\n>\n> Check quantized preprocessing and difference between quantized and\n> floating point here:\n>\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L164\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/25228#issuecomment-458350152>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACpnQdmHpQH6BAPRXOG3ZOokV7JhpPttks5vH45bgaJpZM4aUwlo>\n> .\n>\n", "@dpinthinker : Correct or use 0 mean and 1 std if you want to keep your code.\r\n\r\nImage is RGB so pixels have value in range [0-255] for each channel.\r\nThe floating point model is trained with [-1.0, 1.0] as inputs, so the colors are converted from 0-255 to  [-1.0, 1.0].\r\n\r\nHowever quantized model uses uin8_t which have values is between 0-255, so we don't need to do preprocessing.\r\n", "@shashishekhar \r\nThank you!\r\nAfter removing mean and std, it still works wrong.\r\n<img width=\"392\" alt=\"2019-01-30 7 57 18\" src=\"https://user-images.githubusercontent.com/2778945/51948779-d75c7100-2464-11e9-8bc7-182d904d8108.png\">\r\n\r\nIt is the problem of my converted model?", "@dpinthinker : Using TF Hub to retrain Inception V3 for mnist images  --> for this can u plz share the url@tf-hub which module u used. And what was the test accuracy while retraining.\r\n\r\nAlso one question, did u check post retraining about the same image prediction before quantization and post quantization. We cant judge a model, by just one image prediction, it has to be some bunch of images, and make the score for the model.\r\n\r\nAlso i think it would be better, if u can share the code from step 1 to end. It will help review your steps followed for any mistakes.", "@ANSHUMAN87 \r\n# Retraining Model with TFHub\r\nUsing default retrain.py's default model, as following\r\npython3.6 [retrain.py](https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py\r\n) --image_dir mnist_imgs/\r\n\r\nIt should be `https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1`\r\n\r\n# Retrained model works fine\r\n\r\n[label_image.py](https://github.com/tensorflow/tensorflow/raw/master/tensorflow/examples/label_image/label_image.py)\r\n\r\n```\r\npython3.6 label_image.py --graph=output_graph.pb --labels=output_labels.txt --input_layer=Placeholder --output_layer=final_result --image=s3.jpg\r\n2019-02-02 08:18:24.687863: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n3 0.92819667\r\n2 0.027902907\r\n5 0.018210107\r\n9 0.010902734\r\n7 0.0056838035\r\n```\r\n\r\n# Converting to float TFLite model and deploy\r\nIt works fine, using the following cmd to convert.\r\n```\r\ntflite_convert \\\r\n  --output_file=inceptionv3_mnist.tflite \\\r\n  --graph_def_file=output_graph.pb \\\r\n  --input_shape=1,299,299,3 \\\r\n  --input_arrays=Placeholder \\\r\n  --output_arrays=final_result\r\n```\r\n\r\n# Converting to Quantized TFLite model and deploy \r\nIt works wrong, using the following cmd to convert.\r\n```\r\ntflite_convert \\\r\n  --output_file=inceptionv3_mnist_quantized_uint8.tflite \\\r\n  --graph_def_file=output_graph.pb \\\r\n  --input_shape=1,299,299,3 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_arrays=Placeholder \\\r\n  --output_arrays=final_result \\\r\n  --mean_values=128 \\\r\n  --std_dev_values=127 \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=255\r\n```", "What values are your inputs during float training preprocessed to?\r\n\r\nBased on those values you have to set std_value and mean_value accordingly. \r\n\r\nThe quantized model expects [0,255] inputs, but also expects these values to map to float.\r\nmean_value = an integer value from [0,255] that corresponds to float 0.0f.\r\nstd_value = 255 / (float_max - float_min)", "@suharshs \r\n\r\nThank you for your detailed answer. \r\n\r\nThe preprocessed floating model used is from TFHub. The url of it is [https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1)\r\n\r\nBut I didn't find any info of the std_value and mean_value from it.\r\nIf possible, could you help loop the tfhub guys in this issue?", "The link you provided says: \"The input images are expected to have color values in the range [0,1]\". This means the mean_value and std_value can be computed using those formulas above, mapping [0, 255] to [0, 1].\r\nmean_value  = 0. \r\nstd_value = 255/ (1-0) = 255\r\n\r\nThanks!", "@suharshs Sorry to reply late. It is still not working. So sad!", "Hi @dpinthinker ,\r\n\r\nBased on feedback that the contrib/quantize quantization-aware training tool is a bit brittle and hard to use on some model architectures, we have released a [post-training integer quantization tool](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba), that requires a small calibration dataset. Please take a look at the [tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb) and give it a try, it should work much better! And let us know if you run into any issues.\r\n\r\nClosing this issue, since we are rethinking and working on an api to replace contrib/quantize quantization-aware-training (although post-training quantization above should be sufficient for the majority of use cases).\r\n\r\nThis flow does not require std values or mean values.\r\n\r\nThanks!\r\n-Suharsh"]}, {"number": 25227, "title": "Porting codebase utilizing tf.slim to TF-2.0", "body": "A lot of my TensorFlow  codebase depends upon the use of pretrained models as a part of [tensorflow/models/research/slim/nets](https://github.com/tensorflow/models/tree/master/research/slim/nets) repository.\r\n\r\nI routinely utilize the dictionary of `end points` provided by `tf.slim` models in my codebase. Furthermore, the model definitions in the aforementioned repository is impressive and general in the sense that for models like `resnet_v2`, it allows me to directly specify the `output_stride` which leads to an automatic specification of dilation rates. \r\n\r\nAs I aim to write my new codes in TF-2.0 ( as well as thinking of porting my existing codebase to TF-2.0), I realize (based on documentation and my knowledge of Keras !!), that there is no concept of `end point` dictionaries and `output_stride` (wherever applicable). \r\n\r\nHence, I am confused about how to handle these situations. \r\n\r\nI believe that it could be very reasonable for the TensorFlow team to provide documentation on this issue for it could really make transition to TF-2.0,  very simple and practical. \r\n", "comments": ["To simplify the migration to TensorFlow 2.0, there is a [conversion tool](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) which updates TensorFlow 1.x Python code to use TensorFlow 2.0. Please take a look if haven't already. Thanks!", "Thank you. But does it specifically address the case of `end_point` dictionaries and `output_stride` ?", "@martinwicke Can you PTAL? Thanks!", "It does not. In fact, it does not handle anything specific to slim. \r\n\r\nYou can continue to use slim in 2.0, and if you are so dependent on its specifics, you may want to. slim makes heavy use of variable scopes and collections, and writing code using it will never truly feel like TensorFlow 2.0, but using tf.compat.v1 you will be able to make it work.\r\n\r\nWe will be publishing more detailed conversion guides. I will close this issue since we are already working on that (and we'll only forget to close it otherwise).", "@martinwicke To make it clear: \r\n1. The upgrade script does not handle this problem. \r\n2. Using tf.compat.v1 will not make it work. \r\n3. The only \u201cmore detailed conversion guide\u201d I was able to find 9 month since this issue was opened is only short paragraph [here](https://www.tensorflow.org/guide/migrate) which basically state \u201cit is not simple and it is your problem\u201d.\r\n\r\nYour statement was addressed to the person how opened this issue, but let me inform you how many people in fact this issue affects. If anyone at Google cares of course\u2026\r\n\r\nI came across this problem while trying to understand what\u2019s going wrong with [DeepLabCut](https://github.com/AlexEMG/DeepLabCut) ( hello @AlexEMG ) software. The DeepLabCut is a marker-less pose estimation software for analyzing behavior of various subjects in a video streams. It has a [Nature Neuroscience paper](https://www.nature.com/articles/s41593-018-0209-y/metrics) and become so popular that it [\"powers a motion-tracking revolution\"](https://www.nature.com/articles/d41586-019-02942-5). So as result, many Labs want to use DeepLabCut now because it, well, \u201chave a Nature paper\u201d!\r\n\r\nLooking inside of DeepLabCut\u2019s code unveil several severe technical problems however. These problems (so far) do not affect the main effect (e.g. pose detection) but anyone familiar with long term software support understands where it all goes\u2026\r\n\r\nDeepLabCut has been built on top of [Human Pose Estimation with TensorFlow](https://github.com/eldar/pose-tensorflow) project also presented in [DeeperCut](https://arxiv.org/abs/1605.03170) and [ArtTrack](https://arxiv.org/abs/1612.01465) papers. The DeeperCut rely on ResNet which was in tf.contrib.slim (TF 1.x) and therefore not available in TF 2.0 anymore...\r\n\r\nSo what do we have here. Google wrote TensorFlow 1.x and despite disastrous code quality it become popular (because it well\u2026 \u201cMade by Google\u201dand it is AI!). Then after TF 1.x become popular more and more people began to have severe problems (e.g. computational graph debugging issues) caused by initially disastrous design decisions. Then someone at Google finally realize that it make sense to try to fix everything, so as result TF 2.x was spitted out. As a consequence of that all code from tf.contrib was wiped out of TF 2.x betraying another Google's own creature [TF-Slim library](https://ai.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html) because \u201cit will never truly feel like TensorFlow 2.0\u201d leaving many people dealing with many issues by themselves...\r\n\r\nThis remind me worst practice used many times by Microsoft with their various \u201ccool APIs\u201d dropped 2 years after rolling out.\r\n", "@pichuan @tomerk How far are we with the slim port?", "Hi @martinwicke , I saw your mention here just now.\r\nShort answer: I do not have an ETA.\r\n\r\nLong answer:\r\nAfter my last update here: https://github.com/google-research/tf-slim/pull/1#issuecomment-570648906 , I did spend some time trying to incorporate @tomerk 's suggestion. \r\n\r\nThis is taking more time than I expected, and I don't think I fully understand the issue (or the solution) to give an ETA.\r\nNote that even if/when I resolve this, there will still be 2 other pending tasks that I don't plan to take on.\r\nGiven my schedule in the next few weeks, I won't be able to look further into this right now until at least February.", "While fighting with a similar porting, I bumped into this [colab notebook](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/migrate.ipynb#scrollTo=ulaB1ymO4pw5) containing a very juicy paragraph on the subject:\r\n\r\n> \r\n\r\n> # A note on Slim & contrib.layers\r\n> \r\n> A large amount of older TensorFlow 1.x code uses the Slim library, which was packaged with TensorFlow 1.x as tf.contrib.layers. As a contrib module, this is no longer available in TensorFlow 2.0, even in tf.compat.v1. Converting code using Slim to TF 2.0 is more involved than converting repositories that use v1.layers. In fact, it may make sense to convert your Slim code to v1.layers first, then convert to Keras.\r\n> \r\n>    -  Remove arg_scopes, all args need to be explicit\r\n>    -  If you use them, split normalizer_fn and activation_fn into their own layers\r\n>    -  Separable conv layers map to one or more different Keras layers (depthwise, pointwise, and separable Keras layers)\r\n>    -  Slim and v1.layers have different arg names & default values\r\n>    -  Some args have different scales\r\n>    -  If you use Slim pre-trained models, try out Keras's pre-traimed models from tf.keras.applications or TF Hub's TF2 SavedModels exported from the original Slim code.\r\n> \r\n> Some tf.contrib layers might not have been moved to core TensorFlow but have instead been moved to the TF add-ons package.\r\n\r\nI hope it is useful to other people!", "Are you kidding me?? Why this issue is closed while there is no solution yet?\r\n\r\nI spent days to learn Python lang to be able to work with TF. Then I followed instructions on installing and running TFv2.0 explain [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). After facing lots of unexpected issues trying to install TF(illegal instruction and building from source ..) finally I'm done installing it on my Ubuntu server. Now, I have `no attribute 'contrib'`  and interestingly, everyone is suggesting me to downgrade to TFv1.x to pass this stage too. Is't this nasty?\r\n I'm exhausted trying to launch TF.", "Hi Draxdave@, we're sorry you've found this to be a painful experience! Here's some information that might be useful to you or others who come across this issue in the future:\r\n\r\n1. That link you put is from the TF object detection library. The team that owns it is actively working on releasing the tf 2.x version of the object detection library + training scripts. If you want to use the training scripts for it that have already been released you need to use TF 1.x.\r\n\r\n2. Contrib is not packaged with Tensorflow 2.x. The design discussions around that decision can be found here https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n\r\n3. Due to it's design, TF slim does not work with eager execution enabled. That said, it has been made it available (with extremely light support) as a TF 2.0-binary compatible library (as long as you call `tf.compat.v1.disable_eager_execution()`) in https://github.com/google-research/tf-slim\r\n\r\nWe would welcome community contributions that make tf-slim work with v2 behavior enabled.", "@tomerk First of all, thanks for your quick response. \r\nYes I'm trying to use Object detection library. This is what I did (may save some time for others) :\r\n\r\n- I Installed TFv2.2 as a default released version but it gave me `Illegal instruction` so I built it from the source. After building and installing it, I got `no attribute 'contrib'` Error.\r\n- So I built TFv1.14 release as a tutorial was moving on with and again I had to build it from the source and install it, though I got #32947 issue afterwards. \r\n- This time I switched on TFv1.15.0rc2 according to an active Tensorflow contributor suggestion [here](https://github.com/tensorflow/tensorflow/issues/32947#issuecomment-538171238) and building from source stuff. I ended up with #30728 Keras issue.\r\n- Now I'm building TFv1.13.1 as some claimed that it works [here](https://github.com/tensorflow/tensorflow/issues/30728#issuecomment-511564565)\r\n- However, I found pre-built TF versions suitable for old processors (cpu/gpu) but they were not compatible with my target servers specs. See #19584 discussion and [here](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/103) \r\n\r\n\r\n_My machines specs:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                1\r\nOn-line CPU(s) list:   0\r\nThread(s) per core:    1\r\nCore(s) per socket:    1\r\nSocket(s):             1\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 26\r\nModel name:            Intel Core i7 9xx (Nehalem Class Core i7)\r\nStepping:              3\r\nCPU MHz:               3292.060\r\nBogoMIPS:              6584.12\r\nHypervisor vendor:     KVM\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              4096K\r\nNUMA node0 CPU(s):     0\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx lm constant_tsc rep_good nopl tsc_known_freq pni ssse3 cx16 sse4_1 sse4_2 x2apic popcnt hypervisor lahf_lm kaiser_\r\n\r\n\r\n**I'll update this post in case I find a solution other than Pytorch and downgrading**\r\n\r\n> Hi Draxdave@, we're sorry you've found this to be a painful experience! Here's some information that might be useful to you or others who come across this issue in the future:\r\n> \r\n> 1. That link you put is from the TF object detection library. The team that owns it is actively working on releasing the tf 2.x version of the object detection library + training scripts. If you want to use the training scripts for it that have already been released you need to use TF 1.x.\r\n> 2. Contrib is not packaged with Tensorflow 2.x. The design discussions around that decision can be found here https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n> 3. Due to it's design, TF slim does not work with eager execution enabled. That said, it has been made it available (with extremely light support) as a TF 2.0-binary compatible library (as long as you call `tf.compat.v1.disable_eager_execution()`) in https://github.com/google-research/tf-slim\r\n> \r\n> We would welcome community contributions that make tf-slim work with v2 behavior enabled.\r\n\r\n", "Facing the same problem trying to use Object detection library, has there been any update?", "> Facing the same problem trying to use Object detection library, has there been any update?\r\n\r\nTFv1.13 seems working for now, however I have not tested it in my real world project yet. Many warnings and rubbish message is being thrown at runtime which you shouldn't care about.  \r\n", "I am also facing major issues trying to work around getting tf.contrib.slim dependent code to work for my project. I started by downloading tf 1.4 version [ downgrade] but that does not help. I am considering re-writing the model layers in pure tensorflow instead to workaround this issue. Any guidance on how layer functionality from slim packages map to the more tensorflow 2  libraries?", "I think this is one of the reasons that many new projects (including google research projects) still use TF 1.x. An important issues like this are closed without providing any solution. I believe more and more AI projects will migrate to PyTorch once it has mature deployment tools for edge devices."]}, {"number": 25226, "title": "\"ModuleNotFoundError : No module named 'keras' \" Error while 'import keras' in Spyder(Python 3.5) launched through a environment with python 3.5 .", "body": "\r\n**System information**\r\n- Windows 8.1\r\n- TensorFlow version: 1.10.0\r\n- Python version:3.5\r\n- Installed using  conda\r\n- GCC/Compiler version\r\n\r\n\r\n\r\n\r\nI had created a new conda environment named py35 for using spyder. py35 with Python3.5. I had also installed theano,tensorflow and keras using conda install commands with env activated . In admin cmd import command works fine for all three libraries. But in Spyder it return the error \"ModuleNotFoundError: No module named  'keras'\" \r\n\r\n\r\n", "comments": ["Please take a look at a similar [issues](https://github.com/keras-team/keras/issues/4889)."]}, {"number": 25225, "title": "Issue #23410 fix - Change deprecated 'np.asscalar(a)' function", "body": "'np.asscalar(a)' is deprecated since NumPy v1.16, use 'a.item()' instead'. \r\nSee 'numpy/lib/type_check.py'.", "comments": ["This was fixed in #25169\r\n\r\nUnfortunately this PR was automatically reverted in 51c7409562e917fd3fe2012ae57699547d0aebec shortly after."]}, {"number": 25224, "title": "ModuleNotFoundError: No module named 'tensorflow.core.framework'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I had an issue when I run\r\n`from keras.models import Sequential`\r\non jupyter\r\n\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-cce439ae43df> in <module>\r\n      1 import numpy as np\r\n----> 2 from keras.models import Sequential\r\n      3 from keras.layers import Dense\r\n      4 from keras.utils import np_utils\r\n      5 from sklearn.model_selection import train_test_split\r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\keras\\__init__.py in <module>\r\n      1 from __future__ import absolute_import\r\n      2 \r\n----> 3 from . import utils\r\n      4 from . import activations\r\n      5 from . import applications\r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\keras\\utils\\__init__.py in <module>\r\n      4 from . import data_utils\r\n      5 from . import io_utils\r\n----> 6 from . import conv_utils\r\n      7 \r\n      8 # Globally-importable utils.\r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py in <module>\r\n      7 from six.moves import range\r\n      8 import numpy as np\r\n----> 9 from .. import backend as K\r\n     10 \r\n     11 \r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\keras\\backend\\__init__.py in <module>\r\n     87 elif _BACKEND == 'tensorflow':\r\n     88     sys.stderr.write('Using TensorFlow backend.\\n')\r\n---> 89     from .tensorflow_backend import *\r\n     90 else:\r\n     91     # Try and load external backend.\r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py in <module>\r\n      4 \r\n      5 import tensorflow as tf\r\n----> 6 from tensorflow.python.framework import ops as tf_ops\r\n      7 from tensorflow.python.training import moving_averages\r\n      8 from tensorflow.python.ops import tensor_array_ops\r\n\r\n~\\Downloads\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in <module>\r\n     30 from six.moves import xrange  # pylint: disable=redefined-builtin\r\n     31 \r\n---> 32 from tensorflow.core.framework import attr_value_pb2\r\n     33 from tensorflow.core.framework import function_pb2\r\n     34 from tensorflow.core.framework import graph_pb2\r\n\r\nModuleNotFoundError: No module named 'tensorflow.core.framework'\r\n```\r\n\r\nCould anyone solve this issue?\r\n\r\n", "The import fails probably because your Anaconda or local Jupyter notebook environment is missing the `keras` package. For the `keras` installation in Anaconda please refer to\r\n\r\nhttps://anaconda.org/conda-forge/keras\r\n\r\nIf you're just running Jupyter locally then you need to ensure that you've installed `keras` via `pip`.", "Thank you!\r\nI have tried installing keras in Anaconda however still had same issue.\r\nAnd finally it worked after I re-installed anaconda, python and all libraries.\r\n", "Installing Keras didnt work for me "]}, {"number": 25223, "title": "Raspberry PI example label image patch", "body": "In the Makefile, there were missing options that lead to compile / linking error :\r\n* `absl` , according to [issue 22240](https://github.com/tensorflow/tensorflow/issues/22240)\r\n* `nsync`, according to [issue12810](https://github.com/tensorflow/tensorflow/issues/12810)\r\n\r\nAlso in label_image.cc , there was an compile error as described in [issue 5200](https://github.com/tensorflow/tensorflow/issues/5200), the error can be solved if the included header `<stdio.h>` is moved before `<jpeglib.h>`.\r\n\r\nThe modified Makefile and label_image.cc is tested in my Raspbian Stretch 9 and GCC 6.3.0, it works well .\r\n\r\n", "comments": ["Nagging Reviewer @caisq: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 44 days with no activity and the `awaiting review` label has been applied."]}, {"number": 25222, "title": "Duplicated Java outer classname", "body": "**Describe the current behavior**\r\nOption values of `java_outer_classname` in `tensorflow/compiler/tf2xla/host_compute_metadata.proto` and `tensorflow/compiler/tf2xla/tf2xla.proto` are the same, result in protoc failure for Java:\r\n\r\n~~~\r\n> protoc: stdout: . stderr: tensorflow/compiler/xla/rpc/xla_service.proto: warning: Import tensorflow/compiler/xla/xla_data.proto but not used.\r\n  tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/framework/graph.proto but not used.\r\n  tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/protobuf/cluster.proto but not used.\r\n  org/tensorflow/tf2xla/Tf2XlaProtos.java: Tried to write the same file twice.\r\n~~~\r\n\r\n**Describe the expected behavior**\r\nprotoc succeeds without error.\r\n`java_outer_classname` in `tensorflow/compiler/tf2xla/host_compute_metadata.proto` may be renamed to `HostComputeMetadataProtos`\r\n\r\n**Code to reproduce the issue**\r\n\r\nJust protoc\r\n\r\n**Other info / logs**\r\n", "comments": ["@YuanWenqing  Could you describe more about the issue and context. Is this error during installation?\r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!", "@jvishnuvardhan I think this issue is just a compiling error when both `tensorflow/compiler/tf2xla/tf2xla.proto` and `tensorflow/compiler/tf2xla/host_compute_metadata.proto` are put in input files, no matter with operating system or architecture. Try this command:\r\n\r\n~~~shell\r\nmkdir /tmp/tf\r\ncd /path/to/tensorflow-project\r\nprotoc --proto_path=. --java_out=/tmp/tf tensorflow/compiler/tf2xla/tf2xla.proto tensorflow/compiler/tf2xla/host_compute_metadata.proto\r\n~~~\r\n\r\nThen you'll get:\r\n~~~bash\r\norg/tensorflow/tf2xla/Tf2XlaProtos.java: Tried to write the same file twice.\r\n~~~\r\n\r\nLet me explain my situation to make this issue more clear and what happens. Actually what I need is a jar including all proto class from tensorflow and serving. Compiling of proto files in serving depends on proto files in tensorflow, so I put them all in one project and build them, like <https://github.com/YuanWenqing/tensorflow4j>. As you can see in the previous, building failed because of duplicated java_outer_classname option value in these two proto files.\r\nHope it is clear now :)", "@jvishnuvardhan I\u2019m not sure why I was added to this, I work on TensorFlow.js and have no state here :)", "@jlebar Could you please take a look at this issue? Thanks!", "@jvishnuvardhan I'm also not sure why I was added to this -- I work on XLA.", "@YuanWenqing Is this still an issue. I know this is a stale issue. I am not sure whether it was resolved or not. Could you check with recent TF versions and let us know whether the issue persists. thanks!", "I suppose this issue is very clear and just a typo mistake. \r\n\r\nLook at these two files and check:\r\n\r\n[tensorflow/compiler/tf2xla/host_compute_metadata.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/host_compute_metadata.proto)\r\n[tensorflow/compiler/tf2xla/tf2xla.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/tf2xla.proto)\r\n\r\nThey both share the same `java_outer_classname` option value, so that error `org/tensorflow/tf2xla/Tf2XlaProtos.java: Tried to write the same file twice.` encountered when compiling proto files under `tensorflow/compiler/tf2xla` into java.\r\n\r\nI see there is a naming convention in this project for the `java_outer_classname` option value: a snake-to-camel conversion from the filename and then `Protos` suffix appended. So `java_outer_classname` option value should be like `HostComputeMetadataProtos` in file [tensorflow/compiler/tf2xla/host_compute_metadata.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/host_compute_metadata.proto) , but now `Tf2XlaProtos` is there. \r\n\r\nIsn't this a simple typo mistake, right?\r\n", "Still facing the same issue. @YuanWenqing Any solution or update on the issue?", "> Still facing the same issue. @YuanWenqing Any solution or update on the issue?\r\n\r\nNo better solution but correct it manually \u256e(\u256f\u25bd\u2570)\u256d", "@sjamesr @jvishnuvardhan Please help with this. We are still facing the same issue while compiling proto to java sources. ", "what happened to this issue? this prevents one from compiling the java protocol buffers without modification.", "@YuanWenqing \r\nIs this still an issue?\r\nCould you please check with recent TF version 2.6.0 and let us know whether the issue persists?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25222\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/25222\">No</a>\n"]}, {"number": 25221, "title": "fix build wheel parse error", "body": "Sat Jan 26 03:08:49 UTC 2019 : === Building wheel\r\nerror in tensorflow setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at \"'< 1.14.0'\"", "comments": ["I encountered a similar issue, can verify that the proposed change fixes the build failure.", "Actually, tensorflow_estimator 1.13 is not out yet and I still face the following error after the fix:\r\n```\r\n  Could not find a version that satisfies the requirement tensorflow-estimator<1.14.0,>=1.13.0 (from tensorflow==1.12.0) (from versions: 1.10.6, 1.10.7, 1.10.8, 1.10.9, 1.10.10, 1.10.11, 1.10.12, 1.13.0rc0)\r\nNo matching distribution found for tensorflow-estimator<1.14.0,>=1.13.0 (from tensorflow==1.12.0)\r\n```\r\n\r\nChange the requirement to `>= 1.13rc0` will fix the build issue:\r\n```diff\r\ndiff --git a/tensorflow/tools/pip_package/setup.py b/tensorflow/tools/pip_package/setup.py\r\nindex 678531643c..982aeff05d 100644\r\n--- a/tensorflow/tools/pip_package/setup.py\r\n+++ b/tensorflow/tools/pip_package/setup.py\r\n@@ -58,7 +58,7 @@ REQUIRED_PACKAGES = [\r\n     'six >= 1.10.0',\r\n     'protobuf >= 3.6.1',\r\n     'tensorboard >= 1.12.0, < 1.13.0',\r\n-    'tensorflow_estimator >= 1.13.0 < 1.14.0',\r\n+    'tensorflow_estimator >= 1.13.0rc0, < 1.14.0',\r\n     'termcolor >= 1.1.0',\r\n ]\r\n```", "Huh, Im positive that I tested that >=1.13 <1.14 pulled in the rc0 build. Is that not the case?", "@case540 The error was encountered when I used Ubuntu 18.04 with the following version of python and pip (system default python3):\r\n```\r\nubuntu@ubuntu:~$ python3 --version\r\nPython 3.6.7\r\nubuntu@ubuntu:~$ pip3 --version\r\npip 18.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)\r\n```", "This problem is resoved with PR #25244."]}, {"number": 25220, "title": "Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nNo, compiled from Source \r\n- TensorFlow version (use command below):\r\n1.12\r\n- Python version:\r\n2.7\r\n- Bazel version (if compiling from source):\r\nBazel release 0.17.2\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\n- CUDA/cuDNN version:\r\nCuda 10.0/ cuDNN 7.3.1\r\n- GPU model and memory:\r\nNvidia GeForce RTX 2080 (Two gpus) each with 8 GM memory\r\n\r\n**Describe the current behavior**\r\n\r\nI've compiled tensorflow using the following command to get libtensorflow_cc.so. (Non Monolithic build)\r\n`bazel build --config=cuda //tensorflow:libtensorflow_cc.so`\r\n\r\nI would like to use multiple gpus at inference time in C++. Currently I have two GeForce RTX 2080 GPUs. So I'm running two threads (with thread id 0 and 1) in a c++ standalone example. Currently I'm using label_image ([main.cc)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc). \r\n\r\nI want to have two tensorflow session, one session for each thread using each gpu. Thread with thread_id 0 uses gpu:0 and thread with thread_id 1 uses gpu:1\r\n\r\nMy load_graph method looks as follows:\r\n\r\n```\r\nStatus LoadGraph(const string& graph_file_name,\r\n                 std::shared_ptr<tensorflow::Session>* session, int i_threadid) {\r\n  tensorflow::GraphDef graph_def;\r\n  printf(\".. IN load graph %d\\n\", i_threadid);\r\n  Status load_graph_status =\r\n      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\r\n  if (!load_graph_status.ok()) {\r\n    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\r\n                                        graph_file_name, \"'\");\r\n  }\r\n  tensorflow::SessionOptions session_options;\r\n  if(i_threadid == 0) {\r\n    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n    tensorflow::graph::SetDefaultDevice(\"/device:GPU:0\", &graph_def);\r\n  } else if(i_threadid == 1) {\r\n    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n    tensorflow::graph::SetDefaultDevice(\"/device:GPU:1\", &graph_def);\r\n  }\r\n  session_options.config.mutable_gpu_options()->set_allow_growth(true);\r\n  session_options.config.set_allow_soft_placement(true);\r\n  session->reset(tensorflow::NewSession(session_options));\r\n\r\n  //tensorflow::graph::SetDefaultDevice(\"/cpu:0\", &graph_def);\r\n  Status session_create_status = (*session)->Create(graph_def);\r\n  if (!session_create_status.ok()) {\r\n    return session_create_status;\r\n  }\r\n  return Status::OK();\r\n}\r\n```\r\n\r\nIn the else part: if I use the following line:    \r\n`session_options.config.mutable_gpu_options()->set_visible_device_list(\"1\");`, I'm getting the following error:\r\n```\r\n2019-01-26 21:47:00.872434: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-01-26 21:47:01.130371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-01-26 21:47:01.130396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-26 21:47:01.388128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-26 21:47:01.388163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-26 21:47:01.388170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-26 21:47:01.388331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2019-01-26 21:47:01.632637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 7.76GiB freeMemory: 7.47GiB\r\n2019-01-26 21:47:01.632673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 1\r\n2019-01-26 21:47:01.898614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-26 21:47:01.898649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      1 \r\n2019-01-26 21:47:01.898655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N \r\n2019-01-26 21:47:01.898776: E tensorflow/core/common_runtime/session.cc:64] Failed to create session: Already exists: TensorFlow device (GPU:0) is being mapped to multiple CUDA devices (1 now, and 0 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083\r\nSegmentation fault (core dumped)\r\n\r\n```\r\n\r\nSo I added    `session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");` so the second thread with thread_1 is creating session that uses both GPUs (or atleast initializing to utilize both gpus) but the graph runs on the second gpu.\r\nNow I'm getting the output as follows without any error:\r\n```\r\n2019-01-26 21:51:20.126025: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\n2019-01-26 21:51:20.380183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:17:00.0\r\ntotalMemory: 7.77GiB freeMemory: 7.62GiB\r\n2019-01-26 21:51:20.380209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2019-01-26 21:51:20.637473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-26 21:51:20.637508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \r\n2019-01-26 21:51:20.637513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \r\n2019-01-26 21:51:20.637668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2019-01-26 21:51:20.892308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: \r\nname: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.8\r\npciBusID: 0000:65:00.0\r\ntotalMemory: 7.76GiB freeMemory: 7.47GiB\r\n2019-01-26 21:51:20.892414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1\r\n2019-01-26 21:51:21.142666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-26 21:51:21.142699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 \r\n2019-01-26 21:51:21.142705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N N \r\n2019-01-26 21:51:21.142709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   N N \r\n2019-01-26 21:51:21.142932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7337 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5)\r\n2019-01-26 21:51:21.143217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7187 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080, pci bus id: 0000:65:00.0, compute capability: 7.5)\r\n\r\n```\r\n\r\nAnd the resizing image is done using the following code as I want to run this resizing computation on CPU to get better efficiency\r\n```\r\nStatus LoadImageResizeGraph(std::shared_ptr<tensorflow::Session>* session, const int input_height,\r\n                               const int input_width, const float input_mean,\r\n                               const float input_std) {\r\n  string output_name = \"normalized\";\r\n  auto root = tensorflow::Scope::NewRootScope();\r\n  using namespace ::tensorflow::ops;  // NOLINT(build/namespaces)\r\n\r\n  // use a placeholder to read input data\r\n  auto file_reader =\r\n      Placeholder(root.WithOpName(\"input\"), tensorflow::DataType::DT_STRING);\r\n\r\n  // Now try to figure out what kind of file it is and decode it.\r\n  const int wanted_channels = 3;\r\n  tensorflow::Output image_reader;\r\n  \r\n    image_reader = DecodeJpeg(root.WithOpName(\"jpeg_reader\"), file_reader,\r\n                              DecodeJpeg::Channels(wanted_channels));\r\n  // Now cast the image data to float so we can do normal math on it.\r\n  auto float_caster =\r\n      Cast(root.WithOpName(\"float_caster\"), image_reader, tensorflow::DT_FLOAT);\r\n  // The convention for image ops in TensorFlow is that all images are expected\r\n  // to be in batches, so that they're four-dimensional arrays with indices of\r\n  // [batch, height, width, channel]. Because we only have a single image, we\r\n  // have to add a batch dimension of 1 to the start with ExpandDims().\r\n  auto dims_expander = ExpandDims(root, float_caster, 0);\r\n  // Bilinearly resize the image to fit the required dimensions.\r\n  auto resized = ResizeBilinear(\r\n      root, dims_expander,\r\n      Const(root.WithOpName(\"size\"), {input_height, input_width}));\r\n  // Subtract the mean and divide by the scale.\r\n  Div(root.WithOpName(output_name), Sub(root, resized, {input_mean}),\r\n      {input_std});\r\n\r\n  // This runs the GraphDef network definition that we've just constructed, and\r\n  // returns the results in the output tensor.\r\n  tensorflow::GraphDef graph_def;\r\n  TF_RETURN_IF_ERROR(root.ToGraphDef(&graph_def));\r\n\r\n  tensorflow::SessionOptions session_options;\r\n  session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n  session_options.config.mutable_gpu_options()->set_allow_growth(true);\r\n  session_options.config.set_allow_soft_placement(true);\r\n  session->reset(tensorflow::NewSession(session_options));\r\n  tensorflow::graph::SetDefaultDevice(\"/cpu:0\", &graph_def);\r\n  TF_RETURN_IF_ERROR((*session)->Create(graph_def));\r\n  return Status::OK();\r\n}\r\n\r\n```\r\n\r\nI'm using Inception_v3 as specified in the [main.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc) and want to run inference on 1000 images. Each thread runs 500 images each in parallel for efficient runtime.\r\n\r\nBut at the end of the program, I'm getting the following error\r\n\r\n```\r\n[8.853 s] Finished running 1000 images successfully\r\n2019-01-26 21:59:10.712562: F tensorflow/stream_executor/cuda/cuda_driver.cc:206] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)\r\nAborted (core dumped)\r\n```\r\n\r\nI searched the issues history and seems other people are facing the similar problem but no successful resolution has been metioned\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/3509\r\nhttps://github.com/tensorflow/tensorflow/issues/10961\r\nhttps://github.com/tensorflow/tensorflow/issues/526\r\n\r\nIn the issue #526, one of the owners of the StreamExecutor has mentioned some resolution \r\nhttps://github.com/tensorflow/tensorflow/issues/526#issuecomment-236259722 \r\nBut it is not working for me.\r\n\r\nCan someone please help me to fix this error and use multiple gpus with multiple threads in a single program.\r\n\r\nThank you\r\n\r\n**Code to reproduce the issue**\r\nMy sample code:\r\n```\r\n\r\nint Initialize(const string& graph_file_name, int i_nthreads) {\r\n  int32 input_width = 299;\r\n  int32 input_height = 299;\r\n  float input_mean = 0;\r\n  float input_std = 255;\r\n  nthreads = i_nthreads;\r\n\r\n  // Initialize all sessions;\r\n  for(int i = 0; i < nthreads; i++) {\r\n\r\n    int threadid = i;\r\n    if(threadid == 0) {\r\n      LoadGraph(graph_file_name, &global_session_00, 0);\r\n    } else if (threadid == 1) {\r\n      LoadGraph(graph_file_name, &global_session_10, 1);\r\n    }\r\n  }\r\n\r\n  LoadImageResizeGraph(&global_session_01, input_height, input_width, input_mean, input_std);\r\n}\r\n\r\nvoid *Perform_Computation_Parallel(void *arg1)\r\n{\r\n\r\n  \r\n  string input_layer = \"input\";\r\n  string output_layer = \"InceptionV3/Predictions/Reshape_1\";\r\n\r\n\r\n  int threadid = (int)(((size_t)(arg1)));\r\n  printf(\"threadid %d\\n\", threadid);\r\n\r\n  std::shared_ptr<tensorflow::Session> session;\r\n  if(threadid == 0) {\r\n    session = global_session_00;\r\n  } else if(threadid == 1) {\r\n    session = global_session_10;\r\n  }\r\n\r\n  char *image_path = (char *)malloc(1000000);\r\n  for(int r = threadid; r < global_number_of_images; r += nthreads)\r\n  {\r\n    sprintf(image_path, \"%s\", global_images[r]);\r\n    printf(\"%s --------> %d \\n\", image_path, threadid);\r\n\r\n    std::string image_file(image_path);\r\n    std::vector<Tensor> resized_tensors;\r\n\r\n    // read file_name into a tensor named input\r\n    Tensor input_image(tensorflow::DT_STRING, tensorflow::TensorShape());\r\n    \r\n    ReadEntireFile(tensorflow::Env::Default(), image_file, &input_image);\r\n\r\n    Status run_resize_status = global_session_01->Run({{\"input\", input_image}},\r\n                                      {\"normalized\"}, {}, &resized_tensors);\r\n\r\n    if (!run_resize_status.ok()) {\r\n      LOG(ERROR) << \"Running resize graph failed: \" << run_resize_status;\r\n    } \r\n    // else if(run_resize_status.ok()){\r\n    //   std::cout << \"successfully resized image \"<< image_file << \"\\n\";\r\n    // }\r\n\r\n    // Status read_tensor_status =\r\n    //         ReadTensorFromImageFile(image_file, input_height, input_width, input_mean,\r\n    //                                 input_std, &resized_tensors);\r\n    // if (!read_tensor_status.ok()) {\r\n    //   LOG(ERROR) << read_tensor_status;\r\n    //   //return -1;\r\n    // }\r\n    const Tensor& resized_tensor = resized_tensors[0];\r\n\r\n\r\n    // Actually run the image through the model.\r\n    std::vector<Tensor> outputs;\r\n    Status run_status = session->Run({{input_layer, resized_tensor}},\r\n                                     {output_layer}, {}, &outputs);\r\n    if (!run_status.ok()) {\r\n      LOG(ERROR) << \"Running model failed: \" << run_status;\r\n      //return -1;\r\n    }\r\n  }\r\n\r\n    \r\n    \r\n}\r\n\r\nvoid Perform_Computation(void)\r\n{\r\n    for(long long int i = 1; i < nthreads; i++) pthread_create(&threads[i], NULL, Perform_Computation_Parallel, (void *)(i));\r\n    Perform_Computation_Parallel(0);\r\n    for(long long int i = 1; i < nthreads; i++) pthread_join(threads[i], NULL);\r\n}\r\n\r\nint main(int argc, char* argv[]) {\r\n  //printf(\"Hello World!!!\\n\");\r\n\r\n  // These are the command-line flags the program can understand.\r\n  // They define where the graph and input data is located, and what kind of\r\n  // input the model expects. If you train your own model, or use something\r\n  // other than inception_v3, then you'll need to update these.\r\n  string image = \"./data/grace_hopper.jpg\";\r\n  string graph =\r\n      \"./data/inception_v3_2016_08_28_frozen.pb\";\r\n  string labels =\r\n      \"./data/imagenet_slim_labels.txt\";\r\n  int32 input_width = 299;\r\n  int32 input_height = 299;\r\n  float input_mean = 0;\r\n  float input_std = 255;\r\n  string input_layer = \"input\";\r\n  string output_layer = \"InceptionV3/Predictions/Reshape_1\";\r\n  bool self_test = false;\r\n  string root_dir = \"\";\r\n  string test_file = \"./data/images/test_images_1000.txt\";\r\n  string out_file = \"./data/results/test_results_1000.txt\";\r\n\r\n  //nthreads = 2;\r\n  int ret_init_val = Initialize(graph, nthreads);\r\n  std::cout << \" Initialization done --------------------------\\n\";\r\n  FILE *fp = fopen(test_file.c_str(), \"r\");\r\n  int nol = 0;\r\n\r\n\r\n  char *line = (char *)malloc(2048);\r\n\r\n  while (!feof(fp))\r\n  {\r\n    line[0] = '\\0';\r\n    fgets(line, 2048, fp);\r\n    if (line[0] == '\\0')  break;\r\n    nol++;\r\n  }\r\n  fclose(fp);\r\n\r\n  global_number_of_images = nol;\r\n\r\n  global_images = (char **)malloc(global_number_of_images * sizeof(char *));\r\n  \r\n\r\n  fp = fopen(test_file.c_str(), \"r\");\r\n  for(int q = 0; q < global_number_of_images; q++)\r\n  {\r\n    line[0] = '\\0';\r\n    fgets(line, 2048, fp);\r\n    line[strlen(line)-1] = '\\0';\r\n    global_images[q] = (char *)malloc(strlen(line)+10);\r\n    sprintf(global_images[q], \"%s\", line);\r\n  }\r\n  fclose(fp);\r\n  double t0 = elapsed();\r\n  Perform_Computation();\r\n  printf (\"[%.3f s] Finished runnning %d images successfully\\n\", elapsed() - t0, nol);\r\n  return 0;\r\n}\r\n```\r\n", "comments": ["@aselle @henline Any help on how to resolve this bug or if I'm doing something wrong. \r\n\r\nI'm getting the following error while trying to use two  gpus on two parallel threads: \r\n**2019-01-26 21:59:10.712562: F tensorflow/stream_executor/cuda/cuda_driver.cc:206] Check failed: CUDA_SUCCESS == cuCtxSetCurrent(cuda_context->context()) (0 vs. 4)\r\nAborted (core dumped)**\r\n\r\nCurrently my GPUs are running on DEFAULT mode, and I also tried with EXCLUSIVE_PROCESS as well but still facing the same issue.", "pinging @tensorflowbutler \r\n\r\nIt seems it is a bug in tensorflow. Can you please help me to resolve this issue ?", "Hi @azaks2 ,\r\n\r\nCan you please let me know if I'm doing something wrong here ? Or how can I resolve this issue to efficiently use both of my GPUs at inference time in two separate sessions ?\r\n\r\nThank you\r\n\r\n", "Ping.. Any update on this one ? Is there a way I can resolve this issue ?", "@tumusudheer sorry for the late response. I was trying to reproduce your problem. Your repro code is incomplete and missing things like the model file, the `#include`s, build script, etc. It would be very helpful if you could provide a complete example. Thanks.", "Hi @aaroey,\r\n\r\nThank you very much. I pushed the code to my [git](https://github.com/tumusudheer/tensorflow_cpp_inference_multiple_gpus)\r\n\r\nPlease let me know if you have any trouble accessing the code.\r\n\r\nThe .so file is compiled using ```bazel build --config=cuda //tensorflow:libtensorflow_cc.so``` please refer to README file\r\n\r\n\r\nBasically I've a bunch of images (assume 1000),  and I've two gpus. I want to create two sessions, each one using one gpu.  And want to run two parallel threads so that each thread can take a session and run inference for half of the input images (500) each. So in that way to gpus are sharing the load. Preporcessing such as resizing the images should be handled by cpu.\r\n\r\nI'm using inceptionv3 graph as given [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image)\r\nThank you\r\n\r\n", "Thanks for the code. I ran it with TF master instead of r1.12 and it seemed to work well, below are the logs:\r\n```\r\n++ ./bin/main_multiple_images_parallel_gpu.out\r\n.. IN load graph 0\r\n2019-05-13 13:23:32.728379: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-13 13:23:32.765179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594195000 Hz\r\n2019-05-13 13:23:32.769016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561cc3c7f1b0 executing computations on platform Host. Devices:\r\n2019-05-13 13:23:32.769043: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-13 13:23:32.772489: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-05-13 13:23:33.310919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561cc3d39010 executing computations on platform CUDA. Devices:\r\n2019-05-13 13:23:33.310986: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN V, Compute Capability 7.0\r\n2019-05-13 13:23:33.311001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Quadro M2000, Compute Capability 5.2\r\n2019-05-13 13:23:33.312823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\nname: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:04:00.0\r\n2019-05-13 13:23:33.313799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 1 with properties: \r\nname: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\npciBusID: 0000:03:00.0\r\n2019-05-13 13:23:33.317937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1705] Ignoring visible gpu device (device: 1, name: Quadro M2000, pci bus id: 0000:03:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 7.0.\r\n2019-05-13 13:23:33.317970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n2019-05-13 13:23:33.318438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.9.0\r\n2019-05-13 13:23:33.320926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-13 13:23:33.320958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 1 \r\n2019-05-13 13:23:33.320976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N N \r\n2019-05-13 13:23:33.320990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 1:   N N \r\n2019-05-13 13:23:33.323733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n.. IN load graph 1\r\n2019-05-13 13:23:33.572213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\nname: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:04:00.0\r\n2019-05-13 13:23:33.572805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 1 with properties: \r\nname: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\npciBusID: 0000:03:00.0\r\n2019-05-13 13:23:33.575175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1705] Ignoring visible gpu device (device: 1, name: Quadro M2000, pci bus id: 0000:03:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 7.0.\r\n2019-05-13 13:23:33.575195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n2019-05-13 13:23:33.575247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-13 13:23:33.575259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 1 \r\n2019-05-13 13:23:33.575268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N N \r\n2019-05-13 13:23:33.575276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 1:   N N \r\n2019-05-13 13:23:33.576871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n2019-05-13 13:23:33.736453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\nname: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\npciBusID: 0000:04:00.0\r\n2019-05-13 13:23:33.737695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n2019-05-13 13:23:33.737739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-13 13:23:33.737753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 \r\n2019-05-13 13:23:33.737762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N \r\n2019-05-13 13:23:33.739178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n Initialization done --------------------------\r\nthreadid 0\r\n./data/grace_hopper.jpg --------> 0 \r\nthreadid 1\r\n./data/grace_hopper.jpg --------> 1 \r\n2019-05-13 13:23:33.748316: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1328] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-05-13 13:23:37.116325: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\nMOdel ran succesfully : output size MOdel ran succesfully : output size 11\r\n\r\n[5.178 s] Finished runnning 2 images successfully\r\n2019-05-13 13:23:39.110162: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x7f627888eb80: UNKNOWN ERROR (4)\r\n2019-05-13 13:23:39.118689: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x7f627888eb80: UNKNOWN ERROR (4)\r\n\r\n```\r\n\r\nThere are errors at the end about failures of destroying cuda events, but I think they're not related to this issue. So would you help to check that the problem is gone with TF master branch?\r\n\r\nI also did a minor change to your code: \r\n```\r\n$ git diff src/main_multiple_images_parallel.cc\r\ndiff --git a/src/main_multiple_images_parallel.cc b/src/main_multiple_images_parallel.cc\r\nindex ebc498c4..18e06185 100644\r\n--- a/src/main_multiple_images_parallel.cc\r\n+++ b/src/main_multiple_images_parallel.cc\r\n@@ -171,7 +171,7 @@ Status LoadGraph(const string& graph_file_name,\r\n   }\r\n   tensorflow::SessionOptions session_options;\r\n   if(i_threadid == 0) {\r\n-    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n+    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n     tensorflow::graph::SetDefaultDevice(\"/device:GPU:0\", &graph_def);\r\n   } else if(i_threadid == 1) {\r\n     session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n```\r\nas otherwise there would be gpu device initialization problems.", "Sorry, in my run above it actually ignored my second GPU. I'll retry and get back later.", "Hi @aaroey,\r\n\r\nJust checking if you are waiting for me to check something from my side ? Did you find the reason for the error and how to fix it? \r\n\r\nThank you", "@tumusudheer I can reproduce the error now. From the cuda documentation:\r\n```\r\nCUDA_ERROR_DEINITIALIZED = 4\r\nThis indicates that the CUDA driver is in the process of shutting down. \r\n```\r\nI'm not sure what caused that, maybe something related to stream executor. \r\n\r\n@chsigg could you help to take a look?", "Thank you very much @aaroey. ", "Ugh, yeah -- session options are weird. You cannot have two sessions with different visible_device_list. The config is statically initialized along with the first session.\r\n\r\nWhat I've done in the past is to update the graph to assign the nodes to one or the other GPU:\r\nhttps://github.com/mlperf/training/blob/e8237dc295deab8f9064dc2e0651c8234710c0d5/reinforcement/tensorflow/minigo/cc/dual_net/tf_dual_net.cc#L136\r\n\r\nLet me know if this works.", "> Thanks for the code. I ran it with TF master instead of r1.12 and it seemed to work well, below are the logs:\r\n> \r\n> ```\r\n> ++ ./bin/main_multiple_images_parallel_gpu.out\r\n> .. IN load graph 0\r\n> 2019-05-13 13:23:32.728379: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n> 2019-05-13 13:23:32.765179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2594195000 Hz\r\n> 2019-05-13 13:23:32.769016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561cc3c7f1b0 executing computations on platform Host. Devices:\r\n> 2019-05-13 13:23:32.769043: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n> 2019-05-13 13:23:32.772489: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n> 2019-05-13 13:23:33.310919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561cc3d39010 executing computations on platform CUDA. Devices:\r\n> 2019-05-13 13:23:33.310986: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN V, Compute Capability 7.0\r\n> 2019-05-13 13:23:33.311001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Quadro M2000, Compute Capability 5.2\r\n> 2019-05-13 13:23:33.312823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\n> name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\n> pciBusID: 0000:04:00.0\r\n> 2019-05-13 13:23:33.313799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 1 with properties: \r\n> name: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\n> pciBusID: 0000:03:00.0\r\n> 2019-05-13 13:23:33.317937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1705] Ignoring visible gpu device (device: 1, name: Quadro M2000, pci bus id: 0000:03:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 7.0.\r\n> 2019-05-13 13:23:33.317970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n> 2019-05-13 13:23:33.318438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.9.0\r\n> 2019-05-13 13:23:33.320926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-05-13 13:23:33.320958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 1 \r\n> 2019-05-13 13:23:33.320976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N N \r\n> 2019-05-13 13:23:33.320990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 1:   N N \r\n> 2019-05-13 13:23:33.323733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n> .. IN load graph 1\r\n> 2019-05-13 13:23:33.572213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\n> name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\n> pciBusID: 0000:04:00.0\r\n> 2019-05-13 13:23:33.572805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 1 with properties: \r\n> name: Quadro M2000 major: 5 minor: 2 memoryClockRate(GHz): 1.1625\r\n> pciBusID: 0000:03:00.0\r\n> 2019-05-13 13:23:33.575175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1705] Ignoring visible gpu device (device: 1, name: Quadro M2000, pci bus id: 0000:03:00.0, compute capability: 5.2) with Cuda compute capability 5.2. The minimum required Cuda capability is 7.0.\r\n> 2019-05-13 13:23:33.575195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n> 2019-05-13 13:23:33.575247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-05-13 13:23:33.575259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 1 \r\n> 2019-05-13 13:23:33.575268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N N \r\n> 2019-05-13 13:23:33.575276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 1:   N N \r\n> 2019-05-13 13:23:33.576871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n> 2019-05-13 13:23:33.736453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Found device 0 with properties: \r\n> name: TITAN V major: 7 minor: 0 memoryClockRate(GHz): 1.455\r\n> pciBusID: 0000:04:00.0\r\n> 2019-05-13 13:23:33.737695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1751] Adding visible gpu devices: 0\r\n> 2019-05-13 13:23:33.737739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1179] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2019-05-13 13:23:33.737753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1185]      0 \r\n> 2019-05-13 13:23:33.737762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1198] 0:   N \r\n> 2019-05-13 13:23:33.739178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1324] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10508 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:04:00.0, compute capability: 7.0)\r\n>  Initialization done --------------------------\r\n> threadid 0\r\n> ./data/grace_hopper.jpg --------> 0 \r\n> threadid 1\r\n> ./data/grace_hopper.jpg --------> 1 \r\n> 2019-05-13 13:23:33.748316: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1328] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n> 2019-05-13 13:23:37.116325: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n> MOdel ran succesfully : output size MOdel ran succesfully : output size 11\r\n> \r\n> [5.178 s] Finished runnning 2 images successfully\r\n> 2019-05-13 13:23:39.110162: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x7f627888eb80: UNKNOWN ERROR (4)\r\n> 2019-05-13 13:23:39.118689: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x7f627888eb80: UNKNOWN ERROR (4)\r\n> ```\r\n> \r\n> There are errors at the end about failures of destroying cuda events, but I think they're not related to this issue. So would you help to check that the problem is gone with TF master branch?\r\n> \r\n> I also did a minor change to your code:\r\n> \r\n> ```\r\n> $ git diff src/main_multiple_images_parallel.cc\r\n> diff --git a/src/main_multiple_images_parallel.cc b/src/main_multiple_images_parallel.cc\r\n> index ebc498c4..18e06185 100644\r\n> --- a/src/main_multiple_images_parallel.cc\r\n> +++ b/src/main_multiple_images_parallel.cc\r\n> @@ -171,7 +171,7 @@ Status LoadGraph(const string& graph_file_name,\r\n>    }\r\n>    tensorflow::SessionOptions session_options;\r\n>    if(i_threadid == 0) {\r\n> -    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n> +    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n>      tensorflow::graph::SetDefaultDevice(\"/device:GPU:0\", &graph_def);\r\n>    } else if(i_threadid == 1) {\r\n>      session_options.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n> ```\r\n> \r\n> as otherwise there would be gpu device initialization problems.\r\n\r\nhi @aaroey , i came up with the same error when using multiple sessions on different GPUs.\r\nthe error occurs when destroying CUDA context at the end of my program,  which is the same as yours . have you figure it out?\r\n\r\n> 2019-07-10 12:02:06.644458: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x37bb990: UNKNOWN ERROR (4)\r\n> 2019-07-10 12:02:06.644493: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x37bb990: UNKNOWN ERROR (4)\r\n> 2019-07-10 12:02:06.644507: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x37bb990: UNKNOWN ERROR (4)\r\n> 2019-07-10 12:02:06.644512: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x37bb990: UNKNOWN ERROR (4)\r\n", "Closing because of lack of activity.  If you want to reopen please first try @chsigg 's [suggestion](https://github.com/tensorflow/tensorflow/issues/25220#issuecomment-498225295)."]}, {"number": 25219, "title": "Replace deprecated test_session with cached_session", "body": "While running tests:\r\n```\r\nbazel test -s --verbose_failures --config=opt //tensorflow/python:image_ops_test\r\n```\r\n\r\nthere are many deprected warnings:\r\n```\r\nFrom /usr/lib/python2.7/contextlib.py:84: test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\n```\r\n\r\nThis fix replace deprecated test_session with cached_session.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 25217, "title": "Merge #24666 into 1.13 release", "body": "In #24666 support for OpenSSL 1.1.0 was added, which fixes a major issue that caused TF to be unbuildable in a number of environments. Unfortunately that PR was merged too late to make it into the 1.13 release candidate, and yet without it another version of TF will go by unbuildable for me.\r\n\r\nIs there any chance this patch can be merged into the next RC and make it onto the final 1.13 release? I think it's a small change, overall, that ends up being the difference between being able to use TF, or not at all for me and other running more up-to-date environments.\r\n\r\nI wasn't sure what the most appropriate way to communicate this request was, hopefully posting this issue is ok.\r\n\r\nThank you!", "comments": ["So wait, apparently this was already [cherry-picked](https://github.com/tensorflow/tensorflow/commit/23cd023b71da89a637b9979418d2118e85d3add8) into the 1.13 branch? I thought @aselle had mentioned it not making it into the release?", "Sorry for  any confusion. This was merged into the r1.13 branch in #25125. I believe it was in 1.13.0-rc0, but it will be certainly in 1.13.0-rc1  In the future, you can open a cherry pick request. ", "Thank you @aselle!"]}, {"number": 25216, "title": "tf.contrib.nn.alpha_dropout returns tensor with no shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.6.7\r\n\r\n**Describe the current behavior**\r\nI tried out selu activation and also used alpha_dropout for that. When wrapping tf.contrib.nn.alpha_dropout() around my dense layer, a Tensor with no shape will be returned and therefore I can't feed the next layer with this tensor.\r\n\r\nA solution for this can be to assign the input shape to the returned tensor, like in the tf.nn.dropout() Layer.", "comments": ["In order to expedite the troubleshooting process, Can you please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25214, "title": "lite: incorrectly written comparison in slice.cc", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): commit [01dec19](https://github.com/tensorflow/tensorflow/tree/01dec19e827ab46ff7ff5dbd6142ba5a26efc4da)\r\n- GCC/Compiler version (if compiling from source): gcc 8.2.1 20181127\r\n\r\n**Describe the current behavior**\r\n\r\nAn assertion in [tensorflow/lite/kernels/slice.cc](https://github.com/tensorflow/tensorflow/blob/01dec19e827ab46ff7ff5dbd6142ba5a26efc4da/tensorflow/lite/kernels/slice.cc#L120):\r\n\r\n    TF_LITE_ENSURE(context, NumDimensions(begin) == NumDimensions(size) == 1);\r\n\r\nIt evaluates the first `==` and then compares the result (true or false) with 1. It's probably not what's intended.\r\n\r\n**Describe the expected behavior**\r\n\r\nBased on the description of Slice, I believe, it should be:\r\n\r\n    TF_LITE_ENSURE(context, NumDimensions(begin) == 1 && NumDimensions(size) == 1);", "comments": ["@shibe2 it makes sense. Would you like to open a PR for it?", "@yongtang I can create PR.", "We weren't able to get to this fix in the last year, so closing the issue since we're unlikely to be able to work on it. Please reopen if you think this is a mistake."]}, {"number": 25213, "title": "Build tensorflow on windows with mkl fails", "body": "**System information**\r\n- OS Platform: windows server 2012 R2 standard\r\n- TensorFlow build from source:\r\n- TensorFlow version: r1.13\r\n- Python version: 3.6.7\r\n- Bazel version: 0.21\r\n- Compiler version (if compiling from source): 15.4\r\n\r\n\r\nI'm trying build tf on windows with mkl running this command:\r\n```\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nbuild failed with this error:\r\n```\r\nINFO: From Executing genrule //tensorflow/python:pywrap_tensorflow_filtered_def_file:\r\nsymbols=108196, taken=36183, dupes=0\r\nERROR: C:/tfb/tensorflow/tensorflow/lite/toco/python/BUILD:44:1: Linking of rule '//tensorflow/lite/toco/python:_tensorflow_wrap_toco.so' failed (Exit 1000): link.exe failed: error executing command\r\n  cd C:/users/awaizman101364/_bazel_awaizman101364/5eebsbmp/execroot/org_tensorflow\r\n  SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.16299.0\\um\\x\r\n64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\\r\n10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/AWaizman101364/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/AWaizman101364/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\AWAIZM~1\\AppData\\Local\\Temp\\2\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\AWAIZM~1\\AppData\\Local\\Temp\\2\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/python/_tensorflow_wrap_toc\r\no.so-2.params /OPT:ICF /OPT:REF\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\n\r\nbazel-out/x64_windows-opt/bin/external/com_google_absl/absl/numeric/int128.lib : fatal error LNK1000: Internal error during CImplib::EmitThunk\r\n\r\n  Version 14.00.24210.0\r\n\r\n  ExceptionCode            = C0000005\r\n  ExceptionFlags           = 00000000\r\n  ExceptionAddress         = 00007FF787836896 (00007FF787820000) \"C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin\\amd64\\link.exe\"\r\n  NumberParameters         = 00000002\r\n  ExceptionInformation[ 0] = 0000000000000000\r\n  ExceptionInformation[ 1] = 0000000000000008\r\n\r\nCONTEXT:\r\n  Rax    = 0000000000000000  R8     = 00007FF78791FBE0\r\n  Rbx    = 0000000000000000  R9     = 00007FF78791E9F0\r\n  Rcx    = 0000000000000000  R10    = 0000000000000000\r\n  Rdx    = 00007FF78791FBD8  R11    = 0000000000000000\r\n  Rsp    = 00000084E2C2E1B8  R12    = 00007FF7878ED950\r\n  Rbp    = 00000084E4E5D5A0  E13    = 0000000000000000\r\n  Rsi    = 0000000000008000  R14    = 0000000000000000\r\n  Rdi    = 00000084E57B2100  R15    = 0000000000000000\r\n  Rip    = 00007FF787836896  EFlags = 0000000000010246\r\n  SegCs  = 0000000000000033  SegDs  = 000000000000002B\r\n  SegSs  = 000000000000002B  SegEs  = 000000000000002B\r\n  SegFs  = 0000000000000053  SegGs  = 000000000000002B\r\n  Dr0    = 0000000000000000  Dr3    = 0000000000000000\r\n  Dr1    = 0000000000000000  Dr6    = 0000000000000000\r\n  Dr2    = 0000000000000000  Dr7    = 0000000000000000\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 606.229s, Critical Path: 369.31s\r\nINFO: 4122 processes: 4122 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nIs there a way to build tf with mkl on windows?\r\n\r\nThanks.", "comments": ["@awaizman1 we are investigating the issue. But if you would like to get the installer without having to build one there is a conda package available on Anaconda. \r\n\"conda install tensorflow-mkl\" is the command", "Thanks!\r\n\r\nI am aware of the Conda package but since our dev, CI and deployment environments are not using Conda (we use pip + venv) I can\u2019t use it. \r\n\r\nIs there another way I can get a .whl to install using pip?\r\n\r\nBTW, I succeeded to compile v1.10 using cmake (the latest official version which supported cmake) but I saw a significant performance improvement in v1.12 which is required by our product. \r\n\r\nThanks. ", "Right now, the only build that's available for windows is in Anaconda's main channel. We are working on the feature to distribute pip wheels which will be made available shortly. In the meantime, we are investigating the linker issue to be able to provide an alternative solution to compile. \r\nQQ, Did you have to use the workaround provided to compile for 1.10 in cmake?", "Compiling v1.10 with cmake was smooth - I didn\u2019t face any issue thus no workaround applied. \r\n\r\nThanks. ", "Hi @preethivenkatesh \r\n\r\nAny progress with this issue? can I do something that could help.\r\n\r\nThanks!\r\n\r\n", "The workaround is supposed to work on a clean machine that has just the Visual studio installs mentioned in the comment. Anyways, we expect the new MKL library with the patch to be available shortly that eliminates the linker issue. We will notify once we have the fix released.", "I am facing the same problem.", "Hi @preethivenkatesh\r\n\r\nCould you please share this issue status? Does latest version includes the fix for mkl build issue?\r\n\r\nThanks.", "we have dedicated eng resource prioritizing this req now. I'll notify as soon as we have a procedure to get a working build.", "D:\\CodeSofteware\\tensorflow-r1.13\\bazel-bin\\external\\protobuf_archive\\protoc_lib.lib : fatal error LNK1000: Internal error during CImplib::EmitThunk\r\n Version 14.00.24215.1\r\n    ExceptionCode            = C0000005\r\n    ExceptionFlags           = 00000000\r\n    ExceptionAddress         = 00ACD798 (00AB0000) \"D:\\CodeSofteware\\VisualStudio2015\\VC\\bin\\x86_amd64\\link.exe\"\r\n    NumberParameters         = 00000002\r\n    ExceptionInformation[ 0] = 00000000\r\n    ExceptionInformation[ 1] = 00000004\r\n  CONTEXT:\r\n    Eax    = 000DDEEE  Esp    = 00EFE62C\r\n    Ebx    = 00B82E44  Ebp    = 00EFE630\r\n    Ecx    = 00000000  Esi    = 00000000\r\n    Edx    = 00B83C68  Edi    = 00000000\r\n    Eip    = 00ACD798  EFlags = 00010296\r\n    SegCs  = 00000023  SegDs  = 0000002B\r\n    SegSs  = 0000002B  SegEs  = 0000002B\r\n    SegFs  = 00000053  SegGs  = 0000002B\r\n    Dr0    = 00000000  Dr3    = 00000000\r\n    Dr1    = 00000000  Dr6    = 00000000\r\n    Dr2    = 00000000  Dr7    = 00000000\r\n========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========\r\n\r\nI got this error too, when using C++ API builed by bazel0.21.0\r\nany one solved it?? Tell me and thanks!", "I got it working by putting `libiomp5md.dll` and `mklml.dll` on the `PATH` before starting the build of the source modified with the following patch:\r\n```patch\r\ndiff -ruN tensorflow-1.14.0-rc1/third_party/mkl/mkl.BUILD tensorflow-1.14.0-rc1-windows/third_party/mkl/mkl.BUILD\r\n--- tensorflow-1.14.0-rc1/third_party/mkl/mkl.BUILD\t2019-06-08 11:23:20.000000000 +0900\r\n+++ tensorflow-1.14.0-rc1-windows/third_party/mkl/mkl.BUILD\t2019-06-12 08:30:41.232683854 +0900\r\n@@ -35,11 +35,23 @@\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n \r\n+cc_import(\r\n+    name = \"iomp5\",\r\n+    interface_library = \"lib/libiomp5md.lib\",\r\n+    system_provided = 1,\r\n+)\r\n+\r\n+cc_import(\r\n+    name = \"mklml\",\r\n+    interface_library = \"lib/mklml.lib\",\r\n+    system_provided = 1,\r\n+)\r\n+\r\n cc_library(\r\n     name = \"mkl_libs_windows\",\r\n-    srcs = [\r\n-        \"lib/libiomp5md.lib\",\r\n-        \"lib/mklml.lib\",\r\n+    deps = [\r\n+        \"iomp5\",\r\n+        \"mklml\",\r\n     ],\r\n     linkopts = [\"/FORCE:MULTIPLE\"],\r\n     visibility = [\"//visibility:public\"],\r\n```\r\n\r\nWhat this does is \"trick\" Bazel into not using `/WHOLEARCHIVE` on them, which is apparently the cause of the problem. This is what has already been done to make CUDA work on Windows:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda/BUILD.windows.tpl#L57", "`cc_import` is usually a good thing", "The linker issue appears when /WHOLEARCHIVE switch is used along with link.exe on windows when using Visual Studio 2015. Alternatively, you can install the hotfix available at [https://support.microsoft.com/en-us/help/4020481/fix-link-exe-crashes-with-a-fatal-lnk1000-error-when-you-use-wholearch](https://support.microsoft.com/en-us/help/4020481/fix-link-exe-crashes-with-a-fatal-lnk1000-error-when-you-use-wholearch) to overcome this issue. ", "Build is now working with mkldnn. We needed hotfix and cc_import to fix the issue. ", "The mkl issue with TensorFlow on windos is fixed. Please find the build instructions here:\r\nhttps://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide#wind_B_S", "fixed. closing this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25213\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25213\">No</a>\n"]}, {"number": 25212, "title": "Keras API reproducibility", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic (Google Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary from pip\r\n- TensorFlow version (use command below): 2.0.0-dev20190125 (module 'tensorflow' has no attribute 'GIT_VERSION', module 'tensorflow' has no attribute 'VERSION')\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nEverytime I build a keras model, the model is initialized differently. I have Googled and applied all methods I have found, including seeding seeds for environment variable, random package, np.random package, and tf.random. (I used Google Colab.)\r\n\r\n**Describe the expected behavior**\r\nSince I have set all seeds, I should get the same initialization every time I run the same code.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nRun the following code in Google Colab:\r\n```\r\n!pip install tf-nightly-2.0-preview\r\nimport os\r\nos.environ['PYTHONHASHSEED'] = '0'\r\nimport random\r\nrandom.seed(0)\r\nimport numpy as np\r\nnp.random.seed(0)\r\nimport tensorflow as tf\r\ntf.random.set_seed(0)\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.layers as layers\r\ndense = layers.Dense(1, input_shape=(1,),\r\n        kernel_initializer=keras.initializers.GlorotUniform(seed=0),\r\n        bias_initializer=keras.initializers.Constant(value=0.))\r\nprint(hash(dense))\r\n```\r\nSample output:\r\n```\r\n2.0.0-dev20190125\r\n-9223363310562421858\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["Your code does not demonstrate the issue you reported, because the result of `hash(dense)` is unrelated to the value of the weights in the dense layer.", "@ppwwyyxx Thank you for your explanation. It helps!"]}, {"number": 25211, "title": "Using transform_graph to quantize yolo-tiny-v2, performance & mAP drops. ", "body": "I am using tranform_graph to quantize darkflow's tiny-yolo-v2 *.pb\r\n[https://github.com/thtrieu/darkflow](https://github.com/thtrieu/darkflow)\r\n\r\nfound several issues\r\n1. when i use video to do inference (or images) the performance drops to very low with gpu enable\r\nfrom **5.xx fps to 1.xx fps**, and i can feel the images inference speed drops, by input with lots of pictures.\r\n\r\n2. after use tranform_graph's quantization, the mAP drops from **68% to 13%** (with my 500 pics selcted from pascal_voc2012)\r\nthe function i used list below\r\n```\r\n../tensorflow-src/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=_151875_12_28_02_36_51_tiny-yolo-v2.pb  --inputs='input' --outputs='output' --out_graph=_151875_12_28_02_36_51_tiny-yolo-v2.quantize_node.pb --transforms='remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  merge_duplicate_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\n3. after quantize mAP drops, i use \"insert_logging\" & \"freeze_requantization_ranges\"\r\ntried to bring back mAP with that pascal_voc selected 500 pictures\r\nbut the mAP drops to **0%.**\r\n\r\n4. i wrote a script try to fix the logging file\r\nI go thorough logging with single image, one by one \r\nif the mAP goes up, i merge with the last highest mAP ...\r\nuntil the last image\r\nso comes out with a valid logging file, which bring back the mAP to **62%** \r\n\r\n\r\nnow the questions are\r\n1. is that possible to quantize yolo-tiny-v2?\r\nbecause the mAP and performance drops so much (even thou the mAP can be adjust back)\r\n2. are there any other neural network models that you suggest me to use? \r\nmy target is to create some quantized small models for running on ic chips\r\n3. does that means logging and freeze_requantization_ranges might not suitable for tiny-yolo-v2?\r\n\r\nThanks for your time for reading this\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 8fbb96ca119d 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 8fbb96ca119d 4.15.0-34-generic #37~16.04.1-Ubuntu SMP Tue Aug 28 10:44:06 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                 1.15.4\r\nprotobuf              3.6.1\r\ntensorflow-gpu        1.12.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.12.0\r\ntf.GIT_VERSION = v1.12.0-0-ga6d8ffae09\r\ntf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSat Jan 26 05:56:00 2019\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.87                 Driver Version: 390.87                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro GV100        Off  | 00000000:18:00.0 Off |                  Off |\r\n| 40%   54C    P0    50W / 250W |      1MiB / 32508MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro GV100        Off  | 00000000:3B:00.0 Off |                  Off |\r\n| 40%   54C    P0    50W / 250W |      0MiB / 32508MiB |      5%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n\r\n\r\n\r\n", "comments": ["We're deprecating the graph transform tool, and recommend TF Lite for quantization, so I'm closing this one.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25211\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25211\">No</a>\n"]}, {"number": 25210, "title": "tf-gpu's conda Installation works with python 3.6, but not with python 2.7", "body": "\r\nHi, I installed tensorflow-gpu on the same machine using Conda.\r\n\r\nPython 3.6 version works:\r\n\r\n$ python\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \r\n[GCC 7.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2019-01-25 22:38:46.581588: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA\r\n2019-01-25 22:38:46.788700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.91GiB freeMemory: 11.29GiB\r\n2019-01-25 22:38:46.788736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2019-01-25 22:38:54.632438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-01-25 22:38:54.632511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2019-01-25 22:38:54.632550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2019-01-25 22:38:54.633135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10927 MB memory) -> physical GPU (device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1\r\n2019-01-25 22:38:55.230804: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1\r\n\r\nBut Python 2.7 version doesn't work (I tried with versions 1.12.0 + 1.10.0):\r\n\r\n$ python\r\nPython 2.7.15 |Anaconda, Inc.| (default, Dec 14 2018, 19:04:19) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n2019-01-25 22:41:40.507815: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA\r\n2019-01-25 22:41:41.043892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 11.91GiB freeMemory: 415.94MiB\r\n2019-01-25 22:41:41.043925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2019-01-25 22:41:41.044186: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/vraunak/anaconda2/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1494, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/vraunak/anaconda2/envs/python27/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 626, in __init__\r\n    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\nHow could I make tf work with python 2.7? Is any specific version required?\r\n", "comments": ["TF GPU is correctly installed in your system, I suspect you need to update cuda drivers.\r\n@gunan WDYT?", "Thanks for the reply, anyway I could install compatible tf-gpu version,\r\nwithout updating the drivers? Anywhere I could find which tfgpu versions are\r\ncompatible with different cuda versions?\r\n\r\nOn Mon, Jan 28, 2019, 2:00 PM ymodak <notifications@github.com wrote:\r\n\r\n> TF GPU is correctly installed in your system, I suspect you need to update\r\n> cuda drivers.\r\n> @gunan <https://github.com/gunan> WDYT?\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/25210#issuecomment-458256626>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AQa2LKRr0XjCQh1q5Mcy8wRPjxDBSs7yks5vH0jWgaJpZM4aT93I>\r\n> .\r\n>\r\n", "conda builds are managed by anaconda or the community (conda forge packages)\r\nTherefore I am not 100% sure on what they build for. You will need to reach out to them."]}, {"number": 25209, "title": "Error on using feature_columns with tf.keras", "body": "\r\n\r\n\r\n**System information**\r\n- TensorFlow version: 1.12\r\n- Doc Link: https://github.com/tensorflow/docs/blob/b4d8d7096099c2b0a7df6a0564bf6eca8c96c4a0/site/en/tutorials/structured_data/feature_cols_keras.ipynb\r\n\r\n**Describe the documentation issue**\r\nwhen comment `tf.enable_eager_execution()`, the demo code fails. \r\n\r\n`ValueError: logits and labels must have the same shape ((32, 1) vs (32,))`\r\n\r\n\r\n\r\n", "comments": ["This error doesn't appear in 2.0 under https://www.tensorflow.org/tutorials/structured_data/feature_columns, and we've upgraded our testing since then.  I'm closing this as fixed; thank you for bug report!"]}, {"number": 25208, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "ImportError                               Traceback (most recent call last)\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-cefece728b07> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 import numpy as np\r\n      3 \r\n      4 matrix1 = np.array([(2,2,2),(2,2,2),(2,2,2)],dtype = 'int32')\r\n      5 matrix2 = np.array([(1,1,1),(1,1,1),(1,1,1)],dtype = 'int32')\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 try:\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 from tensorflow.python.tools import component_api_helper\r\n\r\n~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\david\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\david\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\david\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\david\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\david\\Anaconda3\\envs\\tensorflow\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["Please provide following information. Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 25207, "title": "ImportError: cannot import name abs", "body": "Hi, I am getting the error while importing tensorflow:\r\n\r\n$ python\r\nPython 2.7.15 |Anaconda custom (64-bit)| (default, Dec 14 2018, 19:04:19) \r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/activations/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 36, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/engine/__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 35, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"/home/vraunak/anaconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/keras/backend/__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name abs\r\n\r\nRemoving protobuf and reinstalling still gives the same error.\r\n\r\n**System information**\r\n- Linux CentOS\r\n- Python version: 2.7 tensorflow 1.12.0 \r\n- Installed using conda", "comments": ["Can you try,\r\n>pip install --upgrade keras", "@ymodak thanks, it got resolved but now I am getting https://github.com/tensorflow/tensorflow/issues/25210. please take a look. thanks."]}]