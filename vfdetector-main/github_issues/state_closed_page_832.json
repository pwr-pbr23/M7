[{"number": 28567, "title": "[ROCm] Forward type names from gpu prefix to cuda prefix", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan here, this PR is the second one in the whole series. It is dependent on the infrastructure provided from PR #28564\r\n\r\n@chsigg @whchung", "comments": []}, {"number": 28566, "title": "[ROCm] remove GPU_LAUNCH_KERNEL macro", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan here, this PR is the sixth one in the whole series.\r\n\r\n@chsigg @whchung", "comments": []}, {"number": 28565, "title": "[ROCm] Creating GpuLaunchKernel", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan here, this PR is the fifth one in the whole series.\r\n\r\n@chsigg @whchung", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28565) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28565) for more info**.\n\n<!-- ok -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28565) for more info**.\n\n<!-- need_author_consent -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28565) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28565) for more info**.\n\n<!-- ok -->", "It looks like this PR is already merged [here](https://github.com/tensorflow/tensorflow/commit/921ee23536ae07286478b0ce0180ad816d0b3b63), but github is not showing it correctly. I'm not entirely sure what repercussion will have if I close PR, so will just leave it. FYI, @chsigg @whchung, ", "@jerryyin you should be able to close this PR without causing trouble on the mainline."]}, {"number": 28564, "title": "[ROCm] Creating Cuda forwarding alias macros", "body": "This PR is a follow-up to the original PR #28343. The reviewer requested to break down the original large PR to a series of smaller ones. According to the plan [here](https://github.com/tensorflow/tensorflow/pull/28343#issuecomment-491025711), this PR is the first one in the whole series.\r\n\r\n@chsigg @whchung ", "comments": ["> need to use macro to guard the header to prevent multiple inclusion\r\n\r\nThanks for reminding. Code updated to add include guard, changes not yet applied to website ui."]}, {"number": 28563, "title": "TFLite GPU delegate produces very different results", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04)\r\n- Mobile device if the issue happens on mobile device: LG V30 Android 8.0.0\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 0.0.0-gpu-experimental\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\n\r\nWhen DeepLab [mobilenetv2_coco_voc_trainaug](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz) is run on a sample image using GPU delegate, the result is significantly different to that generated using CPU. The TFLite model was converted using:\r\n\r\n```\r\ntflite_convert \\\r\n--output_file= mobilenetv2_coco_voc_trainaug.tflite \\\r\n--graph_def_file=$FROZEN_GRAPH \\\r\n--input_arrays=sub_7 \\\r\n--output_arrays=ResizeBilinear_3 \\\r\n--inference_type=FLOAT \\\r\n--inference_input_type=FLOAT\r\n```\r\nAnd, here's the [TFLite model](https://drive.google.com/file/d/1rlD4uBxKegUuWVPBzllFVTQXlJTNb1g0/view?usp=sharing) used.\r\n\r\nCPU gives the following (using random colours):\r\n\r\n![Screenshot_2019-05-10-13-30-19](https://user-images.githubusercontent.com/14197204/57501503-2438c900-732b-11e9-930d-0f9ba51451fe.png)\r\n\r\n\r\nGPU delegate gives the following (using random colours):\r\n\r\n![Screenshot_2019-05-10-13-29-13](https://user-images.githubusercontent.com/14197204/57501509-2733b980-732b-11e9-824c-bb9c360926ee.png)\r\n\r\nBasically, all pixels here are (wrongly) identified to be of the same class (at index 0) because the scores for class 0 are always the highest.\r\n\r\nSimilarly, if [mobilenetv2_ade20k_train](http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz) is used, the two also give very different results:\r\n\r\nCPU gives the following (using random colours):\r\n\r\n![Screenshot_2019-05-10-04-10-59](https://user-images.githubusercontent.com/14197204/57530071-c4b5da00-7379-11e9-94f1-2d87282eb9eb.png)\r\n\r\n\r\nGPU delegate gives the following (using random colours):\r\n\r\n![Screenshot_2019-05-10-04-09-41](https://user-images.githubusercontent.com/14197204/57530083-cc757e80-7379-11e9-859e-41804569b851.png)\r\n\r\nHere's the [TFLite model](https://drive.google.com/file/d/1RdHs85WPxL1u0TUlPHwCqdY6KOn5XuYe/view?usp=sharing) used.\r\n\r\nIndeed, the models use operations like `SPACE_TO_BATCH`, but as far as I know these operations should only affect speed performance.\r\n\r\n**Describe the expected behavior**\r\n\r\nCPU and GPU delegate should produce the same masks.", "comments": ["Can I ask how the [deeplabv3_257_mv_gpu.tflite](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite) provided in the guide was converted? Was it done using `tflite_convert` or was there some tweaking to make it work on GPU?", "@hoonkai \r\n\r\nIf I remember correctly, the only thing we changed for that model was 513x513 input image to 257x257 because the network wouldn't fit into the GPU memory.  The rest should be the same.  I think one of the engineers just ran toco on the frozen graph def.", "@hoonkai \r\n\r\nI also downloaded your model, and you're correct that `SPACE_TO_BATCH_ND` etc. are not supported, but should run, in theory, fine.  But because of the residual connections, e.g. `decoder/ResizeBilinear_1`, I'm not 100% sure whether it will work.  We often only run 100% mobile GPU-compatible graphs, and haven't done extensive testing for partially runnable graphs.", "@impjdi Right. So, do you think `toco` was run on the frozen graph def provided with [mobilenetv2_coco_voc_trainaug ](http://download.tensorflow.org/models/deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz)? If so, wouldn't that produce the same model as running `tflite_convert`? For some reason, the one I generated contains `SPACE_TO_BATCH_ND`, etc. but the one provided in the guide doesn't.", "@impjdi  You mentioned the residual connections may cause the failure. Do you know why GPU delegates don't handle residual connections in particular?", "This there any progress on this issue? I face similar problem and I wish to know how to convert mobilenetv2_coco_voc_trainaug model to the tflite version without BATCH_TO_SPACE_ND (like the  deeplabv3_257_mv_gpu.tflite )", "@hoonkai \r\n\r\nSorry, I didn't get time to investigate this.  I'm a bit swamped with CVPR preparation for the last 2-3 weeks.  I might have more cycles after CVPR.\r\n\r\nI am not certain how our team obtained the deeplab v3 model.  I requested at one point, and I was given the TFLite model directly.  Then it didn't run because 513x513 was too huge for GPU memory; custom modification was performed to scale it down to 257x257, but I was not directly involved.  Do you want me to chase down the person and ask how the TFLite model file was created?", "@impjdi \r\nI wish you can chase down. This issue is related to my issue #29618 and #29509 ", "@hoonkai Thanks for picking this up again. Can I ask why you think residual connections may not work properly on GPU?", "@hoonkai \r\nHi, I figured out a brute method to get rid of BATCH_TO_SPACE:\r\n1. open your <path_to_your_tensorflow>/tensorflow/python/ops/nn_impl.py\r\n2. At line 514: add one more parameter \"dilations=[1, rate[0], rate[1], 1],\"\r\n3. At line 520: modify \"dilation_rate=rate,\" to \"dilation_rate=[1, 1],\"\r\n4. rerun your deeplab script (for example: local_test_mobilenetv2.sh), change checkpoint and crop size according to your requirement. For me, size = 257, check_point = http://download.tensorflow.org/models/deeplabv3_mnv2_dm05_pascal_trainaug_2018_10_01.tar.gz.\r\n5. convert .pb to .tflite, input layer = sub_2, output layer = ResizeBilinear_2\r\n6. You will get a model with structure identical to deeplabv3_257_mv_gpu.tflite!!\r\n", "Any progress deleting all those awful BATCH_TO_SPACE_ND layers that makes converted tflite files run like a turtle...?", "> Any progress deleting all those awful BATCH_TO_SPACE_ND layers that makes converted tflite files run like a turtle...?\r\n\r\nMy method works good for me. I removed all BATCH_TO_SPACE_ND and it gives me correct result.", "Sure, but my 514 and 520 lines are comments... xD", "> Sure, but my 514 and 520 lines are comments... xD\r\n\r\nMy version is 1.13.0rc0\r\nI hope this screenshot can help find the piece of code.\r\nLine 514 is new and Line 520 is modified\r\n![Screenshot from 2019-07-18 10-03-07](https://user-images.githubusercontent.com/43549654/61477421-09df2500-a944-11e9-9b42-d04a8d7480de.png)\r\n\r\n", " Hello, I managed to change those lines in nn_impl.py and now my converted model has no batchtospace stuff. Thank you very much.\r\n \r\n Now the only difference between my converted model and the oficial one is the input quantization. Mine is 0 <= q <= 255 while oficial's is -1 \u2264 q \u2264 0.9921875. I have no luck finding something to modify this, Does anyone know how to \"magically\" change this?", "I have been trying my .tflite because it infered bad results, but then I realized that the previous generated .pb does that, too. So It seems that the first conversion to .pb isn't working...", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28563\">No</a>\n"]}, {"number": 28562, "title": "Documentation update to DeviceSpec", "body": "This PR updates one of the DeviceSpec methods - `__eq__`.\r\n\r\nBelow is the current explanation available in doc for the method.\r\n\r\n```\r\n__eq__(other)\r\n\r\nReturn self==value.\r\n```\r\n\r\nAdded some more explanation to it.", "comments": ["@dynamicwebpaige Thanks for the review. ", "@qlzh727 Updated changes. Please let me know if this looks good."]}, {"number": 28561, "title": "Documentation update to DeviceSpec", "body": "This PR updates one of the DeviceSpec methods - `__eq`.\r\n\r\nBelow is the current explanation available in doc for the method. \r\n\r\n```\r\n__eq__(other)\r\n\r\nReturn self==value.\r\n```\r\n\r\nAdded some more explanation to it.", "comments": ["New PR created. This included changes that were not meant to be a part of this PR"]}, {"number": 28560, "title": "code update to Dimension methods", "body": "Code updates for:\r\n\r\n1. __radd__\r\n2. __rsub__\r\n3. __rmul__", "comments": ["Since Dimension is getting deprecated. I will focus on TensorShape class. Creating new PR with changed results."]}, {"number": 28559, "title": "Fix FP16 build file.", "body": "PiperOrigin-RevId: 247226940", "comments": []}, {"number": 28558, "title": "Tensorflow v2 Variable name uniquification for Keras Layers in eager is inconsistent", "body": "Tensorflow v2.0a\r\n\r\nWhen creating e.g. keras models I would assume, that when I run `make_generator_model` twice in eager mode that the `trainable_variable` names are identical.\r\n\r\n**Why would I assume this?**\r\nBecause the `tf.train.Checkpoint` and `Checkpointable` api makes you believe that variables are coupled with their corresponding object/class and uniquification of variables would be no longer necessary. And indeed, this is the case when creating a variable with the same name twice (as can be seen at the end of the code)\r\n\r\n**What do I get instead?**\r\nIn the below example the variables of the second `make_generator_model()` call will be `uniquified`.\r\n```\r\n# First call\r\n['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']\r\n\r\n# Second\r\n['dense_1/kernel:0', 'batch_normalization_v2_3/gamma:0', 'batch_normalization_v2_3/beta:0', 'conv2d_transpose_3/kernel:0', 'batch_normalization_v2_4/gamma:0', 'batch_normalization_v2_4/beta:0', 'conv2d_transpose_4/kernel:0', 'batch_normalization_v2_5/gamma:0', 'batch_normalization_v2_5/beta:0', 'conv2d_transpose_5/kernel:0']\r\n\r\n# Third\r\n['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']\r\n\r\n# Fourth\r\n['dense/kernel:0', 'batch_normalization_v2/gamma:0', 'batch_normalization_v2/beta:0', 'conv2d_transpose/kernel:0', 'batch_normalization_v2_1/gamma:0', 'batch_normalization_v2_1/beta:0', 'conv2d_transpose_1/kernel:0', 'batch_normalization_v2_2/gamma:0', 'batch_normalization_v2_2/beta:0', 'conv2d_transpose_2/kernel:0']\r\n\r\n# Manual Creation\r\n<tf.Variable 'test:0' shape=() dtype=int32, numpy=1>\r\n<tf.Variable 'test:0' shape=() dtype=int32, numpy=1>\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\ndef make_generator_model():\r\n    model = tf.keras.Sequential()\r\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Reshape((7, 7, 256)))\r\n    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\r\n\r\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 7, 7, 128)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\r\n    assert model.output_shape == (None, 14, 14, 64)\r\n    model.add(layers.BatchNormalization())\r\n    model.add(layers.LeakyReLU())\r\n\r\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\r\n    assert model.output_shape == (None, 28, 28, 1)\r\n\r\n    return model\r\n\r\n\r\nm1 = make_generator_model()\r\nnoise = tf.random.normal([1, 100])\r\ngenerated_image = m1(noise, training=False)\r\nprint([v.name for v in m1.trainable_variables])\r\n\r\nm2 = make_generator_model()\r\nnoise = tf.random.normal([1, 100])\r\ngenerated_image = m2(noise, training=False)\r\nprint([v.name for v in m2.trainable_variables])\r\n\r\nwith tf.Graph().as_default():\r\n    m1 = make_generator_model()\r\n    noise = tf.random.normal([1, 100])\r\n    generated_image = m1(noise, training=False)\r\n    print([v.name for v in m1.trainable_variables])\r\n\r\nwith tf.Graph().as_default():\r\n    m2 = make_generator_model()\r\n    noise = tf.random.normal([1, 100])\r\n    generated_image = m2(noise, training=False)\r\n    print([v.name for v in m2.trainable_variables])\r\n\r\na = tf.Variable(1, name='test')\r\nb = tf.Variable(1, name='test')\r\nprint(a)\r\nprint(b)\r\n```", "comments": ["@sleighsoft : I was able to get the output mentioned above when I tried running the code snippet with TensorFlow 2.0.0-alpha on Colab.", "Ideally I would like to get the same functionality as is currently available when in graph mode.\r\nSo when calling `make_generator_model()` I have the choice to generate a new one \"in a new graph so to speak\" (the `with tf.Graph().as_default()` example) or to just use the same weights as in the existing scope (the `a`, `b` example).\r\n\r\nI found this to be useful when having a training \"graph\" and an evaluation \"graph\". Or is this just me thinking the \"old (tf1)\" way?", "**Update:**\r\nThe \"problem\" is here: https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/engine/base_layer.py#L1372\r\n\r\nThere seems to be a global name space for layers. Which is a little confusing considering tf-2 wants to drop most/all of the global states.\r\n\r\nWouldn't it be better if e.g. `tf.keras.Sequential` would handle name collisions instead of the layers themselves? And if the user wants, alternative names can be provided to each layer anyway.\r\n", "This is intended behavior where most users won't pass in `name`, while Sequential can do that, functional or subclass cannot.", "I assumed it to be intended. That's why I put `incosistent` in the name.\r\nAlso, why is it possible to scope variables with `tf.name_scope` but not `tf.keras.layers`.\r\nThis makes working with subclassed tf.keras.Models a nightmare as behavior is not as one would expect.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndense1 = tf.keras.layers.Dense(10)\r\nvar1 = tf.Variable([10])\r\n\r\nwith tf.name_scope('scoped'):\r\n    dense2 = tf.keras.layers.Dense(15)\r\n    var2 = tf.Variable([15])\r\n\r\ndense1(tf.ones([10, 15]))\r\ndense2(tf.ones([10, 20]))\r\n\r\nprint(dense1.variables[0].name)\r\nprint(var1.name)\r\nprint(dense2.variables[0].name)\r\nprint(var2.name)\r\n```\r\nThis prints\r\n```\r\ndense/kernel:0\r\nVariable:0\r\ndense_1/kernel:0\r\nscoped/Variable:0\r\n```\r\nI would expect the `dense2` to be `scoped/dense/kernel:0`", "Sorry to hear that. The issue you're seeing here is because your layer is not called, thus the variable is deferred created. What can be done here is to do extra layer.build((15,).\r\nI admit that this can be surprising behavior, and I can make it eagerly created if that's what you prefer.", "I find it confusing that for the dense2 case the tf.name_scope has no effect. If creating it eagerly solves that problem that's nice.\r\n\r\nBut: If you would turn it into being eagerly created, will it be consistent when the code is run in graph mode instead?\r\n\r\nHaving a behavior that stays the same no matter if the code is executed in eager or graph is the best option.\r\n", "I'm not sure why this is not graph/eager mode agnostic?", "**Updated example**\r\nI hope this shows the confusion a little better.\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndense = tf.keras.layers.Dense(10)\r\nvar1 = tf.Variable([10])\r\n\r\nwith tf.name_scope('scoped'):\r\n    dense1 = tf.keras.layers.Dense(15)\r\n    var2 = tf.Variable([15])\r\n    dense2 = tf.keras.layers.Dense(15)\r\n    dense3 = tf.keras.layers.Dense(15)\r\n    dense3.build((15,))\r\n\r\ndense(tf.ones([10, 15]))\r\ndense1(tf.ones([10, 20]))\r\n\r\nwith tf.name_scope('scoped'):\r\n    dense2(tf.ones([10, 20]))\r\n\r\nprint(dense.variables[0].name)\r\nprint(var1.name)\r\nprint(dense1.variables[0].name) # Created in tf.name_scope('scoped') & First called in root name_scope\r\nprint(var2.name)\r\nprint(dense2.variables[0].name) # Created in tf.name_scope('scoped') & First called in tf.name_scope('scoped')\r\nprint(dense3.variables[0].name) # Called .build on it within tf.name_scope('scoped')\r\n```\r\n\r\nPrints\r\n```\r\ndense/kernel:0                # Ok\r\nVariable:0                    # Ok\r\ndense_1/kernel:0              # Why is it called dense_1 ?        Expected: scoped/dense/kernel:0\r\nscoped/Variable:0             # Ok\r\nscoped/dense_2/kernel:0       # Why is it called scoped/dense_2 ? Expected: scoped/dense_1/kernel:0\r\nscoped/kernel:0               # Where did the layer naming go?    Expected: scoped/dense_2/kernel:0\r\n```", "@sleighsoft as mentioned in early reply, Dense(15) does not create variable for you yet. You'd need to either call layer.build(input_shape) or y = layer(x) in order to create the variables.", "@tanzhenyu I understand what is happening. Though, I do not understand why it has to be that way.\r\n\r\nFor example, why is `dense3` called `scoped/kernel:0`? Why did keras not add `dense` in the name, even though I called `.build()` on it.\r\n\r\nI feel like we are talking past each other.", "@sleighsoft You should look through this function\r\nhttps://github.com/tensorflow/tensorflow/blob/e1c98eeb8ff8a25d2ecdcf1961974d6c9c1e3df4/tensorflow/python/keras/engine/base_layer.py#L534\r\n\r\n", "@llan-ml Can you be a little more specific, please?", "The command `layer = Dense(10)` only invokes `__init__` of the class `Dense`. \r\n\r\nHowever, the variables for `Dense` are created in `Dense.build` https://github.com/tensorflow/tensorflow/blob/e1c98eeb8ff8a25d2ecdcf1961974d6c9c1e3df4/tensorflow/python/keras/layers/core.py#L998\r\nwhich is automatically called when we call `layer(inputs)` within the scope `self._name_scope()`\r\nhttps://github.com/tensorflow/tensorflow/blob/e1c98eeb8ff8a25d2ecdcf1961974d6c9c1e3df4/tensorflow/python/keras/engine/base_layer.py#L610-L614\r\n\r\nAs for your example\r\n<pre>\r\nwith tf.name_scope('scoped'):\r\n    dense2(tf.ones([10, 20]))\r\n</pre>\r\nsince all related variables are created during the first call `layer(inputs)`, any subsequent calls do not invoke `dense2.build` to create new variables and the scope `scoped` does not change the name of already created variables.\r\n", "Thank you for elaborating your answer.\r\nI still fail to see how this resolves the confusing naming.\r\n\r\n```python\r\ndense = tf.keras.layers.Dense(15)\r\ndense(tf.ones([10, 20]))\r\n\r\nwith tf.name_scope('first'):\r\n    denseN = tf.keras.layers.Dense(15)\r\n\r\nwith tf.name_scope('scoped'):\r\n    denseN(tf.ones([10, 20]))\r\n    \r\nprint(dense.variables[0].name)\r\nprint(denseN.variables[0].name)\r\n```\r\nPrints\r\n```\r\ndense/kernel:0\r\nscoped/dense_1/kernel:0\r\n```\r\nExpected\r\n```\r\ndense/kernel:0\r\nscoped/dense/kernel:0\r\n```", "@sleighsoft Use `dense = tf.keras.layers.Dense(15, name=\"dense\")` and `denseN = tf.keras.layers.Dense(15, name=\"dense\")`.\r\n\r\nIf you do not explicitly pass the argument `name`, keras will automatically use the class name and count how many times it has been used, regardless of its parent scopes. It appends `_1` to the name of `denseN`.\r\n\r\nAlso, if you only pass `name=\"dense\"` to `dense = tf.keras.layers.Dense(15, name=\"dense\")`, the output is still what you expect, because the background counter does not count.", "@llan-ml That does not really help in making the behavior more understandable but it at least offers a \"solution\".\r\n\r\nThank you for your help :)\r\nMuch appreciated!\r\n\r\nI still believe this should be reworked but it would break current code.\r\nSo if any tensorflower thinks this issue should be closed, do so. Otherwise I'll let it stay open.", "This makes restoring a keras model for evaluation in the same program where the training graph/model exists impossible without overriding the training model.\r\n\r\nLike so:\r\n```python\r\n\r\ntrain_model = MyModel()\r\n\r\ntrain_one_epoch(train_model)\r\ncheckpoint_path = save_checkpoint(train_model)\r\n\r\neval_model = MyModel()\r\nrestore(checkpoint_path, eval_model) # This will fail due to the second call to MyModel() which creates different \"unique\" layer names.\r\neval(eval_model)\r\n\r\ntrain_one_epoch(train_model)\r\n```", "@llan-ml Your solution should be the default behavior because that resembles the current design principle of tensorflow v2.", "The reason is that when you create the layer, no input shape is passed, variables are not created, so the name scope you put over there does not impact variable naming.\r\nAn option is to enforce the input shape and make sure variables are created when layer is created, but users of Sequential model without passing in input shape would not like it.", "I see a global name space as a remnant of TF1, which is confusing to have in an object based, encapsulated, TF2.\r\n\r\nI understand your reluctance to change it as it might break a couple of things but it is definitely not understandable/beautiful code as is.\r\n\r\nThis has nothing to do with when the layer gets created, even though we always seem to circle back to that argument.", "@sleighsoft Re \"This has nothing to do with when the layer gets created\" -- not true. See this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L1615-L1621\r\n\r\nThis is called when layer is created. If you don't pass in \"name\", something will be created for you and be used for the lifetime of this layer.", "So the question becomes the following.\r\nShould calling, for example `tf.keras.layers.Dense()`, twice in the same scope give you a layer that references the same underlying variables or not.\r\n\r\nI can understand both options. But then it should be more clear in the documentation what the implication of **not specifying** a name in the constructor does.\r\n\r\nBecause this does not tell the whole story.\r\n![grafik](https://user-images.githubusercontent.com/9438971/59751428-c7f98b00-9280-11e9-96ae-dfde065bd2c8.png)\r\n", "To answer your question, it should not. Here's the general conclusion:\r\n1. name_scope isn't object oriented. This might go away in the long term.\r\n2. layer names are made unique today by global state, which is not idea as well. Many of these global and uniqueness has mainly to do with two things: a) that variable name cannot duplicate, b) that Tensorboard requires naming to group things. \r\n\r\nSo I would suggest working with tf.keras.layers.Dense(name='your own layer name'), not name scope.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28558\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28558\">No</a>\n", "@tanzhenyu Will do so. Thank you for explaining everything :)"]}, {"number": 28557, "title": "tensorflow 1.13.1 and RTX 2080 Ti", "body": "tensorflow 1.13.1 can only  identify the max compute capability is 72, but RTX 2080 ti is 75,how can they are used together ?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there and can provide better support for such issues. Thanks!\r\n"]}, {"number": 28556, "title": "AttributeError: module 'tensorflow_hub.tf_v1' has no attribute 'estimator'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.3 (18D109)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.13.1\r\n- Python version:  3.7.2\r\n- Installed using virtualenv? pip? conda?: virtualenv \r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: Intel Iris Plus Graphics 640 1536 MB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI just installed Tensorflow-hub and Tensorflow as described in the Tensorflow installation instructions. When I run the following code:\r\n\r\n`import tensorflow as tf\r\nimport pandas as pd\r\nimport tensorflow_hub as hub\r\nimport numpy as np\r\nimport os, sys\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n# get cosine similairty matrix\r\ndef cos_sim(input_vectors):\r\n    similarity = cosine_similarity(input_vectors)\r\n    return similarity\r\n\r\n# get topN similar sentences\r\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\r\n    # find the index of sentence in list\r\n    index = sentence_list.index(sentence)\r\n    # get the corresponding row in similarity matrix\r\n    similarity_row = np.array(similarity_matrix[index, :])\r\n    # get the indices of top similar\r\n    indices = similarity_row.argsort()[-topN:][::-1]\r\n    return [sentence_list[i] for i in indices]\r\n\r\n\r\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\r\n\r\n# Import the Universal Sentence Encoder's TF Hub module\r\nembed = hub.Module(module_url)\r\n\r\n# Reduce logging output.\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nsentences_list = [\r\n    # phone related\r\n    'My phone is slow',\r\n    'My phone is not good',\r\n    'I need to change my phone. It does not work well',\r\n    'How is your phone?',\r\n\r\n    # age related\r\n    'What is your age?',\r\n    'How old are you?',\r\n    'I am 10 years old',\r\n\r\n    # weather related\r\n    'It is raining today',\r\n    'Would it be sunny tomorrow?',\r\n    'The summers are here.'\r\n]\r\n\r\nwith tf.Session() as session:\r\n\r\n  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\r\n  sentences_embeddings = session.run(embed(sentences_list))\r\n\r\nsimilarity_matrix = cos_sim(np.array(sentences_embeddings))\r\n\r\nsentence = \"It is raining today\"\r\ntop_similar = get_top_similar(sentence, sentences_list, similarity_matrix, 3)\r\n\r\n# printing the list using loop\r\nfor x in range(len(top_similar)):\r\n    print(top_similar[x])\r\n\r\n\r\nThen I get the following issue after running it:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/svea/PycharmProjects/untitled/main.py\", line 3, in <module>\r\n    import tensorflow_hub as hub\r\n  File \"/Users/svea/PycharmProjects/untitled/venv/lib/python3.7/site-packages/tensorflow_hub/__init__.py\", line 30, in <module>\r\n    from tensorflow_hub.estimator import LatestModuleExporter\r\n  File \"/Users/svea/PycharmProjects/untitled/venv/lib/python3.7/site-packages/tensorflow_hub/estimator.py\", line 63, in <module>\r\n    class LatestModuleExporter(tf_v1.estimator.Exporter):\r\nAttributeError: module 'tensorflow_hub.tf_v1' has no attribute 'estimator'\r\n\r\nThis is my first work with Tensorflow and I could not find any solutions. Thank you for any help!\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@sveah Please reference this issue [link](https://github.com/tensorflow/hub/issues/289). As this is related to tensorflow hub request you to please post this in TF Hub repo [link](https://github.com/tensorflow/hub/issues/new). Hence this issue will be closed."]}, {"number": 28555, "title": "Building TensorFlow Lite Micro for TARGET=bluepill fails", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: b02f70947d\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 0.25.1\r\n- GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 7.3.1\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n**Describe the problem**\r\nWhen building Tensorflow Lite Micro for the bluepill target the build fails on the file tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc with the following message:\r\n\r\nIn file included from ./tensorflow/lite/kernels/internal/common.h:49:0,\r\n        from tensorflow/lite/experimental/micro/kernels/depthwise_conv.cc:18:\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In constructor 'gemmlowp::Mutex::Mutex()':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:70:13: error: 'pthread_mutex_init' was not declared in this scope\r\n    Mutex() { pthread_mutex_init(&m, NULL); }\r\n                   ^~~~~~~~~~~~~~~~~~\r\n             pthread_mutex_t\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In destructor 'gemmlowp::Mutex::~Mutex()':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: error: 'pthread_mutex_destroy' was not declared in this scope\r\n   ~Mutex() { pthread_mutex_destroy(&m); }\r\n              ^~~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:71:14: note: suggested alternative: 'pthread_mutexattr_t'\r\n   ~Mutex() { pthread_mutex_destroy(&m); }\r\n              ^~~~~~~~~~~~~~~~~~~~~\r\n              pthread_mutexattr_t\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Lock()':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: error: 'pthread_mutex_lock' was not declared in this scope\r\n   void Lock() { pthread_mutex_lock(&m); }\r\n                 ^~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:73:17: note: suggested alternative: 'pthread_mutex_t'\r\n   void Lock() { pthread_mutex_lock(&m); }\r\n                 ^~~~~~~~~~~~~~~~~~\r\n                 pthread_mutex_t\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h: In member function 'void gemmlowp::Mutex::Unlock()':\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: error: 'pthread_mutex_unlock' was not declared in this scope\r\n   void Unlock() { pthread_mutex_unlock(&m); }\r\n                   ^~~~~~~~~~~~~~~~~~~~\r\ntensorflow/lite/experimental/micro/tools/make/downloads/gemmlowp/profiling/instrumentation.h:74:19: note: suggested alternative: 'pthread_mutex_t'\r\n   void Unlock() { pthread_mutex_unlock(&m); }\r\n                   ^~~~~~~~~~~~~~~~~~~~\r\n                   pthread_mutex_t\r\ntensorflow/lite/experimental/micro/tools/make/Makefile:209: recipe for target 'tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o' failed\r\nmake: *** [tensorflow/lite/experimental/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/experimental/micro/kernels/depthwise_conv.o] Error 1\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=bluepill test\r\n\r\n\r\n**Any other info / logs**\r\nReverting commit a52f5b54e8 fixes the build issue.", "comments": ["Hi Jens,\r\n\r\nThis is because we accidentally leaked profiling code to the TF Lite part and broke TFL micro. It should have been solved by now. Could you try again?\r\n\r\nThanks,", "> Hi Jens,\r\n> \r\n> This is because we accidentally leaked profiling code to the TF Lite part and broke TFL micro. It should have been solved by now. Could you try again?\r\n> \r\n> Thanks,\r\n\r\nHi!\r\n\r\nIt compiles fine now, although the tests themselves fails with the message included in logs.txt.\r\n\r\n[logs.txt](https://github.com/tensorflow/tensorflow/files/3330454/logs.txt)\r\n", "Eh... WaitUntilLine\r\n\r\nSounds like you need to type enter on you console?", "Thanks Jens! Unfortunately there are some timeout issues with the Renode software emulation testing. I'm closing this one since the original issue is solved, but we have a new team who will be helping improve the emulation tests on our side, so hopefully the bluepill testing should become more reliable.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28555\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28555\">No</a>\n"]}, {"number": 28554, "title": "\"The tensorboard module is not an IPython extension.\" in Google Colab", "body": "I am running the `image_summaries.ipynb` notebook from this [tutorial](https://www.tensorflow.org/tensorboard/r2/image_summaries) on Colab. Even on Colab, `%load_ext tensorboard` is not working and the message is - \r\n\r\n`The tensorboard module is not an IPython extension.`\r\n\r\nTensorFlow version:  2.0.0-dev20190509\r\n", "comments": ["I tried running the tutorial on Colab but was not able to reproduce the issue. However while running it on jupyter notebook, I do hit it. ", "Okay. ", "I think that you can try it :  %load_ext tensorboard.notebook", "That worked @valb21. Thanks", "@sayakpaul Now since ```tensorboard.notebook``` is depreciated, this error still pops up.", "Cc: @ymodak ", "@sayakpaul I was able to execute the colab successfully in latest tf nightly build.(TensorFlow version:  2.0.0-dev20190520) Can you please confirm? Thanks!", "> @sayakpaul Now since `tensorboard.notebook` is depreciated, this error still pops up.\r\n\r\n@Shivanshmundra I am on `2.0.0-alpha0` locally and hence I am able to use the `tensorboard.notebook`. ", "@ymodak on Colab, it runs. But locally, I did not check with `tf-nightly` since I use the local setup for work. However, the `tensorboard.notebook` works just fine locally. ", "With ```tf-nightly``` it should work on your local as well since on colab we use ```tf-nightly```. I will close this issue now, feel free to reopen if you still have problems. Thanks!", " If with   tf-nightly and %load_ext tensorboard still throws the exception as it happened to me\r\nRuntime -> Restart and Run all \r\nsolves it", "Here is the code that is called when running `%load_ext tensorboard`. It is handled by IPython.\r\n\r\nIt basically looks in your venv site-packages for `tensorboard`, and if it finds it, then it calls `load_ipython_extension` in that module.\r\n\r\nFor `tensorboard@2.7`, `load_ipython_extension` does not exist. This is the problem.\r\n\r\n```python\r\n    def load_extension(self, module_str):\r\n        \"\"\"Load an IPython extension by its module name.\r\n\r\n        Returns the string \"already loaded\" if the extension is already loaded,\r\n        \"no load function\" if the module doesn't have a load_ipython_extension\r\n        function, or None if it succeeded.\r\n        \"\"\"\r\n        if module_str in self.loaded:\r\n            return \"already loaded\"\r\n\r\n        from IPython.utils.syspathcontext import prepended_to_syspath\r\n\r\n        with self.shell.builtin_trap:\r\n            if module_str not in sys.modules:\r\n                with prepended_to_syspath(self.ipython_extension_dir):\r\n                    mod = import_module(module_str)\r\n                    if mod.__file__.startswith(self.ipython_extension_dir):\r\n                        print((\"Loading extensions from {dir} is deprecated. \"\r\n                               \"We recommend managing extensions like any \"\r\n                               \"other Python packages, in site-packages.\").format(\r\n                              dir=compress_user(self.ipython_extension_dir)))\r\n            mod = sys.modules[module_str]\r\n            if self._call_load_ipython_extension(mod):\r\n                self.loaded.add(module_str)\r\n            else:\r\n                return \"no load function\"\r\n```\r\n\r\nInside `tensorboard.notebook`:\r\n\r\n```python\r\ndef load_ipython_extension(ipython):\r\n    \"\"\"Deprecated: use `%load_ext tensorboard` instead.\r\n\r\n    Raises:\r\n      RuntimeError: Always.\r\n    \"\"\"\r\n    raise RuntimeError(\r\n        \"Use '%load_ext tensorboard' instead of '%load_ext tensorboard.notebook'.\"\r\n    )\r\n```\r\n\r\nSo this will never work.\r\n\r\nI don't see how this can ever work...\r\n\r\nI guess you could workaround it by adding a module to `~/.ipython/extension` that calls the tensorboard register funtion.", "Here is the commit that broke it: https://github.com/tensorflow/tensorboard/commit/d9092143511cb04e4bfc904820305f1be45c67b3#diff-9e231460ca998d6884f936ea1c13c1eef8ef7164479999a472373c6ee210cb50\r\n\r\n@wchargin \r\n\r\nIt looks like the function was relocated to `tensorboard/__init__.py` which is not in the published python package (I can't see it in my `site-packages`).\r\n\r\nI'm guessing something misconfigured in the https://github.com/tensorflow/tensorboard/blob/master/tensorboard/BUILD.\r\n\r\n`pip show -f tensorboard` shows the `__init__.py` file is there though.\r\n\r\n---\r\n\r\n**UPDATE**\r\n\r\nI re-installed and the init file is now there so it is fixed.\r\n\r\nMaybe it is a bug with Poetry which I used to install. Or could have been a partial install that failed. Wierd indeed."]}, {"number": 28553, "title": "TensorFlow test execution fails with Bazel v0.24.1 on s390x", "body": "**System information**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 18.04 s390x**\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **NA**\r\nTensorFlow installed from (source or binary): **Source**\r\nTensorFlow version: **current master** \r\nPython version: **2.7.x**\r\nInstalled using virtualenv? pip? conda?: **Building from source**\r\nBazel version (if compiling from source): **v0.24.1**\r\nGCC/Compiler version (if compiling from source): **7.4.0 (Ubuntu 18.04)**\r\nJAVA : **openjdk version \"11.0.2\" 2019-01-15**\r\n**OpenJDK Runtime Environment (build 11.0.2+9-Ubuntu-3ubuntu118.04.3)**\r\nCUDA/cuDNN version: **NA**\r\nGPU model and memory: **NA**\r\n\r\n**Describe the problem**\r\nTensorflow test command fails with Bazel v0.24.1 \r\n\r\n**Details:** \r\n\r\nI have built Bazel v0.24.1 from source using steps mentioned below: \r\n\r\n```\r\nmkdir bazel && cd bazel\r\nwget https://github.com/bazelbuild/bazel/releases/download/0.24.1/bazel-0.24.1-dist.zip\r\nunzip bazel-0.24.1-dist.zip \r\nchmod -R +w .\r\nenv EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\" bash ./compile.sh\r\nexport PATH=$PATH:/bazel/output/\r\n```\r\n\r\nThen built TensorFlow master :\r\n```\r\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-s390x\r\nexport PATH=$JAVA_HOME/bin:$PATH\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_CUDA=0\r\nexport TF_NEED_MKL=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_AWS=0\r\nexport TF_ENABLE_XLA=0\r\nyes \"\" | ./configure\r\n\r\n# Build\r\nbazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" build  --define=tensorflow_mkldnn_contraction_kernel=0 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\nTried to execute tests:\r\n\r\n```\r\n# Run bazel test command. Double test timeouts to avoid flakes.\r\n bazel  --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test \\\r\n    --test_tag_filters=-gpu,-benchmark-test -k \\\r\n  --test_timeout 300,450,1200,3600 --build_tests_only \\\r\n  --test_output=errors -- \\\r\n  //tensorflow/... -//tensorflow/compiler/...  \r\n```\r\n  \r\nAt this step, test execution failed with below error:\r\n`ERROR: /external/bazel_tools/tools/jdk/BUILD:443:14: Configurable attribute \"actual\" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.``\r\n\r\nTried to provide `--host_javabase` to test command however `--host_javabase` option not getting recognized.\r\n`[bazel FATAL src/main/cpp/blaze.cc:1329] Unknown startup option: '--host_javabase=@local_jdk//:jdk'`\r\n\r\nCould you please provide some suggestions/inputs on this? How to execute TensorFlow tests? \r\n\r\n", "comments": ["@gunan Could you please advice", "Hi @Nayana-ibm , For Power, we have started to include `--host_javabase=@bazel_tools//tools/jdk:remote_jdk11` on the command line. Perhaps you can try that.\r\n\r\nBecause it is a test option (and not a bazel general option) it has to be included after the `test` argument (and not before)", "@Nayana-ibm Request you to please try the tested build configurations [link](https://www.tensorflow.org/install/source#linux), Please install relevant bazel version as per the TF version . Let us know how it progresses. Thanks!", "@wdirons I tried with `--host_javabase=@bazel_tools//tools/jdk:remote_jdk11` option after test argument however facing same issue: \r\n\r\n` bazel test --host_javabase=@bazel_tools//tools/jdk:remote_jdk11 --test_tag_filters=-gpu,-benchmark-test -k --test_timeout 300,450,1200,3600 --build_tests_only --test_output=errors -- //tensorflow/... -//tensorflow/compiler/...`\r\n\r\n`ERROR: /home/external/bazel_tools/tools/jdk/BUILD:460:14: Configurable attribute \"actual\" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.``", "@muddham ya..I have checked build configurations link for all latest versions however I need to build TF master on s390x ", "For Power we are building bazel with the jdk embedded in it. Perhaps that needs to be done for s390 also. @tjakob , can you share how that is done for Power, or do you other ideas to solve this problem? ", "@Nayana-ibm Can you try `--host_javabase=@local_jdk//:jdk` again? (but after the test command)\r\nThis should use the local java installation.\r\nI can give more information on how to embed the jdk into a bazel build, but if the local installation is an option, it is much easier.", "@meteorcloudy Are you aware of any changes in bazel that would createproblems in detecting java?", "@cushon Any idea on this?", "cc @iirina", "@gunan @meteorcloudy Finally I could build Bazel, TensorFlow and execute TF tests using Bazel 0.24.1 and OpenJDK 11 on s390x Ubuntu 18.04\r\n\r\n**Bazel build** :  env **EXTRA_BAZEL_ARGS=\"--host_javabase=@local_jdk//:jdk\"** bash ./compile.sh\r\n**TF build** :  bazel --host_jvm_args=\"-Xms512m\" --host_jvm_args=\"-Xmx1024m\" build  --define=tensorflow_mkldnn_contraction_kernel=0 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\n**TF Test case execution:** \r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" test **--host_javabase=\"@local_jdk//:jdk\"** \r\n    --test_tag_filters=-gpu,-benchmark-test -k \r\n  --test_timeout 300,450,1200,3600 --build_tests_only \r\n  --test_output=errors -- \r\n  //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/... \r\n  -//tensorflow/core/platform/cloud/... -//tensorflow/contrib/lite/... \r\n  -//tensorflow/contrib/cloud/... -//tensorflow/java/... \r\n\r\nIs this correct option to use?", "Further some tests are failing with an error :\r\n```\r\nERROR: /external/mkl_dnn/BUILD.bazel:101:1: Couldn't build file external/mkl_dnn/_objs/mkldnn_single_threaded/utils.pic.o: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1)\r\nexternal/mkl_dnn/src/common/utils.cpp:22:10: fatal error: xmmintrin.h: No such file or directory\r\n #include \"xmmintrin.h\"\r\n          ^~~~~~~~~~~~~\r\ncompilation terminated.\r\n```\r\n\r\nThis is specific to Intel and not working on s390x platform.\r\n\r\nTried with export TF_NEED_MKL=0 but same issue.\r\n\r\nThis file is included through this[ commit](https://github.com/tensorflow/tensorflow/commit/2f345d145e261213168745a0d3f1aef19282c458) in `tensorflow/core/framework/variant_test.cc`\r\n@ebrevdo : Do you any idea on this? How to skip this on s390x?\r\n", "Parts of the file need to be guarded with some ifdefs.  I can add them next\nweek.\n\nOn Sat, May 18, 2019, 8:41 AM Nayana Thorat <notifications@github.com>\nwrote:\n\n> Further some tests are failing with an error :\n>\n> ERROR: /external/mkl_dnn/BUILD.bazel:101:1: Couldn't build file external/mkl_dnn/_objs/mkldnn_single_threaded/utils.pic.o: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1)\n> external/mkl_dnn/src/common/utils.cpp:22:10: fatal error: xmmintrin.h: No such file or directory\n>  #include \"xmmintrin.h\"\n>           ^~~~~~~~~~~~~\n> compilation terminated.\n>\n> This is specific to Intel and not working on s390x platform.\n>\n> Tried with export TF_NEED_MKL=0 but same issue.\n>\n> This file is included through this commit\n> <https://github.com/tensorflow/tensorflow/commit/2f345d145e261213168745a0d3f1aef19282c458>\n> in tensorflow/core/framework/variant_test.cc\n> @ebrevdo <https://github.com/ebrevdo> : Do you any idea on this? How to\n> skip this on s390x?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28553?email_source=notifications&email_token=AANWFG6XFYOGI5DH46ZRUWLPWAPTPA5CNFSM4HL24AGKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVWQ4EQ#issuecomment-493686290>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG65TPLLBKBMEXPIWFTPWAPTPANCNFSM4HL24AGA>\n> .\n>\n", "We can probably check the value of EIGEN_VECTORIZE_SSE after importing\neigen headers as a guard.\n\nOn Sat, May 18, 2019, 11:12 PM ebrevdo <notifications@github.com> wrote:\n\n> Parts of the file need to be guarded with some ifdefs. I can add them next\n> week.\n>\n> On Sat, May 18, 2019, 8:41 AM Nayana Thorat <notifications@github.com>\n> wrote:\n>\n> > Further some tests are failing with an error :\n> >\n> > ERROR: /external/mkl_dnn/BUILD.bazel:101:1: Couldn't build file\n> external/mkl_dnn/_objs/mkldnn_single_threaded/utils.pic.o: C++ compilation\n> of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1)\n> > external/mkl_dnn/src/common/utils.cpp:22:10: fatal error: xmmintrin.h:\n> No such file or directory\n> > #include \"xmmintrin.h\"\n> > ^~~~~~~~~~~~~\n> > compilation terminated.\n> >\n> > This is specific to Intel and not working on s390x platform.\n> >\n> > Tried with export TF_NEED_MKL=0 but same issue.\n> >\n> > This file is included through this commit\n> > <\n> https://github.com/tensorflow/tensorflow/commit/2f345d145e261213168745a0d3f1aef19282c458\n> >\n> > in tensorflow/core/framework/variant_test.cc\n> > @ebrevdo <https://github.com/ebrevdo> : Do you any idea on this? How to\n> > skip this on s390x?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/28553?email_source=notifications&email_token=AANWFG6XFYOGI5DH46ZRUWLPWAPTPA5CNFSM4HL24AGKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVWQ4EQ#issuecomment-493686290\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/AANWFG65TPLLBKBMEXPIWFTPWAPTPANCNFSM4HL24AGA\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28553?email_source=notifications&email_token=AANWFGZQEIMUD3VDHC4DIM3PWDVWFA5CNFSM4HL24AGKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVW3STY#issuecomment-493730127>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2UMETVBSRQJXRK7R3PWDVWFANCNFSM4HL24AGA>\n> .\n>\n", "\r\n@ebrevdo Is the file guarded with ifdefs?  Could you please point to commit id?", "ping!\r\nany updates here?\r\nIs this still a problem with newer bazel versions?", "I believe someone sent a PR to guard.\n\nOn Fri, Nov 8, 2019, 7:06 AM Gunhan Gulsoy <notifications@github.com> wrote:\n\n> ping!\n> any updates here?\n> Is this still a problem with newer bazel versions?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/28553?email_source=notifications&email_token=AANWFG2AI6TMHQ76QXBT2F3QSWMAHA5CNFSM4HL24AGKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDSXUXY#issuecomment-551909983>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG5VFSBHTUPM7UQNYCTQSWMAHANCNFSM4HL24AGA>\n> .\n>\n", "OK, then closing the issue.\r\nPlease reopen if this is still not resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28553\">No</a>\n"]}, {"number": 28552, "title": "W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.", "body": "This is running on tensorflow v2 alpha on a GPU (NVIDIA RTX 2080ti)\r\n\r\nThis message keeps showing up in the logs, and I'm not sure how to dig further. Training has been very slow, and I'm guessing that it's because of this:\r\n\r\n    W ./tensorflow/core/framework/model.h:202] Encountered a stop event that was not preceded by a start event.\r\n\r\nThis error occurs when I call the model.fit function:\r\n\r\n    def get_model():\r\n        model_input = Input(shape=input_shape[1:], name=\"input_layer\")\r\n        x = Conv2D(32, (3, 10), padding=\"same\", name=\"layer1a\")(model_input)\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\"relu\")(x)\r\n        x = MaxPool2D()(x)\r\n\r\n        x = Conv2D(32, (3, 10), padding=\"same\")(x)\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\"relu\")(x)\r\n        x = MaxPool2D()(x)\r\n\r\n        x = Conv2D(32, (3, 10), padding=\"same\")(x)\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\"relu\")(x)\r\n        x = MaxPool2D()(x)\r\n\r\n        x = Conv2D(32, (3, 10), padding=\"same\")(x)\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\"relu\")(x)\r\n        x = MaxPool2D()(x)\r\n\r\n        x = Flatten()(x)\r\n        x = Dense(64)(x)\r\n        x = BatchNormalization()(x)\r\n        x = Activation(\"relu\")(x)\r\n        model_output = Dense(number_classes, activation='softmax')(x)\r\n        model = Model(inputs=model_input, outputs=model_output)\r\n        model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\")\r\n        return model\r\n\r\n    model = get_model()\r\n\r\n    model.fit(train_dataset, steps_per_epoch=total_size // args.batch_size, epochs=args.epochs, validation_data=validation_dataset, validation_steps=500)\r\n\r\nwhere both train_dataset and validation_dataset are instances of tf.data:\r\n\r\n    def preprocess(self, x):\r\n    \timage, target = _parse_function_train(x)\r\n    \timage = tf.image.convert_image_dtype(image, tf.float32)\r\n    \timage = tf.image.resize_with_pad(image, target_height=self.image_height, target_width=self.image_width)\r\n    \treturn image, target\r\n\r\n    def get_datasets(self):\r\n    \tself.data[\"train_curated\"] = tf.data.TFRecordDataset(\"train_curated.tfrecords\")\r\n    \tdataset = self.data[\"train_curated\"].shuffle(args.train_curated_count)\r\n    \tvalidation_size = int(args.validation_size_split_ratio * args.train_curated_count)\r\n\r\n    \tvalidation_dataset = dataset.take(validation_size)\r\n    \tvalidation_dataset = validation_dataset.map(self.preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    \tvalidation_dataset = validation_dataset.shuffle(validation_size)\r\n    \tvalidation_dataset = validation_dataset.cache()\r\n    \tvalidation_dataset = validation_dataset.repeat()\r\n    \tvalidation_dataset = validation_dataset.batch(args.batch_size)\r\n    \tvalidation_dataset = validation_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    \ttrain_dataset = dataset.skip(validation_size)\r\n    \ttrain_dataset = train_dataset.map(self.preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    \ttrain_dataset = train_dataset.shuffle(args.train_curated_count - validation_size)\r\n    \ttrain_dataset = train_dataset.repeat()\r\n    \ttrain_dataset = train_dataset.batch(args.batch_size)\r\n    \ttrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n    \treturn train_dataset, validation_dataset\r\n\r\n\r\n", "comments": ["I'm running into the same issue. I think it has to do with the tf.data.Dataset.shuffle method. Haven't found a fix just yet, because as you said, it works, it just throws this warning and runs fairly slow.", "@vgoklani In order to expedite the trouble-shooting process, please provide a minimum code snippet to reproduce the issue reported here. Thanks!", "@gadagashwini thank you for looking into this -- encountered the same when testing pix2pix on tf-gpu 2.0 https://www.tensorflow.org/alpha/tutorials/generative/pix2pix. Used `pip install -q tensorflow-gpu==2.0.0-alpha0` and have no other tf versions active. \r\nCan also verify @Atrus619 comment on shuffle: when `train_dataset = train_dataset.shuffle(BUFFER_SIZE)` is commented, no error appears. \r\n", "I meet the same trouble", "Same here, model takes ages to train", "I am getting the same error on TensorFlow 1.13.1 using the MNIST dataset from tensorflow-datasets (1.0.2).", "Note that even though this warning is unexpected, it should have no effect on performance and will be removed in future versions of TensorFlow to avoid confusion.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28552\">No</a>\n", "I got a same error and it may be related to parallel calls used in map().\r\nSo I shuffle the dataset before map it, then it works correctly.\r\n", "I had the same error when I used TF 1.14 installed using conda. However, when I remove it and install it using pip, the error is gone and the training in about 4 times faster."]}, {"number": 28551, "title": "Tensorflow build issue on Windows", "body": "**System information**\r\n\r\n- OS Platform and Distribution: Windows 10 1809 Build\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest (get with `git clone`)\r\n- Python version: 2.7.16\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.23.0\r\n- CUDA/cuDNN version: not installed\r\n\r\n**Problem**\r\n\r\nI need to build Tensorflow on Windows for further use in my C++ VS project. There are appear errors when trying to build from source with Bazel. I tried to follow the installation instructions set out on the official website: https://www.tensorflow.org/install/source_windows\r\n\r\nBuild command (CPU only):\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**Logs**\r\n\r\n> Starting local Bazel server and connecting to it...\r\n> ERROR: C:/users/lenovo/desktop/tensorflow/third_party/python_runtime/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):\r\n>         File \"C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl\", line 344\r\n>                 _create_local_python_repository(repository_ctx)\r\n>         File \"C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl\", line 296, in _create_local_python_repository\r\n>                 _get_numpy_include(repository_ctx, python_bin)\r\n>         File \"C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl\", line 276, in _get_numpy_include\r\n>                 _execute(repository_ctx, [python_bin, \"-c\",...\"], <2 more arguments>)\r\n>         File \"C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl\", line 56, in _execute\r\n>                 _fail(\"\\n\".join([error_msg.strip() if ... \"\"]))\r\n>         File \"C:/users/lenovo/desktop/tensorflow/third_party/py/python_configure.bzl\", line 27, in _fail\r\n>                 fail((\"%sPython Configuration Error:%...)))\r\n> Python Configuration Error: Problem getting numpy include path.\r\n> Traceback (most recent call last):\r\n>   File \"<string>\", line 1, in <module>\r\n> ImportError: No module named numpy\r\n> Is numpy installed?\r\n>  and referenced by '//third_party/python_runtime:headers'\r\n> ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\n> INFO: Elapsed time: 31.933s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (195 packages loaded, 3695 targets configured)\r\n>     Fetching @grpc; fetching 11s\r\n>     Fetching @llvm; fetching 9s\r\n>     Fetching @boringssl; fetching\r\n>     Fetching @local_config_python;", "comments": ["Please let us know which version of TensorFlow you are using. Also try to check whether numpy is installed or not. If yes which version of numpy is installed and what is the path for the same. Thanks!", "> Please let us know which version of TensorFlow you are using. Also try to check whether numpy is installed or not. If yes which version of numpy is installed and what is the path for the same. Thanks!\r\n\r\nnumpy install path: c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.16.3 version)\r\n\r\nWhere in sources I can view library version?", "Did you get chance to look on [this](https://stackoverflow.com/questions/48547709/tensorflow-1-5-build-failing-missing-path). Let us know if that helps. Thanks!", "Use `--action_env` in Bazel command? I tried it.", "Did you try following the steps mentioned in [TensorFlow website](https://www.tensorflow.org/install/source_windows).If not, please give it a try and let us know if you are still stuck. Also let us know which TensorFlow version you are using.Thanks! ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28550, "title": "generate_examples_lib updated with Round op", "body": "", "comments": ["@achowdhery Can you please review this PR? Thanks!", "Can one of the admins verify this patch?", "Thanks for your contribution, Siju.\r\nThe test was added via https://github.com/tensorflow/tensorflow/pull/24918"]}, {"number": 28549, "title": "Added 8-bit Quantization Support for ELU.", "body": "", "comments": ["@achowdhery, this is kind of important PR for me, would be great to see your feedback on the PR\r\n\r\nRegards \r\nAmit ", "@achowdhery , i have optimized the implementation, kindly have a look.\r\n\r\nRegards\r\nAmit", "Added support for uint8 and int8 as well for ELU\r\n\r\n\r\nRegards\r\nAmit", "Added Support in Toco as well.\r\n\r\nRegards\r\nAmit", "@achowdhery , i have updated the PR with all the scenarios and tested with model as well, kindly check, this is very important PR for me.\r\n\r\nRegards\r\nAmit", "@amitsrivastava78 Could you please resolve the conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28548, "title": "TfLite file close after usage generic fix", "body": "", "comments": ["@achowdhery Can you please review this PR? Thanks!", "Can one of the admins verify this patch?", "@Dayananda-V  Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28547, "title": "ssd_mobilenet_v2 model convert tflite errors", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow 1.12, python 3.6.5\r\nI use tensorflow object_detection train ssd_mobilenet_v2 model on my own dataset, and this code(https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) convert trained ckpt model to pb, and use this .pb file predict on my dataset, the result is very good. but I use tf.contrib.lite.TFLiteConverte convert this .pb file to .tflite file, outputs some erros :\r\n:RuntimeError: TOCO failed see console for info. Converting unsupporteOp node missing output type attribute,   NonMaxSuppressionV3,  and so on...\r\nthis is my convert code:\r\ngraph_def_file = 'graph.pb'\r\ninput_arrays = ['image_tensor']\r\noutput_arrays = ['num_detections', 'detection_boxes', 'detecction_scores', 'detection_class']\r\nconverter = tf.contrib.lite.TFFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shapes={'image_tensor':[1,300,300,3]})\r\n#converter.post_training_quantize=True\r\n#converter.default_ranges_stats=(0,6)\r\ntflite_model = converter.convert()\r\nopen('detect.tflite', 'wb').write(tflite_model)\r\ndoes anyone meet thsi problems, thanks a lot\r\n", "comments": ["@yjfncu Request you to please refer this [link](https://github.com/tensorflow/tensorflow/issues/22106). Please let us know how it progresses. Thanks!", "@yjfncu Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28546, "title": "TfLite Fill quantization support added", "body": "Quantization support added for #28438", "comments": ["@achowdhery Can you please review this PR? Thanks!", "@Dayananda-V Can you please resolve conflicts? Thanks!", "@gbaned \r\n\r\nconflicts resolved.", "@Dayananda-V Could you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28545, "title": "problem with installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC Yosemite 10.10.5 (14F27)\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: 1.13.1\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 4 GB 1600 MHz DDR3\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nAlthough tensorflow is installed and pip install tensorflow shows that it is already satisfied, but I cannot import tensorflow library\r\n\r\nFile \"/Users/botaduisenbay/Desktop/tester.py\", line 9, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): no suitable image found.  Did find:\r\n\t/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: truncated mach-o error: segment __LINKEDIT extends to 365926664 which is past end of file 154042368\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Which CUDA/cuDNN version you are using? Also did you try installing TensorFlow from this [link](https://www.tensorflow.org/install/source). Please let us know. Thanks!", "> Which CUDA/cuDNN version you are using? Also did you try installing TensorFlow from this [link](https://www.tensorflow.org/install/source). Please let us know. Thanks!\r\n\r\nused CUDA 7.5, then updated, but it didn't help. \r\nI have tried that method of installation a long time ago. It does not cause a problem with cuda, but still didn't work.\r\n\r\nThe issue is resolved by updating macos to high sierra, as older versions of tensorflow was working, while 1.13 requires Mac OS X 10.13 "]}, {"number": 28544, "title": "problem with installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC Yosemite 10.10.5 (14F27)\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: 1.13.1\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: 4 GB 1600 MHz DDR3\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nAlthough tensorflow is installed and pip install tensorflow shows that it is already satisfied, but I cannot import tensorflow library\r\n\r\nFile \"/Users/botaduisenbay/Desktop/tester.py\", line 9, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/botaduisenbay/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): no suitable image found.  Did find:\r\n\t/Users/botaduisenbay/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: truncated mach-o error: segment __LINKEDIT extends to 365926664 which is past end of file 154042368\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["duplicate #28545"]}, {"number": 28543, "title": "TF Lite toco/tflite warning fix", "body": "toco/tflite module warning fix", "comments": ["@achowdhery Can you please review this PR? Thanks!", "Can one of the admins verify this patch?"]}, {"number": 28542, "title": "TF Framework regularizers_test missing test case add", "body": "1. test_regularization_shared_layer\r\n2. test_regularization_shared_model\r\n3. test_regularization_shared_layer_in_different_models\r\ntest cases added\r\n", "comments": ["@omalleyt12 Can you please review this PR ? Thanks!"]}, {"number": 28541, "title": "tf.data.Dataset.padded_batch does not actually pad data when working on multiple GPUs.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nX\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- **Python version**:\r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nX\r\n- **GCC/Compiler version (if compiling from source)**:\r\nX\r\n- **CUDA/cuDNN version**:\r\n10.0\r\n- **GPU model and memory**:\r\nTesla K80 / 12GB RAM\r\n- **Exact command to reproduce**:\r\nSee test below\r\n\r\n### Describe the problem\r\nThe problem appears when one wants to use a multi-gpu-compiled model with variable-shaped data, fed from a tf.data pipeline. \r\nBatching data using padded_batch (compulsory here, since the data is of variable shape) does not seem to really pad the tensors comprised in a batch. Calling `fit`or `predict `after doing so raises:\r\n`ValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica`.\r\n\r\nSee below a MVCE:\r\n- A python generator is used to generate tensors of variable shape: (4, 4, 1), then (5, 5, 1), then (4, 4, 1), then (5, 5, 1), etc...\r\n- A tf.data Dataset is built using `tf.data.Dataset.from_generator`, including a consistency check.\r\n- A dummy model is built using the tf.keras functional API and a `tf.distribute.MirroredStrategy` on several GPUS.\r\n\r\n### Source code / logs\r\nMVCE:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef data_generator():\r\n    \"\"\"\r\n    Creates a generator of variable_sized tensors.\r\n        tensor 1: (4, 4, 1)\r\n        tensor 2: (5, 5, 1)\r\n        tensor 3: (4, 4, 1)\r\n        tensor 4: (5, 5, 1)\r\n        ...\r\n    \"\"\"\r\n    i = 0\r\n\r\n    while True:\r\n        size = (4 + i % 2, 4 + i % 2, 1)\r\n        x = tf.random.normal(shape=size)\r\n        yield x\r\n        i += 1\r\n\r\n\r\ndef build_data_pipeline():\r\n    \"\"\"\r\n    Builds a tf.data pipeline yielding variably-sized tensors.\r\n    \"\"\"\r\n    dataset = tf.data.Dataset.from_generator(\r\n        generator=data_generator,\r\n        output_types=tf.float32,\r\n        output_shapes=[None, None, 1])\r\n\r\n    # Padded batch, should output (batch_size, 5, 5, 1) - shaped tensors.\r\n    dataset = dataset.padded_batch(\r\n        batch_size=2,\r\n        padded_shapes=(None, None, 1),\r\n        padding_values=0.)\r\n\r\n    # Juste making sure that the tf.data pipeline is as expected\r\n    it = iter(dataset)\r\n    batch_1 = next(it)  #~= tf.data.experimental.get_single_element(dataset)\r\n    batch_2 = next(it)  #~= tf.data.experimental.get_single_element(dataset)\r\n\r\n    np.testing.assert_allclose(batch_1.shape, batch_2.shape)\r\n    np.testing.assert_allclose(batch_1.shape, (2, 5, 5, 1))\r\n\r\n    return dataset\r\n\r\n\r\ndef run_on_multi_gpus():\r\n    \"\"\"\r\n    Building multi-gpu model using MirroredStrategy,\r\n    And trying to predict\r\n    \"\"\"\r\n    strat = tf.distribute.MirroredStrategy()\r\n\r\n    with strat.scope():\r\n\r\n        i_ = tf.keras.layers.Input((None, None, 1))\r\n        model = tf.keras.models.Model(i_, i_)\r\n\r\n        model.compile(optimizer='adam',\r\n                      loss='binary_crossentropy')\r\n\r\n    dataset = build_data_pipeline()\r\n    model.predict(dataset, steps=1, verbose=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_on_multi_gpus()\r\n```\r\n", "comments": ["@jvishnuvardhan the error is coming from TF distribution strategy, which I am not familiar with. Could you please re-assign this to someone from that team? Thank you.", "Why do you think it's not padded if these statements pass:\r\n```\r\n    np.testing.assert_allclose(batch_1.shape, batch_2.shape)\r\n    np.testing.assert_allclose(batch_1.shape, (2, 5, 5, 1))\r\n```\r\n\r\nLooks padded, doesn't it?  Since all of the batches are ..., 5,5,1.", "Hi,\r\nI agree that the title is misleading: \r\nThe issue here comes when using a `padded_batch` `tf.data.dataset` with a multi_gpu model, built using a `MirroredStrategy`.\r\n\r\nDoing so raises:\r\n`ValueError: Input tensor shapes do not match for distributed tensor inputs PerReplica`. This error prints along each sub-batch (one sub-batch per worker).\r\nConsequently, The MVCE above will print two tensors: \r\n- one of shape (1, 4, 4, 1), \r\n- the other of shape (1, 5, 5, 1)\r\n\r\nMy issue here is that I can not train a Distributed model on varying-sized input.", "Yes, it's an issue and we are currently working on fixing it.  Thanks for reporting.  ", "FYI it should work if there were fully defined shapes instead of None in padded_batch.", "@isaprykin thanks, good to know", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28541\">No</a>\n", "I don't think the issue is resolved yet, Igor simply suggested a workaround for now. The bug is still there and needs to be fixed. ", "I think this issue should now be fixed. Please re-open if not.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28541\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28541\">No</a>\n"]}, {"number": 28540, "title": "Crashing on TF Detect Android sample with Yolov3-tiny model", "body": "I converted Yolov3-tiny model (Trained from scratch) with [DW2TF](https://github.com/jinyu121/DW2TF). Then I put .pb file to assets and changed MODE == DetectorMode.YOLO in **[DetectorActivity.java](https://gist.github.com/androuino/2b67d88d00bc52ac181740fa475828c4)**.\r\n\r\nWhen I start TF Detect app I have a crash:\r\n```\r\n--------- beginning of crash\r\n05-09 17:07:39.959 13973-13999/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.demo, PID: 13973\r\n    java.lang.IllegalArgumentException: No Operation named [input] in the Graph\r\n        at org.tensorflow.Session$Runner.operationByName(Session.java:372)\r\n        at org.tensorflow.Session$Runner.feed(Session.java:142)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.addFeed(TensorFlowInferenceInterface.java:577)\r\n        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.feed(TensorFlowInferenceInterface.java:318)\r\n        at org.tensorflow.demo.TensorFlowYoloDetector.recognizeImage(TensorFlowYoloDetector.java:154)\r\n        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:297)\r\n        at android.os.Handler.handleCallback(Handler.java:815)\r\n        at android.os.Handler.dispatchMessage(Handler.java:104)\r\n        at android.os.Looper.loop(Looper.java:207)\r\n        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n```\r\n\r\nThank you in advance for the help.", "comments": ["hello\uff0cdid you solve this problem\uff1f", "You need to update the input/output tensor names that are reference in the sample. Unfortunately, in-place swapping out models in inference code doesn't always \"just work\" due to how different models expose the input/output names."]}, {"number": 28539, "title": "Tutorials next_step page is giving 404", "body": "\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/next_steps\r\n\r\n## Description of the issue (what needs changing):\r\nThis page is not opening\r\n\r\n### Clear description\r\n\r\nLink is getting 404\r\n\r\n\r\n\r\n### Request visuals, if applicable\r\n\r\n![404 \u00a0 \u00a0 Page Not Found \u00a0 \u00a0 TensorFlow](https://user-images.githubusercontent.com/1620769/57435595-65fa3e80-725b-11e9-81da-8394e7147ca7.png)\r\n\r\n\r\n", "comments": ["@beingsagir Where did you find the link to `next_steps`? Could you share that link? Thanks!", "@jvishnuvardhan \r\n[Please visit this link](https://www.tensorflow.org/tutorials/keras)\r\n\r\n**search for:** next steps.", "@beingsagir Thanks for finding this missing link. In the coming weeks, TF website will be updated. There are lots of changes we are making that's why there are some missing links.\r\n \r\nThis missing link was updated in master [here](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/keras/index.md). When the TF website is updated, you don't see the last line \r\n`Additional TensorFlow and machine learning resources are listed in next steps.`\r\n\r\nThanks", "Closing this issue. Thanks!"]}, {"number": 28538, "title": "IDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution Linux Ubuntu 16.04\r\n\r\n- TensorFlow installed from pip:\r\n- TensorFlow version 1.13:\r\n- Python version 3.5:\r\n- Installed using created virtualenvironment and then used ubuntu command to install :\r\n\r\n- GPU model and memory   NVIDIA Corporation GT218 [GeForce 210] (rev a2)\r\n \r\n\r\n\r\n\r\n# Install NVIDIA driver\r\napt-get install --no-install-recommends nvidia-410 .\r\n\r\nto check this nvidia installation i used  \r\nnvidia-smi \r\ncommand it ended up with above error message\r\n\r\nhttps://www.tensorflow.org/install/gpu followedsame link\r\n\r\nIs this issue is because the GPU version is not supported??\r\nIf so please to share the GPU compatible version and memory size", "comments": ["@hmchaitra This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.If you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}]