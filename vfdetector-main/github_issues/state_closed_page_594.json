[{"number": 35854, "title": "TFLu: Add stm32f4 and build target", "body": "Add new TARGET=stm32f4 that is working with Renode.\r\nAdd new <build> target that will just build the test binaries.\r\nAdd new CI script for this as well.\r\nThe purpose of this is CMSIS-NN regression.", "comments": ["Gentle ping for review.", "Ready for merge", "Ready for merge", "Do you know why these tests fail? They didn't fail on the first and second commit, so I wonder why they fail now."]}, {"number": 35853, "title": "Custom Metrics The tensor cannot be accessed here.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: 2.1.0 \r\n- **Python version**: 3.7\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nThe Custom Metric I use cannot be used when compiling the following error is thrown.\r\nThe Custom Metric is:\r\n\r\n`\r\n\r\nclass CaseAccuracy(tf.keras.metrics.Metric):\r\n    def __init__(self, name='case_accuracy', thresholds: float = 0.5, num_classes: int = 24,\r\n                 dtype=tf.float32, **kwargs):\r\n        super(CaseAccuracy, self).__init__(name=name, dtype=dtype, **kwargs)\r\n        self.thresholds = thresholds\r\n        self.num_classes = num_classes\r\n        self.correct_samples = self.add_weight(name='correct_samples', shape=(1,), initializer='zeros', dtype=self.dtype)\r\n        self.num_samples = self.add_weight(name='num_samples', shape=(1,), initializer='zeros', dtype=self.dtype)\r\n        self.accuracy = self.add_weight(name='case_accuracy', shape=(1,), initializer='zeros', dtype=self.dtype)\r\n\r\n    def update_state(self, y_true, y_pred, **kwargs):\r\n        \"\"\"\r\n        Updates the state of the Metric.\r\n\r\n        :param y_true:\r\n        :param y_pred:\r\n        :return:\r\n        \"\"\"\r\n        # Get binary results for predictions\r\n        y_pred = tf.where(y_pred >= self.thresholds, 1., 0.)\r\n\r\n        # Get binary result if prediction is correct\r\n        corr = tf.where(y_pred == y_true, 1., 0.)\r\n\r\n        # Get binary result if all predictions for a sample are correct\r\n        all_corr = tf.where(tf.reduce_sum(corr, axis=-1) == y_pred.shape[-1], 1., 0.,)\r\n        self.correct_samples = self.correct_samples + tf.reduce_sum(all_corr, axis=-1)\r\n        self.num_samples = tf.add(self.num_samples, self.num_classes, name='num_samples_add')\r\n        self.accuracy = tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_div')\r\n        return self.accuracy\r\n\r\n    def result(self):\r\n        return tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_result')\r\n\r\n    def reset_states(self):\r\n        # The state of the metric will be reset at the start of each epoch.\r\n        self.correct_samples.assign(0.)\r\n        self.num_samples.assign(0.)\r\n        self.accuracy.assign(0.)\r\n`\r\n\r\n### Source code / logs\r\n`\r\nfrom aggregation.model import get_model, CaseAccuracy\r\ninput_shapes = [(20, 24), (10, 24)]\r\nnum_classes = 24\r\nmodel = get_model(input_shapes, num_classes)\r\ntry:\r\n    model.compile(\r\n        optimizer='adam',\r\n        loss='categorical_crossentropy',\r\n        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\r\n                 CaseAccuracy()]\r\n    )\r\n    model.summary()\r\nexcept Exception as e:\r\n    print(e)\r\n\r\n    \r\n2020-01-14 09:05:44.297580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-14 09:05:48.583880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-14 09:05:48.606855: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2020-01-14 09:05:48.609872: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ...\r\n2020-01-14 09:05:48.610491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: \r\n2020-01-14 09:05:48.610973: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nThe tensor 'Tensor(\"add:0\", dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=update_state, id=1923611490848); accessed from: FuncGraph(name=keras_graph, id=1923588214736).\r\n`", "comments": ["@Robmosh91,\r\nI tried to reproduce the issue but I'm facing an error stating `ModuleNotFoundError: No module named 'aggregation'`. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/918cb316a1ae2deca67b03cdf3013805/35853.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Hi,\r\nsorry for the inconvenience. aggregation is an internal module.\r\nI added an updated code snipped and a sample model below.\r\nThe Metric is still the same.\r\n`\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ninput_shapes = [(20, 24), (10, 24)]\r\nnum_classes = 24\r\ninputs = [keras.Input(shape=shape, name='InputStream_{}'.format(i)) for i, shape in enumerate(input_shapes)]\r\nstreams = []\r\nfor _input in inputs:\r\n    x = _input\r\n    x = keras.layers.GlobalMaxPool1D()(x)\r\n    streams.append(x)\r\n\r\nx = keras.layers.Concatenate()(streams)\r\nx = keras.layers.Dropout(rate=0.1)(x)\r\nx = keras.layers.Dense(256)(x)\r\nx = keras.layers.ReLU()(x)\r\nx = keras.layers.Dense(num_classes, activation='sigmoid')(x)\r\nmodel = keras.Model(inputs=inputs, outputs=[x])\r\n\r\n\r\nclass CaseAccuracy(tf.keras.metrics.Metric):\r\n    def __init__(self, name='case_accuracy', thresholds: float = 0.5, num_classes: int = 24,\r\n                 dtype=tf.float32, **kwargs):\r\n        super(CaseAccuracy, self).__init__(name=name, dtype=dtype, **kwargs)\r\n        self.thresholds = thresholds\r\n        self.num_classes = tf.cast(num_classes, dtype=dtype)\r\n        self.correct_samples = self.add_weight(name='correct_samples', shape=(1,), initializer='zeros',\r\n                                               dtype=self.dtype)\r\n        self.num_samples = self.add_weight(name='num_samples', shape=(1,), initializer='zeros', dtype=self.dtype)\r\n        self.accuracy = self.add_weight(name='case_accuracy', shape=(1,), initializer='zeros', dtype=self.dtype)\r\n\r\n    def update_state(self, y_true, y_pred, **kwargs):\r\n        \"\"\"\r\n        Updates the state of the Metric.\r\n\r\n        :param y_true:\r\n        :param y_pred:\r\n        :return:\r\n        \"\"\"\r\n        # Get binary results for predictions\r\n        y_pred = tf.where(y_pred >= self.thresholds, 1., 0.)\r\n        # Get binary result if prediction is correct\r\n        corr = tf.where(y_pred == y_true, 1., 0.)\r\n        # Get binary result if all predictions for a sample are correct\r\n        all_corr = tf.where(tf.reduce_sum(corr, axis=-1) == self.num_classes, 1., 0., )\r\n        self.correct_samples = self.correct_samples + tf.reduce_sum(all_corr, axis=-1)\r\n        self.num_samples = tf.add(self.num_samples, self.num_classes, name='num_samples_add')\r\n\r\n        # Set accuracy\r\n        self.accuracy = tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_div')\r\n\r\n    def result(self):\r\n        return tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_result')\r\n\r\n    def reset_states(self):\r\n        # The state of the metric will be reset at the start of each epoch.\r\n        self.correct_samples.assign(0.)\r\n        self.num_samples.assign(0.)\r\n        self.accuracy.assign(0.)\r\n\r\n\r\nmodel.compile(\r\n    optimizer='adam',\r\n    loss='categorical_crossentropy',\r\n    metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall(),\r\n             CaseAccuracy()]\r\n)\r\n```\r\n`", "Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/10267a0fa932103ec7ef5efea893812d/35853_copy.ipynb). Thanks!", "Is there any news for this issue or anybody who can help with a workaround for now?", "The custom loss function should be an instance of `tf.keras.losses.Loss`\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/Model#methods_2", "Hi @ymodak , I\u2019m actually trying use a custom metric. As loss I use focal loss which I have implemented in my own and it runs just fine. Thanks for the help!", "Hi @ymodak , can you please reopen this as it\u2019s not solved, I still cannot use the custom metric and have no clue why. I don\u2019t want to use it as loss, but as a metric during training.\r\nCan you please help to solve the issue?", "Hi @Robmosh91 , instead of\r\n```python\r\nself.accuracy = tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_div')\r\nreturn self.accuracy\r\n```\r\ninside `update_state`, you should do\r\n```python\r\nself.accuracy.assign_add(tf.divide(self.correct_samples, self.num_samples, name='case_accuracy_div'))\r\n```\r\nAlso in `result` you should return `self.accuracy`. This one is just for removing redundant code and following best practice. This step might not be related to your error."]}, {"number": 35852, "title": "ValueError when using AUC metric with multi label flag", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from pip\r\n- TensorFlow version 2.1.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CUDA10.1 and cnDNN7.6\r\n- GPU model and memory: Tesla K80\r\n\r\n**Describe the current behavior**\r\nDuring the compilation of a model the below error occurs. The error is duo to the use of the `tf.keras.metrics.AUC` class with the `multi_label=True` option used. When `multi_label=False`, the model compiles without errors.\r\n\r\n**Describe the expected behavior**\r\nModel should compiles without errors.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.optimizers import SGD\r\n\r\ninputs = Input(shape=(10,))\r\noutput = Dense(3, activation=\"sigmoid\")(inputs)\r\n\r\nmodel = Model(\r\n    inputs=inputs, \r\n    outputs=output\r\n)\r\n\r\nmodel.compile(\r\n    loss='binary_crossentropy',\r\n    optimizer=SGD(lr=1e-3, momentum=0.9), \r\n    metrics=[tf.keras.metrics.AUC(multi_label=True)]\r\n)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in zeros(shape, dtype, name)\r\n   2439         shape = constant_op._tensor_shape_tensor_conversion_function(\r\n-> 2440             tensor_shape.TensorShape(shape))\r\n   2441       except (TypeError, ValueError):\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)\r\n    333     raise ValueError(\r\n--> 334         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\n    335   s_list = s.as_list()\r\n\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (200, None)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-dc556804b7cd> in <module>\r\n     15     loss='binary_crossentropy',\r\n     16     optimizer=SGD(lr=1e-3, momentum=0.9),\r\n---> 17     metrics=[tf.keras.metrics.AUC(multi_label=True)]\r\n     18 )\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\r\n    437           targets=self._targets,\r\n    438           skip_target_masks=self._prepare_skip_target_masks(),\r\n--> 439           masks=self._prepare_output_masks())\r\n    440 \r\n    441       # Prepare sample weight modes. List with the same length as model outputs.\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_metrics(self, outputs, targets, skip_target_masks, sample_weights, masks, return_weighted_metrics, return_weighted_and_unweighted_metrics)\r\n   2002           metric_results.extend(\r\n   2003               self._handle_per_output_metrics(self._per_output_metrics[i],\r\n-> 2004                                               target, output, output_mask))\r\n   2005         if return_weighted_and_unweighted_metrics or return_weighted_metrics:\r\n   2006           metric_results.extend(\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _handle_per_output_metrics(self, metrics_dict, y_true, y_pred, mask, weights)\r\n   1953       with K.name_scope(metric_name):\r\n   1954         metric_result = training_utils.call_metric_function(\r\n-> 1955             metric_fn, y_true, y_pred, weights=weights, mask=mask)\r\n   1956         metric_results.append(metric_result)\r\n   1957     return metric_results\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py in call_metric_function(metric_fn, y_true, y_pred, weights, mask)\r\n   1153 \r\n   1154   if y_pred is not None:\r\n-> 1155     return metric_fn(y_true, y_pred, sample_weight=weights)\r\n   1156   # `Mean` metric only takes a single value.\r\n   1157   return metric_fn(y_true, sample_weight=weights)\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in __call__(self, *args, **kwargs)\r\n    194     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top\r\n    195     return distributed_training_utils.call_replica_local_fn(\r\n--> 196         replica_local_fn, *args, **kwargs)\r\n    197 \r\n    198   @property\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)\r\n   1133     with strategy.scope():\r\n   1134       return strategy.extended.call_for_each_replica(fn, args, kwargs)\r\n-> 1135   return fn(*args, **kwargs)\r\n   1136 \r\n   1137 \r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in replica_local_fn(*args, **kwargs)\r\n    177     def replica_local_fn(*args, **kwargs):\r\n    178       \"\"\"Updates the state of the metric in a replica-local context.\"\"\"\r\n--> 179       update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n    180       with ops.control_dependencies([update_op]):\r\n    181         result_t = self.result()  # pylint: disable=not-callable\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)\r\n     74 \r\n     75     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\r\n---> 76       update_op = update_state_fn(*args, **kwargs)\r\n     77     if update_op is not None:  # update_op will be None in eager execution.\r\n     78       metric_obj.add_update(update_op)\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in update_state(self, y_true, y_pred, sample_weight)\r\n   1883     deps = []\r\n   1884     if not self._built:\r\n-> 1885       self._build(y_true.shape)\r\n   1886 \r\n   1887     if self.multi_label or (self.label_weights is not None):\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in _build(self, shape)\r\n   1844         'true_positives',\r\n   1845         shape=variable_shape,\r\n-> 1846         initializer=init_ops.zeros_initializer)\r\n   1847     self.true_negatives = self.add_weight(\r\n   1848         'true_negatives',\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/metrics.py in add_weight(self, name, shape, aggregation, synchronization, initializer, dtype)\r\n    274         collections=[],\r\n    275         synchronization=synchronization,\r\n--> 276         aggregation=aggregation)\r\n    277 \r\n    278   ### End: For use by subclasses ###\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\r\n    444         synchronization=synchronization,\r\n    445         aggregation=aggregation,\r\n--> 446         caching_device=caching_device)\r\n    447     backend.track_variable(variable)\r\n    448 \r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    742         dtype=dtype,\r\n    743         initializer=initializer,\r\n--> 744         **kwargs_for_getter)\r\n    745 \r\n    746     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\r\n    140       synchronization=synchronization,\r\n    141       aggregation=aggregation,\r\n--> 142       shape=variable_shape if variable_shape else None)\r\n    143 \r\n    144 \r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    256   def __call__(cls, *args, **kwargs):\r\n    257     if cls is VariableV1:\r\n--> 258       return cls._variable_v1_call(*args, **kwargs)\r\n    259     elif cls is Variable:\r\n    260       return cls._variable_v2_call(*args, **kwargs)\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\r\n    217         synchronization=synchronization,\r\n    218         aggregation=aggregation,\r\n--> 219         shape=shape)\r\n    220 \r\n    221   def _variable_v2_call(cls,\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in <lambda>(**kwargs)\r\n    195                         shape=None):\r\n    196     \"\"\"Call on Variable class. Useful to force the signature.\"\"\"\r\n--> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n    198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n    199       previous_getter = _make_getter(getter, previous_getter)\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2594         synchronization=synchronization,\r\n   2595         aggregation=aggregation,\r\n-> 2596         shape=shape)\r\n   2597   else:\r\n   2598     return variables.RefVariable(\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    260       return cls._variable_v2_call(*args, **kwargs)\r\n    261     else:\r\n--> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    263 \r\n    264 \r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\r\n   1409           aggregation=aggregation,\r\n   1410           shape=shape,\r\n-> 1411           distribute_strategy=distribute_strategy)\r\n   1412 \r\n   1413   def _init_from_args(self,\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\r\n   1540           with ops.name_scope(\"Initializer\"), device_context_manager(None):\r\n   1541             initial_value = ops.convert_to_tensor(\r\n-> 1542                 initial_value() if init_from_fn else initial_value,\r\n   1543                 name=\"initial_value\", dtype=dtype)\r\n   1544           if shape is not None:\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in <lambda>()\r\n    120           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\r\n    121         initializer = initializer()\r\n--> 122       init_val = lambda: initializer(shape, dtype=dtype)\r\n    123       variable_dtype = dtype.base_dtype\r\n    124   if use_resource is None:\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\r\n    112     if dtype is None:\r\n    113       dtype = self.dtype\r\n--> 114     return array_ops.zeros(shape, dtype)\r\n    115 \r\n    116   def get_config(self):\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in zeros(shape, dtype, name)\r\n   2441       except (TypeError, ValueError):\r\n   2442         # Happens when shape is a list with tensor elements\r\n-> 2443         shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n   2444     if not shape._shape_tuple():\r\n   2445       shape = reshape(shape, [-1])  # Ensure it's a vector\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1312 \r\n   1313     if ret is None:\r\n-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1315 \r\n   1316     if ret is NotImplemented:\r\n\r\n~/miniconda3/envs/autotagging/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _tensor_shape_tensor_conversion_function(s, dtype, name, as_ref)\r\n    332   if not s.is_fully_defined():\r\n    333     raise ValueError(\r\n--> 334         \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\r\n    335   s_list = s.as_list()\r\n    336   int64_value = 0\r\n\r\nValueError: Cannot convert a partially known TensorShape to a Tensor: (200, None)\r\n```", "comments": ["Was able to reproduce the issue. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/956ffdfdf8ab09650c570b6b3caf8220/35852.ipynb). Thanks!", "Having the same problems. Hope this is resolved soon", "@dmirecki \r\nCan you please try with nightly version (`!pip install tf-nightly `) and see if the problem still persists. I am not seeing any issue with nightly version.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/5afc10120066fcfa0d6da4fab49c6178/35852.ipynb). Please, close the issue if the issue was resolved.Thanks!", "@ravikyram You are right. With that fix we can compile model. But there is still an error during training. Please, look at this [colab notebook](https://colab.research.google.com/drive/1uIujbu0vkYhV6iZFHj7A_dJuUYYmLgwS)", "@sashulyak @dmirecki This was resolved in recent `tf-nightly`. As this was resolved, I am closing this isssue. Please feel free to reopen if the issue persists for you. [Here](https://colab.research.google.com/gist/jvishnuvardhan/45a783e8bae69541f4d4193bb5dd390e/copy-of-35852.ipynb) is the gist for y/our reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35852\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35852\">No</a>\n"]}, {"number": 35851, "title": "TensorFlow Lite with Python", "body": "Hi,\r\n\r\nis there a way (documented procedure) for preparing \"Python wheel\" to just install the interpreter to run inferences with TensorFlow Lite.  On the platform I use (arm64) I have option to run an inference with CPU or GPU. How this will be evaluated via Interpreter, is there an option to have more possible IP blocks to run inference on?\r\nI'm using the 1.13.2 version.", "comments": ["We have prebuilt tf lite interpreter python wheel for TF 1.14 arm64 platform.\r\nSee https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter\r\n", "is that possible to have more available accelerators? so what if GPU and\nCPU are available and both capability to run tf-lite inference?\n\nOn Wed, Jan 15, 2020 at 1:22 AM Yasir Modak <notifications@github.com>\nwrote:\n\n> We have prebuilt tf lite interpreter python wheel for TF 1.14 arm64\n> platform.\n> See\n> https://www.tensorflow.org/lite/guide/python#install_just_the_tensorflow_lite_interpreter\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35851?email_source=notifications&email_token=ALGLVQTSTNRKCXOTUMSF7WTQ5ZJL3A5CNFSM4KGOT2UKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI6TPXI#issuecomment-574437341>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQUC3WO23VEIK7GJDUTQ5ZJL3ANCNFSM4KGOT2UA>\n> .\n>\n", "Unfortunately not. For using gpu inference you have to implement gpu delegate. Thanks!"]}, {"number": 35850, "title": "My TFLite model works with 32-bit ARM-based architecture but doesn't work with 64-bit AArch64 architecture", "body": "Hi,\r\n\r\n1. I've created a TFLite model and ran it on a device with 32-bit ARM-based CPUs and it worked fine.\r\n2. I've done the same thing for 64-bit architecture and there seems to be a problem:\r\nI built the libtensorflowlite.so files using Bazel build (used --cpu=arm64-v8a for 64bit) as your guides explains. I pushed the complied files to my Android platform and tried to run the model.\r\nI don't get any errors, the model is allegedly terminated successfully but it reports 0 [ms] runtimes and no output.\r\n\r\n**Do you have any idea what can be the problem?** \r\n", "comments": ["You'd better provide your own error reporter to check if there is any error from your TFLite Interpreter.\r\nhttps://www.tensorflow.org/lite/api_docs/cc/class/tflite/error-reporter", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 35849, "title": "Unexpected device usage in optimizer apply_gradients(zip())", "body": "Hi,\r\nI have two GPUs and test a liner model on TF 2.1.0. This is the main part of the code:\r\n\r\n```\r\nDEVICE = \"/gpu:1\"\r\n\r\nwith tf.device(DEVICE):\r\n    grads = tf.Variable(0.0)\r\n    opt = tf.optimizers.Adam(1e-6)\r\n\r\n@tf.function\r\ndef train_one_batch(model,train_data,train_label):\r\n    print(\"tracing batch\")\r\n    with tf.device(DEVICE):\r\n        with tf.GradientTape() as tape:\r\n            predicts = model(train_data)\r\n            loss = tf.nn.l2_loss(predicts - train_label)\r\n            grads = tape.gradient(loss, [model.W, model.b])\r\n            opt.apply_gradients(zip(grads, [model.W, model.b])) \r\n```\r\n\r\nI make sure that all other tensors/ops are on GPU:1,so this program is surposed to run on GPU:1 only without GPU:0. But check the nvidia-smi console while running:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 31%   43C    P2    54W / 250W |  10571MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 29%   40C    P2    53W / 250W |  10635MiB / 10989MiB |      7%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nAccording to the memory-usage of GPU:0, this GPU is already in use. When I delete the last line \"opt.apply_gradients(zip(grads, [model.W, model.b])) \" and run it again, nvidia-smi shows:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n| 31%   43C    P2    54W / 250W |    165MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  Off  | 00000000:82:00.0 Off |                  N/A |\r\n| 30%   40C    P2    53W / 250W |  10571MiB / 10989MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nIt shows that GPU:0 is not in use as expected.\r\n\r\nSo,this is the problem,why the line \"opt.apply_gradients(zip(grads, [model.W, model.b])) \"\r\nuse an unexperted device despite I have alrealy specified one. Are there any other APIs to avoid this problem.\r\n\r\nAnd whats more, that line also causes another tracing behavior(retrace) in the tf.function after first trace.\r\n\r\nThanks", "comments": ["@SongLingzhi \r\nLooks like code is incomplete. Can you help us with colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster", "Thank you for your reply, I will send the full-version code via email later, maybe tomorrow~\r\n \r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"ravikyram\"<notifications@github.com&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2020\u5e741\u670814\u65e5(\u661f\u671f\u4e8c) \u665a\u4e0a11:26\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;; \r\n\u6284\u9001: \"Sgt.Royce\"<550247896@qq.com&gt;; \"Mention\"<mention@noreply.github.com&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Unexpected device usage in optimizer apply_gradients(zip()) (#35849)\r\n\r\n\r\n\r\n\r\n@SongLingzhi\r\n Looks like code is incomplete. Can you help us with colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "Hello, this is the full-version code, some word has been replaced by \"sensitive\" due to sensitive reason.\r\n", "@SongLingzhi \r\nPlease, share the code. Thanks!", "@ravikyram \r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\r\n\r\nimport tensorflow as tf\r\nimport pickle\r\n\r\nDATA_DIR = \"/sensitive_data/\"\r\nDEVICE = \"/gpu:1\"\r\nRANDOM_SEED = 12345\r\nONEHOT_LENGTH = 1375432\r\nDEBUG = True\r\n\r\nif DEBUG:\r\n    tf.debugging.set_log_device_placement(True)\r\n    print(tf.config.experimental.list_physical_devices('GPU')) # [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\r\n\r\n\r\n@tf.function\r\ndef _parse_function(example_proto):\r\n    with tf.device(\"/cpu:0\"):\r\n        feature_description = {\r\n            'sensitiveA': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=\"\"),\r\n            'sensitiveB': tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=0.0),\r\n            'sensitiveC': tf.io.FixedLenFeature(shape=[], dtype=tf.float32, default_value=0.0),\r\n            'sensitiveD': tf.io.FixedLenFeature(shape=[], dtype=tf.string, default_value=\"\"),\r\n            'sensitiveE': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.int64, default_value=0, allow_missing=True),\r\n        }\r\n        return tf.io.parse_single_example(example_proto, feature_description)\r\n\r\n\r\nclass Model():\r\n    def __init__(self):\r\n        with tf.device(DEVICE):\r\n            self.W = tf.Variable(tf.random.uniform(shape=[ONEHOT_LENGTH,1],minval=0,maxval=1.0/ONEHOT_LENGTH,seed=RANDOM_SEED))\r\n            self.b = tf.Variable(170.0)\r\n\r\n    @tf.function\r\n    def __call__(self, _x):\r\n        with tf.device(DEVICE):\r\n            x = tf.dtypes.cast(_x, tf.float32)\r\n            return tf.matmul(x, self.W) + self.b\r\n\r\n\r\nwith tf.device(DEVICE):\r\n    grads = tf.Variable(0.0)\r\n    opt = tf.optimizers.Adam(1e-6)\r\n\r\n@tf.function\r\ndef train_one_batch(model,train_data,train_label):\r\n    print(\"tracing batch\")\r\n    with tf.device(DEVICE):\r\n        with tf.GradientTape() as tape:\r\n            predicts = model(train_data)\r\n            loss = tf.nn.l2_loss(predicts - train_label)\r\n            grads = tape.gradient(loss, [model.W, model.b])\r\n            opt.apply_gradients(zip(grads, [model.W, model.b]))   #fixme: causes GPU0 uasge and retrace\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    sensitive_tfrecord = pickle.load(open(\"./sensitive.pkl\",\"rb\"))\r\n    for i in range(len(sensitive_tfrecord)):\r\n        sensitive_tfrecord[i] = DATA_DIR + sensitive_tfrecord[i]\r\n\r\n    dataset = tf.data.TFRecordDataset(sensitive_tfrecord).map(_parse_function,num_parallel_calls=22)\r\n\r\n    with tf.device(DEVICE):\r\n        model = Model()\r\n\r\n        for data in dataset.batch(20):\r\n            train_one_batch(model, data[\"sensitiveE\"], data[\"sensitiveB\"])\r\n```", "Logging device placement should print devices (gpu,cpu) used to execute the operations.\r\nSee https://www.tensorflow.org/guide/gpu#manual_device_placement\r\nWhat information is logged for `apply_gradients` op?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35848, "title": "Feature request: RecallAtPrecision Metric", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 2.1.0\r\n- **Python version**: (3, 6, 8, 'final', 0)\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\nFeature request:\r\ntf.keras.metrics.RecallAtPrecision\r\n\r\nThank you all for including PrecisionAtRecall, would it be possible to also add in the inverse, Recall At Precision? I tried adding a custom metric, but its rather difficult to replicate the pattern, since I cannot inherit the `SensitivitySpecificityBase` class defined in the source code.\r\n\r\nThank you!", "comments": ["hello! how is this going?\r\nanything I can do to help?\r\n\r\nThank you", "@tangbyron Please feel free to send us a PR. You can implement it the same way `PrecisionAtRecall` has been implemented. Not sure what you mean by you cannot use `SensitivitySpecificityBase`, you should be able to use it the same way it is in `PrecisionAtRecall`.\r\n\r\nThank you!", "@pavithrasv perfect - working on it now. thank you!", "`RecallAtPrecision` has been added.", "thank you!"]}, {"number": 35847, "title": "The result of NNAPI hardswish quantization implementation is different with tensorflow hardswish", "body": "Recently,  I ran AI benchmark v4 model(Mobilenet-v3-quant.tflite) by label_image. But I got the different result by tflite and NNAPI implementation quantization. The detailed difference is just 1 per quant HardSwish layer. And HardSwish is splited by NNAPI delegate as below: \r\n\r\n> // Lower hardswish according to the following equation:\r\n> // hard_swish[x] = x (ReLU6(x + 3)) / 6 == x * (Relu_N1_to_1(x/3) * 3 + 3) / 6\r\n> // = 0.5x * Relu_N1_to_1(x/3) + 0.5x\r\n\r\nin nnpai_delegate.cc at the line around 510.\r\nBTW, NNAPI uses int32 quantization but tflite uses int16.", "comments": ["A couple of questions:\r\n1. Which device are you using?\r\n2. Could you provide the test input & output so that we can reproduce it? Or you can try using \"nnapi-reference\" to see if you still see the same off be 1 error.\r\n3. Is this causing significant accuracy problem for the whole model? \r\n\r\nbackground:\r\nNNAPI does allow off by 1 error for hardware accelerators, to allow different implementation / optimization.\r\nAnd I believe NNAPI is using 8 bit quantization, the int32 scalars were just for passing certain parameters.", "Sorry for lately responding,  Actually, NNAPI use 8-bit quantization, but I mean the implementation of the operator is  32bit int32, and I checked the implementation of tflite that was int16 implementation.\r\n> I used Samsung test devices.\r\n> I compare the tflite with NNAPI-sample drv.\r\n>In my Opinion,  the NNAPI seems to be the higher accuracy.", "I guess you meant the intermediate tensors are int16 for TFLite CPU implementation. And since NNAPI delegate uses lower level operations to compute hard_swish, the intermediate output are 8bit. Please correct me if that's incorrect.\r\n\r\nBack to my original question, is this off-by-1 error causing significant difference in the accuracy for the model?\r\n\r\n(also, FYI, for Android R, NNAPI will be adding a fused hard_swish which hopefully would be faster and more accurate: https://android.googlesource.com/platform/frameworks/ml/+/refs/heads/master/nn/runtime/include/NeuralNetworks.h#5388)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35847\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35847\">No</a>\n"]}, {"number": 35846, "title": "tf.image.random_brightness do not reproducible with multiprocessing on tf.data.Dataset.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.1.0-rc1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: V10.1.243\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI use tf.image.random_brightness api for augmentation. With dataset api I put those augmentation functions in parser method and use Dataset.map api for applying the method to the dataset and _Transformed dataset_ is not reproducible when I put non negative integer > 0 in _num_parallel_calls_ argument of Dataset.map api. It is also the same if I use tf.data.experimental.AUTOTUNE for num_parallel_calls argument.\r\n\r\n**Code to reproduce the issue**\r\n[here is the colab link to reproduce the issue](https://colab.research.google.com/drive/1uNpn1Rf1_WvG2lnAS41g36IDWOB2IW7-)\r\n\r\n**Other info / logs**\r\nIt looks like tf.random.uniform method is the reason of happening this. The implementation of tf.image.random_brightness use tensorflow random uniform method and the method is not reproducible with multiprocessing. I have tested tf.image.random_contrast and tf.image.random_saturation. They are not reproducible also.\r\n", "comments": ["I am closing this issue. It can be solved by using **tf.image.stateless_random_brightness** starting from TensorFlow 2.4.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35846\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35846\">No</a>\n"]}, {"number": 35845, "title": " TensorT* nullptr check to avoid segfault", "body": "Check TensorT* tensor for nullptr before accessing tensor->shape[channel_dim_index] to avoid segfault.", "comments": []}, {"number": 35844, "title": "Aggregate lists of trainable variables from different Keras models in Tensorflow v2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.1.0-rc1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI have two neural networks (e.g., f and g) implemented using `tensorflow.keras.Model`. I simply want to compute the gradient of f(g(x)). However, I was not able to aggregate the lists of trainable variables of both models. \r\n\r\nAccording to Tensorflow V2 documentations: `If you need to aggregate lists of variables (like tf.Graph.get_collection(tf.GraphKeys.VARIABLES)), use the .variables and .trainable_variables attributes of the Layer and Model objects.`\r\n\r\nI tried to do so but I am getting this weird error:\r\n\r\n    /content/drive/My Drive/Colab/IIC/IIC.py in train_step(self, x, gx, head)\r\n        197 \r\n        198                 gradients = tape.gradient(loss, trainable_variables)\r\n    --> 199                 self.optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n        200                 self.train_loss(loss)\r\n        201 \r\n    \r\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n        432         _ = self.iterations\r\n        433         self._create_hypers()\r\n    --> 434         self._create_slots(var_list)\r\n        435 \r\n        436       if not grads_and_vars:\r\n    \r\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/adam.py in _create_slots(self, var_list)\r\n        147     # Separate for-loops to respect the ordering of slot variables from v1.\r\n        148     for var in var_list:\r\n    --> 149       self.add_slot(var, 'm')\r\n        150     for var in var_list:\r\n        151       self.add_slot(var, 'v')\r\n    \r\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in add_slot(self, var, slot_name, initializer)\r\n        572     if slot_name not in self._slot_names:\r\n        573       self._slot_names.append(slot_name)\r\n    --> 574     var_key = _var_key(var)\r\n        575     slot_dict = self._slots.setdefault(var_key, {})\r\n        576     weight = slot_dict.get(slot_name, None)\r\n    \r\n    /tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _var_key(var)\r\n       1063   if hasattr(var, \"_distributed_container\"):\r\n       1064     var = var._distributed_container()\r\n    -> 1065   if var._in_graph_mode:\r\n       1066     return var._shared_name\r\n       1067   return var._unique_id\r\n    \r\n    AttributeError: 'list' object has no attribute '_in_graph_mode'\r\n\r\nThe `trainable_variables` is just a list of trainable variables from both models f and g.\r\n\r\n\r\n    trainable_variables = [f.trainable_variables, g.trainable_variables]\r\n\r\n\r\n\r\n", "comments": ["@nairouz \r\n\r\nRequest you to provide colab link or simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\n\r\nHere is a toy example:\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n# Install TensorFlow\r\ntry:\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# Add a channels dimension\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(32)\r\n\r\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n\r\n\r\nclass StupidModel1(Model):\r\n  def __init__(self):\r\n    super(StupidModel1, self).__init__()\r\n    self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.flatten = Flatten()\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    return x\r\n\r\nclass StupidModel2(Model):\r\n  def __init__(self):\r\n    super(StupidModel2, self).__init__()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n# Create an instance of the model\r\nstupid_model_1 = StupidModel1()\r\nstupid_model_2 = StupidModel2()\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n@tf.function\r\ndef train_step(images, labels):\r\n  with tf.GradientTape() as tape:\r\n    predictions = stupid_model_2(stupid_model_1(images))\r\n    loss = loss_object(labels, predictions)\r\n  trainable_variables_list = [stupid_model_1.trainable_variables, stupid_model_2.trainable_variables]\r\n  gradients = tape.gradient(loss, trainable_variables_list)\r\n  optimizer.apply_gradients(zip(gradients, trainable_variables_list))\r\n\r\n  train_loss(loss)\r\n  train_accuracy(labels, predictions)\r\n\r\n@tf.function\r\ndef test_step(images, labels):\r\n  predictions = stupid_model_2(stupid_model_1(images))\r\n  t_loss = loss_object(labels, predictions)\r\n\r\n  test_loss(t_loss)\r\n  test_accuracy(labels, predictions)\r\n\r\nEPOCHS = 5\r\n\r\nfor epoch in range(EPOCHS):\r\n  # Reset the metrics at the start of the next epoch\r\n  train_loss.reset_states()\r\n  train_accuracy.reset_states()\r\n  test_loss.reset_states()\r\n  test_accuracy.reset_states()\r\n\r\n  for images, labels in train_ds:\r\n    train_step(images, labels)\r\n\r\n  for test_images, test_labels in test_ds:\r\n    test_step(test_images, test_labels)\r\n\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n  print(template.format(epoch+1,\r\n                        train_loss.result(),\r\n                        train_accuracy.result()*100,\r\n                        test_loss.result(),\r\n                        test_accuracy.result()*100))\r\n```\r\n", "It is not a bug!!\r\nI am sorry.", "If it is not a bug, then what is the preferred way to solve this? I have  a similar case.\r\nCalling apply_gradients() twice, once for every model with its own trainable variables, does not give an error but might be wrong since the optimizer keeps internal state. Another solution would be to create two optimizers, one for every model, and then call apply_gradients separately for every model. \r\n\r\nAgain, what is the preferred solution here if it is not a bug?", "> If it is not a bug, then what is the preferred way to solve this? I have a similar case.\r\n> Calling apply_gradients() twice, once for every model with its own trainable variables, does not give an error but might be wrong since the optimizer keeps internal state. Another solution would be to create two optimizers, one for every model, and then call apply_gradients separately for every model.\r\n> \r\n> Again, what is the preferred solution here if it is not a bug?\r\n\r\nThe list of trainable_weights might be [list of variable], instead of [[list of variable], [list of variable]]."]}, {"number": 35843, "title": "Keras does not allow using custom iterator from a Python generator", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying to run model.fit() on a tf.keras model using a custom data generator on my database. While this generator derives from tf.data.Dataset, it is not a subclass because there some preprocessing involved. However, it supports an iterator that can generate batches of input-output pairs and is compatible with the Keras guidelines. However, tf.keras does not let me use this iterator and throws the following ValueError:\r\n![image](https://user-images.githubusercontent.com/14254187/72313857-9774a680-3641-11ea-9ab2-af8deefd7307.png)\r\n\r\n`ValueError: For performance reasons Keras `fit`, `evaluate` and`predict` accept tf.data `Datasets` as input but not iterators that have been manually generated from Datasets by users. Please directly pass in the original `Dataset` object instead of passing in iter(dataset).`\r\n\r\nIs there a flag to overrule this behavior, which seems like a performance enhancement issue and not a fundamental inability of the package?", "comments": ["@PrieureDeSion,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "I created a keras Sequence wrapper that calls my custom iterator and that works fine. Slightly inconvenient and certainly inefficient, but gets the job done."]}, {"number": 35842, "title": "tensorflow/python/eager/BUILD:14:1: in cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in dependency graph", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS: Windows 10 Pro x64 18362\r\n- TensorFlow installed from (source or binary): source (master, 8782b7679c12780fd914827abf4e79ceb51d6b41)\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.5.3/3.6.8/3.7.6/3.8.1\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 1.2.1\r\n- GCC/Compiler version (if compiling from source): VS2017 14.16.27023\r\n- CUDA/cuDNN/tensorrt version: 10.2.89, 7.6.5.32, 7.0.0.11\r\n- GPU model and memory: RTX 2080Ti, 11GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/tensorflow/venv36/Scripts/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/tensorflow/venv36/lib/site-packages\"\r\nbuild --python_path=\"C:/tensorflow/venv36/Scripts/python.exe\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,7.5\"\r\nbuild --config=cuda\r\n# build --config=tensorrt\r\nbuild:opt --copt=/arch:AVX\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --define=override_eigen_strong_inline=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n\r\n```\r\n\r\nIf I enable tensorrt and compile using the method described in [this article](https://tensorflow.google.cn/install/source_windows#gpu_support), I get the following error:\r\n\r\n\r\n```\r\nERROR: C:/tensorflow/tensorflow/python/eager/BUILD:14:1: in cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in dependency graph:\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n    //tensorflow/python/keras/distribute:distribute_strategy_test_lib\r\n    //tensorflow/python/distribute:tpu_strategy\r\n    //tensorflow/python/distribute:values\r\n    //tensorflow/python:framework_ops\r\n    //tensorflow/python:tensor_conversion_registry\r\n    //tensorflow/python/eager:context\r\n    //tensorflow/python/eager:executor\r\n    //tensorflow/python:pywrap_tfe\r\n    //tensorflow/python:pywrap_tensorflow\r\n    //tensorflow/python:pywrap_tensorflow_internal\r\n    //tensorflow/python:pywrap_tensorflow_internal.py\r\n    //tensorflow/python:pywrap_tensorflow_internal_py_wrap\r\n.-> //tensorflow/python/eager:pywrap_tfe_lib\r\n|   //tensorflow/python:ndarray_tensor_bridge\r\n|   //tensorflow/python:bfloat16_lib\r\n|   //tensorflow/python:safe_ptr\r\n|   //tensorflow/c/eager:c_api\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_master_service\r\n|   //tensorflow/core/distributed_runtime:master\r\n|   //tensorflow/core/distributed_runtime:master_session\r\n|   //tensorflow/core/distributed_runtime:scheduler\r\n|   //tensorflow/core:tensorflow_opensource\r\n|   //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n|   //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n`-- //tensorflow/python/eager:pywrap_tfe_lib\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\n```\r\n\r\n", "comments": ["After updating to master(ffa0b12d433b25b9e30f94f1f5e80ba8ae89adbb), new problems appeared:\r\n```\r\nERROR: F:/tensorflow/tensorflow/python/BUILD:5491:1: in cc_binary rule //tensorflow/python:_pywrap_tensorflow_internal.so: cycle in dependency graph:\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n    //tensorflow/python/distribute:multi_process_runner\r\n    //tensorflow/python/compat:v2_compat\r\n    //tensorflow/python:control_flow_v2_toggles\r\n    //tensorflow/python:control_flow_util\r\n    //tensorflow/python:platform\r\n    //tensorflow/python:_pywrap_util_port\r\n    //tensorflow/python:_pywrap_util_port.pyd\r\n    //tensorflow/python:_pywrap_util_port_pyd_copy\r\n    //tensorflow/python:_pywrap_util_port.so\r\n    //tensorflow/python:_pywrap_tensorflow_internal_windows\r\n    //tensorflow/python:_pywrap_tensorflow_internal.dll\r\n    //tensorflow/python:_pywrap_tensorflow_internal.dll_rule\r\n.-> //tensorflow/python:_pywrap_tensorflow_internal.so\r\n|   //tensorflow/python:py_func_lib\r\n|   //tensorflow/python:ndarray_tensor\r\n|   //tensorflow/python:ndarray_tensor_bridge\r\n|   //tensorflow/python:bfloat16_lib\r\n|   //tensorflow/python:safe_ptr\r\n|   //tensorflow/c/eager:c_api\r\n|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n|   //tensorflow/core/distributed_runtime:master_session\r\n|   //tensorflow/core/distributed_runtime:scheduler\r\n|   //tensorflow/core:tensorflow_opensource\r\n|   //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n|   //tensorflow/compiler/tf2tensorrt:trt_engine_resource_op_kernels\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n`-- //tensorflow/python:_pywrap_tensorflow_internal.so\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\nINFO: Elapsed time: 2.081s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (7 packages loaded, 2852 targets configured)\r\n```", "Apologies for the delay in response. Is this still an issue? Also the bazel version requirements have been updated. See https://github.com/tensorflow/tensorflow/blob/5d615f2f05d6ecfc37951ef574e359829cb9e3e0/configure.py#L52\r\nThanks!", "> Apologies for the delay in response. Is this still an issue? Also the bazel version requirements have been updated. See\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/5d615f2f05d6ecfc37951ef574e359829cb9e3e0/configure.py#L52\r\n> \r\n> \r\n> Thanks!\r\n\r\nthank you for your reply! Due to the influence of COVID-19 recently, I have no access to the computer used for compilation. I may not be able to resume work until mid-May, and I must reply in time if I could have access to computer!", "> Apologies for the delay in response. Is this still an issue? Also the bazel version requirements have been updated. See\r\n> \r\n> https://github.com/tensorflow/tensorflow/blob/5d615f2f05d6ecfc37951ef574e359829cb9e3e0/configure.py#L52\r\n> \r\n> \r\n> Thanks!\r\n\r\nIn the r2.2 branch (2b96f36) and master (1b49bd1), the problem still exists.\r\n\r\n### System information\r\n\r\nOS: Windows 10 Pro x64 18362\r\nTensorFlow installed from (source or binary): source (r2.2, 2b96f36), (master 1b49bd1)\r\nTensorFlow version: 2.2.0, master\r\nPython version: 3.5.3/3.6.8/3.7.7/3.8.2\r\nInstalled using virtualenv? pip? conda?: virtualenv\r\nBazel version (if compiling from source): 2.0.0(r2.2), 3.0.0(master)\r\nGCC/Compiler version (if compiling from source): VS2017 14.16.27023/VS2019 14.25.28610\r\nCUDA/cuDNN/tensorrt version: 10.2.89, 7.6.5.32, 7.0.0.11\r\nGPU model and memory: RTX 2080Ti, 11GB", "I get a similar cycle on 1.15, if I enable `--config=tensorrt` on Windows", "@vistart \r\nCould you please try on the latest stable tf version 2.4.1 and let us know if this is still an issue.", "> @vistart\r\n> Could you please try on the latest stable tf version 2.4.1 and let us know if this is still an issue.\r\n\r\nOne year has passed since the last submission, and the software environment has changed. Currently I am waiting for NVIDIA to release TensorRT 8.0 for Windows. After the release of this version, I will test the compilation for the first time and report the results here.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35842\">No</a>\n", "System information\r\n\r\nOS: Windows 10 Pro x64 19043\r\nTensorFlow installed from (source or binary): source (r2.6, 6efc4052cbb)\r\nTensorFlow version: 2.6.0\r\nPython version: 3.9.6\r\nInstalled using virtualenv? pip? conda?: virtualenv\r\nBazel version (if compiling from source): 3.7.2\r\nGCC/Compiler version (if compiling from source): VS2019 16.10.3\r\nCUDA/cuDNN/tensorrt version: 11.3.1, 8.2.1, 8.0.1\r\nGPU model and memory: RTX 4060, 6GB\r\n\r\nThe problem remains, but the details are different:\r\n```\r\nERROR: D:/projects/tensorflow/tensorflow/python/BUILD:3345:8: in genrule rule //tensorflow/python:pywrap_tensorflow_import_lib_file: cycle in dependency graph:\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n    //tensorflow/python/keras/distribute:dataset_creator_model_fit_test_base\r\n    //tensorflow/python/platform:client_testlib\r\n    //tensorflow/python:framework_test_lib\r\n    //tensorflow/python/framework:test_lib\r\n    //tensorflow/python:platform\r\n    //tensorflow/python/platform:platform\r\n    //tensorflow/python/lib/io:lib\r\n    //tensorflow/python/lib/io:_pywrap_record_io\r\n    //tensorflow/python/lib/io:_pywrap_record_io.pyd\r\n    //tensorflow/python/lib/io:_pywrap_record_io_pyd_copy\r\n    //tensorflow/python/lib/io:_pywrap_record_io.so\r\n    //tensorflow/python:_pywrap_tensorflow_internal_windows\r\n.-> //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n|   //tensorflow/python:safe_ptr\r\n|   //tensorflow/python/lib/core:safe_ptr\r\n|   //tensorflow/c:c_api_no_xla\r\n|   //tensorflow/compiler/mlir/tfr:node_expansion_pass\r\n|   //tensorflow/core/common_runtime/eager:core_no_xla\r\n|   //tensorflow/core/common_runtime/eager:execute\r\n|   //tensorflow/core/common_runtime/eager:mkl_eager_op_rewrite\r\n|   //tensorflow/core:all_kernels\r\n|   //tensorflow/core:all_kernels_impl\r\n|   //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\n|   //tensorflow/compiler/tf2tensorrt:trt_conversion\r\n|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n`-- //tensorflow/python:pywrap_tensorflow_import_lib_file\r\nThis cycle occurred because of a configuration option\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted\r\n```"]}, {"number": 35841, "title": "ValueError: Input array not provided for operation 'reshape'.", "body": "**System information**\r\n- MacOS 10.12 (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from source: pip\r\n- TensorFlow version (or github SHA if from source): 1.15.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-d6e20568d7d5> in <module>()\r\n      5 \r\n      6 # Load TFLite model and allocate tensors.\r\n----> 7 interpreter = tf.lite.Interpreter(model_path=\"/Users/zhuxinyu/Downloads/logits_model(2).tflite\")\r\n      8 interpreter.allocate_tensors()\r\n      9 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow_core/lite/python/interpreter.py in __init__(self, model_path, model_content, experimental_delegates)\r\n    204       self._interpreter = (\r\n    205           _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n--> 206               model_path))\r\n    207       if not self._interpreter:\r\n    208         raise ValueError('Failed to open {}'.format(model_path))\r\n\r\nValueError: Input array not provided for operation 'reshape'.\r\n```\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_session(infer_sess, [infer_graph.get_tensor_by_name('Infer/DKT/input_buf:0'), infer_graph.get_tensor_by_name('Infer/DKT/input_len:0')], [infer_graph.get_tensor_by_name('Infer/DKT/before_output:0')])\r\nconverter.post_training_quantize = True\r\n```\r\nI get the quantized model but when I load the mode using `interpreter = tf.lite.Interpreter(model_path=\"/Users/zhuxinyu/Downloads/logits_model(2).tflite\")`\uff0cit says it does not support reshape operation\uff0cbut it seems to be inevitable in my network(it happens in a dense layer i think)\uff0chow can I make the tf.layer.dense not using reshape inside?\r\n", "comments": ["@TianHongZXY Can you please provide a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Download this model and run the code below, please modify the model_path to make the model reachable. Thank you!\r\n[logits_model.tflite.zip](https://github.com/tensorflow/tensorflow/files/4063604/logits_model.tflite.zip)\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"logits_model(2).tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\nprint(output_details)\r\n# Test model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_buf = np.array(np.random.randint(0, 50, input_shape), dtype=np.int32)\r\ninput_len = np.array([1, 15], dtype=np.int32)\r\n# input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_buf)\r\ninterpreter.set_tensor(input_details[1]['index'], input_len)\r\n\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data.shape)\r\nprint(output_data)\r\n```", "@TianHongZXY I could reproduce the issue with `TF1.15`. However, there is no issue when I used `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/f548a567f7d383782be57f3c8bfce16d/untitled765.ipynb). Thanks!", "@jvishnuvardhan Yes, it works! Thank you very much!"]}, {"number": 35840, "title": "Eager code with tf.function decoration is >2x slower than without it.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version: \r\n    TF 2.0.0\r\n- Python version:\r\n   Python 3.7.6\r\n- CUDA/cuDNN version:\r\n   10.0.130\r\n- GPU model and memory:\r\n   GTX2080\r\n\r\n**Describe the current behavior**\r\nI walk through the ```@tf.function``` usage examples from [Tensorflow Tutorial](https://www.tensorflow.org/guide/function). There is a example shows that the eager code version of  ```conv_layer``` with ```@tf.function``` decorator could run faster than without it. However, when I imitate it and write eager code version  of ```embedding_layer```, the decorator one is >2x slower than the not decorator one. See the code below\r\n\r\n**Code to reproduce the issue**\r\n```\r\n# check time it for embedding layer\r\nfrom tensorflow.keras.initializers import RandomUniform\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput_dim = 1000\r\noutput_dim = 300\r\nemb_layer = tf.keras.layers.Embedding(input_dim, output_dim, RandomUniform(-1,1))\r\n\r\n@tf.function\r\ndef emb_fn(indices):\r\n    return emb_layer(indices)\r\n\r\n\r\nindices = np.random.randint(0, input_dim-1, size=64)\r\nindices = tf.constant(indices)\r\n# warm up\r\nemb_layer(indices); emb_fn(indices)\r\n\r\nprint(\"Eager embed: \", timeit.timeit(lambda: emb_layer(indices), number=10000))\r\nprint(\"Function embed: \", timeit.timeit(lambda: emb_fn(indices), number=10000))\r\n```\r\n**The result**\r\nEager embed:  1.895560858771205\r\nFunction embed:  4.15901411511004\r\n\r\n\r\n", "comments": ["@HansiZeng,\r\nI tried to reproduce the issue but could not find much difference in the execution time. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/63ea6bcef523087f47a14ef0eb8401a5/35840.ipynb).", "@amahendrakar, \r\nI found TF: 2.1.0 actually fixed the issue as you code shown, but in TF: 2.0.0 the running time difference exists. To reproduce the result, you can convert your TF version to 2.0.0.", "Was able to reproduce the issue with TF 2.0. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/d4b1ca4e863f070c923a4ce9d36ee58a/35840_copy.ipynb). Thanks!", "@HansiZeng There were couple of important performance improvements after releasing `TF2.0`. So please use latest stable version for better performance. Another reason for the small difference in performance is due to overhead of the @tf.function. \r\n\r\nBest and more detailed answer for a similar question was provided [here](https://github.com/tensorflow/tensorflow/issues/35720#issuecomment-574188882) by @mdanatg as follows\r\n\r\n> Unfortunately, tf.function is not always faster for small computations. The reason is quite simple: calling an empty tf.function is more expensive than calling an empty Python function. This is especially true when the computations involve scalars, when the speed gained inside the function is too small to recover the cost of calling it.\r\n> \r\n> This can sometimes be avoided by one of the following:\r\n> \r\n> vectorizing the computation (e.g. instead of operating on one example at a time, operate on an entire batch); here is such an example\r\n> adding experimental_compile=True, which can greatly speed up complex scalar computations when XLA support is available\r\n> you may also speed things up by moving more computation inside the tf.function iself\r\n> This is something that we're addressing, as there are a few inefficiencies in the current implementation.\r\n\r\nI am closing this issue. Please let me know if I am mistaken. Thanks!"]}, {"number": 35839, "title": "[INTEL MKL] Use buffer as primitive key", "body": "This is a patch for #35297. We find the address of const tensor will change in some cases, but the data buffer of const tensor is always the same. So caching tensor's buffer instead of its address is a safer way.\r\n", "comments": []}, {"number": 35838, "title": "fix variable name in log", "body": "should log the output type instead of the input type.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35838) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n>  **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n>  **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35838) for more info**.\r\n\r\n@googlebot  I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35838) for more info**.\n\n<!-- ok -->"]}, {"number": 35837, "title": "TF2.0 Error checkpointing custom map", "body": "I am subclassing a keras model with custom layers. Each layer wraps a dictionary of parameters that is used when generating they layers. It seems these param dictionaries are not set before the training checkpoint is made in Tensorflow, they are set after, which causes an error. I am not sure how to fix this, as the `ValueError` being raised also gives outdated information (`tf.contrib` no longer exists). \r\n\r\n> ValueError: Unable to save the object {'units': 32, 'activation':\r\n> 'tanh', 'recurrent_initializer': 'glorot_uniform', 'dropout': 0,\r\n> 'return_sequences': True} (a dictionary wrapper constructed\r\n> automatically on attribute assignment). The wrapped dictionary was\r\n> modified outside the wrapper (its final value was {'units': 32,\r\n> 'activation': 'tanh', 'recurrent_initializer': 'glorot_uniform',\r\n> 'dropout': 0, 'return_sequences': True}, its value when a checkpoint\r\n> dependency was added was None), which breaks restoration on object\r\n> creation.\r\n> \r\n> If you don't need this dictionary checkpointed, wrap it in a\r\n> tf.contrib.checkpoint.NoDependency object; it will be automatically\r\n> un-wrapped and subsequently ignored.\r\n\r\n\r\nHere's an example of the Layer that is throwing this issue:\r\n\r\n\r\n    class RecurrentConfig(BaseLayer):\r\n        '''Basic configurable recurrent layer'''\r\n        def __init__(self, params: Dict[Any, Any], mode: ModeKeys, layer_name: str = '', **kwargs):\r\n            self.layer_name = layer_name\r\n            self.cell_name = params.pop('cell', 'GRU')\r\n            self.num_layers = params.pop('num_layers', 1)\r\n            kwargs['name'] = layer_name\r\n            super().__init__(params, mode, **kwargs)\r\n            if layer_name == '':\r\n                self.layer_name = self.cell_name\r\n            self.layers: List[layers.Layer] = stack_layers(self.params,\r\n                                                           self.num_layers,\r\n                                                           self.cell_name)\r\n    \r\n        def call(self, inputs: np.ndarray) -> layers.Layer:\r\n            '''This function is a sequential/functional call to this layers logic\r\n            Args:\r\n                inputs: Array to be processed within this layer\r\n            Returns:\r\n                inputs processed through this layer'''\r\n            processed = inputs\r\n            for layer in self.layers:\r\n                processed = layer(processed)\r\n            return processed\r\n    \r\n        @staticmethod\r\n        def default_params() -> Dict[Any, Any]:\r\n            return{\r\n                'units': 32,\r\n                'recurrent_initializer': 'glorot_uniform',\r\n                'dropout': 0,\r\n                'recurrent_dropout': 0,\r\n                'activation': 'tanh',\r\n                'return_sequences': True\r\n            }\r\n\r\nBaseLayer.py\r\n\r\n    '''Basic ABC for a keras style layer'''\r\n    \r\n    from typing import Dict, Any\r\n    \r\n    from tensorflow.keras import layers\r\n    from mosaix_py.mosaix_learn.configurable import Configurable\r\n    \r\n    class BaseLayer(Configurable, layers.Layer):\r\n        '''Base configurable Keras layer'''\r\n        def get_config(self) -> Dict[str, Any]:\r\n            '''Return configuration dictionary as part of keras serialization'''\r\n            config = super().get_config()\r\n            config.update(self.params)\r\n            return config\r\n    \r\n        @staticmethod\r\n        def default_params() -> Dict[Any, Any]:\r\n            raise NotImplementedError('Layer does not implement default params')\r\n\r\n", "comments": ["@Jbiloki \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nRequest you to provide simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Jbiloki \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram Yes, I solved the problem here https://stackoverflow.com/questions/59725989/tensorflow-checkpoint-custom-map"]}, {"number": 35836, "title": "Update BinaryAccuracy for assert", "body": "fixes #35490 \r\nRaises value error if `y_true` and `y_pred` have different shapes for computing BinaryAccuracy metric.", "comments": ["Thank you! This fix is incorrect as there are use cases where the shapes can be different but we can squeeze or expand dimensions to match the shapes. The right fix should be in precision/recall. I have that fix out already, will be submitted in the next day or two."]}, {"number": 35835, "title": "model.predict leaks memory", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. We are using our own custom layer to build a keras model.\r\n\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac & Ubuntu 16.04. Python 3.7 \r\n- TensorFlow installed from (source or binary):\r\nUsing the stable version of tensorflow 2.0.0, but tf 2.1 appears to exhibit the same issue\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version:\r\n3.7.6\r\n\r\n\r\n**Describe the current behavior**\r\nWhen calling model.predict in a loop memory grows to many gigs. Below is a very simple example of what we're doing that experiences severe memory growth. Granted there are many layers involved, some custom. Each call to predict is growing in memory size.\r\n\r\n```\r\nmodel = tf.keras.Model(sent_input, preds)\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\r\nmodel.load_weights(...)\r\nwhile True:\r\n    inputs = prepare_inputs(x)\r\n    model.predict(inputs)\r\n```\r\n\r\nI have seen some other reports that suggest predict_on_batch might be used, but this also exhibits the same issue.\r\n\r\n\r\n**Describe the expected behavior**\r\nI would expect to be able to call predict as many times as necessary and for memory consumption to remain mostly constant.\r\n\r\n**Other info / logs**\r\nIn my debugging I was able to find this using pympler. I was able to attach an image of what I believe is the culprit  object being leaked, but I could be mistaken on this. It looks like EagerTensors are being leaked. It appears the TimeDistributedLayer is keeping references to them.\r\n\r\n```\r\n                                                 types |   # objects |   total size\r\n====================================================== | =========== | ============\r\n                                                  dict |          20 |      2.07 KB\r\n    tensorflow.python.framework.tensor_shape.Dimension |          13 |    832     B\r\n           tensorflow.python.framework.ops.EagerTensor |           3 |    528     B\r\n                                                  list |           5 |    520     B\r\n  tensorflow.python.framework.tensor_shape.TensorShape |           5 |    320     B\r\n```\r\n\r\n![objgraph-qwasabxu](https://user-images.githubusercontent.com/5382738/72302369-73a86500-362f-11ea-9044-93cd2b1605e2.png)\r\n\r\n", "comments": ["The code change suggested here seems to resolve the issue, though I am not sure this is the correct change. https://github.com/tensorflow/tensorflow/issues/33178#issuecomment-549079302\r\n\r\nI do not see this change available in any of the pip packages available. Though it sounds like it may be available on a nightly branch somewhere? \r\n\r\nI noticed that 2.1 has a leak still, so i'm not sure if the change has been pulled in yet?", "The PR for the mentioned issue is this one :  #33441 ", "@weihnachten033 @sspohrer this PR has been merged 2 days ago, so it should be availabe in the latest tf-nightly. It is not in TF 2.1.0 and will be available in the next TF release. ", "@sspohrer were you able to test with latest tf-nightly ?", "@sspohrer could you please respond to the above comment", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@sspohrer \r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 35834, "title": "[ROCm] Fix the ROCm CSB breakage - 200113 - 2", "body": "The following commit breaks following tests in the ROCm nightly CSB testing\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/e57c8e87f5203fa6ec49338e3ed639adb0e14c03\r\n\r\n```\r\n//tensorflow/python/debug:check_numerics_callback_test_gpu\r\n//tensorflow/python/debug:debug_v2_ops_test_gpu\r\n//tensorflow/python/distribute:ctl_correctness_test_gpu\r\n//tensorflow/python/distribute:custom_training_loop_test_gpu\r\n//tensorflow/python/eager:def_function_test_gpu\r\n//tensorflow/python/eager:forwardprop_test_gpu\r\n//tensorflow/python/eager:function_test_gpu\r\n//tensorflow/python:loss_scaling_gradient_tape_test_gpu\r\n//tensorflow/python:op_callbacks_test_gpu\r\n```\r\n\r\nThey all fail with the following error\r\n\r\n```\r\n...\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/op_callbacks_test_gpu.runfiles/org_tensorflow/tensorflow/python/autograph/pyct/loader.py\", line 50, in load_source\r\n    spec.loader.exec_module(module)\r\n  File \"<frozen importlib._bootstrap_external>\", line 661, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 767, in get_code\r\n  File \"<frozen importlib._bootstrap_external>\", line 727, in source_to_code\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/tmp/tmp4r6l4f6a.py\", line 19\r\n    () = loop_vars\r\n```\r\n\r\nThe fix is to properly handle the case when `loop_vars` is `None` (which was inadvertently? removed by the breakign commit)\r\n\r\n\r\n------------\r\n\r\n\r\n/cc @chsigg @cheshire @whchung ", "comments": ["As a side note, `() = a` is valid in Python 3.6 and newer, but breaks in 3.5. So what seemed like an unnecessary check seems to be still required."]}, {"number": 35833, "title": "Keras Estimator fails on regression task while underlying model works", "body": "**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: Same issue using tf2.0.0-beta1-cpu\r\n and tf1.14.0-gpu\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 10.1.168/cuDNN 7.6.2\r\n- GPU model and memory: NVIDIA GeForce RTX 2060, 6GB dedicated memory\r\n\r\n**Describe the current behavior**\r\nA convolutional reggression (last layer has linear activation and one neuron) network built with tf.keras is shown to fit the MNIST dataset (I know that MNIST is a classification task; this is an example) when converted to a dataset.\r\n\r\nWhen the same model is packaged into an estimator using  `tf.keras.estimator.model_to_estimator` no error messages occur, however the model no longer fits. The loss does not decrease.\r\n\r\nI had made a Stackoverflow question about this (https://stackoverflow.com/q/59631744/9988487) with no traction whatsoever. After some more trying to get it to work, I believe it is a bug.\r\n\r\n**Describe the expected behavior**\r\nThe keras estimator should have the same behaviour as the underlying model. Change the USE_ESTIMATOR variable to see that the underlying model works.\r\n**Code to reproduce the issue**\r\n```\r\n# python 3.6. Tested with tensorflow-gpu-1.14 and tensorflow-cpu-2.0\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef get_model(IM_WIDTH=28, num_color_channels=1):\r\n    \"\"\"Create a very simple convolutional neural network using a tf.keras Functional Model.\"\"\"\r\n    input = tf.keras.Input(shape=(IM_WIDTH, IM_WIDTH, num_color_channels))\r\n    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(input)\r\n    x = tf.keras.layers.MaxPooling2D(3)(x)\r\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\r\n    x = tf.keras.layers.MaxPooling2D(3)(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(64, activation='relu')(x)\r\n    output = tf.keras.layers.Dense(1, activation='linear')(x)\r\n    model = tf.keras.Model(inputs=[input], outputs=[output])\r\n    model.compile(optimizer='adam', loss=\"mae\",\r\n                  metrics=['mae'])\r\n    model.summary()\r\n    return model\r\n\r\n\r\ndef input_fun(train=True):\r\n    \"\"\"Load MNIST and return the training or test set as a tf.data.Dataset; Valid input function for tf.estimator\"\"\"\r\n    (train_images, train_labels), (eval_images, eval_labels) = tf.keras.datasets.mnist.load_data()\r\n    train_images = train_images.reshape((60_000, 28, 28, 1)).astype(np.float32) / 255.\r\n    eval_images = eval_images.reshape((10_000, 28, 28, 1)).astype(np.float32) / 255.\r\n    # train_labels = train_labels.astype(np.float32)  # these two lines don't affect behaviour.\r\n    # eval_labels = eval_labels.astype(np.float32)\r\n    # For a neural network with one neuron in the final layer, it doesn't seem to matter if target data is float or int.\r\n\r\n    if train:\r\n        dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n        dataset = dataset.shuffle(buffer_size=100).repeat(None).batch(32).prefetch(1)\r\n    else:\r\n        dataset = tf.data.Dataset.from_tensor_slices((eval_images, eval_labels))\r\n        dataset = dataset.batch(32).prefetch(1)  # note: prefetching does not affect behaviour\r\n\r\n    return dataset\r\n\r\n\r\nmodel = get_model()\r\ntrain_input_fn = lambda: input_fun(train=True)\r\neval_input_fn = lambda: input_fun(train=False)\r\n\r\nNUM_EPOCHS, STEPS_PER_EPOCH = 4, 1875  # 1875 = number_of_train_images(=60.000)  /  batch_size(=32)\r\nUSE_ESTIMATOR = False  # change this to compare model/estimator. Estimator performs much worse for no apparent reason\r\nif USE_ESTIMATOR:\r\n    estimator = tf.keras.estimator.model_to_estimator(\r\n        keras_model=model, model_dir=\"model_directory\",\r\n        config=tf.estimator.RunConfig(save_checkpoints_steps=200, save_summary_steps=200))\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=0)\r\n\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n    print(\"Training complete. Evaluating Estimator:\")\r\n    print(estimator.evaluate(eval_input_fn))\r\n    # final train loss with estimator: ~2.5 (mean abs. error).\r\nelse:\r\n    dataset = train_input_fn()\r\n    model.fit(dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=NUM_EPOCHS)\r\n    print(\"Training complete. Evaluating Keras model:\")\r\n    print(model.evaluate(eval_input_fn()))\r\n    # final train loss with Keras model: ~0.4 (mean abs. error).\r\n```\r\n", "comments": ["@adomasbaliuka,\r\nI tried to reproduce the issue and did not observe much difference in the mean absolute error. Please check the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/1c5ca231d078e9cba55269e54541cd5a/35833.ipynb).", "@amahendrakar \r\nI confirm that there is not much difference when using the gist you provided (which uses tensorflow v2.1). The most likely explenation is that this issue is solved in version 2.1. \r\n\r\nHave you tried to reproduce the issue with tf-1.x ? It would be nice to know if the issue lies with older versions of tensorflow or with the specific setup on my machine.", "Was able to reproduce the issue with TF 1.15. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/c44ac3fa99c62342ec0546eaa5ee65fc/35833_1-15.ipynb). Thanks!", "@adomasbaliuka I agree with you that there is a significant difference between results from the model and the estimator if we use `TF1.15`. I think there may not be any more updates to `TF1.15` branch. If there is any security related issues, then only there will be updates to `TF1.15` branch.\r\n\r\nI ran your code with `tf-nightly`. I don't see any significant difference between output from model and estimator.\r\n\r\nThe following is the output from the model (USE_ESTIMATOR = False)\r\n```\r\nTraining complete. Evaluating Keras model:\r\n313/313 [==============================] - 2s 7ms/step - loss: 0.4018 - mae: 0.4021\r\n[0.4018059968948364, 0.4020615816116333]\r\n```\r\n\r\nThe following is the output from the estimator (USE_ESTIMATOR = True)\r\n```\r\nTraining complete. Evaluating Estimator:\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Starting evaluation at 2020-03-18T23:15:15Z\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Restoring parameters from model_directory/model.ckpt-7500\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Inference Time : 2.14818s\r\nINFO:tensorflow:Finished evaluation at 2020-03-18-23:15:17\r\nINFO:tensorflow:Saving dict for global step 7500: global_step = 7500, loss = 0.39566746, mae = 0.39566746\r\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 7500: model_directory/model.ckpt-7500\r\n{'loss': 0.39566746, 'mae': 0.39566746, 'global_step': 7500}\r\n```\r\n\r\nPlease close the issue as this was resolved in the recent `TF2.x` versions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35833\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35833\">No</a>\n"]}, {"number": 35832, "title": "[ROCm] Fix for the ROCm CSB breakage - 200113 - 1", "body": "The ROCm CSB was broken by the following commit :\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/880cad85987e8948774f9bae24b1420074534f00\r\n\r\nThe commit leads to linker errors like below :\r\n\r\n```\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/serialize_sparse_op/serialize_sparse_op.o:\r\nserialize_sparse_op.cc:function tensorflow::(anonymous namespace)::SerializeManySparseOp<tensorflow::tstring, tensorflow::Variant>::Compute(tensorflow::OpKernelContext*): error:\r\nundefined reference to 'tensorflow::DataTypeToEnum<tensorflow::tstring>::value'  \r\n```\r\n\r\nThe breakage seems to be something that should affect all platforms (not just ROCm)\r\n\r\nThe cause of the error is that the static member varibale `DataTypeToEnum<T>::value` is declared,\r\nbut not defined in the file `tensorflow/core/framework/types.h` file.  Since it si a `constexpr` value, most of it's uses are optimized by the time we get to the linker.\r\n\r\nHowever any use of it that survives to the linking stage, for e.g. use as a \"reference\",  will lead to linker errors. The commit above seems to introduce two such uses.\r\n\r\n\r\nThe first commit in this PR works around the linker error, by creating a local variable, to avoid the use of `DataTypeToEnum<T>::value` as a reference.\r\n\r\nThe second commit adds the explicit definition of `DataTypeToEnum<T>::value` in  `types.cc` file\r\n\r\nBoth commits are independent of each other and will fix the breakage. You can choose to pick either or both, and I can rebase the PR accordingly.\r\n\r\nthanks\r\n\r\n-----------------------------------\r\n\r\n/cc @chsigg @cheshire @whchung \r\n", "comments": ["/cc @jerryyin ", "I prefer just the change to `types.cc` file, but am OK with the fix in both places.", "Thanks a lot for that fix. I've been trying for two days to understand the problem. Compiling TF with Bazel every time you do a small change isn't very optimal to find a problem at the linking stage.\r\n\r\nCPU: AMD Ryzen Threadripper 2950X 16-Core Processor\r\nGPUs: \r\n        1. GeForce RTX 2080\r\n        2. GeForce GTX 960\r\n\r\nCUDA: 10.2\r\nNvidia Linux drivers: 440.44"]}, {"number": 35830, "title": "Why error occurs but programme is not affacted.", "body": "I use python to train a model and export the model. Then I use c++ to import the exported model and use it to predict data. However, during the prediction process, error message is printed during the screen, but the programme run normally with no exception. Below is the error message.\r\n\r\n`2020-01-14 03:29:49.794822: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:51.689021: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n2020-01-14 03:29:51.689069: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\nInvalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:51.702364: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:53.728503: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:55.683826: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:55.686735: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n2020-01-14 03:29:55.686879: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n2020-01-14 03:29:55.686918: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\n2020-01-14 03:29:55.687032: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n2020-01-14 03:29:55.687120: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]\r\nError running session: Invalid argument: Name: <unknown>, Feature: 100 (data type: float) is required but could not be found.\r\n         [[{{node ParseExample/ParseExample}}]]`\r\n\r\nBelow is the code that use the model in c++.\r\n`float NodeEVEngine::getmlev(int curturn,vector<float> &inputfeature,bool allin,bool turnover){\r\n//    MyTimer::GetInstance()->start(\"prepare\");\r\n    Example example;\r\n    Features features;\r\n    int featurelen = inputfeature.size();\r\n    for (int i = 0; i < featurelen; i++) {\r\n        FloatList floatList;\r\n        floatList.add_value(inputfeature[i]);\r\n        Feature feature;\r\n        *feature.mutable_float_list() = floatList;\r\n        features.mutable_feature()->operator[](to_string(i)) = feature;\r\n    }\r\n    *example.mutable_features()=features;\r\n    std::string test_str;\r\n    example.SerializeToString(&test_str);\r\n    tensorflow::Input input({test_str});\r\n    vector<string> outputname({\"dnn/logits/BiasAdd:0\"});\r\n    std::vector<Tensor> outputs;\r\n    Status run_status;\r\n//    MyTimer::GetInstance()->stopandprintandclear(\"prepare\");\r\n//    MyTimer::GetInstance()->start(\"mlev\");\r\n    if (allin)run_status = m_allinbundle[curturn-PREFLOPNO]->session->Run({{\"input_example_tensor\",input.tensor()}}, outputname, {}, &outputs);\r\n    else if (turnover)run_status = m_turnoverbundle[curturn-PREFLOPNO]->session->Run({{\"input_example_tensor\",input.tensor()}}, outputname, {}, &outputs);\r\n    else run_status = m_bundle[curturn-PREFLOPNO]->session->Run({{\"input_example_tensor\",input.tensor()}}, outputname, {}, &outputs);\r\n    if (!run_status.ok()) {\r\n        cout << \"Error running session: \" << run_status << std::endl;\r\n        return -1;\r\n    }\r\n//    MyTimer::GetInstance()->stopandprintandclear(\"mlev\");\r\n//    PrintVector(inputfeature,\"inputfeature:\");\r\n    return outputs[0].tensor<float,2>()(0,0);\r\n}`", "comments": []}, {"number": 35829, "title": "Corrected the tf.linalg.diag_part examples", "body": "This fixes #35760.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35829) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35829) for more info**.\n\n<!-- ok -->"]}, {"number": 35828, "title": "tf.contrib.cudnn_rnn.CudnnGRU runtime error: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab\r\n- TensorFlow version (use command below):\r\n1.15.0\r\n- Python version:\r\n3.6.9\r\n- CUDA/cuDNN version:\r\nCuda compilation tools, release 10.0, V10.0.130\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI am trying to convert my CPU GRU code to a CudnnGRU implementation. When I run the code [here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9), my script runs for a seemingly random of number of training epochs (normally between 0 and 20) before the session crashes, often with a CUDA_ERROR_ILLEGAL_ADDRESS error. I see similar behaviour when I run the script on my institution's hardware (TF version 1.14, CUDA Version: 10.1).\r\n\r\n**Code to reproduce the issue**\r\nSee Google colab sheet [here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9). I select the GPU as the hardware accelerator in the notebook settings.\r\n\r\n**Other info / logs**\r\n```\r\nLearning Rate: 0.001 \r\nTotal number of portions: 10 \r\nbatch_length: 500 \r\nn_mini_batches 25 \r\nmini_batch_length 20 \r\nnumber of MEG channels/PCs: 5 \r\n\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:342: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:345: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nAlpha coefficients will be soft-plus transformed\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py:193: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\r\n\r\nBeginning training...\r\n0\r\n1\r\n2\r\n3\r\n4\r\n```\r\n\r\nPlease see the colab-jupyter.log file attached.\r\n\r\nApologies for my ignorance and thank you in advance for any kind help.\r\n[colab-jupyter.log](https://github.com/tensorflow/tensorflow/files/4055241/colab-jupyter.log)\r\n", "comments": ["Same here! \r\nI have tried tf2.1.\r\n\r\nThese are my logs\r\n``\r\nJan 14, 2020, 11:37:46 AM | WARNING | WARNING:root:kernel 6f0febf6-6ed2-4820-a9ca-0deef6ba6b6d restarted\r\n-- | -- | --\r\nJan 14, 2020, 11:37:46 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nJan 14, 2020, 11:37:45 AM | WARNING | 2020-01-14 10:37:45.132229: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nJan 14, 2020, 11:37:45 AM | WARNING | 2020-01-14 10:37:45.132148: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\nJan 14, 2020, 11:37:34 AM | WARNING | 2020-01-14 10:37:34.845228: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\r\nJan 14, 2020, 11:37:30 AM | WARNING | 2020-01-14 10:37:30.658416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nJan 14, 2020, 11:37:30 AM | WARNING | 2020-01-14 10:37:30.658401: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\nJan 14, 2020, 11:37:30 AM | WARNING | 2020-01-14 10:37:30.658283: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\nJan 14, 2020, 11:35:55 AM | WARNING | WARNING:root:kernel 6f0febf6-6ed2-4820-a9ca-0deef6ba6b6d restarted\r\nJan 14, 2020, 11:35:55 AM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\n``", "@RCTimms Sorry for late response. \r\nI can reproduce the issue with `TF1.15.3`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/0379988c1c214a22a1233d3445e8025c/debug_code_nbatches.ipynb) is the gist. \r\n\r\nHave you tried to implement this with `TF2.x`? I am not sure what is the root-cause but there were lot of performance improvements with `TF2.x` that might help you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35828\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35828\">No</a>\n"]}, {"number": 35827, "title": "NullPointerException when trying to access Classifier.Recognition / result.getLocation()", "body": "**Setup:** \r\nI've used Google's Cloud AutoML Vision Object Detection platform to create a custom model for object recognition using this tutorial:\r\n\r\nhttps://cloud.google.com/vision/automl/object-detection/docs/edge-quickstart\r\n\r\nThen added the model to the following app template:\r\n\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\n\r\nThen I overcame the following two errors: \r\n\r\n1) `Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.` I modified TF_OD_API_INPUT_SIZE accordingly. \r\n\r\n2) ` tflite ml google [1, 20, 4] and a Java object with shape [1, 10, 4]. ` I modified NUM_DETECTIONS according to my custom model.\r\n\r\nFinally got the model working. \r\n\r\nNow I want to send the box locations using Bluetooth to a HC-05. I added two public classes: BluetoothArduinoBridge, and LocationSender. Both of them attached as txt files.\r\n\r\n**Problem**\r\nApp crashes when objects are recognized. \r\n\r\n**Code to reproduce the issue**\r\nI modified the code at DetectorActivity.java, lines 203-218 to this: \r\n\r\n```\r\n            for (final Classifier.Recognition result : results) {\r\n              final RectF location = result.getLocation();\r\n              if (location != null && result.getConfidence() >= minimumConfidence) {\r\n                canvas.drawRect(location, paint);\r\n\r\n                cropToFrameTransform.mapRect(location);\r\n\r\n                result.setLocation(location);\r\n                mappedRecognitions.add(result);\r\n\r\n                //----------------------------------------\r\n                mLocationSender.send_location((location.bottom),(location.left),(location.right)\r\n                ,(location.top));\r\n                //----------------------------------------\r\n              }\r\n            }\r\n\r\n```\r\n**Other info / logs**\r\n```\r\nE/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 16690\r\n    java.lang.NullPointerException: Attempt to invoke virtual method 'void org.tensorflow.lite.examples.detection.LocationSender.send_location(float, float, float, float)' on a null object reference\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:213)\r\n```\r\n\r\n**Question**\r\nHow can I send the box locations using mLocationSender.send_location???\r\n\r\n**Note**\r\nI'm quite new to Android so this might probably be a dumb question. Please be gentle. \r\n[LocationSender.txt](https://github.com/tensorflow/tensorflow/files/4055214/LocationSender.txt)\r\n[BluetoothArduinoBridge.txt](https://github.com/tensorflow/tensorflow/files/4055215/BluetoothArduinoBridge.txt)\r\n", "comments": ["Hi dfvr1994, \r\n\r\nIt seems that mLocationSender is null in your modified code, you should check when it is assigned a new object.\r\n\r\nI am glad to hear that you overcame the adaption to your custom model.\r\n\r\nAs this is not a bug from original code and for https://github.com/tensorflow/examples in particular, I will close this issue."]}, {"number": 35826, "title": "Bug in Transfer learning + Distributed Training", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.0\r\n- **Python version**: 3.7\r\n- **CUDA/cuDNN version**: cuda 10.0, cudnn 7\r\n- **GPU model and memory**: Quadro P6000, 24GB\r\n\r\n### Describe the problem\r\nUnder the distributed environment, if the model is updated, the trainable_weights of the model is not updated in the distributed training loop. Please see following code for reproducing the bug.\r\n\r\nI first created 2 Conv2D layers. I created the first model using only 1 Conv2D layer, and it works fine. Then, I update the mode to create a 2-Conv2D model, then there's the bug. Outside the training loop, there are trainable_weights (2 kernel + 2 bias), but inside the training loop, there's only 2 trainable_weights\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ntf.config.set_soft_device_placement(True)\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n  tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n# Begin\r\nx_in = np.random.randn(2, 64, 64, 3).astype(np.float32)\r\ngt = np.random.randn(2, 64, 64, 3).astype(np.float32)\r\n\r\nlayer1 = keras.layers.Conv2D(\r\n        input_shape=(None, None, None, 3), filters=3,\r\n        kernel_size=3, strides=1, padding='same',\r\n        name='conv1')\r\nlayer2 = keras.layers.Conv2D(\r\n        input_shape=(None, None, None, 3), filters=3,\r\n        kernel_size=3, strides=1, padding='same',\r\n        name='conv2')\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n@tf.function\r\ndef train():\r\n  def train_step():\r\n    with tf.GradientTape() as tape:\r\n      loss = tf.reduce_mean((model(x_in) - gt) ** 2)\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    tf.print(\"Length of trainable_weights: \", len(model.trainable_weights), \"Length of grads: \", len(grads))\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n  strategy.experimental_run_v2(train_step)\r\n\r\nprint('------ First model ------')\r\nwith strategy.scope():\r\n  x = keras.Input((64, 64, 3))\r\n  model = keras.Model(inputs=x, outputs=layer1(x))\r\n  optimizer = keras.optimizers.Adam(0.1, amsgrad=True)\r\nprint(\"Length of trainable_weights: \", len(model.trainable_weights))\r\nprint(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())\r\n# Print: \r\n# Length of trainable_weights:  2\r\n# conv1/bias:0 [0. 0. 0.]\r\n\r\nfor i in range(2):\r\n  train()\r\n# Print:\r\n# Length of trainable_weights:  2 Length of grads:  2\r\n# Length of trainable_weights:  2 Length of grads:  2\r\n\r\nprint(\"Length of trainable_weights: \", len(model.trainable_weights))\r\nprint(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())\r\n# Print:\r\n# Length of trainable_weights:  2\r\n# conv1/bias:0 [0.03003622 0.03190297 0.02856238]\r\n\r\n\r\nprint('------ Change model ------')\r\nwith strategy.scope():\r\n  x = keras.Input((64, 64, 3))\r\n  model = keras.Model(inputs=x, outputs=layer2(layer1(x)))\r\nprint(\"Length of trainable_weights: \", len(model.trainable_weights))\r\nprint(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())\r\n# Print: \r\n# Length of trainable_weights:  4\r\n# conv2/bias:0 [0. 0. 0.]\r\n\r\nfor i in range(2):\r\n  train()\r\n# Print:\r\n# Length of trainable_weights:  2 Length of grads:  2\r\n# Length of trainable_weights:  2 Length of grads:  2\r\n\r\nprint(\"Length of trainable_weights: \", len(model.trainable_weights))\r\nprint(model.trainable_weights[-1].values[0].name, model.trainable_weights[-1].values[0].numpy())\r\n# Print: \r\n# Length of trainable_weights:  4\r\n# conv2/bias:0 [0. 0. 0.]\r\n\r\n\r\n\r\n```\r\n", "comments": ["Hi, [@tf.function](https://www.tensorflow.org/api_docs/python/tf/function) decorator compiles a function into a callable TensorFlow graph, and in your case the second training loop is still using the graph compiled from the first model. This is because the \"model\" parameter captured in your train() function  refers to the first model when the graph compilation happens, and TensorFlow never compiles a new graph in your case.\r\n\r\nTo compile a new graph, you can either\r\n\r\n1. Remove the @tf.function decorator of `train()` (slower execution https://www.tensorflow.org/guide/function)\r\n2. Force TensorFlow to recompile a new graph by redefining a train function with @tf.function decorator, e.g:\r\n   ```python\r\n   def train():\r\n     ...\r\n\r\n   # define the first model\r\n   ...\r\n   @tf.function\r\n   def train_first_model():\r\n      train()\r\n\r\n   train_first_model()\r\n\r\n   # define the second model\r\n   ...\r\n   @tf.function\r\n   def train_second_model():\r\n     train()\r\n   \r\n   train_second_model()\r\n   ```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35826\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35826\">No</a>\n"]}, {"number": 35825, "title": "Extend `tf.keras.Model.evaluate` with `class_weight`", "body": "**System information**\r\n- TensorFlow version (you are using): \r\nTF 2.0.0\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n**Describe the feature and the current behaviour/state.**\r\nCurrently `tf.keras.models.Model.fit` method allows the user to pass either 'sample_weight' and 'class_weight' parameters. These are used to compute at [some point](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training.py#L2527 ) a standardised 'sample_weights' and used later on while calculating the loss.\r\n\r\nThis feature request is about extending the 'tf.keras.Model.evaluate' API so that is permits using `class_weight` directly. The `evaluate` function already permits for `sample_weight`.\r\n\r\n\r\n**Will this change the current api? How?**\r\ncurrent API\r\n```\r\nevaluate(\r\n    x=None,\r\n    y=None,\r\n    batch_size=None,\r\n    verbose=1,\r\n    sample_weight=None,\r\n    steps=None,\r\n    callbacks=None,\r\n    max_queue_size=10,\r\n    workers=1,\r\n    use_multiprocessing=False\r\n)\r\n```\r\nnew API\r\n```\r\nevaluate(\r\n    x=None,\r\n    y=None,\r\n    batch_size=None,\r\n    verbose=1,\r\n    sample_weight=None,\r\n>>>    class_weight=None,\r\n    steps=None,\r\n    callbacks=None,\r\n    max_queue_size=10,\r\n    workers=1,\r\n    use_multiprocessing=False\r\n)\r\n```\r\n**Who will benefit with this feature?**\r\n\r\nThose users of the API who would like to perform the evaluation of a model that was trained with bespoke class weights.\r\n\r\n**Any Other info.**\r\n", "comments": ["Since this issue is opened for a long time, let me work on this.", "Is there any solution please to add class_weight to the evaluate() method?", "Sorry for the long wait. I checked with Francois about the issue, and seems that it is expected to not having class_weights for model.evaluate.\r\n\r\nQuote from Francois:\r\n\r\n\"The reason we don't support class weights in evaluate is that the class_weight argument represents sample weights that are computed from the labels, but the labels should not be an input to the model during evaluation. During training this is fine, but during evaluation this represents a data leak from the labels to your metrics. If you used class weighting in evaluate, your score would not be reproducible on real test data (when you don't have the labels).\r\n\r\nSo this is conceptually wrong.\"\r\n\r\nI am closing this issue since it is working as intended."]}, {"number": 35824, "title": "TimeDistributed Layer Does Not Support Multiple Inputs", "body": "Python Version: 3.76\r\nTensorFlow Version: 2.1\r\nOS: Windows 10\r\n\r\nIssue:\r\nIt does not seem the TimeDistributed layer supports multiple inputs. See example code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Concatenate, RepeatVector, Activation, Dot, Bidirectional, Embedding, Input, SpatialDropout1D, LSTM, Dropout, Lambda, Conv1D, Attention, AdditiveAttention, GlobalAveragePooling1D, TimeDistributed, AveragePooling1D\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\ndef example_2():\r\n    # Encode each timestep\r\n    input_1 = Input(shape=(None,), dtype='int64', name=\"Input1\")\r\n    input_2 = Input(shape=(None,), dtype='int64', name=\"Input2\")\r\n\r\n    output = Concatenate([input_1, input_2])\r\n    output = TimeDistributed(output)([input_1, input_2])\r\n\r\n    model = Model([input_1, input_2], output)\r\n    model.compile(loss='categorical_crossentropy',\r\n                  optimizer='rmsprop',\r\n                  metrics=['accuracy'])\r\n\r\n    return model\r\n\r\n\r\ninput_1 = np.array([[1, 2, 3, 4, 5, 6, 7]])\r\ninput_2 = np.array([[1, 1, 1, 1, 1, 1, 1]])\r\ny = np.array([1, 0, 1, 1, 0, 0, 1])\r\n\r\nexample_2().fit(input=[input_1, input_2], output=y)\r\n```\r\nI get the following issue:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py\", line 26, in <module>\r\n    example_2().fit(input=[input_1, input_2], output=y)\r\n  File \"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py\", line 12, in example_2\r\n    output = TimeDistributed(output)([input_1, input_2])\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 748, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 2116, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py\", line 197, in build\r\n    input_shape = tensor_shape.TensorShape(input_shape).as_list()\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 771, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 771, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 716, in as_dimension\r\n    return Dimension(value)\r\n  File \"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\", line 200, in __init__\r\n    None)\r\n  File \"<string>\", line 3, in raise_from\r\nTypeError: Dimension value must be integer or None or have an __index__ method, got TensorShape([None, None])\r\n```\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Issue replicating for given code in 2.1 and [tf-nightly](https://colab.sandbox.google.com/gist/oanush/5d1b14800f5f161a4de952e93c7e6482/35824.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35824\">No</a>\n"]}]