[{"number": 36038, "title": "Showing index instead of labels in word embedding projector", "body": "For large embedding words, I can't show the word itself but the index instead appear. And so, I can't estimate the performance of the word embedding. So, I think words should appear instead of index so, how I can benefit from it?\r\n\r\n**System information**\r\n- http://projector.tensorflow.org/\r\n\r\nYou can add an option so, I can choose either index or words.\r\n\r\nSorry for my bad English.\r\n", "comments": ["@zynabsmaan Where are you seeing index? Can you please share a screenshot or please give more details. I also think this is not a bug in TF. Can you please post it in Stackoverflow where there is a large community to help. We will also answer there that are not support type questions.\r\n\r\n<img width=\"797\" alt=\"Screen Shot 2020-06-03 at 12 19 30 PM\" src=\"https://user-images.githubusercontent.com/46058173/83679863-f7d69700-a594-11ea-907f-56d838957645.png\">\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36037, "title": "Fix issue with ReLU layer and integer dtype", "body": "\r\nThis fix tries to address the issue raised in #35430 where\r\nReLU layer + integer dtype causes conversion error.\r\nThis PR fixes the isse by replacing float `0.` with `0`.\r\n\r\nThis fix fixes #35430.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Can you please resolve conflicts? Thanks!", "Thanks @qlzh727 @gbaned, the PR has been rebased with conflict resolved. Please take a look.", "Thanks @qlzh727 for the review. The PR has been updated with test case moved to `backend_test.py/test_relu()`. Please take a look and let me know if there are any issues."]}, {"number": 36036, "title": "Allow CuDNN 7.2 to convert float -> fp16 for tensor core ops", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow using cuDNN 7.2 and above\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNow, if users want, cuDNN can convert fp32 inputs to fp16 to allow using tensor cores even when inputs are fp32. Before, users had to explicitly convert fp32 to fp16 using AMP. [Link](https://devblogs.nvidia.com/tensor-ops-made-easier-in-cudnn/). This is as simple as setting `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION` enum value and passing it to `cudnnSetConvolutionMathType()` functioin.\r\n\r\n```cpp\r\ncudnnSetConvolutionMathType(cudnnConvDesc, CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION)\r\n```\r\n\r\nand this is the counterpart for RNN:\r\n\r\n```cpp\r\ncudnnSetRNNMatrixMathType(cudnnRnnDesc, CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION)\r\n```\r\n\r\n\r\nWe should let users chose if they want this option, since it involves converting fp32 to fp16.", "comments": ["ping!\r\n\r\nThis would allow 5 ~ 10x speedup for most convolutions.\r\n\r\n@chsigg What do you think?", "CC @reedwm \r\n\r\nWe (TensorFlow team) don't have cycles to actually implement this ourselves, but please feel free to create a PR.", "This probably would have been a good idea, but I don't think it's worth doing anymore since [TensorFloat-32](https://github.com/tensorflow/community/pull/247) is superior. TF32 has the advantage in that there's no risk of underflow or overflow compared to fp32, and gradients tend to underflow on the backwards pass in fp16 unless loss scaling is used. Therefore I'm closing this issue.", "Any chance we can reopen this request considering TF32 is only supported by Ampere, where as the `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION` is supported by Volta and Turing?", "If it can help the performance on non-Ampere GPUs, it seems worth awhile. Would love to contribute!", "If we do this, we should unify the API with the TF32 API (or have a good reason for not unifying) https://github.com/tensorflow/community/pull/247.", "> If it can help the performance on non-Ampere GPUs, it seems worth awhile. Would love to contribute!\r\n\r\nDo you have a fork/branch where you have tested this? If so, I'd like to compile it and test it on VGG19 inference. \r\n\r\nQuestion: do you know if the use of this enum value `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION` is any different from the environment variable `TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32` that was experimentally added by nvidia in their [v18.09 package release](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes/rel_18.09.html#rel_18.09)? The idea of internally casting down to float16 and then back to float32 is described [here](https://docs.nvidia.com/deeplearning/frameworks/tensorflow-user-guide/index.html#tensorcore) for `TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32`. Meanwhile, the [link you reference](https://devblogs.nvidia.com/tensor-ops-made-easier-in-cudnn/) describes the same process except it discusses automatic padding for NCHW packed data. That is the only difference I can see without looking through the source code. ", "@nluehr if we use `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION`, users still need to do loss scaling during training, right? I want to confirm that unlike tf32, using tensor cores in this manner can cause overflow due to the lower dynamic range of fp16. If so, I don't think it's worth exposing this functionality, since users still need loss scaling during training and the only further step to use tensor cores would be using mixed precision or quantization\r\n\r\n> If we do this, we should unify the API with the TF32 API (or have a good reason for not unifying) tensorflow/community#247.\r\n\r\nUnifying will be trickier since the TF32 API has the word `tensor_float_32` in the name. But we can discuss changing this if we decide to allow the use of `CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION`\r\n\r\n> Do you have a fork/branch where you have tested this? If so, I'd like to compile it and test it on VGG19 inference.\r\n\r\nUnfortunately I don't think we do.\r\n\r\n > Question: do you know if the use of this enum value CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION is any different from the environment variable TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32 that was experimentally added by nvidia in their v18.09 package release? The idea of internally casting down to float16 and then back to float32 is described here for TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32. Meanwhile, the link you reference describes the same process except it discusses automatic padding for NCHW packed data. That is the only difference I can see without looking through the source code.\r\n\r\n@nluehr can you answer this?"]}, {"number": 36035, "title": "Include casting to desired type to prevent dtype mismatch error.", "body": "If using a mixed datatype the _logcosh function will throw an error as below:\r\n`Input 'y' of 'Sub' Op has type float32 that does not match type float64 of argument 'x'.`\r\n\r\nAdded a typecast to overcome the issue.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36035) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36035) for more info**.\n\n<!-- ok -->"]}, {"number": 36034, "title": "Possible mem leak in tf.random.categorical", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.0.0\r\n- Python version:3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0.130\r\n- GPU model and memory:1060  6G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIf I set seed, the memory keeps increasing between loops.But if I remove seed, the memory will remains the same.\r\n\r\n**Describe the expected behavior**\r\nWhether I set random seed or not\uff0cthe memory will remains the same.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow_probability as tfp\r\nimport psutil\r\nimport os\r\n\r\nif __name__ == '__main__':\r\n    act_dim = 8\r\n\r\n    model = tf.keras.Sequential([tf.keras.layers.Input(500, ),\r\n                                 tf.keras.layers.Dense(500, activation=\"relu\"),\r\n                                 tf.keras.layers.Dense(act_dim, activation=\"softmax\")])\r\n\r\n    tf.random.set_seed(0)\r\n    print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n\r\n    for j in range(100):\r\n        for i in range(1000):\r\n            x = np.random.rand(500, 500)\r\n            prob = model(x)\r\n\r\n            dist = tfp.distributions.Categorical(probs=prob)\r\n            a = dist.sample()\r\n        print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Issue is replicating on colab with Tf 2.0. \r\nPlease find the gist [here](https://colab.sandbox.google.com/drive/17HhoZooFxxS4iukJAsJDZ6s1DiKGz7IX?authuser=1#scrollTo=kuiNQi0c9EbB). Thanks!", "Thanks for reporting this. I created [this notebook](https://colab.research.google.com/drive/1gsT11Gf7hQVSDplsSEswTJrMggX_GHt4) which demonstrates the issue using `tf.random.categorical` directly, without any invocation of TensorFlow Probability library code. This suggests the issue is at the TF level, not TFP. Kicking back to Yasir to reassign appropriately on the TF side. Please let me know if I can be of further assistance!", "Here's the code from that notebook, which is not viewable outside Google:\r\n\r\nI ran this with `tf-nightly` package.\r\n\r\n```python\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\nimport numpy as np\r\nimport psutil\r\nimport os\r\n\r\nif __name__ == '__main__':\r\n    act_dim = 8\r\n\r\n    model = tf.keras.Sequential([tf.keras.layers.Input(500, ),\r\n                                 tf.keras.layers.Dense(500, activation=\"relu\"),\r\n                                 tf.keras.layers.Dense(act_dim, activation=\"softmax\")])\r\n\r\n    tf.random.set_seed(0)\r\n    print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n\r\n    for j in range(100):\r\n        for i in range(1000):\r\n            x = np.random.rand(500, 500).astype(np.float32)\r\n            prob = model(x)\r\n\r\n            a = tf.random.categorical(logits=tf.math.log(prob), num_samples=1)\r\n        print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n```", "> Here's the code from that notebook, which is not viewable outside Google:\r\n> \r\n> I ran this with `tf-nightly` package.\r\n> \r\n> ```python\r\n> import tensorflow.compat.v2 as tf\r\n> tf.enable_v2_behavior()\r\n> import numpy as np\r\n> import psutil\r\n> import os\r\n> \r\n> if __name__ == '__main__':\r\n>     act_dim = 8\r\n> \r\n>     model = tf.keras.Sequential([tf.keras.layers.Input(500, ),\r\n>                                  tf.keras.layers.Dense(500, activation=\"relu\"),\r\n>                                  tf.keras.layers.Dense(act_dim, activation=\"softmax\")])\r\n> \r\n>     tf.random.set_seed(0)\r\n>     print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n> \r\n>     for j in range(100):\r\n>         for i in range(1000):\r\n>             x = np.random.rand(500, 500).astype(np.float32)\r\n>             prob = model(x)\r\n> \r\n>             a = tf.random.categorical(logits=tf.math.log(prob), num_samples=1)\r\n>         print(\"memory used:\", psutil.Process(os.getpid()).memory_info().rss)\r\n> ```\r\n\r\nThank you for your reply.I ran this code in my environment(tensorflow-gpu-2.0.0 by using conda install tensorflow-gpu),If I set seed,the the memory keeps increasing,if I do not set seed,the memory will keep remains the same.But when I ran this code in colab,whether I set random seed or not\uff0cthe memory will keep increasing.I don't know why.", "I don't know why `tf.random.set_seed` makes a difference here, but `tf.random.categorical` has known memory usage issues, because it uses the [Gumbel-distribution trick](https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/) which uses a lot of memory. You can try this code that manually implement the sampler not using the Gumbel trick:\r\n\r\n```python\r\ndef categorical(logits, num_samples, dtype=None, seed=None):\r\n  logits = tf.convert_to_tensor(logits, name=\"logits\")\r\n  seed1, seed2 = tf.compat.v1.get_seed(seed)\r\n\r\n  dt = tf.float32\r\n  batch = logits.shape.dims[0].value\r\n  num_classes = logits.shape.dims[1].value\r\n  if num_classes == 0:\r\n    # Delegates to native op to raise the proper error.\r\n    return tf.random.categorical(\r\n        logits, num_samples, seed=seed1, seed2=seed2, output_dtype=dtype)\r\n  # u ~ Uniform[0.0, 1.0)\r\n  u = tf.random.uniform(\r\n      shape=[batch, num_samples],\r\n      seed=seed1, seed2=seed2, dtype=dt)\r\n  # for numerical stability\r\n  max_logit = tf.math.reduce_max(logits, axis=1, keepdims=True)\r\n  logits = logits - max_logit\r\n  pdf = tf.cast(tf.math.exp(logits), dtype=dt)  # not normalized\r\n  cdf = tf.math.cumsum(pdf, axis=1)\r\n  cdf_last= cdf[:, -1:]\r\n  u = u * cdf_last\r\n  if num_samples == 0 or batch == 0:\r\n    # A tf.searchsorted bug workaround\r\n    return tf.zeros([batch, num_samples])\r\n  else:\r\n    return tf.searchsorted(cdf, u, side=\"right\")  # upper_bound\r\n```", "I'm having a similar issue for some Reinforcement Learning code where the issue is more pronounced. If I set a seed and then train an agent which samples actions randomly at each time step the memory leak becomes very pronounced. I can build an example if required but the issue is the same as above.\r\n\r\n**System information**\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n    TensorFlow installed from (source or binary):binary\r\n    TensorFlow version (use command below):2.0.0 (also verified on 2.1.0)\r\n    Python version:3.7.5\r\n    Bazel version (if compiling from source): N/A\r\n    GCC/Compiler version (if compiling from source): N/A\r\n    CUDA/cuDNN version: N/A\r\n    GPU model and memory: N/A\r\n\r\n**Update:** I have now raised [my own issue](https://github.com/tensorflow/tensorflow/issues/37252).", "I believe this may be caused by caching which `convert_to_eager_tensor` performs. Notably look at https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/framework/constant_op.py#L68-L72 which mentions that some values can be cached.\r\n\r\nIn the C++ code, https://github.com/tensorflow/tensorflow/blob/7072568ed6b735e347fb87bc84a2b83daf806e3f/tensorflow/python/eager/pywrap_tensor.cc#L307-L313 performs the caching -- currently for Python scalars only.\r\n\r\nWhen a seed is set, it causes operation seeds to be generated by random.Random generator, which are probably then converted to eager tensors -- and the problem arises.", "@feidieufo ,\r\nCan you please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/36034#issuecomment-576930993) and try in latest stable version v2.7 and let us know if the issue still persists.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36033, "title": "tensorflow1.12 hangs at MapAndBatchDatasetOp::Dataset::Iterator::RunnerThread", "body": "- OS Platform and Distribution: aarch64 \r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.1\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n\r\npreprocessing script is MapAndBatchDataset+PrefetchDataset, When we run training for a long time, we find that the training will be hung. We found that when hanging, the value of batch_results_.front()->num_calls in the following logic of map_and_batch_dataset_op.cc is always 1\r\n\r\n    Status GetNextInternal(IteratorContext* ctx,\r\n                             std::vector<Tensor>* out_tensors,\r\n                             bool* end_of_sequence) override {\r\n        std::shared_ptr<BatchResult> result;\r\n        {\r\n          mutex_lock l(*mu_);\r\n          EnsureRunnerThreadStarted(ctx);\r\n          while (batch_results_.empty() ||\r\n                 batch_results_.front()->num_calls > 0) {\r\n            RecordStop(ctx);\r\n            cond_var_->wait(l);\r\n            RecordStart(ctx);\r\n          }\r\n          std::swap(result, batch_results_.front());\r\n          batch_results_.pop_front();\r\n          cond_var_->notify_all();\r\n        }\r\n        return ProcessResult(ctx, result, out_tensors, end_of_sequence);\r\n      }\r\n\r\nWe found another function, which will reduce result-> num_calls by one. We will modify the property of result-> num_calls to atomic and there will be no problem. Does this mean that this code does not have a memory barrier.\r\n     void CallCompleted(const std::shared_ptr<BatchResult>& result)\r\n          LOCKS_EXCLUDED(*mu_) {\r\n        mutex_lock l(*mu_);\r\n        num_calls_--;\r\n        result->num_calls--;\r\n        cond_var_->notify_all();\r\n      }\r\n\r\nOne more thing to say is that we only found the problem on ARM OS(aarch64), but not on Linux Ubuntu OS(X86).\r\n\r\nWe want to know if there is a bug in this snippet of TensorFlow on aarch64 and how to fix it.\r\n\r\nHere is the call stack when hung:\r\nThread 374 (LWP 20847):\r\n#0  0x0000ffffb997d22c in pthread_cond_wait@@GLIBC_2.17 () from target:/lib/aarch64-linux-gnu/libpthread.so.0\r\n#1  0x0000ffffb11adb30 in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n#2  0x0000ffffb5501c5c in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x0000ffffb55012d4 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x0000ffffb54fe7f0 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x0000ffffb3c962d4 in tensorflow::data::(anonymous namespace)::MapAndBatchDatasetOp::Dataset::Iterator::RunnerThread(std::shared_ptr<tensorflow::data::IteratorContext> const&) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x0000ffffb11b3e14 in ?? () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n#7  0x0000ffffb9977088 in start_thread () from target:/lib/aarch64-linux-gnu/libpthread.so.0\r\n#8  0x0000ffffb98054ec in ?? () from target:/lib/aarch64-linux-gnu/libc.so.6\r\n\r\nThread 372 (LWP 20844):\r\n#0  0x0000ffffb997d22c in pthread_cond_wait@@GLIBC_2.17 () from target:/lib/aarch64-linux-gnu/libpthread.so.0\r\n#1  0x0000ffffb11adb30 in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n#2  0x0000ffffb5501c5c in nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x0000ffffb55012d4 in nsync::nsync_sem_wait_with_cancel_(nsync::waiter*, timespec, nsync::nsync_note_s_*) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x0000ffffb54fe7f0 in nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x0000ffffb3c938e4 in tensorflow::data::(anonymous namespace)::MapAndBatchDatasetOp::Dataset::Iterator::GetNextInternal(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*) () from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x0000ffffb3c41918 in tensorflow::data::DatasetBaseIterator::GetNext(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*) ()\r\n   from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x0000ffffb3d25644 in std::_Function_handler<void (), tensorflow::data::PrefetchDatasetOp::Dataset::Iterator::EnsurePrefetchThreadStarted(tensorflow::data::IteratorContext*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from target:/usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x0000ffffb11b3e14 in ?? () from target:/usr/lib/aarch64-linux-gnu/libstdc++.so.6\r\n#9  0x0000ffffb9977088 in start_thread () from target:/lib/aarch64-linux-gnu/libpthread.so.0\r\n#10 0x0000ffffb98054ec in ?? () from target:/lib/aarch64-linux-gnu/libc.so.6", "comments": ["@jsimsa @ravikyram \r\nThis problem has troubled me for a long time. Can you help analyze this problem?", "@yanqingshang I am sorry to hear that this has been an issue for you. Unfortunately, the team does not have resources to investigate issues in dated version of TensorFlow. My suggestion would be to updated to more recent version of TensorFlow. The latest version is TF 2.1 and if you require TF 1 compatibility, it would be TF 1.15.", "I meet a problem just like yours. The dataset hangs at the end of reading data. @yanqingshang ", "@jacquesqiao \r\nYou can try my workaround to see if it solves your problem. for tf1.12 version, the official does not have resources to investigate issues in dated version of TensorFlow.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36033\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36033\">No</a>\n"]}, {"number": 36032, "title": "[ROCm] Fix for compile error in //tensorflow/compiler/xla:refcounting_hash_map_test", "body": "On the ROCm platform, we currently get the following compile failure for the test\r\n`//tensorflow/compiler/xla:refcounting_hash_map_test`\r\n\r\n```\r\n...\r\nERROR: /root/tensorflow/tensorflow/compiler/xla/BUILD:928:1: C++ compilation of rule '//tensorflow/compiler/xla:refcounting_hash_map_test' failed (Exit 1)\r\ntensorflow/compiler/xla/refcounting_hash_map_test.cc: In member function 'virtual void xla::{anonymous}::RefcountingHashMapTest_ForEachEmpty_Test::TestBody()':\r\ntensorflow/compiler/xla/refcounting_hash_map_test.cc:80:3: error: 'int64' was not declared in this scope\r\n   int64 count = 0;\r\n...\r\n\r\n```\r\n\r\nThis fix resolves the compile error, and gets the test passing again on the ROCm platform\r\n\r\nOn the ROCm platform, this test is compiled via the following gcc compiler\r\n\r\n```\r\nroot@ixt-rack-04:/root/tensorflow# gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n```\r\n\r\nThe crosstool setup / compile invocation on the ROCm platform is done via\r\n* https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/rocm_configure.bzl\r\n* https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_rocm.tpl\r\n\r\n\r\n/cc @cheshire @whchung ", "comments": ["Should be fixed in https://github.com/tensorflow/tensorflow/commit/1bb2e82cdffc62a363a3c68dbdaf31826ee3358a", "@cheshire thank you."]}, {"number": 36031, "title": "[ROCm] Fix for compile error in //tensorflow/compiler/xla/service:dynamic_padder_test", "body": "\r\nOn the ROCm platform, we currently get the following compile failure for the test\r\n`//tensorflow/compiler/xla/service:dynamic_padder_test`\r\n\r\n```\r\n...\r\nINFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/sandbox\r\nERROR: /root/tensorflow/tensorflow/compiler/xla/service/BUILD:2311:1: C++ compilation of rule '//tensorflow/compiler/xla/service:dynamic_padder_test_gpu' failed (Exit 1)\r\ntensorflow/compiler/xla/service/dynamic_padder_test.cc: In member function 'virtual void xla::{anonymous}::ExecutionTest_ScatterUpdate_Test::TestBody()':\r\ntensorflow/compiler/xla/service/dynamic_padder_test.cc:266:7: error: 'StrReplaceAll' is not a member of 'absl'\r\n       absl::StrReplaceAll(hlo_text, {{\"INDICES_BOUND\", \"2\"}});\r\n       ^\r\ntensorflow/compiler/xla/service/dynamic_padder_test.cc:281:7: error: 'StrReplaceAll' is not a member of 'absl'\r\n       absl::StrReplaceAll(hlo_text, {{\"INDICES_BOUND\", \"4\"}});\r\n...\r\n\r\n```\r\n\r\nThis fix resolves the compile error, and gets the test passing again on the ROCm platform\r\n\r\nOn the ROCm platform, this test is compiled via the following gcc compiler\r\n\r\n```\r\nroot@ixt-rack-04:/root/tensorflow# gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n```\r\n\r\nThe crosstool setup / compile invocation on the ROCm platform is done via\r\n* https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/rocm_configure.bzl\r\n* https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_rocm.tpl\r\n\r\n\r\n/cc @cheshire @whchung ", "comments": ["Looks good, but note that such changes usually require a corresponding change in the BUILD file as well.", "Should be fixed in https://github.com/tensorflow/tensorflow/commit/48a1f6cac71e216c076941e4fb449613bac59f05", "@cheshire thank you."]}, {"number": 36030, "title": "[ROCm] Fix for compile error in //tensorflow/compiler/xla:debug_optio\u2026", "body": "\u2026ns_parsers_test\r\n\r\nOn the ROCm platform, we currently get the following compile failure for the test\r\n`tensorflow/compiler/xla/debug_options_parsers_test`\r\n\r\n```\r\n...\r\nexternal/com_google_googletest/googletest/include/gtest/internal/gtest-port.h:881:23: note:   'testing::internal::string'\r\n typedef ::std::string string;\r\n                       ^\r\ntensorflow/compiler/xla/debug_options_parsers_test.cc:29:33: error: 'test_map' was not declared in this scope\r\n   unordered_map<string, string> test_map;\r\n                                 ^\r\ntensorflow/compiler/xla/debug_options_parsers_test.cc:30:10: error: expected ';' before 'test_string'\r\n   string test_string = \"aa=bb,cc,dd=,ee=ff=gg\";\r\n...\r\n\r\n```\r\n\r\nThis fix resolves the compile error, and gets the test passing again on the ROCm platform\r\n\r\nOn the ROCm platform, this test is compiled via the following gcc compiler\r\n\r\n```\r\nroot@ixt-rack-04:/root/tensorflow# gcc --version\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n```\r\n\r\nThe crosstool setup / compile invocation on the ROCm platform is done via\r\n * https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/rocm_configure.bzl\r\n * https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_rocm.tpl\r\n\r\n\r\n\r\n/cc @cheshire @whchung ", "comments": []}, {"number": 36029, "title": "inference result is unstable", "body": "when i do inference use the same input at several times,  the inference result will be error at some times, this is terrible.  I don't know why, and anyone can help me? Thank you. ", "comments": ["@18970901926 \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "hi,\r\n&nbsp;&nbsp; I have sovlved the problem by updating tf version for a higher. I stil have a question, the tf c version doesn't work when it contains LSTM block, until replacing LSTM with others. It reply that op doesn't register. It really unconvinent. And i don't know how to register. The tf.contrib module don't work. Can you help me. The LSTM cell is better than others.\r\n&nbsp; &nbsp; &nbsp; Thank you for your reading and replying.\r\n\r\n\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"gadagashwini\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2020\u5e741\u670820\u65e5(\u661f\u671f\u4e00) \u665a\u4e0a6:21\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u529f\u592b\u718a\u732b\"<2551482565@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] inference result is unstable (#36029)\r\n\r\n\r\n\r\n\r\n@18970901926\r\n Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@18970901926,\r\nPlease provide the complete standalone code replicate the reported issue and also provide tensorflow version. Thanks!", "hi,&nbsp;\r\n&nbsp; &nbsp; The tensorflow c version is&nbsp; r1.13,&nbsp; here is the error information.\r\n&nbsp; &nbsp; &nbsp;\r\n\r\n&nbsp; &nbsp;Please see the attachment for the complete standalone code which replicate the reported issue.\r\n&nbsp; &nbsp; Thanks.\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"gadagashwini\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2020\u5e741\u670821\u65e5(\u661f\u671f\u4e8c) \u665a\u4e0a7:21\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u529f\u592b\u718a\u732b\"<2551482565@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] inference result is unstable (#36029)\r\n\r\n\r\n\r\n\r\n@18970901926,\r\n Please provide the complete standalone code replicate the reported issue and also provide tensorflow version. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\n\r\n\u4eceQQ\u90ae\u7bb1\u53d1\u6765\u7684\u8d85\u5927\u9644\u4ef6\r\n\r\ndemo.zip (78.90M, 2020\u5e7402\u670821\u65e5 10:12 \u5230\u671f)\u8fdb\u5165\u4e0b\u8f7d\u9875\u9762\uff1ahttp://mail.qq.com/cgi-bin/ftnExs_download?k=5b31666266b925c8a3b972fa1731061c00505401090205574b095257541c000754014b5a01500d1e00005201080704515000565031393457035c094c4b5844335b&t=exs_ftn_download&code=f1fb1143", "@18970901926, Could you fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) and provide all the information to analyze the issue. Thanks!", "@18970901926, Any update!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36028, "title": "Fix missing git depenendency for Jupyter docker image.", "body": "The jupyter notebook docker image includes a tutorial named \"overfit and underfit\" that has a step that requires git to run. The nightly-py3-jupyter image does not include git in the docker image. This is my attempt to add git to that image. \r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36028) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "@traviscollins thank you for your contribution, please sign CLA.", "I tried but after logging into my google account through the CLA process, I get a 500 error from the google site. \n\n\n> On Jan 19, 2020, at 11:20 PM, gbaned <notifications@github.com> wrote:\n> \n> \ufeff\n> @traviscollins thank you for your contribution, please sign CLA.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "@traviscollins Please try again and keep us posted. Because this cannot be proceed until CLA reflects YES. Thanks!", "@traviscollins gentle ping to sign CLA and keep us posted. Thanks!", "There\u2019s nothing I can do here. The CLA site returns a 500 error and no useful explanation. You\u2019ll have to contact the CLA group to fix the issue, accept this issue (I consent to the terms), or close the issue. ", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36028) for more info**.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36028) for more info**.\n\n<!-- cla_yes -->", "@traviscollins Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Thanks for this, sorry about the trouble with the CLA. Another PR already added git to the images; I've verified that `docker run -it --rm tensorflow/tensorflow:nightly-jupyter git version` works, so I'll close this PR."]}, {"number": 36027, "title": "Fix segfault when attempting to convert string to float16.", "body": "To make sure this gets fixed, add test for converting string to any numeric type.\r\n\r\nPiperOrigin-RevId: 286650886\r\nChange-Id: I81f770ec2bbd33a863e8057ce198c679912fa8e0", "comments": ["There was a merge conflict that I missed before, but now the branch should be good."]}, {"number": 36026, "title": "Fix segfault when attempting to convert string to float16.", "body": "To make sure this gets fixed, add test for converting string to any numeric type.\r\n\r\nPiperOrigin-RevId: 286650886\r\nChange-Id: I81f770ec2bbd33a863e8057ce198c679912fa8e0", "comments": ["There was a merge conflict that I missed before, but now the branch should be good."]}, {"number": 36025, "title": "Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro, Kernel 5.3.18\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de 2.1.0`\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.5\r\n- GPU model and memory: RTX 2070 8GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nImport a ResNet .pb model and run it. The following error will occur and whether setting `allow_growth` or not does not help:\r\n\r\n```\r\n2020-01-18 12:56:02.661655: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2020-01-18 12:56:02.666829: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nTraceback (most recent call last):\r\n  File \"/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1367, in _do_call\r\n    return fn(*args)\r\n  File \"/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1352, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/home/abcdabcd987/.pyenv/versions/3.7.4/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\", line 1445, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.\r\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet_v1_50/conv1/Conv2D}}]]\r\n\t [[output/_3]]\r\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node resnet_v1_50/conv1/Conv2D}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nRun it successfully\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nMODEL_PB_FILENAME = 'resnet_v1_50.pb'\r\nINPUT_TENSOR_NAME = 'input:0'\r\nOUTPUT_TENSOR_NAME = 'output:0'\r\n\r\ndef main():\r\n    with tf.device('/GPU:0'):\r\n        with tf.compat.v1.gfile.GFile(MODEL_PB_FILENAME, \"rb\") as f:\r\n            graph_def = tf.compat.v1.GraphDef()\r\n            graph_def.ParseFromString(f.read())\r\n            g_in = tf.import_graph_def(graph_def, name=\"\")\r\n\r\n    config = tf.compat.v1.ConfigProto()\r\n    # config.allow_soft_placement = True\r\n    # config.log_device_placement = True\r\n    # config.gpu_options.allow_growth = True\r\n\r\n    sess = tf.compat.v1.Session(graph=g_in, config=config)\r\n\r\n    batch_x = np.random.randn(1, 224, 224, 3);\r\n    output = sess.run(\r\n        OUTPUT_TENSOR_NAME,\r\n        feed_dict={INPUT_TENSOR_NAME: batch_x})[0]\r\n    print(output)\r\n    print(tf.test.is_gpu_available())\r\n    print('config', list(map(hex, config.SerializeToString())))\r\n\r\n\r\nmain()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n[test_tf.log](https://github.com/tensorflow/tensorflow/files/4081770/test_tf.log)\r\n", "comments": ["please check if enough memory on that particular GPU is available, sometimes this error occurs due to insufficient memory", "@Lasers67 Thanks for the notice. I saw some people run into this issue because of OOM, but I'm not in this situation. `nvidia-smi` reports zero memory usage. The model, ResNet50, takes very few memory, especially when the batch size is one. And the same script runs on the same machine on other TensorFlow builds/versions.", "Can you please try setting `allow_growth`  option at the top of your code:\r\n```python\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.compat.v1.Session(config=config)\r\n\r\n# your code \r\n```\r\nAlso what is your nvidia driver version?", "Hi @ymodak, I tried `allow_growth`. It's the same.  The driver version is `440.44`.", "Unfortunately I don't have easy access to an RTX 2070 so I cannot reproduce this error.\r\n\r\nYou said\r\n\r\n> And the same script runs on the same machine on other TensorFlow builds/versions.\r\n\r\nWhat other builds did you try?  And does this work on tf-nightly?", "Hello, I was having the same issue when instantiating a relatively small network on an RTX 2080 8GB. @ymodak's suggestion of setting `allow_growth=True` seems to solve the issue for me, although I don't really understand why this step is necessary.\r\n\r\nMy setup is as follows:\r\n * tensorflow: v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n * driver: 418.87.00\r\n * CUDA: 10.1\r\n * cuDNN: v7.6.5.32\r\n * python: 3.6.7\r\n * platform: CentOS 7 kernel 3.10.0-957.27.2.el7.x86_64\r\n\r\nEdited to add: The issue was only occurring for me when trying to use `tf.keras Conv*` layers; a different network with only `tf.keras.Dense` layers worked fine on the GPU, without having to set `allow_growth=True`.", "@ymodak's suggestion solved the issue for me. After digging around a little more, it seems like this is a cuDNN issue with RTX GPUs (see https://github.com/tensorflow/tensorflow/issues/25446). \r\nAlso seeing as tf.config is available in TF >=2.0, the way to perform this setting should be\r\n```\r\ngpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\r\nfor device in gpu_devices:\r\n    tf.config.experimental.set_memory_growth(device, True)\r\n```\r\nIf you are running on specific GPUs on a multi-GPU system, of course set CUDA_VISIBLE_DEVICES before this step.", "I am using:\r\n\r\n- CUDA Version 10.0.130\r\n- #define CUDNN_MAJOR 7\r\n #define CUDNN_MINOR 4\r\n #define CUDNN_PATCHLEVEL 2\r\n- tensorflow 1.15.2\r\n\r\nThe thing that worked for me is downgrading to tensorflow 1.14 \r\n\r\n- pip uninstall tensorflow-gpu\r\n- pip install tensorflow-gpu==1.14\r\n\r\nTaking advise from this post:\r\nhttps://stackoverflow.com/questions/58097025/0-unknown-failed-to-get-convolution-algorithm-this-is-probably-because-cudnn?answertab=active#tab-top", "This solution worked for me.\r\n\r\n> @ymodak's suggestion solved the issue for me. After digging around a little more, it seems like this is a cuDNN issue with RTX GPUs (see #25446).\r\n> Also seeing as tf.config is available in TF >=2.0, the way to perform this setting should be\r\n> \r\n> ```\r\n> gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\r\n> for device in gpu_devices:\r\n>     tf.config.experimental.set_memory_growth(device, True)\r\n> ```\r\n> \r\n> If you are running on specific GPUs on a multi-GPU system, of course set CUDA_VISIBLE_DEVICES before this step.\r\n\r\n", "@abcdabcd987 \r\nIs this still an issue.", "@abcdabcd987\r\nplease update if you still face this issue on later versions of tf.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36025\">No</a>\n"]}, {"number": 36024, "title": "Tensorflow Lite New Converter does not allow to use inference_input_type and inference_output_type with V2 APIs", "body": "**System information**\r\n- OS Platform and Distribution (Linux, MacOS):\r\n- TensorFlow installed from (official python wheel):\r\n- TensorFlow version (v2.1.0):\r\n\r\n\r\n```python\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    converter.experimental_new_converter = True\r\n    converter.experimental_new_quantizer = True\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n\r\n    tflite_model = converter.convert()\r\n```\r\n\r\nAbove code segment is expected to produce uint8 inference_input_type and inference_output_type, yet there is no uint8 conversion of the input layers and it ended up in identifying as floating_model when used in example [label_image.cc](https://github.com/tensorflow/tensorflow/blob/6ef62c6d2e90675eed0bb6ed10d8c5761ab365c1/tensorflow/lite/examples/label_image/label_image.cc#L226)\r\n\r\n```\r\n...\r\nmobilenetv2_1.00_224/global_average_pooling2d/Mean <type 'numpy.int8'>\r\nmobilenetv2_1.00_224/global_average_pooling2d/Mean/reduction_indices <type 'numpy.int32'>\r\nmobilenetv2_1.00_224/out_relu/Relu <type 'numpy.int8'>\r\ninput_1 <type 'numpy.float32'>\r\nIdentity <type 'numpy.float32'>\r\n```\r\nModel used: [mobilenet_v2_1.0_224_quant.tgz](https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz)\r\n\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Unable to produce true uint8 input/output layers.\r\n", "comments": ["I used [netron](https://github.com/lutzroeder/netron) to visualize your model. Please take a look.\r\n<img width=\"791\" alt=\"Screen Shot 2020-03-03 at 2 34 50 PM\" src=\"https://user-images.githubusercontent.com/42785357/75826347-2fe2f980-5d5c-11ea-857e-e04abbeaace3.png\">\r\n", "Closing this issue since it's resolved. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36024\">No</a>\n", "No clear what was the solution.", "> No clear what was the solution.\r\n\r\nAs I have noticed, this has been fixed and included in Release [v2.3.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.3.0) -> Checkout Release Notes (tflite section)\r\n\r\n-------\r\n\r\n## tf.lite: (Snapshot of Release Note section)\r\n\r\n#### Converter\r\n\r\n-  Restored `inference_input_type` and `inference_output_type` flags in TF 2.x TFLiteConverter (backward compatible with TF 1.x) to support integer (tf.int8, tf.uint8) input and output types in post training full integer quantized models.\r\n\r\n- Added support for converting and resizing models with dynamic (placeholder) dimensions. Previously, there was only limited support for dynamic batch size, and even that did not guarantee that the model could be properly resized at runtime.\r\n\r\n- Enabled experimental support for a new quantization mode with 16-bit activations and 8-bit weights. See `lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8.`\r\n"]}, {"number": 36023, "title": "Fixes to the release builds, based on first run attempt", "body": "", "comments": []}, {"number": 36022, "title": "Fixes to the release builds, based on first run attempt", "body": "", "comments": []}, {"number": 36021, "title": "Loading a tf.Module with tf.saved_model.load missing attributes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.1.0-rc2-17-ge5bf8de410 2.1.0`\r\n- Python version: `3.7.5`\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\nWhen saving and then loading a `tf.Module` instance using `tf.saved_model`, some of the attributes, such as `trainable_variables` are missing from the loaded object.\r\n\r\n**Describe the expected behavior**\r\nI would expect attributes like `trainable_variables` to persist through the serialization/deserialization.\r\n\r\n**Code and logs to reproduce the issue**\r\nFor example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nclass TestModule(tf.Module):\r\n    def __init__(self, value):\r\n        self.variable = tf.Variable(value)\r\n\r\n\r\nmodule_1 = TestModule(value=9000)\r\ntf.saved_model.save(module_1, \"./foo\")\r\nmodule_2 = tf.saved_model.load(\"./foo\")\r\n\r\nassert module_1.variable.numpy() == module_2.variable.numpy()\r\nassert module_1.trainable_variables == (module_1.variable, )\r\nassert module_2.trainable_variables == (module_2.variable, )\r\nassert module_1.trainable_variables == module_2.trainable_variables\r\n```\r\n\r\nraises \r\n\r\n```\r\n$ python -m tests.test_tf_saved_model_trainable_variables_missing\r\nTraceback (most recent call last):\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/Users/hartikainen/conda/envs/softlearning-3/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/hartikainen/github/rail-berkeley/softlearning-3/tests/test_tf_saved_model_trainable_variables_missing.py\", line 15, in <module>\r\n    assert module_2.trainable_variables == (module_2.variable, )\r\nAttributeError: '_UserObject' object has no attribute 'trainable_variables'\r\n```\r\n", "comments": ["Issue replicating for TF-2.1 and [tf-nightly](https://colab.sandbox.google.com/gist/oanush/c264d49024c257acbbbc2595ade8b468/36021.ipynb) .Thank you", "We have discussed reviving Modules as Modules instead of a minimal type. For Keras Models @k-w-w has implemented it.\r\n\r\nI don't have plans to do this for Module, and I think the Module maintainer @tomhennigan was mildly opposed. But if someone felt strongly it's probably something they could get done. The implementation itself would be quite simple, basically [adding one of these for Module](https://github.com/tensorflow/tensorflow/blob/de45c4b29563b420b3ac9e626d0ba41cb8f2bd46/tensorflow/python/training/tracking/data_structures.py#L1020).", "@allenlavoie thanks for the response, I think this makes sense. Assuming that Modules won't get fully revived, what's the suggested way of serializing objects like above? Is the best practice to manually implement the `__{set,get}state__` functions and use pickle, or is there some better way to do it? Or maybe I should use the `saved_model` like above and manually set the fields back to the deserialized object?", "One option is to add attributes before save. So if you know you'll want a list of variables, you can collect that and assign it to a different attribute. We do save lists, tuples, dicts with string keys, and namedtuples, just not `@property`s or methods.\r\n\r\nAnother option is to use GradientTape to collect trainable variables implicitly (tape.trainable_variables() I think).", "@hartikainen please confirm if above comment helps resolve the issue.", "It's still a little unsatisfactory that the deserialization does not fully recover the item that was serialized. The above solutions would require some custom code for deserialization even for attributes that exist by default for `tf.Module`, which makes me feel like this way of saving/loading should raise `NotImplementedError` if the user doesn't manually implement the deserialization behavior? Or preferably the `tf.Module` would get fully deserialized.", "Hi @hartikainen ! I was able to resolve this issue in TF 2.6 . In the  above code , model_2 missed instantiation like model_1 , attaching [gist ](https://colab.research.google.com/gist/mohantym/1e577b3e7040530c7157588c4d3419ac/github_36021.ipynb ) for reference.", "Hey @mohantym. Thanks for taking a look at this. I don't think the suggested solution in the notebook is quite what we want. It doesn't seem to be loading the model, but rather just instantiating two models with the same value.", ".", "@k-w-w Do you know why we didn't revive `trainable_variables` ?", "Yes, because `trainable_variables` is a property of `tf.Module`, and not a TF Trackable object, so it does not get restored from the SavedModel. ", "@hartikainen \r\nPlease refer to above comment and confirm if this is still an issue or can we move this to closed status.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36021\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36021\">No</a>\n"]}, {"number": 36020, "title": "can't import tensorflow ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): don't understand question\r\n- TensorFlow version: tensorflow-gpu 1.8.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 9.1.85\r\n- GPU model and memory: Quadro k2100M cuda cores 3.0 2GB memory\r\n\r\n\r\nI installed CUDA 9.1.85 and  CUDnn files and then i ran a wheel supporting my pc requirements but getting this error\r\n\r\nMicrosoft Windows [Version 6.3.9600]\r\n(c) 2013 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\Users\\AliJunaid>python\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\AliJunaid\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\AliJunaid\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\AliJunaid\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\AliJunaid\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n>>> exit()\r\n\r\nC:\\Users\\AliJunaid>\r\n", "comments": ["TF 1.8 GPU prebuit binary supports cuda 9.0\r\nSee https://www.tensorflow.org/install/source#gpu\r\nYou also need to update cuda, cudnn, cupti paths.\r\nSee https://www.tensorflow.org/install/gpu#windows_setup", "@Ali-Junaid \r\nPlease, see tested build configuration for TF 1.8 GPU for [windows](https://www.tensorflow.org/install/source_windows#gpu). Thanks!", "@Ali-Junaid \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36020\">No</a>\n"]}, {"number": 36019, "title": "[ROCm] Adding/Removing no_rocm tag to/from tests", "body": "Adding/Removing no_rocm tag to/from tests/subtests that are currently\r\nfailing/passing on the ROCm platform\r\n\r\nSeparating this commit out as a separate PR from PR #35764\r\n\r\n\r\n/cc @cheshire @whchung ", "comments": ["@deven-amd Can you please check failed build errors? Thanks!", "@gbaned,    pushed out a commit to fix the buildier errors. \r\n\r\nplease re-approve and re-run the CI tests. thanks", "@deven-amd Can you please resolve conflicts? Thanks!", "@gbaned I have rebased the PR to resolve the merge-conflcts", "> @gbaned I have rebased the PR to resolve the merge-conflcts\r\n\r\n@deven-amd  Still, conflicts appearing. Could you please resolve those? Thanks!", "@gbaned , rebased PR again to resolve the new merge conflicts", "@gbaned, rebased yet again to resolve another merge conflict that showed up today"]}, {"number": 36018, "title": "[ROCm] Changing cuda* names to gpu* names in a couple of tests", "body": "Separating this commit out as a separate PR from OPR #35764 \r\n\r\n/cc @cheshire @whchung ", "comments": []}, {"number": 36017, "title": "[ROCm] Fixing a bug in the previous commit to nccl_manager_test.cc file. ", "body": "This test has not been compiling (in ROCm mode) since this commit broke it :\r\nhttps://github.com/tensorflow/tensorflow/commit/d65a5f1bdf5dc536efe9cfcdd64acc2d6273b6e2\r\n\r\nAlso updating the run_cc_core.sh file to not filter out `//tensorflow/core/nccl:nccl_manager_test`\r\nThat test has the `no_rocm` tag on it (in the upstream repo)\r\n\r\n/cc @cheshire @whchung ", "comments": []}, {"number": 36016, "title": "Error optimizing my TFLite model for GPU usage", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro for Workstations (Build: 18363.592)\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (or github SHA if from source): 2.2.0-dev20200115\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ntf_lite_converter.py :\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow_core.lite.python.interpreter import Interpreter\r\nfrom tensorflow_core.lite.python.lite import TFLiteConverter, Optimize\r\n\r\nMODEL_PATH = <path to keras model file>\r\nTFLITE_MODEL_PATH = <path to optimized TFLite model file>\r\n\r\nconverter = TFLiteConverter.from_keras_model_file(MODEL_PATH)\r\nconverter.optimizations.append(Optimize.DEFAULT)\r\nconverter.target_spec.supported_types.append(tf.float16)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n\r\nopen(TFLITE_MODEL_PATH, \"wb\").write(tflite_model)\r\n\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n(tf-n) D:\\development\\tensorflow\\ANPR>python tf_lite_converter.py\r\n2020-01-18 14:20:47.702258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nOptimizer:    adagrad\r\nModel path:   output\\adagrad\\glpr-model.h5\r\n2020-01-18 14:20:53.240667: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-18 14:20:53.279154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-18 14:20:53.287304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-18 14:20:53.361116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-18 14:20:53.421716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-18 14:20:53.447055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-18 14:20:53.530899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-18 14:20:53.570409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-18 14:20:53.702842: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-18 14:20:53.708873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-18 14:20:53.715765: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-18 14:20:53.723989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-18 14:20:53.734035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-18 14:20:53.739179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-18 14:20:53.744328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-18 14:20:53.749625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-18 14:20:53.754539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-18 14:20:53.759888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-18 14:20:53.764633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-18 14:20:53.768758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-18 14:20:56.460164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-18 14:20:56.464829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-01-18 14:20:56.467340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-01-18 14:20:56.485286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6302 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\n2020-01-18 14:20:58.024625: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-01-18 14:20:58.029712: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-18 14:20:58.039753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-18 14:20:58.049235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-18 14:20:58.055409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-18 14:20:58.061247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-18 14:20:58.066364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-18 14:20:58.072194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-18 14:20:58.077274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-18 14:20:58.081844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-18 14:20:58.086684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-18 14:20:58.090252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-18 14:20:58.094291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-01-18 14:20:58.097674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-01-18 14:20:58.101224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6302 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-18 14:20:58.210900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize\r\n2020-01-18 14:20:58.215597: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 402 nodes (0), 510 edges (0), time = 12.053ms.\r\n2020-01-18 14:20:58.223386: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 402 nodes (0), 510 edges (0), time = 3.914ms.\r\n2020-01-18 14:20:58.228466: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_forward_gru_1_while_body_1537\r\n2020-01-18 14:20:58.234325: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.239123: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.243351: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_forward_gru_1_while_cond_1536\r\n2020-01-18 14:20:58.249016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-18 14:20:58.253361: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.257939: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_backward_gru_1_while_body_1693\r\n2020-01-18 14:20:58.263220: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.267683: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.271917: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_backward_gru_while_body_1380\r\n2020-01-18 14:20:58.277337: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.281620: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-18 14:20:58.286374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_backward_gru_while_cond_1379\r\n2020-01-18 14:20:58.291542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.296176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.300253: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_forward_gru_while_body_1224\r\n2020-01-18 14:20:58.305713: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.309768: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.314440: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_forward_gru_while_cond_1223\r\n2020-01-18 14:20:58.319815: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.323996: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.328567: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: model_1_bidirectional_1_backward_gru_1_while_cond_1692\r\n2020-01-18 14:20:58.333697: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-18 14:20:58.338136: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\nTraceback (most recent call last):\r\n  File \"tf_lite_converter.py\", line 30, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 1051, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 476, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 215, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-01-18 14:20:59.719318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-18 14:21:01.906381: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.\r\n2020-01-18 14:21:01.906537: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:111] Ignored drop_control_dependency.\r\n2020-01-18 14:21:02.036186: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-18 14:21:02.040149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-18 14:21:02.059114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 computeCapability: 7.5\r\ncoreClock: 1.83GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-01-18 14:21:02.059504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-01-18 14:21:02.062864: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-18 14:21:02.065500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-01-18 14:21:02.066696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-01-18 14:21:02.069723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-01-18 14:21:02.071795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-01-18 14:21:02.076283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-18 14:21:02.076923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0\r\n2020-01-18 14:21:02.582665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-18 14:21:02.582866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105]      0\r\n2020-01-18 14:21:02.582978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0:   N\r\n2020-01-18 14:21:02.583637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6287 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-01-18 14:21:05.868343: E tensorflow/lite/tools/optimize/quantize_weights.cc:474] Quantize weights tool only supports tflite models with one subgraph.\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Andreas\\Anaconda3\\Scripts\\toco_from_protos.exe\\__main__.py\", line 7, in <module>\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"c:\\users\\andreas\\anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: Quantize weights transformation failed.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n[download keras-model](https://drive.google.com/open?id=1GpQkBigddPvprAdplkr8ksT2nYRiAQNQ)\r\n\r\n[download saved_model](https://drive.google.com/open?id=1uEpnjYdWuC9yni9iqS4-Lt-A2sA-l4WB)\r\n\r\n**Failure details**\r\n\r\n**Any other info / logs**\r\n\r\nWithout using the GPU optimization I can convert the Keras model to a TFLite model. This works correctly, but under Android unfortunately only very slowly!\r\n\r\n[GitHub-Project](https://github.com/aboerzel/ALPR-keras)\r\n", "comments": ["@aboerzel Can you please share the keras model. I could not access the model from your links. Thanks!", "I had made a mistake in the file sharing - sorry!\r\n\r\nHere the link to download the [keras-model](https://drive.google.com/open?id=1GpQkBigddPvprAdplkr8ksT2nYRiAQNQ)", "@aboerzel I am able to reproduce the issue. Thanks. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/e0b2d59dbf6a2e1887711b935e460c9d/untitled790.ipynb) is the gist for our reference. Thanks!", "Hello,\r\nI would like to ask briefly what the chances are that the problem will be solved in the near future. A short information would help me a lot in my time planning for my project! It would also be interesting to know if there is a workaround how I can optimize the Keras model for GPU usage?\r\n\r\nMany thanks in advance,\r\nAndreas", "Hello,\r\n\r\nI am stuck on my project with this problem. The error comes up when I apply any optimization to the TFLite converter. Model has some custom ops, so had to set the allow_custom_ops flag True. Please do provide an update if there is any workaround or fix for this issue.\r\n\r\nThanks,\r\nAhmed", "@aboerzel I was able to convert your keras model after some modifications to your conversion code. Please take a look at the [gist](https://colab.research.google.com/gist/jvishnuvardhan/af99913bf322928a48c084b0a6d5df57/36016.ipynb). Please check the instructions [here](https://www.tensorflow.org/lite/performance/post_training_float16_quant) for more details.\r\n\r\nPelase verify it and close the issue if this was resolved for you. Thanks!", "@ahmedalesh Looks like your issue might be different from this issue. Can you please open a new issue with a standalone code. Thanks! You can ping me in that issue. Thanks!", "hi @jvishnuvardhan, \r\n\r\nthis is the issue https://github.com/tensorflow/tensorflow/issues/39564.\r\n\r\nRegards,\r\nAhmed", "Hi @jvishnuvardhan,\r\nthank you for the good news. I'm currently very busy, but I will try to check the solution as soon as possible and close the issue.\r\n\r\nThanks, \r\nAndreas\r\n", "Hi @jvishnuvardhan,\r\nI tried your solution and was now able to successfully convert my keras-model into a tflite-model with GPU optimization. \r\n\r\nThank you very much for your help! \ud83d\udc4d \r\n\r\nAndreas", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36016\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36016\">No</a>\n"]}, {"number": 36015, "title": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):1.15.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, RESIZE_BILINEAR, SOFTMAX. Here is a list of operators for which you will need custom implementations: DecodeJpeg.\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@devbk007, can you provide the complete code to analyze the issue. Thanks", "@devbk007, Provide the minimal code snippet to replicate the issue. Thanks!", "@devbk007, Please take a look at the [#35449](https://github.com/tensorflow/tensorflow/issues/35449#issuecomment-570318723) comment to solve the reported issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36015\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36015\">No</a>\n"]}, {"number": 36014, "title": "[tf1] tf.keras.layers.CudnnGRU cannot be converted and loaded when in subclass of tf.keras.Model", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nFollowing codes run well in training. But when I inference on CPU, I cannot load model because of [tensorflow/hdf5_format.py at r1.14 \u00b7 tensorflow/tensorflow \u00b7 GitHub](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/saving/hdf5_format.py#L373). The code here only works when class is \"Model\" or \"Sequential\" instead of its subclasses. \r\n\r\nI can fix this by change the class Foo's name, but it's not a good idea to hack this. \r\n\r\n```\r\nfrom tensorflow import keras\r\nclass Foo(keras.Model):\r\n  def __init__(self, rnn_dims, use_gpu, **kwargs):\r\n    super().__init__(**kwargs)\r\n    # fixed with self.__class__.__name__ = 'Model'\r\n    if use_gpu:\r\n      self.gru1 = keras.layers.CuDnnGRU(rnn_dims,\r\n                                        return_sequences=True,\r\n                                        return_state=True)\r\n    else:\r\n      self.gru1 = keras.layers.GRU(rnn_dims,\r\n                                        return_sequences=True,\r\n                                        return_state=True,\r\n                                        recurrent_activation='sigmoid',\r\n                                        reset_after=True)    \r\n  def call(self, inputs):\r\n    return self.gru1(inputs)\r\n\r\ndef create_model(rnn_dims, use_gpu):\r\n  foo = Foo(rnn_dims, use_gpu)\r\n  input_layer = keras.Input(shape=(None, rnn_dims))\r\n  output = foo(input_layer)\r\n  # actually there are some other calculations, so I don't want to write everything in Foo here.\r\n  return keras.Model([input_layer], [output])\r\n\r\n# Cannot load model\r\nmodel = create_model(rnn_dims, False)\r\nmodel.load_weights('path_to_model')\r\n```\r\n\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\n\r\nPeople use tensorflow 1.1x versions along with tf.keras.layers.CudnnGRU in subclass of tf.keras.Model. \r\n\r\n**Any Other info.**\r\nI know tf2 has a tf.keras.layers.GRU compatibale with GRU and CudnnGRU\uff0cbut it's a long way to upgrade for now for me.\r\n", "comments": ["Hi @Yablon, since this is an issue for TF1 and we are not going to make any new release for it, I would suggest u to upgrade to TF2 and use tf.compat.v1 if you still need to access any v1 feature. \r\n\r\nI am closing this issue for now, and feel free to reopen if you still hit same error with TF latest releases."]}, {"number": 36013, "title": "Add GPU headers to //tensorflow/core:headers.", "body": "Fixes #35576", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36013) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36013) for more info**.\n\n<!-- ok -->", "Thanks for the PR @abcdabcd987! This issue should be fixed in 2.2. Closing."]}, {"number": 36012, "title": "Upgrade dependencies", "body": "* `curl`:\r\n  * [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482)\r\n  * [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\r\n* `sqlite3`:\r\n  * [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646)\r\n  * [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\r\n  * [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\r\n", "comments": []}, {"number": 36011, "title": "Upgrade dependencies", "body": "* `curl`:\r\n  * [CVE-2019-5482](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5482)\r\n  * [CVE-2019-5481](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-5481)\r\n* `sqlite3`:\r\n  * [CVE-2019-19646](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19646)\r\n  * [CVE-2019-19645](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-19645)\r\n  * [CVE-2019-16168](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16168)\r\n", "comments": []}, {"number": 36010, "title": "Custom layer go_backwards does not work", "body": "I am implementing a custom recurrent class that inherits tf.layers.Layer, when using the Bidirectional wrapper I get the error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-3-7bd5b5269810> in <module>\r\n----> 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)\r\n    434     if backward_layer is None:\r\n    435       self.backward_layer = self._recreate_layer_from_config(\r\n--> 436           layer, go_backwards=True)\r\n    437     else:\r\n    438       self.backward_layer = backward_layer\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)\r\n    493     config = layer.get_config()\r\n    494     if go_backwards:\r\n--> 495       config['go_backwards'] = not config['go_backwards']\r\n    496     if 'custom_objects' in tf_inspect.getfullargspec(\r\n    497         layer.__class__.from_config).args:\r\n\r\nKeyError: 'go_backwards'\r\n```\r\nThis is the code for the layer itself:\r\n\r\n```\r\nclass RecurrentConfig(BaseLayer):\r\n    '''Basic configurable recurrent layer'''\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.layers: List[layers.Layer] = stack_layers(self.params,\r\n                                                       self.num_layers,\r\n                                                       self.layer_name)\r\n\r\n    def call(self, inputs: np.ndarray) -> layers.Layer:\r\n        '''This function is a sequential/functional call to this layers logic\r\n        Args:\r\n            inputs: Array to be processed within this layer\r\n        Returns:\r\n            inputs processed through this layer'''\r\n        processed = inputs\r\n        for layer in self.layers:\r\n            processed = layer(processed)\r\n        return processed\r\n\r\n    @staticmethod\r\n    def default_params() -> Dict[Any, Any]:\r\n        return{\r\n            'units': 32,\r\n            'recurrent_initializer': 'glorot_uniform',\r\n            'dropout': 0,\r\n            'recurrent_dropout': 0,\r\n            'activation': None,\r\n            'return_sequences': True\r\n        }\r\n```\r\n\r\nI have attempted to add the go_backwards to the config that is retrieved when get_config() is called but this results in another error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-7bd5b5269810> in <module>\r\n----> 1 a = TimeDistributed(Bidirectional(char_recurrent_cell))\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in __init__(self, layer, merge_mode, weights, backward_layer, **kwargs)\r\n    430     # Recreate the forward layer from the original layer config, so that it will\r\n    431     # not carry over any state from the layer.\r\n--> 432     self.forward_layer = self._recreate_layer_from_config(layer)\r\n    433 \r\n    434     if backward_layer is None:\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py in _recreate_layer_from_config(self, layer, go_backwards)\r\n    506       return layer.__class__.from_config(config, custom_objects=custom_objects)\r\n    507     else:\r\n--> 508       return layer.__class__.from_config(config)\r\n    509 \r\n    510   @tf_utils.shape_type_conversion\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in from_config(cls, config)\r\n    517         A layer instance.\r\n    518     \"\"\"\r\n--> 519     return cls(**config)\r\n    520 \r\n    521   def compute_output_shape(self, input_shape):\r\n\r\n~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/recurrent_layers.py in __init__(self, *args, **kwargs)\r\n     12     '''Basic configurable recurrent layer'''\r\n     13     def __init__(self, *args, **kwargs):\r\n---> 14         super().__init__(*args, **kwargs)\r\n     15         self.layers: List[layers.Layer] = stack_layers(self.params,\r\n     16                                                        self.num_layers,\r\n\r\n~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/layers/base_layer.py in __init__(self, params, mode, layer_name, num_layers, cust_name, **kwargs)\r\n     17                  cust_name: str = '',\r\n     18                  **kwargs):\r\n---> 19         super().__init__(params, mode, **kwargs)\r\n     20         self.layer_name = layer_name\r\n     21         self.cust_name = cust_name\r\n\r\n~/nlpv3-general/nlp-lib/src/main/python/mosaix_py/mosaix_learn/configurable.py in __init__(self, params, mode, **kwargs)\r\n     61 \r\n     62     def __init__(self, params: Dict[AnyStr, Any], mode: ModeKeys, **kwargs):\r\n---> 63         super().__init__(**kwargs) #type: ignore\r\n     64         self._params = _parse_params(params, self.default_params())\r\n     65         self._mode = mode\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __init__(self, trainable, name, dtype, dynamic, **kwargs)\r\n    184     }\r\n    185     # Validate optional keyword arguments.\r\n--> 186     generic_utils.validate_kwargs(kwargs, allowed_kwargs)\r\n    187 \r\n    188     # Mutable properties\r\n\r\n~/opt/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)\r\n    716   for kwarg in kwargs:\r\n    717     if kwarg not in allowed_kwargs:\r\n--> 718       raise TypeError(error_message, kwarg)\r\n\r\nTypeError: ('Keyword argument not understood:', 'go_backwards')\r\n```\r\n\r\nVersion info is:\r\ntf_version: '2.1.0-dev20191125'\r\ngit_version: 'v1.12.1-19144-gf39f4ea3fa'", "comments": ["@Jbiloki, can you provide the complete code to replicate the issue. Thanks", "Sure, here's a small example\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nclass DummyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.a = tf.keras.layers.LSTM(2)\r\n    \r\n    def call(inputs):\r\n        return self.a(inputs)\r\n    \r\ntf.keras.layers.Bidirectional(DummyLayer())\r\n```", "Was able to reproduce the issue with 2.1 and 2.2.0.dev20200121. \r\nPlease find the [gist](https://colab.research.google.com/gist/gadagashwini/08f950c8e5c24bca3e5db5531df29f4a/untitled352.ipynb). Thanks!", "I could reproduce the issue with `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/8bae6dfdf3bd7cc94f89f16d6101ee7f/untitled801.ipynb) is the gist for our reference. Thanks!", "Hi @Jbiloki. \r\n\r\nThanks for reporting the issue. For any layer that need to work with Bidirectional wrapper, it need to support processing the inputs in a reversed order. For all the default Keras RNN layers (LSTM/GRU), they all have a init param (go_backwards), and when it is configured to True, it will process the inputs in the reverse order.\r\n\r\nYou will need to update your custom RNN layer to have that init param as well if you want to use it with Bidirectional wrapper. So far the docstring for Bidirectional didn't state this point clearly, and we will update that.\r\n\r\nAs a workaround, you can also choose to implement a custom RNN cell, which define the math calculation for one single time step. Passing the cell to base RNN layer and wrap the RNN layer with Bidirectional wrapper, and the default RNN layer will handle the go_backwards correctly.", "@qlzh727 Thank you for the response, I believe for our environment the optimal solution is to have layers implement the go_backwards in the __init__ in the future we may implement custom cells which we can try the alternative method.\r\n\r\nFor now I am trying your initial suggestion but I am not sure I understand the correct implementation. I am getting keyword argument error not understood for go_backwards\r\n```\r\n\r\nimport tensorflow as tf\r\nclass DummyLayer(tf.keras.layers.Layer):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(go_backwards=True, *args, **kwargs)\r\n        #self.a = tf.keras.layers.LSTM(2, go_backwards=True)\r\n    \r\n    def call(inputs):\r\n        return None#self.a(inputs)\r\n    \r\ntf.keras.layers.Bidirectional(DummyLayer())\r\n```", "Writing the custom layer with go_backwards will take quite some work, some sample code will be like:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass DummyLayer(tf.keras.layers.Layer):\r\n  def __init__(self, go_backwards=False, *args, **kwargs):\r\n    super(DummyLayer, self).__init__(*args, **kwargs)\r\n    self.go_backwards = go_backwards\r\n    self.a = tf.keras.layers.LSTM(2, go_backwards=self.go_backwards)\r\n\r\n  def call(self, inputs):\r\n    return self.a(inputs)\r\n\r\n  @property\r\n  def return_sequences(self):\r\n    return self.a.return_sequences\r\n\r\n  @property\r\n  def return_state(self):\r\n    return self.a.return_state\r\n\r\n  def get_config(self):\r\n    config = {'go_backwards': self.go_backwards}\r\n    base_config = super(DummyLayer, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\ntf.keras.layers.Bidirectional(DummyLayer())\r\n```", "I see, thank you for the starter I will see if I can get it working!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36010\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36010\">No</a>\n", "We have update the docstring with more details about the requirement for the custom layer. "]}, {"number": 36008, "title": "XLA drops performance across the nodes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): TF benchmarks\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): Running with Horovod 0.18.2 via docker\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.5\r\n- GPU model and memory: 8xV100-16GB (2 nodes, each with 4xV100GPU's)\r\n- MPI version: 4.0.0\r\n- Mellanox OFED version: 4.7-3.2.9.0\r\n- GPUDirect RDMA - nvidia-peer-memory version: 1.0-8\r\n\r\n**Describe the current behavior**\r\nI am running TF benchmarks with Horovod in distributed mode, I see the scaling efficiency drops to from 76% (within the node) to 98% (when crossing the nodes), see below the throughput:\r\n\r\n-1xGPU within the node total images/sec:     ~1200\r\n-4xGPU's within the node total images/sec:   ~4712  | Scaling Efficiency (1200x4) / 4712 = 98%\r\n-8xGPU's across the nodes total images/sec: ~7302 |  Scaling Efficiency (1200x8) / 7302 = 76%\r\n\r\n**Describe the expected behavior**\r\nWe are expecting scaling efficiency across the nodes over 95%, it is around ~9120 img/sec. Please notice that the scaling efficiency drops only when crossing the nodes\r\n\r\n**Code to reproduce the issue**\r\n`mpirun -np 8 -H <ib>:4,<ib>:4 --allow-run-as-root -x NCCL_NET_GDR_LEVEL=3 -x NCCL_DEBUG_SUBSYS=NET -x NCCL_IB_DISABLE=0 -mca btl_tcp_if_include ib0 -x NCCL_SOCKET_IFNAME=ib0 -x NCCL_DEBUG=INFO -x HOROVOD_MPI_THREADS_DISABLE=1 --bind-to none --map-by slot --mca plm_rsh_args \"-p 12345\" python tf_cnn_benchmarks.py --data_format=NCHW --distortions=false --use_fp16=True --variable_update=horovod --model=resnet50 --batch_size=128 --xla=true`\r\n\r\n**Other info / logs**\r\n\r\n**Tracelog 1xGPU:**\r\n```\r\nStep    Img/sec total_loss\r\n1       images/sec: 1187.6 +/- 0.0 (jitter = 0.0)       7.817\r\n10      images/sec: 1201.3 +/- 3.4 (jitter = 9.5)       7.869\r\n20      images/sec: 1200.5 +/- 2.5 (jitter = 6.3)       7.915\r\n30      images/sec: 1202.0 +/- 3.3 (jitter = 8.5)       7.884\r\n40      images/sec: 1202.0 +/- 2.7 (jitter = 7.0)       7.836\r\n50      images/sec: 1200.3 +/- 2.2 (jitter = 6.4)       7.806\r\n60      images/sec: 1201.2 +/- 2.0 (jitter = 6.4)       7.852\r\n70      images/sec: 1202.1 +/- 2.0 (jitter = 7.5)       7.856\r\n80      images/sec: 1201.1 +/- 1.9 (jitter = 7.2)       7.754\r\n90      images/sec: 1200.5 +/- 1.7 (jitter = 6.8)       7.806\r\n100     images/sec: 1200.7 +/- 1.6 (jitter = 6.7)       7.764\r\n----------------------------------------------------------------\r\ntotal images/sec: 1199.87\r\n----------------------------------------------------------------\r\n```\r\n\r\n\r\n**Tracelog 8xGPU's:**\r\n```\r\nStep    Img/sec total_loss\r\n1       images/sec: 922.9 +/- 0.0 (jitter = 0.0)        7.787\r\n1       images/sec: 926.8 +/- 0.0 (jitter = 0.0)        7.707\r\n1       images/sec: 927.0 +/- 0.0 (jitter = 0.0)        7.720\r\n1       images/sec: 925.4 +/- 0.0 (jitter = 0.0)        7.881\r\n1       images/sec: 922.4 +/- 0.0 (jitter = 0.0)        7.846\r\n1       images/sec: 926.3 +/- 0.0 (jitter = 0.0)        7.694\r\n1       images/sec: 924.9 +/- 0.0 (jitter = 0.0)        7.683\r\n1       images/sec: 902.0 +/- 0.0 (jitter = 0.0)        7.762\r\n10      images/sec: 926.2 +/- 7.6 (jitter = 25.6)       7.725\r\n10      images/sec: 926.6 +/- 8.5 (jitter = 18.2)       7.647\r\n10      images/sec: 926.6 +/- 7.6 (jitter = 26.9)       7.659\r\n10      images/sec: 926.7 +/- 7.4 (jitter = 26.2)       7.703\r\n10      images/sec: 927.9 +/- 8.2 (jitter = 8.3)        7.613\r\n10      images/sec: 927.2 +/- 7.9 (jitter = 23.9)       7.708\r\n10      images/sec: 927.6 +/- 7.0 (jitter = 11.8)       7.710\r\n10      images/sec: 927.2 +/- 7.4 (jitter = 24.8)       7.669\r\n20      images/sec: 925.7 +/- 4.5 (jitter = 20.9)       7.620\r\n20      images/sec: 926.0 +/- 4.5 (jitter = 21.1)       7.585\r\n20      images/sec: 926.0 +/- 5.1 (jitter = 18.1)       7.530\r\n20      images/sec: 926.0 +/- 4.7 (jitter = 21.0)       7.600\r\n20      images/sec: 926.8 +/- 4.5 (jitter = 17.3)       7.524\r\n20      images/sec: 926.9 +/- 4.9 (jitter = 14.5)       7.638\r\n20      images/sec: 926.6 +/- 4.4 (jitter = 21.8)       7.588\r\n20      images/sec: 926.5 +/- 5.1 (jitter = 19.2)       7.682\r\n30      images/sec: 919.9 +/- 4.1 (jitter = 25.1)       7.567\r\n30      images/sec: 920.1 +/- 4.4 (jitter = 24.5)       7.529\r\n30      images/sec: 920.1 +/- 4.1 (jitter = 24.0)       7.546\r\n30      images/sec: 920.0 +/- 4.1 (jitter = 24.5)       7.578\r\n30      images/sec: 920.5 +/- 4.4 (jitter = 23.7)       7.469\r\n30      images/sec: 920.5 +/- 4.2 (jitter = 24.9)       7.483\r\n30      images/sec: 920.7 +/- 4.2 (jitter = 18.8)       7.489\r\n30      images/sec: 920.6 +/- 4.3 (jitter = 25.2)       7.526\r\n40      images/sec: 916.3 +/- 3.9 (jitter = 25.3)       7.541\r\n40      images/sec: 916.2 +/- 3.9 (jitter = 23.2)       7.444\r\n40      images/sec: 916.2 +/- 4.1 (jitter = 25.1)       7.515\r\n40      images/sec: 915.9 +/- 4.0 (jitter = 26.4)       7.476\r\n40      images/sec: 917.0 +/- 4.1 (jitter = 26.4)       7.534\r\n40      images/sec: 916.8 +/- 4.1 (jitter = 27.9)       7.502\r\n40      images/sec: 916.7 +/- 4.0 (jitter = 26.5)       7.500\r\n40      images/sec: 916.5 +/- 4.3 (jitter = 23.3)       7.529\r\n50      images/sec: 911.2 +/- 3.7 (jitter = 24.8)       7.523\r\n50      images/sec: 911.2 +/- 3.8 (jitter = 31.4)       7.461\r\n50      images/sec: 911.1 +/- 3.7 (jitter = 27.8)       7.461\r\n50      images/sec: 911.6 +/- 3.9 (jitter = 29.1)       7.524\r\n50      images/sec: 911.7 +/- 3.8 (jitter = 24.2)       7.512\r\n50      images/sec: 911.7 +/- 3.9 (jitter = 24.0)       7.419\r\n50      images/sec: 910.9 +/- 3.7 (jitter = 27.9)       7.446\r\n50      images/sec: 911.8 +/- 3.8 (jitter = 27.6)       7.467\r\n60      images/sec: 910.2 +/- 3.5 (jitter = 29.9)       7.513\r\n60      images/sec: 910.1 +/- 3.4 (jitter = 29.8)       7.435\r\n60      images/sec: 910.2 +/- 3.5 (jitter = 23.2)       7.478\r\n60      images/sec: 910.2 +/- 3.4 (jitter = 31.2)       7.463\r\n60      images/sec: 910.7 +/- 3.5 (jitter = 26.3)       7.487\r\n60      images/sec: 910.5 +/- 3.6 (jitter = 28.8)       7.436\r\n60      images/sec: 910.6 +/- 3.6 (jitter = 26.5)       7.408\r\n60      images/sec: 910.6 +/- 3.5 (jitter = 27.9)       7.446\r\n70      images/sec: 913.4 +/- 3.2 (jitter = 26.4)       7.409\r\n70      images/sec: 913.3 +/- 3.2 (jitter = 28.5)       7.441\r\n70      images/sec: 913.4 +/- 3.2 (jitter = 27.8)       7.459\r\n70      images/sec: 913.4 +/- 3.3 (jitter = 24.5)       7.440\r\n70      images/sec: 913.8 +/- 3.4 (jitter = 27.0)       7.459\r\n70      images/sec: 913.8 +/- 3.3 (jitter = 26.4)       7.483\r\n70      images/sec: 913.9 +/- 3.2 (jitter = 28.7)       7.385\r\n70      images/sec: 913.6 +/- 3.3 (jitter = 26.3)       7.445\r\n80      images/sec: 915.7 +/- 3.0 (jitter = 30.2)       7.407\r\n80      images/sec: 915.8 +/- 3.0 (jitter = 26.0)       7.407\r\n80      images/sec: 915.8 +/- 3.0 (jitter = 25.1)       7.468\r\n80      images/sec: 915.8 +/- 3.0 (jitter = 25.3)       7.472\r\n80      images/sec: 916.2 +/- 3.0 (jitter = 27.4)       7.467\r\n80      images/sec: 916.3 +/- 3.1 (jitter = 27.9)       7.443\r\n80      images/sec: 916.1 +/- 3.1 (jitter = 27.7)       7.491\r\n80      images/sec: 915.4 +/- 3.2 (jitter = 24.0)       7.553\r\n90      images/sec: 914.7 +/- 2.8 (jitter = 26.5)       7.404\r\n90      images/sec: 914.6 +/- 2.8 (jitter = 28.5)       7.408\r\n90      images/sec: 914.7 +/- 2.8 (jitter = 25.3)       7.457\r\n90      images/sec: 914.7 +/- 2.9 (jitter = 25.1)       7.405\r\n90      images/sec: 915.0 +/- 2.9 (jitter = 24.0)       7.437\r\n90      images/sec: 915.0 +/- 2.8 (jitter = 27.7)       7.504\r\n90      images/sec: 915.0 +/- 3.0 (jitter = 28.1)       7.431\r\n90      images/sec: 915.1 +/- 2.9 (jitter = 26.5)       7.430\r\n100     images/sec: 913.2 +/- 2.6 (jitter = 26.9)       7.890\r\n----------------------------------------------------------------\r\ntotal images/sec: 7301.54\r\n----------------------------------------------------------------\r\n```\r\n\r\n\r\n\r\n", "comments": ["Ping @dubey; though the issue is about horovod, does NCCL in TF core suffer from the same issue?", "Can you describe the cluster interconnect?  I can imagine things slowing down if you have NVLink within hosts, but something much slower like TCP over ethernet across hosts.", "hi @dubey, I am using Mellanox InfiniBand ConnectX-5 network adapters with 100Gbit/s over IPoIB", "@dubey the log @vilmara pasted is from single node 8xGPU, so networking is not an issue here, but it\u2019s horovod so there are some IPCs around.\r\n\r\nWe have similar observation that global XLA scope slows down the inter-process (both single node and multi-node) all reduces. Wrapping different parts of the model into different JIT scope seems to increase the overlap between communication and computation. This only happened when XLA is enabled.", "Hang on, from @vilmara's comments it seems like 4x GPUs is one node, and 8x GPUs is \"across nodes\".  But @byronyi is claiming that 8x GPU is on a single node.\r\n\r\nCan we please clarify the exact topology with 4 and 8 GPU runs?", "I\u2019m sorry, looking it again it is indeed 4x GPU per node.", "> Hang on, from @vilmara's comments it seems like 4x GPUs is one node, and 8x GPUs is \"across nodes\". But @byronyi is claiming that 8x GPU is on a single node.\r\n> \r\n> Can we please clarify the exact topology with 4 and 8 GPU runs?\r\n\r\nhi @dubey / @byronyi, the system is configured with 2 nodes (each with 4xV100GPU's) for a total of 8xV100-16GB GPU's. I see scaling degradation happens only \"across nodes\"", "Thanks for the clarification @vilmara.\r\n\r\nIn general the scaling across nodes may be lower because of lower bandwidth interconnect.  In this case since the model size is not too large it may be possible to tune some parameters to get better performance.\r\n\r\nFor NCCL, the number of rings and the number of threads per socket may be useful.  I'm not familiar with Horovod tuning.  @byronyi may have more ideas, I think he's used NCCL over IB.\r\n\r\nIf you're still stuck, I suggest reaching out to Horovod and/or NCCL owners.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Thanks for the update!, closing it now"]}]