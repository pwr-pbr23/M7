[{"number": 47981, "title": "Infinite loop when using reduce() inside tf.data.Dataset.map()", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to create a 3D image dataset using my previous dataset of sorted 2D images. To do so, I use map() method of tf.data.Dataset along with a reduce() function that takes some 2D images from the same dataset (or a copy, from what I tried it doesn't matter) and adds them together. Nevertheless, when I use functools.reduce() or tf.foldl() functions inside of a tf.data.Dataset.map(), even in a simpler case, it enters an infinite loop.\r\nFor the example, I've simplified the case to a dummy dataset of 2 elements with shape (2, 3). And I'm using reduce() instead of tf.fold() to avoid using a generator in the input of the function.\r\nThe result is also an infinite loop.\r\n\r\n**Describe the expected behavior**\r\nIn my understanding, I should get a new dataset with one element of shape (1, 2, 3). But if this transformation is not possible, I think it should raise an error.\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\nfrom functools import reduce\r\n\r\nds = tf.data.Dataset.from_tensor_slices([[[1,2,3],[4,5,6]],[[7,8,9], [10,11,12]]])\r\nnew_ds = ds.map(lambda _: reduce(lambda a, b: tf.concat([tf.expand_dims(a[0], axis=0),tf.expand_dims(b[0], axis=0)], 0), ds))\r\n", "comments": ["I am able to replicate this on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/90a4b28f7d03d5ac1bf3ed277c79814b/untitled571.ipynb)", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f00a5a52c1232499a1bcea5f2f401404/untitled206.ipynb)..Thanks !"]}, {"number": 47973, "title": "Mask RCNN tflite detection speed on android too slow", "body": "**System information**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n  Android mobile\r\n- TensorFlow installed from (source or binary):\r\n   Binary\r\n- TensorFlow version (use command below):\r\n   2.4.0\r\n- Python version:\r\n  3.7\r\n\r\n**Describe the current behavior**\r\n\r\nWe have converted Mask RCNN to TFLite using this link: \r\n\r\nhttps://wathek.medium.com/convert-mask-r-cnn-model-to-tflite-with-tensorflow-2-3-57160d3be18d and Tensorflow 2.4.0\r\n\r\nWe have used byte buffers for the input and output tensors and have the GPU delegate, multithreading enabled. However it is taking around 35 seconds for detection, which is really not feasible in terms of latency. \r\n\r\n**Describe the expected behavior**\r\n\r\nWe expected the inference time to be around 3-5 seconds. How can we reduce the time taken for detection? Are there any changes we can make to the model other than those done in the link above? Any help would be appreciated, thank you.\r\n\r\n", "comments": ["@MeghaGhosh \r\n\r\nPlease share simple standalone code such that we can replicate the issue reported or a colab gist with the error reported.", "@srjoglekar246 could you take a look?", "@MeghaGhosh it would be good to see the TFLite model. Can you share a link to the file?", "Hey, this is the google drive link to the tflite file:\r\n\r\nhttps://drive.google.com/file/d/1S6T5GUHa9WrcK2WZa0DrHpD68w9jHnTX/view?usp=sharing\r\n\r\nThis is the code to run it on android:\r\n\r\nimageBitmapForModel = BitmapFactory.decodeResource(con.getResources(), R.drawable.b1);\r\n  if (imageBitmapForModel != null) {\r\n      Log.e(\"Edgar\", \"Suces\" + height + \"width\" + width);\r\n  }\r\n\r\n  ImageProcessor imageProcessor =\r\n          new ImageProcessor.Builder()\r\n                  .add(new ResizeOp(height, width, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))\r\n                  .add(new ResizeWithCropOrPadOp(1024,1024))\r\n                  .build();\r\n\r\n  TensorImage tImage = new TensorImage(DataType.FLOAT32);\r\n  tImage.load(imageBitmapForModel);\r\n  tImage = imageProcessor.process(tImage);\r\n\r\n  imgMeta = ByteBuffer.allocateDirect(1 * 18 * 4);\r\n  imgMeta.order(ByteOrder.nativeOrder());\r\n\r\n  anchors = ByteBuffer.allocateDirect(1 * 261888 * 4 * 4);\r\n  anchors.order(ByteOrder.nativeOrder());\r\n\r\n  imgMeta.rewind();\r\n\r\n  for (int i = 0; i < 18; i++) {\r\n      imgMeta.putFloat(0.0f);\r\n  }\r\n  anchors.rewind();\r\n  for (int i = 0; i < 261888; i++) {\r\n      for (int k = 0; k < 4; k++) {\r\n          anchors.putFloat(0.0f);\r\n      }\r\n\r\n  }\r\n\r\n  Object[] inputs = {tImage.getBuffer(), imgMeta, anchors};\r\n  Map<Integer, Object> outputs = new HashMap<>();\r\n  outputs.put(0, new float[1][1][1]);\r\n  outputs.put(1, new float[1][1000][6][4]);\r\n  outputs.put(2, new float[1][1000][6]);\r\n  outputs.put(3, new float[1][100][6]);\r\n  outputs.put(4, new float[1][100][28][28][6]);\r\n  outputs.put(5, new float[1][1][4]);\r\n  outputs.put(6, new float[1][1][2]);\r\n\r\n  HashMap outputProbabilityBuffers = new HashMap<>();\r\n  ByteBuffer output0 = ByteBuffer.allocateDirect(16000);\r\n  output0.order(ByteOrder.nativeOrder());\r\n  output0.rewind();\r\n  ByteBuffer output1 = ByteBuffer.allocateDirect(1000 * 6 * 4 *4);\r\n  output1.order(ByteOrder.nativeOrder());\r\n  output1.rewind();\r\n  ByteBuffer  output2 = ByteBuffer.allocateDirect(1000 * 6 * 4);\r\n  output2.order(ByteOrder.nativeOrder());\r\n  output2.rewind();\r\n  ByteBuffer output3 = ByteBuffer.allocateDirect(100 * 6 * 4);\r\n  output3.order(ByteOrder.nativeOrder());\r\n  output3.rewind();\r\n\r\n    float[][][] output3 = new float[1][100][6];\r\n  ByteBuffer  output4 = ByteBuffer.allocateDirect(100 * 28 * 28 * 6 * 4);\r\n  output4.order(ByteOrder.nativeOrder());\r\n  output4.rewind();\r\n  ByteBuffer  output5 = ByteBuffer.allocateDirect(4190208);\r\n  output5.order(ByteOrder.nativeOrder());\r\n  output5.rewind();\r\n  ByteBuffer   output6 = ByteBuffer.allocateDirect(2095104);\r\n  output6.order(ByteOrder.nativeOrder());\r\n  output6.rewind();\r\n  outputProbabilityBuffers.put(0, output0);\r\n  outputProbabilityBuffers.put(1, output1);\r\n  outputProbabilityBuffers.put(2, output2);\r\n  outputProbabilityBuffers.put(3, output3);\r\n  outputProbabilityBuffers.put(4, output4);\r\n  outputProbabilityBuffers.put(5, output5);\r\n  outputProbabilityBuffers.put(6, output6);\r\n\r\n\r\n  Date date = Calendar.getInstance().getTime();\r\n  DateFormat formatter = new SimpleDateFormat(\"dd/MM/yyyy-mm-ss\");\r\n  String today = formatter.format(date);\r\n  Log.e(\"edgar Start\", \"\" + today);\r\n\r\n  tfLite.runForMultipleInputsOutputs(inputs, outputProbabilityBuffers);\r\n\r\n      Date dates = Calendar.getInstance().getTime();\r\n      DateFormat formatterdd = new SimpleDateFormat(\"dd/MM/yyyy-mm-ss\");\r\n      String todays = formatterdd.format(dates);\r\n      Log.e(\"edgar End\", \"\" + todays);\r\n\r\n      final ArrayList<DetectionResult> recognitions = new ArrayList<>(NUM_DETECTIONS);\r\n      return recognitions;", "There are two issues here:\r\n\r\n1. The model's input size, and consequently the internal tensors, are too large for existing mobile applications.\r\n2. Acceleration (by GPU) isn't working because the input has dynamic batch size. Usually you can set a static batch size by providing `batch_size` to [keras.input]\r\n\r\nSo I would suggest, if possible, making the model *smaller* and *simpler*, hopefully with static batch sizes. Maybe you can distill the large MaskRCNN model to a smaller student model that has same Inputs & Outputs?", "Okay thanks, I'll look into this.", "Hi @srjoglekar246 I have removed the mask head, number of output tensors and reduced the image size (to 384*384), with a major compromise in accuracy to bring the inference time down to 6 seconds. (this is with multithreading)\r\n\r\nThis speed somewhat works for us however there are some problems. If you could shed some light as to how we can fix this:\r\n\r\nThe tflite output on android is different from the Keras h5 output. Reducing the image size has led to significantly fewer detections. For eg: we are getting these results in keras for a particular image:\r\n\r\n[[[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n  [0.6697102  0.61200994 0.71017647 0.6631437  3.         0.95587164]\r\n  [0.6538168  0.695219   0.68414    0.7051539  3.         0.8865816 ]\r\n  [0.4635428  0.55197054 0.5702299  0.700203   2.         0.87292403]\r\n  [0.6545733  0.67137647 0.6891192  0.70570266 3.         0.85790217]\r\n  [0.6573738  0.62910855 0.7056983  0.7020159  3.         0.78753364]\r\n  [0.         0.         0.         0.         0.         0.        ]\r\n  [0.         0.         0.         0.         0.         0.        ]\r\n  [0.         0.         0.         0.         0.         0.        ]....]\r\n\r\n\r\nFor the converted tflite model, we are getting only 2 outputs. This is not feasible as we have significantly compromised on accuracy. Also the results are quite weird as in the detections are repeated until the last one. We have faced this with each version of the model with different image sizes. For example, in our latest model the results are similar to this:\r\n\r\n[[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]....\r\n.........................................................................................................................................\r\n..........................................................................................................................................\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.7725331  0.2580187  0.7878663  0.2675068  3.         0.97869164]\r\n[0.6697102  0.61200994 0.71017647 0.6631437  3.         0.95587164]] \r\n\r\nWhy are the detections being repeated? For eg: if there are 5 detections in total, the first 95 instances will be the same repeated values and then the last 5 instances will vary and will be our results.  I am sharing both the keras as well as tflite model here. Any help would be appreciated. The code is as mentioned above, we are using tensorflow lite support library to resize our image. The rest of the inputs match the keras inference.\r\n\r\nThis is the tflite model: https://drive.google.com/file/d/1ZUthHpTu1JtukSdTaHXaJzadh_iYdZF0/view?usp=sharing\r\n\r\nThis is the keras h5 model: https://drive.google.com/file/d/1pWrhJIplxkToi0hum9Ao-livFC45wz8b/view?usp=sharing\r\n\r\n\r\n", "Hi, I am new in tflite, and I got the same issue with you, Did you solve the problem? "]}, {"number": 47958, "title": "\" ValueError: assignment destination is read-only\" when using Pandas DataFrame", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code I suppose (see below)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): Not applicable\r\n- GCC/Compiler version (if compiling from source): Not applicable\r\n- CUDA/cuDNN version: Using CPU\r\n- GPU model and memory: Using CPU\r\n\r\n\r\n**Describe the current behavior**\r\nThis code\r\n```python\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\ndf = pd.DataFrame()\r\ndf[\"x\"] = tf.linspace(0, 100, 20)\r\ndf[\"x\"] /= 2\r\n```\r\nproduces the following error\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-796d4d4e000c> in <module>\r\n      4 df = pd.DataFrame()\r\n      5 df[\"x\"] = tf.linspace(0, 100, 20)\r\n----> 6 df[\"x\"] /= 2\r\n\r\n~/.local/lib/python3.8/site-packages/pandas/core/generic.py in __itruediv__(self, other)\r\n  11338 \r\n  11339     def __itruediv__(self, other):\r\n> 11340         return self._inplace_method(\r\n  11341             other, type(self).__truediv__  # type: ignore[operator]\r\n  11342         )\r\n\r\n~/.local/lib/python3.8/site-packages/pandas/core/generic.py in _inplace_method(self, other, op)\r\n  11315         ):\r\n  11316             # GH#36498 this inplace op can _actually_ be inplace.\r\n> 11317             self._values[:] = result._values\r\n  11318             return self\r\n  11319 \r\n\r\nValueError: assignment destination is read-only\r\n```\r\n\r\n**Describe the expected behavior**\r\nNormal assignment of the operation, like with Numpy.\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame()\r\ndf[\"x\"] = np.linspace(0, 100, 20)\r\ndf[\"x\"] /= 2\r\n```\r\n\r\n**Workaround**\r\n```python\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\ndf = pd.DataFrame()\r\ndf[\"x\"] = pd.Series(tf.linspace(0, 100, 20))\r\ndf[\"x\"] /= 2\r\n```", "comments": ["> ~/.local/lib/python3.8/site-packages/pandas/core/generic.py in _inplace_method(self, other, op)\r\n\r\n@JulianWgs,\r\nFrom the error log, it looks like the error is thrown from the pandas module. \r\n\r\nIn this case, could you please get in touch with pandas support regarding the error. Thanks!", "Thanks for the fast reply :)\n\nThis is true. However something in the Tensorflow code sets the array to be non writeable. The error does not happen when the exception is thrown, but when the tensor is assigned to the column of Pandas. The Numpy implementation handles it \u201ecorrectly\u201c.\n\nDo you understand what I mean?\n\nSorry, for not clarifying this earlier.\n\nGreetings\nJulian", "import pandas as pd\r\nimport tensorflow as tf\r\n\r\ndf = pd.DataFrame()\r\n\r\ndf[\"y\"] = tf.linspace(0, 100, 20)\r\n\r\ndef f(t):\r\n    return t/2\r\n\r\ndf[\"x\"]=df[\"y\"].apply(f)\r\ndf.drop(\"y\",axis=1,inplace=True)\r\n", "@JulianWgs,\r\nThe behavior is expected as the `Tensors` are **`immutable`**.  The problem can be resolved by replacing\r\n`df[\"x\"] = tf.linspace(0.0, 100.0, 20)` with `df[\"x\"] = tf.linspace(0.0, 100.0, 20).numpy()`.\r\n\r\nPlease find [the Gist](https://colab.research.google.com/gist/rmothukuru/b73d93b7ae4cf4477224570184d08530/gh_v.ipynb) of working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Thank you for your response :)\r\n\r\nI know that Tensors are immutable, but Pandas Series are not. Tensorflow wrongly casts the Tensor as an immutable Series. As you can see in the workaround when using `pd.Series` the Tensor gets cast correctly as mutable. This is the correct behavior and should **also** happen, when directly assigning the Tensor.\r\n\r\nLet me give you another example:\r\nTuples are also immutable, yet they correctly cast as mutable to a Pandas Series.\r\n\r\n```python\r\nimport pandas as pd\r\n\r\nimmutable_tuple = tuple(range(5))\r\nprint(immutable_tuple)\r\n\r\n# Tuples are not mutable (this does not work, because tuples are immutable)\r\nimmutable_tuple[2] = 5 \r\n\r\n# Assign tuple to Pandas Series\r\ndf = pd.DataFrame()\r\ndf[\"tuple\"] = immutable_tuple\r\ndf\r\n\r\n# Pandas Series, based on a tuple, is now mutable\r\ndf[\"tuple\"][2] = 5\r\ndf\r\n```\r\n\r\nTo get closer to the origin of the bug, I would look at the difference of the call stacks between assignment to a Pandas Series and `pd.Series`.\r\n\r\nBest regards\r\nJulian", "Was able to replicate the issue in [TF v2.5](https://colab.research.google.com/gist/sushreebarsa/19763db786e1aecd738bb6bf75c157de/untitled171.ipynb#scrollTo=Lel12_wU092S) &[ TF 2.6.0-dev20210531](https://colab.research.google.com/gist/sushreebarsa/875d8818661966c886b932799d014010/untitled170.ipynb),Please find the gists ..Thanks !"]}, {"number": 47947, "title": "Optimize unique_op by repetition rate", "body": "My test about unique op:\r\n\r\n> Input: size:20\\*1000k, uniqued size:5\\*1000k.\r\n\r\n> cmp test:\r\n\r\n>> Origin: The initial size of the hash table in unique_op is 2*input_size\r\n\r\n>> My test: The initial size of the hash table in unique_op is 1*input_size(change source code\"tensorflow/core/kernels/unique_op.cc\").\r\n\r\n> cmp result:\r\n>> Origin: spend: 2.517s\r\n>> My test: spend: 1.709s(\u219132.1%)\r\n\r\n**As the size of the hash table increases, memory bind will increase.**\r\nI think unique_op should be added a default params of repetition rate, default value is non-repeating(hash table size is 2*N)\u3002Repetition rate will help us do some prediction, like: Adjust the initial size of the hash table to improve performance\u3002\r\nIf the user does not know the repetition rate of his data, he should use default value.\r\nIf the user knows the repetition rate of his data, he will adjust this args to get a better performance.\r\nDo you think this view makes sense? If it makes sense, I would be happy to discuss & implement this part of the code. ^v^\r\n\r\nMy test code:\r\n```\r\nfrom __future__ import print_function\r\nimport random\r\nimport tensorflow as tf\r\nimport time\r\n\r\nsize = 20 * 1000000\r\nsum=0\r\nkeys, idx = tf.unique(tf.random_uniform(shape=[int(size)], dtype=tf.int64, maxval=int(size/5)), version='v2')\r\n\r\nsess = tf.Session()\r\nwith tf.Session() as sess:\r\n    for i in range(0,10):\r\n       start = time.time()\r\n       _ = sess.run(keys)\r\n       sum+=(time.time() - start)\r\n    print(\"avg time:\", sum / 10);\r\n```", "comments": ["I suggest repetition rate is a range of [0, 1), 0 is non-repeating, 1 is completely repetitive.\r\nhash_table_size = 2 - repetition_rate.\r\nIf user use default value(non-repeating), hash_table_size is origin(2 * input_size).\r\nIf user add param of repetition_rate(my test, 0.75), hash_table_size is (1.25 * input_size).\r\n\r\nTo facilitate troubleshooting, we need to find a way to know whether the hash table has been expanded. If there is an expansion, a warning will be issued to notify the user that the wrong usage method may cause performance degradation.\r\n", "Heuristic hash table size maybe a good idea ~", "copy that. @kkimdev ", "Q: Is this request from a real world use case? If so, do you mind sharing with us?  I'm trying to gauge the importance of this and if it'll be useful more broadly.  Thanks!", "A:\r\nScene: Ps save embedding hash table, and workers get embedding by features.\r\nWe need unique_op to filter repeated features(hash key) for decreasing net pressure and decreasing lookup embedding hash table times.\r\nNow we search 2,000,000 features in hash table once(like my test code).\r\nWe will search more keys in the future.\r\n\r\nData: I'm so sorry the data cannot be given to you due to its sensitivity, but the characteristics of the data are the same as in the example.\r\n@kkimdev "]}, {"number": 47941, "title": "Add additive/multiplicative angluar margin loss", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAdditive angular margin loss can create highly discriminative embedding appropriate for re-identification and cross-domain instance matching tasks. Such a layer is currently accessible only via third-party implementations.\r\n\r\n**Will this change the current api? How?**\r\nWill add a new layer that allows for angular penalties.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone working in cross domain instance matching/ identification problems. Additionally, can be used as a stepping stone to implement certain SOTA instance matching architectures.\r\n**Any Other info.**\r\nN/A", "comments": ["It's sounds good "]}, {"number": 47929, "title": "tfcompile for arm", "body": "I'm looking for a step-by-step procedure or example of how to use a compile (XLA/AOT) for the arm64 device. \r\nIs it possible to cross-compile the TensorFlow graph into executable code? (e.g. Linux/windows as a host machine for Linux-based arm64 device)\r\nPlease, if you can link me, I would very much appreciate it.\r\n\r\nThank you.", "comments": []}, {"number": 47927, "title": "TFLiteSavedModelConverterV2 breaks ordering of multi output models", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installation (pip package or built from source): binary and source \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0-dev20210319, 5abe2b515db51d3aa35f6acf5869c1ce51a6b09c\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.lite.TFLiteConverter.from_saved_model` doesn't support multi output models and outputs non-deterministic ordering which breaks users of the generated flatbuffers.\r\n\r\nThis is due to the fact that TFLite converter relies on the output ordering, but the saved model import function purposefully breaks this assumption by scrambling the inputs and outputs:\r\nhttps://github.com/tensorflow/tensorflow/blob/c36991b0367a12bc81bf37dfdc5cf4793c7a8bde/tensorflow/compiler/mlir/tensorflow/translate/import_model.cc#L3708-L3721\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.lite.TFLiteConverter.from_saved_model` should behave the same way as `tf.lite.TFLiteConverter.from_keras_model` and preserve the output ordering. This issue is especially problematic when relying on models using `@tf.function(experimental_implements=True)` which are only supported by the saved model converter.\r\n\r\n**Standalone code to reproduce the issue**\r\nI opened PR #47928 which includes a failing test case (093f927aa9c217bfbecf7350c3c3f651964587af) that reproduces this issue when running the unittests with `bazelisk test tensorflow/lite/python:lite_v2_test --test_output=all`.\r\n", "comments": ["For the multiple input/output naming/ordering issue, we recommend using TFLite signature def API.\r\n\r\nSignatureDef provides meaningful/generic names for inputs/outputs which doesn't rely on specific model details. More on SignatureDefs here.\r\nIf your saved model has a defined signatureDef then it will be exported during conversion to TFLite and then you can use the Signature inputs/outputs for inference and not relying on inputs/outputs order or tensor names.\r\n\r\nSignatureDef support is new thing available in nightly and will be available in 2.5\r\n\r\nThen use the nightly for converting to tflite. The generated tflite file will have the SignatureDef details.\r\nUsing Python API (as an example) you can do something like\r\n\r\n```\r\nmy_signature = interpreter.get_signature_runner(\"my_method\")\r\nresults = my_signature(input_1=input_tensor_1, input_2=input_tensor_2)\r\nprint(results[\"my_output\"])\r\n```", "@abattery Thanks for the fast response.\r\n\r\nI should've elaborated a bit more about the use case in my description. I am aware that users of saved model should not rely on the ordering of the output. However, this issue came up when using `tf.lite.TFLiteConverter.from_keras_model` together with models that use `@tf.function(experimental_implements=...)`.\r\n\r\nThis means `TFLiteKerasModelConverterV2` will use `TFLiteSavedModelConverterV2` under the hood\r\nhttps://github.com/tensorflow/tensorflow/blob/52a6477e956d5be2c90e9e799ed3e0ad4512f77e/tensorflow/lite/python/lite.py#L951 in cases where the model uses `@tf.function(experimental_implements=...)`:\r\nhttps://github.com/tensorflow/tensorflow/blob/52a6477e956d5be2c90e9e799ed3e0ad4512f77e/tensorflow/lite/python/lite.py#L570-L573\r\n\r\nTherefore,  depending on the kind of model passed to `TFLiteKerasModelConverterV2`,  the output would either be orderer or not depending on an implementation detail. This e.g. breaks [this existing unittest](https://github.com/tensorflow/tensorflow/blob/52a6477e956d5be2c90e9e799ed3e0ad4512f77e/tensorflow/lite/python/lite_v2_test.py#L1331-L1371) if the saved model code path would be used internally due to `experimental_implements` .\r\nI think this behaviour is quite confusing for users since I don't think it is documented anywhere in the Keras to TFLite conversion docs and breaks with the assumption that Keras model outputs are ordered which as far as I know holds true anywhere else in the code. So intuitively I would also expect the TFLite model directly converted from a Keras model to have the same ordering.\r\n\r\nI agree that for saved models itself the story is a bit different since as far as I know one always had to refer to tensors directly by their names or via a signature def. So the behaviour here is consistent, but for the Keras model conversion I do think this could lead to serious bugs in user space, or am I missing something obvious here?\r\n\r\nIn any case I will take a closer look at `SignatureDef` and how we can use it for our need. We are mainly targeting TFLite Micro and I haven't looked into how we can use the `SignatureDef` in that context.", "For the above cases, I would recommend exporting your keras model to the corresponding saved model with a signature def before putting it to the TensorFlow Lite converter API for now to avoid the multi output names/ordering issue.\r\n\r\nThank you for providing the valuable feedback. At least, there should be improvements in the document side.", "Reopened for looking for another solution.", "@abattery Thanks for re-opening the issue. I tried out using `SignatureDefs` with Keras models and discovered a few bugs and usability issues which can be reproduced in [this colab notebook](https://colab.research.google.com/drive/12GWxCkECNYNDMx0EaI379lIQEvZgsW4i?usp=sharing).\r\n\r\nThe first problem is related to using full int8 quantization (/cc @MeghnaNatraj) by setting\r\n```python\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n```\r\nThis breaks the signature runner since the signature still refers to tensors that [have been removed when converting to full int8 quantization](https://github.com/tensorflow/tensorflow/blob/305926798c36e2babfd31395317db33c55490d5b/tensorflow/lite/python/lite.py#L745).\r\n\r\nThe second problem is more a usability issue. Since the signature input and output names are set via Keras layer names, the signature of a Keras model depends on whether layers with the same name have already been instantiated prior to model saving which would lead to a change in leayer names.\r\nI think this is problematic since users will need to rely on fixed names in their TFLite code when relying on signature defs as far as I can tell. Otherwise this means during training great care needs to be taken that the model is saved in a fresh environment to not mess up the names (this can be very tricky during fine tuning where multiple instances of the same model might exist in the training code).\r\n\r\nLet's construct a model using dictionaries as inputs and outputs which would provide a unique mapping in multi-output models that does not rely on the ordering\r\n```python\r\nmodel = tf.keras.Model({\"input\": input_tensor}, {\"boxes\": boxes, \"scores\": scores})\r\n\r\nprint(model.input)\r\n# {'input': <KerasTensor: shape=(None, 32, 32, 128) dtype=float32 (created by layer 'input_4')>}\r\n\r\nprint(model.output)\r\n# {'boxes': <KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'tf.reshape_6')>,\r\n# 'scores': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'tf.reshape_7')>}\r\n```\r\n\r\nUnfortunately the signature def would include layer names that might not be uniquely identifiable across model training runs.\r\n```python\r\nprint(interpreter.get_signature_list()[\"serving_default\"])\r\n# {'inputs': ['input_4'], 'outputs': ['tf.reshape_6', 'tf.reshape_7']}\r\n```\r\n\r\nPlease take a look at [this colab notebook](https://colab.research.google.com/drive/12GWxCkECNYNDMx0EaI379lIQEvZgsW4i?usp=sharing) for a fully reproducible code example.\r\n\r\nFor me, all of these issues would be solved by maintaining the input/output order of the Keras model during serialization and conversion to TFLite so that the order of `model.inputs` and `model.outputs` (returns a list of tensors) are preserved during conversion.\r\nAlternatively, relying on `SignatureDefs` would work very well too, if it supports full int8 conversion and if there exists an easy way to repeatedly name inputs and outputs during Keras model saving that does not depend on the automatic layer naming of Keras.", "@abattery @MeghnaNatraj Do you have any recommendation on how to workaround this bug until there is a proper fix for it implemented?", "Actually, you can override the output signature names by wrapping the keras model.\r\n\r\nFor example,\r\n\r\n```\r\n@tf.function(input_signatures=[tf.TensorSpec([None, 32, 32, 128], dtype=tf.float32, name=\"input_tensor\")])\r\ndef override_output_signatures(input_tensor):\r\n   outputs = keras_model(input_tensor)\r\n   return {\"boxes\": outputs.boxes, \"scores\": outputs.scores}\r\n\r\nsignatures = override_output_signatures.get_concrete_function()\r\ntf.saved_model.save(model, export_dir=export_path, signatures=signatures)\r\n```", "Cool, thanks. Would be great if this could be automatically done when constructing the Keras model in the future.\r\n\r\nBut that still will break with `converter.inference_output_type = tf.int8` since it modifies the flatbuffer after the fact, or am I missing something?", "@lgeiger Could you help us create a separate issue for the int8 overriding case with the signature definitions? This thread has a number of the discussion points and it is not easy to focus on when we invite more folks to look at the issue. It would be great if you can provide a minimal example as a form of the gist regarding the above issue.", "Thanks @lgeiger for finding a bug! I took a look at the above colab. It looks like the signature API can be broken after the model optimization is applied. Could you copy and paste the comment (https://github.com/tensorflow/tensorflow/issues/47927#issuecomment-804240854) to create a separate issue? I will take a look at this issue with the team.", "fyi @karimnosseir ", "Thanks for taking a look. I opened #48148 which only includes the description of the bug that breaks the signature API with full integer models.\r\n\r\nSorry for mixing the two problems, this issue can now focus purely on the API usability.", "The signature def API is designed intentionally without having an intuitive order. It would be better to rely on the names instead of the order.\r\n\r\n", "> The signature def API is designed intentionally without having an intuitive order. It would be better to rely on the names instead of the order.\r\n\r\nI agree that using the signature def API makes a lot of sense when working with `tf.functions` and saved models directly.\r\n\r\nHowever, I think users only working with Keras models tend to expect that output ordering is preserved similar to how Python functions returning tuples work. So when converting models with `tf.lite.TFLiteConverter.from_keras_model` the fact that output ordering is not preserved is surprising since the fact that models get serialized as saved models during conversion is only an implementation detail that changed in TF 2.4.\r\nIs there a concrete reason for why preserving output ordering is not possible for Keras models? When saving and loading Keras models using `model.save` and `tf.keras.models.load_model` the ordering is preserved as well, which is why I am confused that this is not possible when converting to TFLite."]}, {"number": 47924, "title": "Gpu memory issue", "body": "**System information**\r\n- I am using keras finetuned vgg model with 2 dense layers\r\n-  Linux Ubuntu 18.04 (Colab)\r\n- TensorFlow version (use command below): 2.4.3\r\n- Python version: 3.7\r\n- CUDA Version: 11.2   \r\n- GPU model and memory: Tesla P100 with 16 gb\r\n\r\n**Describe the current behavior**\r\n\r\nhere is the log after loading model\r\nFri Mar 19 17:02:34 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   36C    P0    33W / 250W |   **5703MiB / 16280MiB** |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nit's using 5gb gpu memory.\r\n\r\nAfter making one prediction.\r\n\r\n`Fri Mar 19 17:02:55 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   36C    P0    38W / 250W |   **9969MiB / 16280MiB** |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\nGpu usage is increased almost duoble.\r\n\r\n\r\nCan you tell me it's a normal usage or there is something wrong with my code.", "comments": ["For reproduce the issue please see this colab notebook : https://colab.research.google.com/drive/1QwfH6aXK8vTLuNslP4qcSMh50fT2K1Gt?usp=sharing", "@kbrajwani \r\nI ran this code on tf 2.3,2.4 and nightly, please refer to [the gist](https://colab.research.google.com/gist/Saduf2019/ac73d579cad60bf40eb00fbc76483055/untitled567.ipynb) and would you want to upgrade to nightly and let us know if it helps.", "Hey @Saduf2019 tf-nightly works like charm . Can you advise me one thing? what is the best process to deploy model on gpu configure machine to take full benefits of gpu.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47924\">No</a>\n", "Hey @Saduf2019 i am again in same problem this time i am running the program with tensorflow nightly in aws dl ami instance and its eating all gpu memory. In google colab it's working fine. So can you tell me it's tensorflow who makes a difference or cuda versions. ", "Hey @Saduf2019 can you please check following gist where we can reproduce the memory issue. It's working fine if I run a cell in notebook as your gist. But when i run as python file in background it's not releasing memory. Why is taking too much memory to predict and load the model? I don't think it's a correct thing.\r\nhttps://colab.research.google.com/drive/1QwfH6aXK8vTLuNslP4qcSMh50fT2K1Gt?usp=sharing\r\n\r\nFor better clarification i have compared pytorch behaviour also.", "@kbrajwani Can you please run the code by enabling `tf.config.experimental.set_memory_growth` as shown below. Please check the [TF page](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth) for more information.\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntry:\r\n  tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nexcept:\r\n  # Invalid device or cannot modify virtual devices once initialized.\r\n  pass\r\n```\r\n\r\nAlso, can you try Keras under TF. Instead of importing keras as `import keras`, can you try importing as `from tensorflow import keras`.\r\n\r\nPlease let us know how it progresses. Thanks!", "@jvishnuvardhan I have tried both things you can check [gist ](https://colab.research.google.com/drive/1QwfH6aXK8vTLuNslP4qcSMh50fT2K1Gt?usp=sharing) but it doesn't work. Can you tell me how much memory it have to use as per normal execution.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanjoy Have you find any solution?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/87bdfd06c474e9c06cabb9f82b6b9a52/untitled168.ipynb)..Thanks !", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ba4b1a12082231b36608c352f826a045/untitled250.ipynb) ..Thanks!"]}, {"number": 47923, "title": "Shared svdf OpData should be renamed to OpDataSvdf", "body": "@tensorflow/micro\r\n\r\nNoticed this while reviewing #47911\r\n\r\nSimilar to \r\nhttps://github.com/tensorflow/tensorflow/blob/d73ba75a4c1b26b91cc20cb86866d775c02dc479/tensorflow/lite/micro/kernels/conv.h#L27-L28\r\n\r\nthe svdf struct should be OpDataSvdf instead of\r\nhttps://github.com/tensorflow/tensorflow/blob/d73ba75a4c1b26b91cc20cb86866d775c02dc479/tensorflow/lite/micro/kernels/svdf.h#L23-L24\r\n\r\nThis renaming will likely happen after the CEVA optimized kernel is merged so we should include reference, cmsis_nn, xtensa and ceva in the renaming.", "comments": []}, {"number": 47905, "title": "tflite runtime API + NNAPI delegate refusing layers from a MobileNetV2SSD trained with Object Detection API, with high inference time", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): training on Ubuntu 18.04, inference on Yocto\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15 (tflite runtime 2.3)\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11\r\n- GPU model and memory: Titan-v 12GB\r\n\r\n\r\n**Describe the current behavior**\r\nI trained a Object detection model (MobileNetV2SSD) from scratch using the Object Detection API. I used TF version 1.15 as I needed to perform quantization-aware training. My pipeline config file includes the following block:\r\n\r\n```\r\ngraph_rewriter {\r\n  quantization { \r\n  delay: 48000\r\n  weight_bits: 8\r\n  activation_bits: 8\r\n  } \r\n}\r\n```\r\nThe model was then exported using the `object_detection/export_tflite_ssd_graph.py` script and converted to tflite format as described [here](https://neuralet.com/article/quantization-of-tensorflow-object-detection-api-models/) (full integer quantization). I then tested the model with the tflite runtime API + NNAPI on a board running YOCTO and equipped with a NPU. The inference time was very high (0.5 s) and I had many warnings from the NNAPI delegate which refused quantization operators (see below). Did I do something wrong?\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nInference code:\r\n```\r\nimport tflite_runtime.interpreter as tflite\r\nimport sys\r\nimport cv2\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\nmodel_path =\"model_path\"\r\nimg_path = \"img_path\"\r\n\r\ninterpreter = tflite.Interpreter(model_path=model_path)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\nheight = input_details[0]['shape'][1]\r\nwidth = input_details[0]['shape'][2]\r\n\r\nimg = cv2.imread(img_path)\r\nimg = cv2.resize(img, (300,300))\r\n\r\nfor  i in range(100):\r\n       \r\n        input_data = np.expand_dims(img, axis=0)\r\n        input_data = (input_data/255.0).astype(np.float32)\r\n\r\n        interpreter.set_tensor(input_details[0]['index'], input_data)\r\n        t1 = time.time()\r\n        interpreter.invoke()\r\n        t2 = time.time()\r\n\r\n        output_data = interpreter.get_tensor(output_details[0]['index'])\r\n        print(t2-t1)\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n**INFO: Created TensorFlow Lite delegate for NNAPI.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator FAKE_QUANT (v1) refused by NNAPI delegate: Unsupported operation type.\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: OP Version different from 1\r\nWARNING: Operator QUANTIZE (v2) refused by NNAPI delegate: Output should be kTfLiteUInt8.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator DEQUANTIZE (v2) refused by NNAPI delegate: NN API supports int8 type since version 1.2 but only for symmetric quantization.\r\nWARNING: Operator CUSTOM (v1) refused by NNAPI delegate: Unsupported operation type.\r\nApplied NNAPI delegate.**\r\n\r\n", "comments": ["@miaowang14 could you take a look?", "Most of the errors here are related to the quantize / dequantize nodes. And I am not expecting FAKE_QUANT nodes in the exported model, which seems indicating that there seems something wrong with exporting script.\r\n\r\n@lev-prol , have you seen similar issues before?"]}, {"number": 47901, "title": "Feature Request for N-Dimensional Operations", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes. I would prefer that.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently tensorflow has 1D, 2D and 3D layers for various operations. I would like to request N-dimensional convolutional layers (i.e. ConvND, MaxPoolingND, etc). I can see that there is some reference to convND here (https://github.com/tensorflow/tensorflow/blob/8e985babf8c8ed248551aa42dc9b21fd757739d1/tensorflow/core/grappler/optimizers/constant_folding.cc). \r\n\r\n\r\n**Will this change the current api? How?**\r\nYes. \r\n- ConvND, MaxPoolND, AveragePoolingND, CroppingND, GlobalAveragePoolingND, ZeroPaddingND would be added to the already existing API. \r\n- Other convolutional Layers can be a specific form of the more generalized N dimensional layers.\r\n**Who will benefit with this feature?**\r\n- Anyone willing to explore these operations on more than 3 dimensions\r\n**Any Other info.**\r\n", "comments": ["@ymodak  i am having some issues while implementing convND on python. Particularly in the convert_data_format function. Is there some generalized way to add more dimensions to the data_format? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9e3c906aca96739cc263fa1bc9671252fe084c9d/tensorflow/python/keras/utils/conv_utils.py#L27-L47"]}, {"number": 47893, "title": "TFLite Documentation Request: How do per-axis quantizations work with subsequent layers?", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/lite/performance/quantization_spec\r\n\r\n## Description of issue (what needs changing):\r\nDocumentation on how per-axis quantization works with subsequent layers\r\n\r\n### Clear description\r\n\r\nI am working on implementing the algorithms for TFLite layers in FPGA and I'm uncertain how the quantization spec expects subsequent layers to deal with per-axis quantizations. For example, following the tutorial [here](https://www.tensorflow.org/lite/performance/post_training_integer_quant) there is a 12-channel convolution which is pooled and flattened before being fed to a fully connected layer. The output of the convolution op has different quantization scales for each of the 12 channels. But then flattening the tensor it would seem this information is lost? The two options I can see are either rescaling the tensor so that the reshaped tensor has the same quantization uniformly. That would seemingly eliminate any benefits of per-axis quantization if it's just being undone in the next step. If the reshape does not re-scale them to the same scheme, wouldn't that then introduce a massive difference in the distribution of data going to the fully connected layer?\r\n\r\nThe logical explanation to me is that per-axis quantizations are only useful when feeding one convolutional layer to another convolutional layer. If the output of a convolution operation goes to anything other than a convolution, the output is re-scaled to match the same quantization scheme for all channels. But I'm unsure if that's what actually happens\r\n\r\n### Correct links\r\n\r\nN/A\r\n\r\n### Parameters defined\r\n\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nN/A\r\n\r\n### Usage example\r\n\r\nN/A\r\n\r\n### Submit a pull request?\r\n\r\nI do not know enough to submit a PR but if someone with knowledge can answer my question I can contribute it to the docs.\r\n", "comments": ["@daverim could you take a look?"]}, {"number": 47871, "title": "Unnecessary Device to Host copies", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):3.0.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.1\r\n- GPU model and memory:8.1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nThere are issues regarding unnecessary DtoH copies. When The tensor is originated on the device using any (GPU-friendly) dataset subclass or any other type of pipeline (one particular example is CuPy and DLPack), there are unnecessary DtoH copies happening. As a more explicit example when CuPy and keras are used DLPack invokes unexpected DtoH copies, which makes DLPack meaningless\r\n\r\n**Describe the expected behavior**\r\n\r\nShould eliminate the unnecessary DtoH and HtoD copies.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n`\r\n!pip install cupy-cuda111\r\nimport os\r\nprint(os.getpid())\r\nimport cupy as cp\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\nfrom tensorflow.keras.models import Model\r\n\r\nn = 10000000\r\n\r\na = Input(shape=(n))\r\noutput = Lambda(lambda x: x ** 2)(a)\r\nmodel = Model(a, output)\r\n\r\nx = cp.arange(n, dtype=cp.float32).reshape(1, n)\r\ndltensor = x.toDlpack()\r\nx2 = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\ncp.cuda.nvtx.RangePush('model.predict')\r\ny = model.predict(x2)\r\ncp.cuda.nvtx.RangePop()\r\nprint('done')\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@kushanam \r\nI ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/3c469caabcf199cf34cd9e79db4a62c7/untitled563.ipynb).", "> @kushanam\r\n> I ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/3c469caabcf199cf34cd9e79db4a62c7/untitled563.ipynb).\r\n\r\nI can't see the gist unfortunately due to API rate limit. I will try again, but wonder if you could share the error?", "> @kushanam\r\n> I ran the code shared and face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/3c469caabcf199cf34cd9e79db4a62c7/untitled563.ipynb).\r\n\r\nSorry, was able to run and fixed the example.", "Glad to hear that, please move the issue to closed status as resolved.", "@Saduf2019 , The issue is not resolved, What I fixed is not the issue, but the given example witch used threw an error.", "@kushanam \r\nI ran the updated code and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/71784a638349f3e8c2eb5f427f9393a8/untitled566.ipynb), can you please share a colab gist with the error reported.", "> @kushanam\r\n> I ran the updated code and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/71784a638349f3e8c2eb5f427f9393a8/untitled566.ipynb), can you please share a colab gist with the error reported.\r\n\r\nSeems like the Cuda version of the colab you are running on is 11.0:\r\n!cat /usr/local/cuda/version.txt\r\nCUDA Version 11.0.228\r\n\r\nfor that, please install cupy 11 rather:\r\n`!pip install cupy-cuda110`\r\n", "@VoVAllen  Can you please take a look at this issue/ tag its owner? Thanks!", "@ymodak I'm not tensorflow people. I believe it should be someone who owns Keras data processing part."]}, {"number": 47866, "title": "Incompatibility between `set_visible_devices()` and `from_dlpack()`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: GTX 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\n\r\nAfter selecting which GPU to use via `tf.config.experimental.set_visible_devices()`, converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` only works for device 0. Using other devices results in the error `InvalidArgumentError: GPU:1 unknown device.`\r\n\r\n**Describe the expected behavior**\r\n\r\nConverting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` should work for all devices.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport cupy\r\nimport random\r\n\r\nimport tensorflow as tf\r\n\r\n# gpu_to_use = 0      # Works\r\ngpu_to_use = 1        # Errors\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\nif gpus:\r\n    tf.config.experimental.set_visible_devices(gpus[gpu_to_use], \"GPU\")\r\n\r\n# Converting from TF to CuPy with dlpack works for both devices\r\ntensor = tf.random.uniform((10,))\r\n\r\ndltensor = tf.experimental.dlpack.to_dlpack(tensor)\r\narray1 = cupy.fromDlpack(dltensor)\r\n\r\n# Converting from CuPy to TF with dlpack only works for device 0\r\narray1 = cupy.array([random.uniform(0.0, 1.0) for i in range(10)], dtype=cupy.float32)\r\ndltensor = array1.toDlpack()\r\nx = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\n# Using device 1 results in the following error\r\n\r\n# Traceback (most recent call last):\r\n#   File \"examples/multi-gpu/tf-dlpack-repro.py\", line 22, in <module>\r\n#     x = tf.experimental.dlpack.from_dlpack(dltensor)\r\n#   File \"/home/karl/miniconda3/envs/nvtabular_dev_11.0/lib/python3.8/site-packages/tensorflow/python/dlpack/dlpack.py\", line 66, in from_dlpack\r\n#     return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)\r\n# tensorflow.python.framework.errors_impl.InvalidArgumentError: GPU:1 unknown device.\r\n```\r\n", "comments": ["@jermainewang @VoVAllen any thoughts on this?  We're trying to use dlpack for our multiGPU dataloader but running into issues getting data onto the appropriate GPU on the TF DLPack side.", "If we use `tf.config.experimental.set_visible_devices(gpus[1])`, the real gpu 1 will be labeled as 0 or 1 in tensorflow? This brings in the inconsistent device labeling between tensorflow and other framework, which is not ideal. Could you use `CUDA_VISIBLE_DEVICES` to control this instead?", "Unfortunately, we can't use `CUDA_VISIBLE_DEVICES`, because we're trying to integrate Horovod and need to pick which GPU to use after Tensorflow has already been initialized. :slightly_frowning_face: \r\n\r\nGiven the possibility of different device numbering across frameworks, it seems like Tensorflow should be using the physical (instead of logical) device number when converting DLpack objects coming from other frameworks. Is there a reasonable way to do that?", "@rmothukuru,\r\nI was able to reproduce the issue with TF v2.4 and TF-nightly. Please check the attached screenshot for reference. \r\n\r\n![Screenshot 2021-03-18 9 22 42 PM](https://user-images.githubusercontent.com/57165142/111658196-21199880-8832-11eb-9b7d-8ff492e21958.png)\r\n\r\n\r\nThanks!", "I didn't see an easy way to resolve this issue. Probably Tensorflow should preserve the GPU labeling on the logical devices after `set_visible_device`", "OOC, how did `array1`'s device get to be GPU:1?  Is it inferring that from the earlier call to `cupy.fromDlpack`?\r\n\r\n> Probably Tensorflow should preserve the GPU labeling on the logical devices after set_visible_device\r\n\r\nCan `TFE_FromDlpackCapsule` reconstruct that from `tf.config.experimental.get_visible_devices(\"GPU\")` and `tf.config.experimental.list_visible_devices(\"GPU\")`?", "@sanjoy I'm not entirely sure, but the same error happens even when skipping the earlier call to `cupy.fromDlpack`, so I'm guessing it's a side effect of `set_visible_devices` somehow?", "Was able to replicate the issue in TF nightly-2.6.0.dev20210603,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/8957dc224050ebcc25a06df67c1e517d/untitled164.ipynb?authuser=1)..Thanks !", "Reproduced on TF 2.5.0.", "Any update on this? this makes the library useless"]}, {"number": 47858, "title": "Quantization Aware Training does not seem to perform per-channel quantization with AllValuesQuantizer", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\nCustom code\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n\r\n-   **TensorFlow installed from (source or binary)**:\r\nAnaconda Navigator\r\n\r\n-   **TensorFlow version (use command below)**:\r\ntensorflow-gpu 2.3.0\r\n\r\n-   **Python version**:\r\n3.7.9\r\n\r\n-   **CUDA/cuDNN version**:\r\ncudatoolkit 10.1.243\r\ncudnn 7.6.5\r\n\r\n-   **GPU model and memory**:\r\nGTX 1070 8GB\r\n\r\n\r\n**Describe the current behavior**\r\nI have quantized two CNN models with QAT (AllValuesQuantizer), one with per-tensor and one with per-axis (per-channel) quantization. When saving the models in .h5 format and inspecting them i Netron, I note that each QuantizeWrapper layer's parameters both have scalar kernel_min and kernel_max.\r\n\r\n**Describe the expected behavior**\r\nAs I have understood from [this paper](https://arxiv.org/abs/1712.05877), the min/max values of the kernel are what defines the scale and zero-point quantization parameters. For per-tensor quantization it is reasonable that the model only has a single min and max value, as the whole tensor has the same scale and zero-point. HOWEVER, for per-channel quantization (where each channel has its own scale and zero-point) I believe that kernel_min and kernel_max should be vectors? Why aren't they?\r\n\r\n[Here is one of my per-tensor models (AllValues)](https://i.stack.imgur.com/HsbTF.png)\r\n\r\n[Here is one of my per-axis models (AllValues)](https://i.stack.imgur.com/uWMRI.png)\r\n\r\nIn [this github issue](https://github.com/tensorflow/tensorflow/issues/34299) someone mentions that QAT automatically uses per-tensor quantization (as of march 2020), but that this is subject to change. To me it looks like QAT (at least AllValuesQuantizer) still only uses per-tensor quantization? If that's the case, why is there a parameter that I can set to enable per-tensor quantization (See AllValuesQuantizer's per-axis boolean)?\r\n\r\nI also noted in the [source code for the AllValuesQuantizer](https://github.com/tensorflow/model-optimization/blob/v0.5.0/tensorflow_model_optimization/python/core/quantization/keras/quantizers.py#L291-L366) that self.per_axis is never passed to the next function, so what is that even variable used for?\r\n\r\nSo; does QAT even perform per-channel quantization? Doesn't seem like it to me. How can I use per-channel quantization with the AllValuesQuantizer?\r\n\r\n**Standalone code to reproduce my models**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\n# Possible quantization aware quantizers:\r\nQAT_ALL_VALUES = tfmot.quantization.keras.quantizers.AllValuesQuantizer\r\nQAT_LAST_VALUE = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nQAT_MA = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\n\r\ndef quantization_aware_training(model, save, w_bits, a_bits, symmetric, per_axis, narrow_range, quantizer, batch_size=64, epochs=2):\r\n\r\n    # Create quantized model's name string\r\n    name = model.name + '_'\r\n    name = name + str(w_bits) + 'wbits_' + str(a_bits) + 'abits_'\r\n\r\n    if symmetric:\r\n        name = name + 'sym_'\r\n    else:\r\n        name = name + 'asym_'\r\n\r\n    if narrow_range:\r\n        name = name + 'narr_'\r\n    else:\r\n        name = name + 'full_'\r\n\r\n    if per_axis:\r\n        name = name + 'perch_'\r\n    else:\r\n        name = name + 'perten_'\r\n\r\n    if quantizer == QAT_ALL_VALUES:\r\n        name = name + 'AV'\r\n    elif quantizer == QAT_LAST_VALUE:\r\n        name = name + 'LV'\r\n    elif quantizer == QAT_MA:\r\n        name = name + 'MA'\r\n\r\n    # Quantization\r\n    # *****\r\n    quantize_apply = tfmot.quantization.keras.quantize_apply\r\n    quantize_model = tfmot.quantization.keras.quantize_model\r\n    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\n    clone_model = tf.keras.models.clone_model\r\n    quantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n    supported_layers = [\r\n        tf.keras.layers.Conv2D,\r\n    ]\r\n\r\n    class Quantizer(tfmot.quantization.keras.QuantizeConfig):\r\n        # Configure how to quantize weights.\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n        def set_quantize_weights(self, layer, quantize_weights):\r\n            # Add this line for each item returned in `get_weights_and_quantizers`\r\n            # , in the same order\r\n            layer.kernel = quantize_weights[0]\r\n\r\n        def set_quantize_activations(self, layer, quantize_activations):\r\n            # Add this line for each item returned in `get_activations_and_quantizers`\r\n            # , in the same order.\r\n            layer.activation = quantize_activations[0]\r\n\r\n        # Configure how to quantize outputs (may be equivalent to activations).\r\n        def get_output_quantizers(self, layer):\r\n            return []\r\n\r\n        def get_config(self):\r\n            return {}\r\n\r\n    class ConvQuantizer(Quantizer):\r\n        # Configure weights to quantize with 4-bit instead of 8-bits.\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    class DepthwiseQuantizer(Quantizer):\r\n        # Configure weights to quantize with 4-bit instead of 8-bits.\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.depthwise_kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    # Instead of simply using quantize_annotate_model or quantize_model we must use\r\n    # quantize_annotate_layer since it's the only one with a quantize_config argument\r\n    def quantize_all_layers(layer):\r\n        if isinstance(layer, tf.keras.layers.DepthwiseConv2D):\r\n            return quantize_annotate_layer(layer, quantize_config=DepthwiseQuantizer())\r\n        elif isinstance(layer, tf.keras.layers.Conv2D):\r\n            return quantize_annotate_layer(layer, quantize_config=ConvQuantizer())\r\n        return layer\r\n\r\n    annotated_model = clone_model(\r\n        model,\r\n        clone_function=quantize_all_layers\r\n    )\r\n\r\n    with quantize_scope(\r\n        {'Quantizer': Quantizer},\r\n        {'ConvQuantizer': ConvQuantizer},\r\n            {'DepthwiseQuantizer': DepthwiseQuantizer}):\r\n        q_aware_model = quantize_apply(annotated_model)\r\n\r\n    # *****\r\n\r\n    # Compile and train model\r\n    optimizer = keras.optimizers.Adam(\r\n        learning_rate=0.001)\r\n    q_aware_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n        from_logits=True),\r\n        optimizer=optimizer, metrics=['sparse_categorical_accuracy'])\r\n\r\n    (train_images, train_labels),_ = keras.datasets.cifar10.load_data()\r\n\r\n    q_aware_model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, verbose=1,\r\n                      validation_split=0.1)\r\n\r\n    if save:\r\n        save_path = 'models/temp/' + name\r\n        q_aware_model.save(save_path + '.h5')\r\n\r\n    return q_aware_model\r\n\r\n\r\ndef temp_net():\r\n    dropout = 0.1\r\n\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=(32, 32, 3)))\r\n    model.add(keras.layers.BatchNormalization())\r\n    model.add(keras.layers.Activation('relu'))\r\n\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\n    model._name = \"temp_net\"\r\n\r\n    return model\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    q_model = quantization_aware_training(model=temp_net(), save=True,\r\n                                          w_bits=8, a_bits=8, symmetric=False, narrow_range=False, per_axis=False, quantizer=QAT_ALL_VALUES, batch_size=64, epochs=1)\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47858\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47858\">No</a>\n", "Re-titled and re-worded the issue so that it is easier to understand.", "@daverim could you triage this issue?", "Hi, sorry for the late reply:\r\n\r\nAs you can see in the conv default config, the weight quantizer extends LastValueQuantizer and sets per_axis=True\r\nhttps://github.com/tensorflow/model-optimization/blob/9193d70f6e7c9f78f7c63336bd68620c4bc6c2ca/tensorflow_model_optimization/python/core/quantization/keras/default_8bit/default_8bit_quantizers.py#L22\r\n\r\nSince you are using a custom scheme, you'll need to set this value for yourself. You can copy this quantizer or modify your code to set per_axis=True for conv quantizers.", "Hi again, I tried setting per_axis=True with the LastValueQuantizer for a small test network. However when doing this I get some shape-related issue I can't understand. per_axis=False works without any error.\r\n\r\nI used the following code, where I define custom conv and dense quantizers with parameters set by the _get_QAT_model function.\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\n\r\n# Possible quantization aware quantizers:\r\nQAT_ALL_VALUES = tfmot.quantization.keras.quantizers.AllValuesQuantizer\r\nQAT_LAST_VALUE = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nQAT_MA = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\n\r\ndef _get_QAT_model(model, w_bits, a_bits, symmetric, per_axis, narrow_range, quantizer):\r\n    quantize_apply = tfmot.quantization.keras.quantize_apply\r\n    quantize_model = tfmot.quantization.keras.quantize_model\r\n    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\n    clone_model = tf.keras.models.clone_model\r\n    quantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n    class Quantizer(tfmot.quantization.keras.QuantizeConfig):\r\n        # Configure how to quantize weights.\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n        def set_quantize_weights(self, layer, quantize_weights):\r\n            # Add this line for each item returned in `get_weights_and_quantizers`\r\n            # , in the same order\r\n            layer.kernel = quantize_weights[0]\r\n\r\n        def set_quantize_activations(self, layer, quantize_activations):\r\n            # Add this line for each item returned in `get_activations_and_quantizers`\r\n            # , in the same order.\r\n            layer.activation = quantize_activations[0]\r\n\r\n        # Configure how to quantize outputs (may be equivalent to activations).\r\n        def get_output_quantizers(self, layer):\r\n            return []\r\n\r\n        def get_config(self):\r\n            return {}\r\n\r\n    class ConvQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    class DenseQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, quantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    def quantize_all_layers(layer):\r\n        if isinstance(layer, keras.layers.Conv2D):\r\n            return quantize_annotate_layer(layer, quantize_config=ConvQuantizer())\r\n        if isinstance(layer, keras.layers.Dense):\r\n            return quantize_annotate_layer(layer, quantize_config=DenseQuantizer())\r\n        return layer\r\n\r\n    annotated_model = clone_model(\r\n        model,\r\n        clone_function=quantize_all_layers\r\n    )\r\n\r\n    with quantize_scope(\r\n        {'Quantizer': Quantizer},\r\n        {'ConvQuantizer': ConvQuantizer},\r\n            {'DenseQuantizer': DenseQuantizer}):\r\n        q_aware_model = quantize_apply(annotated_model)\r\n\r\n    return q_aware_model\r\n\r\n\r\ndef test_net():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Conv2D(\r\n        32, (3, 3), padding='same', input_shape=(32, 32, 3)))\r\n    model.add(keras.layers.BatchNormalization())\r\n    model.add(keras.layers.Activation('relu'))\r\n\r\n    model.add(keras.layers.Conv2D(10, (1, 1), padding='same'))\r\n    model.add(keras.layers.Activation('softmax'))\r\n\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\n    model._name = \"test_net\"\r\n\r\n    return model\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = test_net()\r\n    # When setting per_axis=False it works. When per_axis=True an error occurs.\r\n    model = _get_QAT_model(model, w_bits=8, a_bits=8, symmetric=True,\r\n                           per_axis=True, narrow_range=False, quantizer=QAT_LAST_VALUE)\r\n    print(model.summary())\r\n\r\n```\r\n\r\nError output:\r\n\r\n```\r\n  File \"c:\\Users\\Lucas\\Universitet\\Examensarbete\\cnn-optimization-vt2021\\TEMP_QAT2.py\", line 109, in <module>\r\n    per_axis=True, narrow_range=False, quantizer=QAT_LAST_VALUE)\r\n  File \"c:\\Users\\Lucas\\Universitet\\Examensarbete\\cnn-optimization-vt2021\\TEMP_QAT2.py\", line 82, in _get_QAT_model\r\n    q_aware_model = quantize_apply(annotated_model)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\", line 421, in quantize_apply\r\n    transformed_model, input_tensors=None, clone_function=_quantize)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 426, in clone_model\r\n    model, input_tensors=input_tensors, layer_fn=clone_function)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 335, in _clone_sequential_model\r\n    cloned_model = Sequential(layers=layers, name=model.name)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 142, in __init__\r\n    self.add(layer)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 221, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 926, in __call__\r\n    input_list)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 1117, in _functional_construction_call\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 258, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in user code:\r\n\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize_wrapper.py:144 call  *\r\n        quantized_weight = utils.smart_cond(\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\utils.py:54 smart_cond  *\r\n        pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantizers.py:190 __call__  *\r\n        weights['max_var'],\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quant_ops.py:200 LastValueQuantize  *\r\n        assign_min = tf_compat.assign(min_var, range_min, name='AssignMinLast')\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\compat.py:28 assign  *\r\n        return ref.assign(value, name=name)\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:858 assign  **\r\n        self._shape.assert_is_compatible_with(value_tensor.shape)\r\n    C:\\Users\\Lucas\\.conda\\envs\\env\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1134 assert_is_compatible_with\r\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\r\n\r\n    ValueError: Shapes () and (32,) are incompatible\r\n```", "Hi Lucas,\r\n\r\nSeems that the weight vars are not shaped correctly. Try adding the following class:\r\n\r\n```\r\n class ConvWeightsQuantizer(LastValueQuantizer):\r\n \r\n        def build(self, tensor_shape, name, layer):\r\n           min_weight = layer.add_weight(\r\n               name + '_min',\r\n               shape=(tensor_shape[-1],),\r\n               initializer=tf.keras.initializers.Constant(-6.0),\r\n               trainable=False)\r\n           max_weight = layer.add_weight(\r\n               name + '_max',\r\n               shape=(tensor_shape[-1],),\r\n               initializer=tf.keras.initializers.Constant(6.0),\r\n               trainable=False)\r\n          return {'min_var': min_weight, 'max_var': max_weight}\r\n```\r\n\r\nAnd modifying your ConvQuantizer to \r\n\r\n```\r\nclass ConvQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, ConvWeightsQuantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n```", "Hi. By also setting the DenseQuantizer to use ConvWeightsQuantizer this works! Otherwise I got a similar error but for shape (10,).\r\n\r\nThanks a lot!", "@daverim Hi again. I still seem to have a problem with DepthwiseConv2D layers. When using the WeightsQuantizer above with my DepthwiseQuantizer and a small network with a DepthwiseConv2D layer I get a model with scalar depthwise_kernel_min/max values, instead of vectors.\r\n\r\nI also noted in some other models that DepthwiseConv2D layers do seem to get quantized per-channel, regardless of my per_axis flag. I noted this by looking at floating point weights in a h5 model in Netron, and int weights in the converted tflite model, and then by using the input-activation-scale calculating the bias scales:\r\n\r\n*bias_scale = float_bias / (int_bias * previous_activation_scale)*\r\n\r\nThese scales were indeed different for each bias, and hence the DepthwiseConv2D layers seem to be quantized per-channel.\r\n\r\nHowever, again, that model (and the small test model shared here) both only have scalar depthwise_kernel_min/max values. How can I extract the array of min/max values?\r\n\r\n**New code with test model that has scalar variables:**\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\n\r\n\r\n# Possible quantization aware quantizers:\r\nQAT_ALL_VALUES = tfmot.quantization.keras.quantizers.AllValuesQuantizer\r\nQAT_LAST_VALUE = tfmot.quantization.keras.quantizers.LastValueQuantizer\r\nQAT_MA = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\r\n\r\n\r\ndef _get_QAT_model(model, w_bits, a_bits, symmetric, per_axis, narrow_range, quantizer):\r\n    quantize_apply = tfmot.quantization.keras.quantize_apply\r\n    quantize_model = tfmot.quantization.keras.quantize_model\r\n    quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\r\n    clone_model = tf.keras.models.clone_model\r\n    quantize_scope = tfmot.quantization.keras.quantize_scope\r\n\r\n    class Quantizer(tfmot.quantization.keras.QuantizeConfig):\r\n        # Configure how to quantize weights.\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, tfmot.quantization.keras.quantizers.LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n        def set_quantize_weights(self, layer, quantize_weights):\r\n            # Add this line for each item returned in `get_weights_and_quantizers`\r\n            # , in the same order\r\n            layer.kernel = quantize_weights[0]\r\n\r\n        def set_quantize_activations(self, layer, quantize_activations):\r\n            # Add this line for each item returned in `get_activations_and_quantizers`\r\n            # , in the same order.\r\n            layer.activation = quantize_activations[0]\r\n\r\n        # Configure how to quantize outputs (may be equivalent to activations).\r\n        def get_output_quantizers(self, layer):\r\n            return []\r\n\r\n        def get_config(self):\r\n            return {}\r\n\r\n    class ConvWeightsQuantizer(QAT_LAST_VALUE):\r\n\r\n        def build(self, tensor_shape, name, layer):\r\n            min_weight = layer.add_weight(\r\n                name + '_min',\r\n                shape=(tensor_shape[-1],),\r\n                initializer=tf.keras.initializers.Constant(-6.0),\r\n                trainable=False)\r\n            max_weight = layer.add_weight(\r\n                name + '_max',\r\n                shape=(tensor_shape[-1],),\r\n                initializer=tf.keras.initializers.Constant(6.0),\r\n                trainable=False)\r\n            return {'min_var': min_weight, 'max_var': max_weight}\r\n\r\n    class ConvQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, ConvWeightsQuantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    class DepthwiseQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.depthwise_kernel, ConvWeightsQuantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    class DenseQuantizer(Quantizer):\r\n        # Configure how to quantize weights\r\n        def get_weights_and_quantizers(self, layer):\r\n            return [(layer.kernel, ConvWeightsQuantizer(num_bits=w_bits, symmetric=symmetric, narrow_range=narrow_range, per_axis=per_axis))]\r\n\r\n        # Configure how to quantize activations.\r\n        def get_activations_and_quantizers(self, layer):\r\n            return [(layer.activation, tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=a_bits, symmetric=False, narrow_range=False, per_axis=False))]\r\n\r\n    def quantize_all_layers(layer):\r\n        if isinstance(layer, keras.layers.DepthwiseConv2D):\r\n            return quantize_annotate_layer(layer, quantize_config=DepthwiseQuantizer())\r\n        elif isinstance(layer, keras.layers.Conv2D):\r\n            return quantize_annotate_layer(layer, quantize_config=ConvQuantizer())\r\n        elif isinstance(layer, keras.layers.Dense):\r\n            return quantize_annotate_layer(layer, quantize_config=DenseQuantizer())\r\n        return layer\r\n\r\n    annotated_model = clone_model(\r\n        model,\r\n        clone_function=quantize_all_layers\r\n    )\r\n\r\n    with quantize_scope(\r\n        {'Quantizer': Quantizer},\r\n        {'ConvQuantizer': ConvQuantizer},\r\n        {'DepthwiseQuantizer': DepthwiseQuantizer},\r\n            {'DenseQuantizer': DenseQuantizer}):\r\n        q_aware_model = quantize_apply(annotated_model)\r\n\r\n    return q_aware_model\r\n\r\n\r\ndef test_net():\r\n    model = keras.Sequential()\r\n    model.add(keras.layers.Conv2D(\r\n        32, (3, 3), padding='same', input_shape=(32, 32, 3)))\r\n    model.add(keras.layers.BatchNormalization())\r\n    model.add(keras.layers.Activation('relu'))\r\n\r\n    model.add(keras.layers.DepthwiseConv2D(1, 1))\r\n\r\n    model.add(keras.layers.Flatten())\r\n    model.add(keras.layers.Dense(10, activation='softmax'))\r\n\r\n    model._name = \"test_net\"\r\n\r\n    return model\r\n\r\n\r\ndef print_kernel_info(model):\r\n    for i, layer in enumerate(model.layers):\r\n        if 'depthwise_conv' in layer.name:\r\n            variables = layer.variables\r\n            print(\"kernel_min: \" +\r\n                  str(np.float(variables[4])) + \", kernel_max: \" + str(np.float(variables[5])))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = test_net()\r\n    # When setting per_axis=False it works. When per_axis=True an error occurs.\r\n    model = _get_QAT_model(model, w_bits=8, a_bits=8, symmetric=True,\r\n                           per_axis=True, narrow_range=False, quantizer=QAT_LAST_VALUE)\r\n\r\n    print_kernel_info(model)\r\n```\r\n"]}, {"number": 47853, "title": "Creating ragged tensors is incredibly slow", "body": "I'm running the following code on a Google Colab instance with GPU support enabled:\r\n```\r\ntf.ragged.constant(np.random.randint(10, size = 10_000_000))\r\n```\r\nThe code takes 15 seconds to finish. In comparison, `tf.constant(np.random.randint(10, size = 10_000_000))` takes only 50 milliseconds to finish.", "comments": ["\r\nI am able to replicate the issue reported on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/51cc6e1d071d86111071a731fbf09d26/untitled564.ipynb).", "@bjourne Recently Colab has some problems in accessing `gpu`. So, even when you select `gpu` it is still running in `cpu`. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/6a958d7f505101b1ddac8aec2ca93e6e/untitled564.ipynb).\r\n\r\nAdding the following code to check `gpu` usage throws error\r\n\r\n```\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n```\r\n\r\nCan you please run it on GPU and let us know if there is any issue there. Thanks!", "Yes, same thing with a GPU:\r\n```import tensorflow as tf\r\nimport numpy as np\r\nfrom time import time\r\n\r\ndevice_name = tf.test.gpu_device_name()\r\nif device_name != '/device:GPU:0':\r\n  raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n\r\nst = time()\r\ntf.ragged.constant(np.random.randint(10, size = 10_000_000))\r\nprint(time() - st)\r\n```\r\nThe code takes 14.8 seconds to finish.", "It looks like most of the time is being spent in `_find_scalar_and_max_depth`.  Two options to speed up that function:\r\n\r\n1. short-circuit and return early once we find any scalar value.  (But this may prevent us from giving good error messages if the user supplies an input with scalar values at different nesting depths.)\r\n2. change `np.ndim(pylist) != 0` to `(isinstance(pylist, np.ndarray) and np.ndim(pylist) != 0` -- calling `ndim` seems to take up a fair amount of time.  Not sure if this will break any existing use-cases.\r\n\r\nBut even with these fixes, `tf.ragged.constant` will probably still be significantly slower than `tf.constant` (especially if the input is a numpy array), since `tf.constant` is implemented in c and `tf.ragged.constant` is implemented in Python.\r\n\r\nIf you want to ingest ragged large tensors quickly, and it's possible to get them into a format other than nested python lists, then you might consider using one of the following:\r\n\r\n* The `RaggedTensor` factory methods, such as `tf.ragged.from_row_splits` or `tf.ragged.from_row_lengths`.  (Use `validate=False` to prevent validation ops from being added to the graph, which can slow things down.)\r\n* The `RaggedTensor.from_tensor` method, which has optional `padding` and `lengths` arguments you can use to specify which values are padding.\r\n* If you have a list of numpy arrays (for each row), then you could use `tf.ragged.stack(list_of_numpy_arrays)`.\r\n\r\nAll of these options will be substantially faster than `tf.ragged.constant`.\r\n\r\nWhat is the actual shape of ragged tensors you want to ingest?  This could impact what approach will be best for ingesting them.  Examples:\r\n\r\n- a 2d tensor [dense, ragged] where `dense` and `ragged` are both very large.\r\n- a 3d tensor [dense_1, ragged_1, ragged_2, dense_2], where `dense_1` and `ragged_1` are large, but `ragged_2` and `dense_2` are small.\r\n\r\n(In the example you used for timing, you pass in a 1D list, which can't even be ragged -- RaggedTensors always have rank>1.)\r\n", "It's a 2d-tensor [n, size]. n is 10k and size varies between 100 to 1000.", "I struggle with the same issue, have you managed to improve the efficiency?", "`tf.ragged.stack` works fast for me.\r\n\r\n    tf.ragged.stack([np.random.rand(ii+9999) for ii in range(9999)])\r\nTime:\r\n\r\n    CPU times: user 2.3 s, sys: 3.49 s, total: 5.79 s\r\n    Wall time: 3.31 s", "@bjourne Is this still an issue for you? I think this is intended and @edloper provided couple of suggestions to improve. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yeah it's still an issue! The code I pasted still runs dog slow. ", "I have a list of 2d numpy arrays `(m,n)` where `m` is variable (order of 1000) and `n` is fixed (say 1000). I have around 10000 such arrays. The final ragged tensor should be of shape `(10000, None, 1000)`. `RaggedTensor.stack` is fast however it results into a ragged tensor of `ragged_rank=2`, shape=`(10000, None, None)`. How can I convert this into a tensor which has a ragged rank of 1. `RaggedTensor.constant` gives correct results, however it is incredibly slow.", "> I have a list of 2d numpy arrays `(m,n)` where `m` is variable (order of 1000) and `n` is fixed (say 1000). I have around 10000 such arrays. The final ragged tensor should be of shape `(10000, None, 1000)`. `RaggedTensor.stack` is fast however it results into a ragged tensor of `ragged_rank=2`, shape=`(10000, None, None)`. How can I convert this into a tensor which has a ragged rank of 1. `RaggedTensor.constant` gives correct results, however it is incredibly slow.\r\n\r\nHi @deepakagrawal,\r\nTry using factory class-methods such as:\r\n\r\n- [tf.RaggedTensor.from_value_rowids](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#from_value_rowids), \r\n- [tf.RaggedTensor.from_row_lengths](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#from_row_lengths), and \r\n- [tf.RaggedTensor.from_row_splits](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#from_row_splits).\r\n\r\nSee here for further reference https://www.tensorflow.org/guide/ragged_tensor#constructing_a_ragged_tensor\r\n\r\n\r\n\r\nBelow is an example of using `tf.RaggedTensor.from_row_splits` to convert a `(100000, 1000)` numpy-array into a `(1000, None, 1000)` ragged-tensor. https://colab.research.google.com/drive/1WnyvzoB8oE5vGXq5iYHJAAv7RQDADVU8?usp=sharing\r\n\r\n```\r\nCPU times: user 16.5 ms, sys: 18.6 ms, total: 35.1 ms\r\nWall time: 39.7 ms\r\n```", "@deepakagrawal: +1 to using the `RaggedTensor` factory methods.  In your example, you could convert the numpy arrays into a RaggedTensor using `RaggedTensor.from_row_lengths` as follows:\r\n\r\n```\r\nimport numpy as np\r\nnp_arrays = [np.zeros([i, 1000]) for i in range(1000)]  # example array.\r\n\r\nragged_tensor = tf.RaggedTensor.from_row_lengths(\r\n    values=tf.concat(np_arrays, axis=0),\r\n    row_lengths=[a.shape[0] for a in np_arrays])\r\n```\r\n\r\n", "Thanks, This worked perfectly"]}, {"number": 47849, "title": "C API Binary Release for 2.4.0 Has Invalid Symbol Table", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (also affects other Ubuntu releases, see below)\r\n- TensorFlow installed from (source or binary): C API binary from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz\r\n- TensorFlow version: 2.4.0\r\n- GCC/Compiler version (if compiling from source): 9.3.0-17ubuntu1~20.04\r\n \r\n**Describe the problem:**\r\n\r\nThe binary release of the C API distributed under the link above appears to have a malformed symbol table, which prevents `ld` from being able to link against it when using the versions distributed in Ubuntu 16.04, 18.04, and 20.04. It seems likely that this affects other platforms as well, but I haven't tested those myself.\r\n\r\nFrom https://sourceware.org/bugzilla/show_bug.cgi?id=24857, suggests that newer versions of binutils may have fixed this, but I still see this failure on every version of gcc/ld that I've tried, including ones that should have the fix identified in that issue. That issue also notes that:\r\n\r\n> The real bug is in whatever linker set .dynsym sh_info too low for the shared lib.\r\n\r\nwhich suggests that the root issue here might be in whatever build toolchain is producing the C API binaries that tensorflow is publishing.\r\n\r\nThis has been reported previously in https://github.com/tensorflow/tensorflow/issues/41382, but the resolution there seems to have been \"Use a different linker that doesn't complain about this issue\", which isn't really a solution to the root of the problem. Switching linkers isn't always possible for an existing project, or at least it can require a nontrivial amount of work. It would be better to fix whatever is building this malformed symbol table.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Create a directory and put the following in `Dockerfile` in that directory.\r\n\r\n```dockerfile\r\nFROM ubuntu:20.04\r\n\r\nRUN apt-get update && apt-get install -y g++ wget\r\n\r\nRUN mkdir /build\r\nRUN cd /build && wget -qO- https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-2.4.0.tar.gz | tar -xvz\r\n\r\nCOPY repro.cc repro.cc\r\n```\r\n\r\nwith the following file in `repro.cc` next to the Dockerfile:\r\n\r\n```c++\r\n#include <stdio.h>\r\n#include <tensorflow/c/c_api.h>\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n  return 0;\r\n}\r\n```\r\n\r\n2. Build and run the resulting image, and attempt to compile the repro file.\r\n\r\n```bash\r\n$ docker build . -t repro:20.04\r\n$ docker run -it repro:20.04\r\nroot@fc196abb4665:/# gcc -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 857 (>= sh_info of 2)\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 1944 (>= sh_info of 2)\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2314 (>= sh_info of 2)\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2502 (>= sh_info of 2)\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2724 (>= sh_info of 2)\r\n/usr/bin/ld: /build/lib/libtensorflow.so: .dynsym local symbol at index 2725 (>= sh_info of 2)\r\n/usr/bin/ld: /tmp/ccomNmut.o: in function `main':\r\nrepro.cc:(.text+0x9): undefined reference to `TF_Version'\r\ncollect2: error: ld returned 1 exit status\r\n```\r\n\r\n**Additional Info:**\r\n\r\nI can repro this issue on all recent Ubuntu versions I've tried (it's easy to test different versions by changing the tag of the base image in the Dockerfile posted above).\r\n\r\nAs noted in the issue linked above, the easiest workaround for this issue seems to be to use a different linker. `gold` happily accepts the release binary without complaint:\r\n\r\n```bash\r\nroot@fc196abb4665:/ gcc -fuse-ld=gold -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc\r\nroot@fc196abb4665:/ ./a.out\r\nHello from TensorFlow C library version 2.4.0\r\n```\r\n\r\n`lld` builds successfully, but warns (and provides more info about the specific symbols that seem to be misplaced):\r\n\r\n```bash\r\nroot@fc196abb4665:/ gcc -fuse-ld=lld -I /build/include/ -L /build/lib -ltensorflow -Wl,-rpath /build/lib repro.cc\r\nld.lld: warning: found local symbol '_ZN4absl14lts_2020_02_2518container_internal18global_next_sampleE' in global part of symbol table in file /build/lib/libtensorflow.so\r\nld.lld: warning: found local symbol 'tl_thread_handler_context' in global part of symbol table in file /build/lib/libtensorflow.so\r\nld.lld: warning: found local symbol '_ZN10tensorflow27ScopedMemoryDebugAnnotation11annotation_E' in global part of symbol table in file /build/lib/libtensorflow.so\r\nld.lld: warning: found local symbol 'tl_logging_thread_id' in global part of symbol table in file /build/lib/libtensorflow.so\r\nld.lld: warning: found local symbol '_ZN9grpc_core7ExecCtx9exec_ctx_E' in global part of symbol table in file /build/lib/libtensorflow.so\r\nld.lld: warning: found local symbol '_ZN9grpc_core26ApplicationCallbackExecCtx18callback_exec_ctx_E' in global part of symbol table in file /build/lib/libtensorflow.so\r\n```\r\n\r\nHowever, as noted above, not everyone can easily swap out the linker in their project for another one, and the root issue here seems to be the actual release file, rather than the linker that's rejecting it.", "comments": ["**UPDATE:** I can reproduce the issue with the tensorflow release binaries by building tensorflow from source using the google-provided bazel base image. I'm using the following docker image / build script:\r\n\r\n`Dockerfile-tensorflow`\r\n\r\n```dockerfile\r\n# -*-dockerfile-*-\r\nFROM l.gcr.io/google/bazel:3.1.0\r\n\r\nARG USER_ID=1000\r\nARG GROUP_ID=1000\r\n\r\n# The tensorflow bazel build depends on having access to numpy.\r\nRUN apt-get update && apt-get install --yes python3-numpy\r\n\r\nRUN groupadd -g ${GROUP_ID} builder\r\nRUN useradd --create-home -u ${USER_ID} -g ${GROUP_ID} builder\r\n\r\nUSER builder\r\n\r\nCOPY --chown=builder:builder tensorflow /src/tensorflow\r\n\r\nWORKDIR /src/tensorflow\r\n\r\nENV PYTHON_BIN_PATH=/usr/bin/python3\r\n```\r\n\r\n`build_tensorflow.sh`\r\n\r\n```bash\r\n#!/bin/bash\r\n\r\nset -euo pipefail\r\n\r\nrm -rf ./output\r\nmkdir ./output\r\n\r\ncache_dir=$(realpath ~/.cache/bazel-third_party_build)\r\noutput_dir=$(realpath ./output)\r\ndocker_build_dir=\"$(mktemp -d -t tensorflow_docker_build-XXXXXXXX)\"\r\n\r\necho \"==============================\"\r\necho \"Building tensorflow...\"\r\necho \"  cache_dir=$cache_dir\"\r\necho \"  output_dir=$output_dir\"\r\necho \"  docker_build_dir=$docker_build_dir\"\r\necho \"==============================\"\r\n\r\n# Build docker container in a temporary subdir\r\ncp Dockerfile-tensorflow $docker_build_dir/Dockerfile\r\ncp -r tensorflow $docker_build_dir/tensorflow\r\npushd $docker_build_dir\r\ndocker build --build-arg USER_ID=$(id -u) --build-arg GROUP_ID=$(id -g) . -t tensorflow-builder\r\npopd\r\n\r\ndocker run \\\r\n       -it \\\r\n       -v $output_dir:/home/builder/output \\\r\n       -v $cache_dir:/home/builder/.cache/bazel \\\r\n       --entrypoint '/bin/bash' \\\r\n       tensorflow-builder:latest \\\r\n       -c 'bazel build \\\r\n       --config=avx_linux --config=v2 --config=monolithic \\\r\n       --config=noaws --config=nogcp --config=nohdfs --config=nonccl \\\r\n       //tensorflow/tools/lib_package:libtensorflow \\\r\n       && cp ./bazel-out/k8-opt/bin/tensorflow/tools/lib_package/libtensorflow.tar.gz /home/builder/output'\r\n\r\n# Clean up\r\nrm -rf \"$docker_build_dir\"\r\n\r\necho \"============================================\"\r\necho \"Built tensorflow to $output_dir\"\r\necho \"============================================\"\r\n```\r\n\r\nif I try to build a program against the `.so` produced in `libtensorflow.tar.gz`, I get the same errors as the ones described above.", "It looks like the root issue might be that the build above uses `gold` as the default linker, which seems to produce a binary that `ld`/`lld` aren't happy with (but `gold` is fine with, unsurprisingly). If I pass `--linkopt=\"-fuse-ld=ld\"` to the `bazel build` invocation, then the resulting binary seems to work as expected.\r\n\r\nI'm not quite sure what's causing the bazel build to use gold by default. I see that we're passing `-fuse-ld=gold` as a link flag by default in the [gcc7_manylinux2010](https://github.com/tensorflow/tensorflow/blob/635b85be0c89e587887b32ebc488f6bf782a90e4/third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010/cc_toolchain_config.bzl#L1400-L1408) toolchain, but it looks like the linux cpu release build sets `build:release_cpu_linux --crosstool_top=//third_party/toolchains/preconfig/ubuntu16.04/gcc7_manylinux2010-nvcc-cuda10.1:toolchain` and that toolchain doesn't mention gold anywhere. It's definitely possible I'm missing something here though: this is the first time I've tried to use bazel for anything."]}, {"number": 47838, "title": "Compressed pruned model is the same size as compressed baseline model", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\nCode from pruning documentation:\r\nhttps://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n\r\n-   **TensorFlow installed from (source or binary)**:\r\nAnaconda Navigator\r\n\r\n-   **TensorFlow version (use command below)**:\r\ntensorflow-gpu 2.3.0\r\n\r\n-   **Python version**:\r\n3.7.9\r\n\r\n-   **CUDA/cuDNN version**:\r\ncudatoolkit 10.1.243\r\ncudnn 7.6.5\r\n\r\n-   **GPU model and memory**:\r\nGTX 1070 8GB\r\n\r\n\r\n### Describe the problem\r\nI followed the code provided in the documentation and pruned a model. I expected the compressed pruned model to be of smaller size than the baseline. [Here](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#create_3x_smaller_models_from_pruning), the baseline is compared to the pruned model, and is definitely smaller.\r\nIn my case, they are both the same size.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_model_optimization as tfmot\r\n\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\nimport numpy as np\r\nimport tempfile\r\nimport os\r\nimport zipfile\r\n\r\n\r\ndef get_gzipped_model_size(model):\r\n  # Returns size of gzipped model, in bytes.\r\n  import os\r\n  import zipfile\r\n\r\n  _, keras_file = tempfile.mkstemp('.h5')\r\n  model.save(keras_file, include_optimizer=False)\r\n\r\n  _, zipped_file = tempfile.mkstemp('.zip')\r\n  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\r\n    f.write(keras_file)\r\n\r\n  return os.path.getsize(zipped_file)\r\n\r\n\r\ninput_shape = [20]\r\nx_train = np.random.randn(1, 20).astype(np.float32)\r\ny_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\r\n\r\n\r\ndef setup_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Dense(20, input_shape=input_shape),\r\n      tf.keras.layers.Flatten()\r\n  ])\r\n  return model\r\n\r\ndef setup_pretrained_weights():\r\n  model = setup_model()\r\n\r\n  model.compile(\r\n      loss=tf.keras.losses.categorical_crossentropy,\r\n      optimizer='adam',\r\n      metrics=['accuracy']\r\n  )\r\n\r\n  model.fit(x_train, y_train)\r\n\r\n  _, pretrained_weights = tempfile.mkstemp('.tf')\r\n\r\n  model.save_weights(pretrained_weights)\r\n\r\n  return pretrained_weights\r\n\r\n\r\npretrained_weights = setup_pretrained_weights()\r\n\r\n\r\ndef test():\r\n    base_model = setup_model()\r\n    base_model.load_weights(pretrained_weights)\r\n    model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(base_model)\r\n\r\n    model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\r\n\r\n    print(\"Size of gzipped baseline model: %.2f bytes\" % (get_gzipped_model_size(base_model)))\r\n    print(\"Size of gzipped pruned model without stripping: %.2f bytes\" % (get_gzipped_model_size(model_for_pruning)))\r\n    print(\"Size of gzipped pruned model with stripping: %.2f bytes\" % (get_gzipped_model_size(model_for_export)))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test()\r\n```\r\n\r\n\r\n**Output:**\r\n\r\n_Size of gzipped baseline model: 2935.00 bytes_\r\n_Size of gzipped pruned model without stripping: 3360.00 bytes_\r\n_Size of gzipped pruned model with stripping: 2935.00 bytes_", "comments": ["Fyi @daverim ", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3f87a23e8149a052e552d00145327dd3/47838.ipynb). Thanks!", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/51d91ac138dc53f1e07274dba6c8a820/untitled163.ipynb#scrollTo=LlXcmXPJmh9p)..Thanks !"]}, {"number": 47819, "title": "UnboundLocalError: local variable 'kwargs' referenced before assignment", "body": "- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: RTX 3070\r\n\r\nWhen I try load model from json with ``` model_from_json()``` from model with loss added by ```add_loss()``` and ```tf.add_n()```  method I get:\r\n\r\n```\r\nUnboundLocalError                         Traceback (most recent call last)\r\n\r\n<ipython-input-1-ebe792783517> in <module>()\r\n     36 json_file.close()\r\n     37 \r\n---> 38 loaded_model = model_from_json(loaded_model_json)  # UnboundLocalError: local variable 'kwargs' referenced before assignment\r\n     39 print(\"model_from_json ok\")\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/model_config.py in model_from_json(json_string, custom_objects)\r\n    129   config = json.loads(json_string)\r\n    130   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top\r\n--> 131   return deserialize(config, custom_objects=custom_objects)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)\r\n    175       module_objects=LOCAL.ALL_OBJECTS,\r\n    176       custom_objects=custom_objects,\r\n--> 177       printable_module_name='layer')\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)\r\n    356             custom_objects=dict(\r\n    357                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n--> 358                 list(custom_objects.items())))\r\n    359       with CustomObjectScope(custom_objects):\r\n    360         return cls.from_config(cls_config)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in from_config(cls, config, custom_objects)\r\n    667     \"\"\"\r\n    668     input_tensors, output_tensors, created_layers = reconstruct_from_config(\r\n--> 669         config, custom_objects)\r\n    670     model = cls(inputs=input_tensors, outputs=output_tensors,\r\n    671                 name=config.get('name'))\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1283       if layer in unprocessed_nodes:\r\n   1284         for node_data in unprocessed_nodes.pop(layer):\r\n-> 1285           process_node(layer, node_data)\r\n   1286 \r\n   1287   input_tensors = []\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)\r\n   1231         input_tensors = (\r\n   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))\r\n-> 1233       output_tensors = layer(input_tensors, **kwargs)\r\n   1234 \r\n   1235       # Update node index map.\r\n\r\nUnboundLocalError: local variable 'kwargs' referenced before assignment\r\n```\r\n\r\n", "comments": ["Code to replicate issue:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.keras.models import model_from_json\r\nfrom tensorflow.keras import Input, Model, regularizers, layers\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n\r\ninputs = Input(shape=(10,))\r\nd = layers.Dense(10, kernel_initializer='ones')\r\nx = d(inputs)\r\noutputs = layers.Dense(1)(x)\r\nmodel = Model(inputs, outputs)\r\n  \r\nmodel.add_loss(tf.reduce_mean(outputs))\r\n\r\nreg_losses = [\r\n    tf.cast(tf.size(input=w), tf.float32) for w in model.trainable_weights\r\n    ]\r\nmodel.add_loss(tf.add_n(reg_losses))\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer=\"adam\", loss=[None] * len(model.outputs))\r\nmodel.fit(np.random.random((2, 10)))\r\n\r\nmodel_json = model.to_json()\r\njson_filename = \"model.json\"\r\nwith open(json_filename, \"w\") as json_file:\r\n    json_file.write(model_json)\r\nmodel.save_weights(\"model.h5\")\r\n\r\njson_file = open(json_filename, 'r')\r\nloaded_model_json = json_file.read()\r\njson_file.close()\r\n\r\nloaded_model = model_from_json(loaded_model_json)  # UnboundLocalError: local variable 'kwargs' referenced before assignment\r\nprint(\"model_from_json ok\")\r\n```", "I am able to reproduce this issue on tf 2.3, 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8c0a1907110db7311a310579b73f4b78/untitled563.ipynb).\r\nThanks!", "@kiflowb777,\r\nCan you please confirm if you are trying to Migrate the Code from `Tensorflow 1.x` to `2.x`? If not, can you try `decorating` your `Code` with `tf.function` instead of `tf.compat.v1.disable_eager_execution()` and check if the issue persists? Thanks!\r\n", "@rmothukuru \r\nYes, I used a script for migration, training and solution work well, but the problem appears when loading the model from json.\r\nMy project is based on a **MaskRCNN** that requires the use of a ```tf.compat.v1.disable_eager_execution()```,  without it a training does not work. \r\nWith ```tf.function``` MaskRCNN doesn't work.\r\nhttps://github.com/matterport/Mask_RCNN", "I am still able to reproduce the error on TensorFlow 2.5.0", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5c1ea1cbc834a7329cc27f325eee2c51/untitled162.ipynb)..Thanks !", "Same problem on tf 2.8. Exactly from deserialize part. What is really Frustrating is that the tf.compat.v1.disable_eager_execution() have to be required when you want to get some gradient information on **Keras model.** Because **you do not** give the GradientTape support on keras. \r\n\r\n> @kiflowb777, Can you please confirm if you are trying to Migrate the Code from `Tensorflow 1.x` to `2.x`? If not, can you try `decorating` your `Code` with `tf.function` instead of `tf.compat.v1.disable_eager_execution()` and check if the issue persists? Thanks!\r\n\r\n"]}, {"number": 47816, "title": "tf.keras Lambda layer fails to infer output shape when run_functions_eagerly is set to True", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory: 2048 Ti\r\n\r\n**Describe the current behavior**\r\nBuilding a keras model in functional mode with a lambda layer is failing in eager mode.\r\nThe lambda layer is trying to infer its output spec. To do so, it runs the `tf.function` decorated lambda function, but without using autograph and ends up raising an `OperatorNotAllowedInGraphError` exception.\r\n\r\nWe can bypass the problem by temporarily switching off the eager mode during the model construction.\r\n\r\n**Describe the expected behavior**\r\nThe layer should be able to infer the output shape in eager mode.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\n\r\ndef build_model_fail():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n    output = tf.keras.layers.Lambda(lambda_fn)(input)\r\n    return tf.keras.Model(inputs=input, outputs=output)\r\n\r\n\r\ndef build_model_success():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n\r\n    # temporarily setting off the eager execution\r\n    # allows the lambda layer to infer the output spec.\r\n    tf.config.run_functions_eagerly(False)\r\n    output = tf.keras.layers.Lambda(lambda_fn)(input)\r\n\r\n    # switching back to eager for runtime debugging\r\n    tf.config.run_functions_eagerly(True)\r\n\r\n    return tf.keras.Model(inputs=input, outputs=output)\r\n\r\n\r\n@tf.function\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # this works\r\n    model = build_model_success()\r\n    model(5)\r\n\r\n    # this doesn't work\r\n    model = build_model_fail()\r\n    model(5)\r\n\r\n```\r\n\r\n", "comments": ["@nicolaspi \r\nI ran the code shared, please refer to the [gist](https://colab.research.google.com/gist/Saduf2019/1f0c20e4a980dcc01aff9d70b15c4c60/untitled563.ipynb) and confirm.", "> @nicolaspi\r\n> I ran the code shared, please refer to the [gist](https://colab.research.google.com/gist/Saduf2019/1f0c20e4a980dcc01aff9d70b15c4c60/untitled563.ipynb) and confirm.\r\n\r\nI confirm. It is also worthy to note that with the successful build the lambda is run in eager mode and can be debugged.", "@ymodak \r\nI am able to replicate the issue reported on tf 2.3,2.4 ad nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/19e2d3271b93ef10c9c5799123bb449c/untitled564.ipynb).", "It happens also when you are not using explicit lambda:\r\nhttps://github.com/tensorflow/tensorflow/issues/44906#issuecomment-729868988", "@nicolaspi can you try:\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\n\r\ndef build_model_fail():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n    output = tf.keras.layers.Lambda(converted_lambda)(input)\r\n    return tf.keras.Model(inputs=input, outputs=output)\r\n\r\n\r\ndef build_model_success():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n\r\n    # temporarily setting off the eager execution\r\n    # allows the lambda layer to infer the output spec.\r\n    tf.config.run_functions_eagerly(False)\r\n    output = tf.keras.layers.Lambda(converted_lambda)(input)\r\n\r\n    # switching back to eager for runtime debugging\r\n    tf.config.run_functions_eagerly(True)\r\n\r\n    return tf.keras.Model(inputs=input, outputs=output)\r\n\r\nconverted_lambda = tf.autograph.to_graph(lambda_fn.python_function)\r\n\r\n@tf.function\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # this works\r\n    model = build_model_success()\r\n    model(5)\r\n\r\n    # this doesn't work\r\n    model = build_model_fail()\r\n    model(5)\r\n```\r\n@mdanatg  What kind of solution/contribution do we want to collect here?", "> @nicolaspi can you try:\r\n\r\nYour code runs and `lambda_fn` is executed eagerly, but the python code that is ran is the compiled autograph version exported as a tmp file:\r\n```python\r\ndef outer_factory():\r\n\r\n    def inner_factory(ag__):\r\n\r\n        def tf__lambda_fn(input):\r\n            with ag__.FunctionScope('lambda_fn', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\r\n                do_return = False\r\n                retval_ = ag__.UndefinedReturnValue()\r\n                i = ag__.converted_call(ag__.ld(tf).constant, (0,), dict(dtype=ag__.ld(tf).int32), fscope)\r\n\r\n                def get_state():\r\n                    return (i,)\r\n\r\n                def set_state(vars_):\r\n                    nonlocal i\r\n                    (i,) = vars_\r\n\r\n                def loop_body():\r\n                    nonlocal i\r\n                    ag__.ld(print)('eager iteration', ag__.ld(i))\r\n                    ag__.converted_call(ag__.ld(tf).print, ('loop iteration', ag__.ld(i)), None, fscope)\r\n                    i = (ag__.ld(i) + 1)\r\n\r\n                def loop_test():\r\n                    return (ag__.ld(i) < ag__.ld(input))\r\n                ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('i',), {})\r\n                try:\r\n                    do_return = True\r\n                    retval_ = ag__.ld(input)\r\n                except:\r\n                    do_return = False\r\n                    raise\r\n                return fscope.ret(retval_, do_return)\r\n        return tf__lambda_fn\r\n    return inner_factory\r\n``` \r\nIt would be useful to be able to run the original code eagerly for debugging purposes.\r\n", "Have you tried with:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef build_model_fail():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n    output = tf.keras.layers.Lambda(my_lambda_func)(input)\r\n    model = tf.keras.Model(inputs=input, outputs=output)\r\n    return model\r\n\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n\r\n\r\ndef my_lambda_func(x):\r\n\r\n    return tf.py_function(lambda_fn, [x], tf.int32)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = build_model_fail()\r\n    model(5)\r\n```", "Thanks, but you have hint a better solution using `tf.autograph.to_graph` that might solve the issue in the general case:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import func_graph\r\nfrom tensorflow.python.keras.engine.keras_tensor import KerasTensor\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\n\r\ndef build_model():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n    output_shape = infer_output(lambda_fn, input)\r\n    output = tf.keras.layers.Lambda(lambda_fn,\r\n                                    output_shape=output_shape,\r\n                                    dynamic=True)(input)\r\n    model = tf.keras.Model(inputs=input, outputs=output)\r\n    return model\r\n\r\n\r\ndef infer_output(func, input: KerasTensor):\r\n    scratch_graph = func_graph.FuncGraph(__name__ + '_scratch_graph')\r\n    _func = tf.autograph.to_graph(func.python_function)\r\n    with scratch_graph.as_default():\r\n        output = _func(input._to_placeholder())\r\n    return output.shape\r\n\r\n\r\n@tf.function\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        print(\"eager iteration\", i)\r\n        tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = build_model()\r\n    model(5)\r\n\r\n```\r\n\r\nNow this works both in graph and eager mode (with proper debugging capabilities). I suggest the `infer_output` logic using the autograph conversion step to be embedded in `Lambda`.", "For lambda with dynamic we have also another issue https://github.com/tensorflow/tensorflow/issues/44906", "Yes, but do you agree that I used `dynamic=True` only as a way to specify the output's shape using the public API (for the POC) and that this is not a necessary condition?", "> dynamic: Set this to True if your layer should only be run eagerly, and should not be used to generate a static computation graph. This would be the case for a Tree-RNN or a recursive network, for example, or generally for any layer that manipulates tensors using Python control flow.\n\nThe current documentation in master still specify dynamic also for layers with Python control flow.", "Here is a POC without dynamic:\r\n\r\n```python \r\n\r\nimport tensorflow as tf\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\n\r\ndef _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n    function_fn = self.function\r\n    self.function = tf.autograph.to_graph(self.function.python_function)\r\n    output = super(type(self),\r\n                   self)._infer_output_signature(inputs, args, kwargs,\r\n                                                 input_masks)\r\n    self.function = function_fn\r\n    return output\r\n\r\n\r\ntf.keras.layers.Lambda._infer_output_signature = _infer_output_signature\r\n\r\n\r\ndef build_model():\r\n    input = tf.keras.Input(dtype=tf.int32, shape=(), batch_size=1)\r\n    output = tf.keras.layers.Lambda(lambda_fn)(input)\r\n    model = tf.keras.Model(inputs=input, outputs=output)\r\n    return model\r\n\r\n\r\n@tf.function\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        print(\"eager iteration\", i)\r\n        tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = build_model()\r\n    model(5)\r\n\r\n```", "In you last example are you running always in eager mode?\r\n\r\n```\r\ndef lambda_fn(input):\r\n    i = tf.constant(0, dtype=tf.int32)\r\n    while i < input:\r\n        if tf.executing_eagerly():\r\n          print(\"eager iteration\", i)\r\n        else:\r\n          tf.print(\"loop iteration\", i)\r\n        i = i + 1\r\n    return input\r\n```", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/0a188d7f7dba62b9d8117113f5af7a47/untitled222.ipynb) ..Thanks !"]}, {"number": 47815, "title": "TensorFlow 2.4.1 offline build fails", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.2.89/7\r\n- GPU model and memory: V100 \r\n\r\n**Describe the problem**\r\n\r\nI try to do an offline compilation of TensorFlow. I am able to partially circumvent dependencies download problems through the `--distdir` Bazel flag (locating third parties I got in `./deps` directory):\r\n\r\n```\r\nbazel build --config=mkl --config=opt --distdir=deps //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nHowever, it fails first on two `bazel_rules` retrievals (`io_bazel_rules_go` and `io_bazel_rules_docker`) but I corrected these issues with the following patch in `WORKSPACE`:\r\n\r\n```\r\ndiff --git a/WORKSPACE b/WORKSPACE\r\nindex 9db1d9b80eb..4284b9834cb 100644\r\n--- a/WORKSPACE\r\n+++ b/WORKSPACE\r\n@@ -20,6 +20,40 @@ tf_repositories()\r\n \r\n register_toolchains(\"@local_config_python//:py_toolchain\")\r\n \r\n+# workaround for git fetch bazel_rules_go\r\n+load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\n+\r\n+http_archive(\r\n+    name = \"io_bazel_rules_go\",\r\n+    sha256 = \"7c10271940c6bce577d51a075ae77728964db285dac0a46614a7934dc34303e6\",\r\n+    urls = [\r\n+        \"https://mirror.bazel.build/github.com/bazelbuild/rules_go/releases/download/v0.26.0/rules_go-v0.26.0.tar.gz\",\r\n+        \"https://github.com/bazelbuild/rules_go/releases/download/v0.26.0/rules_go-v0.26.0.tar.gz\",\r\n+    ],\r\n+)\r\n+\r\n+load(\"@io_bazel_rules_go//go:deps.bzl\", \"go_register_toolchains\", \"go_rules_dependencies\")\r\n+\r\n+go_rules_dependencies()\r\n+\r\n+go_register_toolchains(version = \"1.16\")\r\n+##\r\n+\r\n+# workaround for git fetch bazel_rules_docker\r\n+load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\r\n+\r\n+http_archive(\r\n+    name = \"io_bazel_rules_docker\",\r\n+    sha256 = \"1698624e878b0607052ae6131aa216d45ebb63871ec497f26c67455b34119c80\",\r\n+    strip_prefix = \"rules_docker-0.15.0\",\r\n+    urls = [\"https://github.com/bazelbuild/rules_docker/releases/download/v0.15.0/rules_docker-v0.15.0.tar.gz\"],\r\n+)\r\n+\r\n+load(\"@io_bazel_rules_docker//toolchains/docker:toolchain.bzl\",\r\n+    docker_toolchain_configure=\"toolchain_configure\"\r\n+)\r\n+##\r\n+\r\n load(\"@io_bazel_rules_closure//closure:defs.bzl\", \"closure_repositories\")\r\n \r\n closure_repositories()\r\n```\r\nAnd now I am stuck in getting `go_sdk`:\r\n\r\n```\r\nINFO: Repository go_sdk instantiated at:\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:39:23: in <toplevel>\r\n  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr\r\nivate/sdk.bzl:453:28: in go_register_toolchains\r\n  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr\r\nivate/sdk.bzl:129:21: in go_download_sdk\r\nRepository rule _go_download_sdk defined at:\r\n  /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/pr\r\nivate/sdk.bzl:116:35: in <toplevel>\r\nWARNING: Download from https://golang.org/dl/?mode=json&include=all failed: class java.io.IOException \r\nconnect timed out\r\nWARNING: Download from https://golang.google.cn/dl/?mode=json&include=all failed: class java.io.IOExce\r\nption connect timed out\r\nERROR: An error occurred during the fetch of repository 'go_sdk':\r\n   Traceback (most recent call last):\r\n        File \"/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_ru\r\nles_go/go/private/sdk.bzl\", line 70, column 21, in _go_download_sdk_impl\r\n                ctx.download(\r\nError in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=al\r\nl, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3\r\nd3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out\r\nINFO: Repository nasm instantiated at:\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:105:27: in tf_repositories\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:58:9: in initialize_third_party\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/nasm/workspace.bzl:6:29: in repo\r\nRepository rule third_party_http_archive defined at:\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/repo.bzl:216:43: in <toplevel>\r\nINFO: Repository icu instantiated at:\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/WORKSPACE:19:16: in <toplevel>\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:105:27: in tf_repositories\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/tensorflow/workspace.bzl:55:8: in initialize_third_party\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/icu/workspace.bzl:11:29: in repo\r\nRepository rule third_party_http_archive defined at:\r\n  /<path>/<user>/tensorflow-2.4/tensorflow/third_party/repo.bzl:216:43: in <toplevel>\r\nInternal error thrown during build. Printing stack trace: java.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):\r\n        File \"/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl\", line 70, column 21, in _go_download_sdk_impl\r\n                ctx.download(\r\nError in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])\r\n[...]\r\nFAILED: Build did NOT complete successfully (229 packages loaded, 3879 targets configured)\r\nInternal error thrown during build. Printing stack trace: java.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):\r\n        File \"/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl\", line 70, column 21, in _go_download_sdk_impl\r\n                ctx.download(\r\nError in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])\r\n[...]\r\njava.lang.IllegalStateException: Unexpected analysis error: ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]} -> ErrorInfo{exception=Traceback (most recent call last):\r\n        File \"/dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/io_bazel_rules_go/go/private/sdk.bzl\", line 70, column 21, in _go_download_sdk_impl\r\n                ctx.download(\r\nError in download: java.io.IOException: Error downloading [https://golang.org/dl/?mode=json&include=all, https://golang.google.cn/dl/?mode=json&include=all] to /dev/shm/cache/bazel/_bazel_<user>/5ce46efa3d3981c979b373758ad421d2/external/go_sdk/versions.json: connect timed out, rootCauses=[REPOSITORY_DIRECTORY:@go_sdk], cycles=[], isCatastrophic=false, rootCauseOfException=REPOSITORY_DIRECTORY:@go_sdk, isDirectlyTransient=false, isTransitivelyTransient=true}, ([ConfiguredTargetKey{label=//tensorflow/tools/pip_package:build_pip_package, config=BuildConfigurationValue.Key[5571dcfb554a54b9da4a644e7486ba4a135325361c2a73df816ddb7739d121f6]}])\r\n[...]\r\nFAILED: Build did NOT complete successfully (229 packages loaded, 3879 targets configured)\r\n```\r\nFrom what I get, it comes from Bazel internals and TensorFlow dependencies to `io_bazel_rules_go` (that is a bit hard to find since it is not directly in TensorFlow sources, or so it seems).\r\n\r\nIs there anyway to circumvent this issue and to build TensorFlow offline?", "comments": ["OS Platform and Distribution (e.g., Linux Ubuntu 20.04): RHEL 7.7\r\nTensorFlow version: 2.4.1\r\nPython version: 3.8.5\r\nCPU \r\nNVIDIA Corporation GP102\r\nI try to run this code \r\npython -m CycleGAN_TensorFlow.create_cyclegan_dataset --image_path_a=folder_a --image_path_b=folder_b --dataset_name=\"horse2zebra_train\" --do_shuffle=0\r\n\r\nI have this error \r\nRuntimeError: Input pipelines based on Queues are not supported when eager execution is enabled. Please use tf.data to ingest data into your model instead.", "> OS Platform and Distribution (e.g., Linux Ubuntu 20.04): RHEL 7.7\r\n> TensorFlow version: 2.4.1\r\n> Python version: 3.8.5\r\n> CPU\r\n> NVIDIA Corporation GP102\r\n> I try to run this code\r\n> python -m CycleGAN_TensorFlow.create_cyclegan_dataset --image_path_a=folder_a --image_path_b=folder_b --dataset_name=\"horse2zebra_train\" --do_shuffle=0\r\n> \r\n> I have this error\r\n> RuntimeError: Input pipelines based on Queues are not supported when eager execution is enabled. Please use tf.data to ingest data into your model instead.\r\n\r\nYou should open another issue as it is not related with the original one.", "I'm suffering from the same issue. any updates? @angerson "]}, {"number": 47806, "title": "TFLM Magic Wand example perform late ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Himax WE-1\r\n\r\nHi, \r\n\r\nI would like to ask for comments from developers who have implemented the magic wand example.\r\n\r\nI ran the magic wand example with a Himax WE-1 board. However, there was a problem with slow reaction.\r\n1. After executing the program, it is not recognized even if one of the wing ring slopes is executed continuously. For example, running the wing several times didn't bring up anything in the prompt window.\r\n\r\n2. As it still did not recognize, I restarted by pressing the reset button. So, the previously entered action was continuously displayed. In other words, when the program was executed, it could not be launched and was pushed and operated. For example, after entering the wing multiple times in the previous operation, the prompt window did not appear, so as soon as the reset button was pressed, the wing was launched twice in succession.\r\n\r\nWhen running other examples supported by the Himax board, it does not appear to be a problem in the prompt window as there was no problem.\r\n\r\nI would like to hear from developers who are having the same problem as me or who have no problem when running magic wand.\r\n\r\nThanks\r\n\r\n\r\n\r\n", "comments": []}, {"number": 47791, "title": "Inconsistent Embedding Layer functionality when initialized under MirroredStrategy()", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:  GeForce GTC 1080 Ti - 12 Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n \r\nI'm passing in a tensor to an embedding layer. The tensor contains a value greater than the vocab size specified when initializing the embedding layer. This throws an error, as expected, when the layer is initialized outside the `tf.distribute.MirroredStrategy()`. In other words, if I create a model on a CPU machine or a single GPU machine, passing an invalid tensor throws an error. This is the correct behavior. \r\n\r\nHowever, if I pass in this tensor under the MirroredStrategy() then the error is no longer thrown and the model continues to train\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe expected behavior is that an error should be thrown if an invalid tensor (i.e. a tensor with vocab size greater than the specified value to an embedding layer) is passed to an embedding layer regardless of the strategy used for data parallelism. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import Sequential, load_model, Model \r\n\r\nembed_dim = 64; maxlen = 152; vocab_size = 8\r\nK.clear_session()\r\nX_tk = np.random.randint(1, vocab_size, (10, 152))\r\nX_mask_tk = np.random.randint(1, vocab_size + 1, (10, 152)) #The +1 is for the mask token\r\nl = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n# print(l(X_tk)) #this works\r\n# print(l(X_mask_tk)) #this doesn't work\r\n\r\nmodel = keras.Sequential([layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])\r\n# print(model(X_tk)) #this works\r\n# print(model(X_mask_tk)) #this doesn't works\r\n\r\nmodel2 = keras.Sequential([layers.Input(shape=(maxlen,)), layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])\r\nmodel2.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')\r\n# model2.fit(X_mask_tk, X_tk) #This doesn't work\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope(): #im using 2 gpus but I reckon this issue would occur even with 1 gpu\r\n\r\n    model3 = keras.Sequential([layers.Input(shape=(maxlen,)), layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)])\r\n    model3.compile(optimizer='Adam', loss='sparse_categorical_crossentropy')\r\nmodel3.fit(X_mask_tk, X_tk) #This works\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis should be the expected error when running `model3.fit(...)`:\r\n```\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  indices[3,21] = 8 is not in [0, 8)\r\n\t [[node sequential_1/embedding_2/embedding_lookup (defined at <ipython-input-8-1c81a1dbdc4a>:25) ]]\r\n\t [[sequential_1/embedding_2/embedding_lookup/_24]]\r\n  (1) Invalid argument:  indices[3,21] = 8 is not in [0, 8)\r\n\t [[node sequential_1/embedding_2/embedding_lookup (defined at <ipython-input-8-1c81a1dbdc4a>:25) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_25411]\r\n```\r\n", "comments": ["@nikitamaia  any update here?", "Hi @UCRajkumar, I was able to reproduce the issue on a colab GPU runtime, so thank you for providing the reproducible code. No update for now, but will keep this thread updated if something changes.", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/42f037e4b64cca0e9ad3fccec01a657c/untitled199.ipynb)..Thanks !"]}, {"number": 47748, "title": "C API gradient tape support", "body": "**System information**\r\n- TensorFlow version: 2.4.1\r\n- Are you willing to contribute it: Probably not\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nGradients are being ported to C++, with the tape API in [c/eager/gradients.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/gradients.h).\r\n\r\nHowever, there is no equivalent C API, and as per https://github.com/tensorflow/community/pull/335 it does not seem there are current plans to add one.\r\n\r\nI assume this is on the radar somewhere, but I don't see any tracking issues, so I'm making this as tracking (and a request).\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will expose the existing gradient tape api to the C API.  Gradient registration should be exposed as well to allow for registering custom gradients (it seems to be tape specific?).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nIt will enable eager mode to support gradients for projects using the C API bindings, including the [official JVM bindings](https://github.com/tensorflow/java) (which is what prompted this issue) and those that depend on it.", "comments": ["/cc @joanafilipa", "For JVM, are they working directly with C++ interfaces https://github.com/tensorflow/java/pull/283?"]}, {"number": 47746, "title": "Failed to convert QAT model to tflite when I use tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8", "body": "1. System information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nTensorFlow installation (pip package or built from source):pip package\r\nTensorFlow library (version, if pip package or github SHA, if built from source):2.3.1\r\n2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\nWhen I use tf.lite.TFLiteConverter to convert keras ACTIVATIONS_INT16_WEIGHTS_INT8 QAT model to tflite file\u3002There are some bugs and the conversion is failed.\r\nThis is my convert code and Bugs.\r\nquant_converter1 = tf.lite.TFLiteConverter.from_keras_model(model1_quant)\r\nquant_converter1.optimizations = [tf.lite.Optimize.DEFAULT]\r\nquant_converter1.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\nquant_tflite_model1 = quant_converter1.convert()\r\nwith open(model1_tflite_quant, 'wb') as f:\r\nf.write(quant_tflite_model1)\r\nas\r\n**I updated the TensorFlow to the latest stable version v2.4.1 and also face the same error** @amahendrakar\r\n![image](https://user-images.githubusercontent.com/12438262/110884962-e10c6000-82dd-11eb-9659-af72df2f8edd.png)\r\n\r\n", "comments": ["@ethkim could you triage this issue?"]}, {"number": 47744, "title": "undeclared includion error when building libtensorflowlite.so", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacOS Big Sur Version 11.2.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version:\r\nmaster branch af4b0dfbb251cae9f6407c851cfd5bc11a172b3b\r\n- Python version:\r\n3.8\r\n- Installed using virtualenv? pip? conda?:\r\nconda\r\n- Bazel version (if compiling from source):\r\n3.7.2\r\n- GCC/Compiler version (if compiling from source):\r\nApple clang version 12.0.0 (clang-1200.0.32.29) \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen compiling libtensorflowlite.so with //tensorflow/lite/delegates/flex::delegate as dependency, build fail with undeclared includions in error.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build \\\r\n//tensorflow/lite:libtensorflowlite.so \\\r\n--config=monolithic \\\r\n--cxxopt='--std=c++14' \\\r\n-c opt \\\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=112\r\nINFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=@org_tensorflow//third_party/toolchains/java:tf_java_toolchain --//tensorflow/core/kernels/mlir_generated:enable_gpu=false --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /Users/richardyao/Code/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/richardyao/opt/miniconda3/bin/python3 --action_env PYTHON_LIB_PATH=/Users/richardyao/opt/miniconda3/lib/python3.8/site-packages --python_path=/Users/richardyao/opt/miniconda3/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /Users/richardyao/Code/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/richardyao/Code/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/richardyao/Code/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:monolithic in file /Users/richardyao/Code/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:macos in file /Users/richardyao/Code/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Analyzed target //tensorflow/lite:libtensorflowlite.so (3 packages loaded, 158 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /Users/richardyao/Code/tensorflow/tensorflow/compiler/mlir/xla/BUILD:262:11: undeclared inclusion(s) in rule '//tensorflow/compiler/mlir/xla:hlo_utils':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/mlir/xla/hlo_utils.cc':\r\n  'bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen/mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc'\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1429.033s, Critical Path: 828.11s\r\nINFO: 1071 processes: 13 internal, 1058 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n", "comments": ["@thaink could you take a look at this?", "I am able to build with that command using linux.\r\nCan you try to build //tensorflow/lite:ltensorflowlite, it will have different file format based on the target operating system.", "Is it possible to build for linux with macOS? I was also able to complete build with 2.4.0, but the resulting .so somehow have issue during linking. \r\nld.lld: error: /root/project/ext/tensorflow/lib/linux/cpu/libtensorflowlite.so: unknown file type", "you could try --config=linux or some other in this file https://github.com/tensorflow/tensorflow/blob/master/.bazelrc#L351"]}, {"number": 47728, "title": "fit_generator is calling __getitem__ with id=0 twice in the first epoch", "body": "**System information**\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution: MacOS 11.2.2\r\n- TensorFlow installed from (source or binary): Installed with pip\r\n- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nWhen using a generator and `fit_generator` (or `fit` for the case) method, the first time `__getitem__` is called, repeats the idx = 0, which produces that the generator either sends the same batch twice or have errors since `on_epoch_end` is not called on time.\r\n\r\n**Describe the expected behavior**\r\n__getitem__ should always be called with incremental indexes as argument. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nfrom tensorflow.keras.utils import Sequence\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\nimport numpy as np\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self, batch_size = 512, shape = (10,)):\r\n        self.shape = shape\r\n        self.batch_size = batch_size\r\n\r\n    def on_epoch_end(self):\r\n        # do nothing\r\n        pass\r\n\r\n    def __getitem__(self, idx):\r\n        print(\"[+] Idx: %d\" % idx)\r\n        y = np.ones((10,2))\r\n        y[:,0] = 0\r\n        return np.random.random((10,10)), y\r\n\r\n    def __len__(self):\r\n        return 7\r\n\r\ninp = Input(shape = (10,), dtype = np.float)\r\nx = Dense(10, activation = 'relu')(inp)\r\nout = Dense(2, activation='softmax')(x)\r\nmodel = Model(inputs=inp, outputs= out)\r\n\r\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n\r\ngenerator = DataGenerator()\r\n\r\nmodel.fit_generator(generator, epochs = 2, shuffle = False, workers = 0)\r\n```\r\nThe output of the above code is: \r\n```\r\n[+] Idx: 0\r\nEpoch 1/10\r\n[+] Idx: 0\r\n1/7 [===>..........................] - ETA: 1s - loss: 0.7483 - accuracy: 0.3000[+] Idx: 1\r\n[+] Idx: 2\r\n[+] Idx: 3\r\n[+] Idx: 4\r\n[+] Idx: 5\r\n[+] Idx: 6\r\n7/7 [==============================] - 0s 1ms/step - loss: 0.7374 - accuracy: 0.4012\r\nEpoch 2/10\r\n[+] Idx: 0\r\n1/7 [===>..........................] - ETA: 0s - loss: 0.6861 - accuracy: 0.6000[+] Idx: 1\r\n[+] Idx: 2\r\n[+] Idx: 3\r\n[+] Idx: 4\r\n[+] Idx: 5\r\n[+] Idx: 6\r\n```\r\nWhen it shouldn't  call` __getitem__` twice with `Idx: 0`\r\n", "comments": ["@ymodak \r\nI am able to replicate this on tf 2.3,2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/222be34238eb7807514249bb47a50c14/untitled561.ipynb)", "@Saduf2019, thanks for taking a look. In case it helps, this is not present in tf 2.0. The problem appeared when updating from 2.0 to 2.4.", "I don't see this moving forward soon, so in case anyone has the same problem, this is an ugly trick to make the generator work: \r\n\r\n```\r\nfrom tensorflow.keras.utils import Sequence\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Dense\r\nimport numpy as np\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self, batch_size = 512, shape = (10,)):\r\n        self.shape = shape\r\n        self.batch_size = batch_size\r\n        self.first_epoch = True\r\n\r\n    def on_epoch_end(self):\r\n        # do nothing\r\n        pass\r\n\r\n    def __getitem__(self, idx):\r\n        print(\"[+] Idx: %d\" % idx)\r\n        y = np.ones((10,2))\r\n        y[:,0] = 0\r\n        if self.first_epoch:\r\n               self.first_epoch = False\r\n               self.on_epoch_end()\r\n        return np.random.random((10,10)), y\r\n\r\n    def __len__(self):\r\n        return 7\r\n\r\ninp = Input(shape = (10,), dtype = np.float)\r\nx = Dense(10, activation = 'relu')(inp)\r\nout = Dense(2, activation='softmax')(x)\r\nmodel = Model(inputs=inp, outputs= out)\r\n\r\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n\r\ngenerator = DataGenerator()\r\n\r\nmodel.fit_generator(generator, epochs = 2, shuffle = False, workers = 0)\r\n```", "Was able to replicate the issue in TF 2.6.0-dev20210531,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/20eef508d78beaaa4306dd4ead0c3ccd/untitled160.ipynb)..Thanks !"]}, {"number": 47721, "title": "TF C-API GPU not found via TF_SessionListDevices", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4\r\n- CUDA/cuDNN version: 11.0 / 8.0.4\r\n- GPU model and memory: GeForce MX250 2048MB GDDR5\r\n\r\n\r\n\r\nI'm currently working on a CNN deployment via the Tensorflow C-API. When checking for the available devices via **TF_SessionListDevices** only the CPU is listed. I already checked my CUDA installation and verified it on the python side which detects the GPU. Trying to set the device via hex string doesn't fix it.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n    TF_Graph* Graph = TF_NewGraph();\r\n    TF_Status* Status = TF_NewStatus();\r\n\r\n    uint8_t config[13] = { 0xa, 0x7, 0xa, 0x3, 0x47, 0x50, 0x55, 0x10, 0x0, 0x10, 0x1, 0x28, 0x1};\r\n\r\n    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();\r\n    TF_Buffer* RunOpts = NULL;\r\n\r\n    TF_SetConfig(SessionOpts, (void*)config, 13, Status);\r\n\r\n    if (TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_SetConfig OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\", TF_Message(Status));\r\n    }\r\n\r\n    const char* saved_model_dir = \"\\\\model\";\r\n    const char* tags = \"serve\"; // default model serving tag; can change in future\r\n    int ntags = 1;\r\n\r\n    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);\r\n    if (TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_LoadSessionFromSavedModel OK\\n\");\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\", TF_Message(Status));\r\n    }\r\n\r\n\r\n    //Check devices\r\n    TF_DeviceList* dl = TF_SessionListDevices(Session, Status);\r\n    if (TF_GetCode(Status) == TF_OK)\r\n    {\r\n        printf(\"TF_SessionListDevices OK\\n\");\r\n        int count = TF_DeviceListCount(dl);\r\n        const char* list;\r\n        const char* tpe;\r\n        printf(\"DeviceList:\\n\");\r\n        for (int i = 0; i < count; i++)\r\n        {\r\n            list = TF_DeviceListName(dl, i, Status);\r\n            tpe = TF_DeviceListType(dl, i, Status);\r\n            printf(list);\r\n            printf(\"\\n\");\r\n        }\r\n\r\n\r\n        TF_DeleteDeviceList(dl);\r\n    }\r\n    else\r\n    {\r\n        printf(\"%s\", TF_Message(Status));\r\n    }\r\n\r\nExpected output should be two list members with type CPU and GPU each, but only the CPU device is listed.\r\n\r\nAny help on this issue would be appreciated.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I'm not too familiar with the C API, @agarwal-ashish do you know what's going on here?", "Bump"]}, {"number": 47715, "title": "TF Lite GPU Delegate produces weird output", "body": "## System information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.5 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Samsung Galaxy S10(Exynos)**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below):**tags/v2.4.1**\r\n- Python version: **3.6.9 / 2.7.17 both**\r\n- Bazel version (if compiling from source): **3.1.0**\r\n- GCC/Compiler version (if compiling from source): **gcc version 9.3.0 (Ubuntu 9.3.0-11ubuntu0~18.04.1)**\r\n- GPU model and memory: **Exynos**\r\n\r\n## Model Info\r\n\r\n### test_model_512.tflite\r\n\r\n![testmodel512](https://user-images.githubusercontent.com/39717352/110747329-91984680-8281-11eb-8d42-9529fb9eda2a.png)\r\n\r\n### test_model_4096.tflite\r\n\r\n![testmodel4096](https://user-images.githubusercontent.com/39717352/110747341-952bcd80-8281-11eb-8820-a45acd77fe95.png)\r\n\r\n### proof_model_512.tflite\r\n\r\n![proofmodel512](https://user-images.githubusercontent.com/39717352/110747356-9a891800-8281-11eb-9340-e45fe6b17b4f.png)\r\n\r\n### proof_model_4096.tflite\r\n\r\n![proofmodel4096](https://user-images.githubusercontent.com/39717352/110747369-9e1c9f00-8281-11eb-8b14-27582b89581d.png)\r\n\r\n## Symptom\r\n\r\nHello Tensorflow.\r\nI'm trying to use Tensorflow Lite GPU Delegate to accelerate my model on GPU.\r\nBut it produces weird output, refer below test result.\r\n\r\nI think that GPU Delegate frees memory address of `output_1` tensor after `Add`(B=-0.125).\r\nBecause the `proof_model`s that are added `Add`(B=0) operation between `Tanh` and `output_1` does not produce weird outputs.\r\n\r\nWould you please check this?\r\n\r\n## Test results\r\n\r\n### test_model_4096.tflite\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m test_model_4096.tflite \r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #1: CPU value - 0.686587, GPU value - 0, abs diff - 0.686587\r\nOutput #0: element #2: CPU value - 0.720795, GPU value - 0, abs diff - 0.720795\r\nOutput #0: element #3: CPU value - 0.140191, GPU value - 0, abs diff - 0.140191\r\nOutput #0: element #4: CPU value - -0.63919, GPU value - 0, abs diff - 0.63919\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - 0, abs diff - 0.743797\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - 0, abs diff - 0.272364\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 0, abs diff - 0.576355\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - 0, abs diff - 0.757089\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 0, abs diff - 0.39027\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - 0, abs diff - 0.496026\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 4092 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 1977 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.014039ms\r\nGPU time(CPU->GPU->CPU) - 3.01996ms\r\n```\r\n\r\n### test_model_512.tflite\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m test_model_512.tflite\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #1: CPU value - 0.686587, GPU value - 1.7236e-43, abs diff - 0.686587\r\nOutput #0: element #2: CPU value - 0.720795, GPU value - -7529.91, abs diff - 7530.63\r\nOutput #0: element #3: CPU value - 0.140191, GPU value - 1.7236e-43, abs diff - 0.140191\r\nOutput #0: element #4: CPU value - -0.63919, GPU value - -1.08755e-05, abs diff - 0.639179\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - 1.7236e-43, abs diff - 0.743797\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - -9.21461e-06, abs diff - 0.272355\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 1.7236e-43, abs diff - 0.576355\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - -8.09113e-16, abs diff - 0.757089\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 2.4975e-39, abs diff - 0.39027\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - -174.68, abs diff - 174.184\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 407 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 239 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.006577ms\r\nGPU time(CPU->GPU->CPU) - 1.96819ms\r\n```\r\n\r\n### proof_model_4096.tflite\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m proof_model_4096.tflite\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162\r\nOutput #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627\r\nOutput #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331\r\nOutput #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552\r\nOutput #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 1852 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 1977 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.014ms\r\nGPU time(CPU->GPU->CPU) - 4.58219ms\r\n```\r\n\r\n### proof_model_512.tflite\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m proof_model_512.tflite \r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162\r\nOutput #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627\r\nOutput #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331\r\nOutput #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552\r\nOutput #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 228 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 239 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.007116ms\r\nGPU time(CPU->GPU->CPU) - 2.74935ms\r\n```\r\n", "comments": ["@impjdi could you take a look at this report?", "Unfortunately, OpenCL doesn't work too well if you set intermediate tensors as graph output tensors.  For example in `test_model_512.tflite` or `test_model_4096.tflite`, `output_1` is an intermediate tensor also marked as graph output, and this may have a wrong values.  If you are inspecting the model, adding a dummy `ADD` with 0 (or maybe a very small epsilon) as you did in the latter two models is the right workaround for OpenCL.  We spent quite amount of engineering time over multiple times to do some plumbing on OpenGL side, but never got to spend that much time on OpenCL side as it wasn't a priority.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47715\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47715\">No</a>\n", "@impjdi\r\n\r\nThank you for answering.\r\nHowever, do you have any idea or something suspect why OpenCL does not work well?", "Often, those kind of intermediate inspection is mostly for debugging purpose, while the team's interested in performance.  We had one legit use-case on OpenGL where intermediate tensor had to be used, that's why did the plumbing there, but that network never run on OpenCL, so we didn't do that plumbing on OpenCL side.", "@abattery @impjdi \r\n\r\nMay I ask reopen this issue? There isn't reopen button.\r\n\r\nOpenCL or OpenGL are not the problem.\r\nWhen TF Lite determines outputs of graph, it checks whether the tensor has consumers or not.\r\nIn my case, therefore, `output_1` has a consumer and then cannot be dealt as graph's output.\r\nHave you guys ever discussed about cases like this?\r\n\r\nWith applying my naive patch(based tags/v2.4.1), the model works well.\r\n\r\n<details>\r\n<summary>patch</summary>\r\n\r\n```\r\ndiff --git a/tensorflow/lite/delegates/gpu/cl/inference_context.cc b/tensorflow/lite/delegates/gpu/cl/inference_context.cc\r\nindex ca0c0319f54..ae550d4842a 100644\r\n--- a/tensorflow/lite/delegates/gpu/cl/inference_context.cc\r\n+++ b/tensorflow/lite/delegates/gpu/cl/inference_context.cc\r\n@@ -176,7 +176,7 @@ absl::Status InferenceContext::InitFromGraph(\r\n   CopyInAndOutIds(graph);\r\n   RETURN_IF_ERROR(ConvertOperations(creation_context.GetDeviceInfo(), graph,\r\n                                     create_info.hints));\r\n-  RETURN_IF_ERROR(Merge());\r\n+  RETURN_IF_ERROR(Merge(graph));\r\n   RETURN_IF_ERROR(AllocateMemory(creation_context.context));\r\n   BindMemoryToOperations();\r\n   RETURN_IF_ERROR(Compile(creation_context));\r\n@@ -396,7 +396,7 @@ absl::Status InferenceContext::ConvertOperations(const DeviceInfo& device_info,\r\n   return absl::OkStatus();\r\n }\r\n \r\n-absl::Status InferenceContext::Merge() {\r\n+absl::Status InferenceContext::Merge(const GraphFloat32& graph) {\r\n   absl::flat_hash_set<ValueId> ready_tensors;\r\n   for (const auto& input_id : input_ids_) {\r\n     ready_tensors.insert(input_id);\r\n@@ -406,7 +406,7 @@ absl::Status InferenceContext::Merge() {\r\n     for (const auto& out_id : node.outputs) {\r\n       ready_tensors.insert(out_id);\r\n     }\r\n-    if (node.outputs.size() != 1) {\r\n+    if (node.outputs.size() != 1 || graph.IsGraphOutput(node.outputs[0])) {\r\n       continue;\r\n     }\r\n     std::vector<int> next_nodes;\r\n@@ -425,6 +425,7 @@ absl::Status InferenceContext::Merge() {\r\n     auto& linkable_node = nodes_[next_nodes[0]];\r\n     if (!linkable_node.operation->IsLinkable() ||\r\n         linkable_node.outputs.size() != 1 ||\r\n+\t      graph.IsGraphOutput(linkable_node.outputs[0]) ||\r\n         !IsReady(ready_tensors, linkable_node)) {\r\n       continue;\r\n     }\r\ndiff --git a/tensorflow/lite/delegates/gpu/cl/inference_context.h b/tensorflow/lite/delegates/gpu/cl/inference_context.h\r\nindex ec8055ebcde..3c8e4d10163 100644\r\n--- a/tensorflow/lite/delegates/gpu/cl/inference_context.h\r\n+++ b/tensorflow/lite/delegates/gpu/cl/inference_context.h\r\n@@ -114,7 +114,7 @@ class InferenceContext {\r\n   void ReserveGraphTensors(const CreateInferenceInfo& create_info,\r\n                            const DeviceInfo& device_info,\r\n                            const GraphFloat32& graph);\r\n-  absl::Status Merge();\r\n+  absl::Status Merge(const GraphFloat32& graph);\r\n   absl::Status AllocateMemory(CLContext* context);\r\n \r\n   absl::Status AllocateMemoryForVariableTensors(CLContext* context);\r\ndiff --git a/tensorflow/lite/delegates/gpu/common/model.cc b/tensorflow/lite/delegates/gpu/common/model.cc\r\nindex 696a747a817..517d632311f 100644\r\n--- a/tensorflow/lite/delegates/gpu/common/model.cc\r\n+++ b/tensorflow/lite/delegates/gpu/common/model.cc\r\n@@ -52,7 +52,8 @@ std::vector<Value*> GraphFloat32::variable_inputs() const {\r\n }\r\n \r\n std::vector<Value*> GraphFloat32::outputs() const {\r\n-  return FilterValues([](const ValueDef& v) { return v.consumers.empty(); });\r\n+  return FilterValues(\r\n+      [&](const ValueDef& v) { return IsGraphOutput(v.value->id); });\r\n }\r\n \r\n std::vector<Value*> GraphFloat32::FindInputs(NodeId id) const {\r\n@@ -73,16 +74,22 @@ bool GraphFloat32::IsGraphInput(ValueId id) const {\r\n   if (id >= values_.size()) {\r\n     return false;\r\n   }\r\n-  return values_[id].producer == nullptr;\r\n+  return std::find(input_values_.begin(), input_values_.end(), id) !=\r\n+         input_values_.end();\r\n }\r\n \r\n bool GraphFloat32::IsGraphOutput(ValueId id) const {\r\n   if (id >= values_.size()) {\r\n     return false;\r\n   }\r\n-  return values_[id].consumers.empty();\r\n+  return std::find(output_values_.begin(), output_values_.end(), id) !=\r\n+         output_values_.end();\r\n }\r\n \r\n+void GraphFloat32::AddInput(ValueId id) { input_values_.push_back(id); };\r\n+\r\n+void GraphFloat32::AddOutput(ValueId id) { output_values_.push_back(id); };\r\n+\r\n Node* GraphFloat32::FindProducer(ValueId id) const {\r\n   if (id >= values_.size()) {\r\n     return nullptr;\r\ndiff --git a/tensorflow/lite/delegates/gpu/common/model.h b/tensorflow/lite/delegates/gpu/common/model.h\r\nindex 2e9aac8c53c..9b83b8a77c8 100644\r\n--- a/tensorflow/lite/delegates/gpu/common/model.h\r\n+++ b/tensorflow/lite/delegates/gpu/common/model.h\r\n@@ -103,6 +103,10 @@ class GraphFloat32 {\r\n \r\n   bool IsGraphOutput(ValueId id) const;\r\n \r\n+  void AddInput(ValueId id);\r\n+\r\n+  void AddOutput(ValueId id);\r\n+\r\n   // @return producer of the given value. Returns nullptr for deleted value.\r\n   Node* FindProducer(ValueId id) const;\r\n \r\n@@ -220,6 +224,10 @@ class GraphFloat32 {\r\n   // We store it by value here to make introspection calls cheaper.\r\n   std::vector<ValueDef> values_;\r\n \r\n+  std::vector<ValueId> input_values_;\r\n+  \r\n+  std::vector<ValueId> output_values_;\r\n+\r\n   std::map<NodeId, NodeDef> nodes_;\r\n   // Node Ids in order of execution.\r\n   std::vector<NodeId> execution_plan_;\r\ndiff --git a/tensorflow/lite/delegates/gpu/common/model_builder.cc b/tensorflow/lite/delegates/gpu/common/model_builder.cc\r\nindex c200f0926aa..51ed9efe61f 100644\r\n--- a/tensorflow/lite/delegates/gpu/common/model_builder.cc\r\n+++ b/tensorflow/lite/delegates/gpu/common/model_builder.cc\r\n@@ -2909,13 +2909,18 @@ TfLiteIntArray* GetOpsToReplace(TfLiteContext* context, bool allow_quant_ops,\r\n absl::Status PrecreateIOTensors(\r\n     TfLiteContext* context, GraphFloat32* graph, TfLiteIntArray* io_tensors,\r\n     absl::flat_hash_map<int, int>* quant_conversion_map,\r\n-    absl::flat_hash_map<int, Value*>* tensor_to_value) {\r\n+    absl::flat_hash_map<int, Value*>* tensor_to_value, bool is_input) {\r\n   for (int i = 0; i < io_tensors->size; ++i) {\r\n     const int tensor_index = io_tensors->data[i];\r\n     const TfLiteTensor& tflite_tensor = context->tensors[tensor_index];\r\n     if (tflite::IsConstantTensor(&tflite_tensor)) continue;\r\n     RETURN_IF_ERROR(ObjectReader::ReadNonConstantTensor(\r\n         context, tensor_to_value, quant_conversion_map, graph, tensor_index));\r\n+\r\n+    if (is_input)\r\n+      graph->AddInput((*tensor_to_value)[tensor_index]->id);\r\n+    else\r\n+      graph->AddOutput((*tensor_to_value)[tensor_index]->id);\r\n   }\r\n   return absl::OkStatus();\r\n }\r\n@@ -2989,12 +2994,12 @@ absl::Status BuildModel(TfLiteContext* context,\r\n   }\r\n   absl::flat_hash_map<int, Value*> tensor_to_value;\r\n   std::vector<ValueId> variable_inputs_to_value_id;\r\n-  RETURN_IF_ERROR(PrecreateIOTensors(context, graph,\r\n-                                     delegate_params->input_tensors,\r\n-                                     quant_conversion_map, &tensor_to_value));\r\n-  RETURN_IF_ERROR(PrecreateIOTensors(context, graph,\r\n-                                     delegate_params->output_tensors,\r\n-                                     quant_conversion_map, &tensor_to_value));\r\n+  RETURN_IF_ERROR(\r\n+      PrecreateIOTensors(context, graph, delegate_params->input_tensors,\r\n+                         quant_conversion_map, &tensor_to_value, true));\r\n+  RETURN_IF_ERROR(\r\n+      PrecreateIOTensors(context, graph, delegate_params->output_tensors,\r\n+                         quant_conversion_map, &tensor_to_value, false));\r\n   for (int i = 0; i < operations.size(); ++i) {\r\n     TfLiteNode* tflite_node;\r\n     TfLiteRegistration* registration;\r\n\r\n```\r\n\r\n</details>\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m ../models/test_model_4096.tflite\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162\r\nOutput #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627\r\nOutput #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331\r\nOutput #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552\r\nOutput #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 1852 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 1977 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.013ms\r\nGPU time(CPU->GPU->CPU) - 3.834ms\r\n```\r\n\r\n```\r\n$ tensorflow/lite/delegates/gpu/cl/testing/run_delegate_testing.sh -m ../models/test_model_512.tflite\r\n\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\n[output_1]\r\nOutput #0: element #5: CPU value - -0.743797, GPU value - -0.743652, abs diff - 0.000144362\r\nOutput #0: element #6: CPU value - -0.272364, GPU value - -0.272217, abs diff - 0.000147194\r\nOutput #0: element #7: CPU value - 0.576355, GPU value - 0.57666, abs diff - 0.000305593\r\nOutput #0: element #8: CPU value - 0.757089, GPU value - 0.756836, abs diff - 0.000252664\r\nOutput #0: element #9: CPU value - 0.39027, GPU value - 0.390381, abs diff - 0.000110865\r\nOutput #0: element #10: CPU value - -0.496026, GPU value - -0.49585, abs diff - 0.000176162\r\nOutput #0: element #11: CPU value - -0.76159, GPU value - -0.761719, abs diff - 0.000128627\r\nOutput #0: element #14: CPU value - 0.757621, GPU value - 0.757812, abs diff - 0.000191331\r\nOutput #0: element #16: CPU value - -0.280204, GPU value - -0.280029, abs diff - 0.000174552\r\nOutput #0: element #17: CPU value - -0.7449, GPU value - -0.745117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 228 different elements, for output #0, threshhold - 0.0001\r\n[output_2]\r\nOutput #1: element #5: CPU value - -0.868797, GPU value - -0.868652, abs diff - 0.000144362\r\nOutput #1: element #6: CPU value - -0.397364, GPU value - -0.397217, abs diff - 0.000147194\r\nOutput #1: element #7: CPU value - 0.451355, GPU value - 0.45166, abs diff - 0.000305593\r\nOutput #1: element #8: CPU value - 0.632089, GPU value - 0.631836, abs diff - 0.000252664\r\nOutput #1: element #9: CPU value - 0.26527, GPU value - 0.265381, abs diff - 0.000110865\r\nOutput #1: element #11: CPU value - -0.88659, GPU value - -0.886719, abs diff - 0.000128627\r\nOutput #1: element #12: CPU value - -0.615389, GPU value - -0.615234, abs diff - 0.000155032\r\nOutput #1: element #14: CPU value - 0.632621, GPU value - 0.632812, abs diff - 0.000191331\r\nOutput #1: element #16: CPU value - -0.405204, GPU value - -0.405029, abs diff - 0.000174552\r\nOutput #1: element #17: CPU value - -0.8699, GPU value - -0.870117, abs diff - 0.000217617\r\nPrinted 10 different elements, threshhold - 0.0001, next different elements skipped\r\nTotal 239 different elements, for output #1, threshhold - 0.0001\r\nCPU time - 0.007077ms\r\nGPU time(CPU->GPU->CPU) - 2.79154ms\r\n```", "> May I ask reopen this issue? There isn't reopen button.\r\n\r\nDone.\r\n\r\n> OpenCL or OpenGL are not the problem.\r\n\r\nTFLite allows any tensor to be the graph output.  You are talking about `GraphFloat32` which is an intermediate representation (IR) of the network for the GPU delegates.\r\n\r\n> When TF Lite determines outputs of graph, it checks whether the tensor has consumers or not.\r\n\r\nThat is correct.  When we started working on the GPU backends, we had to introduce this IR for GPU's purpose, and we made some wrong decision around the graph output.  We defined graph outputs to be any leaf tensors in the network, and didn't allow random access of intermediate tensors.\r\n\r\n> In my case, therefore, output_1 has a consumer and then cannot be dealt as graph's output.\r\n> Have you guys ever discussed about cases like this?\r\n\r\nFor this, let me repeat myself:\r\n\r\n> Often, those kind of intermediate inspection is mostly for debugging purpose, while the team's interested in performance. We had one legit use-case on OpenGL where intermediate tensor had to be used, that's why did the plumbing there, but that network never run on OpenCL, so we didn't do that plumbing on OpenCL side."]}, {"number": 47712, "title": "The Risk of posix_memalign in Ruy which TensorFlow Lite used.", "body": "\r\nHi TF Authors,\r\n\r\nWe recently got a crash as following when we tflite::Interpreter::Invoke the cartoonized [model](https://github.com/margaretmz/Cartoonizer-with-TFLite/blob/master/android/app/src/main/ml/whitebox_cartoon_gan_int8.tflite):\r\n```\r\n2021-03-09 16:07:22.261 18157-18309 W/libc: memalign(64, 411042816) failed: returning null pointer\r\n2021-03-09 15:55:06.148 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: \t#00  pc 001b7b00  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: \t#01  pc 0008cf05  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: \t#02  pc 0008cb99  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: \t#03  pc 001b8e85  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: \t#04  pc 001b8bc1  /data/app/com.magicpal.magicamiti-VBWn90aZTYIGNTtAk2TwYA==/lib/arm/libtensorflowlite.so\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH: memory near r0:\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4513c 00000000 96a45208 00000000 804ff400  .....R........O.\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4514c 00000000 96a451c0 ade93f09 e57af260  .....Q...?..`.z.\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4515c 4f640380 4f6409a0 00000000 00000000  ..dO..dO........\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4516c 804ff400 00000200 00000010 00000010  ..O.............\r\n2021-03-09 15:55:06.150 17799-17959 E/CRASH:     96a4517c ffffffff ffffffff 00000620 ffffffab  ........ .......\r\n2021-03-09 15:55:06.151 17799-17959 E/CRASH:     96a4522c 00000620 00040000 00000620 00000000   ....... .......\r\n\r\n\u2026\u2026( It\u2019s too much to display, check whole logs on  the end of this log snippet,  tensor_crash.txt)\r\n\r\n2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a40 abababab abababab abababab abababab  ................\r\n2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a50 abababab abababab abababab abababab  ................\r\n2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a60 abababab abababab abababab abababab  ................\r\n2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a70 abababab abababab abababab abababab  ................\r\n2021-03-09 15:55:06.167 17799-17959 E/CRASH:     4f640a80 abababab abababab abababab abababab  ................\r\n2021-03-09 15:55:06.193 17799-17823 E/CRASH: other thread is trapped; signum = 11\r\n2021-03-09 15:55:06.194 17799-17823 E/MessageQueue: IdleHandler threw exception\r\n    java.lang.Error: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 00000200\r\n    Build fingerprint: 'Redmi/merlin/merlin:10/QP1A.190711.020/V12.0.8.0.QJOCNXM:user/release-keys'\r\n    Revision: '0'\r\n    pid: 17799, tid: 17959, name: UnityMain  >>> com.magicpal.magicamiti <<<\r\n        r0 96a4515c  r1 00000010  r2 00000620  r3 00000620\r\n        r4 00000200  r5 00000010  r6 00000010  r7 96a45150\r\n        r8 ffffffab  r9 96a45228  sl 00000620  fp 96a45208\r\n        ip 4f640390  sp 96a45100  lr 4f6409b0  pc adfbeb00  cpsr 00004627\r\n    \r\n        at libtensorflowlite.001b7b00(Native Method)\r\n        at libtensorflowlite.0008cf05(Native Method)\r\n        at libtensorflowlite.0008cb99(Native Method)\r\n        at libtensorflowlite.001b8e85(Native Method)\r\n        at libtensorflowlite.001b8bc1(Native Method)\r\n2021-03-09 15:55:06.196 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2\r\n2021-03-09 15:55:06.202 551-1048 D/AudioMixer: setDRCHandler, mStreamType: 3, device: 2\r\n    \r\n    --------- beginning of system\r\n```\r\nWhole logs is here, [tensor_crash.txt](https://github.com/tensorflow/tensorflow/files/6120045/tensor_crash.txt).\r\n\r\nAnd we suspect that the [memalign](https://github.com/google/ruy/blob/3c363dc10d06857ad489c034ebb3bbd6d273dfbd/ruy/system_aligned_alloc.cc#L34) called by Ruy cause this crash quietly.\r\n```\r\n2021-03-09 16:07:22.261 18157-18309 W/libc: memalign(64, 411042816) failed: returning null pointer\r\n```\r\n\r\n411042816 = 392MB is the exact buffer size of the Tensor 154 in [model](https://github.com/margaretmz/Cartoonizer-with-TFLite/blob/master/android/app/src/main/ml/whitebox_cartoon_gan_int8.tflite) we mentioned. So, we believe that the Interpreter could be crashed when it memalign a big memory and got  a null pointer on Invoke stage. \r\n\r\nHowever, why we reallocate memory after Interpreter::AllocateTensors? And did we ever considered the cases, which the big temporary buffers would be allocated in the Invoke flow? We are confused that why not assert the return of memalign, and only return a null pointer without any processing? It\u2019s seem dangerous and  really unfriendly for developers to debug.\r\n\r\nLooking forward to your reply, thx. @renjie-liu @multiverse-tf @wangtz ", "comments": ["@talumbau @renjie-liu  Could you take a look at this bug report?", "Chatted with the @SunAriesCN offline yesterday, a couple of things:\r\n\r\n* The model has a very large intermediate layer which has an output of 1*515*512*1568\r\n* Obviously the interpreter should definitely allocate a buffer larger than that one (what happened in the `allocate tensor`\r\n* Ruy can allocate buffer for faster execution, and in this case, it's allocating 384MB which is the size of the output tensor.\r\n* It's fine to add an `assert` in the align_buffer allocation returned pointer.\r\n* Meanwhile, we should probably look into use a model with smaller intermediate tensor.\r\n", "Thanks for your reply, Renjie. It maybe an implicit demand about the big models inference with TFLite, I think. Because of the privacy polities, we would like running some big models under background threads. So, it will be cool, if we have plans for this in TFLite, and we would like to talk about that."]}]