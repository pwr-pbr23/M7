[{"number": 8754, "title": "tf.nn.max_pool works incorrectly with option:  data_format=\"NCHW\" on GPU", "body": "The max_pooling operation seems to be ignorant of the data_format option of \"NCHW\" and treat the input data as \"NHWC\"\r\n\r\n>>> with tf.device('/gpu:0'):\r\n...     a=tf.zeros((100,3,224,224))\r\n...     b=tf.nn.max_pool(a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=\"SAME\",data_format=\"NCHW\")\r\n... \r\n>>> b\r\n<tf.Tensor 'MaxPool:0' shape=(100, 2, 112, 224) dtype=float32>\r\n\r\nI think the shape of the 'b' variable should be (100,3,112,112) instead of (100, 2, 112, 224). The max pooling operations seems to operate on the wrong axis.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone \r\n### Environment info\r\nOperating System:\r\nUbuntu LTS 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 sept. 15  2016 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 oct.   5 10:46 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp35-cp35m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.1\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nwith tf.device('/gpu:0'):\r\n...     a=tf.zeros((100,3,224,224))\r\n...     b=tf.nn.max_pool(a, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=\"SAME\",data_format=\"NCHW\")\r\n... \r\nb\r\n<tf.Tensor 'MaxPool:0' shape=(100, 2, 112, 224) dtype=float32>\r\n\r\n### What other attempted solutions have you tried?\r\nNone\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Sorry, my error. It needs to change the strides to strides=[1, 1, 2, 2]"]}, {"number": 8753, "title": "TFDBG 'PyFunc' is not found in partition graphs.", "body": "When using tensorflow debugger, the program crashes with the following message :\r\n\r\n\r\n> Traceback (most recent call last):\r\n>   File \"farn_train.py\", line 470, in <module>\r\n>     train()\r\n>   File \"farn_train.py\", line 448, in train\r\n>     _, loss_value = sess.run([train_op, loss])\r\n>   File \"/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 470, in run\r\n>     run_end_resp = self.on_run_end(run_end_req)\r\n>   File \"/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 275, in on_run_end\r\n>     self._dump_root, partition_graphs=partition_graphs)\r\n>   File \"/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 524, in __init__\r\n>     self._load_partition_graphs(partition_graphs, validate)\r\n>   File \"/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 782, in _load_partition_graphs\r\n>     self._validate_dump_with_graphs()\r\n>   File \"/home/qiqi/anaconda2/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 928, in _validate_dump_with_graphs\r\n>     datum.node_name)\r\n> ValueError: Node name 'PyFunc' is not found in partition graphs.\r\n> \r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n> rw-r--r-- 1 root root   560184 10\u6708 31 11:03 /usr/local/cuda/lib64/libcudadevrt.a\r\n> lrwxrwxrwx 1 root root       16 10\u6708 31 11:03 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n> lrwxrwxrwx 1 root root       19 10\u6708 31 11:03 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n> -rwxr-xr-x 1 root root   394472 10\u6708 31 11:03 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n> -rw-r--r-- 1 root root   737516 10\u6708 31 11:03 /usr/local/cuda/lib64/libcudart_static.a\r\n> -rwxr-xr-x 1 root root 79337624 11\u6708  5 08:55 /usr/local/cuda/lib64/libcudnn.so\r\n> -rwxr-xr-x 1 root root 79337624 11\u6708  5 08:55 /usr/local/cuda/lib64/libcudnn.so.5\r\n> -rwxr-xr-x 1 root root 79337624 11\u6708  5 08:55 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n> -rw-r--r-- 1 root root 69756172 11\u6708  5 08:55 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. \r\n'1.0.1'\r\n\r\n\r\n", "comments": ["@hqqxyy It would be most helpful if you could provide a code to reproduce this issue. Thanks.", "@caisq \r\nHi, I try to reproduce this issue in the mnist example. It also wrong in line 928 of debug_data.py. But it is wrong with \r\n\r\n> ValueError: Node name 'accuracy/correct_prediction/ArgMax/dimension' is not found in partition graphs.\r\n\r\nMy code is as follows, I only change the mnist example's data_reader with `tf.train.batch`\r\n\r\n```\r\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\"\"\"Demo of the tfdbg curses CLI: Locating the source of bad numerical values.\r\n\r\nThe neural network in this demo is larged based on the tutorial at:\r\n  tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\n\r\nBut modifications are made so that problematic numerical values (infs and nans)\r\nappear in nodes of the graph during training.\r\n\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport sys\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\n\r\nIMAGE_SIZE = 28\r\nHIDDEN_SIZE = 500\r\nNUM_LABELS = 10\r\nRAND_SEED = 42\r\n\r\n\r\ndef test_pyfunc(x):\r\n    return x\r\n\r\n\r\ndef main(_):\r\n    # Import data\r\n\r\n    with tf.Graph().as_default():\r\n        mnist = input_data.read_data_sets(FLAGS.data_dir,\r\n                                          one_hot=True,\r\n                                          fake_data=FLAGS.fake_data)\r\n\r\n        # def feed_dict(train):\r\n        #   if train or FLAGS.fake_data:\r\n        #     xs, ys = mnist.train.next_batch(FLAGS.train_batch_size,\r\n        #                                     fake_data=FLAGS.fake_data)\r\n        #   else:\r\n        #     xs, ys = mnist.test.images, mnist.test.labels\r\n        #\r\n        #   return {x: xs, y_: ys}\r\n\r\n        images, labels = mnist.train.images, mnist.train.labels\r\n\r\n        def get_image_label():\r\n            ind = np.random.randint(len(images))\r\n            return images[ind].astype(np.float32), labels[ind].astype(np.float32)\r\n\r\n        image_tensor, label_tensor, = tf.py_func(get_image_label, [], [tf.float32, tf.float32])\r\n        image_tensor.set_shape((IMAGE_SIZE ** 2, ))\r\n        label_tensor.set_shape((NUM_LABELS, ))\r\n\r\n        x, y_ = tf.train.batch([image_tensor, label_tensor],\r\n                               batch_size=FLAGS.train_batch_size,\r\n                               num_threads=2)\r\n\r\n\r\n        # Create the MNIST neural network graph.\r\n\r\n        # # Input placeholders.\r\n        # with tf.name_scope(\"input\"):\r\n        #   x = tf.placeholder(\r\n        #       tf.float32, [None, IMAGE_SIZE * IMAGE_SIZE], name=\"x-input\")\r\n        #   y_ = tf.placeholder(tf.float32, [None, NUM_LABELS], name=\"y-input\")\r\n\r\n\r\n        def weight_variable(shape):\r\n            \"\"\"Create a weight variable with appropriate initialization.\"\"\"\r\n            initial = tf.truncated_normal(shape, stddev=0.1, seed=RAND_SEED)\r\n            return tf.Variable(initial)\r\n\r\n        def bias_variable(shape):\r\n            \"\"\"Create a bias variable with appropriate initialization.\"\"\"\r\n            initial = tf.constant(0.1, shape=shape)\r\n            return tf.Variable(initial)\r\n\r\n        def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\r\n            \"\"\"Reusable code for making a simple neural net layer.\"\"\"\r\n            # Adding a name scope ensures logical grouping of the layers in the graph.\r\n            with tf.name_scope(layer_name):\r\n                # This Variable will hold the state of the weights for the layer\r\n                with tf.name_scope(\"weights\"):\r\n                    weights = weight_variable([input_dim, output_dim])\r\n                with tf.name_scope(\"biases\"):\r\n                    biases = bias_variable([output_dim])\r\n                with tf.name_scope(\"Wx_plus_b\"):\r\n                    preactivate = tf.matmul(input_tensor, weights) + biases\r\n\r\n                activations = act(preactivate)\r\n                return activations\r\n\r\n        hidden = nn_layer(x, IMAGE_SIZE**2, HIDDEN_SIZE, \"hidden\")\r\n        y = nn_layer(hidden, HIDDEN_SIZE, NUM_LABELS, \"softmax\", act=tf.nn.softmax)\r\n\r\n        with tf.name_scope(\"cross_entropy\"):\r\n            # The following line is the culprit of the bad numerical values that appear\r\n            # during training of this graph. Log of zero gives inf, which is first seen\r\n            # in the intermediate tensor \"cross_entropy/Log:0\" during the 4th run()\r\n            # call. A multiplication of the inf values with zeros leads to nans,\r\n            # which is first in \"cross_entropy/mul:0\".\r\n            #\r\n            # You can use clipping to fix this issue, e.g.,\r\n            #   diff = y_ * tf.log(tf.clip_by_value(y, 1e-8, 1.0))\r\n\r\n            diff = y_ * tf.log(y)\r\n            with tf.name_scope(\"total\"):\r\n                cross_entropy = -tf.reduce_mean(diff)\r\n\r\n        with tf.name_scope(\"train\"):\r\n            train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\r\n                cross_entropy)\r\n\r\n        with tf.name_scope(\"accuracy\"):\r\n            with tf.name_scope(\"correct_prediction\"):\r\n                correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n            with tf.name_scope(\"accuracy\"):\r\n                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        sess = tf.Session(config=tf.ConfigProto(\r\n            allow_soft_placement=True))\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        if FLAGS.debug:\r\n            sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\r\n            sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n\r\n        tf.train.start_queue_runners(sess=sess)\r\n\r\n        # Add this point, sess is a debug wrapper around the actual Session if\r\n        # FLAGS.debug is true. In that case, calling run() will launch the CLI.\r\n        for i in range(FLAGS.max_steps):\r\n            acc = sess.run(accuracy)\r\n            print(\"Accuracy at step %d: %s\" % (i, acc))\r\n\r\n            sess.run(train_step)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\r\n    parser.add_argument(\r\n        \"--max_steps\",\r\n        type=int,\r\n        default=10,\r\n        help=\"Number of steps to run trainer.\")\r\n    parser.add_argument(\r\n        \"--train_batch_size\",\r\n        type=int,\r\n        default=100,\r\n        help=\"Batch size used during training.\")\r\n    parser.add_argument(\r\n        \"--learning_rate\",\r\n        type=float,\r\n        default=0.025,\r\n        help=\"Initial learning rate.\")\r\n    parser.add_argument(\r\n        \"--data_dir\",\r\n        type=str,\r\n        default=\"/tmp/mnist_data\",\r\n        help=\"Directory for storing data\")\r\n    parser.add_argument(\r\n        \"--ui_type\",\r\n        type=str,\r\n        default=\"curses\",\r\n        help=\"Command-line user interface type (curses | readline)\")\r\n    parser.add_argument(\r\n        \"--fake_data\",\r\n        type=\"bool\",\r\n        nargs=\"?\",\r\n        const=True,\r\n        default=False,\r\n        help=\"Use fake MNIST data for unit testing\")\r\n    parser.add_argument(\r\n        \"--debug\",\r\n        type=\"bool\",\r\n        nargs=\"?\",\r\n        const=True,\r\n        default=False,\r\n        help=\"Use debugger to track down bad values during training\")\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n```\r\n", "@hqqxyy , the reason for the weirdness you are seeing is because your session is used in a multi-threaded way, which is not amenable to command-line interface debugging. Particularly, you do,\r\n``` python\r\ntf.train.start_queue_runners(sess=sess)\r\n```\r\nafter you wrap `sess` with `LocalCLIDebugWrapperSession`. \r\n\r\nSo the program will try to launch the debugger CLI from at least 3 threads (two for your queue runners and one for your main thread that runs the `train_step` op).\r\n\r\nTo workaround this problem, you can call `start_queue_runners` first, and then wrap the `sess`, e.g.,\r\n``` python\r\ntf.train.start_queue_runners(sess=sess)\r\n\r\nif FLAGS.debug:\r\n    sess = tf_debug.LocalCLIDebugWrapperSession(sess, ui_type=FLAGS.ui_type)\r\n    sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\r\n```\r\n\r\nThis will let the debugger CLI run only on the main (training) thread. This solution assumes that you don't want to debug the data queue threads. But you do want to debug those, let me know and I can provide more info.\r\n\r\nI'll leave this bug open for now to remind myself to provide better documentation for this sort of issues.", "Update: `LocalCLIDebugWrapperSesion` now has a new keyword argument that allows you to make the debugger CLI active only on selected threads. It is called `thread_name_filter`. For example, the following line will make the debugger CLI launch only for `session.run()` calls on the main thread.\r\n\r\n```python\r\nsess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter=\"MainThread$\")\r\n```\r\n\r\nFor more details, please see doc at:\r\nhttps://github.com/tensorflow/tensorflow/commit/eb6d9d9685c0a4be542522acf5cb3f6226a7858a#diff-1c3529a3243c362193b85b6689aa24d3\r\n\r\nThis is not available in 1.1, but will be in 1.2."]}, {"number": 8752, "title": "ERROR,I tried to compile Tensorflow source code in 'compute_21' ", "body": "I tried to compile Tensorflow source code in 'compute_21' \r\n\r\n### My computer environment info\r\n    Ubuntu 16.04\r\n    CUDA 8.0\r\n    CUDNN 5.1\r\n    GCC 4.9\r\n\r\n\r\nThe commit hash : a23f5d7f7757623a4ea8c6e1d743d178a0c561c5\r\n Build label: 0.4.5\r\n\r\n\r\n\r\n### tried solutions \r\nhttps://github.com/tensorflow/tensorflow/issues/1498#issuecomment-196542169\r\nReplace the loop on L253--256 with the following:\r\n`nvccopts += r'-gencode=arch=compute_20,\\\"code=sm_21,compute_20\\\" '`\r\n\r\n### Error Logs \r\n external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h(519): error: no instance of function template \"Eigen::internal::SumReducer<T>::reducePacket [with T=Eigen::half]\"\r\n\r\n10 errors detected in the compilation of \"/tmp/tmpxft_00000f04_00000000-7_l2loss_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/bingco/tensorflow/tensorflow/core/kernels/BUILD:2611:1: output 'tensorflow/core/kernels/_objs/l2loss_op_gpu/tensorflow/core/kernels/l2loss_op_gpu.cu.pic.o' was not created.\r\nERROR: /home/bingco/tensorflow/tensorflow/core/kernels/BUILD:2611:1: not all outputs were created or valid.", "comments": ["Unfortunately, TF does not support GPUs that do not have CUDA Compute Capability older than 3.0:\r\nhttps://www.tensorflow.org/install/install_linux", "@gunan Whether can pass the modify the code to implement it\uff1f", "TensorFlow team will not work on this, as we decided to limit our official support to 3.0 and later.\r\nOlder GPUs do not bring as much speed to justify the effort and added complexity required.\r\nYou can try yourself, or try to sync with other users through stackoverflow."]}, {"number": 8751, "title": "Loading new operation - Already Exists Error", "body": "# Problem\r\nWhen loading a new op using `load_op_library`, the ***AlreadyExistsError*** is thrown.\r\nHere is the simple code:\r\n```python\r\nimport os\r\nimport h5py\r\nimport csv\r\nimport time\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport numpy as np\r\nselect_module = tf.load_op_library('./pixel_selector.so')\r\n....\r\n```\r\n\r\n# Environment Info\r\nTF v1.0.1 installed from source on Linux Ubuntu 14.04 with NVIDIA GPU Titan X and Python 2.7.6\r\n", "comments": ["Solved [here](http://stackoverflow.com/questions/43042334/loading-new-operation-in-tensorflow-alreadyexistserror)"]}, {"number": 8750, "title": "Fix code block indent in TensorFlow Linear Model Tutorial", "body": "This should fix the code blocks rendering. Right now they look like this:\r\n\r\n![selection_230](https://cloud.githubusercontent.com/assets/699961/24356320/28145b56-1302-11e7-94c0-15647bd4fa53.png)\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 8749, "title": "s ufw", "body": "Sorry about this. No clue how this got posted. :(", "comments": []}, {"number": 8748, "title": "Semi sync optimizer", "body": "Async SGD is faster than sync SGD but inferior in AUC. \r\n\r\nThis optimizer wrapper would like to provide a balance between speed and AUC. Each worker mainly works in async mode. But after certain number of batches (usually n_of_batch> worker_count but not necessarily), workers should sync together to mitigate the stale gradient issue.\r\n\r\nBased on our test, this is useful for our CTR prediction tasks.  ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Just signed the CLA.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "Thanks for sending the PR@renyi533! Before we can review the change, it looks like the commits were pushed with a different email than the email signed CLA. Could you check? Also is this PR supposed to be for r1.1 branch? ", "We cannot accept this directly into the r1.1 branch.\r\nPlease do the following:\r\n1-Make sure your commits use the same email address as the one you signed your Google CLA\r\n2-Apply your changes on top of master branch, and send a pr against master.\r\n\r\nI will close this now. Please create a new PR with the above instructions."]}, {"number": 8747, "title": "Fix broken link in Android example README", "body": "Issue  #8742 identified that the link to os_setup.html does not work for the master version.\r\n\r\nos_setup.html is still available for old versions ( https://www.tensorflow.org/versions/r0.11/get_started/os_setup#prepare_environment_for_linux ), but it makes more sense to link directly to the Bazel installation instructions.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I just signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@andrewharp do we always recommend the latest bazel version for android?", "@yifeif for Android in particular I'm unaware of a specific reason currently not to use the latest version. That's not to say there couldn't be a regression in the future, though.\r\n\r\nAs long as we verify that the known good version of Bazel works for Android maybe we should prefer that. Is there a new location for that info?", "Actually I recompiled the android sample with bazel 0.4.4 from homebrew a few days ago and failed (I forget the error message).  Upgrade to 0.4.5 solved the problem.", "Thanks for the confirmation @andrewharp, and thanks for the info @BrianOn99. One place I can think of is the android nightly build, which we do usually run with the latest bazel. Sounds like recommending the latest bazel works for now. We can update if there is regression in the future.", "Jenkins, test this please."]}, {"number": 8746, "title": "Gradients are not averaged when using GradientDescentOptimizer", "body": "When using the GradientDescentOptimizer the weights are updated using `var.device(d) -= grad * lr();`. The gradients are not averaged as we could expect. That why my model was working fine with a mini-batch gradient descent but not with the batch gradient descent.\r\n\r\nThe trick is to use `GradientDescentOptimizer(1/m * 0.01)` where `m` is the size of the batch.\r\n\r\nThe corresponding StackOverflow issue is here: http://stackoverflow.com/questions/43027109/tensorflow-weights-increasing-when-using-the-full-dataset-for-the-gradient-desce\r\n", "comments": ["The problem isn't the Optimizer, it's your loss. It should return the mean loss, not the sum. If you're doing an L2 regression, for instance, it should look like this:\r\n\r\n```\r\nl_value = tf.pow(tf.abs(ground_truth - predict), 2) # distance for each individual position of the output matrix of shape = (n_examples, example_data_size)\r\nregression_loss = tf.reduce_sum(l_value, axis=1) # distance per example, shape = (n_examples, 1)\r\ntotal_regression_loss = tf.reduce_mean(regression_loss) # mean distance of all examples, shape = (1)\r\n```\r\n\r\nPS: `tf.abs` is used for convenience, so you can replace the L2 loss for another one (like L1) without having to worry about sign changes, which would yield results in the complex plane.", "My bad (facepalm)."]}, {"number": 8745, "title": "TFRecordWriter doesn't throw an error when disk partition gets out of space", "body": "## Problem\r\nWhen tensorflow.python_io.TFRecordWriter(path) is initialized with a path leading to a device out of free storage space, one can still write() and close() it without receiving an error or exception. The resulting file is empty (0B).\r\n\r\n## Environment info\r\nTF v.1.0.1 installed from pip3 (package tensorflow-gpu) with Python 3.4.2 on a Linux server.\r\n\r\n## Minimal example\r\n    writer = tf.python_io.TFRecordWriter(output_file)\r\n    for i in files_in_shard:\r\n      # ....\r\n      # [prepare record]\r\n      writer.write(example.SerializeToString())\r\n    writer.close()\r\nWorking example e.g. https://github.com/tensorflow/models/blob/master/inception/inception/data/build_image_data.py", "comments": ["@concretevitamin, is this expected behavior for IO?", "@saxenasaurabh: can you take a look? ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 327 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "This is still an issue with v1.7.0. A loop that writes one record each iteration happily continues when the disk has run out of space."]}, {"number": 8744, "title": "DC-ASGD(Delay Compensated Asynchronous Stochastic Gradient Descent)?", "body": "DC-ASGD is Microsoft's very useful algorithm for distributed asynchronous training. Compared with the ordinary ASGD algorithm, DC-ASGD has no significant loss in speed, but can get almost the same effect as Sequential SGD. As far as I know, other mainstream deep learning open source tools have implemented this algorithm, such as: CNTK, Mxnet, Paddle and so on. But in Tensorflow I have not found similar modules.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n[https://github.com/Microsoft/CNTK/issues/1295](https://github.com/Microsoft/CNTK/issues/1295)\r\n[https://github.com/PaddlePaddle/Paddle/issues/185](https://github.com/PaddlePaddle/Paddle/issues/185)\r\n[https://github.com/dmlc/mxnet/pull/3614](https://github.com/dmlc/mxnet/pull/3614)\r\n\r\n### What other attempted solutions have you tried?\r\nI tried to implement this algorithm in Tensorflow by myself.  I do not have enough ability to do this now.\r\n\r\n### The link address of the paper\r\n[Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning](https://arxiv.org/abs/1609.08326)", "comments": ["I am also very interested with this. Any update?", "I can do this ", "@aselle Tensorflow's documentation on distributed computing leads me to believe that the end user is responsible for assigning workers and parameter servers to different devices. Should I then only implement Gradient Descent with Delay Compensation, which the user will have to implement asynchronously?", "@mrry, might be able to give you more direction in this vein (also @tfboyd )", "I haven't read the paper in depth, but from a quick skim of the paper you could implement the update rule as a `tf.train.Optimizer` subclass, and the asynchronous execution would be provided by whatever training loop the user used. (It would presumably \"work\" in a synchronous case as well, because *w<sub>t</sub>* &ndash; *w<sub>bak</sub>* would be zero, and hence it would devolve into classic SGD.)", "I already implemented this. Currently, I am verifying its impact on metrics, such as AUC, in our ads data. I will send the pull request if this is really useful. ", "Is support for the adaptive variance parameter version of DC-ASGD planned on being added? The paper found that it performed better in all cases than the constant version which is currently implemented. ", "Closing this issue, as the PR for DC-ASGD was merged ([#9551](https://github.com/tensorflow/tensorflow/pull/9551)). Thank you! \ud83d\udc4d "]}, {"number": 8743, "title": "Issue when installing from source", "body": "Operating System: Ubuntu 16.04\r\ngcc version: 5.3.0\r\nCUDA: 8.0\r\ncuDNN: v5.1.10 \r\ncommit hash: `bbe056e5a0ab81b67fcb6053400812b3d5805fc7`\r\nbazel version: 0.4.5\r\n\r\nI've installed CUDA and cuDNN correctly, and I'm trying to install TF from source. After performing the configure, when running the command: \r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nan error shows that:\r\n`INFO: From Compiling external/nccl_archive/src/libwrap.cu.cc:\r\ngcc: error trying to exec 'as': execvp: No such file or directory`\r\n`ERROR: /home/machine/.cache/bazel/_bazel_machine/3ded171561e3ac30c3410854211a1a62/external/nccl_archive/BUILD:33:1: output 'external/nccl_archive/_objs/nccl/external/nccl_archive/src/libwrap.cu.pic.o' was not created.`\r\n`ERROR: /home/machine/.cache/bazel/_bazel_machine/3ded171561e3ac30c3410854211a1a62/external/nccl_archive/BUILD:33:1: not all outputs were created or valid.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.`\r\n`INFO: Elapsed time: 120.107s, Critical Path: 57.71s`\r\n\r\nIt seems that there's something wrong with my gcc. However, I've reinstalled it and the error still occurs.\r\nI wanna know if it is the issue of cross-platform options.", "comments": ["The real error message seems to be missing from your logs.\r\nCould you upload all of your terminal output and share the link here?", "Have you tried creating a symlink in /usr/local/bin with the path to execvp()?", "I checked again, and this may be your problem:\r\n```\r\nINFO: From Compiling external/nccl_archive/src/libwrap.cu.cc: gcc: error trying to exec 'as': execvp: No such file or directory\r\n```\r\nLooks like `as` is not installed in your system\r\nCould you try `sudo apt-get install build-essential`", "Closing due to inactivity (don't see an update after the suggestion in the previous comment).\r\n\r\nIf you're still having trouble, please feel free to file a new, updated issue.\r\n"]}, {"number": 8742, "title": "Broken Link in README.md", "body": "In https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android/README.md,\r\nthe link of _Get the recommended Bazel version listed in os_setup.html_ which is pointing to https://www.tensorflow.org/versions/master/get_started/os_setup#source is broken.", "comments": []}, {"number": 8741, "title": "temperature will be higher when I using two different cards in the same machine", "body": "Hey, guys, I am trying to train a CNN model. My machine has 2 K40. When I run my model in the second card, the temperature  will be higher than running in the first card, and the training will be slower and slower.\r\n\r\n\r\n", "comments": ["Is the usage the same in nvidia-smi? Or are you running it on one card exclusively? This might not be a tensorflow issue so I recommend filing a question on stack overflow. In the mean time you could try some primitive troubleshooting by physically removing and swapping cards to clarify if it is a bug caused by device id allocation (doubt it) or mobo or the card itself. \r\n\r\nIt may just run hotter and slower because of the cooling for the card or the power consumption. Monitor through nvidia-smi and when you are sure it is a tensorflow issue then I would suggest re-filing as this might get closed! Good luck!", "Thanks a lot. I will have a try. ", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8740, "title": "Update broken link in README.md", "body": "Updated link (highlighted line number).\r\nIt was pointing to Line number of a old version of the file.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8739, "title": "differentiate package description and license", "body": "differentiate license and package desription. use /* */ make code more clear.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please.", "jenkins plug-in issue, trying again. @tensorflow-jenkins test this please."]}, {"number": 8738, "title": "configure error", "body": "1. git clone https://github.com/tensorflow/tensorflow\r\n2. cd tensorflow\r\n3. ./configure\r\n\r\nSince our server has no GPU, we configure cuda option with no but error occurs by cuda.\r\n\r\n`ERROR: /home/kesl/kms_tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl:828:18: unexpected keyword 'environ' in call to repository_rule(implementation: function, *, attrs: dict or NoneType = None, local: bool = False).\r\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'third_party/gpus/cuda_configure.bzl' has errors.\r\n` \r\n\r\nBut if I configure same thing with branch r1.0 version, it works.", "comments": ["Please update bazel to 0.4.5 and retry.", "@gunan thank you!! It works now!"]}, {"number": 8737, "title": "[CMake] Enabling CPU optimization with MSVC", "body": "Updating CMake docs to add clarity about how to enable advanced CPU instructions \r\nwith MSVC.\r\n\r\nReference: [comment and following ones](https://github.com/tensorflow/tensorflow/issues/8724#issuecomment-289313811)\r\n\r\ncc @mrry ", "comments": ["Can one of the admins verify this patch?", "Can one of the admins verify this patch?", "@mrry Derek could you take a look? Thank you!", "Jenkins, test this please\r\n"]}, {"number": 8736, "title": "Fallback to legacy camera API for problematic devices in android demo app", "body": "Fix #8550 \r\nThe camera 2 API is problematic in some devices.  Similar problem is also dicussed in #7464 .  One of the problem is shown here on a LG G4c device:\r\n![screenshot_2017-03-27-00-47-09](https://cloud.githubusercontent.com/assets/8319689/24341265/23380fd6-12eb-11e7-91f5-f450eded4a42.png)\r\n\r\nA space inversion is taken (left-right mirrored), and color turns green (which is reported also here https://code.google.com/p/android/issues/detail?id=81984).  It effectively makes all 3 demo app not working or crashes.\r\n\r\nThis PR attempts to take legacy camera API as fallback, in case `INFO_SUPPORTED_HARDWARE_LEVEL` is lower than FULL.  It is mostly a copy of the implementation [here](https://github.com/hamidb/tensorflow/blob/api20/tensorflow/examples/android/src/org/tensorflow/demo/CameraConnectionFragment.java).  Here is the working demo if fallback is taken:\r\n![screenshot_2017-03-27-00-48-19](https://cloud.githubusercontent.com/assets/8319689/24341255/06df2fd6-12eb-11e7-81bf-bdcc10fade4d.png)\r\n\r\nI would like to mention that I have no previous experience with the API, so the implementation may not be optimal.  Also please check if the demo still works on Android6+ devices because I only have 2 android5 for testing.  Please also notice that the \"TF Detect\" demo does not show red box on preview image, for which I cannot find out the cause.\r\n\r\n![screenshot_2017-03-27-00-35-07](https://cloud.githubusercontent.com/assets/8319689/24341234/dd392d44-12ea-11e7-8a53-3fdd9de67eff.png)\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@andrewharp could you take a look? Thanks!", "I agree that I have made the demo a bit more complicated and that should be avoided.", "Please also notice that I have hard-coded rotation to be 90 in the legacy camera API, and I don't know how to set it properly.", "@BrianOn99 Could you pull rebase and push again?\r\n@andrewharp is there any better way to set the rotation in the legacy camera API?", "@drpngx @BrianOn99 The demo currently assumes a 90 degree rotation, so I don't see an issue with this, unless I'm misunderstanding something. It should be something we can leave a TODO for and fix for both implementations at a later time.", "@drpngx So do I need to `git push --force`?", "Did you rebase from upstream/master? I think it's usually fine to push force in that case. But do save the SHA1s in case you need to go back to them.", "During rebase I get this error:\r\n\r\n```\r\n~/A/t/tensorflow \u276f\u276f\u276f bazel build --jobs=2 -c opt //tensorflow/examples/android:tensorflow_demo                                           \u23ce master \u2731 \u25fc\r\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.build/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/DataType.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Graph.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/NativeLibrary.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Operation.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/OperationBuilder.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Output.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/SavedModelBundle.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Session.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Shape.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/Tensor.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlow.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/TensorFlowException.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:58:12: in srcs attribute of android_library rule //tensorflow/contrib/android:android_tensorflow_inference_java: please do not import '//tensorflow/java:src/main/java/org/tensorflow/package-info.java' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nERROR: /home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/contrib/android/BUILD:72:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n\tFile \"/home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/workspace.bzl\", line 98\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"/home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/workspace.bzl\", line 87, in _apply_patch\r\n\t\t_execute_and_check_ret_code(repo_ctx, [\"patch\", \"-p1\", \"-d\", r...), <2 more arguments>])\r\n\tFile \"/home/o/AndroidStudioProjects/ttens/tensorflow/tensorflow/workspace.bzl\", line 79, in _execute_and_check_ret_code\r\n\t\tfail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d /home/o/.cache/bazel/_bazel_o/18da53e8d12eeb6697178b6abb9cee58/external/protobuf -i /home/o/AndroidStudioProjects/ttens/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout: \r\nStderr: java.io.IOException: Cannot run program \"patch\" (in directory \"/home/o/.cache/bazel/_bazel_o/18da53e8d12eeb6697178b6abb9cee58/external/protobuf\"): error=2, No such file or directory and referenced by '//tensorflow/contrib/android:libtensorflow_inference.so'.\r\nERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.\r\n```\r\n\r\nI cannot find such a build issue from the internet.  Even a fresh clone and build from master give this issue, while building from my old branch still works.  Any one has idea?", "I am building in this environment:\r\nbazel 0.4.5\r\narch linux\r\ntensorflow commit 34c738cc6d3badcb22e3f72482536ada29bd0e65", "git bisect tells me the offending commit is 8d393ea2fab0ea88ecd11e36d89f186cbc884dbe.  Looks like due to the changes in `tensorflow/workspace.bzl`?", "@jart any idea what might be happening here?\r\n\r\n@BrianOn99 please rebase and push again when you get a chance.", "Is the program `patch` installed from the Arch package manager?", "@jart yes", "@jart It is installed from the package manager as far as I can remember.  I will verify it tonight.", "You are right patch is not installed :-P.  Never thought of this stupid mistake.  Now it has been rebased.  Again, I have not tested it on Android6+ devices.  ", "Jenkins, test this please.", "It does not appear to build\r\n\r\n```\r\nERROR: in target '//external:android/crosstool': no such package '@androidndk//': /home/o/Android/android-ndk-r12b (No such file or directory).\r\n```", "Pushed again and should be OK now.", "We probably don't want any references to android.hardware.Camera or android.hardware.Camera2 in the Activity classes, apart from CameraActivity. Right now there are 3 methods in each activity for handling incoming frames -- I think we can reduce that to 1 if possible and let CameraActivity itself handle supporting the different camera hardwares.", "ping for @BrianOn99 for next steps!", "@andrewharp @vrv The main reason for this quite messy implementation is:\r\n1. Some clean-up code is required for Camera ( specifically `camera.addCallbackBuffer(bytes);`), but not for Camera2.\r\n2. In `DetectorActivity`, the luminance plane is required.  There is different code for the Camera and Camera2 API to extract the luminace plane from a YUV420 format.  If we hide the difference between the API by giving just an RBGA buffer, it makes the luminance plane difficult to extract.  In fact I don't really understand how `DetectorActivity` works so I hope someone else can help.", "@BrianOn99 I suppose you could modify the method prototype to also take in an optional luminance plane. I think that would be better than having Camera references and duplicate processing code creeping into the actual Activity implementations.\r\n\r\nRegarding camera.addCallbackBuffer(bytes) perhaps just have two buffers and return the old one when you get the new one.", "ping for @BrianOn99 !", "Can one of the admins verify this patch?", "@BrianOn99 any update on this?", "@rmlarsen I don\u2019t yet have time to improve the code.  You are welcome to fix it.", "Hello, I am working refactoring the android demo to work with the camera api1 but I am having troubles with the tracker detector(DetectorActivity.java around line 250) \r\n```\r\n final Plane[] planes = image.getPlanes();\r\n      fillBytes(planes, yuvBytes);\r\n\r\n      tracker.onFrame(\r\n          previewWidth,\r\n          previewHeight,\r\n          planes[0].getRowStride(),\r\n          sensorOrientation,\r\n          yuvBytes[0],\r\n          timestamp);\r\ntrackingOverlay.postInvalidate();\r\n```\r\nthat object requires a rowStride wich is provided by the camera api 2 Image object, but  the camera api1 just provides a simple byte[] `public void onPreviewFrame(final byte[] bytes, final Camera camera) `\r\nAny ideas how can I code that section fore the camera api1? ", "@osdamv You should be able to assume that width == stride when using the camera 1 api. This is always the case afaik.", "@osdamv any luck with this?", "@drpngx I can't make work the detector demo with the legacy api, however the another examples work fine .\r\nThe class MultiBoxTracker(its job is draw the boxes, the detection works fine) pretty much assumes everything would have the api2, IMHO  the detector demo requieres a lot of more of refactoring and sadly is a part I don't need for my project, if you wish I can upload my changes ", "@osdamv The object detector demo is able to work without tracking support (it does this when it can't find the native libraries), so if you want, just disable tracking when in fallback mode and I can fix that functionality later.", "@andrewharp Is there any format needed for the commit message? ", "@osdamv For individual commits? Not really, just a blurb about what code changed. The overall PR description is what gets mentioned in the changelog.", "Please see #10771 for a improved PR by @osdamv .", "@osdamv has done some great work with that pull request but the detector demo is still problematic. Not sure whether this issue is fully resolved. Although @andrewharp said he would try and find some time to fix the misbehaving code. I guess the detector issue could be continued on #10348 perhaps?"]}, {"number": 8735, "title": "Fix no attribute prediction_key error", "body": "fix no attribute prediction_key.\r\n\r\n```iris_monitors.py\r\ntf.contrib.learn.prediction_key.PredictionKey.CLASSES),\r\nAttributeError: module 'tensorflow.contrib.learn' has no attribute 'prediction_key'\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Excuse me.\nI signed Contributor License Agreement (CLA).\n\n\n\n2017-03-27 12:10 GMT+09:00 googlebot <notifications@github.com>:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address. Check your\n>    existing CLA data <https://cla.developers.google.com/clas> and verify\n>    that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If you signed the CLA as a corporation, please let us know the\n>    company's name.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8735#issuecomment-289343680>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFgOrQWPSCR4Xp-1pnoxHWaluTTQv6PHks5rpyi_gaJpZM4Mpvky>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please."]}, {"number": 8734, "title": "Tensorflow GPU \"no known devices\" after update from version 0.12.1 to 1.0.1 - everything was fine before", "body": "### Environment info\r\n\r\nOperating System:\r\n\r\nUbuntu Linux 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n\r\nCUDA 8.0.44\r\ncuDNN 5.1.10\r\nDriver version 367.57.0\r\n\r\nOutput of `ls -l /path/to/cuda/lib/libcud*`:\r\n\r\n-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 84163560 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 Mar 27 11:43 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\nA link to the pip package you installed: I don't know what you mean by that :(\r\n\r\nTensorflow versions:\r\n0.12.1 before the issue\r\n1.0.1 caused the issue\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nOn an AWS g2.2xlarge instance with\r\n    * Ubuntu 16.04, \r\n    * CUDA and cuDNN versions as listed above\r\n    * tensorflow-gpu 0.12.1 installed using pip\r\nrun the example code from the [tensorflow \"Using GPUs\" website](https://www.tensorflow.org/tutorials/using_gpu).\r\n\r\nThe output for me is:\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GRID K520\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\r\npciBusID 0000:00:03.0\r\nTotal memory: 3.94GiB\r\nFree memory: 3.91GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0\r\nI tensorflow/core/common_runtime/direct_session.cc:255] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GRID K520, pci bus id: 0000:00:03.0\r\n\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0\r\nb: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] b: (Const)/job:localhost/replica:0/task:0/gpu:0\r\na: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:827] a: (Const)/job:localhost/replica:0/task:0/gpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]\r\n```\r\n\r\nNow update Tensorflow\r\n\r\n`pip install tensorflow-gpu --upgrade`\r\n\r\nand run the example code again. Now the output for me is:\r\n\r\n```\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nDevice mapping: no known devices.\r\nI tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:\r\n\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/cpu:0\r\nb: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] b: (Const)/job:localhost/replica:0/task:0/cpu:0\r\na: (Const): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] a: (Const)/job:localhost/replica:0/task:0/cpu:0\r\n[[ 22.  28.]\r\n [ 49.  64.]]\r\n```\r\n\r\nIt's not even giving me any hints as to what might be the problem, it just says \"no known devices\". \r\n\r\nHow come the GRID K520 was a known device **just** before I updated Tensorflow and now it isn't anymore?\r\n\r\nNothing else happened in the meantime apart from updating Tensorflow, no other changes to the system were made in any way (at least not from my side).\r\n\r\n### Logs or other output that would be helpful\r\n\r\nnvidia-smi output:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.57                 Driver Version: 367.57                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |\r\n| N/A   39C    P0    38W / 125W |      0MiB /  4036MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["I think @tfboyd was able to run TF on AWS successfully. Could you comment?", "@gunan It's been a while so I'm not a hundred percent sure if I remember correctly, but I believe the issue was that an automatic unattended NVIDIA GPU driver update that ran in the background broke everything, because the K520 GPUs in the AWS g2.2xlarge instances were too old and incompatible with whatever driver version updater tried to install. It left the system in an inconsistent state. That whole thing must have happened in the background exactly when I updated TensorFlow, and so I falsely attributed the failure to the TF upgrade. I never really found out whether this was actually the problem since I just stopped using AWS shortly after, but I think you can close this issue.", "@pierluigiferrari Thank you very much for the status update. Then I will close this issue."]}, {"number": 8733, "title": "[WIP] Add a cache layer to record_reader", "body": "#8715", "comments": ["Can one of the admins verify this patch?", "test code:\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport logging\r\nimport argparse\r\nimport os.path\r\nimport sys\r\nimport math\r\nimport time\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.ops import control_flow_ops\r\n\r\n\r\ndef train_fn(sess,label):\r\n  train_fn.counter += 1\r\n  train_start_time = time.time()\r\n  result = sess.run(label)\r\n  duration = time.time() - train_start_time\r\n  print(\"step %d: count = %g, dur = %g\" % (train_fn.counter,len(result),duration))\r\n\r\ndef main(_):\r\n    logger = logging.getLogger('tensorflow')  \r\n    logger.setLevel(logging.INFO)\r\n    global_step = tf.train.get_or_create_global_step()\r\n    reader = data_flow_ops.RecordInput(\"D:\\\\testdata2\\\\tf\\\\*.tfrecord\",parallelism=1,buffer_size=1024,name=\"record_input\",batch_size=40960)\r\n    y = reader.get_yield_op()\r\n    features = tf.parse_example(\r\n        y,\r\n        # Defaults are not specified since both keys are required.\r\n        features={\r\n            'l': tf.FixedLenFeature([], tf.float32),\r\n            'v': tf.VarLenFeature(tf.int64)\r\n        })\r\n    label = features['l']\r\n    features = features['v']\r\n    # Ensure the train_tensor computes grad_updates.\r\n    train_op = control_flow_ops.with_dependencies([global_step], label)  \r\n    train_fn.counter = 0\r\n\r\n    sv = tf.train.Supervisor(logdir=\"t3\",save_summaries_secs=1,global_step = global_step)\r\n    tf.train.basic_train_loop(sv,train_fn,[train_op])\r\n  \r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\nbefore:\r\nINFO:tensorflow:global_step/sec: 0\r\nstep 1: count = 40960, dur = 0.215178\r\nstep 2: count = 40960, dur = 0.206151\r\nstep 3: count = 40960, dur = 0.206683\r\nstep 4: count = 40960, dur = 0.214185\r\nINFO:tensorflow:global_step/sec: 0\r\nstep 5: count = 40960, dur = 0.208147\r\nstep 6: count = 40960, dur = 0.205167\r\nstep 7: count = 40960, dur = 0.206236\r\nstep 8: count = 40960, dur = 0.204632\r\nstep 9: count = 40960, dur = 0.203644\r\nINFO:tensorflow:global_step/sec: 0\r\nstep 10: count = 40960, dur = 0.205663\r\nstep 11: count = 40960, dur = 0.203331\r\nstep 12: count = 40960, dur = 0.208636\r\nstep 13: count = 40960, dur = 0.207647\r\n\r\nAfter:\r\nstep 1: count = 40960, dur = 0.0535386\r\nstep 2: count = 40960, dur = 0.0370266\r\nstep 3: count = 40960, dur = 0.0395277\r\nstep 4: count = 40960, dur = 0.038027\r\nstep 5: count = 40960, dur = 0.038027\r\nstep 6: count = 40960, dur = 0.0395284\r\nstep 7: count = 40960, dur = 0.0350249\r\nstep 8: count = 40960, dur = 0.0355251\r\nstep 9: count = 40960, dur = 0.0350249\r\nstep 10: count = 40960, dur = 0.0355251\r\nstep 11: count = 40960, dur = 0.0370266\r\nstep 12: count = 40960, dur = 0.0385275\r\nstep 13: count = 40960, dur = 0.0370264\r\nstep 14: count = 40960, dur = 0.0370262\r\nstep 15: count = 40960, dur = 0.0375264", "@saxenasaurabh could you take a look at this change? Thanks!", "@saxenasaurabh feel free to comment directly here or LMK if you don't have cycles for this.", "Two unitests failed due to this new implementation doesn't support seek operation. I'm thinking how to resolve it.", "Jenkins, test this please.", "@snnn any progress? ", "Sorry, I'm in vacation. Let me close it first."]}, {"number": 8732, "title": "Update README.md", "body": "add space to fix separation line rendering", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8731, "title": "Error from 'bazel build' after r1.1.0: TensorFlow is not configured to build with GPU support", "body": "### Environment info\r\nOperating System: Ubuntu Desktop 16.04\r\n\r\nInstalled version of CUDA and cuDNN:  8.0, 5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\nlrwxrwxrwx 1 root root       19 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root   558720 Sep 14  2016 /usr/local/cuda/lib64/libcudadevrt.a\r\n-rw-r--r-- 1 root root   775162 Sep 14  2016 /usr/local/cuda/lib64/libcudart_static.a\r\n-rw-r--r-- 1 root root 69756172 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn_static.a\r\nlrwxrwxrwx 1 root root       16 Sep 14  2016 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\n-rwxr-xr-x 1 root root 79337624 Jan 20 18:48 /usr/local/cuda/lib64/libcudnn.so\r\n```\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`): \r\nbbe056e5a0ab81b67fcb6053400812b3d5805fc7 (master branch)\r\n\r\n2. The output of `bazel version`:\r\n\r\n```\r\nBuild label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n```\r\n\r\n----------------------------------------------------------------------\r\n\r\n### If possible, provide a minimal reproducible example. \r\n\r\n\r\n* __Summary__: bazel build says TensorFlow is not configured with GPU support even though ./configure confirmed CUDA support will be enabled. Main error output seems to be (full error shown further down):\r\n```\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. \r\n```\r\nPlease note that I've done this many times so (hopefully) I don't think I'm overlooking some common mistake on my end. Details below.\r\n\r\n* __./configure info__: Here are the non-default (i.e. not just hitting enter) configuration flags I give to ./configure and its corresponding outputs:\r\n\r\n```\r\n./configure \r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] y\r\nXLA JIT support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\n  /home/brandon/bin/caffe/python\r\n  /usr/local/boost_1_63_0\r\nUsing python library path: /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: 5\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with: 6.1 \r\n```\r\n\r\n* __Error from bazel build__: Then I run bazel build as follows with the error message below. \r\n\r\n```\r\n bazel build --config=opt --config=cuda --local_resources 8192,4.0,1.0 //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/brandon/.cache/bazel/_bazel_brandon/9d57692c8667febd112ef76b5ba08968/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 0.199s\r\n```\r\n\r\n-------------------------------------------------------------------------------\r\n\r\nI'm running a GTX 1080. If it's useful as a sanity-check, I've included a portion of nvidia-smi below:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   45C    P8    17W / 240W |    347MiB /  8110MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\nAny idea why bazel thinks TF configured without GPU support? Also, I should note that ./configure did not run initially, but then I ran ``sudo apt-get upgrade bazel``` and that fixed the issue.  __My best guess is this is related to ./configure not playing nice with the new bazel upgrade__ or something similar. This was the first time I've had to upgrade bazel in awhile just to run ./configure. Thanks for any help/suggestions. ", "comments": ["I ran into the same issue, with the difference being I compiled bazel 0.4.5 from source. Also compiling for a p100, using CUDA compute capability 6.0 as per specifications.  \r\n\r\nAll other info is the same as @mckinziebrandon. \r\n\r\nSwitching to branch r1.1 seems to fix the issue for me (head at a23f5d7).", "Thanks for the suggestion to switch branches @p-smirnov , I'll make sure to try re-running build tonight when I get the chance. Strange that a couple days could make all the difference!", "We have recently bumped the minimum bazel requirement to 0.4.5, so having to run bazel update is expected.\r\nHowever, our checks for bazel version seem to be broken, I will look into that.\r\n\r\nJust an idea, could you try running `bazel clean` right before `./configure` to see if it helps with the error?", " @p-smirnov your suggestion worked for me as well. Had no issue building that branch. Thanks for the tip!\r\n\r\n@guanan I just followed your suggestion (ran bazel clean before configure) and still got the same build error. Configure works fine, but running the build command results in the error message in my post. ", "OK, thanks for checking.\r\nI will investigate what has changed in master to cause this. we should only have about 10 days of updates after 1.1 branch cut.", "If it helps, the file local_config_cuda/crosstool/BUILD seems to written correctly when I examine it before running bazel build. After running the build however it is overwritten to raise a cuda not configured error. ", "Interesting, thanks for the additional information.\r\n@damienmg Could this be related to recent changes we made to TF build configs?", "Also, I noticed the Jenkins build is passing, but it builds on an Intel x86_64. I was compiling for a ppc64 architecture, so that may be why the CI isn't catching the error.", "What's in the .bazelrc file? If the --action_env change is not in, then if the environment is dropped, the CUDA configuration get unconfigured.", "https://github.com/tensorflow/tensorflow/commit/fba05c300bf6840e76787680ed7fd1239cdb9ad0 is not in 1.1 branch", "Has this been resolved? I'm gonna close, but please lemme know if I should reopen."]}, {"number": 8730, "title": "Fix broken links in `contrib/learn/python/learn/README.md`", "body": "This fix fixes broken links in `contrib/learn/python/learn/README.md`\r\nby replacing g3doc with appropriate URLs, e.g.,\r\n```\r\n-    ([docs](../../../../g3doc/api_docs/python/contrib.learn.md#LinearClassifier))\r\n+    ([docs](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/LinearClassifier))\r\n```\r\n\r\nNOTE: Inside `contrib/learn/python/learn/README.md` there are still\r\nseveral `g3doc` references after this fix. However, the remaining links are actually\r\ncorrect and are not broken.\r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8729, "title": "conv1d_transpose not implemented", "body": "I would like to perform a conv1d_transpose, but I can\u2019t see any implementation in tensorflow.\r\n\r\nI guess that inserting zeros between each values and then applying a regular conv1d would do the job ?\r\n\r\n(principle explained hereafter with conv2d_tranpose : https://github.com/tensorflow/tensorflow/issues/4306)\r\n\r\nBut I can't find a way to implement this kind of operation with tensorflow :\r\n\r\nimport numpy as np\r\narr = np.arange(1, 10)                 # array([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\nnp.insert(arr, slice(1, None), 0)  # array([1, 0, 2, 0, 3, 0, 4, 0, 5, 0, 6, 0, 7, 0, 8, 0, 9])\r\n\r\nAny ideas ?\r\n\r\nThank you", "comments": ["Likely we will not get to this feature request quickly. I'd suggest asking on stackoverflow if someone has an elegant way to implement this. Otherwise, we'd welcome a contribution from anybody who has time to implement this. To insert zeros you might try using \r\n```\r\narr = np.arange(1, 10) # array([1, 2, 3, 4, 5, 6, 7, 8, 9])\r\nfoo = tf.zeros_like(arr)\r\nstacked = tf.stack([arr, foo], axis=1)\r\nzeros_inserted = tf.reshape(stacked, (stacked.get_shape().num_elements(), ))\r\n```\r\n\r\n", "@aselle I can work on this", "conv1d internally invokes conv2d ([here](https://github.com/tensorflow/tensorflow/blob/3836e7b94a06e1e53e77af839c6c6482565444f9/tensorflow/python/ops/nn_ops.py#L2019)). Should conv1d_transpose, similarly, invoke conv2d_tranpose?", "Hi,\r\nI'm not very good at this (Started working with tf a few weeks ago after finals), but I was wondering if you could use tf.expand_dims for a quick fix\r\n\r\nlike \r\n\r\nnew = tf.expand_dims(old,1)\r\nand than conv2d_transpose?\r\n", "Expanding dims for conv2d_transpose works fine for me.\r\nOne small gotcha is that your filter variables will need to be expanded on axis 0, not axis 1, since the conv2d uses the filter width, not filter height.  So:\r\n\r\nnew_x = tf.expand_dims(old_x, 1)\r\nnew_W = tf.expand_dims(old_W, 0)\r\ndeconv = tf.nn.conv2d_transpose(new_x, new_W, strides, output_shape)\r\ndeconv = tf.squeeze(deconv, axis=1)\r\n", "Added a PR #13105 for conv1d_transpose."]}, {"number": 8728, "title": "PollZmqOp for network communication", "body": "This PR adds `PollZmqOp` for retrieving data over the network (fixes #4836, ref #7951) . A few points for discussion:\r\n\r\n* ZeroMQ is LGPL but has some exceptions which seem to allow static linking. Is that a problem?\r\n* The `Compute` method of the Op is ensured to be thread safe by blocking. This could be improved by having a dictionary of sockets keyed by thread id or a connection pool. However, I have never needed concurrent calls to `Compute` in my work.\r\n* I'm not very familiar with `bazel` so there may be some problems in the dependencies.\r\n\r\nThis op allows for distributed training data generation to minimize the CPU workload on the training machine. For example, one or more machines can generate training data which can be retrieved by a training machine using the op. An example can be found in the test.", "comments": ["Can one of the admins verify this patch?", "Thanks @tillahoffmann! @mrry assign to you because you are working on the new input pipeline. Feel free to reassign as you see fit. Thank you :)", "Here is a small example on how to learn the mean of a distribution by generating data in two separate processes and consuming the data in a single process.\r\n\r\n```python\r\n# consumer.py\r\nimport tensorflow as tf\r\n\r\n# Define parameters\r\naddresses = [\"tcp://localhost:5555\", \"tcp://localhost:5556\"]\r\nbatch_size = 100\r\nmin_after_dequeue = 1000\r\nqueue_capacity = min_after_dequeue + 3 * batch_size\r\n\r\nwith tf.Graph().as_default():\r\n    # Create a queue to hold the data\r\n    queue = tf.RandomShuffleQueue(\r\n        queue_capacity,\r\n        min_after_dequeue,\r\n        [tf.float32], \r\n        [[]],\r\n        ['x']\r\n    )\r\n    \r\n    # Add zmq ops\r\n    enqueue_ops = []\r\n    for address in addresses:\r\n        raw = tf.poll_zmq(b\"\", address, 1000)\r\n        decoded = tf.decode_raw(raw, tf.float32)\r\n        enqueue_ops.append(queue.enqueue_many({'x': decoded}))\r\n        \r\n    # Add a queue runner for the enqueue ops\r\n    qr = tf.train.QueueRunner(queue, enqueue_ops)\r\n    tf.train.add_queue_runner(qr)\r\n    \r\n    # Get the data\r\n    data = queue.dequeue_many(batch_size)\r\n    # Compute the loss\r\n    mean = tf.Variable(0, dtype=tf.float32)\r\n    loss = tf.reduce_mean((data['x'] - mean) ** 2)\r\n    \r\n    # Add optimization op\r\n    train_op = tf.train.AdamOptimizer(1).minimize(loss)\r\n    \r\n    # Setup\r\n    sess = tf.Session()\r\n    sess.run(tf.global_variables_initializer())\r\n    tf.train.start_queue_runners(sess)\r\n    \r\n# Run the training\r\nlosses = []\r\nfor _ in range(100):\r\n    _, loss_val = sess.run([train_op, loss])\r\n    losses.append(loss_val)\r\n    \r\nprint(sess.run(mean))\r\n```\r\n\r\n```python\r\n# producer.py\r\nimport zmq\r\nimport sys\r\nimport numpy as np\r\n\r\n# Get the port to listen on and define the ground truth mean\r\nport = int(sys.argv[1])\r\nmean = 15\r\n\r\n# Create a socket and listen\r\ncontext = zmq.Context()\r\nsocket = context.socket(zmq.REP)\r\nsocket.bind(\"tcp://*:%d\" % port)\r\n\r\n# Respond to requests\r\nwhile True:\r\n    message = socket.recv()\r\n    socket.send(np.random.normal(mean, 1, 1000).astype(np.float32).tostring())\r\n```", "Help someone is hacking me and played me for the idiot I am. I was trying to learn how to write code. I thought I was doing something good I didn't know it did this. I was trying to do some good. My reference was to thinking it had been done wrong. I was wrong. I pulled the cla when I realized what it did immediately and they are driving me crazy. I reset my phone to give off a warning and hoped they would leave me alone but they still won't stop. I was only trying to figure out an equation. I never wrote any code. I thought I was talking to Nvidia and tensor flow. There was a device listed on my account that isn't mine. ", "@tillahoffmann Thanks for submitting this PR! I have some thoughts about the interface, but many of these could be postponed by moving the implementation to `tf.contrib` while we iterate on the design.\r\n\r\n* For most new stateful ops, we try to separate the state from the op-kernels that act on it. This would entail splitting the op into two: a \"constructor\" op that sets up the ZMQ socket/context and adds an object to the `ResourceMgr`, and an \"accessor\" op that retrieves the object from the `ResourceMgr` and invokes a method on it. (For examples, see the implementations of the readers, queues, variables, etc.) The benefits of doing this are clearer when there are multiple implementations of the same state (e.g. a similar RabbitMQ or Kafka resource? Or maybe various RPC protocols?) or multiple \"accessor\" ops that access the same state, and it would be good to understand if we'd like to support such things before adding this to the core library.\r\n\r\n* I assume that the `zmq_msg_recv()` call can block. This would block one of the (finitely many) TF thread pool threads, and could lead to deadlock. Typically we would implement such an operation as an `AsyncOpKernel`. I'm not familiar enough with ZMQ to know the best way to receive a message asynchronously, but hopefully there is a standard pattern that we could use here.\r\n\r\n(Regarding the licensing question, I don't know what the consequences are, but @martinwicke probably does (or knows someone who does). Regarding the post from @RealTimeDeployment, I don't know what we can do to help on this thread, but I'd suggest opening a new issue if you continue to have problems.)", "@mrry, thanks for the feedback. I'm happy to leave this PR open and iterate on the design here unless you have a strong preference to merge into `tf.contrib` first.\r\n\r\n1. I'll have a look into splitting the op into a state and an accessor as suggested (do you have a particular state/op combination in mind that is most closely related?). For now, I will hold off until we've heard from @martinwicke regarding the legal aspects.\r\n2. Yes, `zmq_msg_recv` does indeed block. Once 1. is addressed, I will move the accessor to an async op.", "Yeah, let's wait until we have clarity on the licensing issues. \r\n\r\nIt would be our preference to merge into contrib first if at all possible.", "Sure, I'll make the appropriate changes for merging into contrib once we know more regarding licensing. ", "The ZMQ dependency is fine. \r\n\r\nLet's move this to contrib, and we can iterate on details there.\r\n\r\nThanks!", "The original PR is now migrated to `contrib`. Note that:\r\n\r\n* I haven't really got my head around `bazel` yet so the `BUILD` files are copied together from different modules in `contrib`. They can likely be condensed.\r\n* The ZMQ dependency is still in the main `tensorflow/workspace.bzl`. \r\n\r\n@mrry do you want to merge the existing interface into `contrib` first and then move to the constructor/accessor design in a subsequent PR?", "(OK, just FYI I checked internally and ZMQ licensing is fine.)", "@mrry, long-term it may be worthwhile thinking about mirroring python's object-oriented ZMQ interface to give users maximum flexibility. Thoughts?", "Jenkins, test this please.\n\nOn Apr 20, 2017 1:30 PM, \"Till Hoffmann\" <notifications@github.com> wrote:\n\n> *@tillahoffmann* commented on this pull request.\n> ------------------------------\n>\n> In third_party/libzmq.BUILD\n> <https://github.com/tensorflow/tensorflow/pull/8728#discussion_r112553031>\n> :\n>\n> > @@ -0,0 +1,241 @@\n> +package(default_visibility = [\"//visibility:public\"])\n> +\n> +licenses([\"notice\"])  # LGPL license with exception for static linking (for libzmq)\n> +\n> +genrule(\n> +    name = \"configure\",\n> +    outs = [\"platform.hpp\"],\n> +    cmd = \"cd external/libzmq && ./configure && cd ../..; cp external/libzmq/src/platform.hpp $@\",\n> +)\n>\n> Not sure what the best thing to do here is. Do you build on windows using\n> MSVC? The configure\n> <https://github.com/zeromq/libzmq/blob/3f80657c247bf14ba29d6de3cf8aeea8d0f1bc87/configure.ac>\n> script of libzmq seems pretty sophisticated and I don't know whether just\n> copying the file is sufficient. I'm afraid I don't have a windows machine\n> to test it.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8728#discussion_r112553031>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbe-wM-o-YyAilnjTiz0sv7bjdmwKks5rx8BPgaJpZM4MphR->\n> .\n>\n", "@tillahoffmann there is a problem in the build file. Could you fix (and maybe try `bazel build -c opt tensorflow/...`)?\r\n\r\n```\r\nERROR: /workspace/tensorflow/contrib/io/BUILD:36:1: no such package 'tensorflow/contrib/io/kernels': BUILD file not found on package path and referenced by '//tensorflow/contrib/io:io_ops_kernels'.\r\nERROR: Analysis of target '//tensorflow/contrib/io:io_ops_kernels' failed; build aborted.\r\n```", "@tillahoffmann I think given the extra zmq dependency, and the lack of clarity for exactly how the core system would use this, I'd prefer to keep the core code slim.\r\n\r\nBecause this is a nicely self-contained op, this is actually a great candidate to release / deploy as a custom, external operation via load_library.  If this op becomes popular, we can then assess the tradeoff of adding it to the core vs. the additional dependency / portability issues it creates.  It'll hopefully also be more clear why this op is even useful (and for example, why we wouldn't want an alternative to zeromq instead).  Without more context / use patterns, it's hard to know :).\r\n\r\nI'm going to close this for now, but feel free to ping me or open an issue to discuss this further.", "@vrv, thanks for the input. I think there is already a desire for network communication as part of tensorflow (cf. #7951, #4836). In particular, network io ops\r\n\r\n* enable decentralised data preprocessing in arbitrary languages and technologies.\r\n* provide an interface with external systems (in particular for reinforcement learning) such as game emulators, robots, etc.\r\n\r\nAs you mentioned, the dependency is a bit of an issue. Do you have other basic communication libraries in mind that would provide similar functionality?\r\n\r\nGiven @mrry's comments regarding proper resource management, I'm happy to leave this PR closed. Let me know whether there is appetite for proper ZMQ integration. Otherwise, we will manage network communication externally as you suggested."]}, {"number": 8727, "title": "NameError: name 'core' is not defined", "body": "when i run a command in the terminal\r\n```\r\nc:\\python\r\n>>> import tensorflow as tf \r\n```\r\nit show the error \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"E:\\Tools\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 51, in <module>\r\n    del core\r\nNameError: name 'core' is not defined\r\n```\r\ni have a searching but don't find an answer, i would happy that someone gives me the answer\r\n", "comments": ["@nb312, can you please provide information on\r\n1) what version of tensorflow you are using\r\n2) where did you get the installation package\r\n3) what your operating system (and version) is\r\n4) is your platform 32-bit or 64-bit\r\n5) whether you are using any python platforms such as anaconda. \r\n", "1.the latest version of tensorflow1.0\r\n2.use the command \"pip3 install --upgrade tensorflow\"\r\n3. windows 7\r\n4.64-bit\r\n5.have no other platforms", "The Python version is 3.52.\r\n when I close the Terminal,then open it again the code run ok with no error.I don't  know what's wrong with this.", "We have seen issues like this in windows before.\r\nUsually, retrying install, or even rebooting can help.\r\n\r\nLooks like the issue is resolved for you, so I will close.", "I have face the same issue. Even i had reinstalled it. \r\nI am using the latest version with python=5.3\r\nI have installed it using conda command.\r\n\r\nPlease help", "There is no such python version.\r\nPlease try installing via pip. or see the other recommendations above."]}, {"number": 8726, "title": "Retraining inception tutorial bazel error", "body": "Hi , I am following along this tutorial https://www.tensorflow.org/tutorials/image_retraining , I performed all the steps and they went fine even training but when I entered the command below for evaluating the model , I got an error after few minutes \r\n\r\n```\r\nbazel build tensorflow/examples/label_image:label_image && \\\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n--graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt \\\r\n--output_layer=final_result \\\r\n--image=$HOME/flower_photos/daisy/21652746_cc379e0eea_m.jpg\r\n```\r\n\r\n```\r\nERROR: /home/saurabh/saurabh/tensorflow/tensorflow/core/kernels/BUILD:1468:1: C++ compilation of rule '//tensorflow/core/kernels:resource_variable_ops' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 106 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nNo space left on device\r\nNo space left on device\r\nNo space left on device\r\nNo space left on device\r\nNo space left on device\r\nTarget //tensorflow/examples/label_image:label_image failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 201.779s, Critical Path: 150.97s\r\nsaurabh@saurabh-MS-7923:~/saurabh/tensorflow$ \r\n\r\n```", "comments": ["Have you tried passing the ```--local_resources``` option to bazel build? It seems that you've run out of space, and that option is useful if you're constrained by RAM for example. \r\n\r\nDescription given in the output of \"bazel help build\":\r\n```\r\n --local_resources (comma-separated available amount of RAM (in MB), CPU (in cores) and available I/O (1.0 being average workstation); default: see description)\r\n```\r\n\r\nExample usage (what I use):\r\n```\r\n--local_resources 4096,2.0,1.0\r\n```\r\n\r\nJust a guess to try before hearing back from the tensorflow folks. ", "I'll give that a shot today , thanks ", "I ran into a possibly similar memory problem with label_image.py.  It worked correctly on the first run, but every run after that crashed with memory error.  \r\n\r\nHere is the error output:\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ************************************************************************************xxxxxxxxxxxxxxxx\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 1.5KiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.\r\nE tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.\r\n\t [[Node: mixed_3/conv/batchnorm/beta = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [384] values: -1.1053501 -0.46053475 -0.92515361...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\r\n    return fn(*args)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\r\n    status, run_metadata)\r\n  File \"/Applications/anaconda/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: mixed_3/conv/batchnorm/beta = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [384] values: -1.1053501 -0.46053475 -0.92515361...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_files/label_image.py\", line 22, in <module>\r\n    predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': image_data})\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.\r\n\t [[Node: mixed_3/conv/batchnorm/beta = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [384] values: -1.1053501 -0.46053475 -0.92515361...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op 'mixed_3/conv/batchnorm/beta', defined at:\r\n  File \"tf_files/label_image.py\", line 16, in <module>\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 285, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInternalError (see above for traceback): Dst tensor is not initialized.\r\n\t [[Node: mixed_3/conv/batchnorm/beta = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [384] values: -1.1053501 -0.46053475 -0.92515361...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8725, "title": "semantic about softmax_cross_entropy_with_logits", "body": "In [https://www.tensorflow.org/get_started/mnist/beginners](url), it says:\r\n\r\n> we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b)\r\n\r\nIn [https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits](url), it says\r\n\r\n> logits: Unscaled log probabilities.\r\n\r\nHere is the question, take the following code for example:\r\n\r\n```\r\nz = tf.matmul(x, W) + b)\r\ncost = tf.nn.softmax_cross_entropy_with_logits(logits=z, labels=the_lables)\r\n```\r\n\r\n**z** is not log probability, why pass it to the parameter **logits**?\r\n", "comments": ["This belongs on StackOverflow (it has actually been asked many times on SO, see links below). __z__ is the log probability. That is a standard interpretation for the output of an affine transformation being fed to a softmax layer. For more, see:\r\n\r\n* [SO: Difference between softmax and sofmtax with logits](http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with)\r\n* [SO: tensorflow and unscaled log probabilities](http://stackoverflow.com/questions/40871797/tensorflow-softmax-cross-entropy-with-logits-asks-for-unscaled-log-probabilities)", "As @mckinziebrandon  indicates, this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Because **softmax** can operate on any array, I think the concept **logits** is not necessary, and it'll be more easy."]}]