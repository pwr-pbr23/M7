[{"number": 2383, "title": "machine learning beginner: how to get custom input data to train a conv net?", "body": "I have started learning tensorflow recently. I am trying to input my custom python code as training data. I have generated random exponential signals and want the network to learn from that. This is the code I am using for generating signal-\n\n`import matplotlib.pyplot as plt\nimport random\nimport numpy as np\n\nlorange= 1\nhirange= 10\namplitude= random.uniform(-10,10)\nt= 10\nrandom.seed()\ntau=random.uniform(lorange,hirange)\nx=np.arange(t)\n\nplt.xlabel('t=time\")\nplt.ylabel('x(t)')\nplt.plot(x, amplitude*np.exp(-x/tau))\nplt.show()`\n\nHow can I use this graph as input vector in tensorflow?\n", "comments": ["Examples for tf.learn [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/skflow) should make things easier for you. \n", "Questions like this should go on StackOverflow, not Github Issues.\n"]}, {"number": 2382, "title": "ValueError: GraphDef cannot be larger than 2GB", "body": "I'm trying to implement a deep autoencoder with tensorflow. The RBM pretraining works just fine, but when it comes to fine tuning, it raises the error: 'ValueError: GraphDef cannot be larger than 2GB'. My input is an array in the shape of [12396, 8192], and here is my layers: [8192 16384 8192 4096 2048 1024 512 256 512 1024 2048 4096 8192 16384 8192]. \nI know where the problem is, but I have no idea how to fix it. I have thought about using multiple graph, but what if my input is too big to even store one layer? Besides I don't know how many graph I should use. Set up a graph for every layer? That would be too slow and unnecessary.\nThank you!\n`  ...\n\n```\ndef __init__(self, input_num, layers, rbm_learning_rate, deepnn_learning_rate, rbm_num_epoch, \n             deepnn_num_epoch, momentum=0, batch_size=128, data_type='float32'):\n    self.input_num = input_num\n    self.layers = layers\n    self.n_layers = len(self.layers)\n    self.rbm_learning_rate = rbm_learning_rate\n    self.deepnn_learning_rate = deepnn_learning_rate\n    if momentum == 0:\n        self.momentum = []            \n        for _ in range(self.n_layers):            \n            self.momentum.append(1)\n    self.rbm_num_epoch = rbm_num_epoch\n    self.deepnn_num_epoch = deepnn_num_epoch\n    self.batch_size = batch_size\n    self.data_type = data_type\n    self.rbm_list = []\n    self.rbm_list.append(RBM(self.input_num, self.layers[0], self.rbm_num_epoch, \n                            self.momentum[0], self.rbm_learning_rate[0], self.batch_size, self.data_type))            \n    for i in range(self.n_layers-1):\n        self.rbm_list.append(RBM(self.layers[i], self.layers[i+1], self.rbm_num_epoch,\n                            self.momentum[i], self.rbm_learning_rate[i], self.batch_size, self.data_type))\n\ndef pretrain(self, train_set):\n    self.W_list = []\n    self.b_list = []\n    self.a_list = []   \n    if not cmp(train_set.dtype, self.data_type):\n        train_set.dtype = self.data_type        \n    next_train = train_set        \n    for i, rboltz in enumerate(self.rbm_list):\n        next_train = self._pretrain_and_get_para(rboltz, next_train)\n\ndef _pretrain_and_get_para(self, rboltz, next_train):\n    output, W_out, a_out, b_out = rboltz.fit(next_train)\n    self.W_list.append(W_out)\n    self.a_list.append(a_out)\n    self.b_list.append(b_out)\n    return output\n\ndef fine_tune(self, train_set):\n    m, _ = train_set.shape\n    self.num_per_epoch = m / self.batch_size         \n    train_batch = tf.placeholder(self.data_type, [None, self.input_num])        \n    logits = self._build_model(train_batch)\n    loss = self._loss(logits, train_batch)\n    train_op = self._training(loss)\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init)\n        for _ in range(self.deepnn_num_epoch):\n            for i in range(self.num_per_epoch):\n                _, cost = sess.run([train_op, loss], feed_dict = self._feed_build(train_batch, train_set, i))\n            print cost    \n\ndef _feed_build(self, train_batch, train_set, i):\n    batch = prepare_data.next_batch(train_set, i, self.batch_size)\n    feed_dict = {train_batch: batch}\n    return feed_dict    \n\ndef _build_model(self, train_batch):      \n    middle_layer = self._make_encoder(train_batch)\n    last_layer = self._make_decoder(middle_layer)\n    return last_layer\n\ndef _make_encoder(self, train_batch):\n    encoder = []\n    encoder.append(train_batch)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('encoder'+str(i)):\n            W = tf.Variable(self.W_list[i], name = 'weights')\n            b = tf.Variable(self.b_list[i], name = 'biases')\n            encoder.append(tf.sigmoid(b + tf.matmul(encoder[i], W)))\n    return encoder[self.n_layers]\n\n\ndef _make_decoder(self, middle_layer):\n    decoder = []\n    decoder.append(middle_layer)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('decoder'+str(i)):\n            W = tf.Variable(self.W_list[self.n_layers-i-1], name = 'weights')\n            a = tf.Variable(self.a_list[self.n_layers-i-1], name = 'biases')\n            decoder.append(tf.sigmoid(a + tf.matmul(decoder[i], W, transpose_b = True)))\n    return decoder[self.n_layers]        \n\ndef _loss(self, logits, labels):\n    loss = tf.nn.l2_loss(logits-labels)\n    return loss\n\ndef _training(self, loss):\n    optimizer = tf.train.GradientDescentOptimizer(self.deepnn_learning_rate)\n    train_op = optimizer.minimize(loss)\n    return train_op `\n```\n", "comments": ["My guess is that your variable creation statements in `_make_encoder()` and `_make_decoder()` are implicitly creating large constants in the graph - but I'm not sure without knowing the implementation of `rboltz.fit(...)`. What is the return type from that function?\n", "Thank you!\nThe return type of `rboltz.fit(...)` is `ndarray`.\nI use `W = tf.Variable(self.W_list[i], name = 'weights')` to create variables. I have try to use `tf.placeholder` instead of `tf.Variable`, but it turned out that 'there are no variables to optimize'. Now I have no idea how to deal with it.\nIt doesn't make sense. Tensorflow can't solve the problem when the graph is very big and deep? There must be a solution. @mrry \n", "@mrry: How does one set a variable without creating a `tf.constant`?\n", "@girving: You can initialize it from a `tf.placeholder()` and feed a value when you run the variable's initializer op. There's no sugar for it, but something like the following should work:\n\n``` python\ninit_val = np.array(...)  # Construct a large numpy array.\ninit_placeholder = tf.placeholder(tf.float32, shape=init_val.shape)\nv = tf.Variable(init_placeholder)\n# ...\nsess.run(v.initializer, feed_dict={init_placeholder: init_val})\n```\n", "@mrry: Alas, that's a bit horrendous, but it would certainly work.  If you have a good idea for sugar we should file a more specific bug, but I'm going to close this one for now. \n", "I'm encountering the same error, when I'm trying to set a very large parameter with value from numpy. E.g.:\r\n```\r\nloaded_value = .... (8096x8096 numpy ndarray)\r\n w = tf.Variable(loaded_value, name=\"w\")\r\n...\r\nsession = tf.Session(config=config)\r\ninit = tf.global_variables_initializer()\r\nsession.run(init) # Crashes\r\n````\r\nAlso note that that is 256MB array, no idea what the 2GB is coming from.", "@wangbm, I was thinking - is there any chance you could reduce your input dimensions it? You may reduce your overall computational cost too.\r\n\r\nMaybe you can split your data into two or more pieces and train models. (I think it was during early periods when researchers doing Image classification & tagging) I heard this method was used when researcher did not sufficient computational memory. Good Luck !", "I had a similar problem. In my case I was using ipython to run my code. Problem was resolved when I restarted ipython to run code again.", "I had the similar problem. When I wanted to create a new feature --feature cross, it occured the error:ValueError: GraphDef cannot be larger than 2GB.  I don't know how to solve the problem. I have tried reducing the dimension by adjusting the hash_bucket_size, but it didn't make sense.", "I have this problem when increasing the number of GPUs I train on from 2 to 4. Not sure why the checkpoint saved graph has to increased dramatically in size for each GPU. ", "@Arjuna197, Are you using MirrorredStrategy? Cause I encounter similar error, 2gb limit when trying to use 4 gpu instead of 1. #25057\r\n\r\n", "> @Arjuna197, Are you using MirrorredStrategy? Cause I encounter similar error, 2gb limit when trying to use 4 gpu instead of 1. #25057\r\n\r\nYes, mirrored strategy. "]}, {"number": 2381, "title": "understanding the current state of dynamic_rnn vs buckets vs scan for seq2seq", "body": "Bear with me as I am not an expert in the practical or theoretical details. \n\nPerhaps because tensorflow has developed so rapidly, I find it a little difficult to find the recommended procedure for RNNs with varying length, at least from my lay end-user perspective. It is unclear to me on May 15, 2016 what tool within tensorflow is best suited to this task.\n\nIt seems that originally RNNs of varying sequence length were handled by \"bucketing\". This means building a \"separate\" model for each of several bucket lengths so that inputs could be padded to the same length without wasting too much gpu memory. There is not presently a \"simple\" example with bucketing, but to those unfamiliar, I would look to the [seq2seq](https://github.com/tensorflow/tensorflow/blob/3c8780451007c27b69f10925d43d1f3501d94106/tensorflow/models/rnn/translate/seq2seq_model.py) model for clarity. Also the [model_with_buckets function](https://github.com/tensorflow/tensorflow/blob/82ff4cd8b0d541ede107d34d8eecc769c91dda11/tensorflow/python/ops/seq2seq.py)\n\nSometime between March and May a number of (maybe?) different tools were added that could possibly be used to avoid this bucketing trick without wasting memory.\n1. [rnn.py/dynamic_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py)\n2. [functional_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L256), though within this code it's not clear without investigation if map_fn or tf.scan (!) would be better suited.\n3. [control_flow_ops](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/control_flow_ops.py)\n\nIn spite of this, the seq2seq model still uses bucketing, and I struggle to find any examples using these newer methods.\n\nCreating detailed documentation is ultimately part of the endgoal, but _for now_ I thought it would be useful to at least make a mention of what the recommended approach is.\n\nThanks.\n\n[relevant issue 208 ](https://github.com/tensorflow/tensorflow/issues/208)\n[relevant reddit with broken link to example](https://www.reddit.com/r/MachineLearning/comments/4dtbks/tensorflow_now_has_an_unofficial_scan_function/)\n", "comments": ["In my understate, `dynamic_rnn` is not used to solve the problem of variable length, but used to run loops within a tensor. So is control flow ops.\n\nI think the good way to deal with variable length, is to divide one sample into multiple ones, and keep the RNN state between them. So the divided multiple samples are treated as one sample. On the other hand, if one sample is shorter, then padding it with zero or something like that. The implement of Keras gives a good example. This should be a better way than bucketing. Though in TensorFlow, it seems have a lot work to do. \n", "My mistake, it seems that `dynamic_rnn` implemented the way I said. But it doesn't matter with control flow ops, you can do it with python loop.\n", "@wb14123: A Python loop doesn't support automatic gradient computation, which is the typical reason to use the fancier control flow ops.  I'm going to close this for now, since questions about how to use TensorFlow are better suited to StackOverflow (use the `tensorflow` tag). \n", "@girving I'm somewhat confused.\r\n\r\nIn [static_rnn](https://github.com/tensorflow/tensorflow/blob/1752d9c8fac5f6cf85a41e77d92e2743adbfc446/tensorflow/python/ops/rnn.py#L1312), it does use Python loop to compute rnn outputs and final state.\r\n```\r\n    for time, input_ in enumerate(inputs):\r\n      if time > 0:\r\n        varscope.reuse_variables()\r\n      # pylint: disable=cell-var-from-loop\r\n      call_cell = lambda: cell(input_, state)\r\n      # pylint: enable=cell-var-from-loop\r\n      if sequence_length is not None:\r\n        (output, state) = _rnn_step(\r\n            time=time,\r\n            sequence_length=sequence_length,\r\n            min_sequence_length=min_sequence_length,\r\n            max_sequence_length=max_sequence_length,\r\n            zero_output=zero_output,\r\n            state=state,\r\n            call_cell=call_cell,\r\n            state_size=cell.state_size)\r\n      else:\r\n        (output, state) = call_cell()\r\n\r\n      outputs.append(output)\r\n```\r\n\r\nYou said \"a Python loop doesn't support automatic gradient computation\", then how does the static_rnn work?\r\n"]}, {"number": 2380, "title": "[skflow] Added support for different parameters per layer in dnn", "body": "Added support for per layer configuration of activation function and\ndropout probabilities. Example usage as per following code:\n\n``` python\ndef model(X, y):\n    l3 = skflow.ops.dnn(\n        X,\n        hidden_units=[10, 20],\n        activation=[tf.nn.tanh, tf.nn.relu6]\n    )\n    return skflow.models.linear_regression(l3, y)\n\nregressor = skflow.TensorFlowEstimator(\n    model_fn=model,\n    n_classes=0,\n    steps=50000,\n    learning_rate=0.1,\n    batch_size=128,\n    verbose=1\n)\n\nregressor.fit(X_t, Y_t)\n```\n\nTested with single activation function and with multiple ones that\nnothing crashes. Seem to be working correctly\n", "comments": ["Can one of the admins verify this patch?\n", "ping for @ilblackdragon  (i hope you are getting these mentions)\n", "Ok, we plan to move / change a bit this function, so it would be good to have this PR in for informing use cases.\n", "@tensorflow-jenkins test this please!\n", "Ping for @ilblackdragon. Is this still in progress?\n", "@rmlarsen when @nmiculinic addressed one naming comment it can go in.\n", "Closing due to inactivity.  Ping if ready to re-open / rebase to master.\n"]}, {"number": 2379, "title": "[skflow] Added verbosity support to Validation monitor", "body": "Aim of this diff is adding verbosity switch to Validation monitor class,\nwhcih previosly had none (It was automatically set to level 1).\n\nI've tested locally on my machine, and works as expected:\n\n``` python\nskflow.monitors.ValidationMonitor\nregressor = skflow.TensorFlowDNNRegressor(hidden_units=[10, 10],\n    steps=5000, learning_rate=0.1, batch_size=1,\n                                          verbose=0)\n\nregressor.fit(X_t, Y_t, monitor=skflow.monitors.ValidationMonitor(X_cv, Y_cv, verbose=0))\n\n```\n\nPrevious code is a bit edited example ones.\n", "comments": ["Can one of the admins verify this patch?\n", "Hi @nmiculinic Thanks for contribution, but we actually refactored both code to run evaluation (via `evaluate` method which now is verbose by default) and monitoring (change a API as well).\n\nI'll close this one for today, please try new APIs and see if there is missing functionality there.\nThanks!\n"]}, {"number": 2378, "title": "Enable password support for Jupyter in a Docker image", "body": "This small update enables Jupyter to support passwords passed as an environment variable when instantianting a docker container. \n\nIn the same way, `jupyter_notebook_config.py` could also be modified to support HTTPS by default. If there is an interest, I could also include these updates in the same PR.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "Merged. Thanks.\n", "Thanks! Do you think we also should add support to HTTPS? It would be passed as `-e USE_HTTPS=1`, otherwise it would use simple HTTP by default.\n"]}, {"number": 2377, "title": "Will there be custom GPU kernels for NHWC Batchnorm in the future?", "body": "NCHW Batchnorm #1759 is one merge away but as it currently stands the NHWC Batchnorm kernel is the single biggest performance killer in very deep networks. My testing shows that Resnet-164's forward pass taking more than twice as much time as that of Resnet-110, simply because the former has an additional BN of increased channel dimension (x4) in each block, despite the two architecture having nearly the same FLOPs.\n\nEdit: Actually meant Resnet-164, not Resnet-152\n", "comments": ["NHWC will be supported by converting to NCHW internally back and forth. Our experience is that in most cases that is okay. If you absolutely don't want that overhead, you can switch your entirely model to NCHW through data_format, and that will be fine. If for some ops, NCHW support isn't available, either a conversion can be added, or add new support. \n", "@zheng-xq: Should I leave this assigned to you, or is it already done? \n", "We are still waiting for the BatchNorm kernel to be merged. Support for other formats will proceed afterwards. \n", "Chatted with @zheng-xq offline: someone on the team is looking at this.\n", "What would be the timeline for the BatchNorm kernel to be merged? \n", "Not sure if it's obsolete. Closing since it might have been done. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 2376, "title": "Beginner question: Error when running the conv net example", "body": "I tried to learn Tensorflow and ran the .py file I created by copying the example code on the Tensorflow website. it runs well initially and printing it's training epochs. But finally, it gave some error showing as below:\n\n(tensorflow)xu@xu-ThinkCentre-M72e:~ $ python BuildConvNet.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 750 Ti\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.2545\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.67GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 750 Ti, pci bus id: 0000:01:00.0)\n0.9092\nstep 0, training accuracy 0.04\nstep 100, training accuracy 0.78\nstep 200, training accuracy 0.9\nstep 300, training accuracy 0.86\nstep 400, training accuracy 0.96\nstep 500, training accuracy 0.94\nstep 600, training accuracy 0.9\nstep 700, training accuracy 0.98\nstep 800, training accuracy 0.96\nstep 900, training accuracy 0.92\nstep 1000, training accuracy 1\nstep 1100, training accuracy 1\nstep 1200, training accuracy 1\nstep 1300, training accuracy 0.94\nstep 1400, training accuracy 0.98\nstep 1500, training accuracy 0.98\nstep 1600, training accuracy 1\nstep 1700, training accuracy 1\nstep 1800, training accuracy 0.98\nstep 1900, training accuracy 1\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (256):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (512):     Total Chunks: 1, Chunks in use: 0 768B allocated for chunks. 6.4KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (1024):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (2048):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (4096):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (8192):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (32768):     Total Chunks: 1, Chunks in use: 0 32.0KiB allocated for chunks. 3.1KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (65536):     Total Chunks: 1, Chunks in use: 0 121.8KiB allocated for chunks. 4.79MiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (131072):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (262144):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (524288):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (1048576):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (2097152):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (4194304):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (8388608):     Total Chunks: 1, Chunks in use: 0 12.21MiB allocated for chunks. 390.6KiB client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (16777216):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (33554432):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (67108864):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:635] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:652] Bin for 29.91MiB was 16.00MiB, Chunk State:\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a40000 of size 31488\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a47b00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a47c00 of size 31488\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a4f700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a4f800 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a4f900 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a4fa00 of size 31488\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57500 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57600 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57800 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57900 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a57a00 of size 4096\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a58a00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a58b00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a58c00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a58d00 of size 3328\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a59a00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a59b00 of size 204800\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a8bb00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x701a8bc00 of size 12845056\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026cbc00 of size 4096\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026ccc00 of size 40960\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d6c00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d6d00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d6e00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d6f00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7200 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7300 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7400 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7500 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7600 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7b00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7026d7c00 of size 80128\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x702709c00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x702709d00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x702709e00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x702709f00 of size 40960\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703349d00 of size 40960\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703353d00 of size 3328\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703354a00 of size 4096\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703355a00 of size 4096\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703356a00 of size 4096\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x70335fa00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x70335fb00 of size 40960\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703369b00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703369c00 of size 12845056\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fa9c00 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fa9d00 of size 3328\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703faaa00 of size 3328\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fab700 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fab800 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fab900 of size 204800\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x703fdd900 of size 204800\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x70400f900 of size 204800\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x704041900 of size 12845056\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x704c81900 of size 12845056\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7058c1900 of size 31360000\nI tensorflow/core/common_runtime/bfc_allocator.cc:670] Chunk at 0x7076a9d00 of size 1485857536\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x7026d7800 of size 768\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x7026eb500 of size 124672\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x702713f00 of size 12803584\nI tensorflow/core/common_runtime/bfc_allocator.cc:679] Free at 0x703357a00 of size 32768\nI tensorflow/core/common_runtime/bfc_allocator.cc:685]      Summary of in-use Chunks by size:\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 35 Chunks of size 256 totalling 8.8KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 4 Chunks of size 3328 totalling 13.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 5 Chunks of size 4096 totalling 20.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 3 Chunks of size 31488 totalling 92.2KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 4 Chunks of size 40960 totalling 160.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 80128 totalling 78.2KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 4 Chunks of size 204800 totalling 800.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 4 Chunks of size 12845056 totalling 49.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 31360000 totalling 29.91MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:688] 1 Chunks of size 1485857536 totalling 1.38GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] Sum Total of in-use chunks: 1.46GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:694] Stats:\nLimit:                  1582759936\nInUse:                  1569798144\nMaxInUse:               1570198272\nNumAllocs:                  275908\nMaxAllocSize:           1485857536\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] **********************************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 29.91MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:900] Resource exhausted: OOM when allocating tensor with shape[10000,1,28,28]\nTraceback (most recent call last):\n  File \"BuildConvNet.py\", line 78, in <module>\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 502, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3334, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[10000,1,28,28]\n     [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Reshape, Variable_2/read)]]\n     [[Node: Mean_3/_1035 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_911_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Conv2D', defined at:\n  File \"BuildConvNet.py\", line 41, in <module>\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  File \"BuildConvNet.py\", line 30, in conv2d\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 295, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/xu/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n\n(tensorflow)xu@xu-ThinkCentre-M72e:~ $ \n", "comments": ["It looks like you're invoking this from a script called `BuildConvNet.py`. Can you share the source of that file?\n", "``` python\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', one_hot=True)\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\nsess.run(tf.initialize_all_variables())\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\nfor i in range(1000):\n  batch = mnist.train.next_batch(50)\n  train_step.run(feed_dict={x: batch[0], y_: batch[1]})\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1,28,28,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.initialize_all_variables())\nfor i in range(2000):\n  batch = mnist.train.next_batch(50)\n  if i%100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch[0], y_: batch[1], keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nprint(\"test accuracy %g\"%accuracy.eval(feed_dict={\n    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n```\n", "OK, it looks like your GPU is running out of memory when you try to run the eval. You seem to have a GTX 750 with 2GiB of RAM. It's possible that copying all of the MNIST test images and labels to the GPU exhausts its memory. I'd suggest batching the test data (e.g. taking 50 images at a time) and aggregating the accuracy in your Python code.\n", "Isn't this means a batch of 50 images? Sorry, I'm a beginner. \n\nbatch = mnist.train.next_batch(50)\n", "Ok, I seems understand what you mean. The last print is the problem. I'll try to revise x and y_ to subset. Thank you very much.\n"]}, {"number": 2375, "title": "Matplotlib incompatibility when using virtualenv", "body": "RuntimeError when importing matplotlib.pyplot in a jupyter notebook from a virtual environment:\n\n`RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are Working with Matplotlib in a virtual enviroment see 'Working with Matplotlib in Virtual environments' in the Matplotlib FAQ`\n\nI have solved the problem using one of the solutions mentioned at: [Matplotlib VirtualEnv FAQ](http://matplotlib.org/faq/virtualenv_faq.html) and that is ok when using python from the command line or a script file, but not with the kernel used in jupyter.\n\nSo, the question is: how to create a jupyter kernel that uses a specific bash which properly setups python to use matplotlib within a virtual environment.\n\nAnother solution is:\n\n```\nimport matplotlib  \nmatplotlib.use('TkAgg')   \nimport matplotlib.pyplot as plt  \n```\n\nBut maybe that is not the best thing to do. So, any suggestions?.\nOperating System: OS X El capitan\nTensorflow version: 0.8.0\n", "comments": ["Just curious why you would post this question here?! This is clearly not an issue in tensorflow.\n", "I'm closing this as it doesn't seem relevant to TensorFlow.\n", "for those who find this with a google search - this is the solution I found that worked at getting matplotlib errors to go away even with jupyter notebooks: http://blog.rousek.name/2015/11/29/adventure-with-matplotlib-virtualenv-and-macosx/\n\nI hope this doesn't reopen the ticket.\n", "For Mac OS X users who still get this issue, the simplest work around I found is to add this line:\r\n`backend: TkAgg`\r\n\r\nin ~/.matplotlib/matplotlibrc\r\n\r\nhttps://stackoverflow.com/questions/34977388/matplotlib-runtimeerror-python-is-not-installed-as-a-framework"]}, {"number": 2374, "title": "Match conv2 weights stddev with cuda-convnet's layer def.", "body": "conv2 initW is 0.01 in cuda-convnet.\n\nhttps://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg\n\nI'm trying [exercise](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html#convolutional-neural-networks).\n\nI change layer type from full connected to locally connected(just convolutional..).\nVanishing-Gradient problem is occurred at conv2.\n\nIs this intended?\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@girving @vrv\n", "I don't follow your message.  Are you saying that your new value helps after you change the network to a different topology?  What are the evaluation accuracies before and after your change?\n", "Hi @girving,\n\nconv2's parameter stddev is different from original [cuda-convnet](https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg).\nI think this may be a typo, because `1e-4` is same value with conv1 layer.\n\nCurrently parameter(1e-4) cause of vanishing-gradient at trying [EXERCISE](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html).\n\n---\n\nThe original  [cuda-convnet](https://code.google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-conv-local-11pct.cfg)'s conv2 initW is `0.01`.\n\n```\n[conv2]\ntype=conv\ninputs=rnorm1\nfilters=64\npadding=2\nstride=1\nfilterSize=5\nchannels=64\nneuron=relu\ninitW=0.01\npartialSum=8\nsharedBiases=1\n```\n\nAnd cifar10.py's conv2 stddev is `1e-4`.\n\n```\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-4, wd=0.0)\n```\n\nI think this may be a typo, because `1e-4` is same value with conv1 layer.\n\n[TensorFlow document](https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html) is questioning\n\n```\nEXERCISE: The model architecture in inference() differs slightly from the CIFAR-10 model specified in \ncuda-convnet. In particular, the top layers of Alex's original model are locally connected and not fully \nconnected. Try editing the architecture to exactly reproduce the locally connected architecture in the top \nlayer.\n```\n\nI'm trying this exercise(unfortunately I cannot find model answer on web).\nI tried replace local3 and local4 to convolutional layer. And if conv2's `stddev` was `1e-4`, vanishing-gradient occurred at conv2 layer.\n", "Here's my current `inference` code.\n\n```\ndef inference(images):\n  \"\"\"Build the CIFAR-10 model.\n\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n\n  Returns:\n    Logits.\n  \"\"\"\n  # We instantiate all variables using tf.get_variable() instead of\n  # tf.Variable() in order to share variables across multiple GPU training runs.\n  # If we only ran this model on a single GPU, we could simplify this function\n  # by replacing all instances of tf.get_variable() with tf.Variable().\n  #\n  # conv1\n  with tf.variable_scope('conv1') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],\n                                         stddev=1e-4, wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv1)\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n                         padding='SAME', name='pool1')\n\n  # norm1\n  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm1')\n\n  # conv2\n  with tf.variable_scope('conv2') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64],\n                                         stddev=1e-2, wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(conv2)\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n                    name='norm2')\n  # pool2\n  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n\n  # local3\n  with tf.variable_scope('local3') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[64]))\n    bias = tf.nn.bias_add(conv, biases)\n    local3 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local3)\n\n  # local4\n  with tf.variable_scope('local4') as scope:\n    kernel = _variable_with_weight_decay('weights', shape=[3, 3, 64, 32],\n                                         stddev=0.04, wd=0.04)\n    conv = tf.nn.conv2d(local3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.1, shape=[32]))\n    bias = tf.nn.bias_add(conv, biases)\n    local4 = tf.nn.relu(bias, name=scope.name)\n    _activation_summary(local4)\n\n  # softmax, i.e. softmax(WX + b)\n  with tf.variable_scope('softmax_linear') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(local4, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n\n    weights = _variable_with_weight_decay('weights', [dim, NUM_CLASSES],\n                                          stddev=1 / float(NUM_CLASSES), wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.nn.softmax(tf.nn.bias_add(tf.matmul(reshape, weights), biases), name=scope.name)\n\n    _activation_summary(softmax_linear)\n\n  return softmax_linear\n```\n\nIf weight stddev was `1e-4` at conv2, evaluation accuracies is just 10%.\n\n```\n2016-05-17 08:29:43.386243: precision @ 1 = 0.100\n```\n\nAnd tensorboard shows vanishing-gradient at conv2 layer.\n\n<img width=\"751\" alt=\"screen-shot-2016-05-16-at-03 28 57\" src=\"https://cloud.githubusercontent.com/assets/932136/15306759/41f20334-1b82-11e6-937b-11af16397d6d.png\">\n", "@vincentvanhoucke: Can I get your eye here?  His change seems reasonable to me (I would think 1e-2 is a better std for both conv layers). \n", "@keiji your solution doesn't exactly replicate Alex's model. You use convolutional layers for local3 and local4, whereas Alex used locally-connected layers that do not share their parameters across patches. My guess is that's possibly one reason why you see such a strong effect of initialization.\nCome to think of it, I don't know how one would do that in TensorFlow. @girving do you know who wrote the tutorial and whether they had a specific trick in mind for locally connected, non-convolutional layers?\n", "@vincentvanhoucke: To make sure I understand: you mean a convolutional layer but using separate filters for each output point?  You could do it with batch matmul and a bunch of reshaping / tiling logic, but it would be quite slow.  I think we'd need a custom op to it fast.  And actually even the reshaping / tiling logic may be out of reach in a performant way.\n", "@girving yes, that's what the original Cifar10 model uses. We should remove that exercise if we don't know ourselves how to do it well at this point, unless the author of the tutorial has something up their sleeve I don't know.\n", "@shlens: Looks like you wrote parts of the deep_cnn tutorial.  Do you know a trick for untied convolutions?\n", "There is no trick. We envisioned the exercise to be difficult because a user would need to invoke several tiling, reshape and matmul operations. Most of the challenge would be in the construction of the tiling operation across the local3 and local4.\n", "As far as I understand, TensorFlow doesn't have convenience operations for making locally-connected layer. We have to implement locally connected logic manually.\n\nI'll retry exercise with this approach.\n", "I'm going to close this PR. Please reopen if you feel strongly that the defaults should match cudaconvnet. As it stands, I'm not convinced it's an issue.\n"]}, {"number": 2373, "title": "Embedding lookup table doesn't mask padding value", "body": "Hi there,\n\nI'm using an _embedding_lookup_ operation in order to generate dense vector representations for each token in my document which are feed to a convolutional neural network (the network architecture is similar to the one in a [WildML article](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)). \n\nEverything works correctly but when I pad my document inserting a padding value in it, the embedding lookup generates a vector for this token too. I think that this approach could alterate the results in the classification task. What I want to achieve is something similar to what Torch [LookupTableMaskZero](https://github.com/Element-Research/rnn#rnn.LookupTableMaskZerol) does. \n\n1) Is correct what I want to do?\n2) Is already implemented something like this? \n3) If not, how can I mask the padding value in order to prevent the generation of the corresponding vector for it? \n\nThank you in advance, \nAlessandro\n", "comments": ["I guess I met the same problem, @aleSuglia . \n\nIn the released \"tranlsate\" codes of a seq2seq model, buckets are used for defining different lengths of encoders and decoders. Every time we get a batch of data, we pad the original input with \"PAD\" to satisfy the required length of a bucket, e.g., [1,2,3] --> [1,2,3,PAD,PAD] for the bucket with length=5. However, we take the PAD as a normal token in embedding. That's strange.\n\nIn training a seq2seq model, we usually reverse the input sequence to be [PAD, PAD, 3, 2, 1]. In this case, the embedding of PAD can change the final decoding result. In my experiments, when testing new data without knowing its corresponding bucket, the decoding result would be different with different buckets.\n\nSo I guess that's a bug? Thanks much!\n\nZiyu\n", "Exactly @LittleYUYU! \n\nI don't know why nobody has noticed it before but for me is a missing feature and not a bug. I hope that somebody, one day, will notice this issue and fix it. \n\nThank you to all in advance.\n\nAlessandro Suglia\n", "@aleSuglia Right! Currently I use a mask which is a matrix with the same dimension as the embedding matrix but with the padding value set to be zero to solve this problem. I just feel confused: why the released sample code doesn't consider this issue? Thanks!\r\n\r\nUPDATE (04/13/17):\r\nI posted this suggestion nearly one year ago and I'd like to update my solution to the padding problem for now. **If the original answer looks confusing, then just skip it.**\r\n\r\nIf you are using RNN, you can certainly pass a \"sequence_length\" parameter, such that the reverse of [1, 2, 3, PAD, PAD] will be [3, 2, 1, PAD, PAD] and the valid RNN steps will be [1, 2, 3], so now the PAD won't affect the RNN anymore. \r\n\r\nIf you need to do any operation on the top of the RNN output, you can design a \"mask\". For example, if your sequence input (in a batch) looks like:\r\n[[1,2,3,PAD,PAD]\r\n[4,5,PAD,PAD,PAD]\r\n[6,7,8,9,PAD]]\r\nthen the mask can be \r\n[[1,1,1,0,0]\r\n[1,1,0,0,0]\r\n[1,1,1,1,0]]\r\nwhere all PAD will be marked as zero otherwise one. Then simply multiply the RNN output by the mask to eliminate the effect of PAD before your further operations (e.g., you may need the sum of each row for some purposes).\r\n\r\nIf you are using CNN, you may design a similar mask before pooling.\r\n\r\n", "I think the sample code doesn't consider the issue because it doesn't affect neural network performance.  If you're seeing significant differences between zero padding an new-token padding, I would use a mask as @LittleYUYU suggests.  `embedding_lookup` doesn't handle this by itself since we try to keep the semantics of each op simple and focused.\n", "This can actually cause a fairly substantial fluctuation in performance in some networks.  Suppose that instead of a convnet, we feed the embeddings to a deep averaging network.  Then the varying number of nonzero pad vectors (according to which training batch the example is assigned in SGD) will very much affect the value of the average embedding.  I've seen this cause a variation of up to 3% accuracy on text classification tasks.  One possible remedy is to set the value of the embedding for the pad index to zero explicitly between each backprop step during training.  This is computationally kind of wasteful (and also requires explicitly feeding the number of nonzero embeddings in each sample to the network), but it does the trick.  I would like to see a feature like Torch's LookupTableMaskZero as well.   \n", "I would also be interested in a feature like this. Both Theano and Keras support it. \n", "Hello,has this problem been solved yet? @girving ", "Hi, I come across this problem and get into this issue by searching. I'm so glad that this question is proposed properly, which indicating a SEEK-THE-PRECISE attitude!\r\n\r\nI'm not very familiar with TF, so @LittleYUYU 's methods description still un-clearly until I come across the code at [Embedding lookup table doesn't mask padding value](http://stackoverflow.com/questions/37255038/embedding-lookup-table-doesnt-mask-padding-value) (the same question in stackoverflow), I rewrite it to support arbitrary PADDING ID value instead only 0.\r\n\r\n```Python\r\n# omit some CONSTANT\r\nraw_mask_array = [[1.]] * PADDING_ID + [[0.]] + [[1.]] * (WORDS_NUM - PADDING_ID - 1)\r\nwith tf.variable_scope(\"Embedding\"):\r\n    lookup_table = tf.get_variable(\"lookup_table\", shape=[WORDS_NUM, EMBEDDING_DIM],\r\n                                   initializer=tf.random_uniform_initializer(\r\n                                               minval=-R, maxval=R),\r\n                                   dtype=DTYPE,\r\n                                   trainable=True)\r\n    mask_padding_lookup_table = tf.get_variable(\"mask_padding_lookup_table\",\r\n                                                initializer=raw_mask_array,\r\n                                                dtype=DTYPE,\r\n                                                trainable=False)\r\nembedding_input = tf.nn.embedding_lookup(lookup_table, id_input)\r\nmask_padding_input = tf.nn.embedding_lookup(mask_padding_lookup_table, id_input)\r\nembedding_input = tf.multiply(embedding_input, mask_padding_input) # broadcast\r\n``` \r\n\r\n-----\r\n\r\nas @youngia said, **following method can't work**. keep it only for record the following response. I'm so sorry to make a wrong method.\r\n\r\n<del>what's more, I make a more simple solution(which has been write down in that stackoverflow page):</del>\r\n\r\n```Python\r\nmask_padding_zero_op = tf.scatter_update(lookup_table, \r\n                                         PADDING_ID, \r\n                                         tf.zeros([EMBEDDING_DIM,], dtype=DTYPE))\r\n# just for explicity\r\nlookup_table = mask_padding_zero_op\r\n```\r\n\r\n<del>just to reset PADDING ID's embedding value to zeros and replace it. see the stackoverflow page for details.</del>\r\n\r\n<del>this code may be naive or not property, writing here for some  people like me searching this. Any errors please point out.</del>", "@memeda it returned \"No gradient defined for operation 'embedding/ScatterUpdate\" when use tf.scatter_update, seems it can't work with embedding variable as trainable", "@youngia , thanks you for response. I really have not use it in practice. I'll delete it \ud83d\ude2d ", "Is there anyway to mask the padding 0s then, I use to do it before embedding but it made my autoencoder lazy (obviously). I've heard that embedding twice with the second time with idx set to some 0 mask worked. Anybody heard of that?", "@memeda @youngia , I follow the code but with slightly change and succeed:\r\n```\r\nmask_padding_zero_op = tf.scatter_update(lookup_table, \r\n                                         PADDING_ID, \r\n                                         tf.zeros([EMBEDDING_DIM,], dtype=DTYPE))\r\nwith tf.control_dependencies([mask_padding_zero_op]):\r\n    # do embedding lookup...\r\n```", "@ay27 i tried but still get the error under tf1.12.0\r\n", "Here is how I deal with the padding value's embedding:\r\nJust like the solution provided by @ay27 , but remember to reset embedding weights after each step's back propagation.\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nDTYPE = tf.float32\r\npad_value = 0\r\n\r\ndef init_trainable_weights(shape):\r\n    return tf.Variable(tf.random.truncated_normal(shape, stddev=1.0, dtype=DTYPE),\r\n                       dtype=DTYPE,\r\n                       trainable=True)\r\n\r\ndef reset_pad_value_embedding(weights):\r\n    tf.compat.v1.scatter_update(weights,\r\n                                pad_value,\r\n                                tf.zeros([weights.shape[1],], dtype=DTYPE))\r\n\r\ndef embedding_lookup_with_padding(embedding_weights, values, op):\r\n    with tf.control_dependencies([op]):\r\n        embeds = tf.nn.embedding_lookup(embedding_weights, values)\r\n    \r\n    return embeds\r\n\r\n# embedding weights\r\nn = 5\r\nd = 4\r\nweights = init_trainable_weights([n, d])\r\n# fixed length list features (batch of 2 samples)\r\n# (n_batches, batch_size, fea_len)\r\nfeas = tf.constant([[[1, 3, pad_value], [2, pad_value, pad_value]]])\r\nlabels = tf.constant([[1, 0]])\r\nsamples = zip(feas, labels)\r\n\r\n\r\n\r\n# dummy model\r\nclass Model:\r\n    def __init__(self):\r\n        self.embed_weights = init_trainable_weights([n, d])\r\n        print(\"***** Initial embed_weights: *****\")\r\n        print(self.embed_weights)\r\n        print()\r\n        self.fc_weights = init_trainable_weights([12, 1])\r\n        reset_pad_value_embedding(self.embed_weights)\r\n        print(\"***** embed_weights after updating padding value's vec *****\")\r\n        print(self.embed_weights)\r\n        print()\r\n        self.opt = tf.optimizers.Adam(learning_rate=0.001)\r\n        self.trainable_weights = [self.embed_weights, self.fc_weights]\r\n        self.epochs = 1\r\n        \r\n    def forward(self, x):\r\n        fea_embeds = tf.nn.embedding_lookup(self.embed_weights, x)\r\n        print(\"***** fea embeds: *****\")\r\n        print(fea_embeds)\r\n        print()\r\n        # concat feature value's embeddings, then fc\r\n        return tf.squeeze(tf.matmul(tf.reshape(fea_embeds, [2, 12]), self.fc_weights), axis=1)\r\n    \r\n    def loss(self, pred, y):\r\n        return (y - pred) ** 2\r\n    \r\n    def train(self, samples):\r\n        for epoch in range(self.epochs):\r\n            for x, y in samples:\r\n                with tf.GradientTape() as tape:\r\n                    pred = self.forward(x)\r\n                    loss = self.loss(pred, tf.cast(y, tf.float32))\r\n                \r\n                print(\"***** padding embedding vec before back-propagation: *****\")\r\n                print(self.embed_weights[0])\r\n                print()\r\n                grads = tape.gradient(loss, self.trainable_weights)\r\n                self.opt.apply_gradients(zip(grads, self.trainable_weights))\r\n                print(\"***** padding embedding vec after back-propagation: *****\")\r\n                print(self.embed_weights[0])\r\n                # actually, they are the same\r\n                print(tf.nn.embedding_lookup(self.embed_weights, [0]))\r\n                print()\r\n                \r\n                # reset pad value's embedding\r\n                reset_pad_value_embedding(self.embed_weights)\r\n                print(\"**** padding embedding vec after back-propagation and after reset: *****\")\r\n                print(self.embed_weights[0])\r\n                print(tf.nn.embedding_lookup(self.embed_weights, [0]))\r\n                \r\nsamples = zip(feas, labels)\r\nmodel = Model()\r\nmodel.train(samples)\r\n```\r\n"]}, {"number": 2372, "title": "Typo fix for comment in worker.proto", "body": "Typo fix, nothing more.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2371, "title": "C++ API header files missing from pip install", "body": "I'm building a custom CPU/GPU operator with the pip binary [install](https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl) linked from\n\nhttps://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#pip-installation\n\nCertain header files seem to be missing from the above binary. When following the [Adding an Op](https://www.tensorflow.org/versions/r0.8/how_tos/adding_an_op/index.html) tutorial the following is missing\n- tensorflow/core/framework/register_types.h\n\nbut also header files for things like the GPU Stream Executor, Temp and Scratch Allocators which appear to be used by the inbuilt tensorflow ops\n\nIs the omission of these header files intentional and if so can we trying using the headers from a source distribution? [Adding an Op](https://www.tensorflow.org/versions/r0.8/how_tos/adding_an_op/index.html) suggests that either a source or binary distribution is suitable for building a custom GPU operator.\n", "comments": []}, {"number": 2370, "title": "installation problem in ubuntu 64 gpu", "body": "Tried installing it on ubuntu but getting this error even using Virtualenv:\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"tensorflow.py\", line 11, in <module>\n    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nAttributeError: 'module' object has no attribute 'Variable'\n```\n### Environment info\n\nOperating System: Ubuntu\nDistributor ID: Ubuntu\nDescription:    Ubuntu 15.04\nx86_64 x86_64 x86_64 GNU/Linux\npython 2.7.9\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root 189170 Jan 12 14:47 /usr/local/cuda-7.5/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan 12 14:47 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan 12 14:47 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan 12 14:47 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan 12 14:47 /usr/local/cuda-7.5/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n### What have you tried?\n\n1.simple install\n2.VirutalEnv install \nBoth didn't worked.\n", "comments": ["I think the problem here is that you have a file called `tensorflow.py` in your current directory. Python will interpret `import tensorflow as tf` as an instruction to load that script, rather than the installed TensorFlow module. Renaming that file to something else should fix the problem.\n", "@mrry Thanks and big facepalm for me :). \n", "No problem, it's happened to me a few times too! :)\n", "Yes, I think its mainly because when we are in terminal we don't see file tree. \n"]}, {"number": 2369, "title": "Fix typos in API docs and Python docstring for ExponentialMovingAverage object.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for this fix!\n"]}, {"number": 2367, "title": "Incorrect gradients for scatter_add", "body": "Hi Tensorflow Development Team,\n\nThank you so much for being so patient with me. I am currently working with scatter_add, and have encountered a None gradient where I believe there should be gradients.\n### Environment info\n\nOperating System: CentOS\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nCUDA version 7.5\ncuDNN version 7.0 (64 bit)\ntensorflow/0.8.0-gpu version\n### Steps to reproduce\n\n`N_rows = 10`\n`N_cols = 3`\n\n`X_ph = tf.placeholder(tf.float32, shape=(None, N_cols))`\n`ind_ph = tf.placeholder(tf.int32, shape=(None))`\n\n`Z = tf.Variable(tf.zeros([N_rows, N_cols]))`\n`Z = Z.assign(tf.zeros([N_rows, N_cols]))`\n`Z_add = tf.scatter_add(Z, ind_ph, X_ph)`\n`Z_sum = tf.reduce_sum(Z_add)`\n\n`grad_op = tf.gradients(Z_sum, X_ph)`\n\n`X = np.array([[1,0,1],[2,2,1]])`\n\n`ind = [0, 1]`\n### What have you tried?\n\nIf you try to differentiate Z_sum with respect to Z_add, you get a nonzero gradient, as expected. However, It seems that if you differentiate Z_sum with respect to Z or X_ph, you get a None, indicating no connection between Z_sum and X_ph or Z. I would definitely expect there to be a connection between Z_add and X_ph/Z. Do you think you could take a look at this and tell me how I might be able to circumvent this issue? Thank you so much for your help!\n\nBest,\nHan\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\nThe output given by tensorflow when evaluating grad_op is\n  File \"scatter_update_grad_test.py\", line 34, in <module>\n    out = sess.run(grad_op, feed_dict={X_ph : X, ind_ph : ind})\n  File \"/share/sw/free/tensorflow/0.8.0/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/share/sw/free/tensorflow/0.8.0/tensorflow/python/client/session.py\", line 523, in _run\n    processed_fetches = self._process_fetches(fetches)\n  File \"/share/sw/free/tensorflow/0.8.0/tensorflow/python/client/session.py\", line 493, in _process_fetches\n    % (subfetch, fetch, type(subfetch), str(e)))\nTypeError: Fetch argument None of None has invalid type <type 'NoneType'>, must be a string or Tensor. (Can not convert a NoneType into a Tensor or Operation.)\n\nIf I take grad_op = tf.gradients(Z_sum, Z_add), I obtain\narray([[ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.],\n       [ 1.,  1.,  1.]], dtype=float32)]\n\nas expected since we are using reduce sum.\n", "comments": ["I suspect there is no registered gradients function for scatter_add. Could anyone confirm? Thank you so much!\n", "Scatter add mutates one of its inputs - it's more like \"scatter assign-add\" - so I don't think it the current gradient system would support it. It certainly doesn't have a registered gradient function.\n\nI think this is covered by #2358 (\"scatter_add for non variable tensors\"). If there were a functional scatter_add-like op that operated on regular tensors, the concern about mutation would go away.\n\nI'm going to close this issue so that the discussion is in one place (on #2358).\n"]}, {"number": 2366, "title": "Functional ops don't accept Python lists as input.", "body": "See error reported in [this Stack Overflow question](http://stackoverflow.com/q/37221092/3574081):\n\n> ``` python\n> import tensorflow as tf\n> elems = [1, 2, 3, 4, 5, 6]\n> squares = tf.map_fn(lambda x: x * x, elems)\n> ```\n> \n> Running this gives this error:\n> \n> ```\n> /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape)\n>     323   else:\n>     324     if values is None:\n> --> 325       raise ValueError(\"None values not supported.\")\n>     326     # if dtype is provided, forces numpy array to be the type\n>     327     # provided if possible.\n> \n> ValueError: None values not supported.\n> ```\n\nThe functions in [`functional_ops.py`](https://github.com/tensorflow/tensorflow/blob/ddd86c0ff110b332171638890ba990d79def877b/tensorflow/python/ops/functional_ops.py) each expect their `elems` inputs to have a `dtype` property. As a side-issue, they convert their input to a tensor multiple times (implicitly on use), which could create a wasteful number of constants if the input is a numpy array.\n", "comments": ["This example is ambiguous. Numpy arrays should be converted to tensors.  Python lists and tuples should be treated as a multi arity input for a mapfn which runs each row of N inputs at the same time.\n"]}, {"number": 2365, "title": "pip install problem:SSLError", "body": "I encountered \" SSLError: hostname 'storage.googleapis.com' doesn't match 'www.google.com' \"\nafter typing \"sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\" \ncan somebody help me fix this? thanks a lot!\n", "comments": ["can you `import ssl` in python?\n", "This is a known issue with some `pip` installations. The workaround is documented [here](https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html#sslerror_ssl_verify_failed): simply download the `.whl` file using `curl` or `wget`, then pass the name of the downloaded file to `pip install` in place of the URL.\n"]}, {"number": 2364, "title": "zlib.h not found when compile tensorflow with GPU support", "body": "I am trying to compile tensorflow from source. I can build it **successfully** with CPU support only( i.e. not use `--config=cuda`) .\n\nBut when I try to build it with GPU support, I get error:\n\n```\n[chaowei@node07 tensorflow]$ export EXTRA_BAZEL_ARGS='-s --verbose_failures --ignore_unsupported_sandboxing --genrule_strategy=standalone --spawn_strategy=standalone --jobs 8'\n[chaowei@node07 tensorflow]$ \n[chaowei@node07 tensorflow]$ /gpfs/home/chaowei/download/bazel-0.1.5/output/bazel  build -c opt --config=cuda --linkopt '-lrt' --copt=\"-DGPR_BACKWARDS_COMPATIBILITY_MODE\" --conlyopt=\"-std=c99\" //tensorflow/tools/pip_package:build_pip_package\n...........\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /gpfs/home/chaowei/.cache/bazel/_bazel_chaowei/2ce35f089de902cec16e4a2c6a450834/external/grpc/BUILD:485:1: C++ compilation of rule '@grpc//:grpc_unsecure' failed: gcc failed: error executing command /gpfs/home/chaowei/software/gcc-6.1.0/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 ... (remaining 39 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nexternal/grpc/src/core/compression/message_compress.c:41:18: fatal error: zlib.h: No such file or directory\n #include <zlib.h>\n                  ^\ncompilation terminated.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 71.894s, Critical Path: 58.77s\n```\n\n**I also compile python3 from source in my computer. And when I `import zlib`, it works fine.**\n\nHere is the information of my system:\n`[chaowei@mgt ~]$ cat /etc/redhat-release Red Hat Enterprise Linux Server release 6.5 (Santiago)`\n\n```\n[chaowei@node07 gcc-6.1.0]$ gcc -v\nbuilt-in specs\u3002\nCOLLECT_GCC=gcc\nCOLLECT_LTO_WRAPPER=/gpfs/home/chaowei/software/gcc-6.1.0/libexec/gcc/x86_64-pc-linux-gnu/6.1.0/lto-wrapper\nTarget\uff1ax86_64-pc-linux-gnu\nConfigured with\uff1a./configure --prefix=/gpfs/home/chaowei/software/gcc-6.1.0\nThread model\uff1aposix\ngcc version 6.1.0 (GCC) \n```\n\nI wonder  why I get `zlib.h` error  when I only build tensorflow with GPU support.\n", "comments": ["You need zlib1g**-dev** installed. \n", "@hholst80 I have tried `yum install zlib-delve` before and it showed `zlib-devel has been installed`\n", "@333caowei: Where is your `zlib.h`? \n", "Closing as duplicate of #2536, which was filed later but seems to have a more useful thread.\n"]}, {"number": 2363, "title": "(skflow) issue in batch normalization after restore", "body": "Hi,\nI'm using Tensorflow [last succeed build #85](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-working/lastSuccessfulBuild/)\n\nI succeed to train a classifier with batch normalization and saving it with classifier.save(logdir) and restoring the classifier with \nclassifier = tf.contrib.skflow.TensorFlowEstimator.restore(logdir)\n\nthe classifier is restored but when using prediction classifier.predict(X_test) it seems that the batch normalization is applied as is in Training mode.\n\nLooking to the code of batch normalization (batch_norm_ops.py)\n`is_training = array_ops_.squeeze(ops.get_collection(\"IS_TRAINING\"))\n    mean, variance = control_flow_ops.cond(is_training, update_mean_var,\n                                           lambda: (ema_mean, ema_var))`\n\nI suppose as the graph is loaded in restore step, the \"IS_TRAINING\" is defined at the state of True at the saved step. and the cond function used will be update_mean_var at predict step , however the \"IS_TRAINING\" is changed at _predict step.\nyou can detect the problem when classifying 2 class problem and adding a batch normalization before the softmax the mean of the softmax of any tested data will be 0.\n\nMaybe a solution will be to use a placeholder in place of the variable is_training\n\nOr I'm missing something ?\n", "comments": ["Hello.\nActually this is an equivalent problem with next issue.\n\nhttps://github.com/tensorflow/tensorflow/issues/2167\n\nThese problems are caused by unsaved graph collections.\n", "@ilblackdragon: Is this a duplicate of #2167?  If so please close as such. \n", "Should be fixed now with current way of restoring via recreating the graph.\n"]}, {"number": 2362, "title": "Tensor flow installation problem: import error", "body": "I am having the following issue when I try to install TensorFlow for Python 2.7 on mac OS. This seems to be an issue with Tensor flow's libraries, thank you for your help!\n\n![screen shot 2016-05-14 at 3 11 14 pm](https://cloud.githubusercontent.com/assets/8773483/15266421/badcb0ec-19e7-11e6-9ad5-599c656cba59.png)\n", "comments": ["It looks like your version of `pip` is not installing things into the path that your version of `python` is expecting. Perhaps you have two versions of Python installed? What output do you get from `which pip` and `which python`? (These paths might give a clue....)\n", "Strangely I am getting this\n\n![screen shot 2016-05-18 at 1 21 02 pm](https://cloud.githubusercontent.com/assets/8773483/15346446/a394568a-1cfb-11e6-9258-c374601d50b7.png)\n", "Which version of OS X are you running? On closer inspection of your first screenshot, the error message is:\n\n```\nSymbol not found: ___sincos_stret\n```\n\nLooking [elsewhere](http://stackoverflow.com/questions/25926495/symbol-not-found-sincos-stret) suggests that this symbol is only available in OS X 10.9 and above.\n", "Closing due to lack of response. Feel free to reopen if you're still having problems (although note that OS X 10.8 or earlier support is probably out of scope...).\n"]}, {"number": 2361, "title": "Why the software history was not kept?", "body": "Hi there, \n\nI'm a researcher studying software evolution. As part of my current research, I'm studying the implications of open-sourcing a proprietary software, for instance, if the project succeed in attracting newcomers. Tensorflow was in my list. However, I observed that the software history of when the software was developed as a proprietary software was not kept after the transition to Github.\n\nKnowing that software history is indispensable for developers (e.g., developers need to refer to history several times a day), I would like to ask **TensorFlow** developers the following four brief questions:\n1. Why did you decide to not keep the software history?\n2. Do the _core developers_ faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems?\n3. Do the _newcomers_ faced any kind of problems, when trying to refer to the old history? If so, how did they solve these problems?\n4. How does the lack of history impacted on software evolution? Does it placed any burden in understanding and evolving the software?\n\nThanks in advance for your collaboration,\n\nGustavo Pinto, PhD\nhttp://www.gustavopinto.org\n", "comments": ["This kind of question is better suited to StackOverflow or an email to discuss@tensorflow.org.  It is not an issue with the TensorFlow code.\n", "Reponse from the mailing list: http://bit.ly/2qU65Vw"]}, {"number": 2360, "title": "Fix incorrect buildozer rule for contrib/ffmpeg/BUILD", "body": "This should fix a integration test failure in nightly builds.\n", "comments": []}, {"number": 2359, "title": "Fix incorrect buildozer rule for contrib/ffmpeg/BUILD", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "Ready to merge. Not sure why the CLA says \"no\". This should fix the ffmpeg integration test-related errors in nightly builds.\n"]}, {"number": 2358, "title": "scatter_add for non variable tensors", "body": "Hi,\n\nI am interested in using scatter_add when the tensor being update is not a variable. Is this possible?\n\nI am looking to do something like this:\n\n`X1_ph = tf.placeholder(tf.float32, shape=(None, 3))`\n`ind_ph = tf.placeholder(tf.int32, shape=(None))`\n\n`#Z = tf.Variable(tf.zeros([10, 3]))`\n`Z = tf.zeros([10, N_feat])`\n\n`X1 = np.array([[1,0.00,1],`\n`[2,0.00,1],`\n`[3,0.00,1],`\n`[5,0.00,1.1],`\n`[6,1.0,1.8]])`\n\n`ind = [0, 1, 1, 0, 0]`\n\n`Z = tf.scatter_add(Z, ind_ph, X1)`\n\nIf I declare Z as a tf.Variable, I can do this, but I need to call this operation hundreds of thousands of times, and do not want to store any copies of Z once I am done with them. If I were to declare Z as a Variable, would there be any way to destroy Z once I am done with it (maybe with a garbage collector or something similar)? Thank you so much for your help!\n", "comments": ["Use `tf.sparse_to_dense`.\n", "Actually, by \"this operation\" do you mean the whole thing, or do you want to allocate a `Z` and do a bunch of separate scatters into it before deallocating it?\n", "I'd like to be able to allocate Z, do a bunch of scatters, and then deallocate it while allocating another Z for a different version. Is there a function I can call to deallocate a variable? \n", "@altaetran: We could certainly make a deallocation op, but I don't think we have one at the moment?  However, using it would be somewhat awkward. \n\n@yuanbyu: Do you have an ideas?\n", "Is there any way to retrofit the scatter_add function to work directly on regular tensors produced by other tensorflow operations? \n", "Non-Variable tensors are immutable, so supporting scatter into them would break an important part of the model.  The uninitialize op is much easier.  Would you be interested in submitting a patch? :)\n", "Judging by #2367, it appears that @altaetran also requires gradients for this operation. It sounds to me like a functional op would be preferable for this purpose.\n", "/me whistles innocently ;) \n\nhttps://github.com/tensorflow/tensorflow/blob/5df4c71c86b28c2a4dd746bd67f00fc0281bd24f/tensorflow/python/ops/math_ops.py#L1525\n", "Excellent.  I won't have time to work on this PR in the near term.  @rryan Would you want to either take over or elaborate about how to use `temporary_variable` for this purpose?\n", "It wont help with the gradient bit (and this is a non-public op) but based on your example code you could do:\n\n``` python\nZ = gen_state_ops._temporary_variable(shape=..., dtype=...)\nZ_name = Z.op.name\ndestroy_op = gen_state_ops._destroy_temporary_variable(Z, var_name=Z_name)\nX1_ph = tf.placeholder(tf.float32, shape=(None, 3))\nind_ph = tf.placeholder(tf.int32, shape=(None))\nZ = tf.scatter_add(Z, ind_ph, X1_ph)\n\n\nwith tf.Session() as sess:\n  for _ in xrange(steps):\n    # run Z with placeholders filled in for X1_ph and ind_ph\n    sess.run(Z, feed_dict={X1_ph: ..., ind_ph: ...}) \n  # clean up -- destroy_op returns the value of Z and destroys the temporary variable\n  sess.run(destroy_op) \n```\n", "@rryan Is it possible to define valid gradients for this using `gradient_override_map` if we use `temporary_variable`?\n", "Removing my assignment since I won't have time to work on this personally.  @rryan Could you comment on my gradient question?  If there's a reasonable path forward we can mark this contributions welcome, but I'm not sure how the temporary variables stuff works.\n", "I thought there was a thread a while back (Feb) from @vanhoucke about how\nto do scatter add without using variables. If you can do without variables,\nyou can use same op to do it using same op hundreds of thousands of times\nby using persistent Tensors, since memory is recycled as soon as Python\nhandle is unassigned\n\nOn Tue, Aug 9, 2016 at 4:36 PM, Geoffrey Irving notifications@github.com\nwrote:\n\n> Removing my assignment since I won't have time to work on this personally.\n> @rryan https://github.com/rryan Could you comment on my gradient\n> question? If there's a reasonable path forward we can mark this\n> contributions welcome, but I'm not sure how the temporary variables stuff\n> works.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2358#issuecomment-238724880,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHKCD1QjFu6UZA-ehlmmJQCecarEeks5qeQ8EgaJpZM4IebkP\n> .\n", "Hey, this sounds really promising. Do you think you could provide a simple example of how one might do this? I'm not too familiar with persistent tensors. Thanks! \n", "sure, I'll put an example together tomorrow\n\nOn Tue, Aug 9, 2016 at 7:19 PM, Han Altae-Tran notifications@github.com\nwrote:\n\n> Hey, this sounds really promising. Do you think you could provide a simple\n> example of how one might do this? I'm not too familiar with persistent\n> tensors. Thanks!\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2358#issuecomment-238748219,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHJhKeaPZYS5TAF9lYh-sM6ZKR7Wgks5qeTVOgaJpZM4IebkP\n> .\n", "@altaetran here's an example that uses a helper I wrote to simplify dealing with persistent tensors\n1. Download [imperative.py](https://raw.githubusercontent.com/yaroslavvb/imperative/georgia/release/imperative.py) and save it in a place where you can import it (ie, same directory as your script)\n\n```\nimport tensorflow as tf\nimport imperative\nenv = imperative.Env(tf)\ntfi = env.tf\n\nN_feat = 3\nZ = tfi.zeros([10, N_feat])\nX1 = tfi.constant([[1,0.00,1],\n[2,0.00,1],\n[3,0.00,1],\n[5,0.00,1.1],\n[6,1.0,1.8]])\n\nind = [0, 1, 1, 0, 0]\nfor right_pos, left_pos in enumerate(ind):\n    new_row = Z[left_pos, :]+X1[right_pos, :]\n    # turn vector into 1-by-x matrix so we can concat it\n    new_row_mat = tfi.reshape(new_row, [1, -1])\n    # make new Tensor with old row replaced by updated version\n    Z = tfi.concat(0, [Z[:left_pos, :], new_row_mat, Z[left_pos+1:, :]])\nprint Z\n\n```\n\nThat should give\n\n```\nITensor([[ 12.           1.           3.89999986]\n [  5.           0.           2.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]\n [  0.           0.           0.        ]], dtype=float32)\n```\n\nI'm working on more docs for imperative.py, meanwhile there's an overview with some slides here\nhttps://github.com/yaroslavvb/imperative/blob/master/imperative_slides.pdf\n", "Would this work? Since this is just using TensorFlow ops under the hood, it propogates gradients too.\r\n\r\n```python\r\ndef scatter_add_tensor(ref, indices, updates, name=None):\r\n    \"\"\"\r\n    Adds sparse updates to a variable reference.\r\n\r\n    This operation outputs ref after the update is done. This makes it easier to chain operations that need to use the\r\n    reset value.\r\n\r\n    Duplicate indices are handled correctly: if multiple indices reference the same location, their contributions add.\r\n\r\n    Requires updates.shape = indices.shape + ref.shape[1:].\r\n    :param ref: A Tensor. Must be one of the following types: float32, float64, int64, int32, uint8, uint16,\r\n        int16, int8, complex64, complex128, qint8, quint8, qint32, half.\r\n    :param indices: A Tensor. Must be one of the following types: int32, int64. A tensor of indices into the first\r\n        dimension of ref.\r\n    :param updates: A Tensor. Must have the same dtype as ref. A tensor of updated values to add to ref\r\n    :param name: A name for the operation (optional).\r\n    :return: Same as ref. Returned as a convenience for operations that want to use the updated values after the update\r\n        is done.\r\n    \"\"\"\r\n    with tf.name_scope(name, 'scatter_add_tensor', [ref, indices, updates]) as scope:\r\n        ref = tf.convert_to_tensor(ref, name='ref')\r\n        indices = tf.convert_to_tensor(indices, name='indices')\r\n        updates = tf.convert_to_tensor(updates, name='updates')\r\n        ref_shape = tf.shape(ref, out_type=indices.dtype, name='ref_shape')\r\n        scattered_updates = tf.scatter_nd(indices, updates, ref_shape, name='scattered_updates')\r\n        with tf.control_dependencies([tf.assert_equal(ref_shape, tf.shape(scattered_updates, out_type=indices.dtype))]):\r\n            output = tf.add(ref, scattered_updates, name=scope)\r\n        return output\r\n```", "Thanks @AdityaGudimella ! I've tested the gradients and it seems to work. Can anybody else confirm?", "@AdityaGudimella you wrote \"Duplicate indices are handled correctly: if multiple indices reference the same location, their contributions add.\" for your function. I've tested this and it seems correct. However, I don't understand why :) Can you please explain? tf.scatter_nd doesn't seems to produce any guarantees if multiple indices reference the same location. \r\n", "@aliosmanulusoy According to #8102, it seems that tf.scatter_nd currently adds up duplicate updates", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "The latest version of Tensorflow (1.2) still only supports mutable Tensors.\r\n\r\nAlso, I think there is a bug of this function: \"ref\" is not updated.", "Any updates here? ", "@bodokaiser I don't think so. They are not working on this thread actively. One solution is to implement your own scatter_add() layer.", "It would really be very useful if this is implemented. ", "@altaetran @yaroslavvb @girving @mrry @rryan @aselle @itsmeolivia (or anyone who has access): Can this be reopened? It still is missing, and still it would be a useful addition, as this could potentially speed up some code (where you would currently need to use `tf.sparse_to_dense`, or `tf.where`, or so instead).", "[tensor_scatter_nd_add](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/tensor_scatter_nd_add) and friends are the solution to this problem I think so no need to reopen.", "Ah thanks, seems I missed that? Or when exactly was this added? (The documentation does not say this.)", "@albertz this was added fairly recently :-)\r\n", "what if I don't need gradient, can I do scatter_update to tensor just like variable?", "Just use tensor_scatter_update\n\nOn Tue, Apr 23, 2019 at 12:47 PM Yuanpu Xie <notifications@github.com>\nwrote:\n\n> what if I don't need gradient, can I do scatter_update to tensor just like\n> variable?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2358#issuecomment-485945652>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPQRSTWSLOOPFYSTMTPR5RTVANCNFSM4CDZXEHQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp  Hey,  thanks! but that api is in Tensorflow 1.13. is there any workaround for tf .1.12?", "No, please upgrade.\n\nOn Tue, Apr 23, 2019 at 1:11 PM Yuanpu Xie <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp> Hey, is there any workaround for tf\n> .1.12?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2358#issuecomment-485953826>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJJSHSYQHO6SYGD3YTPR5UPJANCNFSM4CDZXEHQ>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 2357, "title": "Merging internal changes to github", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Jenkins, please test this\n", "I signed it!\n", "@ilblackdragon @martinwicke This may have conflicts with my last PR with baseEstimator\n"]}, {"number": 2356, "title": "Enable GPU for SoftmaxCrossEntropyWithLogits float64", "body": "Enabled GPU registration for SoftmaxCrossEntropyWithLogits operations of type double. This partially addresses #1140. Tested locally.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins, test this please.\n", "Change looks good to me, thanks!\n"]}, {"number": 2355, "title": "skflow example text_classification needs file text_datasets.py", "body": "### Environment info\n\nOperating System:\n\n0.8.0\n\npip version 0.8.0 installation using\n\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n\ndoes not install text_datasets.py needed to run several skflow examples using text_classification su as \n\ntensorflow/tensorflow/examples/skflow/text_classification.py\n\nthe location of text_datasets.py is at\n\ntensorflow/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py\n\nSee image below comparing the pip installed directory structure with the github structure.\n\n![both](https://cloud.githubusercontent.com/assets/5605614/15260720/47deb1f6-190d-11e6-97cb-02d4ce859929.png)\n", "comments": ["Have you tried one of the nightly builds? https://github.com/tensorflow/tensorflow#installation\n", "sudo pip install --upgrade http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n\nDoes install the file text_datasets.py I will check tonight if the text_classification examples work.\n\nP.S. I work from the github site at \n\nhttps://github.com/hpssjellis/forth-tensorflow.git\n\nwhich installs skflow and lots of other stuff (R, iJupyter, tensorflow etc) onto cloud9.\n\nJeremy Ellis\nTwitter @rocksetta\n", "Thanks Terry. The nightly build fixed the text_classification.py file. Fair bit of number crunching, took about 5 minutes on cloud 9. I think all the text_ ...py files will work now.\n\nP.S.\n All my broken skflow files are in this folder at the moment\n\nhttps://github.com/hpssjellis/forth-tensorflow/tree/master/skflow-examples/broken\n\nI will move them out of the folder after I test them.\n", "Great. Please close this issue and submit new one if you encounter new problems.\n", "Thank you\n"]}, {"number": 2354, "title": "add shuffling option to tf.train.batch which handles variable-length sequence", "body": "tf.train.batch is the only one can handle variable-length sequence, but it does not have shuffling option.\nFor RNN training, shuffling is needed.\n", "comments": ["Use `tf.train.shuffle_batch`.\n", "However, tf.train.shuffle_batch does not offer padding zero to variable-length sequences, which is needed to deal with these kind of data for RNN training.\n", "@ebrevdo: Is there a way to get both bits of functionality in the same routine currently?  If not, what would be the best way to support it? \n", "Right now, batch and batch_join use a PaddingFIFOQueue if dynamic_pad=True otherwise a FIFOQueue.  shuffle_batch uses a ShuffleQueue which does not do batchwise padding.\n\nMaybe by combining the PaddingFIFOQueue with the ShuffleQueue underneath if someone passes dynamic_pad=True to shuffle_batch (which right now doesn't have that parameter)?  Marking as contributions welcome.  I don't have bandwidth to work on this in the near future.\n", "@ebrevdo: I've had a look at this and realized that the output shape of `PaddingFIFOQueue` is still dynamic where the input shape was dynamic. This makes sense, as we can only find out what the maximum size per dimension is when we execute `dequeue_many` at runtime.\nBut the dynamic output shape prevents me from piping the padded tensors into a `ShuffleQueue`.\nSo I'm not sure if it's possible to implement this without making a new C++ op or adjusting the existing ones.\n", "I have spent the last week and maybe more dealing with such [issue](http://stackoverflow.com/questions/43367697/batching-and-shuffling-padded-tf-train-sequenceexample). So, help me to uderstand (and save a lot of time): can we state that currently there is _no way_ of shuffling and batching using the official TF input pipeline utilities?", "You can easily shuffle and batch using tf.train.shuffle_batch *so long as\nyou don't have variable-length input tensors*.\n\nOn Wed, Apr 26, 2017 at 8:16 AM, Giulio Petrucci <notifications@github.com>\nwrote:\n\n> I have spent the last week and maybe more dealing with such issue\n> <http://stackoverflow.com/questions/43367697/batching-and-shuffling-padded-tf-train-sequenceexample>.\n> So, help me to uderstand (and save a lot of time): can we state that\n> currently there is *no way* of shuffling and batching using the official\n> TF input pipeline utilities?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2354#issuecomment-297442635>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw3Ax1vHD16jqGjTyGozxE8l73UGks5rz1_cgaJpZM4IeVSX>\n> .\n>\n", "Hi @ebrevdo and thanks for your reply. Actually I came out with this idea:\r\n* feed my parsed protobufs into a `tf.PaddingFIFOQueue`\r\n* feed it into a `tf.RandomShuffleQueue`\r\nand apparently it works fine. So, here follows the code of my test example (which is quite ugly, but no big deal). Hopefully I will set up something more interesting in one of my projects and maybe share the link here. Any comment is _more_ than welcome.\r\n\r\n```python\r\nimport random\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nMIN_LEN = 6\r\nMAX_LEN = 12\r\nNUM_EXAMPLES = 10\r\nBATCH_SIZE = 7\r\nPATH = 'ciaone.tfrecords'\r\nMIN_AFTER_DEQUEUE = 10\r\nNUM_THREADS = 2\r\nSAFETY_MARGIN = 1\r\nCAPACITY = MIN_AFTER_DEQUEUE + (NUM_THREADS + SAFETY_MARGIN) * BATCH_SIZE\r\n\r\n\r\ndef generate_example():\r\n    \"\"\"Generate an example.\"\"\"\r\n    length = random.randint(MIN_LEN, MAX_LEN)\r\n    input_ = [random.randint(1, 10) for _ in xrange(length)]\r\n    avg = sum([1.0 * item for item in input_]) / len(input_)\r\n    output = [item for item in input_ if item >= avg]\r\n    return input_, output\r\n\r\n\r\ndef encode(input_, output):\r\n    \"\"\"Encode a pair of input, output tensor into a tf.train.Example.\"\"\"\r\n    example = tf.train.Example(\r\n        features=tf.train.Features(\r\n            feature={\r\n                'input': tf.train.Feature(\r\n                    int64_list=tf.train.Int64List(\r\n                        value=input_)),\r\n                'output': tf.train.Feature(\r\n                    int64_list=tf.train.Int64List(\r\n                        value=output))}))\r\n    return example\r\n\r\n\r\ndef decode(example):\r\n    \"\"\"Decode an example into a tuple of input, output tensors.\"\"\"\r\n    features = {\r\n        'input': tf.VarLenFeature(tf.int64),\r\n        'output': tf.VarLenFeature(tf.int64)\r\n    }\r\n    parsed = tf.parse_single_example(\r\n        serialized=example,\r\n        features=features)\r\n    input_ = parsed['input']\r\n    output = parsed['output']\r\n    input_ = tf.sparse_tensor_to_dense(parsed['input'])\r\n    output = tf.sparse_tensor_to_dense(parsed['output'])\r\n    return input_, output\r\n\r\n\r\ndef main():\r\n    \"\"\"Run the example.\"\"\"\r\n\r\n    # 1. decode the records from the file.\r\n    file_queue = tf.train.string_input_producer(\r\n        [PATH], shuffle=True, num_epochs=2)\r\n    reader = tf.TFRecordReader()\r\n    key, value = reader.read(file_queue)\r\n    input_, output = decode(value)\r\n\r\n    # 2. padding queue.\r\n    padding_queue = tf.PaddingFIFOQueue(\r\n        capacity=CAPACITY,\r\n        dtypes=[tf.string, tf.int64, tf.int64],\r\n        shapes=[[], [None], [None]])\r\n    padding_enqueue_op = padding_queue.enqueue([key, input_, output])\r\n    padding_queue_runner = tf.train.QueueRunner(\r\n        padding_queue, [padding_enqueue_op] * NUM_THREADS)\r\n    tf.train.add_queue_runner(padding_queue_runner)\r\n    padding_dequeue_op = padding_queue.dequeue_up_to(BATCH_SIZE)\r\n\r\n    # 3. shuffling queue that tolerates different shapes.\r\n    shuffle_queue = tf.RandomShuffleQueue(\r\n        capacity=CAPACITY,\r\n        min_after_dequeue=MIN_AFTER_DEQUEUE,\r\n        dtypes=[tf.string, tf.int64, tf.int64],\r\n        shapes=None)\r\n    shuffle_enqueue_op = shuffle_queue.enqueue(padding_dequeue_op)\r\n    shuffle_queue_runner = tf.train.QueueRunner(\r\n        shuffle_queue, [shuffle_enqueue_op] * NUM_THREADS)\r\n    tf.train.add_queue_runner(shuffle_queue_runner)\r\n    shuffle_dequeue_op = shuffle_queue.dequeue()\r\n    fetches = shuffle_dequeue_op\r\n\r\n    # 4. run.\r\n    with tf.Session() as sess:\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(tf.global_variables_initializer())\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord)\r\n        try:\r\n            while True:\r\n                keys, inputs, outputs = tuple(sess.run(fetches))\r\n                for k, i in zip(keys, inputs):\r\n                    print(k + ': ' + str(i))\r\n                print(inputs.shape)\r\n                print\r\n        except tf.errors.OutOfRangeError as ex:\r\n            coord.request_stop(ex=ex)\r\n        finally:\r\n            coord.request_stop()\r\n            coord.join(threads)\r\n    print(' -- done -- ')\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Before running the example, generate a bunch of examples\r\n    # and save them in a file as TFRecords.\r\n    with tf.python_io.TFRecordWriter(PATH) as writer:\r\n        for _ in xrange(NUM_EXAMPLES):\r\n            ii, oo = generate_example()\r\n            example = encode(ii, oo)\r\n            writer.write(example.SerializeToString())\r\n    main()\r\n```", "Looks like you are shuffling the *minibatches*, not the individual\nminibatch elements, with your shuffle queue (you're calling enqueue and\ndequeue, not enqueue_many and dequeue_many).  So if there's heavy\ncorrelation locally between inputs, your minibatches will still have that\ncorrelation and you lose the expected benefits of shuffling.\n\nOn Wed, Apr 26, 2017 at 11:02 AM, Giulio Petrucci <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> and thanks for your reply.\n> Actually I came out with this idea:\n>\n>    - feed my parsed protobufs into a tf.PaddingFIFOQueue\n>    - feed it into a tf.RandomShuffleQueue\n>    and apparently it works fine. So, here follows the code of my test\n>    example (which is quite ugly, but no big deal). Hopefully I will set up\n>    something more interesting in one of my projects and maybe share the link\n>    here. Any comment is *more* than welcome.\n>\n> import random\n> import tensorflow as tf\n>\n> MIN_LEN = 6MAX_LEN = 12NUM_EXAMPLES = 10BATCH_SIZE = 7PATH = 'ciaone.tfrecords'MIN_AFTER_DEQUEUE = 10NUM_THREADS = 2SAFETY_MARGIN = 1CAPACITY = MIN_AFTER_DEQUEUE + (NUM_THREADS + SAFETY_MARGIN) * BATCH_SIZE\n>\n> def generate_example():\n>     \"\"\"Generate an example.\"\"\"\n>     length = random.randint(MIN_LEN, MAX_LEN)\n>     input_ = [random.randint(1, 10) for _ in xrange(length)]\n>     avg = sum([1.0 * item for item in input_]) / len(input_)\n>     output = [item for item in input_ if item >= avg]\n>     return input_, output\n>\n> def encode(input_, output):\n>     \"\"\"Encode a pair of input, output tensor into a tf.train.Example.\"\"\"\n>     example = tf.train.Example(\n>         features=tf.train.Features(\n>             feature={\n>                 'input': tf.train.Feature(\n>                     int64_list=tf.train.Int64List(\n>                         value=input_)),\n>                 'output': tf.train.Feature(\n>                     int64_list=tf.train.Int64List(\n>                         value=output))}))\n>     return example\n>\n> def decode(example):\n>     \"\"\"Decode an example into a tuple of input, output tensors.\"\"\"\n>     features = {\n>         'input': tf.VarLenFeature(tf.int64),\n>         'output': tf.VarLenFeature(tf.int64)\n>     }\n>     parsed = tf.parse_single_example(\n>         serialized=example,\n>         features=features)\n>     input_ = parsed['input']\n>     output = parsed['output']\n>     input_ = tf.sparse_tensor_to_dense(parsed['input'])\n>     output = tf.sparse_tensor_to_dense(parsed['output'])\n>     return input_, output\n>\n> def main():\n>     \"\"\"Run the example.\"\"\"\n>\n>     # 1. decode the records from the file.\n>     file_queue = tf.train.string_input_producer(\n>         [PATH], shuffle=True, num_epochs=2)\n>     reader = tf.TFRecordReader()\n>     key, value = reader.read(file_queue)\n>     input_, output = decode(value)\n>\n>     # 2. padding queue.\n>     padding_queue = tf.PaddingFIFOQueue(\n>         capacity=CAPACITY,\n>         dtypes=[tf.string, tf.int64, tf.int64],\n>         shapes=[[], [None], [None]])\n>     padding_enqueue_op = padding_queue.enqueue([key, input_, output])\n>     padding_queue_runner = tf.train.QueueRunner(\n>         padding_queue, [padding_enqueue_op] * NUM_THREADS)\n>     tf.train.add_queue_runner(padding_queue_runner)\n>     padding_dequeue_op = padding_queue.dequeue_up_to(BATCH_SIZE)\n>\n>     # 3. shuffling queue that tolerates different shapes.\n>     shuffle_queue = tf.RandomShuffleQueue(\n>         capacity=CAPACITY,\n>         min_after_dequeue=MIN_AFTER_DEQUEUE,\n>         dtypes=[tf.string, tf.int64, tf.int64],\n>         shapes=None)\n>     shuffle_enqueue_op = shuffle_queue.enqueue(padding_dequeue_op)\n>     shuffle_queue_runner = tf.train.QueueRunner(\n>         shuffle_queue, [shuffle_enqueue_op] * NUM_THREADS)\n>     tf.train.add_queue_runner(shuffle_queue_runner)\n>     shuffle_dequeue_op = shuffle_queue.dequeue()\n>     fetches = shuffle_dequeue_op\n>\n>     # 4. run.\n>     with tf.Session() as sess:\n>         sess.run(tf.local_variables_initializer())\n>         sess.run(tf.global_variables_initializer())\n>         coord = tf.train.Coordinator()\n>         threads = tf.train.start_queue_runners(coord=coord)\n>         try:\n>             while True:\n>                 keys, inputs, outputs = tuple(sess.run(fetches))\n>                 for k, i in zip(keys, inputs):\n>                     print(k + ': ' + str(i))\n>                 print(inputs.shape)\n>                 print\n>         except tf.errors.OutOfRangeError as ex:\n>             coord.request_stop(ex=ex)\n>         finally:\n>             coord.request_stop()\n>             coord.join(threads)\n>     print(' -- done -- ')\n>\n> if __name__ == '__main__':\n>     # Before running the example, generate a bunch of examples\n>     # and save them in a file as TFRecords.\n>     with tf.python_io.TFRecordWriter(PATH) as writer:\n>         for _ in xrange(NUM_EXAMPLES):\n>             ii, oo = generate_example()\n>             example = encode(ii, oo)\n>             writer.write(example.SerializeToString())\n>     main()\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2354#issuecomment-297493018>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3CiGcASU-3oCDyW7aj2IdCB2AZYks5rz4a0gaJpZM4IeVSX>\n> .\n>\n", "Hi @ebrevdo and thanks for the reply. I was shuffling minibatches indeed. So, to sum everything up, there is no way to do that? Otherwise, could you/anyone suggest a modification to my code to shuffle element across different minibatches? Thanks!  \r\n  \r\n(by the way, I thought it is quite a common scenario...)", "It is something folks want to do; and it is unfortunate there is no\nPaddingShuffleQueue.  There was a PR a while back to add it, but the PR was\nwithdrawn (perhaps the code is sitting in the original author's git repo\nstill?).  You can maybe review github issues for \"PaddingShuffleQueue\" to\nsee if the code can still be found.\n\nOn Wed, Apr 26, 2017 at 2:20 PM, Giulio Petrucci <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> and thanks for the reply. I was\n> shuffling minibatches indeed. So, to sum everything up, there is no way to\n> do that? Otherwise, could you/anyone suggest a modification to my code to\n> shuffle element across different minibatches? Thanks!\n>\n> (by the way, I thought it is quite a common scenario...)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2354#issuecomment-297544610>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw_BaqiGZh9GnmcYBxdFbb0OO8_pks5rz7U5gaJpZM4IeVSX>\n> .\n>\n", "Hi @ebrevdo and thanks for the reply. I followed your suggestion and scanned a bit through #5312 and #5147 and [this guy](https://github.com/tensorflow/tensorflow/issues/5147#issuecomment-271086206) made my day. ", "I have taken some inputs from the above suggestions, however facing some issue which I posted on https://stackoverflow.com/questions/44838356/paddingfifoqueue-enqueue-many-throwing-value-error-shapes-must-be-equal-rank\r\n\r\npls let me know if anyone has some clue on what is that I'm doing wrong", "Just a ping to see if there's been any activity on this? Seems like very basic functionality.", "The `tf.data` API supports shuffling elements containing variable-sized tensors, using [`tf.data.Dataset.shuffle()`](https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data/Dataset#shuffle). We're recommending that people use this API for new code, because it tends to be more efficient (and often much simpler!) than code written using `tf.train.batch()` (and related methods).", "Thanks @mrry. Does the new `Dataset` API support automatic bucketing by length? I was hoping to use `tf.contrib.training.bucket_by_sequence_length` and already have everything implemented using the old `tf.train`-based API.", "Yes; see the NMT tutorial code here\n<https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py#L197>\n(this\nbuckets by sequence length + batches with padding).  I believe\ntensor2tensor has similar code here\n<https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/data_reader.py#L117>\n.\n\nOn Fri, Oct 13, 2017 at 6:54 PM, Mohammed AlQuraishi <\nnotifications@github.com> wrote:\n\n> Thanks @mrry <https://github.com/mrry>. Does the new Dataset API support\n> automatic bucketing by length? I was hoping to use\n> tf.contrib.training.bucket_by_sequence_length and already have everything\n> implemented using the old tf.train-based API.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/2354#issuecomment-336601679>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimxJeXPvVg5GxRDZt7TR7uf2H5H20ks5ssBRfgaJpZM4IeVSX>\n> .\n>\n", "Tanks @mrry @ebrevdo ", "I also worked on an RNN model. You can shuffle your training data before batching. For instance, when you deserialize data from TFRecord files, you can use tf.train.string_input_producer(fileList, numEpochs, shuffle) to shuffle data within each epoch.\r\nhttps://www.tensorflow.org/api_docs/python/tf/train/string_input_producer"]}, {"number": 2353, "title": "Tensorboard: graph is shown but no scalar data was found", "body": "I can see the graph, but no scalar is shown.\n## Environment info\n\nMac OS 10.11.4\nAnaconda Python 2.7\nTensoflow installed from binary pip package\nThe output from python -c \"import tensorflow as tf; print(tf.**version**)\": 0.8.0\n## About the code\n\nI am trying to log values of loss using: `tf.scalar_summary('loss',loss)`\n\nI have the writer set to a particular folder: `summary_writer = tf.train.SummaryWriter(LogDir, sess.graph)`\n\nAfter calling `sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)` I do `summary_writer.flush()`\n## What have you tried?\n\nI can see a log file called events.out.tfevents.1463159389.Matheuss-MacBook-Pro.local in the folder I set. The file is about **16Mb** big.\n\nI call `tensorboard --logdir=/Users/mviana/Desktop/tf/log/` --debug and I can verify the log dir is correct.\n\nOn the browser I can see the graph corresponding to my deep neural network, but no scalar is shown.\n\nIf I call `tensorboard --inspect --logdir=/Users/mviana/Desktop/tf/log/` nothing but `Starting TensorBoard 16 on port 6006 (You can navigate to http://0.0.0.0:6006)` is shown in the terminal.\n\nIf I comment the line `tf.scalar_summary('loss',loss)` the log file size remains unchanged. So, it seems that only the graph is being logged.\n", "comments": ["Do you have a call to `tf.merge_all_summaries()` (or something similar) and do you evaluate the op? See e.g. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py#L151 and https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py#L191\n", "Following on cgorman's comment: based on your `session.run` call, it look like you aren't actually running the summary op, or adding the output of the summary op to the SummaryWriter. \n\nPlease take a look at the example code [here](https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py)\n", "Thanks guys. I was missing this:\n\n```\nsummary = sess.run(merged, feed_dict=feed_dict)\nsummary_writer.add_summary(summary, step)\n```\n", "I have a similar issue, except that I was running the example code:\ntensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\nI can see IMAGES and GRAPHS, but no EVENTS or HISTOGRAMS, with the following command:\n./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/mnist_logs/\n\nI compiled the code from source.\n\nThe following is output by adding \"--debug\":\n\n`./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/mnist_logs/ --debug`\n`INFO:tensorflow:TensorBoard is in debug mode.`\n`INFO:tensorflow:Starting TensorBoard in directory /Users/xkang/src/tensorflow`\n`INFO:tensorflow:TensorBoard path_to_run is: {'/tmp/mnist_logs/': None}`\n`INFO:tensorflow:Adding events from directory /tmp/mnist_logs/test`\n`INFO:tensorflow:Constructing EventAccumulator for /tmp/mnist_logs/test`\n`DEBUG:tensorflow:Opening a record reader pointing at /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`\n`DEBUG:tensorflow:No more events in /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`\n`INFO:tensorflow:No path found after /tmp/mnist_logs/test/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`\n`INFO:tensorflow:Adding events from directory /tmp/mnist_logs/train`\n`INFO:tensorflow:Constructing EventAccumulator for /tmp/mnist_logs/train`\n`DEBUG:tensorflow:Opening a record reader pointing at /tmp/mnist_logs/train/events.out.tfevents.1464574397.Kangs-MacBook-Pro.local`\n`INFO:tensorflow:TensorBoard is tag: 20`\n`Starting TensorBoard 20 on port 6006`\n`(You can navigate to http://0.0.0.0:6006)`\n\nAnd this is the output with \"--inspect\":\n\n`./bazel-bin/tensorflow/tensorboard/tensorboard --inspect --logdir=/tmp/mnist_logs/`\n`======================================================================`\n`Processing event files... (this can take a few minutes)`\n`======================================================================`\n\n`Found event files in:`\n`/tmp/mnist_logs/test`\n`/tmp/mnist_logs/train`\n\n`These tags are in /tmp/mnist_logs/test:`\n`audio -`\n`histograms`\n`layer1/activations`\n`layer1/biases`\n`layer1/pre_activations`\n`layer1/weights`\n`layer2/activations`\n`layer2/biases`\n`layer2/pre_activations`\n`layer2/weights`\n`images`\n`input/image/0`\n`input/image/1`\n`input/image/2`\n`input/image/3`\n`input/image/4`\n`input/image/5`\n`input/image/6`\n`input/image/7`\n`input/image/8`\n`input/image/9`\n`scalars`\n`accuracy`\n`cross entropy`\n`dropout_keep_probability`\n`max/layer1/biases`\n`max/layer1/weights`\n`max/layer2/biases`\n`max/layer2/weights`\n`mean/layer1/biases`\n`mean/layer1/weights`\n`mean/layer2/biases`\n`mean/layer2/weights`\n`min/layer1/biases`\n`min/layer1/weights`\n`min/layer2/biases`\n`min/layer2/weights`\n`sttdev/layer1/biases`\n`sttdev/layer1/weights`\n`sttdev/layer2/biases`\n`sttdev/layer2/weights`\n`======================================================================`\n\n`Event statistics for /tmp/mnist_logs/test:`\n`audio -`\n`graph -`\n`histograms`\n`first_step           0`\n`last_step            990`\n`max_step             990`\n`min_step             0`\n`num_steps            100`\n`outoforder_steps     []`\n`images`\n`first_step           0`\n`last_step            990`\n`max_step             990`\n`min_step             0`\n`num_steps            100`\n`outoforder_steps     []`\n`scalars`\n`first_step           0`\n`last_step            990`\n`max_step             990`\n`min_step             0`\n`num_steps            100`\n`outoforder_steps     []`\n`sessionlog:checkpoint -`\n`sessionlog:start -`\n`sessionlog:stop -`\n`======================================================================`\n\n`These tags are in /tmp/mnist_logs/train:`\n`audio -`\n`histograms`\n`layer1/activations`\n`layer1/biases`\n`layer1/pre_activations`\n`layer1/weights`\n`layer2/activations`\n`layer2/biases`\n`layer2/pre_activations`\n`layer2/weights`\n`images`\n`input/image/0`\n`input/image/1`\n`input/image/2`\n`input/image/3`\n`input/image/4`\n`input/image/5`\n`input/image/6`\n`input/image/7`\n`input/image/8`\n`input/image/9`\n`scalars`\n`accuracy`\n`cross entropy`\n`dropout_keep_probability`\n`max/layer1/biases`\n`max/layer1/weights`\n`max/layer2/biases`\n`max/layer2/weights`\n`mean/layer1/biases`\n`mean/layer1/weights`\n`mean/layer2/biases`\n`mean/layer2/weights`\n`min/layer1/biases`\n`min/layer1/weights`\n`min/layer2/biases`\n`min/layer2/weights`\n`sttdev/layer1/biases`\n`sttdev/layer1/weights`\n`sttdev/layer2/biases`\n`sttdev/layer2/weights`\n`======================================================================`\n\n`Event statistics for /tmp/mnist_logs/train:`\n`audio -`\n`graph`\n`first_step           0`\n`last_step            0`\n`max_step             0`\n`min_step             0`\n`num_steps            1`\n`outoforder_steps     []`\n`histograms`\n`first_step           1`\n`last_step            999`\n`max_step             999`\n`min_step             1`\n`num_steps            900`\n`outoforder_steps     []`\n`images`\n`first_step           1`\n`last_step            999`\n`max_step             999`\n`min_step             1`\n`num_steps            900`\n`outoforder_steps     []`\n`scalars`\n`first_step           1`\n`last_step            999`\n`max_step             999`\n`min_step             1`\n`num_steps            900`\n`outoforder_steps     []`\n`sessionlog:checkpoint -`\n`sessionlog:start -`\n`sessionlog:stop -`\n`======================================================================`\n", "@kangxin That's strange. Based on the output from --inspect it looks like TensorBoard is detecting the scalar and histogram data.\n\nCan you upload the events file so I can inspect them?\n", "@danmane Thank you for quick response. I have attached the event files in \n[mnist_logs.zip](https://github.com/tensorflow/tensorflow/files/290179/mnist_logs.zip)\n", "Hi @kangxin, \nI've loaded your event files on my machine, and TensorBoard is displaying them properly. So it seems like the problem is not with the data itself, but with something specific to your TensorBoard or frontend.\n\nIt looks suspiciously similar to #2607, where someone reported that the demo instance at https://www.tensorflow.org/tensorboard/index.html#events is not showing them events or histograms, so maybe it is a problem affecting a particular browser/OS.\n\nPlease check if that url is working for you. If it is broken, then please tell me which browser and operating system you are using. If it is not broken, then there's probably something wrong with your particular TensorBoard, so please give me the following info: \n- Which version of TensorBoard are you using\n- What browser and operating system are you using\n- Does the issue still re-occur if you use a pip-installed TensorBoard? \n", "Hi @danmane,\nThank you for your help!\nMy problem seems the same as #2607. Both events and histograms are empty.\nI am using Safari on OS X 10.11.5. I installed TensorFlow (0.8) from source.\n\nI also find that Chrome could perfectly show events and histograms on Mac. \n", "OK, I've fixed that at head, see: https://github.com/tensorflow/tensorflow/commit/53344688b8be0bb3e73431b1e5a0ad924c024732\n\nThat said, we only guarantee support for Chrome and Firefox, and performance and experience are likely to be best with Chrome.\n", "Was just having similar problems, and double-checked multiple times that everything was set up correctly. Turned out my example ops were too \"cheap\" to compute, and adding a `file_writer.flush()` after adding the summary to the write solved the issue.", "@dandelionmane  I have similar problems when  running the example code:\r\ntensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\r\nI can see only GRAPHS, with the following command:\r\ntensorboard --logdir /tmp/tensorflow/mnist/logs/mnist_with_summaries \r\n Chrome and firefox have the same problem , tensorboard version is  1.0 . run on ubuntu.\r\n\r\n[mnist_with_summaries.tar.gz](https://github.com/tensorflow/tensorflow/files/892446/mnist_with_summaries.tar.gz)\r\n![issue](https://cloud.githubusercontent.com/assets/4977856/24648006/b6aa502a-1954-11e7-9475-60ce2ac9ab6c.png)\r\n\r\n", "verified, adding `writer.flush()` solved the problem. It happened to me that locally was okay but on google cloud no scalar logs are saved. Thank you @rasbt :)", "@liufuyang where to add writer.flush() function?", "@yvaish87 I'd would be best to add it to the step where you want to update the summary, for example, sth. like \r\n\r\n```python\r\nwith tf.Session(graph=g) as sess:\r\n    \r\n    sess.run(tf.global_variables_initializer())\r\n    \r\n    # create FileWrite object that writes the logs\r\n    file_writer = tf.summary.FileWriter(logdir='logs/3', graph=g)\r\n    \r\n    for i in range(5):\r\n        # fetch the summary from the graph\r\n        result, summary = sess.run([train_op, merged_summary],\r\n                                    feed_dict={some_value: i})\r\n        # write the summary to the log\r\n        file_writer.add_summary(summary=summary, global_step=i)\r\n        file_writer.flush()\r\n```", "Thanks @rasbt ! I have the same issue, and either writer.flush or writer.close work for me:\r\n\r\nBefore using flush/close:\r\n\r\n```\r\nEvent statistics for logs/test/2:\r\naudio -\r\ngraph -\r\nhistograms -\r\nimages -\r\nscalars\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\ntensor -\r\n```\r\n\r\nafter:\r\n```\r\nEvent statistics for logs/test/2:\r\naudio -\r\ngraph -\r\nhistograms -\r\nimages -\r\nscalars\r\n   first_step           0\r\n   last_step            24\r\n   max_step             24\r\n   min_step             0\r\n   num_steps            25\r\n   outoforder_steps     []\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\ntensor -\r\n\r\n```\r\n", "found \"Uncaught SyntaxError: Block-scoped declarations (let, const, function, class) not yet supported outside strict mode\"\r\n![2017-07-05 09 03 36](https://user-images.githubusercontent.com/9128530/27845408-c745f24a-6161-11e7-98a0-1b0401faee65.png)\r\n\r\njust add \"use strict\"; to the function blocks in python2.7/site-packages/tensorflow/tensorboard/dist/tf-tensorboard.html, and it works.\r\n![2017-07-05 09 12 20](https://user-images.githubusercontent.com/9128530/27845551-e554eae2-6162-11e7-8fdd-c2c90ff74a2e.png)\r\n", "try to upgrade tensorflow and ensure tensboard and tensorflow are running same versions...i see these inconsitent problems if both are of different versions..", "self.summary_writer.flush() worked for me"]}]