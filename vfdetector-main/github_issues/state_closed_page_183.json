[{"number": 49264, "title": "Oh and 1 other thing", "body": "\r\nU fucking crossed the line with your fucking \"specific version dependencies\"\r\n\r\nERROR: tensorflow 2.5.0 has requirement grpcio~=1.34.0, but you'll have grpcio 1.32.0 which is incompatible.\r\nERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\r\nERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\r\nERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\r\nERROR: apache-beam 2.29.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1, but you'll have avro-python3 1.10.2 which is **incompatible.**\r\n\r\ndoes ur shitty tensorflor 2.5.0 distro needs fuckin' \"grpcio\" ? fine, you make ur fucking shitterflow compatible with any version of grpcio from 0.0001a to 10.31.8 or otherwise go fuck urself and do not release any 2.X shit. period. dipshits.\r\n\r\nBtw feel free to ban from github the fake account registered trught some obscure russin darknet website Im currently using if this helps rais ur pathetic self esteem, I just wish you could be able to ban the incompetence and stupidity that lie in your shitheads as well\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please respect our [Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md)\r\n/cc @theadactyl ", "> \r\n> \r\n> Please respect our [Code of Conduct](https://github.com/tensorflow/tensorflow/blob/master/CODE_OF_CONDUCT.md)\r\n> /cc @theadactyl\r\n\r\nIf you'd care more about code cleanness you would not have to care about issue messages cleanness\r\n.\r\n.\r\n.\r\n.\r\n.\r\nmotherfucker :)"]}, {"number": 49263, "title": "Compilation of TF 2.6 (master) crash", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Cloned master from Git\r\n- TensorFlow version (use command below): refs/heads/master & refs/remotes/origin/HEAD & refs/remotes/origin/master\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): VC 2019\r\n- CUDA/cuDNN version: 11.3 / 8\r\n- GPU model and memory: GTX 1060 (6GB)\r\n\r\n**Describe the current behavior**\r\nI try to compile a monolith c++ library with this command:\r\n\r\n```\r\nbazel --output_user_root=\"E:\\tensorflow_gpu_cpp_v4\\output\" build --local_ram_resources=HOST_RAM*.3 --config=opt --config=cuda --copt=-nvcc_options=disable-warnings --define=no_tensorflow_py_deps=true -s --config=monolithic --define framework_shared_object=false --explain=explain.txt --verbose_explanations --subcommands=pretty_print --repository_cache=\"D:\\Data\\Users\\andre-pl\\Projects\\TF_CACHE\"  //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nDuring compilation I get this error:\r\n```\r\nSUBCOMMAND: # @llvm-project//llvm:config_gen [action 'Executing genrule @llvm-project//llvm:config_gen', configuration: a480bc448cf44313f1f5e14498fa07b461cb9a16e319fb75f5074c1505e5d54e, execution platform: @local_execution_config_platform//:platform]\r\ncd E:/tensorflow_gpu_cpp_v4/output/tqha2ckh/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/cudnn-11.3-windows-x64-v8.2.0.53\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\extras\\CUPTI\\lib64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\extras\\CUPTI\\include;C:\\Program Files\\cudnn-11.3-windows-x64-v8.2.0.53\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.24.28314\\bin\\Hostx64\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\windows\\system32;C:\\windows;C:\\windows\\System32\\Wbem;C:\\windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\windows\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Program Files\\Amazon\\AWSCLI\\;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\;C:\\Program Files\\Bazel;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Microsoft\\Web Platform Installer\\;C:\\Program Files (x86)\\Microsoft ASP.NET\\ASP.NET Web Pages\\v1.0\\;C:\\Program Files\\Microsoft SQL Server\\110\\Tools\\Binn\\;D:\\orbograph\\NuanceEng\\bin;C:\\Program Files\\CMake\\bin;C:\\Program Files\\PuTTY\\;C:\\Program Files\\TortoiseGit\\bin;C:\\Program Files\\SafeNet\\Authentication\\SAC\\x64;C:\\Program Files\\SafeNet\\Authentication\\SAC\\x32;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2021.1.0\\;C:\\msys64\\usr\\bin\\;C:\\Program Files\\Conan\\conan;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\andre-pl\\AppData\\Local\\Microsoft\\WindowsApps;;C:\\Program Files\\Fiddler;C:\\Program Files (x86)\\Sophos\\Sophos SSL VPN Client\\bin;C:\\Users\\andre-pl\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET PYTHON_BIN_PATH=D:/Data/Users/andre-pl/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=D:/Data/Users/andre-pl/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0\r\n    SET TF_CUDA_VERSION=11.3\r\n    SET TF_CUDNN_VERSION=8\r\n  C:/msys64/usr/bin/bash.exe \\\r\n    -c \\\r\n    source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/third_party/llvm/expand_cmake_vars.exe \"ENABLE_BACKTRACES=1\" \"LLVM_BINDIR=/dev/null\" \"LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0\" \"LLVM_ENABLE_ABI_BREAKING_CHECKS=0\" \"LLVM_ENABLE_THREADS=1\" \"LLVM_ENABLE_ZLIB=1\" \"LLVM_HAS_ATOMICS=1\" \"LLVM_INCLUDEDIR=/dev/null\" \"LLVM_INFODIR=/dev/null\" \"LLVM_MANDIR=/dev/null\" \"LLVM_NATIVE_TARGET=1\" \"LLVM_NATIVE_TARGETINFO=1\" \"LLVM_NATIVE_TARGETMC=1\" \"LLVM_NATIVE_ASMPRINTER=1\" \"LLVM_NATIVE_ASMPARSER=1\" \"LLVM_NATIVE_DISASSEMBLER=1\" \"LLVM_PREFIX=/dev/null\" \"LLVM_VERSION_MAJOR=0\" \"LLVM_VERSION_MINOR=0\" \"LLVM_VERSION_PATCH=0\" \"PACKAGE_NAME=llvm\" \"PACKAGE_STRING=llvm tensorflow-trunk\" \"PACKAGE_VERSION=tensorflow-trunk\" \"RETSIGTYPE=void\" \"LLVM_HOST_TRIPLE=x86_64-pc-win32\" \"LLVM_DEFAULT_TARGET_TRIPLE=x86_64-pc-win32\" \"LLVM_NATIVE_ARCH=X86\" \"HAVE_ERRNO_H=1\" \"HAVE_EXECINFO_H=1\" \"HAVE_FCNTL_H=1\" \"HAVE_FENV_H=1\" \"HAVE_INTTYPES_H=1\" \"HAVE_MALLOC_H=1\" \"HAVE_SIGNAL_H=1\" \"HAVE_STDINT_H=1\" \"HAVE_SYS_STAT_H=1\" \"HAVE_SYS_TYPES_H=1\" \"HAVE_ZLIB_H=1\" \"BACKTRACE_HEADER=execinfo.h\" \"HAVE_GETCWD=1\" \"HAVE_INT64_T=1\" \"HAVE_STRERROR=1\" \"HAVE_STRTOLL=1\" \"HAVE_SYSCONF=1\" \"HAVE_UINT64_T=1\" \"HAVE__CHSIZE_S=1\" \"HAVE___CHKSTK=1\" \"stricmp=_stricmp\" \"strdup=_strdup\" \"LTDL_SHLIB_EXT=.dll\"< external/llvm-project/llvm/include/llvm/Config/config.h.cmake > bazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include/llvm/Config/config.h\r\nERROR: E:/tensorflow_gpu_cpp_v4/output/tqha2ckh/external/llvm-project/llvm/BUILD:53:18: Executing genrule @llvm-project//llvm:llvm_config_gen failed (Exit -1073741819): bash.exe failed: error executing command\r\n  cd E:/tensorflow_gpu_cpp_v4/output/tqha2ckh/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\libnvvp;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\include;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\extras\\CUPTI\\lib64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\\extras\\CUPTI\\include;C:\\Program Files\\cudnn-11.3-windows-x64-v8.2.0.53\\bin;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\Tools\\MSVC\\14.24.28314\\bin\\Hostx64\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Professional\\VC\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\windows\\system32;C:\\windows;C:\\windows\\System32\\Wbem;C:\\windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\windows\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn\\;C:\\Program Files\\Amazon\\AWSCLI\\;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\;C:\\Program Files\\Bazel;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Microsoft\\Web Platform Installer\\;C:\\Program Files (x86)\\Microsoft ASP.NET\\ASP.NET Web Pages\\v1.0\\;C:\\Program Files\\Microsoft SQL Server\\110\\Tools\\Binn\\;D:\\orbograph\\NuanceEng\\bin;C:\\Program Files\\CMake\\bin;C:\\Program Files\\PuTTY\\;C:\\Program Files\\TortoiseGit\\bin;C:\\Program Files\\SafeNet\\Authentication\\SAC\\x64;C:\\Program Files\\SafeNet\\Authentication\\SAC\\x32;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2021.1.0\\;C:\\msys64\\usr\\bin\\;C:\\Program Files\\Conan\\conan;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;D:\\Data\\Users\\andre-pl\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\andre-pl\\AppData\\Local\\Microsoft\\WindowsApps;;C:\\Program Files\\Fiddler;C:\\Program Files (x86)\\Sophos\\Sophos SSL VPN Client\\bin;C:\\Users\\andre-pl\\AppData\\Local\\Microsoft\\WindowsApps\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/third_party/llvm/expand_cmake_vars.exe \"ENABLE_BACKTRACES=1\" \"LLVM_BINDIR=/dev/null\" \"LLVM_DISABLE_ABI_BREAKING_CHECKS_ENFORCING=0\" \"LLVM_ENABLE_ABI_BREAKING_CHECKS=0\" \"LLVM_ENABLE_THREADS=1\" \"LLVM_ENABLE_ZLIB=1\" \"LLVM_HAS_ATOMICS=1\" \"LLVM_INCLUDEDIR=/dev/null\" \"LLVM_INFODIR=/dev/null\" \"LLVM_MANDIR=/dev/null\" \"LLVM_NATIVE_TARGET=1\" \"LLVM_NATIVE_TARGETINFO=1\" \"LLVM_NATIVE_TARGETMC=1\" \"LLVM_NATIVE_ASMPRINTER=1\" \"LLVM_NATIVE_ASMPARSER=1\" \"LLVM_NATIVE_DISASSEMBLER=1\" \"LLVM_PREFIX=/dev/null\" \"LLVM_VERSION_MAJOR=0\" \"LLVM_VERSION_MINOR=0\" \"LLVM_VERSION_PATCH=0\" \"PACKAGE_NAME=llvm\" \"PACKAGE_STRING=llvm tensorflow-trunk\" \"PACKAGE_VERSION=tensorflow-trunk\" \"RETSIGTYPE=void\" \"LLVM_HOST_TRIPLE=x86_64-pc-win32\" \"LLVM_DEFAULT_TARGET_TRIPLE=x86_64-pc-win32\" \"LLVM_NATIVE_ARCH=X86\" \"HAVE_ERRNO_H=1\" \"HAVE_EXECINFO_H=1\" \"HAVE_FCNTL_H=1\" \"HAVE_FENV_H=1\" \"HAVE_INTTYPES_H=1\" \"HAVE_MALLOC_H=1\" \"HAVE_SIGNAL_H=1\" \"HAVE_STDINT_H=1\" \"HAVE_SYS_STAT_H=1\" \"HAVE_SYS_TYPES_H=1\" \"HAVE_ZLIB_H=1\" \"BACKTRACE_HEADER=execinfo.h\" \"HAVE_GETCWD=1\" \"HAVE_INT64_T=1\" \"HAVE_STRERROR=1\" \"HAVE_STRTOLL=1\" \"HAVE_SYSCONF=1\" \"HAVE_UINT64_T=1\" \"HAVE__CHSIZE_S=1\" \"HAVE___CHKSTK=1\" \"stricmp=_stricmp\" \"strdup=_strdup\" \"LTDL_SHLIB_EXT=.dll\"< external/llvm-project/llvm/include/llvm/Config/llvm-config.h.cmake > bazel-out/x64_windows-opt-exec-50AE0418/bin/external/llvm-project/llvm/include/llvm/Config/llvm-config.h\r\nExecution platform: @local_execution_config_platform//:platform\r\n      0 [main] bash (43208) C:\\msys64\\usr\\bin\\bash.exe: *** fatal error - add_item (\"\\??\\C:\\msys64\", \"/\", ...) failed, errno 1\r\nStack trace:\r\nFrame        Function    Args\r\n000FFFF8630  00180062835 (00180296FE2, 00180272E41, 0000000003F, 000FFFF8B10)\r\n000FFFF8B60  001800488F2 (000FFFFFFFF, 00180020010, 000FFFFABCA, 000FFFF9BB0)\r\n000FFFF9B70  00180048931 (000FFFF9BB0, 00000000001, 0000000003F, 00000000001)\r\n000FFFF9C00  001800E5D0D (00000000000, 00140000024, 00000000000, 000FFFFCC50)\r\n000FFFFCC70  001801363B5 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCCE0  00180048F75 (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCDA0  0018004794A (00000000000, 00000000000, 00000000000, 00000000000)\r\n000FFFFCE50  00180047A0C (00000000000, 00000000000, 00000000000, 00000000000)\r\nEnd of stack trace\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 29.040s, Critical Path: 23.42s\r\nINFO: 67 processes: 67 internal.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Currently TF is broken on Windows (see for example #49210 for a previous instance).\r\n\r\nIt is better to always give the commit hash if building from master", "Adding \"--jobs 1\" to build parameters helped to overcome the issue.", "> It is better to always give the commit hash if building from master\r\n\r\nGood point! ", "@AndreyPlotkinOr \r\nAre you still facing this issue, please let us know.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49263\">No</a>\n"]}, {"number": 49262, "title": "Keras mask propagation between recurrent, pooling and Dense layers from Masking layer", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n\r\n**Current behavior**\r\n\r\nFrom what I understand, the mask created by the Masking layer propagates only through certain layers, and I am having trouble figuring out through which layers and if the end result will vary depending on this mask's absence.\r\n\r\nIn my example, I have inputs of shape `(None, timesteps, features)` already embedded and 0-padded in order to have constant `timesteps` value. I want to pass the inputs through some recurrent layers, and then perform either classification (or token classification, but the problem remains the same). The questions are:\r\n1) Do I have a problem if my output layer does not have an `input_mask` (0-padding computed in loss, for example)?\r\n2) Do I need to make custom layers every time I need my mask to propagate (when not modifying the `timesteps` dimension)? \r\n\r\nFor each of the four minimal examples below, I run this piece of code to figure out which layer has or hasn't an `input_mask`  attribute:\r\n```\r\nmodel = make_model(120, 768)\r\nprint(model.summary())\r\nfor layer in model.layers:\r\n    print(layer.name, layer.input_mask)\r\n``` \r\n\r\n**Example 1: Token classification with Masking+LSTM+Dense**\r\n```\r\ndef make_model(m_len: int, emb_dim: int):\r\n    inputs = Input(shape=(m_len, emb_dim))\r\n    x = Masking(mask_value=0)(inputs)\r\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\r\n    output = Dense(1, activation=\"sigmoid\")(x)\r\n    mod = Model(inputs, output)\r\n    mod.compile(\r\n        optimizer=\"Adam\",\r\n        loss=\"binary_crossentropy\",\r\n        metrics=[\"accuracy\"]\r\n    )\r\n    return mod\r\n```\r\noutput:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 120, 768)]        0         \r\n_________________________________________________________________\r\nmasking (Masking)            (None, 120, 768)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 120, 512)          2099200   \r\n_________________________________________________________________\r\ndense (Dense)                (None, 120, 1)            513       \r\n=================================================================\r\nTotal params: 2,099,713\r\nTrainable params: 2,099,713\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\ninput_1 None\r\nmasking None\r\nbidirectional KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='masking/Squeeze:0')\r\ndense KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='Placeholder_2:0')\r\n```\r\nThe mask is clearly propagated through both the recurrent layer and is also here in the Dense layer since the `timesteps` dimension hasn't changed.\r\n\r\n**Example 2: Classification with Masking+LSTM+Dense**\r\n\r\nSame code as above but setting `return_sequences=False`. output:\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 120, 768)]        0         \r\n_________________________________________________________________\r\nmasking (Masking)            (None, 120, 768)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 512)               2099200   \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 513       \r\n=================================================================\r\nTotal params: 2,099,713\r\nTrainable params: 2,099,713\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\ninput_1 None\r\nmasking None\r\nbidirectional KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='masking/Squeeze:0')\r\ndense None\r\n```\r\nWorks as expected so far, the dimension reduction doesn't allow a mask to pass on.\r\n\r\n**Example 3: Classification with Masking+LSTM**\r\n```\r\ndef make_model(m_len: int, emb_dim: int):\r\n    inputs = Input(shape=(m_len, emb_dim))\r\n    x = Masking(mask_value=0)(inputs)\r\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\r\n    output = LSTM(1, activation=\"sigmoid\")(x)\r\n    mod = Model(inputs, output)\r\n    mod.compile(\r\n        optimizer=\"Adam\",\r\n        loss=\"binary_crossentropy\",\r\n        metrics=[\"accuracy\"]\r\n    )\r\n    return mod\r\n```\r\noutput\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 120, 768)]        0         \r\n_________________________________________________________________\r\nmasking (Masking)            (None, 120, 768)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 120, 512)          2099200   \r\n_________________________________________________________________\r\nlstm_1 (LSTM)                (None, 1)                 2056      \r\n=================================================================\r\nTotal params: 2,101,256\r\nTrainable params: 2,101,256\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\ninput_1 None\r\nmasking None\r\nbidirectional KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='masking/Squeeze:0')\r\nlstm_1 KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='Placeholder_2:0')\r\n```\r\nEven though the `timesteps` dim is removed, the mask is still effective on the output layer, is this behavior expected?\r\n\r\n**Example 4: Classification with Masking+LSTM+AveragePooling+Flatten+Dense**\r\n```\r\ndef make_model(m_len: int, emb_dim: int):\r\n    inputs = Input(shape=(m_len, emb_dim))\r\n    x = Masking(mask_value=0)(inputs)\r\n    x = Bidirectional(LSTM(256, return_sequences=True))(x)\r\n    x = AveragePooling1D(pool_size=512, data_format='channels_first')(x)\r\n    x = Flatten()(x)\r\n    output = Dense(1, activation=\"sigmoid\")(x)\r\n    mod = Model(inputs, output)\r\n    mod.compile(\r\n        optimizer=\"Adam\",\r\n        loss=\"binary_crossentropy\",\r\n        metrics=[\"accuracy\"]\r\n    )\r\n    return mod\r\n```\r\noutput:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 120, 768)]        0         \r\n_________________________________________________________________\r\nmasking (Masking)            (None, 120, 768)          0         \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 120, 512)          2099200   \r\n_________________________________________________________________\r\naverage_pooling1d (AveragePo (None, 120, 1)            0         \r\n_________________________________________________________________\r\nflatten (Flatten)            (None, 120)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 1)                 121       \r\n=================================================================\r\nTotal params: 2,099,321\r\nTrainable params: 2,099,321\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\ninput_1 None\r\nmasking None\r\nbidirectional KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='masking/Squeeze:0')\r\naverage_pooling1d KerasTensor(type_spec=TensorSpec(shape=(None, 120), dtype=tf.bool, name=None), name='Placeholder_2:0')\r\nflatten None\r\ndense None\r\n```\r\nThis one I am a bit less confident about, but averaging on the last dimension doesn't change the `timesteps` dimension and yet the mask isn't propagated. I thought maybe the averaging could create \"0\" vectors that would be ignored afterwards but the mask is computed by Masking and is 120 long boolean here, no matter what. Is there a reason why AveragePooling1D is not passing the mask to the Flatten layer?\r\n\r\n\r\n", "comments": ["Sorry for the late reply.\r\n\r\nFor example 3, the result is expected (since you are printing layer.input_mask). For the last LSTM layer, its input has a shape of (None, 120, 512), which contains the timestep dim. The LSTM layer need to properly ignore those masked inputs. The output of the LSTM has no timestep dim, so the mask propagation stops there.\r\n\r\nFor example 4, the AveragePooling1D layer doesn't support mask at all (even its input is 3D). If you check the code, the call method doesn't take a kwarg mask, which indicate it ignores the masks. https://github.com/keras-team/keras/blob/a5a6a53eceb4bb0957fcbe577f56941ae8062d8f/keras/layers/pooling.py#L65\r\n\r\nI am not sure if this is bug or a feature. Also, propagating the mask to next layer seems problematic if the value of pooling/strides is not 1. ", "Closing as working as intended. Feel free to reopen this issue if there is any other items need to be addressed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49262\">No</a>\n"]}, {"number": 49261, "title": "How team has made an animated 3D block diagram on tensorflow website?", "body": "Hello Team,\r\n\r\nThis issue is not related to tensorflow package or any of its working. I am learning designing and I just wanted to know how google tensorflow team has made such a beautiful animated block diagram on the website [www.tensorflow.org](https://www.tensorflow.org/).\r\n\r\nI'm referring to this diagram:\r\n![Untitled](https://user-images.githubusercontent.com/7824529/118642791-6558f300-b7f9-11eb-984b-867b05ec6316.png)\r\n\r\nCan anyone suggest what software are needed and from where can I start. The motion light animation and building up of diagram looks pretty awesome the moment website opens.\r\n\r\nPlease let me know. Thank you.", "comments": ["@smartaquarius10 \r\nYou can start with [flutter](https://www.udemy.com/course/flutter-bootcamp-with-dart/?utm_source=adwords&utm_medium=udemyads&utm_campaign=GoogleFlutter_v.PROF_la.EN_cc.INDIA&utm_content=deal4584&utm_term=_._ag_113517924890_._ad_501402139574_._kw_flutter+tutorial_._de_c_._dm__._pl__._ti_kwd-432000815711_._li_1007768_._pd__._&matchtype=e&gclid=Cj0KCQjw7pKFBhDUARIsAFUoMDbSbmce9FoJKlIK0PaSPxzb3B2cFDzvuIyW5sDOXxZD3FFjmx8XGf0aArIFEALw_wcB).\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\nThanks!", "Sure. I wasn't aware of that. Really sorry. In future, will ask such questions there only. Thanks for helping :)", "@smartaquarius10 \r\nPlease move the issue to closed status as it is resolved.", "Sure. ", "> Sure.\r\n\r\nAlso,if you are keen on designing from scratch, you can always import an image in to illustrator, trace it then expand to get [svg](https://www.svgator.com) object", "@Saduf2019 , Hmm ok. But r u sure that we can make such images or block diagrams in flutter. I checked but didn\u2019t find anything useful. Do you know any software through which we can make such kind of static 3d block diagrams. "]}, {"number": 49260, "title": "Cannot link to built tensorflow.lib with Visual C++ -- undefined symbols; ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n-> Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n-> NA\r\n- TensorFlow installed from (source or binary):\r\n-> Source\r\n- TensorFlow version:\r\n-> r2.4.1 (also tried with master)\r\n- Python version:\r\n-> 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n-> No\r\n- Bazel version (if compiling from source):\r\n-> 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n-> Visual C++ 2019\r\n- CUDA/cuDNN version:\r\n-> 10.2 / 7.6.5\r\n- GPU model and memory:\r\n-> NVIDIA Quadro P400\r\n\r\n\r\n\r\n**Describe the problem**\r\n1) I build tensorflow.dll, tensorflow.lib and install_headers thanks to Bazel. The three \"Build completed successfully\" (see the three logs).\r\nWhen I try to link to tensorflow.lib I get unresolved symbols\r\n**error LNK2019: \"public: virtual __cdecl tensorflow::SavedModelBundleInterface::~SavedModelBundleInterface(void)\" (??1SavedModelBundleInterface@tensorflow@@UEAA@XZ) r\u00e9f\u00e9renc\u00e9 dans la fonction \"int public: __cdecl tensorflow::SavedModelBundleLite::SavedModelBundleLite(void)'::1'::dtor$0\" (?dtor$0@?0???0SavedModelBundleLite@tensorflow@@QEAA@XZ@4HA)\".**\r\n\r\n2) I had TF_EXPORT and #include \"tensorflow/core/platform/macros.h\" in loader.h, and then rebuild from src. I can compil but I get the following error when I run the .exe\r\n**The procedure entry point ??1SavedModelBundleInterface@tensorflow@@UEAA@XZ could not be located in the dynamic link library C:\\Users\\bob\\Documents\\TF\\test\\x64\\Debug\\TF_cpp.exe**\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1) python ./configure.py\r\n        \r\n        You have bazel 3.1.0 installed.\r\n        Please specify the location of python. [Default is C:\\Users\\bob\\AppData\\Local\\Programs\\Python\\Python38\\python.exe]:\r\n\r\n        Found possible Python library paths:\r\n        C:\\Users\\bob\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\r\n        Please input the desired Python library path to use. Default is [C:\\Users\\bob\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages]\r\n        \r\n        Do you wish to build TensorFlow with ROCm support? [y/N]: N\r\n        No ROCm support will be enabled for TensorFlow.\r\n        \r\n        Do you wish to build TensorFlow with CUDA support? [y/N]: N\r\n        No CUDA support will be enabled for TensorFlow.\r\n        \r\n        Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n        \r\n        Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\n        Eigen strong inline overridden.\r\n        \r\n        Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n        Not configuring the WORKSPACE for Android builds.\r\n        \r\n          Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. \r\n          See .bazelrc for more details.\r\n          --config=mkl # Build with MKL support.\r\n          --config=mkl_aarch64 # Build with oneDNN support for Aarch64.\r\n          --config=monolithic # Config for mostly static monolithic build.\r\n          --config=ngraph # Build with Intel nGraph support.\r\n          --config=numa # Build with NUMA support.\r\n          --config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.\r\n          --config=v2 # Build TensorFlow 2.x instead of 1.x.\r\n          Preconfigured Bazel build configs to DISABLE default on features:\r\n          --config=noaws # Disable AWS S3 filesystem support.\r\n          --config=nogcp # Disable GCP support.\r\n          --config=nohdfs # Disable HDFS support.\r\n          --config=nonccl # Disable NVIDIA NCCL support.\r\n\r\n2) bazel build --config=opt tensorflow:tensorflow.dll\r\n3) bazel build --config=opt tensorflow:tensorflow.lib\r\n4) bazel build --config=opt tensorflow:install_headers\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n[dll.log](https://github.com/tensorflow/tensorflow/files/6500648/dll.log)\r\n[headers.log](https://github.com/tensorflow/tensorflow/files/6500649/headers.log)\r\n[lib.log](https://github.com/tensorflow/tensorflow/files/6500650/lib.log)\r\n\r\n\r\n", "comments": ["@magondra \r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49260\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49260\">No</a>\n"]}, {"number": 49259, "title": "issu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Please fill the issue template.", "@whitehatedk \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49259\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49259\">No</a>\n"]}, {"number": 49258, "title": "[TFLite] Failed to create Hexagon delegate on REALME x50 Pro", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:realme x50 Pro\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI use this device on Hexagon delegate before and get expected results. However, after some time, maybe because of system security patch upgrade, I can never create delegate on this device. When I run benchmark on Heaxagon delegate, it is aborted. \r\n![image](https://user-images.githubusercontent.com/21187757/118629720-70fbe800-b800-11eb-95e7-521366ecb873.png)\r\nAnd when in Android App, it will  cause following log, which seems to fastrpc called failed:\r\n\r\n05-17 21:28:02.051   869   869 I com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/rpcmem_android.c:143: rpcmem_init_internal: opened ION device fd 77, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)\r\n05-17 21:28:02.051   869   869 I com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2540: fastrpc_apps_user_init done\r\n05-17 21:28:02.051   869   869 I com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1971: open_device_node: no access to default device of domain 3, open thru HAL\r\n05-17 21:28:02.052   644   644 E SELinux : avc:  denied  { find } for interface=vendor.qti.hardware.dsp::IDspService sid=u:r:untrusted_app:s0:c110,c257,c512,c768 pid=869 scontext=u:r:untrusted_app:s0:c110,c257,c512,c768 tcontext=u:object_r:vendor_hal_dspmanager_hwservice:s0 tclass=hwservice_manager permissive=0\r\n05-17 21:28:02.052   869   869 E dsp-client: DspClient.cpp (42): Error: DspClient: unable to acquire dspservice instance\r\n05-17 21:28:02.052   869   869 E dsp-client: DspClient.cpp (82): Error: openSession: IDspManager session is NULL\r\n05-17 21:28:02.052   869   869 E dsp-client: DspClient.cpp (131): Error: open_hal_session: failed to open session, error -1\r\n05-17 21:28:02.052   869   869 E com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1988: Error 0: open_device_node failed for domain 3 (errno Success)\r\n05-17 21:28:02.052   869   869 E com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2342: Error 0x57: apps_dev_init failed for domain 3, errno Success\r\n05-17 21:28:02.052   869   869 E com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2440: Error 0x57: open_dev (-1) failed for domain 3\r\n05-17 21:28:02.052   869   869 E com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1038: Error 0x57: remote_handle_open failed for hexagon_nn\r\n05-17 21:28:02.052   869   869 E com.example.energy: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:918: Error 0x2c: remote_handle_invoke failed for handle 0xffffffff, method 27 on domain 3 (sc 0x1b000100)\r\n\r\n\r\n**Describe the expected behavior**\r\ncreate Hexagon delegate successfully\r\n\r\n", "comments": ["@karimnosseir could you take a look?", "Hi,\r\n\r\nI think it's the SELinux polic \"SELinux : avc: denied { find }\"\r\nLooks like in some system update they added policy to disabled access to the DSP.\r\n\r\nIf your device is rooted can you run\r\nadb root\r\nadb set enforce 0\r\n\r\nthen retry\r\n\r\nThanks", "cc @jvishnuvardhan ", "thanks for your reply. Yes, I also suspected to be a problem with the SELinux policy. But my deivce has not been rooted, I can not try to disable SELinux.\r\n\r\nMore vendors may update security patches in the future, this may affect more devices that use Hexagon delegate.  What can we do to reduce this phenomenon?\r\n\r\nLooking forward to your reply\uff0c thanks.", "Qualcomm is working with manufacturers to resolve issues. So hopefully with future security patches they will lift the restriction.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49258\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49258\">No</a>\n"]}, {"number": 49256, "title": "Question VM UPGRADE Tensorflow", "body": "Hello\r\n\r\nQuick question, we are upgrading our systems from standalone to Virtual Machine environment Via VMware. \r\n\r\nSpecs is GPU Tesla T4 \r\n\r\nWe are unsure for Tensorflow what would work best however maybe you might know?\r\n\r\nwhat works best to move Tensor flow onto the VM enviroment ? ie what hardware specs. \r\n\r\n\r\nthanks\r\n\r\nCH", "comments": ["I don't think that we handle any specific request for VMware here but you could rely on third_party official info:\r\nhttps://docs.vmware.com/en/VMware-vSphere-Bitfusion/index.html\r\nhttps://docs.vmware.com/en/VMware-vSphere-Bitfusion/3.0/Example-Guide/GUID-1C053853-4D83-4D94-A6F3-D6958478AAB2.html", "@CHIMRLINUX \r\n\r\nCould please check the above comment and move this closed status,if it is resolved.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please close this activity.\r\n\r\nThanks\r\n\r\nCiaran\r\n\r\nFrom: google-ml-butler[bot] ***@***.***>\r\nSent: Friday 4 June 2021 12:01\r\nTo: tensorflow/tensorflow ***@***.***>\r\nCc: Ciaran Harte ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [tensorflow/tensorflow] Question VM UPGRADE Tensorflow (#49256)\r\n\r\n\r\nThis issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/49256#issuecomment-854618088>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AUDYRNVIMO5UF4HWFWF4B6TTRCXAFANCNFSM45CD6MGQ>.\r\nNOTE: This email originated from outside the organization. Do not click links or open attachments unless you recognize the sender and know the content is safe.\r\n"]}, {"number": 49255, "title": "Is  CUPTI error  due to tensorflow version or NVIDIA CUPTI libary API change ?", "body": "The problem i met is in this issue  https://github.com/tensorflow/tensorflow/issues/35860.  I just want to confirm what raises this probelm. Is this due to the tensorflow version or the new version of   NVIDIA CUPTI libary? In tensorflow version belowing 2.0, my program runs normally. ", "comments": ["@lyw615 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nAlso please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-585390769).Hope it helps.Thanks!", "> @tilakrayal  ,\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n> \r\n> Also please take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-585390769).Hope it helps.Thanks!\r\n\r\nThank you for your reply. I have read the comments you mentioned  before opening the issue.  I prepare to updating my project written by keras and tensorflow 1.x  with tensorflow 2.2.  The problem my project meet  is  describing in issue https://github.com/tensorflow/tensorflow/issues/35860  , and has been solved.   What confuses me  is the reason. From the comments your mentioned, it's  likely NVIDIA CUPTI libary API change.  So , is this due to the tensorflow higher version  or my local changed  version of NVIDIA CUPTI libary   API  ?", "@lyw615 ,\r\n\r\nPlease take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-585390769).Hope it helps.\r\n\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a TensorFlow bug or feature request. There is also a larger community that reads questions there. Also please feel free to move this issue to closed status if  issue is resolved.\r\n\r\nThanks!", "@lyw615 Its due to nvidia cupti api changes. See https://github.com/tensorflow/tensorflow/issues/35860#issuecomment-844217651\r\n", "@lyw615 ,\r\n\r\nPlease move the issue to closed status as it answers your question.Thanks!", "The problem solved by closing tensorboard. For any better resolution , please feel free to leave messages.", "@lyw615 ,\r\n\r\nGlad the issue is resolved for you, please move this to closed status.", "@tilakrayal This issue caused by change of nvidia cupti api . Has the newest version of tensorflow solved the problem?", "That's good point but the release notes for TF 2.5 don't mention any information regarding it. \r\nNote that the recent TF version (2.5) supports cuda 11.2 and cudnn 8.1 if you want to test it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49255\">No</a>\n"]}, {"number": 49254, "title": "Performance drop if I do some operation between predictions.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): conda\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nI am using CPU to predict, the issue occurs in both MKL(installed by conda, tested both default and TF_MKL_DISABLED=1) and non-MKL tensorflow(installed by pip).\r\n\r\nIf I run code like this\r\n```\r\n# warm up\r\nfor i in range(1000):\r\n    predict my data\r\n```\r\nthe time cost would be normal, but if change to \r\n```\r\n# warm up\r\nfor i in range(1000):\r\n    do some operation, let's denote it as op\r\n    predict my data\r\n```\r\nThe performance will drop, depending on op time cost, it would drop from about 10ms each batch to about 20ms when `TF_MKL_DISABLED=1`\r\n\r\nNormally, each batch cost about 10ms, the less cost op is, the less drop would be. If op is 1ms, predict cost would be 11ms, if op is 5ms, predict cost would be 13ms.\r\n\r\n**Describe the expected behavior**\r\nno matter what operation between the predicts, the performance would not drop.\r\n\r\n**Standalone code to reproduce the issue**\r\nI do not know whether this is a normal behavior. If code is needed, I would post a colab.\r\n\r\n", "comments": ["@Litchilitchy  Please provide the simple standalone code/ colab link to reproduce the issue at our end.\r\n\r\nAlso, Please upgrade Tensorflow version since 1.x is no more supported actively.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49253, "title": "dependency conflict between tensorflow and tensorflow-text", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 7.5.0-3ubuntu1~18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis command keeps trying to install different versions of various packages:\r\n\r\n```pip install --upgrade pip tensorflow tensorflow-text```\r\n\r\nIt makes the Flax CI builds fail on Github ([example](https://github.com/google/flax/runs/2605209071?check_suite_focus=true)). We temporary fixed this by using `tensorflow==2.4.1`: https://github.com/google/flax/pull/1327\r\n", "comments": ["This seems more like using a broken pip resolver that backtracks too much. TF uses the resolver before this.", "@marcvanzee \r\nIs this still an issue.", "Hi @marcvanzee ! I think it is not replicating in [TF 2.8 version ](https://colab.sandbox.google.com/gist/mohantym/a7180399bb7b9b0bf5648ee5e4091590/github_49253.ipynb). Attaching gist in [2.5](https://colab.sandbox.google.com/gist/mohantym/d80def0855f8b046ab4458c0b511a738/github_49253.ipynb#scrollTo=M-IhZeMsTpkp) too for reference too. Can you let us know from your side?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49253\">No</a>\n"]}, {"number": 49252, "title": "SparseCategoricalAccuracy y_true and y_pred shape", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy\r\n\r\n## Description of issue (what needs changing):\r\nIt seems that the usage example conflicts with documentation. I suppose that the usage example is correct https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/metrics.py#L3474-L3478\r\n\r\nIn usage example, `y_true` takes ground truth labels as input and `y_pred` takes logits/probabilities as input\r\n```\r\nm = tf.keras.metrics.SparseCategoricalAccuracy()\r\nm.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])\r\nm.result().numpy()\r\n```\r\n\r\nIn documentation, it states that:\r\n`y_true` and `y_pred` should have the same shape.\r\n\r\n\r\n<!--StartFragment-->\r\ny_true | Ground truth values. shape =&nbsp;[batch_size,&nbsp;d0,&nbsp;..&nbsp;dN].\r\n-- | --\r\ny_pred | The predicted values. shape =&nbsp;[batch_size,&nbsp;d0,&nbsp;..&nbsp;dN].\r\n\r\n<!--EndFragment-->\r\n\r\nThe documentation may need update.\r\n", "comments": ["@ziruizhuang \r\nCan you please let us know what has to be updated [please share more specific details].", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Fixed in https://github.com/keras-team/keras/commit/4244fba008d686f644b2ea3711e1897edf32432b."]}, {"number": 49251, "title": "[ROCm] Replacing csrMM with SpMM for ROCm 4.2+ ", "body": "Using the generalized SpMM function for Matrix Multiplication for ROCm>=4.2, aligning with the CUDA implementation.\r\nIn conjunction with https://github.com/tensorflow/tensorflow/pull/49305", "comments": ["@cheshire or @chsigg for review", "@penpornk for sparse kernels, but otherwise LG", "@penpornk bumping this thread. ", "@stevenireeves  Can you please resolve conflicts? Thanks!", "@gbaned conflict should be resolved. "]}, {"number": 49250, "title": "micro: port operator SPACE_TO_DEPTH from lite", "body": "For micro, implement the SPACE_TO_DEPTH kernel and test, and add both to the build.\r\n\r\nThis covers PR4 and 5 in the sequence of PRs, outlined in issue #45824, to port operator SPACE_TO_DEPTH from lite to micro.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "I see the following errors in our internal build (and some similar ones in the open TFLite Micro build above, without the unused function that only seems to be caught internally):\r\n\r\n```c++\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:112:24:   required from here\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:59:54: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* input_dims = IntArrayFromInts(args.input_dims);\r\n                                                 ~~~~~^~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:70:52: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* input_indexes = IntArrayFromInts(kInputIndexesData);\r\n                                                    ^~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:73:53: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* output_indexes = IntArrayFromInts(kOutputIndexesData);\r\n                                                     ^~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:84:55: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* expect_dims = IntArrayFromInts(args.expect_dims);\r\n                                                  ~~~~~^~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc: In instantiation of \u2018void {anonymous}::TestSpaceToDepth(const {anonymous}::SpaceToDepthTest<T>&) [with T = signed char]\u2019:\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:134:24:   required from here\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:59:54: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* input_dims = IntArrayFromInts(args.input_dims);\r\n                                                 ~~~~~^~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:70:52: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* input_indexes = IntArrayFromInts(kInputIndexesData);\r\n                                                    ^~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:73:53: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* output_indexes = IntArrayFromInts(kOutputIndexesData);\r\n                                                     ^~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:84:55: error: invalid conversion from \u2018const int*\u2019 to \u2018int*\u2019 [-fpermissive]\r\n   TfLiteIntArray* expect_dims = IntArrayFromInts(args.expect_dims);\r\n                                                  ~~~~~^~~~~~~~~~~\r\nIn file included from tensorflow/lite/micro/kernels/space_to_depth_test.cc:20:\r\n./tensorflow/lite/micro/test_helpers.h:164:39: note:   initializing argument 1 of \u2018TfLiteIntArray* tflite::testing::IntArrayFromInts(int*)\u2019\r\n TfLiteIntArray* IntArrayFromInts(int* int_array);\r\n                                  ~~~~~^~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:36:6: error: \u2018void {anonymous}::ExpectNear(const T*, const T*, int, float) [with T = signed char]\u2019 used but never defined [-Werror]\r\n void ExpectNear(const T a[], const T b[], int size, float tolerance = 1e-5) {\r\n      ^~~~~~~~~~\r\ntensorflow/lite/micro/kernels/space_to_depth_test.cc:36:6: error: \u2018void {anonymous}::ExpectNear(const T*, const T*, int, float) [with T = float]\u2019 used but never defined [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n```", "Looks like I got caught by a recent interface change in master 822ddbf. That's a good thing, because what I was doing\u2014passing a const array to `IntArrayFromInts`\u2014was undefined behavior, and the new interface prevents that. Fixed in 0781269.", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 49249, "title": "tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead. If you need this feature, please file a feature request at https://github.com/tensorflow/tensorflow/issues/new", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Can you fill the ISSUE template with all the info required? Thanks", "Duplicate issue : #36581 #31504\r\n\r\nCould you please fill the issue template.Also Please provide the simple standalone code/ colab link to reproduce the issue at our end.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49248, "title": "build failed with tensorrt 8.0", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.9.4\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source):  9.3.0\r\n- CUDA/cuDNN version: 11.3 / 8.2.0\r\n- GPU model and memory:\r\nrtx3060 GDDR6 6GB\r\n\r\n\r\n**Describe the problem**\r\nbazel build failed\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbuild with tensorrt support  ( tensorrt 8.0 )\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nERROR: /home/alan/repo/tensorflow/tensorflow/compiler/tf2tensorrt/BUILD:39:11: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/tensorrt_stub/nvinfer_stub.pic.d ... (remaining 141 argument(s) skipped)\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:5:7: error: declaration of \u2018void* createInferBuilder_INTERNAL(void*, int)\u2019 has a different exception specifier\r\n    5 | void* createInferBuilder_INTERNAL(void* logger, int version) {\r\n      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:8083:30: note: from previous declaration \u2018void* createInferBuilder_INTERNAL(void*, int32_t) noexcept\u2019\r\n 8083 | extern \"C\" TENSORRTAPI void* createInferBuilder_INTERNAL(void* logger, int32_t version) noexcept;\r\n      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:12:7: error: declaration of \u2018void* createInferRuntime_INTERNAL(void*, int)\u2019 has a different exception specifier\r\n   12 | void* createInferRuntime_INTERNAL(void* logger, int version) {\r\n      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,\r\n                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2253:30: note: from previous declaration \u2018void* createInferRuntime_INTERNAL(void*, int32_t) noexcept\u2019\r\n 2253 | extern \"C\" TENSORRTAPI void* createInferRuntime_INTERNAL(void* logger, int32_t version) noexcept;\r\n      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:\r\n./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:33:28: error: declaration of \u2018nvinfer1::IPluginRegistry* getPluginRegistry()\u2019 has a different exception specifier\r\n   33 | nvinfer1::IPluginRegistry* getPluginRegistry() {\r\n      |                            ^~~~~~~~~~~~~~~~~\r\nIn file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,\r\n                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:\r\nbazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration \u2018nvinfer1::IPluginRegistry* getPluginRegistry() noexcept\u2019\r\n 2264 | extern \"C\" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;\r\n      |                                                   ^~~~~~~~~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4508.840s, Critical Path: 130.04s\r\nINFO: 23345 processes: 10553 internal, 12792 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["@alanpurple \r\nCould you please check with the compatiblity mentioned below\r\n\r\nCompatibility:\r\nTensorRT 8.0.0 EA has been tested with the following:\r\ncuDNN 8.2.0\r\nTensorFlow 1.15.5\r\nPyTorch 1.8.0\r\nONNX 1.8.0\r\nThis TensorRT release supports CUDA:\r\n10.2\r\n11.0 \r\n11.1 \r\n11.2 \r\n[11.3(update)](https://developer.nvidia.com/cuda-toolkit-archive)\r\nfor more information please check [this](https://docs.nvidia.com/deeplearning/tensorrt/release-notes/tensorrt-8.html#tensorrt-8)\r\nThanks\r\n", "@UsharaniPagadala \r\n\r\nYou're telling TF2.5 does not support TensorRT 8.0? for now? forever?", "I think it is a dup https://github.com/tensorflow/tensorflow/issues/49150", "@alanpurple \r\nYou may track this [issue](https://github.com/tensorflow/tensorflow/issues/49150) to know the progress . To avoid duplication Could you please move this to closed status and also subscribe/ follow that as it is being tracking there.Thanks\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49248\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49248\">No</a>\n"]}, {"number": 49247, "title": "nnlib", "body": "Tflite offers _**libhexagon_nn_skel.so**_ to use hexagon delegate.I wonder is this library produced by nnlib?\r\nBecause when i used nnlib to get my own _**libhexagon_nn_skel.so**_  and applied it in tflite,something went wrong.\r\n\r\nThe logcat:\r\nvendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\nvendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1022: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (**_dlerror signature verify start failed for libhexagon_nn_skel_v66.so_**)\r\nvendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1059: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n\r\nI have signed my device and my own .so as Hexagon SDK said but still got the same wrong.So could you tell me what the tflite do to make your **_libhexagon_nn_skel_v66.so_** avaliable?Thanks  a lot!!!\r\n\r\n", "comments": ["@karimnosseir could you take a look at this?", "@1173710201 Hi,\r\nYes it is built using nnlib.\r\n\r\n1) Are you calling the init with correct path to your custom library \r\n```\r\nTfLiteHexagonInitWithPath('path/to/my/lib');\r\n```\r\nSee the [guide](https://www.tensorflow.org/lite/performance/hexagon_delegate)\r\n\r\n2) One other thing is which version are you using, if you're building your own version you want to make sure it matches what libhexagon_interface expects.\r\n\r\nIf you can send sample on how you init, run and push libraries with the logcat, i can check if ther is an issue.\r\n\r\nThanks", "@karimnosseir Thanks for your reply!\r\n1.I used the default **_TfLiteHexagonInit()_** before, and i pushed my **_.so_** to **_/system/vendor/lib/rfsa/adsp_** or the path got by _**getApplicationInfo().nativeLibraryDir**_. The part of logcat:\r\n```\r\nvendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0xd for oemconfig.so (Permission denied)\r\n2021-05-19 19:13:37.977 16298-16402/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:64::error: -1: AEE_SUCCESS == (nErr = mod_table_close(handle, errStr, errStrLen, dlErr))\r\n2021-05-19 19:13:37.977 16298-16402/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\n2021-05-19 19:13:37.980 16298-16402/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\n2021-05-19 19:13:37.980 16298-16402/org.tensorflow.lite.examples.classification I/chatty: uid=10273(org.tensorflow.lite.examples.classification) identical 2 lines\r\n2021-05-19 19:13:37.981 16298-16402/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\n2021-05-19 19:13:37.981 16298-16402/org.tensorflow.lite.examples.classification W/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0xd for testsig.so (Permission denied)\r\n```\r\n\r\nAs the logcat show,i suppose it worked? I used TfLiteHexagonInitWithPath(\u201c/system/vendor/lib/rfsa/adsp\u201d) instead and nothing changed.\r\n\r\n2. yes,i do have this problem.For now,i deleted the return sentence in the VerifyDelegate() of **_/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc_** to let the program continue to run.But there're still some bugs:\r\n```\r\n2021-05-19 19:13:37.982 16298-16298/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1022: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open oemconfig.so, errno 2 (no such file\r\n2021-05-19 19:13:37.982 16298-16298/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1059: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n2021-05-19 19:13:37.982 16298-16298/org.tensorflow.lite.examples.classification W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n2021-05-19 19:13:37.982 16298-16298/org.tensorflow.lite.examples.classification W/tflite: Incompatible versions between interface library and libhexagon_skel 137729 vs -1. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n```\r\nI have seen [guide](https://www.tensorflow.org/lite/performance/hexagon_delegate),and i'm still a little coufused.i see the tensorflow/third_party/hexagon/workspace.bzl:\r\n```\r\ndef repo():\r\n    tf_http_archive(\r\n        name = \"hexagon_nn\",\r\n        sha256 = \"ade87358f2231f4d079449cbbc047c17db5a4d5cedbc230b2795adc1116e2fa5\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_headers_v1.20.0.2.tgz\",\r\n            # Repeated to bypass 'at least two urls' check. TODO(karimnosseir): add original source of this package.\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_headers_v1.20.0.2.tgz\",\r\n        ],\r\n        build_file = \"//third_party/hexagon:BUILD\",\r\n    )\r\n```\r\ni'm not sure how to make libhexagon_nn_skel_v66.so match libhexagon_interface,since tensorflow-lite-hexagon.aar is build locally too.\r\n\r\n3.Could you tell me the version of hexagon SDK you used to build nnlib?For me it is 3.5.2.After creating hexagon delegate,the logcat shows as follows:\r\n```\r\n2021-05-19 19:13:37.986 16298-16298/org.tensorflow.lite.examples.classification I/tflite: TfLiteHexagonDelegate delegate: 3 nodes delegated out of 125 nodes with 1 partitions.\r\n2021-05-19 19:13:37.986 16298-16402/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\n2021-05-19 19:13:37.987 16298-16298/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1022: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror signature verify start failed for libhexagon_nn_skel_v66.so)\r\n2021-05-19 19:13:37.987 16298-16298/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1059: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n2021-05-19 19:13:37.989 16298-16298/org.tensorflow.lite.examples.classification E/tensorflow: ClassifierActivity: Failed to create classifier.\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: hexagon_nn_config failed. Error: -1\r\n    Delegate kernel was not initialized\r\n    Node number 125 (TfLiteHexagonDelegate) failed to prepare.\r\n```\r\nIt seems the delegate had successfully replaced nodes but failed to execute and report signature error.\r\nThanks for your answer again!\r\n", "I am not sure about your setup, so to be able to know what's in your setup.\r\nCan you try these steps using the [benchmark tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark)\r\n\r\nBuild the tool:\r\nbazel build -c opt  --config=android_arm64  tensorflow/lite/tools/benchmark:benchmark_model\r\n\r\nPush the tool:\r\nadb push bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp\r\n\r\nPush the model:\r\nadb push path/to/model /data/local/tmp/myModel.tflite\r\n\r\nPush the libraries (both libhexagon_interface and your custom libhexagon_nn_sekl):\r\nadb push path/to/libraries /data/local/tmp/\r\n\r\nRun the benchmark:\r\nadb shell \"export ADSP_LIBRARY_PATH=\\\"/data/local/tmp;/system/lib/rfsa/adsp;/system/vendor/lib/rfsa/adsp;/dsp\\\" && LD_LIBRARY_PATH=/data/local/tmp  /data/local/tmp/benchmark_model --graph=/data/local/tmp/myModel.tflite --num_threads=4 --use_hexagon=true\"\r\n\r\n\r\nPlease let me know what you get, if it didn't work please capture the logcat and share it.\r\n\r\nThanks", "@karimnosseir Thanks for your answer! The logcat:\r\n```\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [/data/local/tmp/inception_v3_quant.tflite]\r\n#threads used for CPU inference: [4]\r\nUse Hexagon: [1]\r\nLoaded model /data/local/tmp/inception_v3_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libcdsprpc.so\r\nWARNING: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\nINFO: Hexagon Delegate is not supported.\r\n\r\nCould not create Hexagon delegate: platform may not support delegate or required libraries are missing\r\nThe input model file size (MB): 23.9429\r\nInitialized session in 4.78ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=12 first=70426 curr=35925 min=35925 max=70426 avg=41825.3 std=9528\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=36240 curr=36011 min=35905 max=37085 avg=36156.4 std=210\r\n\r\nInference timings in us: Init: 4780, First inference: 70426, Warmup (avg): 41825.3, Inference (avg): 36156.4\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=7.72656 overall=20.2188\r\n```\r\nIn android app(like image_classification),the incompatible version error occurs when i use custom libhexagon_nn_sekl and custom  libhexagon_interface.While i use  libhexagon_nn_skel provided by tflite and custom  libhexagon_interface,this error will not happen.\r\nAs the logcat shows,no matter whar version of libhexagon_interface and libhexagon_nn_skel i used,the result was the same.This is weird.\r\nThank you in advance again!", "Since you want to build your own. Remove this [check](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc#L121-L125)\r\nand retry\r\n\r\nThanks\r\n", "@karimnosseir Thanks,this solves one porblem.Now i can create Hexagon Delegate successfully.But when it comes to the actual execution,there's still a problem.The logcat:\r\n```\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/rpcmem_android.c:143: rpcmem_init_internal: opened ION device fd 90, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2531: fastrpc_apps_user_init done\r\norg.tensorflow.lite.examples.classification I/dsp_delegate: ---create---done\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/rpcmem_android.c:143: rpcmem_init_internal: opened ION device fd 93, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2531: fastrpc_apps_user_init done\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2120: Successfully opened fastrpc_shell_3\r\norg.tensorflow.lite.examples.classification E/ion: ioctl c0044901 failed with code -1: Inappropriate ioctl for device\r\norg.tensorflow.lite.examples.classification I/.classification: type=1400 audit(0.0:17972): avc: denied { search } for name=\"/\" dev=\"sde49\" ino=2 scontext=u:r:untrusted_app_27:s0:c17,c257,c512,c768 tcontext=u:object_r:adsprpcd_file:s0 tclass=dir permissive=1 app=org.tensorflow.lite.examples.classification\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2307: Successfully created user PD on domain 3 (attrs 0x0, debug_trace 0x0)\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_perf.c:232: fastrpc_perf_init: enabled RPC traces (kernel 0, dsp 0) with frequency 1000\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/log_config.c:319: file_watcher_thread starting for domain 3\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_latency.c:92: FastRPC latency thread started for QoS\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:119: listener thread starting\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1036: remote_handle_open: Successfully opened handle 0xd940d450 for adsp_current_process on domain 3\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/mod_table.c:677: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xbc27ad08 (idx 0)\r\norg.tensorflow.lite.examples.classification W/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0x2 for oemconfig.so (No such file or directory)\r\norg.tensorflow.lite.examples.classification I/tflite: TfLiteHexagonDelegate delegate: 3 nodes delegated out of 125 nodes with 1 partitions.\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\norg.tensorflow.lite.examples.classification I/chatty: uid=10273(org.tensorflow.lite.examples.classification) identical 2 lines\r\norg.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\norg.tensorflow.lite.examples.classification W/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0x2 for testsig.so (No such file or directory)\r\norg.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1022: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror signature verify start failed for libhexagon_nn_skel_v66.so)\r\norg.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1059: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\norg.tensorflow.lite.examples.classification E/tensorflow: ClassifierActivity: Failed to create classifier.\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: hexagon_nn_config failed. Error: -1\r\n    Delegate kernel was not initialized\r\n    Node number 125 (TfLiteHexagonDelegate) failed to prepare.\r\n```\r\nAs the last few lines show,the signature error occurs when the hexagon delegate wants to call the Dsp.In fact I'm not surprised this error happened,because **_qualcomm_** (and Hexagon SDK 's specification) says we can only call the Dsp using _**unsigned PD**_  since ordinary phone's **_secure boot_** is abled.\r\nBut why the **_libhexagon_nn_skel_v66.so_**  provided by tflite could work fine on my phone while my own _**.so**_  couldn't?i don't see any way to set dsp using _**unsigned PD**_ in tflite.This baffles me.\r\nCould you untangle my confusion?That will help a lot.Thanks!", "@1173710201 \r\nThe libraries provided by TFLite are signed libraries, so they are trusted and can be used on non-rooted devices. \r\nIf you want to use your own then you will need to have a rooted phone.", "@karimnosseir could you tell me how you signed the libraries? As far as I know,we need hexagon SDK.But for me, this way didn't work.And my phone is rooted already.My whole operation process is as follows\uff1a\r\n1.Get nnlib from:https://source.codeaurora.org/quic/hexagon_nn/nnlib \r\n2.Use hexagon SDK to build nnlib:make tree VERBOSE=1 V=hexagon_Release_dynamic_toolv83_v66 V66=1\r\n3.Use hexagon SDK to sign libhexagon_nn_skel.so provided by nnlib:./signer.py sign -i libhexagon_nn_skel.so -o out_dir", "@1173710201 \r\nFor production trusted use, QC is the one that sign the libraries, it's not something we do.\r\n\r\nIf your device is rooted, then can you run these commands before running the benchmark tool \r\n```\r\nadb root\r\nadb shell setenforce 0\r\n```\r\n", "```\r\nxmy@xmy:~$ adb root\r\nadbd is already running as root\r\nxmy@xmy:~$ adb shell setenforce 0\r\nxmy@xmy:~$ adb shell getenforce\r\nPermissive\r\n```\r\nAnd i think the SELinux is permissive too.\r\n```\r\nxmy@xmy:~$  adb shell getprop ro.boot.secureboot\r\n1\r\n```\r\nBut the secure boot is abled.\r\nAnyway,thanks for your help again.At least i know the reason now.\r\ni\u2018ll try to use unsigned PD in tflite to solve my problem.\r\nThanks!", "@1173710201 Did you run after running these commands ? Can you share the results ?", "The benchmark:\r\n```\r\nxmy@xmy:~/tensorflow$ adb shell\r\nception_v3_quant.tflite --num_threads=4 --use_hexagon=true                    <\r\nSTARTING!\r\nLog parameter values verbosely: [0]\r\nNum threads: [4]\r\nGraph: [/data/local/tmp/inception_v3_quant.tflite]\r\n#threads used for CPU inference: [4]\r\nUse Hexagon: [1]\r\nLoaded model /data/local/tmp/inception_v3_quant.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nloaded libcdsprpc.so\r\nINFO: TfLiteHexagonDelegate delegate: 125 nodes delegated out of 125 nodes with 1 partitions.\r\n\r\nERROR: hexagon_nn_config failed. Error: -1\r\nERROR: Delegate kernel was not initialized\r\nERROR: Node number 125 (TfLiteHexagonDelegate) failed to prepare.\r\n\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply Hexagon delegate.\r\nBenchmarking failed.\r\n1|umi:/ # \r\n```\r\nThe android studio:\r\n```\r\n2021-05-26 10:35:38.971 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/rpcmem_android.c:143: rpcmem_init_internal: opened ION device fd 90, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)\r\n2021-05-26 10:35:38.971 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2531: fastrpc_apps_user_init done\r\n2021-05-26 10:35:38.971 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/rpcmem_android.c:143: rpcmem_init_internal: opened ION device fd 93, configured heap IDs: system (0x2000000), contig (0x400000), secure (0x200), secure flags (0x80080000)\r\n2021-05-26 10:35:38.971 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2531: fastrpc_apps_user_init done\r\n2021-05-26 10:35:38.971 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2120: Successfully opened fastrpc_shell_3\r\n2021-05-26 10:35:38.972 13129-13129/org.tensorflow.lite.examples.classification E/ion: ioctl c0044901 failed with code -1: Inappropriate ioctl for device\r\n2021-05-26 10:35:38.987 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:2307: Successfully created user PD on domain 3 (attrs 0x0, debug_trace 0x0)\r\n2021-05-26 10:35:38.987 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_perf.c:232: fastrpc_perf_init: enabled RPC traces (kernel 0, dsp 0) with frequency 1000\r\n2021-05-26 10:35:38.987 13129-13326/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/log_config.c:319: file_watcher_thread starting for domain 3\r\n2021-05-26 10:35:38.987 13129-13327/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_latency.c:92: FastRPC latency thread started for QoS\r\n2021-05-26 10:35:38.988 13129-13325/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/listener_android.c:119: listener thread starting\r\n2021-05-26 10:35:38.988 13129-13129/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1036: remote_handle_open: Successfully opened handle 0xf220d450 for adsp_current_process on domain 3\r\n2021-05-26 10:35:38.988 13129-13325/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/mod_table.c:677: open_mod_table_open_from_static: reverse module apps_std opened with handle 0xc18fad08 (idx 0)\r\n2021-05-26 10:35:38.989 13129-13325/org.tensorflow.lite.examples.classification W/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0x2 for oemconfig.so (No such file or directory)\r\n2021-05-26 10:35:38.992 13129-13129/org.tensorflow.lite.examples.classification I/tflite: TfLiteHexagonDelegate delegate: 3 nodes delegated out of 125 nodes with 1 partitions.\r\n2021-05-26 10:35:38.992 13129-13325/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\n2021-05-26 10:35:38.993 13129-13325/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\n2021-05-26 10:35:38.993 13129-13325/org.tensorflow.lite.examples.classification I/chatty: uid=10273(org.tensorflow.lite.examples.classification) identical 2 lines\r\n2021-05-26 10:35:38.994 13129-13325/org.tensorflow.lite.examples.classification I/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:735: Successfully opened file /system/vendor/lib/rfsa/adsp/testsig-0x38816bf3.so\r\n2021-05-26 10:35:38.994 13129-13325/org.tensorflow.lite.examples.classification W/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:744: Warning: apps_std_fopen_with_env failed with 0x2 for testsig.so (No such file or directory)\r\n2021-05-26 10:35:38.995 13129-13129/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1022: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror signature verify start failed for libhexagon_nn_skel_v66.so)\r\n2021-05-26 10:35:38.995 13129-13129/org.tensorflow.lite.examples.classification E/org.tensorflow.lite.examples.classification: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1059: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp\r\n2021-05-26 10:35:38.995 13129-13129/org.tensorflow.lite.examples.classification I/subgraph: ---node_subset.type---NonPartition\r\n2021-05-26 10:35:38.996 13129-13129/org.tensorflow.lite.examples.classification E/tensorflow: ClassifierActivity: Failed to create classifier.\r\n    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: hexagon_nn_config failed. Error: -1\r\n    Delegate kernel was not initialized\r\n    Node number 125 (TfLiteHexagonDelegate) failed to prepare.\r\n    \r\n    Restored original execution plan after delegate application failure.\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:486)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:88)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:66)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:301)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.<init>(Classifier.java:288)\r\n        at org.tensorflow.lite.examples.classification.tflite.ClassifierQuantizedEfficientNet.<init>(ClassifierQuantizedEfficientNet.java:46)\r\n        at org.tensorflow.lite.examples.classification.tflite.Classifier.create(Classifier.java:133)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.recreateClassifier(ClassifierActivity.java:153)\r\n        at org.tensorflow.lite.examples.classification.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:67)\r\n        at org.tensorflow.lite.examples.classification.CameraActivity$8.onPreviewSizeChosen(CameraActivity.java:502)\r\n        at org.tensorflow.lite.examples.classification.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:363)\r\n        at org.tensorflow.lite.examples.classification.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:368)\r\n        at org.tensorflow.lite.examples.classification.CameraConnectionFragment.access$000(CameraConnectionFragment.java:71)\r\n        at org.tensorflow.lite.examples.classification.CameraConnectionFragment$2.onSurfaceTextureAvailable(CameraConnectionFragment.java:141)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:402)\r\n        at android.view.TextureView.draw(TextureView.java:351)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21395)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21386)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.draw(View.java:22530)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21395)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.draw(View.java:22530)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21395)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1246)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.draw(View.java:22530)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21395)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21386)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21386)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21386)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n2021-05-26 10:35:38.997 13129-13129/org.tensorflow.lite.examples.classification E/tensorflow:     at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21386)\r\n        at android.view.View.draw(View.java:22255)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4540)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4299)\r\n        at android.view.View.draw(View.java:22530)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:819)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21395)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:642)\r\n        at android.view.ViewRootImpl.draw(ViewRootImpl.java:4197)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:3911)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3182)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2037)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8329)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1058)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:880)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:813)\r\n        at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1043)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:236)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7887)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:656)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:967)\r\n```\r\nThe result is the same.i'm sure i solved root and selinux before.", "Closing. You can use the libraries that are available on TF website for now.\r\n\r\nThanks, let us know if you have questions.\r\n\r\n", "Hi @karimnosseir, I want to follow up the question of applying custom libhexagon_nn_skel.so in hexagon delegate.\r\n\r\nI\u2019m currently running TFLite Model Benchmark Tool with hexagon delegate on my rooted Pixel 4 (Android 12, SELinux in permissive mode and disabled secure boot). I have successfully set up the hexagon delegate with libhexagon_interface.so and libhexagon_nn_skel.so both provided by TFLite:\r\n\r\n```\r\n2022-01-31 14:17:18.164 8092-8093/? I/benchmark_model: vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:870: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\n2022-01-31 14:17:18.195 8092-8092/? I/benchmark_model: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1343: remote_handle64_open: Successfully opened handle 0x416d790 for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3\r\n2022-01-31 14:17:18.197 8092-8092/? I/tflite: TfLiteHexagonDelegate delegate: 31 nodes delegated out of 31 nodes with 1 partitions.\r\n2022-01-31 14:17:18.197 8092-8092/? I/tflite: Replacing 31 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.\r\n```\r\n\r\n\r\nNext, I built my own libhexagon_nn_skel.so with version 0x00021A01 (the same as specified in [libhexagon_interface.so](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_nn/hexagon_nn_init.cc#L29)) with hexagon SDK 3.5.4. However, I encountered the following error when using the lib built by myself:\r\n\r\n\r\n```\r\n2022-01-31 14:45:45.049 9032-9073/com.example.mobileml I/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:870: Successfully opened file /system/vendor/lib/rfsa/adsp/libhexagon_nn_skel_v66.so\r\n2022-01-31 14:45:45.056 9032-9032/com.example.mobileml E/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1290: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open oemconfig.so, errno 2 (no such file\r\n2022-01-31 14:45:45.056 9032-9032/com.example.mobileml E/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1340: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp (errno Success)\r\n2022-01-31 14:45:45.056 9032-9032/com.example.mobileml W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.\r\n2022-01-31 14:45:45.056 9032-9032/com.example.mobileml I/tflite: Hexagon Delegate is not supported.\r\n```\r\n\r\nAfter stepping into the source code, I found the return value of [hexagon_nn_version](https://github.com/tensorflow/tensorflow/blob/244b9d77fd42003042968a22d0cda6bea0c01435/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc#L87) is -1, essentially caused by the error of [remote_handle64_open](https://github.com/tensorflow/tensorflow/blob/244b9d77fd42003042968a22d0cda6bea0c01435/tensorflow/lite/delegates/hexagon/hexagon_nn/adsprpc_interface.cc#L140) (as indicated in the log). \r\n\r\nI already attempted to sign the lib with `python elfsigner.py -i libhexagon_nn_skel.so`, and I found the signed lib is in the same size of libhexagon_nn_skel_v66.so provided by TFLite (as I haven't changed any code in nnlib yet). \r\n\r\nCould you please provide any insight about why this function failed only for libhexagon_nn_skel.so built by myself? Is it because the signature I used is incorrect? Thanks for your help!", "Hi @165749 Are you sure you are overriding the correct environment variables to be able to use your library ?\r\n\r\nThis [init](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_delegate.h#L109) for the delegate can take the path to use", "@karimnosseir Thanks for your reply! What I am doing is to replace/add libhexagon_nn_skel(_v66).so in path /vendor/lib/rfsa/adsp, which has already been [added](https://github.com/tensorflow/tensorflow/blob/244b9d77fd42003042968a22d0cda6bea0c01435/tensorflow/lite/delegates/hexagon/hexagon_delegate.cc#L149) to the environment variable ADSP_LIBRARY_PATH. The hexagon delegate was initialized successfully when I added the libs provided by TFLite, but failed for the lib built by myself.\r\n\r\nI also tried to specify [hexagon_lib_path](https://github.com/tensorflow/tensorflow/blob/244b9d77fd42003042968a22d0cda6bea0c01435/tensorflow/lite/tools/delegates/hexagon_delegate_provider.cc#L33) in TFLite Model Benchmark Tool as path /vendor/lib/rfsa/adsp, so that the value can be passed to the API [TfLiteHexagonInitWithPath](https://github.com/tensorflow/tensorflow/blob/e43611f6b4ef0c56ddafbd1677e60ddaacde5ebb/tensorflow/lite/tools/evaluation/utils.cc#L150) you pointed out. However, the error remains the same.\r\n\r\nIs it appropriate to ask how TFLite generates libhexagon_nn_skel.so? I\u2019m still wondering if the error is because my lib is not generated or signed correctly.", "libhexagon_nn_skel*.so libraries are provided and signed by Qualcomm.\r\nBut you can build them locally and use them, assuming you are using compatible versions of the dependencies.\r\n\r\nOne way is to build both libraries using your version of the sdk, since the code of libhexagon_interface is now available on [github](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/hexagon/hexagon_nn).\r\nYou will need to update this [build](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/hexagon/hexagon_nn/BUILD) file and replace all deps with \"@hexagon_nn//\" prefix with ones that uses your files.\r\n", "@karimnosseir Thank you very much! I already noticed that the source code of libhexagon_interface.so was available, and I was actually using libhexagon_interface.so built from TFLite (v2.8.0-rc1) in my previous settings. \r\n\r\nAs for your suggestion, instead of rewriting all deps with \u201c@hexagon_nn//\u201c, I replaced [hexagon_nn_headers.tgz](https://github.com/tensorflow/tensorflow/blob/418428f119753d761154ea64abd151113e07bd3b/third_party/hexagon/workspace.bzl#L12) with the one built by myself (i.e., with files from Hexagon SDK 3.5.4) to make sure the interface is compatible with my libhexagon_nn_skel*.so. However, the error of remote_handle64_open still existed. After stepping into the source code, I found the error appeared in CHECK_DOMAINS_AND_OPEN_HANDLE (which called [remote_handle64_open](https://github.com/tensorflow/tensorflow/blob/244b9d77fd42003042968a22d0cda6bea0c01435/tensorflow/lite/delegates/hexagon/hexagon_nn/adsprpc_interface.cc#L140) and triggered the error) before the actual stub function was invoked.\r\n\r\n\r\nOccasionally, I realized that after rebooting the device, the error for the first time was about signature :\r\n```\r\nE/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1290: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror signature verify start failed for libhexagon_nn_skel_v66.so) (errno \r\n    vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1340: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp (errno Success)\r\n```\r\n\r\n, but it changed to oemconfig.so for later runs:\r\n\r\n```\r\n2022-02-07 14:20:07.161 8879-8879/com.example.mobileml E/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1290: Error 0x80000406: remote_handle_open_domain: dynamic loading failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp on domain 3 (dlerror _rtld_map_object_ex: cannot open oemconfig.so, errno 2 (no such file\r\n2022-02-07 14:20:07.161 8879-8879/com.example.mobileml E/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/fastrpc_apps_user.c:1340: Error 0x80000406: remote_handle64_open failed for file:///libhexagon_nn_skel_v66.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=cdsp (errno Success)\r\n```\r\n\r\nHowever, I already installed TestSig generated by Hexagon SDK on my Pixel 4, and I also tried to sign the libhexagon_nn_skel*.so with `python elfsigner.py -I libhexagon_nn_skel.so` from Hexagon SDK. Does it mean that it only allows OEM signature for libhexagon_nn_skel*.so (as the ones provided by TFLite I guess)?", "You will need to enable root access and disable SELinux, then follow QC test signature and make sure you push it to the correct directory (the information should be available in QC SDK files) - IIRC was \"/system/lib/rfsa/adsp/\"\r\n\r\nIf you want to run on production phone, then your signature is not enough.\r\n", "Thanks for your clarification. I already disabled SELinux and secure boot on my device, pushed test signature into /vendor/lib/rfsa/adsp, and saw the following log (with serial number modified here):\r\n\r\n`I/com.example.mobileml: vendor/qcom/proprietary/adsprpc/src/apps_std_imp.c:870: Successfully opened file /vendor/lib/rfsa/adsp/testsig-0xabcdefg.so\r\n`\r\n\r\nFrom Hexagon SDK document, I found:\r\n\r\n> Independent developers cannot easily sign modules for loading into signed PDs on **production devices**. Doing so requires working with system integrators to sign the modules or install the appropriate credentials on devices at build time. Developers can however install test signatures on **test devices** to facilitate testing code in signed PDs.\r\n\r\nIf I understand correctly, there is no simple way for independent developers to sign libhexagon_nn_skel*.so on production phones (even if the device is rooted)... Do you have any final suggestions about it?"]}, {"number": 49246, "title": "model.fit() with TensorFlow callback set to log each batch causes error on the second model.fit() call", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1 (edit: confirmed also with TF 2.5.0)\r\n- Python version: 3.8.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.0/8.0.4.30\r\n- GPU model and memory: GTX 1080 Ti 11 GB\r\n\r\n**Describe the current behavior**\r\nWhile training a model with TensorBoard callback set to log each batch, another `model.fit()` call fails with error:\r\n```\r\n2021-05-18 03:23:42.686008: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at summary_kernels.cc:210 : Not found: Resource localhost/_AnonymousVar6/class tensorflow::SummaryWriterInterface does not exist.\r\nTraceback (most recent call last):\r\n  File \"error.py\", line 11, in <module>\r\n    model.fit([1, 2, 3, 4, 5])\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 855, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found:  Resource localhost/_AnonymousVar6/class tensorflow::SummaryWriterInterface does not exist.\r\n         [[{{node cond/then/_0/batch_loss}}]]\r\n  (1) Not found:  Resource localhost/_AnonymousVar6/class tensorflow::SummaryWriterInterface does not exist.\r\n         [[{{node cond/then/_0/batch_loss}}]]\r\n         [[GroupCrossDeviceControlEdges_0/Identity_1/_15]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_208]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe second `model.fit()` call should not yield this error\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): no (I do not know the cause currently to think about a fix)\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```py\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Input(shape=(1,)))\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq='batch')])\r\nmodel.fit([1, 2, 3, 4, 5])\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nError message included above", "comments": ["After further investigation, what triggers this issue is either:\r\n```py\r\n        self._pop_writer()\r\n```\r\nin the `on_train_end()` method of the `TensorBoard` callback, or:\r\n```py\r\n        self._close_writers()\r\n```\r\nin the same method.\r\n\r\nWhat possibly helps here is to call:\r\n```py\r\nmodel._reset_compile_cache()\r\n```\r\nbetween first and second `model.fit()` call, like:\r\n```py\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Input(shape=(1,)))\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq='batch')])\r\nmodel._reset_compile_cache()\r\nmodel.fit([1, 2, 3, 4, 5])\r\n```\r\n", "@daniel-kukiela  I can able to reproduce the issue when calling model.fit twice() and after adding model._reset_compile_cache(),  I am not facing any issues as you mentioned above. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/f14ae4401b461238e9edee727dba2c07/untitled.ipynb).\r\n\r\nLooks like the issue is resolved already.  Please let us know if you have  any further questions?\r\n\r\n\r\n\r\n", "For me, what I found is a workaround for a bug, not a solution. I would not call it a solution. `_reset_compile_cache` is not documented method and I doubt that it is meant/designed to be used outside of the class (because of leading `_` in its name) and that this way is or will be a supported way of dealing with this issue.", "You need to recompile your model if you want to run on the same model again but without callbacks:\r\n```python\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Input(shape=(1,)))\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq='batch')])\r\nmodel.compile()\r\nmodel.fit([1, 2, 3, 4, 5])\r\n```", "It does not matter if we do this with or without callbalcks. This:\r\n```py\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Input(shape=(1,)))\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq='batch')])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq='batch')])\r\n```\r\nyields the same error, but:\r\n```py\r\nimport tensorflow.keras as keras\r\n\r\nmodel = keras.Sequential()\r\nmodel.add(keras.layers.Input(shape=(1,)))\r\n\r\nmodel.compile()\r\nmodel.summary()\r\n\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\n```\r\ndoes not (list of callbacks doers not matter, only `update_freq='batch'` in the TB callback) - it works.\r\n\r\nIs `.compile()` really necessary or is it yet another way of workaround? I specifically tracked it further to find out that some sort of cache causes this issue and it does not seem to be necessary to compile the model again. Additionally we'll have to keep an optimizer's instance aside to be able to use it with subsequent `.compile()` calls. Does compile affect anything else?\r\nAdditionally, if we remove `update_freq='batch'` from the TensorBoard callback, everything works without model re-compilation (no matter if I change a list of callbacks or anything else).\r\n\r\nBy taking all of the above into account, I still think that this is some sort of bug, possibly introduced while batch-level logging was moved from the TensorBoard class to the Model class (for some sort of speed improvement of small models).\r\n", "In this 2nd case it will work cause I don't think they are executed.\r\nI don't know if it needs to works with differents callbacks without re-compile.\r\n\r\nIt seems that without recompile it is going to have some conflict on execution\r\n\r\nThis is broken\r\n```\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\n```\r\n\r\nThis is not cause it is not executed:\r\n```\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=2)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\n```", "I'm using other callbacks as well and everything works perfectly fine without re-compilation with another `.fit()` call - all of the callbacks like checkpoints or custom preview callback).\r\n\r\nI'm almost sure that (from your examples) this:\r\n```py\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\n```\r\nis exactly the same what I originally posted (`update_freq=1` and `update_freq='batch'` are exactly same setting) and this:\r\n```py\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=2)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard()])\r\n```\r\nThis actually writes 2 log files but in both cases, there is only one batch executed with `.fit()` (maybe this is what makes it different).", "What I suppose with the TensorBoard callback is that if you change the freq with the second call then when it is executed the first one something it is not ready anymore (e.g. the summary writer node?).\r\n\r\n```python\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=2)])\r\n```\r\n\r\n```python\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=2)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\n```", "Well, it is the same even if you do not change the frequency:\r\n```py\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\n```\r\n\r\nAs far as I tested by subclassing TB class, it's related to writer objects or references which are probably cached with the first `.fit()` call, but as writers are then removed at the training end, whatever is cached is no longer valid.\r\n\r\nAlso, I do not see a reason to call `.compile()` in this case, as this is a TesorBoard callback only which is problematic and not even training itself. Training works ok without `.compile()` call between `.fit()` calls as well as other callbacks. Re-assigning everything like an optimizer, loss, etc feels like makes no sense just for a TensorBoard callback, and adds complexity and a need to make sure that, for example, we are not using a fresh optimizer instance with a second `.fit()` call (for, I hope, obvious reasons).\r\n\r\nWhat seems to be helping here is `model._reset_compile_cache()` (which is also called when you invoke `model.compile()`, that's why it helps the same way here) which suggests that something is being kept cached incorrectly or redundantly.", "Is that the `training_function` is not recreated:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/66852f05e5fec32ea8f3f7c2346f86c7e3e4fcf5/tensorflow/python/keras/engine/training.py#L836-L837\r\n\r\nIf you bypass this if\r\n\r\n```python\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\nmodel.train_function=None\r\nmodel.fit([1, 2, 3, 4, 5], callbacks=[keras.callbacks.TensorBoard(update_freq=1)])\r\n```", "It's over my knowledge to know if it needs to be re-created. Things seem to work correctly if we invalidate the cache only, so maybe `model._reset_compile_cache()` should be called in `.fit()` instead of (or additionally to) `.compile()` since some things that require it are invoked in `.fit()` (callbacks).\r\n\r\nAre we both on the same page speaking of classifying it as a bug? It looks like we have multiple workarounds already, so possibly it can have low priority, but this is something that should be, IMO, fixed in the future. I can make a PR if I know if my way of thinking is correct here.", "It is the same as `_reset_compile_cache` (that it is a \"private\" API) as the name could suggest it is called by `compile` (is public API).\r\n\r\nI think in this specific case `model.train_function=None` (and it is what it is done in `_reset_compile_cache) it is probably enough.\r\n\r\nI don't know if this is the only case that requires to recreate the `train_function` but you can try to submit a new PR covering this test case.\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@daniel-kukiela  Please go ahead and close the issue if you don't have any further queries.", "@MarkDaoust Could you evaluate if we need to add something to the docs?", "@saikumarchalla I'm still considering it a bug, this hasn't changed. I've been looking around to post a PR. I've been trying to search through the code if other places need a similar patch. Just haven't done that yet.", "The discussion shows that it's only the TensorBoard callback that triggers this error. So I think this should be considered a bug not a docs issue. \r\n\r\nNotes:\r\n\r\n`.compile(run_eagerly=True) ` also avoids the issue. \r\n\r\nLooking at `make_train_function` the only reference to summaries I see are [here](https://github.com/tensorflow/tensorflow/blob/r2.5/tensorflow/python/keras/engine/training.py#L848) and  [here](https://github.com/tensorflow/tensorflow/blob/r2.5/tensorflow/python/keras/engine/training.py#L1188) \r\n\r\nIt could be something like that callback being set as the default summary writer, and then something holding a reference to it.\r\nI'm more suspicious of the first link, since that is in the train train_function. Anyone want to try with that line commented out?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49246\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49246\">No</a>\n"]}, {"number": 49245, "title": "AttributeError: module 'tensorflow.compat.v2' has no attribute '__internal__'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 20.04):\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11 / 8\r\n- GPU model and memory: GTX 780\r\n\r\n\r\n\r\nI'm trying to run Keras version 2.2.4 on Ubuntu 20.04 with python 3.8 and tensorflow 2.2.0 with CUDA 11.1 and I get the following error.\r\n\r\nI've been looking around and according to me there is no stable version for python 3.8 using CUDA 11.1, I've tried to install different versions to solve the problem but in all of them I get the same thing.\r\n", "comments": ["Did you try importing keras from tensorflow?\r\n```python\r\nimport tensorflow as tf\r\ntf.keras.layers.Dense(...)\r\n```", "@ymodak I get the same issue. Maybe, the keras version is not enable for ubuntu 20, with CUDA 11 and python 3.8", "TF 2.2.0 Is CUDA 10.1", "P.s. CUDA\u00ae 11 (TensorFlow> = 2.4.0)", "Thank you. I've tryed and it works. Thank you!! :)", "@AldrichCabrera \r\n\r\nCould you please confirm if the issue still persist.Thanks", "@UsharaniPagadala The issue was solved. Thank you!!", "@AldrichCabrera \r\n\r\nThank you for your update, glad its working fine for you, kindly move this issue to closed status as it is resolved.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49245\">No</a>\n"]}, {"number": 49244, "title": "Upgrade Cudnn Frontend repo to v0.3 and use fallback list for ConvBiasAct ops", "body": "- This PR upgrades the Cudnn Frontend repo to v0.3: https://github.com/NVIDIA/cudnn-frontend/releases/tag/v0.3\r\n- The ConvBiasAct ops are updated to support a fallback list to avoid the situation when there is no working engines found in the heuristics list.\r\n- The patch is also updated to be clearer by adding the commit info.\r\n- This PR should be able to replace this pending PR: https://github.com/tensorflow/tensorflow/pull/48906\r\n\r\ncc. @nluehr \r\n", "comments": []}, {"number": 49243, "title": "TOSA legalization clean up and rewrite for consistency", "body": "    1. use llvm::SmallVector<T> if N is not known at compile time, otherwise use llvm::SmallVector<T,N>.\r\n    2. use llvm::SmallVectorImpl<T> as utility function signature.\r\n    3. remove unnecessary ArrayRef<T>() cast and replace it with llvm::makeArrayRef() instead if needed.\r\n    4. rename get1DConstTensor() into getConstTensor()\r\n    5. getConstTensor() is not able to generate TFL::ConstOp now.\r\n    6. getConstTensor() can generates N-D constant now.", "comments": ["Still can't edit reviewer myself. Tagging @rsuderman and @stellaraccident for review request.", "Apologies for this taking some time to merge. We're resolving some issues with it internally."]}, {"number": 49240, "title": "GridSearchCV error when n_jobs=-1", "body": "Hello,\r\n\r\nWhen I use GridSearchCV using njob=-1 I got an error bellow but it work great with njob=1. When using njob=-1 it runs for the first argument of taille_fen (7) but got error for the next. But it work with njob=1...\r\n\r\n\r\n The above exception was the direct cause of the following exception:\r\n\r\n```\r\nPicklingError                             Traceback (most recent call last)\r\n<ipython-input-17-692078694243> in <module>()\r\n     33     grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\r\n     34 \r\n---> 35     res = grid.fit(x_train, y_train,callbacks=[es])\r\n     36     grid_result.append(res)\r\n     37 \r\n\r\n7 frames\r\n/usr/lib/python3.7/concurrent/futures/_base.py in __get_result(self)\r\n    382     def __get_result(self):\r\n    383         if self._exception:\r\n--> 384             raise self._exception\r\n    385         else:\r\n    386             return self._result\r\n\r\nPicklingError: Could not pickle the task to send it to the workers.\r\n```\r\n\r\nMy code is \r\n\r\n```\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.wrappers.scikit_learn import KerasRegressor\r\nfrom sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\r\nimport pickle\r\nfrom google.colab import files\r\n\r\ndim_LSTM = [16,32,64,128]\r\nl1_reg = [0,0.001,0.01,0.1]\r\nl2_reg = [0,0.001,0.01,0.1]\r\ntaille_fen = [7,10,15,30,50]\r\nbs = [512]\r\nlrate = [0.1]\r\n\r\nmax_periodes = 1\r\n\r\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=20, min_delta=1e-4, restore_best_weights=True)\r\ntscv = TimeSeriesSplit(n_splits = 5)\r\n\r\ngrid_result = []\r\nfor bsize in bs:\r\n  for taille in taille_fen:\r\n    param_grid = {'dim_LSTM': dim_LSTM, 'l1_reg': l1_reg, 'l2_reg': l2_reg, 'lrate': lrate, 'taille_fen': [taille], 'bs':[bsize]}\r\n\r\n    dataset = prepare_dataset_XY(serie_entrainement,taille,horizon,bsize)\r\n\r\n    x,y = tuple(zip(*dataset))\r\n    x_train = np.asarray(tf.reshape(np.asarray(x,dtype=np.float32),shape=(np.asarray(x).shape[0]*np.asarray(x).shape[1],taille,1)))\r\n    y_train = np.asarray(tf.reshape(np.asarray(y,dtype=np.float32),shape=(np.asarray(y).shape[0]*np.asarray(y).shape[1])))\r\n\r\n    model = KerasRegressor(build_fn=ModelLSTM, epochs=max_periodes, verbose=2)\r\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\r\n\r\n    res = grid.fit(x_train, y_train,callbacks=[es])\r\n    grid_result.append(res)\r\n```\r\n\r\nFull error \r\n\r\n```\r\n_RemoteTraceback                          Traceback (most recent call last)\r\n_RemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/backend/queues.py\", line 153, in _feed\r\n    obj_ = dumps(obj, reducers=reducers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/backend/reduction.py\", line 271, in dumps\r\n    dump(obj, buf, reducers=reducers, protocol=protocol)\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/backend/reduction.py\", line 264, in dump\r\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 563, in dump\r\n    return Pickler.dump(self, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 437, in dump\r\n    self.save(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 843, in _batch_appends\r\n    save(x)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 884, in _batch_setitems\r\n    save(k)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 890, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 662, in save_reduce\r\n    save(state)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/usr/lib/python3.7/pickle.py\", line 885, in _batch_setitems\r\n    save(v)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 1087, in save_functor\r\n    obj.keywords), obj=obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 1087, in save_functor\r\n    obj.keywords), obj=obj)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 745, in save_function\r\n    *self._dynamic_function_reduce(obj), obj=obj\r\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/cloudpickle/cloudpickle_fast.py\", line 682, in _save_reduce_pickle5\r\n    dictitems=dictitems, obj=obj\r\n  File \"/usr/lib/python3.7/pickle.py\", line 638, in save_reduce\r\n    save(args)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 789, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/lib/python3.7/pickle.py\", line 774, in save_tuple\r\n    save(element)\r\n  File \"/usr/lib/python3.7/pickle.py\", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File \"/usr/local/lib/python3.7/dist-packages/dill/_dill.py\", line 1177, in save_cell\r\n    f = obj.cell_contents\r\nValueError: Cell is empty\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nPicklingError                             Traceback (most recent call last)\r\n<ipython-input-17-692078694243> in <module>()\r\n     33     grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, verbose=3)\r\n     34 \r\n---> 35     res = grid.fit(x_train, y_train,callbacks=[es])\r\n     36     grid_result.append(res)\r\n     37 \r\n\r\n7 frames\r\n/usr/lib/python3.7/concurrent/futures/_base.py in __get_result(self)\r\n    382     def __get_result(self):\r\n    383         if self._exception:\r\n--> 384             raise self._exception\r\n    385         else:\r\n    386             return self._result\r\n\r\nPicklingError: Could not pickle the task to send it to the workers.\r\n```", "comments": ["@AlexandreBourrieau \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks\r\n", "Nodofy have an idea ? IOn Stackoverflow nobody find the solution... Thanks !", "@AlexandreBourrieau \r\nPlease find these [link1](https://forums.databricks.com/questions/20623/picklingerror-could-not-pickle-the-task-to-send-it.html), [links2](https://github.com/joblib/joblib/issues/729), hope it helps.Thanks", "Thanks I finnaly found that by default the backend used is loky wich cannot share memory content between process so I need tio use threading backend to share memory content between thread.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49240\">No</a>\n", "You used cv=tscv  in the code what are the values of tscv? did you setup?"]}, {"number": 49239, "title": "Segfault using XNNPACK with C++ API", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-73-lowlatency x86_64)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below):  2.5.0\r\n- Python version: N/A\r\n- Bazel version (if compiling from source):  4.0.0\r\n- GCC/Compiler version (if compiling from source): GCC 10.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nRunning my C++ code with the XNNPACK backend results in segfault.\r\n\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nSegmentation fault\r\n```\r\n\r\nI used this build command and everything builds without errors. \r\n```\r\nbazel build --define tflite_with_xnnpack=true --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so\r\n```\r\n\r\nThe code compiles without errors but gets segfault when running the code. I also tried to build from the latest nightly release.\r\n\r\nLinking libtensorflowlite.so built without xnnpack works without errors.\r\n\r\nThe same tflite-model works with TFlite Benchmarking Tool using XNNPACK.\r\n\r\n**Describe the expected behavior**\r\nXNNPACK running as the default backend for Tflite without segfault.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nHere is the code I use ->\r\n\r\n```\r\n#include <tensorflow/lite/interpreter.h>\r\n#include <tensorflow/lite/model.h>\r\n#include <tensorflow/lite/kernels/register.h>\r\n#include <iomanip>\r\n#include <iostream>\r\n#include <mutex>\r\n#include <fstream>\r\n#include \"tbb/concurrent_unordered_map.h\"\r\n#include <unicode/unistr.h>\r\n#include <unicode/ustream.h>\r\n#include <unicode/locid.h>\r\n#include <chrono>\r\n#include <boost/algorithm/string.hpp>\r\n\r\nstd::mutex mtx;\r\n\r\nusing namespace std;\r\nusing namespace std::chrono_literals;\r\nusing std::chrono::high_resolution_clock;\r\nusing std::chrono::duration_cast;\r\nusing std::chrono::duration;\r\nusing std::chrono::microseconds;\t\r\n\r\ntflite::ops::builtin::BuiltinOpResolver resolver_sv;\r\nstd::unique_ptr<tflite::FlatBufferModel> model_sv;\r\nstd::unique_ptr<tflite::Interpreter> interpreter_sv;\r\ntbb::concurrent_unordered_map<string, int> vocab_sv;\r\n\r\nfloat predict(string ss) {\r\n\tstd::scoped_lock lock{mtx};\r\n\tint* input;\r\n\tfloat* output; \r\n\tstring s;\r\n\ticu::UnicodeString ustr(ss.c_str());\r\n\tustr.toLower();\r\n\tustr.toUTF8String(s);\r\n\t\r\n\tinput = interpreter_sv->typed_input_tensor<int>(0);\r\n\t\r\n\tstd::vector<std::string> result;\r\n\tboost::algorithm::split(result, s, boost::is_any_of(\" *\"), boost::token_compress_on);\r\n\r\n\tint i = 0;\r\n\tfor(auto& word: result) { \r\n\t\ttry {\r\n\t\t\tinput[i] = vocab_sv.at(word);\r\n\t\t\ti = i+1;\r\n\t\t}\r\n\t\tcatch (const std::out_of_range& oor) {\r\n\t\t\tinput[i] = 1;\r\n\t\t\ti = i+1;\r\n\t\t}\r\n\t}\r\n\twhile(i<=20) {\r\n\t\tinput[i] = 0;\r\n\t\ti = i+1;\r\n\t}\r\n\r\n\tinterpreter_sv->Invoke();\r\n\toutput = interpreter_sv->typed_output_tensor<float>(0);\r\n\r\n\treturn output[0];\r\n}\r\n\r\nvoid init_model(){\r\n\r\n\tmodel_sv = tflite::FlatBufferModel::BuildFromFile(\"/home/gustaf/cprojects/pacific/nlp/en/model.tflite\");\r\n\t\t\r\n\tif (model_sv == nullptr) {\r\n\t\t\tstd::cerr << \"Model not found!\" << std::endl;\r\n\t\t}\r\n\tif (tflite::InterpreterBuilder(*model_sv, resolver_sv)(&interpreter_sv) != kTfLiteOk) {\r\n\t\tstd::cerr << \"Failed to build interpreter!\" << std::endl;\r\n\t\t}\r\n\r\n\tinterpreter_sv->SetNumThreads(1);\r\n\r\n\tif (interpreter_sv->AllocateTensors() != kTfLiteOk) {\r\n\t\tstd::cerr << \"Failed to allocate tensors.\" << std::endl;\r\n\t}\r\n\tif (interpreter_sv->Invoke() != kTfLiteOk) {\r\n\t\tstd::cerr << \"Cannot invoke interpreter\" << std::endl;\r\n\t  }\r\n\t \r\n\tifstream infile(\"/home/gustaf/cprojects/pacific/nlp/en/vocab.txt\");\r\n\tstring a;\r\n\tint b;\r\n\twhile (infile >> a >> b) {vocab_sv.insert(pair<string, int>(a, b));}\r\n\r\n}\r\n\r\nint main (){\r\n\t\r\n\tinit_model();\r\n\t\r\n\tauto t1 = high_resolution_clock::now();\r\n\tpredict(\"This is a warmup text\");\r\n\tauto t2 = high_resolution_clock::now();\r\n\tpredict(\"This is another warmup text\");\r\n\tauto t3 = high_resolution_clock::now();\r\n\tpredict(\"This is an example text\");\r\n\tauto t4 = high_resolution_clock::now();\r\n\tpredict(\"This is another example text\");\r\n\tauto t5 = high_resolution_clock::now();\r\n\tpredict(\"This is the final text\");\r\n\tauto t6 = high_resolution_clock::now();\r\n\r\n\r\nauto ms1 = duration_cast<microseconds>(t2 - t1);\r\nauto ms2 = duration_cast<microseconds>(t3 - t2);\r\nauto ms3 = duration_cast<microseconds>(t4 - t3);\r\nauto ms4 = duration_cast<microseconds>(t5 - t4);\r\nauto ms5 = duration_cast<microseconds>(t6 - t5);\r\n\r\ncout << \"Prediction 1: \" << ms1.count() << \" us\" << endl;\r\ncout << \"Prediction 2: \" << ms2.count() << \" us\" << endl;\r\ncout << \"Prediction 3: \" << ms3.count() << \" us\" << endl;\r\ncout << \"Prediction 4: \" << ms4.count() << \" us\" << endl;\r\ncout << \"Prediction 5: \" << ms5.count() << \" us\" << endl;\r\n\r\n}\r\n\r\n```", "comments": ["@Maratyszcza could you take a look at this?", "@Haag-se Could you post a backtrace right after the segfault? You can get it by running the binary under `gdb`:\r\n```bash\r\ngdb -ex=run --args /path/to/binary --args...\r\nbacktrace\r\n```", "Sure! Here is the output\r\n\r\n```\r\nGNU gdb (Ubuntu 9.2-0ubuntu1~20.04) 9.2\r\nCopyright (C) 2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nType \"show copying\" and \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n    <http://www.gnu.org/software/gdb/documentation/>.\r\n\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from ./alpha...\r\nStarting program: /home/gustaf/cprojects/pacific/alpha \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n__memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:238\r\n238     ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S: No such file or directory.\r\n(gdb) backtrace\r\n#0  __memmove_avx_unaligned_erms () at ../sysdeps/x86_64/multiarch/memmove-vec-unaligned-erms.S:238\r\n#1  0x00007ffff7d4385f in tflite::ops::builtin::gather::Eval(TfLiteContext*, TfLiteNode*) () from /usr/local/lib/libtensorflowlite.so\r\n#2  0x00007ffff7e9ed69 in tflite::Subgraph::Invoke() () from /usr/local/lib/libtensorflowlite.so\r\n#3  0x00007ffff7ea4479 in tflite::Interpreter::Invoke() () from /usr/local/lib/libtensorflowlite.so\r\n#4  0x000055555555a1ad in init_model () at /usr/include/c++/10/bits/unique_ptr.h:421\r\n#5  0x000055555555978c in main () at alpha.cpp:94\r\n(gdb)\r\n```", "The crash happens outside of XNNPACK. I would need the model to debug it.\r\n\r\ncc @multiverse-tf for any guesses on what might be happening here.", "Model attached (had to zip it due to github filetype restrictions).\r\n\r\nIt works if I build Tflite without XNNPACK. It also works using TFlite Benchmarking Tool using XNNPACK.\r\n\r\n[model.tflite.zip](https://github.com/tensorflow/tensorflow/files/6503330/model.tflite.zip)\r\n", "> Model attached (had to zip it due to github filetype restrictions).\r\n> \r\n> It works if I build Tflite without XNNPACK. It also works using TFlite Benchmarking Tool using XNNPACK.\r\nIndeed. I didn't get this segfault either w/ the benchmark model tool. Could you compile the binary w/ \"-c dbg\" instead of \"-c opt\" and then post the backtrace?\r\n\r\nBtw, it looks like you compiled tflite .so first via bazel, then which build tool did you use to compile your app code? If it's a different build tool, I'm not sure whether such a difference, like using different compiler and compiling/linking options, could cause this segfault.\r\n\r\n> \r\n> [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/6503330/model.tflite.zip)\r\n\r\n\r\n\r\n", "Hmm. __memmove_avx_unaligned_erms() looks interesting.\r\n@Haag-se, does your target support AVX instructions? I wonder why AVX instructions are used since it's disabled by default.\r\nHow about adding `--copt=-mno-avx` to your Bazel build command? Also \"monolithic\" doesn't look needed.\r\n```\r\nbazel build -c opt --define tflite_with_xnnpack=true --copt=-mno-avx //tensorflow/lite:libtensorflowlite.so\r\n```", "> Hmm. __memmove_avx_unaligned_erms() looks interesting.\r\n> @Haag-se, does your target support AVX instructions? I wonder why AVX instructions are used since it's disabled by default.\r\n> How about adding `--copt=-mno-avx` to your Bazel build command? Also \"monolithic\" doesn't look needed.\r\n> \r\n> ```\r\n> bazel build -c opt --define tflite_with_xnnpack=true --copt=-mno-avx //tensorflow/lite:libtensorflowlite.so\r\n> ```\r\n\r\nYour suggestion worked, thank you!\r\n\r\nAdding `--copt=-mno-avx` (and removing monolithic) to the build command did the trick.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49239\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49239\">No</a>\n"]}, {"number": 49238, "title": "ValueError: as_list() is not defined on an unknown TensorShape.", "body": "Hi. I try to implement AlexNet with Coco dataset. I want to make multi label classification but tf throws the error. Full error is:\r\n\r\n`\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:758 train_step\r\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:387 update_state\r\n        self.build(y_pred, y_true)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:318 build\r\n        self._metrics, y_true, y_pred)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1163 map_structure_up_to\r\n        **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1258 map_structure_with_tuple_paths_up_to\r\n        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1258 <listcomp>\r\n        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1161 <lambda>\r\n        lambda _, *values: func(*values),  # Discards the path arg.\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:418 _get_metric_objects\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:418 <listcomp>\r\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:439 _get_metric_object\r\n        y_t_rank = len(y_t.shape.as_list())\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:1190 as_list\r\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n\r\n    ValueError: as_list() is not defined on an unknown TensorShape.\r\n`\r\n\r\nHere the Colab link:\r\n[https://colab.research.google.com/drive/12nO4IDQJg02aiduqsMimyMJ6Z8KNlCFw?usp=sharing](url)\r\n\r\nI don't understand where my mistake is.\r\n**System information**\r\n-Google Colab\r\n\r\nYou can collect some of this information using our environment capture\r\nYou can also obtain the TensorFlow version with:\r\nTF 2.0: v2.4.1-0-g85c8b2a817f 2.4.1\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["This was marked as a bug. Is this a new bug or is it tied to the same bugs around make_ndArray() and numpy() methods being missing?", "Check https://github.com/tensorflow/tensorflow/issues/28257#issuecomment-489717572", "> Check [#28257 (comment)](https://github.com/tensorflow/tensorflow/issues/28257#issuecomment-489717572)\r\n\r\nI've read it but in my situation tf.py_function returns a tuple which includes an image and a label. I cannot give the shape of anything. Here the code:\r\n\r\n`ds_train = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: tf.py_function(train_images, [x], [tf.float32, tf.uint8]))`\r\n\r\nactually, tf.py_function's input parameters is just a file name, not an input that is fed into the network.\r\n\r\n\r\n\r\n> This was marked as a bug. Is this a new bug or is it tied to the same bugs around make_ndArray() and numpy() methods being missing?\r\n\r\nI don't know. I just want to get image and label from a csv file which inclide the file name and a multi label.", "update the problem:\r\n\r\nI tried to give the shapes to the image and label but this didn't work.  Here is the test codes and the error I got:\r\n\r\n def train_images(fn):\r\n> file_name = bytes.decode(fn.numpy())\r\n  image = tf.io.read_file(\"train2014/\" + file_name)\r\n  image = tf.image.decode_image(image, channels=3, dtype=tf.float32)\r\n  image = tf.image.resize(image, (227, 227))\r\n  label = tf.convert_to_tensor(train_annotation.loc[file_name].values, dtype=tf.uint8)\r\n  return image, label\r\n\r\ndef val_images(fn):\r\n  >file_name = bytes.decode(fn.numpy())\r\n  image = tf.io.read_file(os.path.join(\"val2014/\", file_name))\r\n  image = tf.image.decode_image(image, channels=3, dtype=tf.float32)\r\n  image = tf.image.resize(image, (227, 227))\r\n  label = tf.convert_to_tensor(val_annotation.loc[file_name].values, dtype=tf.uint8)\r\n  return image, label\r\n\r\ndef test_images(fn):\r\n  >file_name = bytes.decode(fn.numpy())\r\n  image = tf.io.read_file(os.path.join(\"val2014/\", file_name))\r\n  image = tf.image.decode_image(image, channels=3, dtype=tf.float32)\r\n  image = tf.image.resize(image, (227, 227))\r\n  label = tf.convert_to_tensor(test_annotation.loc[file_name].values, dtype=tf.uint8)\r\n  return image, label\r\n\r\ndef shape_function_with_three_dims(func, input, output):\r\n  >holder = tf.py_function(func, [input], output)\r\n  holder[0].set_shape([227, 227, 3])\r\n  holder[1].set_shape([90])\r\n  return holder\r\n\r\nHere I create dataset from both situation.\r\n>ds_train = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: tf.py_function(train_images, [x], [tf.float32, tf.uint8]))\r\nds_valid = tf.data.Dataset.from_tensor_slices(val_annotation.index.values).map(lambda x: tf.py_function(val_images, [x], [tf.float32, tf.uint8]))\r\nds_test = tf.data.Dataset.from_tensor_slices(test_annotation.index.values).map(lambda x: tf.py_function(test_images, [x], [tf.float32, tf.uint8]))\r\n\r\nAnd the error that I got: `ValueError: as_list() is not defined on an unknown TensorShape` \r\n\r\n\r\n>ds_train1 = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: shape_function_with_three_dims(train_images, x, [tf.float32, tf.uint8]))\r\nds_valid1 = tf.data.Dataset.from_tensor_slices(val_annotation.index.values).map(lambda x: shape_function_with_three_dims(val_images, x, [tf.float32, tf.uint8]))\r\nds_test1 = tf.data.Dataset.from_tensor_slices(test_annotation.index.values).map(lambda x: shape_function_with_three_dims(test_images, x, [tf.float32, tf.uint8]))\r\n\r\nAnd the error when I used the ds_train1 and ds_valid1; `ValueError: Input 0 of layer sequential is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (227, 227, 3)`\r\n\r\nI also tested to give an input images and label as two different datasets like here:\r\n\r\n```\r\nds_train_images = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: tf.py_function(train_images, [x], tf.float32))\r\nds_train_labels = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: tf.py_function(val_labels, [x], tf.float32))\r\n```\r\n\r\nand the inevitable result happened. I got an error: `ValueError: `y` argument is not supported when using dataset as input.`\r\n\r\nI just want to create a multi label classifier. If you know how to create a dataset for multi label classifier from the one of the csv file I attach, please share the way with me.\r\nThe rows are image id and the columns are the classes for Coco classes. For examle in test_annotation.csv, the image of which id is 558840 includes person, bottle, cup, spoon, hot dog and dining table\r\n\r\n[val_annotation.csv](https://github.com/tensorflow/tensorflow/files/6501525/val_annotation.csv)\r\n\r\n[train_annotation.csv](https://github.com/tensorflow/tensorflow/files/6501521/train_annotation.csv)\r\n\r\n[test_annotation.csv](https://github.com/tensorflow/tensorflow/files/6501520/test_annotation.csv)\r\n\r\n\r\n\r\n", "Can you share a new update colab with mnist so it will be faster to verify?", "Here the link\r\n[mnist link](https://colab.research.google.com/drive/1GUEQeFm0jK1Vl-GRNh9Kv9Fjj18-5rNU?usp=sharing)\r\nIt seems illogical but I tried to keep the same structure as previous link. And I got the same error. ", "Also in this one you need to set the shape as `tf.py_function` don't return tensors with shape and it doesn't have the output_shape extra param e.g. like the generator (here the feature request/proposal https://github.com/tensorflow/tensorflow/issues/36274)", "If you want an example see how to set the shape with `py_function` at https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic", "with bhack help, finally my code is working. Thanks dude. But,\r\nI had to add some extra code. These are:\r\n\r\nFirst, I had to create a new method to apple tf.py_function and to give shape to the image and label. Here the method: method takes a function, input-x- and outputs and applies tf.py_function.\r\n\r\n>def general_function(func, x, outputs):\r\n image, label = tf.py_function(train_images, [x], outputs)\r\n  image.set_shape(tf.TensorShape([227, 227, 3]))\r\n  label.set_shape(tf.TensorShape([90]))\r\n  return image, label\r\n\r\n\r\nAfter that, when the dataset are created, the batch size must be given using tf.data.Dataset.batch() method. I don't understand why but when I tried to train the model without using .batch() method, but I gave the batch size in model.fit, I got an error -I don't remember which error I got.-\r\n\r\n>ds_train = tf.data.Dataset.from_tensor_slices(train_annotation.index.values).map(lambda x: general_function(train_images, x, [tf.float32, tf.uint8])).batch(batch_size)\r\n\r\nAt last, here my model.fit model's arguments:\r\n>history = model.fit(\r\n    ds_train,\r\n    validation_data=ds_valid,\r\n    epochs=10,\r\n    batch_size=batch_size\r\n)\r\n\r\nNote: Here is the train_images method:\r\n>def train_images(fn):\r\n  file_name = bytes.decode(fn.numpy())\r\n  image = tf.io.read_file(\"train2014/\" + file_name)\r\n  image = tf.image.decode_image(image, channels=3, dtype=tf.float32)\r\n  image = tf.image.resize(image, (227, 227))\r\n  label = tf.convert_to_tensor(train_annotation.loc[file_name].values, dtype=tf.uint8)\r\n  return image, label", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49238\">No</a>\n", "@dundarahmet wow adding one general function just to shape the data work for me like a charm, thanks"]}, {"number": 49235, "title": "Question Pertaining to TanH Implementation for TFLite", "body": "I was reading through the [TFLite Zero Skew Representation paper](https://arxiv.org/pdf/1712.05877.pdf) which makes a reference to the implementation of most mathematical functions [here](https://github.com/tensorflow/tensorflow/blob/4952f981be07b8bf508f8226f83c10cdafa3f0c4/tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h). However, though the paper mentions the implementation of TanH function being available in the file, I couldn't find the fixed-point integer-only inference implementation of TanH. Can someone point me to the correct source please?", "comments": ["https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/integer_ops/tanh.h", "@bhack Thanks for the link. Closing this issue for now.", "@bhack Would you have an idea regarding what's the functional difference between `lite/kernels/internal/reference` and `lite/kernels/internal/optimized` implementations of integer ops? Which one is used by default in TFLite?", "References have more readability and I think they are also fallback of the optimized ones. E.g. see a PR for the TANH  in Micro https://github.com/tensorflow/tensorflow/pull/48052 "]}, {"number": 49234, "title": "API Documentation pages do not allow providing written feedback", "body": "## URL(s) with the issue:\r\n\r\nAll pages in the document including:\r\nhttps://www.tensorflow.org/api_docs/python/tf\r\n\r\n## Description of issue (what needs changing):\r\nRight now, there is no simple means to provide feedback on the generated documentation aside from a thumbs up/thumbs down and limited multiple choice selection, making such feedback hardly useful. The API documentation at this stage is far too terse in many locations making it hard to use or unusable. It would make far more sense to allow written commentary to be collected against each page so that periodically, the software developers for each component could read the commentary and add necessary code comments to address the issues raised by people attempting to use the code.\r\n\r\nGiven that this is tensorflow machine learning documentation, it might even be possible to do natural language processing to extract common themes where people are running into issues.\r\n\r\nThe current system, like the angular documentation does seem like a conscious effort to avoid taking input from users and that really should not be the case.", "comments": ["/cc @lamberta @MarkDaoust @theadactyl ", "@rmothukuru What is tensorflower? I could not see a description in google search or in repos with that name.", "@lonerzzz,\r\nSorry for the confusion. Tensorflower means Google Engineer, working on Tensorflow. ", "Yeah, it's \"TenaorFlow-er\" not \"Tensor-Flower \ud83c\udf38\" .\r\n\r\nIIRC the site used have this feature, and it was just too hard to turn into actionable feedback beyond what we already knew from the ratings. Plus we have plenty of bugs in this queue already, we don't need a whole new queue.\r\n\r\nWe encourage you to use the ratings, we check that signal.\r\nIf you have more specific feedback (it sounds like you do), raise a bug like this one, or send us a PR with a fix.", "If you are using the ratings and pull down options , it would still make sense to have a larger set of selectable options as the set now is incomplete for communicating problems."]}, {"number": 49233, "title": "Should tf 2.5.0 recognize GPU on Windows 10 with CUDA 11.3/cuDNN 8.2 installed?", "body": "[Tf docs](https://www.tensorflow.org/install/gpu#software_requirements) say that GPU support will work with CUDA 11, I assume meaning 11.* (??)\r\n\r\nI have heard that tf GPU 2.5.0 is designed to work with CUDA 11.2 (https://github.com/tensorflow/tensorflow/issues/49190#issuecomment-841656229).\r\n\r\nI have also heard that tf build failed with CUDA 11.3 libraries (#48803).  I assume this means that tf 2.5.0 GPU will NOT WORK with CUDA 11.3.  (??)\r\n\r\nCan any Win 10 x64 users get their GPU recognized by tf 2.5.0 when CUDA 11.3/cuDNN 8.2 are installed?  If not, I assume I must downgrade CUDA to 11.2.2 and cuDNN to 8.1.1.\r\n\r\nMy GPU was recognized by tf 2.4.1 in my environment with CUDA 11.3.  After tf 2.5.0 was installed using pip, my GPU is no longer recognized.  \r\n\r\nThanks in advance!  \r\n\r\n", "comments": ["@pythonic2020 ,\r\n\r\nEvery TensorFlow release is compatible with a certain CUDA/cuDNN version, for more information please take a look at the [tested build configurations](https://www.tensorflow.org/install/source_windows#gpu).Please try installing TensorFlow v2.5 with respective CUDA  and cuDNN  and check if you are facing the same error. \r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow_gpu-2.5.0 | 3.6-3.9 | MSVC 2019 | Bazel 3.1.0 | 8.1 | 11.2\r\ntensorflow_gpu-2.4.0 | 3.6-3.8 | MSVC 2019 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow_gpu-2.3.0 | 3.5-3.8 | MSVC 2019 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow_gpu-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nThanks!\r\n\r\n\r\n\r\n\r\n", "Ok, I will try to downgrade my CUDA/cuDNN and see what happens.  ", "Several hours wasted.  TF 2.5.0 GPU will not recognize GPU using CUDA 11.0 (cuDNN 8.0.4), 11.2 (cuDNN 8.1.1), or 11.3 (cuDNN 8.2).  TF 2.4.1 GPU recognizes my GPU using all three of those.\r\n\r\nSo, I ask again if there is any evidence that TF 2.5.0 GPU will recognize GPU in a Python 3.8 environment on WIndows 10, and, if so, using what CUDA and cuDNN versions?\r\n\r\nExcerpt from system $PATH$:\r\n\r\n`PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include;C:\\tools\\cuda\\bin;`", "I finally got my GPU recognized by TF 2.5.0 using CUDA 11.2.2 + cuDNN 8.1.1.  Before too long, they will move to CUDA 11.3.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49233\">No</a>\n", "@pythonic2020 Sorry for the inconvenience the website was showing older versions of cuda cudnn compatibility. I have updated the docs to reflect the correct versions which you found compatible as well.\r\nhttps://www.tensorflow.org/install/source#gpu", "Thank you, @ymodak!  The outdated versions on the [GPU Support page](https://www.tensorflow.org/install/gpu) are what misled and frustrated me.  I didn't build TF from source.", "> @pythonic2020 Sorry for the inconvenience the website was showing older versions of cuda cudnn compatibility. I have updated the docs to reflect the correct versions which you found compatible as well.\r\n> https://www.tensorflow.org/install/source#gpu\r\n\r\nwhy does it say that it requires Bazel 3.1.0 while in ./comfigure.py it has\r\n\r\n_TF_MAX_BAZEL_VERSION = '3.99.0' ?\r\n\r\nnote: I am referring to tensorflow tag==2.5.0"]}, {"number": 49232, "title": "[ROCm] This change replaces the original assert for detecting multiple", "body": "NCCL managers in favor of a warning log message.\r\n\r\nThe original assert was added to protect against a potential deadlock\r\nscenario (that is specific to ROCm) when multiple hosts are emulated\r\nusing a single host.  This can happen in this scenario because two\r\ndifferent NCCL streams can map to the same hardware queue and block\r\neach other from making forward progress.\r\n\r\nThis deadlock scenario seems to only occur in this specific scenario.\r\n\r\nHowever, the assert was preventing other scenarios involving multiple\r\nNCCL managers from executing even though they did not cause a deadlock.\r\n\r\nFor example, TF creates a single eager context (which creates a NCCL manager)\r\nprior to running a model in eager mode.  However, other eager contexts can be\r\ncreated for single use purposes which results in additional NCCL managers being\r\ncreated but not used duing the model run.  In these scenarios, deadlock is\r\nimpossible and thus the creation of multiple NCCL managers should be allowed\r\nto proceed.\r\n\r\nThe specific case which is prompting this change is that in addition to the\r\nregular eager context, an additional context is created by the MLIR constant\r\nfolding optimization to compute the constant at compile time so that it can\r\nbe used as a replacement later on the compilation process.  This additional\r\ncontext is only used once and cannot cause a deadlock.\r\n\r\nThis change enables these cases to execute successfully.\r\n\r\nIn the future, the original assert will have to be reinstated with\r\nadditional logic restricting its execution to the specific scenario(s)\r\nwhich cause the deadlock.", "comments": ["FYI.\r\n\r\n/cc @cheshire @chsigg ", "@chsigg Gentle ping.", "@chsigg Gentle ping.", "@cheshire gentle ping.", "@jurahul any thoughts on this?", "@rsanthanam-amd sorry I am confused. In general, we try to avoid warning log messages: if it's an error, we should check at least dynamically, and if it's not an error, we should not print anything.\r\n\r\n@jpienaar can we avoid creating the NCCL manager from the TF eager context used by the MLIR bridge?\r\n\r\nCould you clarify the connection with ROCm?", "> @rsanthanam-amd sorry I am confused. In general, we try to avoid warning log messages: if it's an error, we should check at least dynamically, and if it's not an error, we should not print anything.\r\n> \r\n> @jpienaar can we avoid creating the NCCL manager from the TF eager context used by the MLIR bridge?\r\n> \r\n> Could you clarify the connection with ROCm?\r\n\r\nI'm surprised a NCCL manager is created given GPU count is set to 0 there - at least I can't think of where a NCCL manager would be useful without any GPU devices enabled. I think we should be able to avoid that.", "This is a PR from ~21 days ago, is it possible that the bug is no longer there?", "@cheshire, @jpienaar, @rsanthanam-amd Any update on this PR? Please. Thanks!\r\n\r\n", "@rsanthanam-amd do you still hit the original issue?", "@cheshire I recently tried to reproduce the problem but the model I am using is experiencing problems because of the keras decoupling.  I will look into this as soon as I can sort out that problem.", "@rsanthanam-amd is this the issue about checking `isinstance` on `OptimizerV2` by. chance? ", "@cheshire Yes, exactly!", "@rsanthanam-amd could you please describe exactly how did you hit it?", "@cheshire the model i am working with is using some lars optimizer.  Here is the error message: \"TypeError: \"inner_optimizer\" must be an instance of OptimizerV2, but got: <lars_optimizer.LARSOptimizer object at 0x7f6b390250f0>\"", "so the underlying issue is that you are mixing Keras classes from `import keras` and from `from tensorflow.python import keras`. If you remove all the imports of the latter class, the issue should be resolved. Where is your `LARSOptimizer` definition?", "@cheshire Thank you so much!  Your suggestion worked and I got the model running.  This Larsoptimizer seems to be part of the model source.\r\n\r\nI synced our ROCm fork with this repo on Monday and I can confirm that in our synced repo, this multiple nccl manager problem still exists.", "Could you report the call stacks of where the NcclManagers are created?", "Call stack No. 1\r\n-----------------\r\n\r\n#0  0x00007f2ef0b2cfb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2cf7f11834 in tensorflow::MaybeCreateNcclCommunicator() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f2cf7efe26a in tensorflow::EagerContext::EagerContext(tensorflow::SessionOptions const&, tensorflow::ContextDevicePlacementPolicy, bool, tensorflow::DeviceMgr*, bool, tensorflow::Rendezvous*, tensorflow::DistributedFunctionLibraryRuntime*, tensorflow::CollectiveExecutorMgrInterface*, bool) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f2cf15ae71e in TFE_NewContext () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f2ce18c73b5 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#8}, pybind11::object, TFE_ContextOptions const*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::return_value_policy>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#8}&&, pybind11::object (*)(TFE_ContextOptions const*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::return_value_policy const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so\r\n#5  0x00007f2ce18c4e96 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tfe.so\r\n\r\nCall stack No. 2\r\n-----------------\r\n\r\n#0  0x00007f2ef0b2cfb7 in raise () from /lib/x86_64-linux-gnu/libc.so.6\r\n#1  0x00007f2cf7f11834 in tensorflow::MaybeCreateNcclCommunicator() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00007f2cf7efe26a in tensorflow::EagerContext::EagerContext(tensorflow::SessionOptions const&, tensorflow::ContextDevicePlacementPolicy, bool, tensorflow::DeviceMgr*, bool, tensorflow::Rendezvous*, tensorflow::DistributedFunctionLibraryRuntime*, tensorflow::CollectiveExecutorMgrInterface*, bool) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x00007f2cf15ae71e in TFE_NewContext () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x00007f2cf0541fa0 in mlir::TF::ConstantFoldFallbackHook(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&)::{lambda()#2}::operator()() const [clone .isra.161] () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#5  0x00007f2cf15a755f in mlir::TF::ConstantFoldFallbackHook(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&)\r\n    () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#6  0x00007f2cf8ac3595 in mlir::TF::(anonymous namespace)::TFConstantFoldInterface::fold(mlir::Operation*, llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) const () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#7  0x00007f2cfd2fe25a in mlir::Operation::fold(llvm::ArrayRef<mlir::Attribute>, llvm::SmallVectorImpl<mlir::OpFoldResult>&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#8  0x00007f2cfa0e87b0 in mlir::OperationFolder::tryToFold(mlir::OpBuilder&, mlir::Operation*, llvm::SmallVectorImpl<mlir::Value>&, llvm::function_ref<void (mlir::Operation*)>) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#9  0x00007f2cfa0e9a04 in mlir::OperationFolder::tryToFold(mlir::Operation*, llvm::function_ref<void (mlir::Operation*)>, llvm::function_ref<void (mlir::Operation*)>, bool*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#10 0x00007f2cfa0ebc41 in mlir::applyPatternsAndFoldGreedily(llvm::MutableArrayRef<mlir::Region>, mlir::FrozenRewritePatternSet const&, mlir::GreedyRewriteConfig) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#11 0x00007f2cfa457ebb in mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#12 0x00007f2cfa458212 in mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#13 0x00007f2cfa456b71 in mlir::detail::OpToOpPassAdaptor::runOnOperationAsyncImpl(bool) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#14 0x00007f2cfa45808f in mlir::detail::OpToOpPassAdaptor::run(mlir::Pass*, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#15 0x00007f2cfa458212 in mlir::detail::OpToOpPassAdaptor::runPipeline(llvm::iterator_range<llvm::pointee_iterator<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> >*, mlir::Pass> >, mlir::Operation*, mlir::AnalysisManager, bool, unsigned int, mlir::PassInstrumentor*, mlir::PassInstrumentation::PipelineParentInfo const*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#16 0x00007f2cfa4597fa in mlir::PassManager::runPasses(mlir::Operation*, mlir::AnalysisManager) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#17 0x00007f2cfa45a1a5 in mlir::PassManager::run(mlir::Operation*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#18 0x00007f2cf858c55d in tensorflow::CompileGraphSetup(mlir::ModuleOp, llvm::ArrayRef<tensorflow::XlaArgument>, std::vector<int, std::allocator<int> >*, llvm::SmallVector<tensorflow::TensorOrResourceShape, 4u>&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#19 0x00007f2cf858ce1f in tensorflow::BuildHloFromModule(mlir::ModuleOp, xla::XlaBuilder&, llvm::ArrayRef<xla::XlaOp>, std::vector<xla::XlaOp, std::allocator<xla::XlaOp> >&, llvm::ArrayRef<tensorflow::XlaArgument>, llvm::StringRef, llvm::MutableArrayRef<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> > >) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#20 0x00007f2cf8591b3b in tensorflow::BuildHloFromGraph(tensorflow::Graph const&, xla::XlaBuilder&, llvm::ArrayRef<xla::XlaOp>, std::vector<xla::XlaOp, std::--Type <RET> for more, q to quit, c to continue without paging--\r\nallocator<xla::XlaOp> >&, llvm::ArrayRef<tensorflow::XlaArgument>, llvm::ArrayRef<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, llvm::StringRef, tensorflow::FunctionLibraryDefinition const&, tensorflow::GraphDebugInfo const&, llvm::MutableArrayRef<std::unique_ptr<mlir::Pass, std::default_delete<mlir::Pass> > >) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#21 0x00007f2cf85434fd in tensorflow::MlirXlaOpKernel::ConstructXlaOp(tensorflow::XlaOpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#22 0x00007f2cf85447f9 in tensorflow::MlirXlaOpKernel::Compile(tensorflow::XlaOpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#23 0x00007f2cf8577d66 in tensorflow::XlaOpKernel::Compute(tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#24 0x00007f2cf8e60137 in tensorflow::XlaCompilationDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#25 0x00007f2cf85620f9 in tensorflow::GraphCompiler::Compile() ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#26 0x00007f2cf8574d44 in tensorflow::XlaCompiler::CompileGraph(tensorflow::XlaCompiler::CompileOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, absl::lts_20210324::Span<tensorflow::XlaArgument const>, tensorflow::XlaCompilationResult*) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#27 0x00007f2cf857711a in tensorflow::XlaCompiler::CompileFunction(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::NameAttrList const&, absl::lts_20210324::Span<tensorflow::XlaArgument const>, tensorflow::XlaCompilationResult*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#28 0x00007f2cf8544cd0 in std::_Function_handler<tensorflow::Status (tensorflow::XlaCompiler*, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompilationResult*), tensorflow::XlaCompilationCache::Compile(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)::{lambda(tensorflow::XlaCompiler*, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompilationResult*)#1}>::_M_invoke(std::_Any_data const&, tensorflow::XlaCompiler*&&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompilationResult*&&) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#29 0x00007f2cf854bb8f in tensorflow::XlaCompilationCache::CompileStrict(tensorflow::XlaCompilationCache::Entry*, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<tensorflow::Status (tensorflow::XlaCompiler*, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompilationResult*)> const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#30 0x00007f2cf854d9dc in tensorflow::XlaCompilationCache::CompileImpl(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, std::function<tensorflow::Status (tensorflow::XlaCompiler*, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompilationResult*)> const&, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#31 0x00007f2cf854e836 in tensorflow::XlaCompilationCache::Compile(tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompilationCache::CompileMode, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#32 0x00007f2cf1df6da9 in tensorflow::CompileToLocalExecutable(tensorflow::OpKernelContext*, tensorflow::NameAttrList const&, bool, tensorflow::XlaPlatformInfo const&, absl::lts_20210324::Span<tensorflow::Tensor const* const>, absl::lts_20210324::Span<tensorflow::VariableInfo const>, absl::lts_20210324::Span<int const>, tensorflow::XlaCompilationCache::CompileMode, bool, xla::LocalClient**, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n--Type <RET> for more, q to quit, c to continue without paging--\r\n#33 0x00007f2cf1df981c in tensorflow::XlaLocalLaunchBase::Compute(tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#34 0x00007f2ce40495f9 in tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#35 0x00007f2ce414242f in tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::Process(tensorflow::PropagatorState::TaggedNode, long) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#36 0x00007f2ce4143718 in std::_Function_handler<void (), tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::RunTask<tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::ScheduleReady(absl::lts_20210324::InlinedVector<tensorflow::PropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::PropagatorState::TaggedNode> >*, tensorflow::PropagatorState::TaggedNodeReadyQueue*)::{lambda()#2}>(tensorflow::(anonymous namespace)::ExecutorState<tensorflow::PropagatorState>::ScheduleReady(absl::lts_20210324::InlinedVector<tensorflow::PropagatorState::TaggedNode, 8ul, std::allocator<tensorflow::PropagatorState::TaggedNode> >*, tensorflow::PropagatorState::TaggedNodeReadyQueue*)::{lambda()#2}&&)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#37 0x00007f2cf15f2181 in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#38 0x00007f2cf15ef873 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) () from /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#39 0x00007f2ce45dc507 in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) ()\r\n   from /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n#40 0x00007f2ef08d66db in start_thread () from /lib/x86_64-linux-gnu/libpthread.so.0\r\n#41 0x00007f2ef0c0f71f in clone () from /lib/x86_64-linux-gnu/libc.so.6\r\n", "Pending change here for review to avoid the init where there are 0 GPU devices configured, just waiting for reviews to complete.", "https://github.com/tensorflow/tensorflow/commit/fc158eb6d407dc08935be8c06111b31eb7b39465 should avoid creating these now where we have 0 GPUs set.", "@rsanthanam-amd Can you please check @jpienaar's comments and keep us posted ? Thanks!", "@jpienaar @gbaned I verified that jpienaar's fix does indeed address the issue and I am able to run my model without the need for the PR.\r\n\r\nThis PR can be canceled.", "@cheshire Since @jpienaar fixed the underlying issue, can I cancel and close this PR?"]}, {"number": 49231, "title": "updated micro_speech readme.md", "body": "Small fix to the README in the TFLM micro_speech example. The old `TAGS` was still used instead of `OPTIMIZED_KERNEL_DIR`.\r\n\r\nTested the command to generate the binary on macOS.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 49230, "title": "Fixing LocallyConnected2D and LocallyConnected1D layer Save Model to tf issue", "body": "Fixes #48584 and #47689 .\r\n\r\n### My understanding:\r\nThe model save is failing because at the time of saving, `call()` of LocallyConnected2D will be invoked which is not passed with any inputs (None). That is why `compute_output_shape` and `local_conv_matmul` fails and thus `save` fails.\r\n\r\n### Solution that I propose:\r\nWe particularly don't require any input data at the time of saving. So, I am saving the input_shape at the time when LocallyConnected2D instance is `build()` and whenever the parameter `inputs` is `None` in `call()`, I am replacing `inputs` with the dummy tensor with shape `input_shape` which was saved in `build()`.\r\n\r\nAny suggestions on optimisation of solution are welcomed.\r\n\r\ncc @mihaimaruseac , @bhack , @ AdityaKane2001 ", "comments": ["Can you add a test to cover the mentioned tickets case?", "@ashutosh1919 \r\nI ran your code in colab. I made a model with one LocallyConnected2D layer. I passed a random numpy array to the model. Then I tried to save the model, but the notebook crashed due to excessive RAM usage. Gist [here](https://colab.research.google.com/gist/AdityaKane2001/8fb5ba3fb81b4e37021817eb5a65d629/locallyconnectedpr.ipynb#scrollTo=JMg1oc5bl05h).", "@AdityaKane2001 , how are you running code in my branch? I see direct import `import tensorflow as tf` in the gist you shared above.\r\nAlso, try reducing the size of the np array. I tried with (10,10,10,3) and works fine in your gist.", "@ashutosh1919 \r\nIt's sort of a small hack but it works for small changes. I replaced the changed files in local.py of the colab session with your code before importing tensorflow. It then uses the new code.\r\nActually, it's my fault, I forgot to add `implementation = 2` for LocallyConnected2D .", "Updated gist [here](https://colab.research.google.com/gist/AdityaKane2001/7816a20a2340aa5bfe13f1a2ea4aca97/locallyconnectedpr.ipynb). I have used your code as mentioned above. Still facing the same error.", "@AdityaKane2001 , I have resolved the error and added test cases. And btw, I think you can't just change the code for `local.py` in collab. I think you need to `bazel build` again if you change the code. \r\n\r\n@bhack , I have added the test case to save all 3 implementation of models and the problem is there for `LocallyConnected1D` as well. So, I have fixed the code in that too. Bazel tests are passing in my local. Can you please approve the PR so that the external build tests can run?\r\n\r\n### Solution (Attempt 2):\r\nActually at the time of `save()`, it is not that `No inputs are passed`, Inputs are passed but the shape of that is something like `(None, None, None, 2)` which is not the conventional shape (None, 4, 4, 2). So, the solution is, \r\n- To store the shape of input whenever the layer is built.\r\n- Whenever call() is invoked, check whether any of the dimension shape other than first one is `None`. If yes, then replace `input_shape` with `stored_input_shape` otherwise proceed as it is.\r\n\r\nAny other suggestions on this are welcome. Also I would suggest you to please review code and suggest changes because I have raised `ValueErrors` as well as `tf_logging.warn` and I am not sure about the messages that I am populating in that. It may need correction.\r\n\r\ncc @mihaimaruseac @fchollet ", "@ashutosh1919 \r\nAFAIK, it works because we are altering the python code, which is in the very last layer of the API system. \r\n\r\nAnyways, apart from that, everything looks good. \ud83d\udc4d ", "@k-w-w , Thanks for straight forward and easy solution. I got to learn something new here. \r\nI have made required changes. Please review."]}]