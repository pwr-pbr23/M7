[{"number": 4220, "title": "Merge Changes from Internal: Branch 132225803", "body": "", "comments": ["Ping @danmane \n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 4219, "title": "upgrade tensorflow from cpu to gpu", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": []}, {"number": 4218, "title": "seq2seq: Cannot parse tensor from proto: dtype: DT_FLOAT", "body": "My platform is ubuntu 14.04 with tensorflow version of 0.10, CPU version.\n\nI am using functions in `seq2seq.py` to build a model. The code works fine when I use the `embedding_rnn_seq2seq` function, but when I use `embedding_attention_seq2seq`, it has error like this:\n\n`AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'`\n\nTo fix this, I turned the `state_is_tuple` to `false`, but now it has another problem:\n\n```\nW tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_29 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/framework/op_kernel.cc:926] Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\nE tensorflow/core/common_runtime/executor.cc:334] Executor failed to create kernel. Invalid argument: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nE tensorflow/core/client/tensor_c_api.cc:485] Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 730, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 712, in _run_fn\n    status, run_metadata)\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 450, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"main.py\", line 29, in <module>\n    chatbot.main()\n  File \"/mnt/d/DeepQA/chatbot/chatbot.py\", line 208, in main\n    self.sess.run(tf.initialize_all_variables())\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot parse tensor from proto: dtype: DT_FLOAT\ntensor_shape {\n  dim {\n    size: 52772\n  }\n  dim {\n    size: 52767\n  }\n}\nfloat_val: 0\n\n         [[Node: zeros_28 = Const[dtype=DT_FLOAT, value=<Invalid TensorProto: dtype: DT_FLOAT tensor_shape { dim { size: 52772 } dim { size: 52767 } } float_val: 0>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op 'zeros_28', defined at:\n  File \"main.py\", line 29, in <module>\n    chatbot.main()\n  File \"/mnt/d/DeepQA/chatbot/chatbot.py\", line 169, in main\n    self.model = Model(self.args, self.textData)\n  File \"/mnt/d/DeepQA/chatbot/model.py\", line 58, in __init__\n    self.buildNetwork()\n  File \"/mnt/d/DeepQA/chatbot/model.py\", line 118, in buildNetwork\n    self.optOp = opt.minimize(self.lossFct)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 300, in apply_gradients\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/adam.py\", line 118, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/optimizer.py\", line 494, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/slot_creator.py\", line 106, in create_zeros_slot\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/array_ops.py\", line 1131, in zeros\n    output = constant(0, shape=shape, dtype=dtype, name=name)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/constant_op.py\", line 167, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n```\n\nCode is as follows:\n\n```\n# Creation of the rnn cell\nencoDecoCell = tf.nn.rnn_cell.BasicLSTMCell(self.args.hiddenSize, state_is_tuple=False)  # Or GRUCell, LSTMCell(args.hiddenSize)\n#encoDecoCell = tf.nn.rnn_cell.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!)\nencoDecoCell = tf.nn.rnn_cell.MultiRNNCell([encoDecoCell] * self.args.numLayers, state_is_tuple=False)\n\n# Network input (placeholders)\n\nself.encoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(self.args.maxLengthEnco)]  # Batch size * sequence length * input dim\n\nself.decoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(self.args.maxLengthDeco)]  # Same sentence length for input and output (Right ?)\nself.decoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(self.args.maxLengthDeco)]\nself.decoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(self.args.maxLengthDeco)]\n\n# Define the network\n# Here we use an embedding model, it takes integer as input and convert them into word vector for\n# better word representation\ndecoderOutputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n    self.encoderInputs,  # List<[batch=?, inputDim=1]>, list of size args.maxLength\n    self.decoderInputs,  # For training, we force the correct output (feed_previous=False)\n    encoDecoCell,\n    self.textData.getVocabularySize(),\n    self.textData.getVocabularySize(),  # Both encoder and decoder have the same number of class\n    embedding_size=self.args.embeddingSize,  # Dimension of each word\n    output_projection=None,  # Eventually\n    feed_previous=bool(self.args.test)  # When we test (self.args.test), we use previous output as next input (feed_previous)\n)\n```\n", "comments": ["@zhaopku Please add the following information - at least the OS / version info.  FYI this is the default template when you click the green \"New issue\" button on the webpage for this issue.  Thanks!\n\n```\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n### Environment info\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n### What other attempted solutions have you tried?\n\n### Logs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n```\n", "@tatatodd \nOperating System: ubuntu 14.04\nCPU version, no cuda installed\nI installed that from pip3\ntensorflow version: 0.10, link: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl\n\nreproductive sample is just as I mentioned before:\n\n```\nencoDecoCell = tf.nn.rnn_cell.BasicLSTMCell(hiddenSize, state_is_tuple=False)  # Or GRUCell, LSTMCell(hiddenSize)\n#encoDecoCell = tf.nn.rnn_cell.DropoutWrapper(encoDecoCell, input_keep_prob=1.0, output_keep_prob=1.0)  # TODO: Custom values (WARNING: No dropout when testing !!!)\nencoDecoCell = tf.nn.rnn_cell.MultiRNNCell([encoDecoCell] * numLayers, state_is_tuple=False)\n\n\nencoderInputs  = [tf.placeholder(tf.int32,   [None, ]) for _ in range(maxLengthEnco)]  # Batch size * sequence length * input dim\n\ndecoderInputs  = [tf.placeholder(tf.int32,   [None, ], name='inputs') for _ in range(maxLengthDeco)]  # Same sentence length for input and output (Right ?)\ndecoderTargets = [tf.placeholder(tf.int32,   [None, ], name='targets') for _ in range(maxLengthDeco)]\ndecoderWeights = [tf.placeholder(tf.float32, [None, ], name='weights') for _ in range(maxLengthDeco)]\n\ndecoderOutputs, states = tf.nn.seq2seq.embedding_attention_seq2seq(\n    self.encoderInputs,\n    self.decoderInputs, \n    encoDecoCell,\n    self.textData.getVocabularySize(),\n    self.textData.getVocabularySize(),  \n    embedding_size=self.args.embeddingSize, \n    output_projection=None, \n    feed_previous=bool(test)  \n)\n\n\n```\n\nThe first problem of `AttributeError: 'LSTMStateTuple' object has no attribute 'get_shape'`\nwill occur when I set state_is_tuple to true in the first and third line. When I set is to false, I have the second problem, as I mentioned above.\n", "@ebrevdo might have some ideas.\n", "I think @lukaszkaiser knows more about what's going on in the seq2seq code.\n", "I think state_is_tuple should be True, the other option is deprecated. Could you provide the stack trace for that? In particular, where is get_shape() called? (I think it shouldn't be.) Thanks!\n", "@lukaszkaiser Actually the original code is like this:\n\n```\n    with tf.device(getDevice()):\n        model = Model(args, textData)\n\n    writer = tf.train.SummaryWriter(_getSummaryName())\n    saver = tf.train.Saver(max_to_keep=200)  # Arbitrary limit ?\n\n    sess = tf.Session()  # TODO: Replace all sess by sess (not necessary a good idea) ?\n\n    embedding_key_0 = 'embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding:0'\n    embedding_key_1 = 'embedding_rnn_seq2seq/embedding_rnn_decoder/embedding:0'\n\n    var_list = []\n    embedding_list = []\n    cnt = 0\n    for v in tf.all_variables():\n        print(v.name)\n        if v.name != embedding_key_0 and v.name != embedding_key_1:\n            cnt += 1\n            var_list.append(v)\n        else:\n            cnt += 1\n            embedding_list.append(v)\n    print(cnt)\n    print('Initialize variables...')\n\n    sess.run(tf.initialize_all_variables())\n\n\n    print('embedding_0 shape')\n    print(embedding_list[0].get_shape())\n    print('embedding_1 shape')\n    print(embedding_list[1].get_shape())\n\n    print('initializing embeddings from pre-trained word embeddings')\n    embed = loadEmbeddings()\n    embed = np.asarray(embed).reshape([vocab_size, 300])\n\n    assign_op_0 = tf.assign(embedding_list[0], embed)\n    assign_op_1 = tf.assign(embedding_list[1], embed)\n\n\n    sess.run(assign_op_0)\n    sess.run(assign_op_1)\n```\n\nThe code in my first post is part of the `Model`function in the second line. I rewrite the embeddings with pretrained word embeddings.\n", "It looks like you're calling get_shape() on a tuple -- it's not supposed to have a shape, as it's a tuple. You can get all its elements and their shapes by iterating. I'm closing it as it looks fine, but please -- let me know if there is another problem or if get_shape() is called from some library function (it shouldn't).\n", "Sorry, this looks like an unrelated problem with data reading and parsing.", "@lukaszkaiser but if you clearly see the error then it is the same error", "Right - but we solved the underlying cause of that previous problem, which was related to seq2seq and state_is_tuple -- that's what this bug is about. The 5 lines of code you've shown that generate your problem are definitely not related to seq2seq. Maybe you just have some wrong sizes somewhere."]}, {"number": 4217, "title": "the tensorflow binary package in tensorflow r0.10 is build with cudnn5.1 not cudnn4.0", "body": "I use pip to install tf r0.10.\n# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n# Requires CUDA toolkit 7.5 and CuDNN v4. For other versions, see \"Install from sources\" below.\n\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n# Python 2\n\n$ sudo pip install --upgrade $TF_BINARY_URL\n\nWhen I run the program, the program raise error for the cudnn version. \nActually, the binary package is builded with cudnn5.1 and cuda7.5.\n", "comments": ["Yup, our website needs to be pushed but the sources have been updated. \n"]}, {"number": 4216, "title": "Can not load workspace.bzl", "body": "I have a project that I want to build with Bazel, which includes Python code and C++ files. I am trying to use the same approach as in [Syntaxnet](https://github.com/tensorflow/models/tree/master/syntaxnet) from the model zoo. I add `tensorflow` as a submodule to my repository and put the following snippet to my `WORKSPACE` file:\n\n```\nlocal_repository(\n  name = \"org_tensorflow\",\n  path = __workspace_dir__ + \"/tensorflow\",\n)\n\nload('//tensorflow/tensorflow:workspace.bzl', 'tf_workspace')\ntf_workspace(\"tensorflow/\", \"@org_tensorflow\")\n```\n\nwhich results in an error\n\n```\nERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file not found. Unable to load package for '//third_party/gpus:cuda_configure.bzl': BUILD file not found on package path.\n```\n\nThe source of the error is clear to me: [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L3) contains a `load` call with a hard-coded path. SyntaxNet uses an older version of Tensorflow that does not contain this line. \n\nMy question is if this is a bug or not. Is this a recommended way of organizing the project that should be compiled with Tensorflow, and if not, should it be used in Syntaxnet model? Or should I just copy the `WORKSPACE` from Tensorflow's root directory?\n", "comments": ["In addition, I highly suspect that https://github.com/tensorflow/serving is incompatible with the latest Tensorflow for the same reason, see [this line](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/workspace.bzl#L4)\n", "@martinwicke might be able to shed some light on whether this is a bug or not.\n", "These dependent files need to be upgraded to work with TensorFlow at head. @calberti @kirilg you should update the files. I don't think this should be fixed in tensorflow/tensorflow but we'll see.\n\n@damienmg what is the proper way to use TensorFlow as a dependency in a workspace now?\n", "This is the correct way (although the two parameters of tf_worskpace are going away) Unfortuntately, https://github.com/tensorflow/tensorflow/pull/4055 got rolled-back because of an internal breakage. I've rolled it forward as cl/131951691 but it does seems to have hit the github repository yet.\n", "I do see your changes to workspace.bzl and the empty //BUILD file though. I\nthink it is in then.\n\nOn Wed, Sep 7, 2016 at 9:11 AM, Damien Martin-Guillerez <\nnotifications@github.com> wrote:\n\n> This is the correct way (although the two parameters of tf_worskpace are\n> going away) Unfortuntately, #4055\n> https://github.com/tensorflow/tensorflow/pull/4055 got rolled-back\n> because of an internal breakage. I've rolled it forward as cl/131951691\n> https://critique.corp.google.com/#review/131951691 but it does seems to\n> have hit the github repository yet.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4216#issuecomment-245332489,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_fPjAgZfOt8QnW3eas0xSi_31xdDks5qnuI2gaJpZM4J1YGJ\n> .\n", "Oh good let me try then.\n\nOn Wed, Sep 7, 2016 at 6:36 PM Martin Wicke notifications@github.com\nwrote:\n\n> I do see your changes to workspace.bzl and the empty //BUILD file though. I\n> think it is in then.\n> \n> On Wed, Sep 7, 2016 at 9:11 AM, Damien Martin-Guillerez <\n> notifications@github.com> wrote:\n> \n> > This is the correct way (although the two parameters of tf_worskpace are\n> > going away) Unfortuntately, #4055\n> > https://github.com/tensorflow/tensorflow/pull/4055 got rolled-back\n> > because of an internal breakage. I've rolled it forward as cl/131951691\n> > https://critique.corp.google.com/#review/131951691 but it does seems\n> > to\n> > have hit the github repository yet.\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/4216#issuecomment-245332489\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe-auth/AAjO_fPjAgZfOt8QnW3eas0xSi_31xdDks5qnuI2gaJpZM4J1YGJ\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4216#issuecomment-245340342,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADjHf8rEtbrSfXlI6Pc75m9WUPFbcekyks5qnugYgaJpZM4J1YGJ\n> .\n", "Ok I still have a bug on my side.  I will investigate tomorrow and update https://github.com/tensorflow/serving/pull/166 as it goes.\n", "@rizar @damienmg did you ever progress this?\n\nto recreate the problem you can use this file\n\nDockerfile\nhttps://gist.github.com/johndpope/d41a7d6daf8652cbdaff41a2b063c801\n\ndocker build -t syntaxnet .\n\nwhich does all these steps\nhttps://gist.githubusercontent.com/johndpope/fc1c2327a4ae255d9c44dda9b67b5288/raw/371c6dcc8734638981bfc75016b31882e76b90d8/parseyapi.sh\n", "I got it working with some help on this ticket \nhttps://github.com/dsindex/syntaxnet/issues/10\n\nYou can try this build script https://gist.github.com/johndpope/fc1c2327a4ae255d9c44dda9b67b5288\n(just remove the first line  cd / if you're not running on docker)\n\nthe difference being a git checkout commit\n\n# checkout proper version of serving\n\ngit checkout 89e9dfbea055027bc31878ee8da66b54a701a746\ngit submodule update --init --recursive\n", "Looks like the issue was resolved in the end.\r\nClosing."]}, {"number": 4215, "title": "Fix nightly build link in README", "body": "The nightly build link for Linux / GPU / Python 3.5 linked to a wheel for version 0.8.0 for some reason.\n", "comments": ["Can one of the admins verify this patch?\n", "@eagleflo, thanks for your PR! By analyzing the annotation information on this pull request, we identified @caisq, @keveman and @vrv to be potential reviewers\n", "There is a more comprehensive change set soon to be pushed from internal. It covers the issue of obsolete links in this README file, along with the same issue in a few other files. So I am going to close this PR. Thank you all the same.\n"]}, {"number": 4214, "title": "C++ compilation of rule '//tensorflow/core/kernels:gather_nd_op' failed", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nnone\n### Environment info\n\nOperating System:\n\nUbuntu 16.04 64bit\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n# ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Sep  5 22:07 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Sep  5 22:07 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Sep  5 22:07 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Sep  5 22:08 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nI've downloaded these from Nvidia and installed them per \"official\" instructions:\n- cuda_8.0.27_linux.run\n- cuda_8.0.27.1_linux.run\n- cudnn-8.0-linux-x64-v5.1.tgz\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n```\n# git rev-parse HEAD\n2ab7e6326296987ea0ce975afb3434a16d1aa21a\n```\n1. The output of `bazel version`\n\n```\n# bazel version\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n```\n\n```\n# dpkg -l | grep bazel | awk '{print $3}'\n0.3.1\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\n./configure\n\n# Hit ENTER on every question except:\n# Do you wish to build TensorFlow with GPU support? (answer: y)\n# Please specify a list of comma-separated Cuda compute capabilities you want to build with. (answer: 6.1)\n# Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: (answer 8.0)\n```\n### What other attempted solutions have you tried?\n\nTried to google around, hoping it's a known issue. Could not find anything related.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\nGist with last messages during compilation:\n\nhttps://gist.github.com/FlorinAndrei/23ada4fb714776e68c2693502c615305\n", "comments": ["This appears to be caused by an internal compiler error:\n\n```\nERROR: /vagrant/packages/tensorflow/tensorflow/core/kernels/BUILD:371:1: C++ compilation of rule '//tensorflow/core/kernels:gather_nd_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 119 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 431.231s, Critical Path: 406.66s\n```\n", "Thanks for reporting this bug @FlorinAndrei!  I'm closing this out as a duplicate of #2559 that you've already mentioned; support for cuda 8.0 is the main issue.\n"]}, {"number": 4213, "title": "FCNs cannot be used for segmentation of variable size images after training.", "body": "Yann LeCun pointed,  ConvNets don't need to have a fixed-size input.\nOne can train Fully-convolutional network using small images, and than use the model (without scaling or cropping) and feed it with large images for segmentation.\nIt works right away with Keras+Theano.\nIf one trains the same model with TensorFlow as Keras'es backend, during application of pretrained model on image larger than the images used for training, he will get A ValueError due to tensor shape mismatch (same for manual model definition directly in TF).\n\nThe shape check needs to be disabled and the convolution layers need to work properly with variable size during model using/testing phase if the framework is to be competitive.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttps://www.facebook.com/yann.lecun/posts/10152820758292143\nhttp://stackoverflow.com/questions/39050557/fully-convolutional-network-training-image-size\nhttps://www.quora.com/How-does-the-conversion-of-last-layers-of-CNN-from-fully-connected-to-fully-convolutional-allow-it-to-process-images-of-different-size \nhttp://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-pixels.pdf\n### Environment info\n\nOperating System:  Ubuntu 14.04.4 LTS \nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n[cuda_config.txt](https://github.com/tensorflow/tensorflow/files/455666/cuda_config.txt)\n### Error log:\n\nValueError: Cannot feed value of shape (1, 3, 350, 600) for Tensor u'convolution2d_input_2:0', which has shape '(?, 3, 15, 15)'\n", "comments": ["Thanks for the note @Huxwell!\n\n@sherrym might have an opinion on this, or know the right person to redirect this to.\n", "It seems to me you can simply use a placeholder with undefined shape at least this is what I am doing and it works:  x=tf.placeholder(tf.float32,[None,None,None,3],name=\"input_network\"). Not very memory efficient I suppose but it works.\n", "But possibly not if different sized images are in the same batch, never tried.\n", "However for the first part of your question it works as you use same shape inputs for training batches. It is only during inference that you feed different sized batches.\n", "I am testing it right now, rewriting my models from keras to tensorflow, but it's gonna take some time due to CUDNN problems, and I'll post results as soon as I have them\n", "@Huxwell how did that go?", "Changing input size manually in the exported .json file with the model architecture and using it with the same weights in different file works OK.", "Great to hear!"]}, {"number": 4212, "title": "How to implement Multiplexer", "body": "How to takes a single input and select one of many output lines, which is connected to the input, by using placeholder?\n\nFor example if we have a TF variable as follow, how could we forward it to the 1st output ?\n\n```\nintputs = tf.constant(0.2, shape=[2, 2])\nsel = tf.placeholder(tf.int32)\nn_out = 3\nout = xxxx(sel, inputs, n_out)  # here\n\nsess.run(out, feed_dict = {sel: 0}) \n# [[[0.2, 0.2],[0.2, 0.2]], \n   [[0, 0],[0, 0]], \n    [[0, 0],[0, 0]]]\n\n```\n", "comments": ["We primarily use github issues to track bugs and installation issues.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n"]}, {"number": 4211, "title": "Removing deprecated module", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@ofirpress, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @ebrevdo and @benoitsteiner to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! (but with a different email, hopefully thats not a problem...)\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please\n"]}, {"number": 4210, "title": "deepdream tutorial rename_nodes error", "body": "Hi -\n    The tensorflow/examples/deepdream README.md says that Python 2.7 is required. However, loading the deepdream.ipynb notebook fires up the 3.5 kernel (due to info I believe is stored in the notebook itself. \n\nBut the error I am encountering on the line:\ntmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n\nmay or may not be due to a python version issue. The line generates the error:\nTypeError: '<stripped 37632 bytes>' has type <class 'str'>, but expected one of: (<class 'bytes'>,)\n\nI'm running Mac OS X 10.10.5, python 2.7 for the Mac, and 3.5 for everything else. \nBest,\n            - lonce\n", "comments": ["I'm not sure about the Python 2.7 / 3.5 issue, or whether that's related either.\n\nPlease provide the following additional info.  FYI this also appears in the default template when you click on the green \"New issue\" button at the top of the webpage for this issue.\n\n```\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n```\n\nAlso - what do you mean by \"everything else\" in \"...python 2.7 for the Mac, and 3.5 for everything else.\"\n", "Hi tatatodd,\n\n> 1. A link to the pip package you installed:\n>    https://www.python.org/ftp/python/3.5.2/python-3.5.2-macosx10.6.pkg\n> 2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n>    https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0-py3-none-any.whl \n> \n> Also - what do you mean by \"everything else\" in \"...python 2.7 for the Mac, and 3.5 for everything else.\"\n> I mean I also have python 2.7 installed (I understand it is needed by OSX for something), but for my own development, I use 3.5.\n\nThis may be more info than you need, by the traceback for the error follow: \nTypeError                                 Traceback (most recent call last)\n<ipython-input-4-f63c00409564> in <module>()\n     56 # internal structure. We are going to visualize \"Conv2D\" nodes.\n     57 tmp_def = rename_nodes(graph_def, lambda s:\"/\".join(s.split('_',1)))\n---> 58 show_graph(tmp_def)\n\n<ipython-input-4-f63c00409564> in show_graph(graph_def, max_const_size)\n     35     if hasattr(graph_def, 'as_graph_def'):\n     36         graph_def = graph_def.as_graph_def()\n---> 37     strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n     38     code = \"\"\"\n     39         <script>\n\n<ipython-input-4-f63c00409564> in strip_consts(graph_def, max_const_size)\n     18             size = len(tensor.tensor_content)\n     19             if size > max_const_size:\n---> 20                 tensor.tensor_content = \"<stripped %d bytes>\"%size\n     21     return strip_def\n     22 \n\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/python_message.py in field_setter(self, new_value)\n    662     # Testing the value for truthiness captures all of the proto3 defaults\n    663     # (0, 0.0, enum 0, and False).\n--> 664     new_value = type_checker.CheckValue(new_value)\n    665     if clear_when_set_to_default and not new_value:\n    666       self._fields.pop(field, None)\n\n/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/google/protobuf/internal/type_checkers.py in CheckValue(self, proposed_value)\n    106       message = ('%.1024r has type %s, but expected one of: %s' %\n    107                  (proposed_value, type(proposed_value), self._acceptable_types))\n--> 108       raise TypeError(message)\n    109     return proposed_value\n    110 \n\nTypeError: '<stripped 37632 bytes>' has type <class 'str'>, but expected one of: (<class 'bytes'>,)\n\nThank you,\n                      - lonce\n", "Can you manually change the ipynb's kernel to your py2.7 kernel spec (the jupyter menu)? If you don't have a py2.7 kernel spec you can create one using the standard jupyter techniques (google search). You may want to use virtualenv to isolate your tensorflow related setup.\n", "DeepDream almost works with Python 3.5. I've submitted a pull request in #4366 but until then, you can just change the line in strip_consts to:\n\n```\ntensor.tensor_content = bytes(\"<stripped %d bytes>\"%size, 'utf-8')\n```\n\nPython 3.5 seems to be stricter about implicit conversions, in this case refusing to convert string -> byte without knowing the encoding (which defines the conversion,)\n", "Sweet! That did the trick, Matthias.\nThanks to all.\n", "Python 2.7 didn't like that. Needed to revert the line of code to get it to run."]}, {"number": 4209, "title": "tf.concat returns a list not Tensor, is it a bug?", "body": "My code is below\n\n```\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport shutil\nimport os\n\nclass Model(object):\n    def __init__(self, input_length, input_dim=10, filter_nums=[3,4,5], window_sizes=[2,3,4]):\n        self.filter_nums = filter_nums\n        self.window_sizes = window_sizes\n\n        # placeholder\n        self._inputs = tf.placeholder(tf.float32, shape=[None, input_dim,\n        input_length], name=\"inputs\")\n\n    def _conv2d(self, inputs, filter_num, window_size, init_scale=0.1, data_format=\"NHWC\", name=\"conv2d\"):\n        # Implement a conv2d for nlp task, which in_heigth = filter_height.\n        # assume inputs dim is 4.\n        with tf.variable_scope(name):\n            if data_format == \"NHWC\":\n                filter_height = int(inputs.get_shape()[1])\n            else:\n                filter_height = int(inputs.get_shape()[2])\n            in_channels = int(inputs.get_shape()[-1])\n            W = tf.get_variable(\"W\", shape=[filter_height, window_size,\n            in_channels, filter_num],\n            initializer=tf.random_uniform_initializer(-init_scale, init_scale))\n            conv = tf.nn.conv2d(inputs, W, [1,1,1,1], \"VALID\")\n            return conv\n\n    def _max_pool(self, value, name=\"max_pool\"):\n        with tf.variable_scope(name):\n            pool_width = int(value.get_shape()[-2])\n            maxpool = tf.nn.max_pool(value, [1,1,pool_width,1], [1,1,1,1], \"VALID\")\n            return maxpool\n\n    def multi_conv_pool(self):\n        pooled = []\n        expanded_inputs = tf.expand_dims(self._inputs, -1)\n        for filter_num, window_size in zip(self.filter_nums,\n        self.window_sizes):\n            conv = self._conv2d(expanded_inputs, filter_num, window_size,\n            name=\"conv2d_width_%d\" % window_size)\n            maxpool = self._max_pool(conv)\n            pooled.append(maxpool)\n        concat_pooled = tf.concat(3, pooled)\n        return concat_pooled\n\n    @property\n    def inputs(self):\n        return self._inputs\n\ndef main():\n    logdir = 'logdir'\n    if os.path.exists(logdir):\n        shutil.rmtree(logdir)\n\n    model = Model(input_length=9, input_dim=4)\n    concat_pooled = model.multi_conv_pool()\n\n    data = np.random.randn(3, 4, 9)\n\n    sess = tf.Session()\n\n    summary_writer = tf.train.SummaryWriter(logdir, graph=sess.graph)\n\n    sess.run(tf.initialize_all_variables())\n    outputs = sess.run([concat_pooled], feed_dict={model.inputs: data})\n    print(\"data(shape = (3, 4, 9)):\")\n    print(data)\n    print(\"after multi conv and pool, its output should be (3, 1, 1, 12):\")\n    print(outputs)\n    print(\"outputs' type:\", type(outputs))\n\nif __name__ == '__main__':\n    main()\n\n```\n\nRun it and gets a list. `outputs` here is the return value of tf.concat(). It should be an array but now it's a list. Could anyone tell me why? Thanks!\n", "comments": ["Solved it. Here is the explanation from `tf.Session().run()` doc.\n\n> Returns:\n>   Either a single value if `fetches` is a single graph element, or\n>   a list of values if `fetches` is a list, or a dictionary with the\n>   same keys as `fetches` if that is a dictionary (described above).\n"]}, {"number": 4208, "title": "Batch Normalization slow?", "body": "I am using tf.contrib.layers.batch_norm\nThis is very slow. Any better implementation?\n", "comments": ["We primarily use github issues for bugs and installation issues.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n\nFYI you might be interested in https://github.com/tensorflow/models/issues/325, which says that 0.10.0rc0 has some known performance regressions which should be fixed by the actual release.\n"]}, {"number": 4207, "title": "Feature request: Ogg (or equivalent) compression for AudioSummary with TensorBoard", "body": "When logging an AudioSummary every couple of epochs, the logs grow quickly (as in several gigabytes), and checking TensorBoard remotely (as in not on the same LAN) also becomes very slow. Would it be possible to have builtin support for Ogg compression or an equivalent lossy format? There are multiple use cases where this would be beneficial.\n", "comments": ["Or in TensorBoard at least, maybe just download an audio file upon hitting play (or just the most recent AudioSummary per default), instead of loading everything?\n", "This is certainly an issue -- for those hitting it here are some workarounds until we get some better codecs in TensorFlow:\r\n\r\n- Use ```tf.summary.audio```'s ```max_outputs``` argument to limit the number of WAVs you're writing to your events file per summary run.\r\n- If using ```tf.Supervisor```, increase your ```save_summaries_secs``` to write summaries less frequently. ", "IMHO a lossless codec like FLAC would greatly improve the situation without degrading the quality of the synthesized audio.", "This is certainly a reasonable suggestion. I've migrated it to our new repository at https://github.com/tensorflow/tensorboard/issues/64. Please feel free to submit PRs there, too!"]}, {"number": 4206, "title": "transient error:  Enqueue operation was cancelled", "body": "[Related issue](http://stackoverflow.com/questions/38678371/tensorflow-enqueue-operation-was-cancelled) seems to still hapen in 0.10rc0\nOperating System: Ubuntu 15.04\nInstalled version of CUDA and cuDNN: CUDA 7.5, cuDNN 4\noutput of `ls -l /path/to/cuda/lib/libcud*`:\n\n```\n-rw-r--r-- 1 root root   322936 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 iul  5 16:00 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 59909104 iul  5 16:01 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.5\n-rwxr-xr-x 1 root root 59909104 iul  5 16:01 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 62025862 iul  5 16:09 /usr/local/cuda-7.5-cuddn4/targets/x86_64-linux/lib/libcudnn_static.a\n```\n\nInstalled tensorflow 0.10 with GPU, for python 2.7 in virtualenv with pip\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl\n0.10.0rc0\n\n[Here's a repo with a self contained reproducible example](https://github.com/titusnicolae/tf-issue/commit/7c13aa79550b6c89701fcc5e3ebf1c46effd0c6f)\ntfwriter.py creates the range.tfrecord file with numbers from 0 to 9 as TFRecords\nWhen running tfreader.py I get one of the following outputs:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.46GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\n(0, ['range.tfrecord:0', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x00'])\n(1, ['range.tfrecord:32', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x01'])\n(2, ['range.tfrecord:64', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x02'])\n(3, ['range.tfrecord:96', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x03'])\n(4, ['range.tfrecord:128', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x04'])\n(5, ['range.tfrecord:160', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x05'])\n(6, ['range.tfrecord:192', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x06'])\n(7, ['range.tfrecord:224', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x07'])\n(8, ['range.tfrecord:256', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x08'])\n(9, ['range.tfrecord:288', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\t'])\n(10, ['range.tfrecord:0', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x00'])\n(11, ['range.tfrecord:32', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x01'])\n(12, ['range.tfrecord:64', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x02'])\n(13, ['range.tfrecord:96', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x03'])\n(14, ['range.tfrecord:128', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x04'])\n(15, ['range.tfrecord:160', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x05'])\n(16, ['range.tfrecord:192', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x06'])\n(17, ['range.tfrecord:224', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x07'])\n(18, ['range.tfrecord:256', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x08'])\n(19, ['range.tfrecord:288', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\t'])\n```\n\nor\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2405\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.46GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\n(0, ['range.tfrecord:0', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x00'])\n(1, ['range.tfrecord:32', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x01'])\n(2, ['range.tfrecord:64', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x02'])\n(3, ['range.tfrecord:96', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x03'])\n(4, ['range.tfrecord:128', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x04'])\n(5, ['range.tfrecord:160', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x05'])\n(6, ['range.tfrecord:192', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x06'])\n(7, ['range.tfrecord:224', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x07'])\n(8, ['range.tfrecord:256', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x08'])\n(9, ['range.tfrecord:288', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\t'])\n(10, ['range.tfrecord:0', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x00'])\n(11, ['range.tfrecord:32', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x01'])\n(12, ['range.tfrecord:64', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x02'])\n(13, ['range.tfrecord:96', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x03'])\n(14, ['range.tfrecord:128', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x04'])\n(15, ['range.tfrecord:160', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x05'])\n(16, ['range.tfrecord:192', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x06'])\n(17, ['range.tfrecord:224', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x07'])\n(18, ['range.tfrecord:256', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\x08'])\n(19, ['range.tfrecord:288', '\\n\\x0e\\n\\x0c\\n\\x03int\\x12\\x05\\x1a\\x03\\n\\x01\\t'])\nE tensorflow/core/client/tensor_c_api.cc:485] FIFOQueue '_0_input_producer' is closed.\n     [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueMany[Tcomponents=[DT_STRING], _class=[\"loc:@input_producer\"], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n```\n", "comments": ["Thanks for the repro instructions @titusnicolae!  Contributions are welcome!\n", "seems ok now, I use tf version 0.11."]}, {"number": 4205, "title": "Misleading argument name on seq2seq module", "body": "Not really major issue, but can make lose some time.\n\nFor the functions [sequence_loss_by_example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L980) and [sequence_loss](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L1026), the name of the parameter `softmax_loss_function` seems to imply that only softmax based loss are compatibles which is not true (for instance `tf.nn.sigmoid_cross_entropy_with_logits` works too). Maybe the name was that just to indicate the default behavior but I find it misleading.\n\nIt shouldn't be really difficult to change (as there are very few references in the repository). I can send a pull request if needed.\n", "comments": []}, {"number": 4204, "title": "[SOLVED] Using opencv with tensorflow causes segfault.", "body": "Using opencv with tensorflow causes segfault.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n[1]    12012 segmentation fault (core dumped)  ./test.py\n```\n### Related GitHub issues or StackOverflow:\n\nhttp://stackoverflow.com/questions/37535765/segmentation-fault-core-dumped-error-for-cifar10-example-tensorflow\nhttps://github.com/tensorflow/tensorflow/issues/1924\n### Environment info\n\n1 Operating System:\nArch linux\n\n```\n$ uname -a\nLinux justes_notebook 4.7.2-1-ARCH #1 SMP PREEMPT Sat Aug 20 23:02:56 CEST 2016 x86_64 GNU/Linux\n```\n\n2 CUDA\ncuda 7.5.18\ncudnn 5.1.3\n\n```\n$ ls -l /opt/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 jul 12 02:19 /opt/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 jul 12 02:19 /opt/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 jul 12 02:19 /opt/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 jul 12 02:19 /opt/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 jul 12 02:19 /opt/cuda/lib/libcudart_static.a\n```\n\n3 The commit hash \n2ab7e6326296987ea0ce975afb3434a16d1aa21a\n\n4 bazel\n\n```\n$ bazel version\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: \u0421\u0440 \u0430\u0432\u0433 24 04:26:11 2016 (1472012771)\nBuild timestamp: 1472012771\nBuild timestamp as int: 1472012771\n```\n\n5 python \n\n```\n$ python --version \nPython 3.5.2\n```\n\n6 opencv\nopencv 3.1.0\n### Example\n\n```\n#!/usr/bin/python\n\nimport cv2\nimport tensorflow as tf\n\nimg = cv2.imread('im.jpg')\nimg = cv2.resize(img, (100,100))\nsess = tf.Session()\n```\n", "comments": ["gdb says about segfault in pthread_mutex_lock () from /usr/lib/libpthread.so.0\nI've tried to downgrade glibc from 2.24-2 to 2.23-5 (since it all began after system upgrade), but it doesn't help.\n\n```\n$ gdb -ex r --args python test.py  \nGNU gdb (GDB) 7.11.1\nCopyright (C) 2016 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from python...(no debugging symbols found)...done.\nStarting program: /usr/bin/python test.py\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/usr/lib/libthread_db.so.1\".\n[New Thread 0x7fffda3a7700 (LWP 6063)]\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n[New Thread 0x7fffb978d700 (LWP 6068)]\n[New Thread 0x7fffb8f8c700 (LWP 6069)]\n[New Thread 0x7fffb3fff700 (LWP 6070)]\n[New Thread 0x7fffb37fe700 (LWP 6071)]\n[New Thread 0x7fffb2ffd700 (LWP 6072)]\n[New Thread 0x7fffb27fc700 (LWP 6073)]\n[New Thread 0x7fffb1ffb700 (LWP 6074)]\n[New Thread 0x7fffb17fa700 (LWP 6075)]\n[New Thread 0x7fffb0ff9700 (LWP 6077)]\n[New Thread 0x7fff97fff700 (LWP 6078)]\n[New Thread 0x7fff977fe700 (LWP 6079)]\n[New Thread 0x7fff96ffd700 (LWP 6080)]\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 960M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\npciBusID 0000:01:00.0\nTotal memory: 1.96GiB\nFree memory: 1.93GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\n[New Thread 0x7fff967fc700 (LWP 6081)]\n[New Thread 0x7fff95ffb700 (LWP 6082)]\n[New Thread 0x7fff957fa700 (LWP 6083)]\n[New Thread 0x7fff94ff9700 (LWP 6084)]\n[New Thread 0x7fff7a441700 (LWP 6085)]\n[New Thread 0x7fff79c40700 (LWP 6086)]\n[New Thread 0x7fff7943f700 (LWP 6087)]\n[New Thread 0x7fff78c3e700 (LWP 6088)]\n[New Thread 0x7fff63fff700 (LWP 6089)]\n[New Thread 0x7fff637fe700 (LWP 6090)]\n\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007ffff76f7ad0 in pthread_mutex_lock () from /usr/lib/libpthread.so.0\n\n\n```\n", "Downgrading opencl-nvidia from 370.23 to 367.35  have solved the problem.\n", "Thanks for the self-debugging and documentation of the fix you found!  Please re-open if you run into further issues.\n"]}, {"number": 4203, "title": "BUG in graph_actions.py", "body": "Hello I am using `tf.contrib.learn.Estimator` to train my model, and size of validation sample is small,(validation sample size=32, eval_batch_size=16)\n\nI have created an EvaluationMonitor to evaluate the validation sample\n\n```\nclass EvaluationMonitor(tf.contrib.learn.monitors.EveryN):\n        def every_n_step_end(self, step, outputs):\n            eval_results = self._estimator.evaluate(\n                input_fn=input_fn_eval,\n                metrics=eval_metrics,\n                steps=None)\n            print \"Evaluation Result: %s\" % eval_results\n```\n\nand the `eval_metrics` is is a list of `tf.contrib.metrics.streaming_sparse_recall_at_k`, `input_fn_eval` is a function to read TFRecord from file\n\nbut in the evaluation step, I always get the exception:\n\n> File \"...../tensorflow/contrib/learn/python/learn/graph_actions.py\", line 610, in _eval_results_to_str\n>     return ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())\n\nWhen i looks inside the evaluate method(Line 632- Line 779) in the source of [`graph_actions.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L632)\n\n```\ndef evaluate(graph,\n             output_dir,\n             checkpoint_path,\n             eval_dict,\n             update_op=None,\n             global_step_tensor=None,\n             supervisor_master='',\n             log_every_steps=10,\n             feed_fn=None,\n             max_steps=None):\n......\ntry:\n      try:\n        while (max_steps is None) or (step < max_steps):\n          step += 1\n          start_time = time.time()\n          feed_dict = feed_fn() if feed_fn is not None else None\n          if update_op is not None:\n            session.run(update_op, feed_dict=feed_dict)\n          else:\n            eval_results = session.run(eval_dict, feed_dict=feed_dict)\n            eval_step = step\n\n          if step % log_every_steps == 0:\n            if eval_step is None or step != eval_step:\n              eval_results = session.run(eval_dict, feed_dict=feed_dict)\n              eval_step = step\n            duration = time.time() - start_time\n            logging.info('Results after %d steps (%.3f sec/batch): %s.',\n                         step, float(duration),\n                         _eval_results_to_str(eval_results))\n      finally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n\n......\n\n # Save summaries for this evaluation.\n  _write_summary_results(output_dir, eval_results, current_global_step)\n\n```\n\nwhen my code call the function,   the `max step=2`(validation sample size=32, eval_batch_size=16) will be less than the default parameter( `log_every_steps=10`), therefore the code only can run the evaluation in the `finally` block, \n\n```\n ....\nfinally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n...\n```\n\nbut for the metrics of t`tf.contrib.metrics.streaming_sparse_recall_at_k,` the `update_op` is not Not, which means the line\n\n```\nif update_op is not None\n    session.run(update_op, feed_dict=feed_dict)\n```\n\nwill alway be invoked, and the batched_data in the queue will be _empty_ when the code jump into `finally` block, therefore an exception of `OutOfRangeError` will be raised, and `eval_results` will be None when the code execute at line\n\n```\n_write_summary_results(output_dir, eval_results, current_global_step)\n```\n\nI think it is caused by the default value of `log_every_steps`, when the `_evaluate_model` method in `tf.contrib.learn.Estimator`  invoke the method in line [679](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L673),\n\n```\neval_results, current_global_step = graph_actions.evaluate(\n          graph=g,\n          output_dir=eval_dir,\n          checkpoint_path=checkpoint_path,\n          eval_dict=eval_dict,\n          update_op=update_op,\n          global_step_tensor=global_step,\n          supervisor_master=self._config.master,\n          feed_fn=feed_fn,\n          max_steps=steps)\n\n```\n\nit did not set the value of `log_every_steps` and  the default value of   `log_every_steps` will be 10, so the exception will be raised when the validation sample size is small (or total evaluation steps is less than 10)\n\nWhen I set the hyper parameter `eval_batch_size` to a small value (eg. 2, which ensure total evaluation 32/2 = 16 > 10), my code is ok\n", "comments": ["Thanks for filing this bug @david-liu!  \n\nPlease add the below information to your bug, to help us investigate.  FYI this text is included in the default template for new issues when you click on the green \"New issue\" button at the top of this page.\n\nAlso - have you tried running on the latest 0.10 release?\n\n```\n### Environment info\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nIf installed from source, provide \n\n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n### What other attempted solutions have you tried?\n\n### Logs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n```\n", "Hello, Thank you response, I have run the code on the latest 0.10 release,\n\nI think we can test with the source code of [A RETRIEVAL-BASED MODEL IN TENSORFLOW](https://github.com/dennybritz/chatbot-retrieval/) , after you download the data for google driver, \nand replace the validation.tfrecords with another validation tensorflow record with small size(2 validation sample,and I have created a r[epository](https://github.com/david-liu/validationTFRecord) to store this file, you can download it directly), and run the training process\n\npython udc_train.py\n\nyou will get the error\n", "@david-liu let's try to narrow down the problem first.  Can you try to fill out the full information that I requested?  E.g. the git commit hash you're installing from source, and other info.  Also please attach the full error log, including the full stack trace.\n\nNote that log_every_steps just configures when logging occurs, and logging more or less often shouldn't affect your results.  Perhaps you should look into why you get an exception during logging, as that sounds like the root cause of your problem.\n", "I'm having almost the exact same problem, but for a slightly different reason.\n\nI'm using `graph_actions.evaluate` to evaluate my mode (through `BaseEstimator`, through `ValidationMonitor`), and I'm using an `input_fn` backed by a reader. I only want to run through my test data once, so I set `num_epochs=1`. I have 260 vectors in my test set, and `batch_size=100`, so as you'd expect, I get back 100, 100, 60 (because `allow_smaller_final_batch=True`), and then the next time  I get an `OutOfRangeError` which looks like it reproduces almost exactly the same problem.\n\nI'm running about 0.10. I haven't had the time to try against master, but it seems like the relevant code hasn't changed there.\n\nThe line that causes the exception is:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L754\n\n```\n      finally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict) # <---- This is the culprit.\n          eval_step = step\n        # Stop session first, before queue runners.\n        session.close()\n```\n\nThis means that `eval_results` never gets set, and the session doesn't even get closed.\n\nThe exception handler catches it just fine at:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L768\n\n```\n    except errors.OutOfRangeError as e:\n      if max_steps is None:\n        logging.info('Input queue is exhausted.')\n      else:\n        logging.warn('Input queue is exhausted: %s.', e)\n```\n\n...but this means that the call at:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L781\n\n```\n  _write_summary_results(output_dir, eval_results, current_global_step)\n```\n\ngets `None` for `eval_results`, which then causes problems at...\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L612\n\n```\n  return ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())\n```\n\nbecause you can't call `.items()` on `None`.\n", "This looks like the guilty commit:\nhttps://github.com/tensorflow/tensorflow/commit/54cf1600b98127d5076def3c567cda1e087c1761\n\nBasically, it seems like if you ever hit either of the 2 except clauses because your `eval_dict` throws the matching exceptions, you're going to then get an exception from `_write_summary_results`.\n", "@ilblackdragon can you comment on this?  Thanks!\n", "Is this still current?", "Closing due to inactivity. Feel free to re-open if you would like us to look again."]}, {"number": 4202, "title": "undefined reference to `tensorflow::CanUseDeepConv2D(...)' building with Makefile method", "body": "Building tensorflow with makefile method gets stuck at \n\n```\ngcc --std=c++11 -DIS_SLIM_BUILD -O0 -I. -I/home/tensorflow/tensorflow/contrib/makefile/downloads/ -I/home/tensorflow/tensorflow/contrib/makefile/downloads/eigen-eigen-9e1b48c333aa -I/home/tensorflow/tensorflow/contrib/makefile/gen/proto/ -I/home/tensorflow/tensorflow/contrib/makefile/gen/proto_text/ -I/usr/local/include -I/usr/local/include/c++/4.9.4/ -I/usr/local/include/c++/4.9.4/arm-linux-gnueabihf/ \\\n-o /home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/core/util/reporter.o /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model.o /home/tensorflow/tensorflow/contrib/makefile/gen/obj/tensorflow/tools/benchmark/benchmark_model_main.o \\\n -Wl,--allow-multiple-definition -Wl,--whole-archive /home/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a -L/usr/local/lib -lstdc++ -lprotobuf -lz -lm -ldl -lpthread\n/home/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(conv_ops.o): In function `tensorflow::LaunchDeepConvOp<Eigen::ThreadPoolDevice, float>::Run(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, int, int, int, int, int, int, int, int, int, int, int, int, int, tensorflow::Tensor*, tensorflow::TensorFormat)':\nconv_ops.cc:(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x58): undefined reference to `tensorflow::CanUseDeepConv2D(int, int, int, int, int, int, int, int)'\nconv_ops.cc:(.text._ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE[_ZN10tensorflow16LaunchDeepConvOpIN5Eigen16ThreadPoolDeviceEfE3RunEPNS_15OpKernelContextERKNS_6TensorES8_iiiiiiiiiiiiiPS6_NS_12TensorFormatE]+0x174): undefined reference to `tensorflow::functor::DeepConv2D<Eigen::ThreadPoolDevice, float>::operator()(tensorflow::OpKernelContext*, tensorflow::Conv2DArgs const&, float const*, float const*, float*)'\ncollect2: error: ld returned 1 exit status\ntensorflow/contrib/makefile/Makefile:485: recipe for target '/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed\nmake: *** [/home/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\n```\n", "comments": ["Duplicate of #4172 \n"]}, {"number": 4201, "title": "After I used SyncReplicasOptimizer, tf hang at sess.run ", "body": "After I used SyncReplicasOptimizer and chief queue runner in distribute tensorflow, it hangs at sess.run.\nBut after I added below:\n        queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n        sv.start_queue_runners(sess, queue_runners)  \nthis queue runs OK.\nAnyone can tell me why?\nI added this code before:\n      if is_chief:\n          sv.start_queue_runners(sess, chief_queue_runner)\n          sess.run(init_token_op)\n", "comments": ["We primarily use github issues to track bugs and installation issues.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n\nSome links that you may find helpful:\nhttps://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html\nhttp://stackoverflow.com/questions/38732502/tensorflow-master-and-worker-service/38749421#38749421\n"]}, {"number": 4200, "title": "TypeError: __init__() got an unexpected keyword argument 'dtype'", "body": "The module is `models/rnn/translate`\nI'm running\n\n```\n$ python -c 'import tensorflow as tf; print tf.__version__'\n0.10.0rc0\n```\n\nI did the training:\n\n```\npython translate.py --data_dir /mnt/ft1/translate/ --train_dir /mnt/ft1/translate/ --size=256 --num_layers=2 --steps_per_checkpoint=50\n```\n\nand then run\n\n```\npython translate.py --decode --data_dir /mnt/ft1/translate/ --train_dir /mnt/ft1/translate/\n```\n\n```\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"translate.py\", line 285, in main\n    decode()\n  File \"translate.py\", line 222, in decode\n    model = create_model(sess, True)\n  File \"translate.py\", line 131, in create_model\n    dtype=dtype)\nTypeError: __init__() got an unexpected keyword argument 'dtype'\n```\n\nMy data dir looks like\n\n```\n-rw-rw-r-- 1 ubuntu ubuntu   21393583 Aug 31 15:21 dev-v2.tgz\n-rw-rw-r-- 1 ubuntu ubuntu 3789873031 Aug 31 15:21 giga-fren.release2.en\nlrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:17 giga-fren.release2.en.gz -> giga-fren.release2.fixed.en.gz\n-rw-rw-r-- 1 ubuntu ubuntu 1214224978 Aug 30 19:55 giga-fren.release2.fixed.en.gz\n-rw-rw-r-- 1 ubuntu ubuntu 1380871453 Aug 29 21:43 giga-fren.release2.fixed.fr.gz\n-rw-rw-r-- 1 ubuntu ubuntu 4565271815 Aug 31 15:19 giga-fren.release2.fr\nlrwxrwxrwx 1 ubuntu ubuntu         30 Aug 31 15:13 giga-fren.release2.fr.gz -> giga-fren.release2.fixed.fr.gz\n-rw-rw-r-- 1 ubuntu ubuntu 2380084216 Sep  2 11:29 giga-fren.release2.ids40000.en\n-rw-rw-r-- 1 ubuntu ubuntu 1222033408 Aug 31 17:17 giga-fren.release2.ids40000.fr\n-rw-r--r-- 1 ubuntu ubuntu     332974 Dec 13  2013 newstest2013.en\n-rw-r--r-- 1 ubuntu ubuntu     393465 Dec 13  2013 newstest2013.fr\n-rw-rw-r-- 1 ubuntu ubuntu     231941 Sep  2 11:29 newstest2013.ids40000.en\n-rw-rw-r-- 1 ubuntu ubuntu     268151 Sep  2 11:29 newstest2013.ids40000.fr\n-rw-rw-r-- 1 ubuntu ubuntu 2595102720 Aug 31 10:53 training-giga-fren.tar\n-rw-rw-r-- 1 ubuntu ubuntu     343512 Aug 31 16:54 vocab40000.en\n-rw-rw-r-- 1 ubuntu ubuntu     380736 Aug 31 16:11 vocab40000.fr\n```\n\nThe `symlink` in the `data_dir` are due to https://github.com/tensorflow/tensorflow/issues/4122\n", "comments": ["Somehow you've ended up with inconsistent versions of (at least) these two files:\n\n```\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py\n```\n\nNote the following commit on Aug 10, which added the dtype argument to the seq2seq_model.Seq2SeqModel constructor:\nhttps://github.com/tensorflow/tensorflow/commit/0e91b8b4db9c40bc8c45bcc69dd18e03d9f898a3\n\nBased on the stack trace and error, it seems that your version of `translate.py` is after the Aug 10 commit, while your version of `seq2seq_model.py` is before the Aug 10 commit.  Note that version 0.10 was before the Aug 10 commit; I'm not sure how your files got out-of-sync.\n\nPlease re-open if this doesn't resolve your problem!\n", "Hello,\n\nI took latest code from github and still I am getting this error,\n\nPreparing WMT data in /tmp\nCreating 3 layers of 1024 units.\nTraceback (most recent call last):\n  File \"translate.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"translate.py\", line 287, in main\n    train()\n  File \"translate.py\", line 152, in train\n    model = create_model(sess, False)\n  File \"translate.py\", line 131, in create_model\n    dtype=dtype)\nTypeError: **init**() got an unexpected keyword argument 'dtype'\n\nAs per your previous comments, I cross check my seq2seq_model.py and translate.py, both files are latest and have required changes to avoid this error.\n\nStill I am facing this issue, can you look into this issue and re-open it?\n\nFYI,\nMy tensorflow revision is 0.10, I am using python-2.7.\nWhat is the latest release revision of tensorflow? I will try updating tensorflow to latest revision and can check again.\n", "@gaurav5670 I think it' the same error I get in my tests.\n", "@loretoparisi , yes it is the same error, I commented \"dtype=dtype\" at line 131 in translate.py file, \nthe error is gone, I am still running the tests. Will update again if I face any issues.\n\nFYI,\nIf you open seq2seq_model.py and check in construtor of Seq2SeqModel, it assign a default value to dtype at line number 59, \ndef **init**(self,\n               source_vocab_size,\n               target_vocab_size,\n               buckets,\n               size,\n               num_layers,\n               max_gradient_norm,\n               batch_size,\n               learning_rate,\n               learning_rate_decay_factor,\n               use_lstm=False,\n               num_samples=512,\n               forward_only=False,\n               dtype=tf.float32):\n\nSo, even if we comment line 131 (\"dtype=dtype\") in translate.py file the tests should run.\n\n@tatatodd, can you please reopen this issue? As it persists with latest tensorflow github code also.\n", "@loretoparisi @gaurav5670 Can you provide the git commit hash of your sources?  `git rev-parse HEAD`\n", "@tatatodd I have downloaded complete tensorflow source as a zip file 2 days before. \nToday morning I cloned it as a repo on my machine and my git commit hash is ca1aa4ad2d4e011e8479319e10d73281a50f7560.\n\nLet me try to train the model in this git clone and check if issue is there or not.\n", "@tatatodd I have tested with git commit hash ca1aa4ad2d4e011e8479319e10d73281a50f7560 and still the problem is present. Getting the same error.\n", "@loretoparisi @gaurav5670\n\nI believe this is the problem.  You've installed tensorflow from source in some location (say dir A), but you also have a default version of tensorflow installed under some other location (say dir B).  Then you run translate.py directly from the sources under A, but imports are being resolved from B.  Your sources under A are up-to-date, but the default installation under B has old code, which causes the failure.\n\nThis line from each of your stacktraces is the reason for my suspicion:\n\n```\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n```\n\nSo your default installation B is under `/usr/local/lib/python2.7/dist-packages/tensorflow/...`, which is different from where you installed your sources.\n\nTo confirm, you can look at the following file.  I'm suspecting you'll see an old file, that doesn't include `dtype` in the `__init__` function.\n\n```\n/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/seq2seq_model.py\n```\n\nI am not a python expert, but I believe you can fix this via the PYTHONPATH environment variable.  Just set it to point at your sources (i.e. dir A), so that imports will be resolved via A instead of B.\nhttps://docs.python.org/2/using/cmdline.html#envvar-PYTHONPATH\n\nLet me know if that helps.\n", "@tatatodd, PYTHONPATH is a export variable where python looks for all module files. If I export this to my tensorflow src folder path then I start getting error for all other python modules like numpy etc.\nSo, this can't work because it will change the default path where python looks for other modules.\n", "@gaurav5670 First off, did you confirm that the following file to confirm the code-mismatch issue?\n\n```\n/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/translate/seq2seq_model.py\n```\n\nIf this is indeed the problem, it seems to me that you can set PYTHONPATH as follows:\n\n```\n# 1) Try this first\nexport PYTHONPATH=/path/to/your/sources:$PYTHONPATH\n\n# 2) If the above doesn't work, try setting dist-packages explicitly, *after* your sources.\nexport PYTHONPATH=/path/to/your/sources:/usr/local/lib/python2.7/dist-packages:$PYTHONPATH\n```\n\nWith (1) we're telling python to first resolve imports from your sources, falling back on whatever your original PYTHONPATH was set to.  With (2) we're telling python to first resolve from your sources, then from dist-packages, and finally from whatever your original PYTHONPATH was set to.\n\nWe're basically trying to set things up so that your sources take precedence over everything else.\n", "@tatatodd  following is the list of files in my /usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/\n\u251c\u2500\u2500 **init**.py\n\u251c\u2500\u2500 **init**.pyc\n\u251c\u2500\u2500 linear.py\n\u251c\u2500\u2500 linear.pyc\n\u251c\u2500\u2500 rnn_cell.py\n\u251c\u2500\u2500 rnn_cell.pyc\n\u251c\u2500\u2500 rnn.py\n\u251c\u2500\u2500 rnn.pyc\n\u251c\u2500\u2500 seq2seq.py\n\u2514\u2500\u2500 seq2seq.pyc\n\n0 directories, 10 files\n\nBy default, on a new terminal environment on my ubuntu 16.04, PYTHONPATH is empty.\nI tried setting export PYTHONPATH =  /usr/local/lib/python2.7/dist-packages/tensorflow/:$PYTHONPATH\nand \nexport PYTHONPATH =  /path/to/mysoruces/tensorflow/:$PYTHONPATH, where tensorflow is my sources directory head.\n\nI am guessing that this might be problem with tensorflow binary release which I have downloaded from \nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#test-the-tensorflow-installation\n\nAnd my machine does not have a GPU so I downloaded a non-GPU version of tensorflow using following commands,\n\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\nsudo pip install --upgrade $TF_BINARY_URL\n\nThere is one more thing which I tried, I installed tensorflow through virtual environment  and still the problem persists.\n", "@loretoparisi @gaurav5670 \nThe files to look at are these:\n\n```\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/translate.py\n```\n\nThese files seem to be out-of-sync somehow.  Perhaps the easiest thing to do is to uninstall everything, and install the latest version of TensorFlow:\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n\nLet me know if you still have issues.\n", "@tatatodd thank you please wait let me check again the whole process.\n", "@loretoparisi with latest version of tensorflow i.e r0.11 the error is gone.\nI could not reproduce this error now. Thanks for your support. You can close this bug now.\n", "The TensorFlow team is happy to be of service.\n", "@jart thanks a lot!\n", "I'm seeing a similar issue with TF on native Windows 10 (TF 0.12) for dense_shape while running the wide_n_deep_tutorial shipped with the package. I pulled the latest version of TF as well but still didn't resolve the problem:\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'dense_shape'\r\n\r\nCannot comment whether this happens on Unix also. I've python 3.5.2 installed. Any ideas?", "I recommend opening a new issue so it can go through triage. Windows support is very new.", "Ok thanks."]}, {"number": 4199, "title": "Unable to create a pip whl. ", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nhttps://github.com/tensorflow/tensorflow/issues/2040\n### Environment info\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n-rw-r--r-- 1 root root  189170 Dec 23  2015 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root      16 Dec 23  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root      19 Dec 23  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root  311596 Dec 23  2015 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root  558020 Dec 23  2015 /usr/local/cuda/lib/libcudart_static.a\nlrwxrwxrwx 1 root root      17 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so -> libcuinj32.so.7.5\nlrwxrwxrwx 1 root root      20 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so.7.5 -> libcuinj32.so.7.5.18\n-rwxr-xr-x 1 root root 5396088 Dec 23  2015 /usr/local/cuda/lib/libcuinj32.so.7.5.18\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n\n9b69ec3960cf9225df557fdc1ab673bd36bde4fb\n1. The output of `bazel version`\n\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp: Thu Jan 01 00:00:00 1970 (0)\nBuild timestamp as int: 0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nQuite simply, I am going through the instructions here(https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#create-the-pip-package-and-install), and get the following error:\n\ntc:/deeplearning/tensorflow$ bazel-bin/tensorflow/tools/pip_package/build_pip_package.sh /tmp/tensorflow_pkg\nbash: bazel-bin/tensorflow/tools/pip_package/build_pip_package.sh: No such file or directory\n### What other attempted solutions have you tried?\n\nThe fix from here (https://github.com/tensorflow/tensorflow/issues/2040), which was to do: cp -r bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/**main**/\\* bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/\n\nseemed to run properly, but this did not work in the end. \n", "comments": ["I tried to reproduce and ended up with a different error with 9b69ec3960cf9225df557fdc1ab673bd36bde4fb checked out, but don't see errors on the latest 0.10 release.\n\nIn general I would recommend using an official release, rather than an arbitrary commit.  FYI the 9b69ec3 commit was on June 6.\n\nHere are instructions on downloading a binary distribution:\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#download-and-setup\n\nFor source installs, you can use the `git clone -b` flag to pick a branch.  E.g. the following will clone the latest 0.10 release:\n\n```\n$ git clone -b v0.10.0rc0 https://github.com/tensorflow/tensorflow\n```\n\nHope this helps!  Please re-open if you still encounter issues on an official release.\n"]}, {"number": 4198, "title": "remove redundancy in moment ops", "body": "We subtracted the shift twice for both the first and second moments. Removing one subtraction slightly improves batch_norm performance. \n", "comments": ["Can one of the admins verify this patch?\n", "@thuyen, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @vincentvanhoucke and @keveman to be potential reviewers\n", "This change is wrong, for subtle reasons. The purpose of taking the difference before squaring is that it's a lot more stable numerically. See e.g.: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\nThis is not a mere theoretical concern: it happens in very common scenarios.\n", "This change still effectively takes the difference before squaring. It just doesn't recompute the difference (m_ss itself is the difference)\n", "Sorry, I had skimmed through your change too quickly. The issue here is different: if you use two separate kernels to compute `x-shift` and `square()` instead of the fused `squared_difference()`, you end up with two large tensors that need to be preserved through the forward _and_ backward pass: 1) the `x` tensor itself needed for the backward pass through `m_ss` 2) `x-shift`, which is needed for the backward pass through `square()`. So, while your change is a bit more efficient computationally, it also effectively doubles the amount of memory that's required to keep the activations around, and that's a killer for large models.\n", "Thanks Vincent. But am still a little bit confused. Isn't `x-shift` currently needed to backpropagate through reduce_sum as well? \nCurrently:\n`m_ss = math_ops.sub(x, shift)`\n`m_ss = math_ops.reduce_sum(m_ss, axes, keep_dims=keep_dims, name=\"mean_ss\")`\n", "Oh I see, `x-shift` might not be needed for backpropagation of `reduce_sum`!\nThanks!\n", "It might be worth documenting this subtlety in the code. It's not obvious at all and has very bad consequences when done wrong.\n"]}, {"number": 4197, "title": "Incorrect documentation for dynamic_rnn", "body": "The help for `tf.nn.dynamic_rnn` states that the optional `initial_state` kwarg, if `cell.state_size` is an integer, must be a `Tensor` of appropriate type and shape `[batch_size x cell.state_size]`. This is incorrect; the shape must be actually be `[batch_size, cell.state_size]`.\n", "comments": ["Thanks for the report!  I'll update this issue once the fixed documentation has been merged in.\n", "OK the docs have been updated, thanks @vladfi1 for filing the issue!\n"]}, {"number": 4196, "title": "Error when running two simultaneous sessions", "body": "Can I run two simultaneous sessions?\n\nI start with: \"with tf.Graph().as_default(), tf.Session() as sess:\"\n\nWhen I run my script alone, the result is OK, but when I run 2 instances the result is completely wrong or throw some errors like:\n\nError max() arg is an empty sequence\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Quadro K2000M\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.745\npciBusID 0000:01:00.0\nTotal memory: 1.95GiB\nFree memory: 1.47GiB\n\n(and also)\nfloat division by zero\n\nCan I do something in my code do avoid these errors?\n\nMaybe check if TF is already running and hold it?\n\nThan you very much.\n\nOS: Arch Linux\n\n$ ls -l /opt/cuda/lib/libcud*\n/opt/cuda/lib/libcudadevrt.a\n/opt/cuda/lib/libcudart.so.7.5.18\n/opt/cuda/lib/libcudart_static.a\n/opt/cuda/lib/libcudnn.so -> /opt/cuda/lib64/libcudnn.so.4.0.7\n\nPIP: 0.9.0\n", "comments": ["If I'm understanding you correctly, you are running two instances of the same python program, each in its own process.  I believe that should work.\n\nOne simple suggestion is to try with the latest release version 0.10.0.\n\nOtherwise, can you try to boil your problem down to a minimal program that exhibits the failure?  That will either narrow in on the problem, or allow me to reproduce.  Thanks!\n", "Actually you probably don't want to be using the same GPU simultaneously from two TensorFlow processes.  It's somewhat safe to pause one while the other uses the GPU, but if they are both actively using the GPU, bad things are likely to happen, I believe.\n", "I have tried using the same GPU from two TF processes simultaneously. Sometimes both run; sometimes one crashes with messages like these:\n\n```\nE tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\nW tensorflow/stream_executor/stream.cc:1334] attempting to perform BLAS operation using StreamExecutor without BLAS support\nE tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\nW tensorflow/stream_executor/stream.cc:1334] attempting to perform BLAS operation using StreamExecutor without BLAS support\nE tensorflow/stream_executor/cuda/cuda_blas.cc:367] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\nW tensorflow/stream_executor/stream.cc:1334] attempting to perform BLAS operation using StreamExecutor without BLAS support\nE tensorflow/core/client/tensor_c_api.cc:485] Blas SGEMM launch failed : a.shape=(128, 100), b.shape=(100, 16), m=128, n=16, k=100\n```\n\nWhy is that?\n", "Agree with the post above (and had the same errors complaining about not finding CUBLAS). I had to close my other tensorflow terminals to re-run a new tensorflow program that is calling a single GPU.\n", "I got the same issue when running 2 Tensorflow/Keras at the same time, each in its process. The error is: \"failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\". Any suggestion ? Thanks a lot.", "I've also encountered this problem today. Any tips? ", "`CUBLAS_STATUS_NOT_INITIALIZED` can be thrown when you are out of memory, which can happen when you run two GPU TensorFlow processes in parallel. TensorFlow doesn't like sharing GPU memory with another process, the solution is to run only a single TensorFlow process per GPU (using `CUDA_VISIBLE_DEVICES=k` to restrict k'th process to k'th GPU) ", "Just to be clear, this means that if I have only one GPU on my machine, I should only have one Tensorflow session (or code) active? No other easy way around it?\r\n\r\nTo be concerete, I have a file `dqn.py` which runs the DQN algorithm on my GPU. Can I run `python dqn.py` in one terminal window, and then open the python script, change the random seed, then run`python dqn.py` again on a second terminal window?\r\n\r\nI assume I'd need to set the GPU memory allocation to be <50% for this to work.", "You should have at most one active TensorFlow process per GPU. Use CUDA_VISIBLE_DEVICES to determine which process gets which GPU (CUDA_VISIBLE_DEVICES='' will disable GPU)", "I'm trying to run 2 models in 2 different process (multiprocess in python). Each process create a session. One load a MTCNN model and the other one load a CNN. 1/20 times, both work fine. \r\nIf I run one or another process alone, work just fine. I set <50% of GPU memory on each process\r\nMy error is:\r\n`InternalError: Failed to create session.`\r\n\r\nOr:\r\n`F tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) `\r\n\r\nAlways the same process fail...", "Its better to use Theano if you want to run two process on same GPU.", "how to create multiple instances using theano. If i change OMP_NUM_THREADS = 2 two cores will be enabled.But i want to create more instances like multi processing instead of having multiple workers.I want to send the multiple requests using theano is it possible?", "By using  `gpu_options.per_process_gpu_memory_fraction` or `gpu_options.allow_growth` to limit memory pre-allocation of each tf process, I was able to run multiple tf processes on the same GPU simultaneously. (of course, your model needs to be small enough in order to do so)\r\nBut I am not sure if there will be any other problems.\r\nI also don't know how this will affect the computing speed of each process. ", "As far as I know, TF can't have more than one session per GPU. It will have problem accessing gpu memory and using the Cuda cores. If you want multiple models go to TensorflowServing", "same problem for multiple process run on same GPU, I have some machine with titanx and ubuntu 14 , others with p40 and centos 7, with `gpu_options.per_process_gpu_memory_fraction` and `gpu_options.allow_growth`, I can run on titanx machine, but always get `CUBLAS_STATUS_NOT_INITIALIZED` on p40 machines", "@tatatodd I think this issue should not be closed, but tagged as Question. In my case we are running Keras and a tensorflow model. And the same error occurs.", "I have a similar problem. I use an HPC server with 6 different compute nodes I run tensorflow simultaneously on two different nodes . I still got an error on one of them. Is that not strange?"]}, {"number": 4195, "title": "Add -O2 to the adding an op HOWTO", "body": "Based on a [request from StackOverflow](http://stackoverflow.com/questions/39280669/best-way-to-modify-a-built-in-tensorflow-kernel/39301780?noredirect=1#comment65975519_39301780) where a user observed that the same code compiled following these instructions is 10x slower than code built as part of the binary installation.\n", "comments": ["@mrry, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @rryan to be potential reviewers\n"]}, {"number": 4194, "title": "Syntax Errors in Python 3.5 : Installed using pip3", "body": "I followed the instructions on [installing TensorFlow using pip](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#optional-install-cuda-gpus-on-linux) and installed both the `Ubuntu/Linux 64-bit, GPU enabled, Python 3.5` and `Ubuntu/Linux 64-bit, GPU enabled, Python 2.7` versions using pip3 and pip2 respectively. The package imports correctly in Python 2.7 but the following SyntaxErrors occur for Python 3.5.\n\n``` terminal\npython3\nPython 3.5.2 (default, Jul  5 2016, 12:43:10) \n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n```\n\n``` python\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n...\n  File \"/home/gszep/.local/lib/python3.5/site-packages/dateutil/parser.py\", line 158\n    l.append(\"%s=%s\" % (attr, `value`))\n                              ^\nSyntaxError: invalid syntax\n```\n", "comments": ["After editing line 158 in `/home/gszep/.local/lib/python3.5/site-packages/dateutil/parser.py`, replacing ``value`` with `'value'` which is probably the ascii character causing the problem, I get a new import error:\n\n``` python\n>>> import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n  File \"/home/gszep/.local/lib/python3.5/site-packages/dateutil/parser.py\", line 303\n    raise ValueError, \"unknown string format\"\n                    ^\nSyntaxError: invalid syntax\n```\n", "Editing lines 303 and 326 in that file finally fixed the problem. The change has to do with the update syntax `ValueError, \"string\"` to  `ValueError(\"string\")`\n", "I got this fixed by installing python-dateutil fom pypi.org.\r\npip install python-dateutil\r\nOR\r\npipenv install python-dateutil"]}, {"number": 4193, "title": "CTC loss is numerically unstable for long sequence lengths", "body": "Hi All, great work TF team, thanks for implementing CTC loss ops recently!\n\nI've noticed that CTC loss becomes quite numerically unstable for long sequences, in the attached code that reproduces the plot from the CTC paper, sequences longer than 10,000 or so degrade the quality of the gradients quite severely.\n\nI'm not sure what the best fix is, trying to improve the numerical stability of CTC loss calculation could help, but I have no idea how difficult this would be.\n\nIn the interim, I suppose it might be worth issuing a warning when the op is used with long sequences, or placing internal consistency checks that warn the user when CTC loss may be producing bogus outputs. Or even just improving the documentation for CTC loss so that users know that this is a pitfall.\n\nThis is also sort of a public service announcement so nobody gets burned on long sequences.\n\nAgain, great work, thanks for all your effort.\n\n:)\n### Environment info\n\nPip Package Version: 0.10.0rc0\n### Minimal Example\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# n = 100\nn = 20000\nk = 4\n\nwith tf.Session() as sess:\n    inputs = tf.zeros((n, 1, k+1), dtype=tf.float32)\n    labels_indices = tf.constant([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]], dtype=tf.int64)\n    labels_values  = tf.constant([0, 1, 2, 3, 4], dtype=tf.int32)\n    labels = tf.SparseTensor(indices=labels_indices, values=labels_values, shape=(1, k+1))\n    sequence_length = np.array([n])\n\n    loss = tf.nn.ctc_loss(inputs, labels, sequence_length)\n    g, = tf.gradients(loss, inputs)\n\n    g_v = sess.run(g)\n\nplt.plot(-g_v[:,0,:])\n```\n", "comments": ["@ebrevdo can you comment on this?  Thanks!\n", "@andykitchen can you reference the CTC paper & plot you referred to?\n", "<del>Hi @ebrevdo the plot I'm talking about is the plot created by the code snipped in the issue.\n\n<del>There's no reference, the instability is pretty obvious once you change n to be on the scale of 10s of thousands. Im not currently aware of any published references analysing the numerical stability of CTC\n\nEDIT: Oops! misunderstood the question\n\nThe paper is Alex Graves' 2006 paper \"Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks\". The plot I'm talking about is figure 4 in this paper. I've added a image for your convenience:\n\n<img width=\"373\" alt=\"screen shot 2016-09-09 at 10 01 48 am\" src=\"https://cloud.githubusercontent.com/assets/29637/18370999/79df2ef0-7674-11e6-83cc-95f542457d55.png\">\n\nI've also added some example plots from the snippet:\n\n![cbakgaaaabjru5erkjggg](https://cloud.githubusercontent.com/assets/29637/18371046/e23f6dac-7674-11e6-8b15-dc2ebb27034b.png)\n![dfrmiwdmpw5f8b19zhtyg87ryaaaaasuvork5cyii](https://cloud.githubusercontent.com/assets/29637/18371048/e46f3e18-7674-11e6-96fa-f5974115a757.png)\n![un1woj5hiiaaaaaelftksuqmcc](https://cloud.githubusercontent.com/assets/29637/18371051/e6dcc256-7674-11e6-9b95-b518ffc5d9fc.png)\n", "ping!\r\nAny updates here?\r\nIs this still a problem with TensorFlow 1.2?", "@gunan I can confirm that I see the same problem on TensorFlow 1.4.", "What happens if you run your lstm with float64 then cast to foat32 for the CTC inputs?  Alternatively set num_inter_op_parallel_threads=1?  Do you still get numerical issues?", "@ebrevdo sorry it took this long to answer.\r\nI am not running any recurrent layers, just OP's code in `Minimal Example` section. Will try `num_inter_op_parallel_threads=1`.\r\n\r\nAlso going to check if cuDNN 7's [`cudnnCTCLoss`](http://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnCTCLoss) exhibits this issue. Btw, anyone working on porting it to tensorflow?", "@ebrevdo so I've set both `inter_op_parallelism_threads=1, intra_op_parallelism_threads=1` in `tf.ConfigProto`, but I am still having the same problem. Just to be clear: [here](https://github.com/standy66/tf_ctc_loss_unstable/blob/master/tf_ctc_loss_unstable.ipynb) is the notebook I'm running. I believe that plots in `In [5], In [6]` should be the same as `In [3], In [4]`, but they are not.", "Apparently, cudnnCTCLoss suffers from the same numerical instability in gradients. [Here](https://github.com/standy66/cudnn7-ctc-loss-test/blob/master/example.cpp) is the sample code that I wrote. So far, all three CTC loss implementations that I checked (tf's, cudnn's and warp-ctc) are unsuitable for sequences with length > 5000 because of these bogus gradients.", "You could try adding float64 support to TF's CTC loss kernel.\n\nOn Wed, Dec 27, 2017, 2:30 PM Andrew Stepanov <notifications@github.com>\nwrote:\n\n> Apparently, cudnnCTCLoss suffers from the same numerical instability in\n> gradients. Here\n> <https://github.com/standy66/cudnn7-ctc-loss-test/blob/master/example.cpp>\n> is the sample code that I wrote. So far, all three CTC loss implementations\n> that I checked (tf's, cudnn's and warp-ctc) are unsuitable for sequences\n> with length > 5000 because of these bogus gradients.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4193#issuecomment-354187083>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim76YqurPQa1EoOSh3KUoPGRQP5VOks5tEsT4gaJpZM4J0ik6>\n> .\n>\n", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I just ran into the same problem, with a speech recording longer than 1100 frames reporting \"No valid path found\". My TF version is 1.4.1.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I will leave this as contributions welcome, if someone would like to contribute, you can try adding float64 support for TF CTC loss kernel as @ebrevdo suggested.", "Added #21822 ", "On request added #31164", "Can we close this?", "Yep\n\nOn Tue, Sep 15, 2020, 3:32 PM bhack <notifications@github.com> wrote:\n\n> Can we close this?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4193#issuecomment-693014148>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG3JKNIEYOMX5XQZG7TSF7TPRANCNFSM4COSFE5A>\n> .\n>\n"]}, {"number": 4192, "title": "Fix typo", "body": "", "comments": ["@chanis, thanks for your PR! By analyzing the annotation information on this pull request, we identified @ilblackdragon, @vrv and @tensorflower-gardener to be potential reviewers\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@chanis Thanks for the PR! Please sign the CLA so we can merge it. \n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 4191, "title": "Fix a bug #3994", "body": "I have fixed the bug[#3994](https://github.com/tensorflow/tensorflow/issues/3994). So, now, we can use `Variables` as gradients with the update.\nCheck if gradient is `tf.Variable`:\n- Yes, convert it to tensor.\n- No, check its type as before.\n\nThen, remake the `grads_and_vars` after checking and converting.\n", "comments": ["Can one of the admins verify this patch?\n", "@DjangoPeng, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @gibiansky to be potential reviewers\n", "@mrry Can you review this please?\n", "Please add a test that passes a `tf.Variable` to `Optimizer.apply_gradients()` in `optimizer_test.py`.\n", "@mrry  I have updated the `optimizer.py` and added the test that passes a `tf.Variable` to `Optimizer.apply_gradient()` in `optimizer_test.py`. But, I added an import of `ops`, because I have to use the `ops.control_dependencies()` to assist that convert gradient to `tf.Variable`. :(\n", "@mrry I have removed the unnecessary import and `control_dependencies`\n", "Great, thanks! This LGTM after the tests pass.\n", "@tensorflow-jenkins test this please.\n", "@mrry Sounds good~BTW, what is LGTM :)\n", "It means \"Looks Good To Me.\" And now the tests have passed, so we can merge it, thanks!\n"]}]